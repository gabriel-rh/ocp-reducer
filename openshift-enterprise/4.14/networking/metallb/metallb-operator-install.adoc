:_mod-docs-content-type: ASSEMBLY
[id="metallb-operator-install"]
= Installing the MetalLB Operator
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: metallb-operator-install

toc::[]

As a cluster administrator, you can add the MetallB Operator so that the Operator can manage the lifecycle for an instance of MetalLB on your cluster.

MetalLB and IP failover are incompatible. If you configured IP failover for your cluster, perform the steps to xref:../../networking/configuring-ipfailover.adoc#nw-ipfailover-remove_configuring-ipfailover[remove IP failover] before you install the Operator.


// Install the Operator with console
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-the-metallb-operator-using-web-console_{context}"]
= Installing the MetalLB Operator from the OperatorHub using the web console

As a cluster administrator, you can install the MetalLB Operator by using the {product-title} web console.

.Prerequisites

* Log in as a user with `cluster-admin` privileges.

.Procedure

. In the {product-title} web console, navigate to *Operators* -> *OperatorHub*.

. Type a keyword into the *Filter by keyword* box or scroll to find the Operator you want. For example, type `metallb` to find the MetalLB Operator.
+
You can also filter options by *Infrastructure Features*. For example, select *Disconnected* if you want to see Operators that work in disconnected environments, also known as restricted network environments.

. On the *Install Operator* page, accept the defaults and click *Install*.

.Verification

. To confirm that the installation is successful:

.. Navigate to the *Operators* -> *Installed Operators* page.

.. Check that the Operator is installed in the `openshift-operators` namespace and that its status is `Succeeded`.

. If the Operator is not installed successfully, check the status of the Operator and review the logs:

.. Navigate to the *Operators* -> *Installed Operators* page and inspect the `Status` column for any errors or failures.

.. Navigate to the *Workloads* -> *Pods* page and check the logs in any pods in the `openshift-operators` project that are reporting issues.

:leveloffset!:

// Install the Operator with CLI
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-metallb-installing-operator-cli_{context}"]
= Installing from OperatorHub using the CLI

Instead of using the {product-title} web console, you can install an Operator from OperatorHub using the CLI. You can use the OpenShift CLI (`oc`) to install the MetalLB Operator.

It is recommended that when using the CLI you install the Operator in the `metallb-system` namespace.

.Prerequisites

* A cluster installed on bare-metal hardware.
* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Create a namespace for the MetalLB Operator by entering the following command:
+
[source,terminal]
----
$ cat << EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: metallb-system
EOF
----

. Create an Operator group custom resource (CR) in the namespace:
+
[source,terminal]
----
$ cat << EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: metallb-operator
  namespace: metallb-system
EOF
----

. Confirm the Operator group is installed in the namespace:
+
[source,terminal]
----
$ oc get operatorgroup -n metallb-system
----
+
.Example output
[source,terminal]
----
NAME               AGE
metallb-operator   14m
----

. Create a `Subscription` CR:
.. Define the `Subscription` CR and save the YAML file, for example, `metallb-sub.yaml`:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator-sub
  namespace: metallb-system
spec:
  channel: stable
  name: metallb-operator
  source: redhat-operators <1>
  sourceNamespace: openshift-marketplace
----
<1> You must specify the `redhat-operators` value.

.. To create the `Subscription` CR, run the following command:
+
[source,terminal]
----
$ oc create -f metallb-sub.yaml
----

. Optional: To ensure BGP and BFD metrics appear in Prometheus, you can label the namespace as in the following command:
+
[source,terminal]
----
$ oc label ns metallb-system "openshift.io/cluster-monitoring=true"
----

.Verification

The verification steps assume the MetalLB Operator is installed in the `metallb-system` namespace.

. Confirm the install plan is in the namespace:
+
[source,terminal]
----
$ oc get installplan -n metallb-system
----
+
.Example output
[source,terminal,subs="attributes+"]
----
NAME            CSV                                   APPROVAL    APPROVED
install-wzg94   metallb-operator.{product-version}.0-nnnnnnnnnnnn   Automatic   true
----
+
[NOTE]
====
Installation of the Operator might take a few seconds.
====

. To verify that the Operator is installed, enter the following command:
+
[source,terminal]
----
$ oc get clusterserviceversion -n metallb-system \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase
----
+
.Example output
[source,terminal,subs="attributes+"]
----
Name                                  Phase
metallb-operator.{product-version}.0-nnnnnnnnnnnn   Succeeded
----

:leveloffset!:

// Starting MetalLB on your cluster
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-metallb-operator-initial-config_{context}"]
= Starting MetalLB on your cluster

After you install the Operator, you need to configure a single instance of a MetalLB custom resource. After you configure the custom resource, the Operator starts MetalLB on your cluster.

.Prerequisites

* Install the OpenShift CLI (`oc`).

* Log in as a user with `cluster-admin` privileges.

* Install the MetalLB Operator.


.Procedure

This procedure assumes the MetalLB Operator is installed in the `metallb-system` namespace. If you installed using the web console substitute `openshift-operators` for the namespace.

. Create a single instance of a MetalLB custom resource:
+
[source,terminal]
----
$ cat << EOF | oc apply -f -
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
EOF
----

.Verification

Confirm that the deployment for the MetalLB controller and the daemon set for the MetalLB speaker are running.

. Verify that the deployment for the controller is running:
+
[source,terminal]
----
$ oc get deployment -n metallb-system controller
----
+
.Example output
[source,terminal]
----
NAME         READY   UP-TO-DATE   AVAILABLE   AGE
controller   1/1     1            1           11m
----

. Verify that the daemon set for the speaker is running:
+
[source,terminal]
----
$ oc get daemonset -n metallb-system speaker
----
+
.Example output
[source,terminal]
----
NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
speaker   6         6         6       6            6           kubernetes.io/os=linux   18m
----
+
The example output indicates 6 speaker pods. The number of speaker pods in your cluster might differ from the example output. Make sure the output indicates one pod for each node in your cluster.


:leveloffset!:

// Deployment specifications for metallb CR
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

[id="nw-metallb-operator-deployment-specifications-for-metallb_{context}"]
= Deployment specifications for MetalLB

When you start an instance of MetalLB using the `MetalLB` custom resource, you can configure deployment specifications in the `MetalLB` custom resource to manage how the `controller` or `speaker` pods deploy and run in your cluster. Use these deployment specifications to manage the following tasks:

* Select nodes for MetalLB pod deployment.
* Manage scheduling by using pod priority and pod affinity.
* Assign CPU limits for MetalLB pods.
* Assign a container RuntimeClass for MetalLB pods.
* Assign metadata for MetalLB pods.

:leveloffset!:

// Deployment specs to limit speaker pods to specific nodes
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

[id="nw-metallb-operator-limit-speaker-to-nodes_{context}"]
= Limit speaker pods to specific nodes

By default, when you start MetalLB with the MetalLB Operator, the Operator starts an instance of a `speaker` pod on each node in the cluster.
Only the nodes with a `speaker` pod can advertise a load balancer IP address.
You can configure the `MetalLB` custom resource with a node selector to specify which nodes run the `speaker` pods.

The most common reason to limit the `speaker` pods to specific nodes is to ensure that only nodes with network interfaces on specific networks advertise load balancer IP addresses.
Only the nodes with a running `speaker` pod are advertised as destinations of the load balancer IP address.

If you limit the `speaker` pods to specific nodes and specify `local` for the external traffic policy of a service, then you must ensure that the application pods for the service are deployed to the same nodes.

.Example configuration to limit speaker pods to worker nodes
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  nodeSelector:  <.>
    node-role.kubernetes.io/worker: ""
  speakerTolerations:   <.>
  - key: "Example"
    operator: "Exists"
    effect: "NoExecute"
----
<.> The example configuration specifies to assign the speaker pods to worker nodes, but you can specify labels that you assigned to nodes or any valid node selector.
<.> In this example configuration, the pod that this toleration is attached to tolerates any taint that matches the `key` value and `effect` value using the `operator`.

After you apply a manifest with the `spec.nodeSelector` field, you can check the number of pods that the Operator deployed with the `oc get daemonset -n metallb-system speaker` command.
Similarly, you can display the nodes that match your labels with a command like `oc get nodes -l node-role.kubernetes.io/worker=`.

You can optionally allow the node to control which speaker pods should, or should not, be scheduled on them by using affinity rules. You can also limit these pods by applying a list of tolerations. For more information about affinity rules, taints, and tolerations, see the additional resources.

:leveloffset!:

// Deployment specs to set RuntimeClass
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

[id="nw-metallb-operator-setting-runtimeclass_{context}"]
= Configuring a container runtime class in a MetalLB deployment

You can optionally assign a container runtime class to `controller` and `speaker` pods by configuring the MetalLB custom resource. For example, for Windows workloads, you can assign a Windows runtime class to the pod, which uses this runtime class for all containers in the pod.

.Prerequisites

* You are logged in as a user with `cluster-admin` privileges.

* You have installed the MetalLB Operator.

.Procedure
. Create a `RuntimeClass` custom resource, such as `myRuntimeClass.yaml`, to define your runtime class:
+
[source,yaml,options="nowrap",role="white-space-pre"]
----
apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: myclass
handler: myconfiguration
----

. Apply the `RuntimeClass` custom resource configuration:
+
[source,bash]
----
$ oc apply -f myRuntimeClass.yaml
----

. Create a `MetalLB` custom resource, such as `MetalLBRuntime.yaml`, to specify the `runtimeClassName` value:
+
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    runtimeClassName: myclass
    annotations: <1>
      controller: demo
  speakerConfig:
    runtimeClassName: myclass
    annotations: <1>
      speaker: demo
----
<1> This example uses `annotations` to add metadata such as build release information or GitHub pull request information. You can populate annotations with characters that are not permitted in labels. However, you cannot use annotations to identify or select objects.

. Apply the `MetalLB` custom resource configuration:
+
[source,bash,options="nowrap",role="white-space-pre"]
----
$ oc apply -f MetalLBRuntime.yaml
----

.Verification
* To view the container runtime for a pod, run the following command:
+
[source,bash,options="nowrap",role="white-space-pre"]
----
$ oc get pod -o custom-columns=NAME:metadata.name,STATUS:.status.phase,RUNTIME_CLASS:.spec.runtimeClassName
----

:leveloffset!:

// Deployment specs to set pod priority and pod affinity
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

[id="nw-metallb-operator-setting-pod-priority-affinity_{context}"]
= Configuring pod priority and pod affinity in a MetalLB deployment

You can optionally assign pod priority and pod affinity rules to `controller` and `speaker` pods by configuring the `MetalLB` custom resource. The pod priority indicates the relative importance of a pod on a node and schedules the pod based on this priority. Set a high priority on your `controller` or `speaker` pod to ensure scheduling priority over other pods on the node.

Pod affinity manages relationships among pods. Assign pod affinity to the `controller` or `speaker` pods to control on what node the scheduler places the pod in the context of pod relationships. For example, you can use pod affinity rules to ensure that certain pods are located on the same node or nodes, which can help improve network communication and reduce latency between those components.

.Prerequisites

* You are logged in as a user with `cluster-admin` privileges.

* You have installed the MetalLB Operator.

* You have started the MetalLB Operator on your cluster.

.Procedure
. Create a `PriorityClass` custom resource, such as `myPriorityClass.yaml`, to configure the priority level. This example defines a `PriorityClass` named `high-priority` with a value of `1000000`. Pods that are assigned this priority class are considered higher priority during scheduling compared to pods with lower priority classes:
+
[source,yaml]
----
apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000
----

. Apply the `PriorityClass` custom resource configuration:
+
[source,bash]
----
$ oc apply -f myPriorityClass.yaml
----

. Create a `MetalLB` custom resource, such as `MetalLBPodConfig.yaml`, to specify the `priorityClassName` and `podAffinity` values:
+
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    priorityClassName: high-priority <1>
    affinity:
      podAffinity: <2>
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
             app: metallb
          topologyKey: kubernetes.io/hostname
  speakerConfig:
    priorityClassName: high-priority
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
             app: metallb
          topologyKey: kubernetes.io/hostname
----
+
<1> Specifies the priority class for the MetalLB controller pods. In this case, it is set to `high-priority`.
<2> Specifies that you are configuring pod affinity rules. These rules dictate how pods are scheduled in relation to other pods or nodes. This configuration instructs the scheduler to schedule pods that have the label `app: metallb` onto nodes that share the same hostname. This helps to co-locate MetalLB-related pods on the same nodes, potentially optimizing network communication, latency, and resource usage between these pods.

. Apply the `MetalLB` custom resource configuration:
+
[source,bash]
----
$ oc apply -f MetalLBPodConfig.yaml
----

.Verification
* To view the priority class that you assigned to pods in the `metallb-system` namespace, run the following command:
+
[source,bash]
----
$ oc get pods -n metallb-system -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName
----
+
.Example output
+
[source,terminal]
----
NAME                                                 PRIORITY
controller-584f5c8cd8-5zbvg                          high-priority
metallb-operator-controller-manager-9c8d9985-szkqg   <none>
metallb-operator-webhook-server-c895594d4-shjgx      <none>
speaker-dddf7                                        high-priority
----

* To verify that the scheduler placed pods according to pod affinity rules, view the metadata for the pod's node or nodes by running the following command:
+
[source,bash]
----
$ oc get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name -n metallb-system
----

:leveloffset!:

// Deployment specs to set pod CPU limits
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/metallb/metallb-operator-install.adoc

[id="nw-metallb-operator-setting-pod-CPU-limits_{context}"]
= Configuring pod CPU limits in a MetalLB deployment

You can optionally assign pod CPU limits to `controller` and `speaker` pods by configuring the `MetalLB` custom resource. Defining CPU limits for the `controller` or `speaker` pods helps you to manage compute resources on the node. This ensures all pods on the node have the necessary compute resources to manage workloads and cluster housekeeping.

.Prerequisites

* You are logged in as a user with `cluster-admin` privileges.

* You have installed the MetalLB Operator.

.Procedure
. Create a `MetalLB` custom resource file, such as `CPULimits.yaml`, to specify the `cpu` value for the `controller` and `speaker` pods:
+
[source,yaml]
----
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    resources:
      limits:
        cpu: "200m"
  speakerConfig:
    resources:
      limits:
        cpu: "300m"
----

. Apply the `MetalLB` custom resource configuration:
+
[source,bash]
----
$ oc apply -f CPULimits.yaml
----

.Verification
* To view compute resources for a pod, run the following command, replacing `<pod_name>` with your target pod:
+
[source,bash]
----
$ oc describe pod <pod_name>
----

:leveloffset!:



[role="_additional-resources"]
[id="additional-resources_metallb-operator-install"]
== Additional resources

* xref:../../nodes/scheduling/nodes-scheduler-node-selectors.adoc#nodes-scheduler-node-selectors[Placing pods on specific nodes using node selectors]
* xref:../../nodes/scheduling/nodes-scheduler-taints-tolerations.adoc#nodes-scheduler-taints-tolerations-about[Understanding taints and tolerations]
* xref:../../nodes/pods/nodes-pods-priority.adoc#nodes-pods-priority-about_nodes-pods-priority[Understanding pod priority]
* xref:../../nodes/scheduling/nodes-scheduler-pod-affinity.adoc#nodes-scheduler-pod-affinity-about_nodes-scheduler-pod-affinity[Understanding pod affinity]

[id="next-steps_{context}"]
== Next steps

* xref:../../networking/metallb/metallb-configure-address-pools.adoc#metallb-configure-address-pools[Configuring MetalLB address pools]

//# includes=_attributes/common-attributes,modules/metallb-installing-using-web-console,modules/nw-metallb-installing-operator-cli,modules/nw-metallb-operator-initial-config,modules/nw-metallb-operator-deployment-specifications-for-metallb,modules/nw-metallb-operator-limit-speaker-to-nodes,modules/nw-metallb-operator-setting-runtimeclass,modules/nw-metallb-operator-setting-pod-priority-affinity,modules/nw-metallb-operator-setting-pod-CPU-limits

:_mod-docs-content-type: ASSEMBLY
[id="migrate-from-openshift-sdn"]
= Migrating from the OpenShift SDN network plugin
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: migrate-from-openshift-sdn

toc::[]

As a cluster administrator, you can migrate to the OVN-Kubernetes network plugin from the OpenShift SDN network plugin.

To learn more about OVN-Kubernetes, read xref:../../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes#about-ovn-kubernetes[About the OVN-Kubernetes network plugin].

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc

[id="nw-ovn-kubernetes-migration-about_{context}"]
= Migration to the OVN-Kubernetes network plugin

Migrating to the OVN-Kubernetes network plugin is a manual process that includes some downtime during which your cluster is unreachable. Although a rollback procedure is provided, the migration is intended to be a one-way process.

A migration to the OVN-Kubernetes network plugin is supported on the following platforms:

* Bare metal hardware
* Amazon Web Services (AWS)
* Google Cloud Platform (GCP)
* IBM Cloud
* Microsoft Azure
* {rh-openstack-first}
* VMware vSphere

[IMPORTANT]
====
Migrating to or from the OVN-Kubernetes network plugin is not supported for managed OpenShift cloud services such as {product-dedicated}, Azure Red Hat OpenShift(ARO), and Red Hat OpenShift Service on AWS (ROSA).
====

[id="considerations-migrating-ovn-kubernetes-network-provider_{context}"]
== Considerations for migrating to the OVN-Kubernetes network plugin

If you have more than 150 nodes in your {product-title} cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.

The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.

While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.

* If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:
+
--
* Namespace isolation
* Egress router pods
--

* If your cluster or surrounding network uses any part of the `100.64.0.0/16` address range, you must choose another unused IP range by specifying the `v4InternalSubnet` spec under the `spec.defaultNetwork.ovnKubernetesConfig` object definition. OVN-Kubernetes uses the IP range `100.64.0.0/16` internally by default.

The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.

[discrete]
[id="namespace-isolation_{context}"]
=== Namespace isolation

OVN-Kubernetes supports only the network policy isolation mode.

[IMPORTANT]
====
If your cluster uses OpenShift SDN configured in either the multitenant or subnet isolation modes, you cannot migrate to the OVN-Kubernetes network plugin.
====

[discrete]
[id="egress-ip-addresses_{context}"]
=== Egress IP addresses

OpenShift SDN supports two different Egress IP modes:

* In the _automatically assigned_ approach, an egress IP address range is assigned to a node.
* In the _manually assigned_ approach, a list of one or more egress IP addresses is assigned to a node.

The migration process supports migrating Egress IP configurations that use the automatically assigned mode.

The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in egress IP address configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Create an `EgressIPs` object
* Add an annotation on a `Node` object

|
* Patch a `NetNamespace` object
* Patch a `HostSubnet` object
|===

For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".

[discrete]
[id="egress-network-policies_{context}"]
=== Egress network policies

The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in egress network policy configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Create an `EgressFirewall` object in a namespace

|
* Create an `EgressNetworkPolicy` object in a namespace
|===

[NOTE]
====
Because the name of an `EgressFirewall` object can only be set to `default`, after the migration all migrated `EgressNetworkPolicy` objects are named `default`, regardless of what the name was under OpenShift SDN.

If you subsequently rollback to OpenShift SDN, all `EgressNetworkPolicy` objects are named `default` as the prior name is lost.

For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".
====

[discrete]
[id="egress-router-pods_{context}"]
=== Egress router pods

OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.

When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.

[discrete]
[id="multicast_{context}"]
=== Multicast

The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:

.Differences in multicast configuration
[cols="1a,1a",options="header"]
|===
|OVN-Kubernetes|OpenShift SDN

|
* Add an annotation on a `Namespace` object

|
* Add an annotation on a `NetNamespace` object
|===

For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".

[discrete]
[id="network-policies_{context}"]
=== Network policies

OVN-Kubernetes fully supports the Kubernetes `NetworkPolicy` API in the `networking.k8s.io/v1` API group. No changes are necessary in your network policies when migrating from OpenShift SDN.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc
// * networking/openshift_sdn/migrate-to-openshift-sdn.adoc

:sdn: OVN-Kubernetes
:previous-sdn: OpenShift SDN
:type: OVNKubernetes

[id="how-the-migration-process-works_{context}"]
= How the migration process works

The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.

.Migrating to {sdn} from {previous-sdn}
[cols="1,1a",options="header"]
|===

|User-initiated steps|Migration activity

|
Set the `migration` field of the `Network.operator.openshift.io` custom resource (CR) named `cluster` to `{type}`. Make sure the `migration` field is `null` before setting it to a value.
|
Cluster Network Operator (CNO):: Updates the status of the `Network.config.openshift.io` CR named `cluster` accordingly.
Machine Config Operator (MCO):: Rolls out an update to the systemd configuration necessary for {sdn}; the MCO updates a single machine per pool at a time by default, causing the total time the migration takes to increase with the size of the cluster.

|Update the `networkType` field of the `Network.config.openshift.io` CR.
|
CNO:: Performs the following actions:
+
--
* Destroys the {previous-sdn} control plane pods.
* Deploys the {sdn} control plane pods.
* Updates the Multus objects to reflect the new network plugin.
--

|
Reboot each node in the cluster.
|
Cluster:: As nodes reboot, the cluster assigns IP addresses to pods on the {sdn} cluster network.

|===

If a rollback to OpenShift SDN is required, the following table describes the process.

.Performing a rollback to OpenShift SDN
[cols="1,1a",options="header"]
|===

|User-initiated steps|Migration activity

|Suspend the MCO to ensure that it does not interrupt the migration.
|The MCO stops.

|
Set the `migration` field of the `Network.operator.openshift.io` custom resource (CR) named `cluster` to `OpenShiftSDN`. Make sure the `migration` field is `null` before setting it to a value.
|
CNO:: Updates the status of the `Network.config.openshift.io` CR named `cluster` accordingly.

|Update the `networkType` field.
|
CNO:: Performs the following actions:
+
--
* Destroys the OVN-Kubernetes control plane pods.
* Deploys the OpenShift SDN control plane pods.
* Updates the Multus objects to reflect the new network plugin.
--

|
Reboot each node in the cluster.
|
Cluster:: As nodes reboot, the cluster assigns IP addresses to pods on the OpenShift-SDN network.

|
Enable the MCO after all nodes in the cluster reboot.
|
MCO:: Rolls out an update to the systemd configuration necessary for OpenShift SDN; the MCO updates a single machine per pool at a time by default, so the total time the migration takes increases with the size of the cluster.

|===

:!sdn:
:!previous-sdn:
:!type:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/ovn_kubernetes_network_provider/migrate-from-openshift-sdn.adoc
// * networking/openshift_sdn/rollback-to-ovn-kubernetes.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-ovn-kubernetes-migration_{context}"]
= Migrating to the OVN-Kubernetes network plugin

As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes.
During the migration, you must reboot every node in your cluster.

[IMPORTANT]
====
While performing the migration, your cluster is unavailable and workloads might be interrupted.
Perform the migration only when an interruption in service is acceptable.
====

.Prerequisites

* A cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.
* Install the OpenShift CLI (`oc`).
* Access to the cluster as a user with the `cluster-admin` role.
* A recent backup of the etcd database is available.
* A reboot can be triggered manually for each node.
* The cluster is in a known good state, without any errors.
* On all cloud platforms after updating software, a security group rule must be in place to allow UDP packets on port `6081` for all nodes.

.Procedure

. To backup the configuration for the cluster network, enter the following command:
+
[source,terminal]
----
$ oc get Network.config.openshift.io cluster -o yaml > cluster-openshift-sdn.yaml
----

. To prepare all the nodes for the migration, set the `migration` field on the Cluster Network Operator configuration object by entering the following command:
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'
----
+
[NOTE]
====
This step does not deploy OVN-Kubernetes immediately. Instead, specifying the `migration` field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster in preparation for the OVN-Kubernetes deployment.
====

. Optional: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:
+
--
* Egress IPs
* Egress firewall
* Multicast
--
+
To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OVNKubernetes",
        "features": {
          "egressIP": <bool>,
          "egressFirewall": <bool>,
          "multicast": <bool>
        }
      }
    }
  }'
----
+
where:
+
--
`bool`: Specifies whether to enable migration of the feature. The default is `true`.
--

. Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:
+
--
* Maximum transmission unit (MTU)
* Geneve (Generic Network Virtualization Encapsulation) overlay network port
* OVN-Kubernetes IPv4 internal subnet
* OVN-Kubernetes IPv6 internal subnet
--
+
To customize either of the previously noted settings, enter and customize the following command. If you do not need to change the default value, omit the key from the patch.
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":<mtu>,
          "genevePort":<port>,
          "v4InternalSubnet":"<ipv4_subnet>",
          "v6InternalSubnet":"<ipv6_subnet>"
    }}}}'
----
+
where:
+
--
`mtu`::
The MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to `100` less than the smallest node MTU value.
`port`::
The UDP port for the Geneve overlay network. If a value is not specified, the default is `6081`. The port cannot be the same as the VXLAN port that is used by OpenShift SDN. The default value for the VXLAN port is `4789`.
`ipv4_subnet`::
An IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your {product-title} installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is `100.64.0.0/16`.
`ipv6_subnet`::
An IPv6 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your {product-title} installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is `fd98::/48`.
--
+
.Example patch command to update `mtu` field
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'
----

. As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:
+
[source,terminal]
----
$ oc get mcp
----
+
A successfully updated node has the following status: `UPDATED=true`, `UPDATING=false`, `DEGRADED=false`.
+
[NOTE]
====
By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.
====

. Confirm the status of the new machine configuration on the hosts:

.. To list the machine configuration state and the name of the applied machine configuration, enter the following command:
+
[source,terminal]
----
$ oc describe node | egrep "hostname|machineconfig"
----
+
.Example output
[source,terminal]
----
kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done
----
+
Verify that the following statements are true:
+
--
 * The value of `machineconfiguration.openshift.io/state` field is `Done`.
 * The value of the `machineconfiguration.openshift.io/currentConfig` field is equal to the value of the `machineconfiguration.openshift.io/desiredConfig` field.
--

.. To confirm that the machine config is correct, enter the following command:
+
[source,terminal]
----
$ oc get machineconfig <config_name> -o yaml | grep ExecStart
----
+
where `<config_name>` is the name of the machine config from the `machineconfiguration.openshift.io/currentConfig` field.
+
The machine config must include the following update to the systemd configuration:
+
[source,plain]
----
ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes
----

.. If a node is stuck in the `NotReady` state, investigate the machine config daemon pod logs and resolve any errors.

... To list the pods, enter the following command:
+
[source,terminal]
----
$ oc get pod -n openshift-machine-config-operator
----
+
.Example output
[source,terminal]
----
NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h
----
+
The names for the config daemon pods are in the following format: `machine-config-daemon-<seq>`. The `<seq>` value is a random five character alphanumeric sequence.

... Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:
+
[source,terminal]
----
$ oc logs <pod> -n openshift-machine-config-operator
----
+
where `pod` is the name of a machine config daemon pod.

... Resolve any errors in the logs shown by the output from the previous command.

. To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:

** To specify the network provider without changing the cluster network IP address block, enter the following command:
+
[source,terminal]
----
$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'
----

** To specify a different cluster network IP address block, enter the following command:
+
[source,terminal]
----
$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "<cidr>",
          "hostPrefix": <prefix>
        }
      ],
      "networkType": "OVNKubernetes"
    }
  }'
----
+
where `cidr` is a CIDR block and `prefix` is the slice of the CIDR block apportioned to each node in your cluster. You cannot use any CIDR block that overlaps with the `100.64.0.0/16` CIDR block because the OVN-Kubernetes network provider uses this block internally.
+
[IMPORTANT]
====
You cannot change the service network address block during the migration.
====

. Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:
+
[source,terminal]
----
$ oc -n openshift-multus rollout status daemonset/multus
----
+
The name of the Multus pods is in the form of `multus-<xxxxx>` where `<xxxxx>` is a random sequence of letters. It might take several moments for the pods to restart.
+
.Example output
[source,text]
----
Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out
----

. To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:

** With the `oc rsh` command, you can use a bash script similar to the following:
+
[source,bash]
----
#!/bin/bash
readarray -t POD_NODES <<< "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE <<< "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" && sleep 3
    done
done
----

** With the `ssh` command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.
+
[source,bash]
----
#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done
----

. Confirm that the migration succeeded:

.. To confirm that the network plugin is OVN-Kubernetes, enter the following command.  The value of `status.networkType` must be `OVNKubernetes`.
+
[source,terminal]
----
$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'
----

.. To confirm that the cluster nodes are in the `Ready` state, enter the following command:
+
[source,terminal]
----
$ oc get nodes
----

.. To confirm that your pods are not in an error state, enter the following command:
+
[source,terminal]
----
$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'
----
+
If pods on a node are in an error state, reboot that node.

.. To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:
+
[source,terminal]
----
$ oc get co
----
+
The status of every cluster Operator must be the following: `AVAILABLE="True"`, `PROGRESSING="False"`, `DEGRADED="False"`. If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.

. Complete the following steps only if the migration succeeds and your cluster is in a good state:

.. To remove the migration configuration from the CNO configuration object, enter the following command:
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'
----

.. To remove custom configuration for the OpenShift SDN network provider, enter the following command:
+
[source,terminal]
----
$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "openshiftSDNConfig": null } } }'
----

.. To remove the OpenShift SDN network provider namespace, enter the following command:
+
[source,terminal]
----
$ oc delete namespace openshift-sdn
----

:leveloffset!:

[role="_additional-resources"]
[id="migrate-from-openshift-sdn-additional-resources"]
== Additional resources

* xref:../../networking/cluster-network-operator.adoc#nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator[Configuration parameters for the OVN-Kubernetes network plugin]
* xref:../../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.adoc#backup-etcd[Backing up etcd]
* xref:../../networking/network_policy/about-network-policy.adoc#about-network-policy[About network policy]
* OVN-Kubernetes capabilities
- xref:../../networking/ovn_kubernetes_network_provider/configuring-egress-ips-ovn.adoc#configuring-egress-ips-ovn[Configuring an egress IP address]
- xref:../../networking/ovn_kubernetes_network_provider/configuring-egress-firewall-ovn.adoc#configuring-egress-firewall-ovn[Configuring an egress firewall for a project]
- xref:../../networking/ovn_kubernetes_network_provider/enabling-multicast.adoc#nw-ovn-kubernetes-enabling-multicast[Enabling multicast for a project]
* OpenShift SDN capabilities
- xref:../../networking/openshift_sdn/assigning-egress-ips.adoc#assigning-egress-ips[Configuring egress IPs for a project]
- xref:../../networking/openshift_sdn/configuring-egress-firewall.adoc#configuring-egress-firewall[Configuring an egress firewall for a project]
- xref:../../networking/openshift_sdn/enabling-multicast.adoc#enabling-multicast[Enabling multicast for a project]
* xref:../../rest_api/operator_apis/network-operator-openshift-io-v1.adoc#network-operator-openshift-io-v1[Network [operator.openshift.io/v1]]

//# includes=_attributes/common-attributes,modules/nw-ovn-kubernetes-migration-about,modules/nw-network-plugin-migration-process,modules/nw-ovn-kubernetes-migration

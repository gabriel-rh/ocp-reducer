:_mod-docs-content-type: ASSEMBLY
// Assembly filename:route-configuration.adoc
// Explains route configuration.
[id="route-configuration"]
= Route configuration
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// common attributes
:product-short-name: OpenShift Dedicated
:toc:
:toc-title:
:experimental:
:imagesdir: images
:OCP: OpenShift Container Platform
:ocp-version: 4.14
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:AWS: Amazon Web Services (AWS)
:GCP: Google Cloud Platform (GCP)
:product-registry: OpenShift image registry
:kebab: image:kebab.png[title="Options menu"]
:rhq-short: Red Hat Quay
:SMProductName: Red Hat OpenShift Service Mesh
:pipelines-title: Red Hat OpenShift Pipelines
:logging-sd: Red Hat OpenShift Logging
:ServerlessProductName: OpenShift Serverless
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:rhoda: Red Hat OpenShift Database Access
:rhoda-short: RHODA
:rhods: Red Hat OpenShift Data Science
:osd: OpenShift Dedicated
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:hcp: hosted control planes
:hcp-title: ROSA with HCP
:hcp-title-first: {product-title} (ROSA) with {hcp} (HCP)
//ROSA CLI variables
:word: Testing this variable let's go www.google.com
:context: route-configuration

toc::[]

//Creating an insecure route
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-creating-a-route_{context}"]
= Creating an HTTP-based route

A route allows you to host your application at a public URL. It can either be secure or unsecured, depending on the network security configuration of your application. An HTTP-based route is an unsecured route that uses the basic HTTP routing protocol and exposes a service on an unsecured application port.

The following procedure describes how to create a simple HTTP-based route to a web application, using the `hello-openshift` application as an example.
//link:https://github.com/openshift/origin/tree/master/examples/hello-openshift[hello-openshift]

.Prerequisites


* You installed the OpenShift CLI (`oc`).
* You are logged in as an administrator.
* You have a web application that exposes a port and a TCP endpoint listening for traffic on the port.

.Procedure

. Create a project called `hello-openshift` by running the following command:
+
[source,terminal]
----
$ oc new-project hello-openshift
----

. Create a pod in the project by running the following command:
+
[source,terminal]
----
$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json
----

. Create a service called `hello-openshift` by running the following command:
+
[source,terminal]
----
$ oc expose pod/hello-openshift
----

. Create an unsecured route to the `hello-openshift` application by running the following command:
+
[source,terminal]
----
$ oc expose svc hello-openshift
----

.Verification

* To verify that the `route` resource that you created, run the following command:
+
[source,terminal]
----
$ oc get routes -o yaml <name of resource> <1>
----
<1> In this example, the route is named `hello-openshift`.

.Sample YAML definition of the created unsecured route:
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: hello-openshift
spec:
  host: hello-openshift-hello-openshift.<Ingress_Domain> <1>
  port:
    targetPort: 8080 <2>
  to:
    kind: Service
    name: hello-openshift
----
<1> `<Ingress_Domain>` is the default ingress domain name. The `ingresses.config/cluster` object is created during the installation and cannot be changed. If you want to specify a different domain, you can specify an alternative cluster domain using the `appsDomain` option.
<2> `targetPort` is the target port on pods that is selected by the service that this route points to.

+
[NOTE]
====
To display your default ingress domain, run the following command:
[source,terminal]
----
$ oc get ingresses.config/cluster -o jsonpath={.spec.domain}
----
====

:leveloffset!:

// Creating a route for router sharding
:leveloffset: +1

// Module included in the following assemblies:
//
// * configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-ingress-controller.adoc
// * networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-ingress-sharding-route-configuration_{context}"]
= Creating a route for Ingress Controller sharding

A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.

The following procedure describes how to create a route for Ingress Controller sharding, using the `hello-openshift` application as an example.

Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You are logged in as a project administrator.
* You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.
* You have configured the Ingress Controller for sharding.

.Procedure

. Create a project called `hello-openshift` by running the following command:
+
[source,terminal]
----
$ oc new-project hello-openshift
----

. Create a pod in the project by running the following command:
+
[source,terminal]
----
$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json
----

. Create a service called `hello-openshift` by running the following command:
+
[source,terminal]
----
$ oc expose pod/hello-openshift
----

. Create a route definition called `hello-openshift-route.yaml`:
+
.YAML definition of the created route for sharding:
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <1>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <2>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
----
<1> Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value `type: sharded`.
<2> The route will be exposed using the value of the `subdomain` field. When you specify the `subdomain` field, you must leave the hostname unset. If you specify both the `host` and `subdomain` fields, then the route will use the value of the `host` field, and ignore the `subdomain` field.

. Use `hello-openshift-route.yaml` to create a route to the `hello-openshift` application by running the following command:
+
[source,terminal]
----
$ oc -n hello-openshift create -f hello-openshift-route.yaml
----

.Verification
* Get the status of the route with the following command:
+
[source,terminal]
----
$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml
----
+
The resulting `Route` resource should look similar to the following:
+
.Example output
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.<apps-sharded.basedomain.example.net> <1>
    routerCanonicalHostname: router-sharded.<apps-sharded.basedomain.example.net> <2>
    routerName: sharded <3>
----
<1> The hostname the Ingress Controller, or router, uses to expose the route. The value of the `host` field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is `<apps-sharded.basedomain.example.net>`.
<2> The hostname of the Ingress Controller.
<3> The name of the Ingress Controller. In this example, the Ingress Controller has the name `sharded`.

:leveloffset!:

//Creating route timeouts
:leveloffset: +1

// Module filename: nw-configuring-route-timeouts.adoc
// Module included in the following assemblies:
// * networking/configuring-routing.adoc
// * networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-aws.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-configuring-route-timeouts_{context}"]
= Configuring route timeouts

You can configure the default timeouts for an existing route when you
have services in need of a low timeout, which is required for Service Level
Availability (SLA) purposes, or a high timeout, for cases with a slow
back end.

.Prerequisites
* You need a deployed Ingress Controller on a running cluster.

.Procedure
. Using the `oc annotate` command, add the timeout to the route:
+
[source,terminal]
----
$ oc annotate route <route_name> \
    --overwrite haproxy.router.openshift.io/timeout=<timeout><time_unit> <1>
----
<1> Supported time units are microseconds (us), milliseconds (ms), seconds (s),
minutes (m), hours (h), or days (d).
+
The following example sets  a timeout of two seconds on a route named `myroute`:
+
[source,terminal]
----
$ oc annotate route myroute --overwrite haproxy.router.openshift.io/timeout=2s
----

:leveloffset!:

//HTTP Strict Transport Security
:leveloffset: +1

// Module filename: nw-enabling-hsts.adoc
// Module included in the following assemblies:
// * networking/configuring-routing.adoc

[id="nw-enabling-hsts_{context}"]
= HTTP Strict Transport Security

HTTP Strict Transport Security (HSTS) policy is a security enhancement, which signals to the browser client that only HTTPS traffic is allowed on the route host. HSTS also optimizes web traffic by signaling HTTPS transport is required, without using HTTP redirects. HSTS is useful for speeding up interactions with websites.

When HSTS policy is enforced, HSTS adds a Strict Transport Security header to HTTP and HTTPS responses from the site. You can use the `insecureEdgeTerminationPolicy` value in a route to redirect HTTP to HTTPS. When HSTS is enforced, the client changes all requests from the HTTP URL to HTTPS before the request is sent, eliminating the need for a redirect.

Cluster administrators can configure HSTS to do the following:

* Enable HSTS per-route
* Disable HSTS per-route
* Enforce HSTS per-domain, for a set of domains, or use namespace labels in combination with domains

[IMPORTANT]
====
HSTS works only with secure routes, either edge-terminated or re-encrypt. The configuration is ineffective on HTTP or passthrough routes.
====

:leveloffset!:

//Enabling HTTP strict transport security per-route
:leveloffset: +2

// Module included in the following assemblies:
// * networking/configuring-routing.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-enabling-hsts-per-route_{context}"]
= Enabling HTTP Strict Transport Security per-route

HTTP strict transport security (HSTS) is implemented in the HAProxy template and applied to edge and re-encrypt routes that have the `haproxy.router.openshift.io/hsts_header` annotation.

.Prerequisites

* You are logged in to the cluster with a user with administrator privileges for the project.
* You installed the `oc` CLI.

.Procedure

* To enable HSTS on a route, add the `haproxy.router.openshift.io/hsts_header` value to the edge-terminated or re-encrypt route. You can use the `oc annotate` tool to do this by running the following command:
+
[source,terminal]
----
$ oc annotate route <route_name> -n <namespace> --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000;\ <1>
includeSubDomains;preload"
----
<1> In this example, the maximum age is set to `31536000` ms, which is approximately eight and a half hours.
+
[NOTE]
====
In this example, the equal sign (`=`) is in quotes. This is required to properly execute the annotate command.
====
+
.Example route configured with an annotation
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/hsts_header: max-age=31536000;includeSubDomains;preload <1> <2> <3>
...
spec:
  host: def.abc.com
  tls:
    termination: "reencrypt"
    ...
  wildcardPolicy: "Subdomain"
----
<1> Required. `max-age` measures the length of time, in seconds, that the HSTS policy is in effect. If set to `0`, it negates the policy.
<2> Optional. When included, `includeSubDomains` tells the client
that all subdomains of the host must have the same HSTS policy as the host.
<3> Optional. When `max-age` is greater than 0, you can add `preload` in  `haproxy.router.openshift.io/hsts_header` to allow external services to include this site in their HSTS preload lists. For example, sites such as Google can construct a list of sites that have `preload` set. Browsers can then use these lists to determine which sites they can communicate with over HTTPS, even before they have interacted with the site. Without `preload` set, browsers must have interacted with the site over HTTPS, at least once, to get the header.

:leveloffset!:

//Disabling HTTP strict transport security per-route
:leveloffset: +2

// Module included in the following assemblies:
// * networking/configuring-routing.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-disabling-hsts_{context}"]
= Disabling HTTP Strict Transport Security per-route

To disable HTTP strict transport security (HSTS) per-route, you can set the `max-age` value in the route annotation to `0`.

.Prerequisites

* You are logged in to the cluster with a user with administrator privileges for the project.
* You installed the `oc` CLI.

.Procedure

* To disable HSTS, set the `max-age` value in the route annotation to `0`, by entering the following command:
+
[source,terminal]
----
$ oc annotate route <route_name> -n <namespace> --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=0"
----
+
[TIP]
====
You can alternatively apply the following YAML to create the config map:

.Example of disabling HSTS per-route
[source,yaml]
----
metadata:
  annotations:
    haproxy.router.openshift.io/hsts_header: max-age=0
----
====

* To disable HSTS for every route in a namespace, enter the following command:
+
[source,terminal]
----
$ oc annotate route --all -n <namespace> --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=0"
----

.Verification

. To query the annotation for all routes, enter the following command:
+
[source,terminal]
----
$ oc get route  --all-namespaces -o go-template='{{range .items}}{{if .metadata.annotations}}{{$a := index .metadata.annotations "haproxy.router.openshift.io/hsts_header"}}{{$n := .metadata.name}}{{with $a}}Name: {{$n}} HSTS: {{$a}}{{"\n"}}{{else}}{{""}}{{end}}{{end}}{{end}}'
----
+
.Example output
[source,terminal]
----
Name: routename HSTS: max-age=0
----

:leveloffset!:

//Enforcing HTTP strict transport security per-domain
:leveloffset: +2

// Module included in the following assemblies:
// * networking/configuring-routing.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-enforcing-hsts-per-domain_{context}"]
= Enforcing HTTP Strict Transport Security per-domain

To enforce HTTP Strict Transport Security (HSTS) per-domain for secure routes, add a `requiredHSTSPolicies` record to the Ingress spec to capture the configuration of the HSTS policy.

If you configure a `requiredHSTSPolicy` to enforce HSTS, then any newly created route must be configured with a compliant HSTS policy annotation.

[NOTE]
====
To handle upgraded clusters with non-compliant HSTS routes, you can update the manifests at the source and apply the updates.
====

[NOTE]
====
You cannot use `oc expose route` or `oc create route` commands to add a route in a domain that enforces HSTS, because the API for these commands does not accept annotations.
====

[IMPORTANT]
====
HSTS cannot be applied to insecure, or non-TLS routes, even if HSTS is requested for all routes globally.
====

.Prerequisites

* You are logged in to the cluster with a user with administrator privileges for the project.
* You installed the `oc` CLI.

.Procedure

. Edit the Ingress config file:
+
[source,terminal]
----
$ oc edit ingresses.config.openshift.io/cluster
----
+
.Example HSTS policy
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: 'hello-openshift-default.apps.username.devcluster.openshift.com'
  requiredHSTSPolicies: <1>
  - domainPatterns: <2>
    - '*hello-openshift-default.apps.username.devcluster.openshift.com'
    - '*hello-openshift-default2.apps.username.devcluster.openshift.com'
    namespaceSelector: <3>
      matchLabels:
        myPolicy: strict
    maxAge: <4>
      smallestMaxAge: 1
      largestMaxAge: 31536000
    preloadPolicy: RequirePreload <5>
    includeSubDomainsPolicy: RequireIncludeSubDomains <6>
  - domainPatterns: <2>
    - 'abc.example.com'
    - '*xyz.example.com'
    namespaceSelector:
      matchLabels: {}
    maxAge: {}
    preloadPolicy: NoOpinion
    includeSubDomainsPolicy: RequireNoIncludeSubDomains
----
<1> Required. `requiredHSTSPolicies` are validated in order, and the first matching `domainPatterns` applies.
<2> Required. You must specify at least one `domainPatterns` hostname. Any number of domains can be listed. You can include multiple sections of enforcing options for different `domainPatterns`.
<3> Optional. If you include `namespaceSelector`, it must match the labels of the project where the routes reside, to enforce the set HSTS policy on the routes. Routes that only match the `namespaceSelector` and not the `domainPatterns` are not validated.
<4> Required. `max-age` measures the length of time, in seconds, that the HSTS policy is in effect. This policy setting allows for a smallest and largest `max-age` to be enforced.

- The `largestMaxAge` value must be between `0` and `2147483647`. It can be left unspecified, which means no upper limit is enforced.
- The `smallestMaxAge` value must be between `0` and `2147483647`. Enter `0` to disable HSTS for troubleshooting, otherwise enter `1` if you never want HSTS to be disabled. It can be left unspecified, which means no lower limit is enforced.
<5> Optional. Including `preload` in `haproxy.router.openshift.io/hsts_header` allows external services to include this site in their HSTS preload lists. Browsers can then use these lists to determine which sites they can communicate with over HTTPS, before they have interacted with the site. Without `preload` set, browsers need to interact at least once with the site to get the header. `preload` can be set with one of the following:

- `RequirePreload`: `preload` is required by the `RequiredHSTSPolicy`.
- `RequireNoPreload`: `preload` is forbidden by the `RequiredHSTSPolicy`.
- `NoOpinion`: `preload` does not matter to the `RequiredHSTSPolicy`.
<6> Optional. `includeSubDomainsPolicy` can be set with one of the following:

- `RequireIncludeSubDomains`: `includeSubDomains` is required by the `RequiredHSTSPolicy`.
- `RequireNoIncludeSubDomains`: `includeSubDomains` is forbidden by the `RequiredHSTSPolicy`.
- `NoOpinion`: `includeSubDomains` does not matter to the `RequiredHSTSPolicy`.
+
. You can apply HSTS to all routes in the cluster or in a particular namespace by entering the `oc annotate command`.
+
* To apply HSTS to all routes in the cluster, enter the `oc annotate command`. For example:
+
[source,terminal]
----
$ oc annotate route --all --all-namespaces --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000"
----
+
* To apply HSTS to all routes in a particular namespace, enter the `oc annotate command`. For example:
+
[source,terminal]
----
$ oc annotate route --all -n my-namespace --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000"
----

.Verification

You can review the HSTS policy you configured. For example:

* To review the `maxAge` set for required HSTS policies, enter the following command:
+
[source,terminal]
----
$ oc get clusteroperator/ingress -n openshift-ingress-operator -o jsonpath='{range .spec.requiredHSTSPolicies[*]}{.spec.requiredHSTSPolicies.maxAgePolicy.largestMaxAge}{"\n"}{end}'
----
+
* To review the HSTS annotations on all routes, enter the following command:
+
[source,terminal]
----
$ oc get route  --all-namespaces -o go-template='{{range .items}}{{if .metadata.annotations}}{{$a := index .metadata.annotations "haproxy.router.openshift.io/hsts_header"}}{{$n := .metadata.name}}{{with $a}}Name: {{$n}} HSTS: {{$a}}{{"\n"}}{{else}}{{""}}{{end}}{{end}}{{end}}'
----
+
.Example output
[source,terminal]
----
Name: <_routename_> HSTS: max-age=31536000;preload;includeSubDomains
----

:leveloffset!:

//Troubleshooting Throughput Issues
:leveloffset: +1

// Module filename: nw-throughput-troubleshoot.adoc
// Module included in the following assemblies:
// * networking/routes/route-configuration.adoc

:_mod-docs-content-type: CONCEPT
[id="nw-throughput-troubleshoot_{context}"]
= Throughput issue troubleshooting methods

Sometimes applications deployed by using {product-title} can cause network throughput issues, such as unusually high latency between specific services.

If pod logs do not reveal any cause of the problem, use the following methods to analyze performance issues:

* Use a packet analyzer, such as `ping` or `tcpdump` to analyze traffic between a pod and its node.
+
For example, link:https://access.redhat.com/solutions/4569211[run the `tcpdump` tool on each pod] while reproducing the behavior that led to the issue. Review the captures on both sides to compare send and receive timestamps to analyze the latency of traffic to and from a pod. Latency can occur in {product-title} if a node interface is overloaded with traffic from other pods, storage devices, or the data plane.
+
[source,terminal]
----
$ tcpdump -s 0 -i any -w /tmp/dump.pcap host <podip 1> && host <podip 2> <1>
----
+
<1> `podip` is the IP address for the pod. Run the `oc get pod <pod_name> -o wide` command to get the IP address of a pod.
+
The `tcpdump` command generates a file at `/tmp/dump.pcap` containing all traffic between these two pods. You can run the analyzer shortly before the issue is reproduced and stop the analyzer shortly after the issue is finished reproducing to minimize the size of the file. You can also link:https://access.redhat.com/solutions/5074041[run a packet analyzer between the nodes] (eliminating the SDN from the equation) with:
+
[source,terminal]
----
$ tcpdump -s 0 -i any -w /tmp/dump.pcap port 4789
----

* Use a bandwidth measuring tool, such as link:https://access.redhat.com/solutions/6129701[`iperf`], to measure streaming throughput and UDP throughput. Locate any bottlenecks by running the tool from the pods first, and then running it from the nodes.


* In some cases, the cluster may mark the node with the router pod as unhealthy due to latency issues. Use worker latency profiles to adjust the frequency that the cluster waits for a status update from the node before taking action.

* If your cluster has designated lower-latency and higher-latency nodes, configure the `spec.nodePlacement` field in the Ingress Controller to control the placement of the router pod.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../nodes/edge/nodes-edge-remote-workers.adoc#nodes-edge-remote-workers-latency[Latency spikes or temporary reduction in throughput to remote workers]


* xref:../../networking/ingress-operator.adoc#nw-ingress-controller-configuration-parameters_configuring-ingress[Ingress Controller configuration
parameters]

//Using cookies to keep route statefulness
:leveloffset: +1

// Module filename: nw-using-cookies-keep-route-statefulness.adoc
// Use module with the following module:
// nw-annotating-a-route-with-a-cookie-name.adoc
//
// Module included in the following assemblies:
//
// * networking/configuring-routing.adoc
[id="nw-using-cookies-keep-route-statefulness_{context}"]
= Using cookies to keep route statefulness

{product-title} provides sticky sessions, which enables stateful application
traffic by ensuring all traffic hits the same endpoint. However, if the endpoint
pod terminates, whether through restart, scaling, or a change in configuration,
this statefulness can disappear.

{product-title} can use cookies to configure session persistence. The Ingress
controller selects an endpoint to handle any user requests, and creates a cookie
for the session. The cookie is passed back in the response to the request and
the user sends the cookie back with the next request in the session. The cookie
tells the Ingress Controller which endpoint is handling the session, ensuring
that client requests use the cookie so that they are routed to the same pod.

[NOTE]
====
Cookies cannot be set on passthrough routes, because the HTTP traffic cannot be seen. Instead, a number is calculated based on the source IP address, which determines the backend.

If backends change, the traffic can be directed to the wrong server, making it less sticky. If you are using a load balancer, which hides source IP, the same number is set for all connections and traffic is sent to the same pod.
====

:leveloffset!:

:leveloffset: +2

// Module filename: nw-annotating-a-route-with-a-cookie-name.adoc
// Use module with the following module:
// nw-using-cookies-keep-route-statefulness.adoc
//
// Module included in the following assemblies:
//
// * networking/configuring-routing.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-annotating-a-route-with-a-cookie-name_{context}"]
= Annotating a route with a cookie

You can set a cookie name to overwrite the default, auto-generated one for the route. This allows the application receiving route traffic to know the cookie name. By deleting the cookie it can force the next request to re-choose an endpoint. So, if a server was overloaded it tries to remove the requests from the client and redistribute them.

.Procedure

. Annotate the route with the specified cookie name:
+
[source,terminal]
----
$ oc annotate route <route_name> router.openshift.io/cookie_name="<cookie_name>"
----
+
--
where:

`<route_name>`:: Specifies the name of the route.
`<cookie_name>`:: Specifies the name for the cookie.
--
+
For example, to annotate the route `my_route` with the cookie name `my_cookie`:
+
[source,terminal]
----
$ oc annotate route my_route router.openshift.io/cookie_name="my_cookie"
----

. Capture the route hostname in a variable:
+
[source,terminal]
----
$ ROUTE_NAME=$(oc get route <route_name> -o jsonpath='{.spec.host}')
----
+
--
where:

`<route_name>`:: Specifies the name of the route.
--

. Save the cookie, and then access the route:
+
[source,terminal]
----
$ curl $ROUTE_NAME -k -c /tmp/cookie_jar
----
+
Use the cookie saved by the previous command when connecting to the route:
+
[source,terminal]
----
$ curl $ROUTE_NAME -k -b /tmp/cookie_jar
----

:leveloffset!:

:leveloffset: +1

// Module filename: nw-path-based-routes.adoc
// Module included in the following assemblies:
// * networking/routes/route-configuration.adoc

[id="nw-path-based-routes_{context}"]
= Path-based routes

Path-based routes specify a path component that can be compared against a URL, which requires that the traffic for the route be HTTP based. Thus, multiple routes can be served using the same hostname, each with a different path. Routers should match routes based on the most specific path to the least. However, this depends on the router implementation.

The following table shows example routes and their accessibility:

.Route availability
[cols="3*", options="header"]
|===
|Route | When Compared to | Accessible
.2+|_www.example.com/test_ |_www.example.com/test_|Yes
|_www.example.com_|No
.2+|_www.example.com/test_ and _www.example.com_ | _www.example.com/test_|Yes
|_www.example.com_|Yes
.2+|_www.example.com_|_www.example.com/text_|Yes (Matched by the host, not the route)
|_www.example.com_|Yes
|===

.An unsecured route with a path

[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test" <1>
  to:
    kind: Service
    name: service-name
----
<1> The path is the only added attribute for a path-based route.

[NOTE]
====
Path-based routing is not available when using passthrough TLS, as the router does not terminate TLS in that case and cannot read the contents of the request.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/ingress-operator.adoc
// * networking/route-configuration.adoc

:_mod-docs-content-type: CONCEPT
[id="nw-http-header-configuration_{context}"]
= HTTP header configuration

{product-title} provides different methods for working with HTTP headers. When setting or deleting headers, you can use specific fields in the Ingress Controller or an individual route to modify request and response headers. You can also set certain headers by using route annotations. The various ways of configuring headers can present challenges when working together.

[NOTE]
====
You can only set or delete headers within an `IngressController` or `Route` CR, you cannot append them. If an HTTP header is set with a value, that value must be complete and not require appending in the future. In situations where it makes sense to append a header, such as the X-Forwarded-For header, use the `spec.httpHeaders.forwardedHeaderPolicy` field, instead of `spec.httpHeaders.actions`.
====

[id="nw-http-header-configuration-order_{context}"]
== Order of precedence

When the same HTTP header is modified both in the Ingress Controller and in a route, HAProxy prioritizes the actions in certain ways depending on whether it is a request or response header.

* For HTTP response headers, actions specified in the Ingress Controller are executed after the actions specified in a route. This means that the actions specified in the Ingress Controller take precedence.

* For HTTP request headers, actions specified in a route are executed after the actions specified in the Ingress Controller. This means that the actions specified in the route take precedence.

For example, a cluster administrator sets the X-Frame-Options response header with the value `DENY` in the Ingress Controller using the following configuration:

.Example `IngressController` spec
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
# ...
spec:
  httpHeaders:
    actions:
      response:
      - name: X-Frame-Options
        action:
          type: Set
          set:
            value: DENY
----

A route owner sets the same response header that the cluster administrator set in the Ingress Controller, but with the value `SAMEORIGIN` using the following configuration:

.Example `Route` spec
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
# ...
spec:
  httpHeaders:
    actions:
      response:
      - name: X-Frame-Options
        action:
          type: Set
          set:
            value: SAMEORIGIN
----

When both the `IngressController` spec and `Route` spec are configuring the X-Frame-Options header, then the value set for this header at the global level in the Ingress Controller will take precedence, even if a specific route allows frames.

This prioritzation occurs because the `haproxy.config` file uses the following logic, where the Ingress Controller is considered the front end and individual routes are considered the back end. The header value `DENY` applied to the front end configurations overrides the same header with the value `SAMEORIGIN` that is set in the back end:

[source,text]
----
frontend public
  http-response set-header X-Frame-Options 'DENY'

frontend fe_sni
  http-response set-header X-Frame-Options 'DENY'

frontend fe_no_sni
  http-response set-header X-Frame-Options 'DENY'

backend be_secure:openshift-monitoring:alertmanager-main
  http-response set-header X-Frame-Options 'SAMEORIGIN'
----

Additionally, any actions defined in either the Ingress Controller or a route override values set using route annotations.

[id="nw-http-header-configuration-special-cases_{context}"]
== Special case headers

The following headers are either prevented entirely from being set or deleted, or allowed under specific circumstances:

.Special case header configuration options
[cols="5*a",options="header"]
|===
|Header name |Configurable using `IngressController` spec |Configurable using `Route` spec |Reason for disallowment |Configurable using another method

|`proxy`
|No
|No
|The `proxy` HTTP request header can be used to exploit vulnerable CGI applications by injecting the header value into the `HTTP_PROXY` environment variable. The `proxy` HTTP request header is also non-standard and prone to error during configuration.
|No

|`host`
|No
|Yes
|When the `host` HTTP request header is set using the `IngressController` CR, HAProxy can fail when looking up the correct route.
|No

|`strict-transport-security`
|No
|No
|The `strict-transport-security` HTTP response header is already handled using route annotations and does not need a separate implementation.
|Yes: the `haproxy.router.openshift.io/hsts_header` route annotation

|`cookie` and `set-cookie`
|No
|No
|The cookies that HAProxy sets are used for session tracking to map client connections to particular back-end servers. Allowing these headers to be set could interfere with HAProxy's session affinity and restrict HAProxy's ownership of a cookie.
|Yes:

* the `haproxy.router.openshift.io/disable_cookie` route annotation
* the `haproxy.router.openshift.io/cookie_name` route annotation

|===

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-route-set-or-delete-http-headers_{context}"]
= Setting or deleting HTTP request and response headers in a route

You can set or delete certain HTTP request and response headers for compliance purposes or other reasons. You can set or delete these headers either for all routes served by an Ingress Controller or for specific routes.

For example, you might want to enable a web application to serve content in alternate locations for specific routes if that content is written in multiple languages, even if there is a default global location specified by the Ingress Controller serving the routes.

The following procedure creates a route that sets the Content-Location HTTP request header so that the URL associated with the application, `\https://app.example.com`, directs to the location `\https://app.example.com/lang/en-us`. Directing application traffic to this location means that anyone using that specific route is accessing web content written in American English.

.Prerequisites
* You have installed the OpenShift CLI (`oc`).
* You are logged into an {product-title} cluster as a project administrator.
* You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.

.Procedure
. Create a route definition and save it in a file called `app-example-route.yaml`:
+
.YAML definition of the created route with HTTP header directives
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
# ...
spec:
  host: app.example.com
  tls:
    termination: edge
  to:
    kind: Service
    name: app-example
  httpHeaders:
    actions: <1>
      response: <2>
      - name: Content-Location <3>
        action:
          type: Set <4>
          set:
            value: /lang/en-us <5>
----
<1> The list of actions you want to perform on the HTTP headers.
<2> The type of header you want to change. In this case, a response header.
<3> The name of the header you want to change. For a list of available headers you can set or delete, see _HTTP header configuration_.
<4> The type of action being taken on the header. This field can have the value `Set` or `Delete`.
<5> When setting HTTP headers, you must provide a `value`. The value can be a string from a list of available directives for that header, for example `DENY`, or it can be a dynamic value that will be interpreted using HAProxy's dynamic value syntax. In this case, the value is set to the relative location of the content.

. Create a route to your existing web application using the newly created route definition:
+
[source,terminal]
----
$ oc -n app-example create -f app-example-route.yaml
----

For HTTP request headers, the actions specified in the route definitions are executed after any actions performed on HTTP request headers in the Ingress Controller. This means that any values set for those request headers in a route will take precedence over the ones set in the Ingress Controller. For more information on the processing order of HTTP headers, see _HTTP header configuration_.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/routes/route-configuration.adoc

[id="nw-route-specific-annotations_{context}"]
= Route-specific annotations

The Ingress Controller can set the default options for all the routes it exposes. An individual route can override some of these defaults by providing specific configurations in its annotations. Red Hat does not support adding a route annotation to an operator-managed route.

[IMPORTANT]
====
To create a whitelist with multiple source IPs or subnets, use a space-delimited list. Any other delimiter type causes the list to be ignored without a warning or error message.
====

//For all the variables outlined in this section, you can set annotations on the
//*route definition* for the route to alter its configuration.

.Route annotations
[cols="3*", options="header"]
|===
|Variable | Description | Environment variable used as default
|`haproxy.router.openshift.io/balance`| Sets the load-balancing algorithm. Available options are `random`, `source`, `roundrobin`, and `leastconn`.  The default value is `source` for TLS passthrough routes. For all other routes, the default is `random`. |`ROUTER_TCP_BALANCE_SCHEME` for passthrough routes. Otherwise, use `ROUTER_LOAD_BALANCE_ALGORITHM`.
|`haproxy.router.openshift.io/disable_cookies`| Disables the use of cookies to track related connections. If set to `'true'` or `'TRUE'`, the balance algorithm is used to choose which back-end serves connections for each incoming HTTP request. |
|`router.openshift.io/cookie_name`| Specifies an optional cookie to use for
this route. The name must consist of any combination of upper and lower case letters, digits, "_",
and "-". The default is the hashed internal key name for the route. |
|`haproxy.router.openshift.io/pod-concurrent-connections`| Sets the maximum number of connections that are allowed to a backing pod from a router. +
Note: If there are multiple pods, each can have this many connections.  If you have multiple routers, there is no coordination among them, each may connect this many times. If not set, or set to 0, there is no limit. |
|`haproxy.router.openshift.io/rate-limit-connections`| Setting `'true'` or `'TRUE'` enables rate limiting functionality which is implemented through stick-tables on the specific backend per route. +
Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks. |
|`haproxy.router.openshift.io/rate-limit-connections.concurrent-tcp`| Limits the number of concurrent TCP connections made through the same source IP address. It accepts a numeric value. +
Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks. |
|`haproxy.router.openshift.io/rate-limit-connections.rate-http`| Limits the rate at which a client with the same source IP address can make HTTP requests. It accepts a numeric value.  +
Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks. |
|`haproxy.router.openshift.io/rate-limit-connections.rate-tcp`| Limits the rate at which a client with the same source IP address can make TCP connections. It accepts a numeric value.  +
Note: Using this annotation provides basic protection against distributed denial-of-service (DDoS) attacks. |
|`haproxy.router.openshift.io/timeout` | Sets a server-side timeout for the route. (TimeUnits) | `ROUTER_DEFAULT_SERVER_TIMEOUT`
|`haproxy.router.openshift.io/timeout-tunnel` | This timeout applies to a tunnel connection, for example, WebSocket over cleartext, edge, reencrypt, or passthrough routes. With cleartext, edge, or reencrypt route types, this annotation is applied as a timeout tunnel with the existing timeout value. For the passthrough route types, the annotation takes precedence over any existing timeout value set. | `ROUTER_DEFAULT_TUNNEL_TIMEOUT`
|`ingresses.config/cluster ingress.operator.openshift.io/hard-stop-after` | You can set either an IngressController or the ingress config . This annotation redeploys the router and configures the HA proxy to emit the haproxy `hard-stop-after` global option, which defines the maximum time allowed to perform a clean soft-stop. | `ROUTER_HARD_STOP_AFTER`
|`router.openshift.io/haproxy.health.check.interval`| Sets the interval for the back-end health checks. (TimeUnits) | `ROUTER_BACKEND_CHECK_INTERVAL`
|`haproxy.router.openshift.io/ip_whitelist`
| Sets an allowlist for the route. The allowlist is a space-separated list of IP addresses and CIDR ranges for the approved source addresses. Requests from IP addresses that are not in the allowlist are dropped.

The maximum number of IP addresses and CIDR ranges directly visible in the `haproxy.config` file is 61. [^1^] |

|`haproxy.router.openshift.io/hsts_header` | Sets a Strict-Transport-Security header for the edge terminated or re-encrypt route. |
|`haproxy.router.openshift.io/log-send-hostname` | Sets the `hostname` field in the Syslog header. Uses the hostname of the system. `log-send-hostname` is enabled by default if any Ingress API logging method, such as sidecar or Syslog facility, is enabled for the router. |
|`haproxy.router.openshift.io/rewrite-target` | Sets the rewrite path of the request on the backend. |
|`router.openshift.io/cookie-same-site` | Sets a value to restrict cookies. The values are:

`Lax`: cookies are transferred between the visited site and third-party sites.

`Strict`: cookies are restricted to the visited site.

`None`: cookies are restricted to the visited site.

This value is applicable to re-encrypt and edge routes only. For more information, see the link:https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite[SameSite cookies documentation].|

|`haproxy.router.openshift.io/set-forwarded-headers` | Sets the policy for handling the `Forwarded` and `X-Forwarded-For` HTTP headers per route. The values are:

`append`: appends the header, preserving any existing header. This is the default value.

`replace`: sets the header, removing any existing header.

`never`: never sets the header, but preserves any existing header.

`if-none`: sets the header if it is not already set.| `ROUTER_SET_FORWARDED_HEADERS`

|===
[.small]
--
1. If the number of IP addresses and CIDR ranges in an allowlist exceeds 61, they are written into a separate file that is then referenced from `haproxy.config`. This file is stored in the `var/lib/haproxy/router/whitelists` folder.
+
[NOTE]
====
To ensure that the addresses are written to the allowlist, check that the full list of CIDR ranges are listed in the Ingress Controller configuration file. The etcd object size limit restricts how large a route annotation can be. Because of this, it creates a threshold for the maximum number of IP addresses and CIDR ranges that you can include in an allowlist.
====
--

[NOTE]
====
Environment variables cannot be edited.
====

.Router timeout variables

`TimeUnits` are represented by a number followed by the unit: `us` *(microseconds), `ms` (milliseconds, default), `s` (seconds), `m` (minutes), `h` *(hours), `d` (days).

The regular expression is: [1-9][0-9]*(`us`\|`ms`\|`s`\|`m`\|`h`\|`d`).
[cols="2,1,2a", options="header"]
|===
|Variable | Default | Description
| `ROUTER_BACKEND_CHECK_INTERVAL` | `5000ms` | Length of time between subsequent liveness checks on back ends.
| `ROUTER_CLIENT_FIN_TIMEOUT` | `1s` | Controls the TCP FIN timeout period for the client connecting to the route. If the FIN sent to close the connection does not answer within the given time, HAProxy closes the connection. This is harmless if set to a low value and uses fewer resources on the router.
| `ROUTER_DEFAULT_CLIENT_TIMEOUT` | `30s` | Length of time that a client has to acknowledge or send data.
| `ROUTER_DEFAULT_CONNECT_TIMEOUT` | `5s` | The maximum connection time.
| `ROUTER_DEFAULT_SERVER_FIN_TIMEOUT` | `1s` | Controls the TCP FIN timeout from the router to the pod backing the route.
| `ROUTER_DEFAULT_SERVER_TIMEOUT` | `30s` | Length of time that a server has to acknowledge or send data.
| `ROUTER_DEFAULT_TUNNEL_TIMEOUT` | `1h` | Length of time for TCP or WebSocket connections to remain open. This timeout period resets whenever HAProxy reloads.
| `ROUTER_SLOWLORIS_HTTP_KEEPALIVE` | `300s` | Set the maximum time to wait for a new HTTP request to appear. If this is set too low, it can cause problems with browsers and applications not expecting a small `keepalive` value.

Some effective timeout values can be the sum of certain variables, rather than the specific expected timeout. For example, `ROUTER_SLOWLORIS_HTTP_KEEPALIVE` adjusts `timeout http-keep-alive`. It is set to `300s` by default, but HAProxy also waits on `tcp-request inspect-delay`, which is set to `5s`. In this case, the overall timeout would be `300s` plus `5s`.
| `ROUTER_SLOWLORIS_TIMEOUT` | `10s` | Length of time the transmission of an HTTP request can take.
| `RELOAD_INTERVAL` | `5s` | Allows the minimum frequency for the router to reload and accept new changes.
| `ROUTER_METRICS_HAPROXY_TIMEOUT` | `5s` | Timeout for the gathering of HAProxy metrics.

|===

.A route setting custom timeout
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/timeout: 5500ms <1>
...
----
<1> Specifies the new timeout with HAProxy supported units (`us`, `ms`, `s`, `m`, `h`, `d`). If the unit is not provided, `ms` is the default.

[NOTE]
====
Setting a server-side timeout value for passthrough routes too low can cause
WebSocket connections to timeout frequently on that route.
====

.A route that allows only one specific IP address
[source,yaml]
----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10
----

.A route that allows several IP addresses
[source,yaml]
----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10 192.168.1.11 192.168.1.12
----

.A route that allows an IP address CIDR network
[source,yaml]
----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.0/24
----

.A route that allows both IP an address and IP address CIDR networks
[source,yaml]
----
metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 180.5.61.153 192.168.1.0/24 10.0.0.0/8
----

.A route specifying a rewrite target
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/rewrite-target: / <1>
...
----
<1> Sets `/` as rewrite path of the request on the backend.

Setting the `haproxy.router.openshift.io/rewrite-target` annotation on a route specifies that the Ingress Controller should rewrite paths in HTTP requests using this route before forwarding the requests to the backend application.
The part of the request path that matches the path specified in `spec.path` is replaced with the rewrite target specified in the annotation.

The following table provides examples of the path rewriting behavior for various combinations of `spec.path`, request path, and rewrite target.

.rewrite-target examples:
[cols="4*", options="header"]
|===
|Route.spec.path|Request path|Rewrite target| Forwarded request path
|/foo|/foo|/|/
|/foo|/foo/|/|/
|/foo|/foo/bar|/|/bar
|/foo|/foo/bar/|/|/bar/
|/foo|/foo|/bar|/bar
|/foo|/foo/|/bar|/bar/
|/foo|/foo/bar|/baz|/baz/bar
|/foo|/foo/bar/|/baz|/baz/bar/
|/foo/|/foo|/|N/A (request path does not match route path)
|/foo/|/foo/|/|/
|/foo/|/foo/bar|/|/bar
|===

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * ingress/configure-ingress-operator.adoc
// * networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-route-admission-policy_{context}"]
= Configuring the route admission policy

Administrators and application developers can run applications in multiple namespaces with the same domain name. This is for organizations where multiple teams develop microservices that are exposed on the same hostname.

[WARNING]
====
Allowing claims across namespaces should only be enabled for clusters with trust between namespaces, otherwise a malicious user could take over a hostname. For this reason, the default admission policy disallows hostname claims across namespaces.
====

.Prerequisites

* Cluster administrator privileges.

.Procedure

* Edit the `.spec.routeAdmission` field of the `ingresscontroller` resource variable using the following command:
+
[source,terminal]
----
$ oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{"spec":{"routeAdmission":{"namespaceOwnership":"InterNamespaceAllowed"}}}' --type=merge
----
+
.Sample Ingress Controller configuration
[source,yaml]
----
spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed
...
----
+
[TIP]
====
You can alternatively apply the following YAML to configure the route admission policy:
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed
----
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-ingress-creating-a-route-via-an-ingress_{context}"]
= Creating a route through an Ingress object

Some ecosystem components have an integration with Ingress resources but not with route resources. To cover this case, {product-title} automatically creates managed route objects when an Ingress object is created. These route objects are deleted when the corresponding Ingress objects are deleted.

.Procedure

. Define an Ingress object in the {product-title} console or by entering the `oc create` command:
+
.YAML Definition of an Ingress
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: "reencrypt" <1>
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert <3>
spec:
  rules:
  - host: www.example.com <2>
    http:
      paths:
      - backend:
          service:
            name: frontend
            port:
              number: 443
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - www.example.com
    secretName: example-com-tls-certificate
----
+
<1> The `route.openshift.io/termination` annotation can be used to configure the `spec.tls.termination` field of the `Route` as `Ingress` has  no field for this. The accepted values are `edge`, `passthrough` and `reencrypt`. All other values are silently ignored. When  the annotation value is unset, `edge` is the default route. The TLS certificate details must be defined in the template file to implement the default edge route.
<2> When working with an `Ingress` object, you must specify an explicit hostname, unlike when working with routes. You can use the `<host_name>.<cluster_ingress_domain>` syntax, for example `apps.openshiftdemos.com`, to take advantage of the `*.<cluster_ingress_domain>` wildcard DNS record and serving certificate for the cluster. Otherwise, you must ensure that there is a DNS record for the chosen hostname.

.. If you specify the `passthrough` value in the `route.openshift.io/termination` annotation, set `path` to `''` and `pathType` to `ImplementationSpecific` in the spec:
+
[source,yaml]
----
  spec:
    rules:
    - host: www.example.com
      http:
        paths:
        - path: ''
          pathType: ImplementationSpecific
          backend:
            service:
              name: frontend
              port:
                number: 443
----
+
[source,terminal]
----
$ oc apply -f ingress.yaml
----
+
<3> The `route.openshift.io/destination-ca-certificate-secret` can be used on an Ingress object to define a route with a custom destination certificate (CA). The annotation references a kubernetes secret, `secret-ca-cert` that will be inserted into the generated route.

.. To specify a route object with a destination CA from an ingress object, you must create a `kubernetes.io/tls` or `Opaque` type secret with a certificate in PEM-encoded format in the `data.tls.crt` specifier of the secret.

+
. List your routes:
+
[source,terminal]
----
$ oc get routes
----
+
The result includes an autogenerated route whose name starts with `frontend-`:
+
[source,terminal]
----
NAME             HOST/PORT         PATH    SERVICES    PORT    TERMINATION          WILDCARD
frontend-gnztq   www.example.com           frontend    443     reencrypt/Redirect   None
----
+
If you inspect this route, it looks this:
+
.YAML Definition of an autogenerated route
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend-gnztq
  ownerReferences:
  - apiVersion: networking.k8s.io/v1
    controller: true
    kind: Ingress
    name: frontend
    uid: 4e6c59cc-704d-4f44-b390-617d879033b6
spec:
  host: www.example.com
  path: /
  port:
    targetPort: https
  tls:
    certificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    insecureEdgeTerminationPolicy: Redirect
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      [...]
      -----END RSA PRIVATE KEY-----
    termination: reencrypt
    destinationCACertificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
  to:
    kind: Service
    name: frontend
----

:leveloffset!:

:leveloffset: +1

// This is included in the following assemblies:
//
// networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-edge-route-with-default-certificate_{context}"]
= Creating a route using the default certificate through an Ingress object

If you create an Ingress object without specifying any TLS configuration, {product-title} generates an insecure route. To create an Ingress object that generates a secure, edge-terminated route using the default ingress certificate, you can specify an empty TLS configuration as follows.

.Prerequisites

* You have a service that you want to expose.
* You have access to the OpenShift CLI (`oc`).

.Procedure

. Create a YAML file for the Ingress object.  In this example, the file is called `example-ingress.yaml`:
+
.YAML definition of an Ingress object
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  ...
spec:
  rules:
    ...
  tls:
  - {} <1>
----
+
<1> Use this exact syntax to specify TLS without specifying a custom certificate.

. Create the Ingress object by running the following command:
+
[source,terminal]
----
$ oc create -f example-ingress.yaml
----

.Verification
* Verify that {product-title} has created the expected route for the Ingress object by running the following command:
+
[source,terminal]
----
$ oc get routes -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: v1
items:
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: frontend-j9sdd <1>
    ...
  spec:
  ...
    tls: <2>
      insecureEdgeTerminationPolicy: Redirect
      termination: edge <3>
  ...
----
<1> The name of the route includes the name of the Ingress object followed by a random suffix.
<2> In order to use the default certificate, the route should not specify `spec.certificate`.
<3> The route should specify the `edge` termination policy.

:leveloffset!:

:leveloffset: +1

// This is included in the following assemblies:
//
// networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-re-encrypt-route-with-custom-certificate_{context}"]
= Creating a route using the destination CA certificate in the Ingress annotation

The `route.openshift.io/destination-ca-certificate-secret` annotation can be used on an Ingress object to define a route with a custom destination CA certificate.

.Prerequisites
* You may have a certificate/key pair in PEM-encoded files, where the certificate is valid for the route host.
* You may have a separate CA certificate in a PEM-encoded file that completes the certificate chain.
* You must have a separate destination CA certificate in a PEM-encoded file.
* You must have a service that you want to expose.


.Procedure

. Add the `route.openshift.io/destination-ca-certificate-secret` to the Ingress annotations:
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: "reencrypt"
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert <1>
...
----
<1> The annotation references a kubernetes secret.

+
. The secret referenced in this annotation will be inserted into the generated route.
+
.Example output
[source,yaml]
----
apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: reencrypt
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert
spec:
...
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: reencrypt
    destinationCACertificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
...
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/routes/route-configuration.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-router-configuring-dual-stack_{context}"]
= Configuring the {product-title} Ingress Controller for dual-stack networking

If your {product-title} cluster is configured for IPv4 and IPv6 dual-stack networking, your cluster is externally reachable by {product-title} routes.

The Ingress Controller automatically serves services that have both IPv4 and IPv6 endpoints, but you can configure the Ingress Controller for single-stack or dual-stack services.

.Prerequisites

* You deployed an {product-title} cluster on bare metal.
* You installed the OpenShift CLI (`oc`).

.Procedure

. To have the Ingress Controller serve traffic over IPv4/IPv6 to a workload, you can create a service YAML file or modify an existing service YAML file by setting the `ipFamilies` and `ipFamilyPolicy` fields. For example:
+
.Sample service YAML file
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  creationTimestamp: yyyy-mm-ddT00:00:00Z
  labels:
    name: <service_name>
    manager: kubectl-create
    operation: Update
    time: yyyy-mm-ddT00:00:00Z
  name: <service_name>
  namespace: <namespace_name>
  resourceVersion: "<resource_version_number>"
  selfLink: "/api/v1/namespaces/<namespace_name>/services/<service_name>"
  uid: <uid_number>
spec:
  clusterIP: 172.30.0.0/16
  clusterIPs: <1>
  - 172.30.0.0/16
  - <second_IP_address>
  ipFamilies: <2>
  - IPv4
  - IPv6
  ipFamilyPolicy: RequireDualStack <3>
  ports:
  - port: 8080
    protocol: TCP
    targetport: 8080
  selector:
    name: <namespace_name>
  sessionAffinity: None
  type: ClusterIP
status:
  loadbalancer: {}
----
<1> In a dual-stack instance, there are two different `clusterIPs` provided.
<2> For a single-stack instance, enter `IPv4` or `IPv6`. For a dual-stack instance, enter both `IPv4` and `IPv6`.
<3> For a single-stack instance, enter `SingleStack`. For a dual-stack instance, enter `RequireDualStack`.
+
These resources generate corresponding `endpoints`. The Ingress Controller now watches `endpointslices`.
+
. To view `endpoints`, enter the following command:
+
[source,terminal]
----
$ oc get endpoints
----
+
. To view `endpointslices`, enter the following command:
+
[source,terminal]
----
$ oc get endpointslices
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../networking/ingress-operator.adoc#configuring-ingress[Specifying an alternative cluster domain using the appsDomain option]

//# includes=_attributes/common-attributes,_attributes/attributes-openshift-dedicated,modules/nw-creating-a-route,modules/nw-ingress-sharding-route-configuration,modules/nw-configuring-route-timeouts,modules/nw-enabling-hsts,modules/nw-enabling-hsts-per-route,modules/nw-disabling-hsts,modules/nw-enforcing-hsts-per-domain,modules/nw-throughput-troubleshoot,modules/nw-using-cookies-keep-route-statefulness,modules/nw-annotating-a-route-with-a-cookie-name,modules/nw-path-based-routes,modules/nw-http-header-configuration,modules/nw-route-set-or-delete-http-headers,modules/nw-route-specific-annotations,modules/nw-route-admission-policy,modules/nw-ingress-creating-a-route-via-an-ingress,modules/nw-ingress-edge-route-default-certificate,modules/nw-ingress-reencrypt-route-custom-cert,modules/nw-router-configuring-dual-stack

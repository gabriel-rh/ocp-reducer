:_mod-docs-content-type: ASSEMBLY
[id="configuring-hardware-offloading"]
= Configuring hardware offloading
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: configuring-hardware-offloading

toc::[]

As a cluster administrator, you can configure hardware offloading on compatible nodes to increase data processing performance and reduce load on host CPUs.

//What is this feature, who needs it?

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: CONCEPT
[id="about-hardware-offloading_{context}"]
= About hardware offloading

Open vSwitch hardware offloading is a method of processing network tasks by diverting them away from the CPU and offloading them to a dedicated processor on a network interface controller.
As a result, clusters can benefit from faster data transfer speeds, reduced CPU workloads, and lower computing costs.

The key element for this feature is a modern class of network interface controllers known as SmartNICs.
A SmartNIC is a network interface controller that is able to handle computationally-heavy network processing tasks.
In the same way that a dedicated graphics card can improve graphics performance, a SmartNIC can improve network performance.
In each case, a dedicated processor improves performance for a specific type of processing task.

In {product-title}, you can configure hardware offloading for bare metal nodes that have a compatible SmartNIC.
Hardware offloading is configured and enabled by the SR-IOV Network Operator.

Hardware offloading is not compatible with all workloads or application types.
Only the following two communication types are supported:

* pod-to-pod
* pod-to-service, where the service is a ClusterIP service backed by a regular pod

In all cases, hardware offloading takes place only when those pods and services are assigned to nodes that have a compatible SmartNIC.
Suppose, for example, that a pod on a node with hardware offloading tries to communicate with a service on a regular node.
On the regular node, all the processing takes place in the kernel, so the overall performance of the pod-to-service communication is limited to the maximum performance of that regular node.
Hardware offloading is not compatible with DPDK applications.

Enabling hardware offloading on a node, but not configuring pods to use, it can result in decreased throughput performance for pod traffic. You cannot configure hardware offloading for pods that are managed by {product-title}.

:leveloffset!:

//Which NICs do we support?

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: REFERENCE
[id="supported_devices_{context}"]
= Supported devices

Hardware offloading is supported on the following network interface controllers:

.Supported network interface controllers
[cols="1,2,1,1"]
|===
|Manufacturer |Model |Vendor ID | Device ID

|Mellanox
|MT27800 Family [ConnectX&#8209;5]
|15b3
|1017

|Mellanox
|MT28880 Family [ConnectX&#8209;5{nbsp}Ex]
|15b3
|1019

|Mellanox
|MT2892 Family [ConnectX&#8209;6 Dx]
|15b3
|101d

|Mellanox
|MT2894 Family [ConnectX-6 Lx]
|15b3
|101f

|Mellanox
|MT42822 BlueField-2 in ConnectX-6 NIC mode
|15b3
|a2d6
|===

:leveloffset!:

[id="configuring-hardware-offloading-prerequisites"]
== Prerequisites

* Your cluster has at least one bare metal machine with a network interface controller that is supported for hardware offloading.
* You xref:../../networking/hardware_networks/installing-sriov-operator.adoc#installing-sr-iov-operator_installing-sriov-operator[installed the SR-IOV Network Operator].
* Your cluster uses the xref:../../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.adoc#about-ovn-kubernetes[OVN-Kubernetes network plugin].
* In your xref:../../networking/cluster-network-operator.adoc#gatewayConfig-object_cluster-network-operator[OVN-Kubernetes network plugin configuration], the `gatewayConfig.routingViaHost` field is set to `false`.

//Configure a machine config pool for hardware offloading
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-machine-config-pool_{context}"]
= Configuring a machine config pool for hardware offloading

To enable hardware offloading, you must first create a dedicated machine config pool and configure it to work with the SR-IOV Network Operator.

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Create a machine config pool for machines you want to use hardware offloading on.

.. Create a file, such as `mcp-offloading.yaml`, with content like the following example:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: mcp-offloading <1>
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,mcp-offloading]} <1>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/mcp-offloading: "" <2>
----
<1> The name of your machine config pool for hardware offloading.
<2> This node role label is used to add nodes to the machine config pool.

.. Apply the configuration for the machine config pool:
+
[source,terminal]
----
$ oc create -f mcp-offloading.yaml
----

. Add nodes to the machine config pool. Label each node with the node role label of your pool:
+
[source,terminal]
----
$ oc label node worker-2 node-role.kubernetes.io/mcp-offloading=""
----

. Optional: To verify that the new pool is created, run the following command:
+
[source,terminal]
----
$ oc get nodes
----
+
--
.Example output
[source,terminal]
----
NAME       STATUS   ROLES                   AGE   VERSION
master-0   Ready    master                  2d    v1.27.3
master-1   Ready    master                  2d    v1.27.3
master-2   Ready    master                  2d    v1.27.3
worker-0   Ready    worker                  2d    v1.27.3
worker-1   Ready    worker                  2d    v1.27.3
worker-2   Ready    mcp-offloading,worker   47h   v1.27.3
worker-3   Ready    mcp-offloading,worker   47h   v1.27.3
----
--

. Add this machine config pool to the `SriovNetworkPoolConfig` custom resource:

.. Create a file, such as `sriov-pool-config.yaml`, with content like the following example:
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkPoolConfig
metadata:
  name: sriovnetworkpoolconfig-offload
  namespace: openshift-sriov-network-operator
spec:
  ovsHardwareOffloadConfig:
    name: mcp-offloading <1>
----
<1> The name of your machine config pool for hardware offloading.

.. Apply the configuration:
+
[source,terminal]
----
$ oc create -f <SriovNetworkPoolConfig_name>.yaml
----
+
[NOTE]
=====
When you apply the configuration specified in a `SriovNetworkPoolConfig` object, the SR-IOV Operator drains and restarts the nodes in the machine config pool.

It might take several minutes for a configuration changes to apply.
=====

:leveloffset!:

//Configuring the SR-IOV network node policy
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: PROCEDURE
[id="configure-sriov-node-policy_{context}"]
= Configuring the SR-IOV network node policy

You can create an SR-IOV network device configuration for a node by creating an SR-IOV network node policy.
To enable hardware offloading, you must define the `.spec.eSwitchMode` field with the value `"switchdev"`.

The following procedure creates an SR-IOV interface for a network interface controller with hardware offloading.

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Create a file, such as `sriov-node-policy.yaml`, with content like the following example:
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-policy <.>
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice <.>
  eSwitchMode: "switchdev" <.>
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 6
  priority: 5
  resourceName: mlxnics
----
<.> The name for the custom resource object.
<.> Required. Hardware offloading is not supported with `vfio-pci`.
<.> Required.

. Apply the configuration for the policy:
+
[source,terminal]
----
$ oc create -f sriov-node-policy.yaml
----
+
[NOTE]
=====
When you apply the configuration specified in a `SriovNetworkPoolConfig` object, the SR-IOV Operator drains and restarts the nodes in the machine config pool.

It might take several minutes for a configuration change to apply.
=====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: PROCEDURE
[id="nw-sriov-hwol-ref-openstack-sriov-policy_{context}"]
= An example SR-IOV network node policy for OpenStack

The following example describes an SR-IOV interface for a network interface controller (NIC) with hardware offloading on {rh-openstack-first}.

.An SR-IOV interface for a NIC with hardware offloading on {rh-openstack}
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: ${name}
  namespace: openshift-sriov-network-operator
spec:
  deviceType: switchdev
  isRdma: true
  nicSelector:
    netFilter: openstack/NetworkID:${net_id}
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: ${name}
----

:leveloffset!:

// Improving network traffic performance using a virtual function
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/hardware_networks/configuring-hardware-offloading.adoc

:_mod-docs-content-type: PROCEDURE
[id="improving-network-traffic-performance-using-vf_{context}"]
= Improving network traffic performance using a virtual function

Follow this procedure to assign a virtual function to the OVN-Kubernetes management port and increase its network traffic performance.

This procedure results in the creation of two pools: the first has a virtual function used by OVN-Kubernetes, and the second comprises the remaining virtual functions.

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Add the `network.operator.openshift.io/smart-nic` label to each worker node with a SmartNIC present by running the following command:
+
[source,terminal]
----
$ oc label node <node-name> network.operator.openshift.io/smart-nic=
----
+
Use the `oc get nodes` command to get a list of the available nodes.

. Create a policy named `sriov-node-mgmt-vf-policy.yaml` for the management port with content such as the following example:
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-mgmt-vf-policy
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  eSwitchMode: "switchdev"
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0#0-0 <.>
  nodeSelector:
    network.operator.openshift.io/smart-nic: ""
  numVfs: 6 <.>
  priority: 5
  resourceName: mgmtvf
----
<.> Replace this device with the appropriate network device for your use case. The `#0-0` part of the `pfNames` value reserves a single virtual function used by OVN-Kubernetes.
<.> The value provided here is an example. Replace this value with one that meets your requirements. For more information, see _SR-IOV network node configuration object_ in the _Additional resources_ section.

. Create a policy named `sriov-node-policy.yaml` with content such as the following example:
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-policy
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  eSwitchMode: "switchdev"
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0#1-5 <.>
  nodeSelector:
    network.operator.openshift.io/smart-nic: ""
  numVfs: 6 <.>
  priority: 5
  resourceName: mlxnics
----
<.> Replace this device with the appropriate network device for your use case.
<.> The value provided here is an example. Replace this value with the value specified in the `sriov-node-mgmt-vf-policy.yaml` file. For more information, see _SR-IOV network node configuration object_ in the _Additional resources_ section.
+
[NOTE]
====
The `sriov-node-mgmt-vf-policy.yaml` file has different values for the `pfNames` and `resourceName` keys than the `sriov-node-policy.yaml` file.
====

. Apply the configuration for both policies:
+
[source,terminal]
----
$ oc create -f sriov-node-policy.yaml
----
+
[source,terminal]
----
$ oc create -f sriov-node-mgmt-vf-policy.yaml
----

. Create a Cluster Network Operator (CNO) ConfigMap in the cluster for the management configuration:

.. Create a ConfigMap named `hardware-offload-config.yaml` with the following contents:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
    name: hardware-offload-config
    namespace: openshift-network-operator
data:
    mgmt-port-resource-name: openshift.io/mgmtvf
----

.. Apply the configuration for the ConfigMap:
+
[source,terminal]
----
$ oc create -f hardware-offload-config.yaml
----

:leveloffset!:

[role="_additional-resources"]
[id="additional-resources_using-vf-improve-network-traffic-performance"]
.Additional resources
* xref:../../networking/hardware_networks/configuring-sriov-device.adoc#nw-sriov-networknodepolicy-object_configuring-sriov-device[SR-IOV network node configuration object]

//Creating a Network Attachment Definition
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: PROCEDURE
[id="create-network-attachment-definition_{context}"]
= Creating a network attachment definition

After you define the machine config pool and the SR-IOV network node policy, you can create a network attachment definition for the network interface card you specified.

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Create a file, such as `net-attach-def.yaml`, with content like the following example:
+
[source,yaml]
----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: net-attach-def <.>
  namespace: net-attach-def <.>
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/mlxnics <.>
spec:
  config: '{"cniVersion":"0.3.1","name":"ovn-kubernetes","type":"ovn-k8s-cni-overlay","ipam":{},"dns":{}}'
----
<.> The name for your network attachment definition.
<.> The namespace for your network attachment definition.
<.> This is the value of the `spec.resourceName` field you specified in the `SriovNetworkNodePolicy` object.

. Apply the configuration for the network attachment definition:
+
[source,terminal]
----
$ oc create -f net-attach-def.yaml
----

.Verification

* Run the following command to see whether the new definition is present:
+
[source,terminal]
----
$ oc get net-attach-def -A
----
+
.Example output
[source,terminal]
----
NAMESPACE         NAME             AGE
net-attach-def    net-attach-def   43h
----

:leveloffset!:

//Adding the network attachment definition to your pods
:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/configuring-hardware-offloading.adoc

:_mod-docs-content-type: PROCEDURE
[id="adding-network-attachment-definition-to-pods_{context}"]
= Adding the network attachment definition to your pods

After you create the machine config pool, the `SriovNetworkPoolConfig` and `SriovNetworkNodePolicy` custom resources, and the network attachment definition, you can apply these configurations to your pods by adding the network attachment definition to your pod specifications.

.Procedure

* In the pod specification, add the `.metadata.annotations.k8s.v1.cni.cncf.io/networks` field and specify the network attachment definition you created for hardware offloading:
+
[source,yaml]
----
....
metadata:
  annotations:
    v1.multus-cni.io/default-network: net-attach-def/net-attach-def <.>
----
<.> The value must be the name and namespace of the network attachment definition you created for hardware offloading.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/nw-sriov-hwol-about-hardware-offloading,modules/nw-sriov-hwol-supported-devices,modules/nw-sriov-hwol-configuring-machine-config-pool,modules/nw-sriov-hwol-creating-sriov-policy,modules/nw-sriov-hwol-ref-openstack-sriov-policy,modules/nw-sriov-hwol-improving-network-traffic-performance,modules/nw-sriov-hwol-creating-network-attachment-definition,modules/nw-sriov-hwol-adding-network-attachment-definitions-to-pods

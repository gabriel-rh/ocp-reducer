:_mod-docs-content-type: ASSEMBLY
[id="installing-troubleshooting"]
= Troubleshooting installation issues
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: installing-troubleshooting

toc::[]

To assist in troubleshooting a failed {product-title} installation, you can gather logs from the bootstrap and control plane machines. You can also get debug information from the installation program. If you are unable to resolve the issue using the logs and debug information, see xref:../support/troubleshooting/troubleshooting-installations.adoc#determining-where-installation-issues-occur_troubleshooting-installations[Determining where installation issues occur] for component-specific troubleshooting.

[NOTE]
====
If your {product-title} installation fails and the debug output or logs contain network timeouts or other connectivity errors, review the guidelines for xref:../installing/install_config/configuring-firewall.adoc#configuring-firewall[configuring your firewall]. Gathering logs from your firewall and load balancer can help you diagnose network-related errors.
====

== Prerequisites

* You attempted to install an {product-title} cluster and the installation failed.

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-troubleshooting.adoc
// * support/troubleshooting/troubleshooting-installations.adoc

:_mod-docs-content-type: PROCEDURE
[id="installation-bootstrap-gather_{context}"]
= Gathering logs from a failed installation

If you gave an SSH key to your installation program, you can gather data about
your failed installation.

[NOTE]
====
You use a different command to gather logs about an unsuccessful installation
than to gather logs from a running cluster. If you must gather logs from a
running cluster, use the `oc adm must-gather` command.
====

.Prerequisites

* Your {product-title} installation failed before the bootstrap process finished. The bootstrap node is running and accessible through SSH.
* The `ssh-agent` process is active on your computer, and you provided the same SSH key to both the `ssh-agent` process and the installation program.
* If you tried to install a cluster on infrastructure that you provisioned, you must have the fully qualified domain names of the bootstrap and control plane nodes.

.Procedure

. Generate the commands that are required to obtain the installation logs from
the bootstrap and control plane machines:
+
--
** If you used installer-provisioned infrastructure, change to the directory that contains the installation program and run the following command:
+
[source,terminal]
----
$ ./openshift-install gather bootstrap --dir <installation_directory> <1>
----
<1> `installation_directory` is the directory you specified when you ran `./openshift-install create cluster`. This directory contains the {product-title}
definition files that the installation program creates.
+
For installer-provisioned infrastructure, the installation program stores
information about the cluster, so you do not specify the hostnames or IP
addresses.

** If you used infrastructure that you provisioned yourself, change to the directory that contains the installation program and run the following
command:
+
[source,terminal]
----
$ ./openshift-install gather bootstrap --dir <installation_directory> \ <1>
    --bootstrap <bootstrap_address> \ <2>
    --master <master_1_address> \ <3>
    --master <master_2_address> \ <3>
    --master <master_3_address>" <3>
----
<1> For `installation_directory`, specify the same directory you specified when you ran `./openshift-install create cluster`. This directory contains the {product-title}
definition files that the installation program creates.
<2> `<bootstrap_address>` is the fully qualified domain name or IP address of
the cluster's bootstrap machine.
<3> For each control plane, or master, machine in your cluster, replace `<master_*_address>` with its fully qualified domain name or IP address.
+
[NOTE]
====
A default cluster contains three control plane machines. List all of your control plane machines as shown, no matter how many your cluster uses.
====
--
+
.Example output
[source,terminal]
----
INFO Pulling debug logs from the bootstrap machine
INFO Bootstrap gather logs captured here "<installation_directory>/log-bundle-<timestamp>.tar.gz"
----
+
If you open a Red Hat support case about your installation failure, include
the compressed logs in the case.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *installing/installing-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="installation-manually-gathering-logs-with-SSH_{context}"]
= Manually gathering logs with SSH access to your host(s)

Manually gather logs in situations where `must-gather` or automated collection
methods do not work.

[IMPORTANT]
====
By default, SSH access to the {product-title} nodes is disabled on the {rh-openstack-first} based installations.
====

.Prerequisites

* You must have SSH access to your host(s).

.Procedure

. Collect the `bootkube.service` service logs from the bootstrap host using the
`journalctl` command by running:
+
[source,terminal]
----
$ journalctl -b -f -u bootkube.service
----

. Collect the bootstrap host's container logs using the podman logs. This is shown
as a loop to get all of the container logs from the host:
+
[source,terminal]
----
$ for pod in $(sudo podman ps -a -q); do sudo podman logs $pod; done
----

. Alternatively, collect the host's container logs using the `tail` command by
running:
+
[source,terminal]
----
# tail -f /var/lib/containers/storage/overlay-containers/*/userdata/ctr.log
----

. Collect the `kubelet.service` and `crio.service` service logs from the master
and worker hosts using the `journalctl` command by running:
+
[source,terminal]
----
$ journalctl -b -f -u kubelet.service -u crio.service
----

. Collect the master and worker host container logs using the `tail` command by
running:
+
[source,terminal]
----
$ sudo tail -f /var/log/containers/*
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *installing/installing-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="installation-manually-gathering-logs-without-SSH_{context}"]
= Manually gathering logs without SSH access to your host(s)

Manually gather logs in situations where `must-gather` or automated collection
methods do not work.

If you do not have SSH access to your node, you can access the systems journal
to investigate what is happening on your host.

.Prerequisites

* Your {product-title} installation must be complete.
* Your API service is still functional.
* You have system administrator privileges.

.Procedure

. Access `journald` unit logs under `/var/log` by running:
+
[source,terminal]
----
$ oc adm node-logs --role=master -u kubelet
----

. Access host file paths under `/var/log` by running:
+
[source,terminal]
----
$ oc adm node-logs --role=master --path=openshift-apiserver
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//

[id="installing-getting-debug-information_{context}"]
= Getting debug information from the installation program

You can use any of the following actions to get debug information from the installation program.

* Look at debug messages from a past installation in the hidden `.openshift_install.log` file. For example, enter:
+
[source,terminal]
----
$ cat ~/<installation_directory>/.openshift_install.log <1>
----
<1> For `installation_directory`, specify the same directory you specified when you ran `./openshift-install create cluster`.

* Change to the directory that contains the installation program and re-run it with `--log-level=debug`:
+
[source,terminal]
----
$ ./openshift-install create cluster --dir <installation_directory> --log-level debug <1>
----
<1> For `installation_directory`, specify the same directory you specified when you ran `./openshift-install create cluster`.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// installing/installing-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="restarting-installation_{context}"]
= Reinstalling the {product-title} cluster

If you are unable to debug and resolve issues in the failed {product-title} installation, consider installing a new {product-title} cluster. Before starting the installation process again, you must complete thorough cleanup.
For a user-provisioned infrastructure (UPI) installation, you must manually destroy the cluster and delete all associated resources. The following procedure is for an installer-provisioned infrastructure (IPI) installation.

.Procedure

. Destroy the cluster and remove all the resources associated with the cluster, including the hidden installer state files in the installation directory:
+
[source,terminal]
----
$ ./openshift-install destroy cluster --dir <installation_directory> <1>
----
<1> `installation_directory` is the directory you specified when you ran `./openshift-install create cluster`. This directory contains the {product-title}
definition files that the installation program creates.

. Before reinstalling the cluster, delete the installation directory:
+
[source,terminal]
----
$ rm -rf <installation_directory>
----

. Follow the procedure for installing a new {product-title} cluster.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/index.adoc#ocp-installation-overview[Installing an {product-title} cluster]

//# includes=_attributes/common-attributes,modules/installation-bootstrap-gather,modules/manually-gathering-logs-with-ssh,modules/manually-gathering-logs-without-ssh,modules/installation-getting-debug-information,modules/restarting-installation

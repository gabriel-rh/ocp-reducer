:_mod-docs-content-type: ASSEMBLY
[id="ipi-install-expanding-the-cluster"]
= Expanding the cluster
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: ipi-install-expanding

toc::[]

After deploying an installer-provisioned {product-title} cluster, you can use the following procedures to expand the number of worker nodes. Ensure that each prospective worker node meets the prerequisites.

[NOTE]
====
Expanding the cluster using RedFish Virtual Media involves meeting minimum firmware requirements. See *Firmware requirements for installing with virtual media* in the *Prerequisites* section for additional details when expanding the cluster using RedFish Virtual Media.
====

:leveloffset: +1

// This is included in the following assemblies:
//
// installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.adoc

:_mod-docs-content-type: PROCEDURE
[id='preparing-the-bare-metal-node_{context}']
= Preparing the bare metal node

To expand your cluster, you must provide the node with the relevant IP address. This can be done with a static configuration, or with a DHCP (Dynamic Host Configuration protocol) server. When expanding the cluster using a DHCP server, each node must have a DHCP reservation.


[IMPORTANT]
.Reserving IP addresses so they become static IP addresses
====
Some administrators prefer to use static IP addresses so that each node's IP address remains constant in the absence of a DHCP server. To configure static IP addresses with NMState, see "Optional: Configuring host network interfaces in the `install-config.yaml` file" in the "Setting up the environment for an OpenShift installation" section for additional details.
====

Preparing the bare metal node requires executing the following procedure from the provisioner node.

.Procedure

. Get the `oc` binary:
+
[source,terminal]
----
$ curl -s https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$VERSION/openshift-client-linux-$VERSION.tar.gz | tar zxvf - oc
----
+
[source,terminal]
----
$ sudo cp oc /usr/local/bin
----

. Power off the bare metal node by using the baseboard management controller (BMC), and ensure it is off.

. Retrieve the user name and password of the bare metal node's baseboard management controller. Then, create `base64` strings from the user name and password:
+
[source,terminal,subs="+quotes"]
----
$ echo -ne "root" | base64
----
+
[source,terminal]
----
$ echo -ne "password" | base64
----

. Create a configuration file for the bare metal node. Depending on whether you are using a static configuration or a DHCP server, use one of the following example `bmh.yaml` files, replacing values in the YAML to match your environment:
+
[source,terminal]
----
$ vim bmh.yaml
----
* *Static configuration* `bmh.yaml`:
+
[source,yaml]
----
---
apiVersion: v1 <1>
kind: Secret
metadata:
 name: openshift-worker-<num>-network-config-secret <2>
 namespace: openshift-machine-api
type: Opaque
stringData:
 nmstate: | <3>
  interfaces: <4>
  - name: <nic1_name> <5>
    type: ethernet
    state: up
    ipv4:
      address:
      - ip: <ip_address> <5>
        prefix-length: 24
      enabled: true
  dns-resolver:
    config:
      server:
      - <dns_ip_address> <5>
  routes:
    config:
    - destination: 0.0.0.0/0
      next-hop-address: <next_hop_ip_address> <5>
      next-hop-interface: <next_hop_nic1_name> <5>
---
apiVersion: v1
kind: Secret
metadata:
  name: openshift-worker-<num>-bmc-secret <2>
  namespace: openshift-machine-api
type: Opaque
data:
  username: <base64_of_uid> <6>
  password: <base64_of_pwd> <6>
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: openshift-worker-<num> <2>
  namespace: openshift-machine-api
spec:
  online: True
  bootMACAddress: <nic1_mac_address> <7>
  bmc:
    address: <protocol>://<bmc_url> <8>
    credentialsName: openshift-worker-<num>-bmc-secret <2>
    disableCertificateVerification: True <9>
    username: <bmc_username> <10>
    password: <bmc_password> <10>
  rootDeviceHints:
    deviceName: <root_device_hint> <11>
  preprovisioningNetworkDataName: openshift-worker-<num>-network-config-secret <12>
----
+
--
<1> To configure the network interface for a newly created node, specify the name of the secret that contains the network configuration. Follow the `nmstate` syntax to define the network configuration for your node. See "Optional: Configuring host network interfaces in the install-config.yaml file" for details on configuring NMState syntax.
<2> Replace `<num>` for the worker number of the bare metal node in the `name` fields, the `credentialsName` field, and the `preprovisioningNetworkDataName` field.
<3> Add the NMState YAML syntax to configure the host interfaces.
<4> Optional: If you have configured the network interface with `nmstate`, and you want to disable an interface, set `state: up` with the IP addresses set to `enabled: false` as shown:
+
[source,yaml]
----
---
   interfaces:
   - name: <nic_name>
     type: ethernet
     state: up
     ipv4:
       enabled: false
     ipv6:
       enabled: false
----
<5> Replace `<nic1_name>`, `<ip_address>`, `<dns_ip_address>`, `<next_hop_ip_address>` and `<next_hop_nic1_name>` with appropriate values.
<6> Replace `<base64_of_uid>` and  `<base64_of_pwd>` with the base64 string of the user name and password.
<7> Replace `<nic1_mac_address>` with the MAC address of the bare metal node's first NIC. See the "BMC addressing" section for additional BMC configuration options.
<8> Replace `<protocol>` with the BMC protocol, such as IPMI, RedFish, or others. Replace `<bmc_url>` with the URL of the bare metal node's baseboard management controller.
<9> To skip certificate validation, set `disableCertificateVerification` to true.
<10> Replace `<bmc_username>` and `<bmc_password>` with the string of the BMC user name and password.
<11> Optional: Replace `<root_device_hint>` with a device path if you specify a root device hint.
<12> Optional: If you have configured the network interface for the newly created node, provide the network configuration secret name in the `preprovisioningNetworkDataName` of the BareMetalHost CR.
--

* *DHCP configuration* `bmh.yaml`:
+
[source,yaml]
----
---
apiVersion: v1
kind: Secret
metadata:
  name: openshift-worker-<num>-bmc-secret <1>
  namespace: openshift-machine-api
type: Opaque
data:
  username: <base64_of_uid> <2>
  password: <base64_of_pwd> <2>
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: openshift-worker-<num> <1>
  namespace: openshift-machine-api
spec:
  online: True
  bootMACAddress: <nic1_mac_address> <3>
  bmc:
    address: <protocol>://<bmc_url> <4>
    credentialsName: openshift-worker-<num>-bmc-secret <1>
    disableCertificateVerification: True <5>
    username: <bmc_username> <6>
    password: <bmc_password> <6>
  rootDeviceHints:
    deviceName: <root_device_hint> <7>
  preprovisioningNetworkDataName: openshift-worker-<num>-network-config-secret <8>
----
+
<1> Replace `<num>` for the worker number of the bare metal node in the `name` fields, the `credentialsName` field, and the `preprovisioningNetworkDataName` field.
+
<2> Replace `<base64_of_uid>` and `<base64_of_pwd>` with the base64 string of the user name and password.
+
<3> Replace `<nic1_mac_address>` with the MAC address of the bare metal node's first NIC. See the "BMC addressing" section for additional BMC configuration options.
+
<4> Replace `<protocol>` with the BMC protocol, such as IPMI, RedFish, or others. Replace `<bmc_url>` with the URL of the bare metal node's baseboard management controller.
+
<5> To skip certificate validation, set `disableCertificateVerification` to true.
+
<6> Replace `<bmc_username>` and `<bmc_password>` with the string of the BMC user name and password.
+
<7> Optional: Replace `<root_device_hint>` with a device path if you specify a root device hint.
+
<8> Optional: If you have configured the network interface for the newly created node, provide the network configuration secret name in the `preprovisioningNetworkDataName` of the BareMetalHost CR.

+
[NOTE]
====
If the MAC address of an existing bare metal node matches the MAC address of a bare metal host that you are attempting to provision, then the Ironic installation will fail. If the host enrollment, inspection, cleaning, or other Ironic steps fail, the Bare Metal Operator retries the installation continuously. See "Diagnosing a host duplicate MAC address" for more information.
====

. Create the bare metal node:
+
[source,terminal]
----
$ oc -n openshift-machine-api create -f bmh.yaml
----
+
.Example output
[source,terminal]
----
secret/openshift-worker-<num>-network-config-secret created
secret/openshift-worker-<num>-bmc-secret created
baremetalhost.metal3.io/openshift-worker-<num> created
----
+
Where `<num>` will be the worker number.

. Power up and inspect the bare metal node:
+
[source,terminal]
----
$ oc -n openshift-machine-api get bmh openshift-worker-<num>
----
+
Where `<num>` is the worker node number.
+
.Example output
[source,terminal]
----
NAME                    STATE       CONSUMER   ONLINE   ERROR
openshift-worker-<num>  available              true
----
+
[NOTE]
====
To allow the worker node to join the cluster, scale the `machineset` object to the number of the `BareMetalHost` objects. You can scale nodes either manually or automatically. To scale nodes automatically, use the `metal3.io/autoscale-to-hosts` annotation for `machineset`.
====

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* See xref:../../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-host-network-interfaces-in-the-install-config-yaml-file_ipi-install-installation-workflow[Optional: Configuring host network interfaces in the install-config.yaml file] for details on configuring the NMState syntax.
* See xref:../../scalability_and_performance/managing-bare-metal-hosts.adoc#automatically-scaling-machines-to-available-bare-metal-hosts_managing-bare-metal-hosts[Automatically scaling machines to the number of available bare metal hosts] for details on automatically scaling machines.

:leveloffset: +1

// This is included in the following assemblies:
//
// installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.adoc

:_mod-docs-content-type: PROCEDURE
[id="replacing-a-bare-metal-control-plane-node_{context}"]
= Replacing a bare-metal control plane node

Use the following procedure to replace an installer-provisioned {product-title} control plane node.

[IMPORTANT]
====
If you reuse the `BareMetalHost` object definition from an existing control plane host, do not leave the `externallyProvisioned` field set to `true`.

Existing control plane `BareMetalHost` objects may have the `externallyProvisioned` flag set to `true` if they were provisioned by the {product-title} installation program.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.

* You have taken an etcd backup.
+
[IMPORTANT]
====
Take an etcd backup before performing this procedure so that you can restore your cluster if you encounter any issues. For more information about taking an etcd backup, see the _Additional resources_ section.
====

.Procedure

. Ensure that the Bare Metal Operator is available:
+
[source,terminal]
----
$ oc get clusteroperator baremetal
----
+
.Example output
[source,terminal]
----
NAME        VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
baremetal   4.12.0   True        False         False      3d15h
----

. Remove the old `BareMetalHost` and `Machine` objects:
+
[source,terminal]
----
$ oc delete bmh -n openshift-machine-api <host_name>
$ oc delete machine -n openshift-machine-api <machine_name>
----
+
Replace `<host_name>` with the name of the host and `<machine_name>` with the name of the machine. The machine name appears under the `CONSUMER` field.
+
After you remove the `BareMetalHost` and `Machine` objects, then the machine controller automatically deletes the `Node` object.

. Create the new `BareMetalHost` object and the secret to store the BMC credentials:
+
[source,terminal]
----
$ cat <<EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: control-plane-<num>-bmc-secret <1>
  namespace: openshift-machine-api
data:
  username: <base64_of_uid> <2>
  password: <base64_of_pwd> <3>
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: control-plane-<num> <1>
  namespace: openshift-machine-api
spec:
  automatedCleaningMode: disabled
  bmc:
    address: <protocol>://<bmc_ip> <4>
    credentialsName: control-plane-<num>-bmc-secret <1>
  bootMACAddress: <NIC1_mac_address> <5>
  bootMode: UEFI
  externallyProvisioned: false
  hardwareProfile: unknown
  online: true
EOF
----
<1> Replace `<num>` for the control plane number of the bare metal node in the `name` fields and the `credentialsName` field.
<2> Replace `<base64_of_uid>` with the `base64` string of the user name.
<3> Replace `<base64_of_pwd>` with the `base64` string of the password.
<4> Replace `<protocol>` with the BMC protocol, such as `redfish`, `redfish-virtualmedia`, `idrac-virtualmedia`, or others. Replace `<bmc_ip>` with the IP address of the bare metal node's baseboard management controller. For additional BMC configuration options, see "BMC addressing" in the _Additional resources_ section.
<5> Replace `<NIC1_mac_address>` with the MAC address of the bare metal node's first NIC.
+
After the inspection is complete, the `BareMetalHost` object is created and available to be provisioned.

. View available `BareMetalHost` objects:
+
[source,terminal]
----
$ oc get bmh -n openshift-machine-api
----
+
.Example output
[source,terminal]
----
NAME                          STATE                    CONSUMER                   ONLINE   ERROR   AGE
control-plane-1.example.com   available                control-plane-1            true             1h10m
control-plane-2.example.com   externally provisioned   control-plane-2            true             4h53m
control-plane-3.example.com   externally provisioned   control-plane-3            true             4h53m
compute-1.example.com         provisioned              compute-1-ktmmx            true             4h53m
compute-1.example.com         provisioned              compute-2-l2zmb            true             4h53m
----
+
There are no `MachineSet` objects for control plane nodes, so you must create a `Machine` object instead. You can copy the `providerSpec` from another control plane `Machine` object.

. Create a `Machine` object:
+
[source,terminal]
----
$ cat <<EOF | oc apply -f -
apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  annotations:
    metal3.io/BareMetalHost: openshift-machine-api/control-plane-<num> <1>
  labels:
    machine.openshift.io/cluster-api-cluster: control-plane-<num> <1>
    machine.openshift.io/cluster-api-machine-role: master
    machine.openshift.io/cluster-api-machine-type: master
  name: control-plane-<num> <1>
  namespace: openshift-machine-api
spec:
  metadata: {}
  providerSpec:
    value:
      apiVersion: baremetal.cluster.k8s.io/v1alpha1
      customDeploy:
        method: install_coreos
      hostSelector: {}
      image:
        checksum: ""
        url: ""
      kind: BareMetalMachineProviderSpec
      metadata:
        creationTimestamp: null
      userData:
        name: master-user-data-managed
EOF
----
<1> Replace `<num>` for the control plane number of the bare metal node in the `name`, `labels` and `annotations` fields.
+
. To view the `BareMetalHost` objects, run the following command:
+
[source,terminal]
----
$ oc get bmh -A
----
+
.Example output
[source,terminal]
----
NAME                          STATE                    CONSUMER                   ONLINE   ERROR   AGE
control-plane-1.example.com   provisioned              control-plane-1            true             2h53m
control-plane-2.example.com   externally provisioned   control-plane-2            true             5h53m
control-plane-3.example.com   externally provisioned   control-plane-3            true             5h53m
compute-1.example.com         provisioned              compute-1-ktmmx            true             5h53m
compute-2.example.com         provisioned              compute-2-l2zmb            true             5h53m
----
+
. After the RHCOS installation, verify that the `BareMetalHost` is added to the cluster:
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source,terminal]
----
NAME                           STATUS      ROLES     AGE   VERSION
control-plane-1.example.com    available   master    4m2s  v1.18.2
control-plane-2.example.com    available   master    141m  v1.18.2
control-plane-3.example.com    available   master    141m  v1.18.2
compute-1.example.com          available   worker    87m   v1.18.2
compute-2.example.com          available   worker    87m   v1.18.2
----
+
[NOTE]
====
After replacement of the new control plane node, the etcd pod running in the new node is in `crashloopback` status. See "Replacing an unhealthy etcd member" in the _Additional resources_ section for more information.
====

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../backup_and_restore/control_plane_backup_and_restore/replacing-unhealthy-etcd-member.adoc#replacing-the-unhealthy-etcd-member[Replacing an unhealthy etcd member]

* xref:../../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.adoc#backing-up-etcd-data_backup-etcd[Backing up etcd]

* xref:../../post_installation_configuration/bare-metal-configuration.adoc#post-install-bare-metal-configuration[Bare metal configuration]

* xref:../../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#bmc-addressing_ipi-install-installation-workflow[BMC addressing]

:leveloffset: +1

// This is included in the following assemblies:
//
// installing_bare_metal_ipi/ipi-install-expanding-the-cluster.adoc

:_mod-docs-content-type: PROCEDURE
[id="preparing-to-deploy-with-virtual-media-on-the-baremetal-network_{context}"]
= Preparing to deploy with Virtual Media on the baremetal network

If the `provisioning` network is enabled and you want to expand the cluster using Virtual Media on the `baremetal` network, use the following procedure.

.Prerequisites

* There is an existing cluster with a `baremetal` network and a `provisioning` network.

.Procedure

. Edit the `provisioning` custom resource (CR) to enable deploying with Virtual Media on the `baremetal` network:
+
[source,terminmal]
----
oc edit provisioning
----
+
[source,yaml]
----
  apiVersion: metal3.io/v1alpha1
  kind: Provisioning
  metadata:
    creationTimestamp: "2021-08-05T18:51:50Z"
    finalizers:
    - provisioning.metal3.io
    generation: 8
    name: provisioning-configuration
    resourceVersion: "551591"
    uid: f76e956f-24c6-4361-aa5b-feaf72c5b526
  spec:
    provisioningDHCPRange: 172.22.0.10,172.22.0.254
    provisioningIP: 172.22.0.3
    provisioningInterface: enp1s0
    provisioningNetwork: Managed
    provisioningNetworkCIDR: 172.22.0.0/24
    virtualMediaViaExternalNetwork: true <1>
  status:
    generations:
    - group: apps
      hash: ""
      lastGeneration: 7
      name: metal3
      namespace: openshift-machine-api
      resource: deployments
    - group: apps
      hash: ""
      lastGeneration: 1
      name: metal3-image-cache
      namespace: openshift-machine-api
      resource: daemonsets
    observedGeneration: 8
    readyReplicas: 0
----
+
<1> Add `virtualMediaViaExternalNetwork: true` to the `provisioning` CR.

. If the image URL exists, edit the `machineset` to use the API VIP address. This step only applies to clusters installed in versions 4.9 or earlier.
+
[source,terminal]
----
oc edit machineset
----
+
[source,yaml]
----
  apiVersion: machine.openshift.io/v1beta1
  kind: MachineSet
  metadata:
    creationTimestamp: "2021-08-05T18:51:52Z"
    generation: 11
    labels:
      machine.openshift.io/cluster-api-cluster: ostest-hwmdt
      machine.openshift.io/cluster-api-machine-role: worker
      machine.openshift.io/cluster-api-machine-type: worker
    name: ostest-hwmdt-worker-0
    namespace: openshift-machine-api
    resourceVersion: "551513"
    uid: fad1c6e0-b9da-4d4a-8d73-286f78788931
  spec:
    replicas: 2
    selector:
      matchLabels:
        machine.openshift.io/cluster-api-cluster: ostest-hwmdt
        machine.openshift.io/cluster-api-machineset: ostest-hwmdt-worker-0
    template:
      metadata:
        labels:
          machine.openshift.io/cluster-api-cluster: ostest-hwmdt
          machine.openshift.io/cluster-api-machine-role: worker
          machine.openshift.io/cluster-api-machine-type: worker
          machine.openshift.io/cluster-api-machineset: ostest-hwmdt-worker-0
      spec:
        metadata: {}
        providerSpec:
          value:
            apiVersion: baremetal.cluster.k8s.io/v1alpha1
            hostSelector: {}
            image:
              checksum: http:/172.22.0.3:6181/images/rhcos-<version>.<architecture>.qcow2.<md5sum> <1>
              url: http://172.22.0.3:6181/images/rhcos-<version>.<architecture>.qcow2 <2>
            kind: BareMetalMachineProviderSpec
            metadata:
              creationTimestamp: null
            userData:
              name: worker-user-data
  status:
    availableReplicas: 2
    fullyLabeledReplicas: 2
    observedGeneration: 11
    readyReplicas: 2
    replicas: 2
----
+
<1> Edit the `checksum` URL to use the API VIP address.
<2> Edit the `url` URL to use the API VIP address.

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: PROCEDURE
[id="ipi-install-diagnosing-duplicate-mac-address_{context}"]
= Diagnosing a duplicate MAC address when provisioning a new host in the cluster

If the MAC address of an existing bare-metal node in the cluster matches the MAC address of a bare-metal host you are attempting to add to the cluster, the Bare Metal Operator associates the host with the existing node. If the host enrollment, inspection, cleaning, or other Ironic steps fail, the Bare Metal Operator retries the installation continuously. A registration error is displayed for the failed bare-metal host.

You can diagnose a duplicate MAC address by examining the bare-metal hosts that are running in the `openshift-machine-api` namespace.

.Prerequisites

* Install an {product-title} cluster on bare metal.
* Install the {product-title} CLI `oc`.
* Log in as a user with `cluster-admin` privileges.

.Procedure

To determine whether a bare-metal host that fails provisioning has the same MAC address as an existing node, do the following:

. Get the bare-metal hosts running in the `openshift-machine-api` namespace:
+
[source,terminal]
----
$ oc get bmh -n openshift-machine-api
----
+
.Example output
[source,terminal]
----
NAME                 STATUS   PROVISIONING STATUS      CONSUMER
openshift-master-0   OK       externally provisioned   openshift-zpwpq-master-0
openshift-master-1   OK       externally provisioned   openshift-zpwpq-master-1
openshift-master-2   OK       externally provisioned   openshift-zpwpq-master-2
openshift-worker-0   OK       provisioned              openshift-zpwpq-worker-0-lv84n
openshift-worker-1   OK       provisioned              openshift-zpwpq-worker-0-zd8lm
openshift-worker-2   error    registering
----

. To see more detailed information about the status of the failing host, run the following command replacing `<bare_metal_host_name>` with the name of the host:
+
[source,terminal]
----
$ oc get -n openshift-machine-api bmh <bare_metal_host_name> -o yaml
----
+
.Example output
[source,yaml]
----
...
status:
  errorCount: 12
  errorMessage: MAC address b4:96:91:1d:7c:20 conflicts with existing node openshift-worker-1
  errorType: registration error
...
----

:leveloffset!:

:leveloffset: +1

// This is included in the following assemblies:
//
// ipi-install-expanding-the-cluster.adoc

:_mod-docs-content-type: PROCEDURE
[id='provisioning-the-bare-metal-node_{context}']
= Provisioning the bare metal node

Provisioning the bare metal node requires executing the following procedure from the provisioner node.

.Procedure

. Ensure the `STATE` is `available` before provisioning the bare metal node.
+
[source,terminal]
----
$  oc -n openshift-machine-api get bmh openshift-worker-<num>
----
+
Where `<num>` is the worker node number.
+
[source,terminal]
----
NAME              STATE     ONLINE ERROR  AGE
openshift-worker  available true          34h
----

. Get a count of the number of worker nodes.
[source,terminal]
+
----
$ oc get nodes
----
+
[source,terminal]
----
NAME                                                STATUS   ROLES           AGE     VERSION
openshift-master-1.openshift.example.com            Ready    master          30h     v1.27.3
openshift-master-2.openshift.example.com            Ready    master          30h     v1.27.3
openshift-master-3.openshift.example.com            Ready    master          30h     v1.27.3
openshift-worker-0.openshift.example.com            Ready    worker          30h     v1.27.3
openshift-worker-1.openshift.example.com            Ready    worker          30h     v1.27.3
----

. Get the compute machine set.
+
[source,terminal]
----
$ oc get machinesets -n openshift-machine-api
----
+
[source,terminal]
----
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
...
openshift-worker-0.example.com      1         1         1       1           55m
openshift-worker-1.example.com      1         1         1       1           55m
----

. Increase the number of worker nodes by one.
+
[source,terminal]
----
$ oc scale --replicas=<num> machineset <machineset> -n openshift-machine-api
----
+
Replace `<num>` with the new number of worker nodes. Replace `<machineset>` with the name of the compute machine set from the previous step.

. Check the status of the bare metal node.
+
[source,terminal]
----
$ oc -n openshift-machine-api get bmh openshift-worker-<num>
----
+
Where `<num>` is the worker node number. The STATE changes from `ready` to `provisioning`.
+
[source,terminal]
----
NAME                    STATE             CONSUMER                          ONLINE   ERROR
openshift-worker-<num>  provisioning      openshift-worker-<num>-65tjz      true
----
+
The `provisioning` status remains until the {product-title} cluster provisions the node. This can take 30 minutes or more. After the node is provisioned, the state will change to `provisioned`.
+
[source,terminal]
----
NAME                    STATE             CONSUMER                          ONLINE   ERROR
openshift-worker-<num>  provisioned       openshift-worker-<num>-65tjz      true
----

. After provisioning completes, ensure the bare metal node is ready.
+
[source,terminal]
----
$ oc get nodes
----
+
[source,terminal]
----
NAME                                          STATUS   ROLES   AGE     VERSION
openshift-master-1.openshift.example.com      Ready    master  30h     v1.27.3
openshift-master-2.openshift.example.com      Ready    master  30h     v1.27.3
openshift-master-3.openshift.example.com      Ready    master  30h     v1.27.3
openshift-worker-0.openshift.example.com      Ready    worker  30h     v1.27.3
openshift-worker-1.openshift.example.com      Ready    worker  30h     v1.27.3
openshift-worker-<num>.openshift.example.com  Ready    worker  3m27s   v1.27.3
----
+
You can also check the kubelet.
+
[source,terminal]
----
$ ssh openshift-worker-<num>
----
+
[source,terminal]
----
[kni@openshift-worker-<num>]$ journalctl -fu kubelet
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/ipi-install-preparing-the-bare-metal-node,modules/ipi-install-replacing-a-bare-metal-control-plane-node,modules/ipi-install-preparing-to-deploy-with-virtual-media-on-the-baremetal-network,modules/ipi-install-diagnosing-duplicate-mac-address,modules/ipi-install-provisioning-the-bare-metal-node

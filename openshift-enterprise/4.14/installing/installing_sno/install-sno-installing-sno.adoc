:_mod-docs-content-type: ASSEMBLY
[id="install-sno-installing-sno"]
= Installing OpenShift on a single node
:context: install-sno-installing-sno-with-the-assisted-installer
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

You can install {sno} using the web-based Assisted Installer and a discovery ISO that you generate using the Assisted Installer. You can also install {sno} by using `coreos-installer` to generate the installation ISO.


[id="installing-sno-assisted-installer"]
== Installing {sno} using the Assisted Installer

To install {product-title} on a single node, use the web-based Assisted Installer wizard to guide you through the process and manage the installation.

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="install-sno-generating-the-discovery-iso-with-the-assisted-installer_{context}"]
= Generating the discovery ISO with the Assisted Installer

Installing {product-title} on a single node requires a discovery ISO, which the Assisted Installer can generate.

.Procedure

. On the administration host, open a browser and navigate to link:https://console.redhat.com/openshift/assisted-installer/clusters[{cluster-manager-first}].

. Click *Create Cluster* to create a new cluster.

. In the *Cluster name* field, enter a name for the cluster.

. In the *Base domain* field, enter a base domain. For example:
+
----
example.com
----
+
All DNS records must be subdomains of this base domain and include the cluster name, for example:
+
----
<cluster-name>.example.com
----
+
[NOTE]
====
You cannot change the base domain or cluster name after cluster installation.
====

. Select *Install single node OpenShift (SNO)* and complete the rest of the wizard steps. Download the discovery ISO.

. Make a note of the discovery ISO URL for installing with virtual media.

[NOTE]
=====
If you enable {VirtProductName} during this process, you must have a second local storage device of at least 50GiB for your virtual machines.
=====

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../storage/persistent_storage/persistent_storage_local/persistent-storage-using-lvms.adoc#persistent-storage-using-lvms_logical-volume-manager-storage[Persistent storage using logical volume manager storage]
* xref:../../virt/about_virt/about-virt.adoc#virt-what-you-can-do-with-virt_about-virt[What you can do with OpenShift Virtualization]

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="install-sno-installing-with-the-assisted-installer_{context}"]
= Installing {sno} with the Assisted Installer

Use the Assisted Installer to install the single-node cluster.

.Procedure

. Attach the {op-system} discovery ISO to the target host.

. Configure the boot drive order in the server BIOS settings to boot from the attached discovery ISO and then reboot the server.

. On the administration host, return to the browser. Wait for the host to appear in the list of discovered hosts. If necessary, reload the link:https://console.redhat.com/openshift/assisted-installer/clusters[*Assisted Clusters*] page and select the cluster name.

. Complete the install wizard steps. Add networking details, including a subnet from the available subnets. Add the SSH public key if necessary.

. Monitor the installation's progress. Watch the cluster events. After the installation process finishes writing the operating system image to the server's hard disk, the server restarts.

. Remove the discovery ISO, and reset the server to boot from the installation drive.
+
The server restarts several times automatically, deploying the control plane.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_sno/install-sno-installing-sno.adoc#installing-with-usb-media_install-sno-installing-sno-with-the-assisted-installer[Creating a bootable ISO image on a USB drive]

* xref:../../installing/installing_sno/install-sno-installing-sno.adoc#install-booting-from-an-iso-over-http-redfish_install-sno-installing-sno-with-the-assisted-installer[Booting from an HTTP-hosted ISO image using the Redfish API]

* xref:../../nodes/nodes/nodes-sno-worker-nodes.adoc#nodes-sno-worker-nodes[Adding worker nodes to {sno} clusters]


[id="install-sno-installing-sno-manually"]
== Installing {sno} manually

To install {product-title} on a single node, first generate the installation ISO, and then boot the server from the ISO. You can monitor the installation using the `openshift-install` installation program.

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="generating-the-install-iso-manually_{context}"]
= Generating the installation ISO with coreos-installer

Installing {product-title} on a single node requires an installation ISO, which you can generate with the following procedure.

.Prerequisites

* Install `podman`.

.Procedure

. Set the {product-title} version:
+
[source,terminal]
----
$ OCP_VERSION=<ocp_version> <1>
----
+
<1> Replace `<ocp_version>` with the current version, for example, `latest-{product-version}`

. Set the host architecture:
+
[source,terminal]
----
$ ARCH=<architecture> <1>
----
<1> Replace `<architecture>` with the target host architecture, for example, `aarch64` or `x86_64`.

. Download the {product-title} client (`oc`) and make it available for use by entering the following commands:
+
[source,terminal]
----
$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-client-linux.tar.gz -o oc.tar.gz
----
+
[source,terminal]
----
$ tar zxf oc.tar.gz
----
+
[source,terminal]
----
$ chmod +x oc
----

. Download the {product-title} installer and make it available for use by entering the following commands:
+
[source,terminal]
----
$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-install-linux.tar.gz -o openshift-install-linux.tar.gz
----
+
[source,terminal]
----
$ tar zxvf openshift-install-linux.tar.gz
----
+
[source,terminal]
----
$ chmod +x openshift-install
----

. Retrieve the {op-system} ISO URL by running the following command:
+
[source,terminal]
----
$ ISO_URL=$(./openshift-install coreos print-stream-json | grep location | grep $ARCH | grep iso | cut -d\" -f4)
----

. Download the {op-system} ISO:
+
[source,terminal]
----
$ curl -L $ISO_URL -o rhcos-live.iso
----

. Prepare the `install-config.yaml` file:
+
[source,yaml]
----
apiVersion: v1
baseDomain: <domain> <1>
compute:
- name: worker
  replicas: 0 <2>
controlPlane:
  name: master
  replicas: 1 <3>
metadata:
  name: <name> <4>
networking: <5>
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16 <6>
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
bootstrapInPlace:
  installationDisk: /dev/disk/by-id/<disk_id> <7>
pullSecret: '<pull_secret>' <8>
sshKey: |
  <ssh_key> <9>
----
<1> Add the cluster domain name.
<2> Set the `compute` replicas to `0`. This makes the control plane node schedulable.
<3> Set the `controlPlane` replicas to `1`. In conjunction with the previous `compute` setting, this setting ensures the cluster runs on a single node.
<4> Set the `metadata` name to the cluster name.
<5> Set the `networking` details. OVN-Kubernetes is the only allowed network plugin type for single-node clusters.
<6> Set the `cidr` value to match the subnet of the {sno} cluster.
<7> Set the path to the installation disk drive, for example, `/dev/disk/by-id/wwn-0x64cd98f04fde100024684cf3034da5c2`.
<8> Copy the {cluster-manager-url-pull} and add the contents to this configuration setting.
<9> Add the public SSH key from the administration host so that you can log in to the cluster after installation.

. Generate {product-title} assets by running the following commands:
+
[source,terminal]
----
$ mkdir ocp
----
+
[source,terminal]
----
$ cp install-config.yaml ocp
----
+
[source,terminal]
----
$ ./openshift-install --dir=ocp create single-node-ignition-config
----

. Embed the ignition data into the {op-system} ISO by running the following commands:
+
[source,terminal]
----
$ alias coreos-installer='podman run --privileged --pull always --rm \
        -v /dev:/dev -v /run/udev:/run/udev -v $PWD:/data \
        -w /data quay.io/coreos/coreos-installer:release'
----
+
[source,terminal]
----
$ coreos-installer iso ignition embed -fi ocp/bootstrap-in-place-for-live-iso.ign rhcos-live.iso
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* See xref:../../post_installation_configuration/enabling-cluster-capabilities.adoc[Enabling cluster capabilities] for more information about enabling cluster capabilities that were disabled prior to installation.
* See xref:../../installing/cluster-capabilities.adoc#explanation_of_capabilities_cluster-capabilities[Optional cluster capabilities in {product-title} {product-version}] for more information about the features provided by each capability.

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="install-sno-monitoring-the-installation-manually_{context}"]
= Monitoring the cluster installation using openshift-install

Use `openshift-install` to monitor the progress of the single-node cluster installation.

.Procedure

. Attach the modified {op-system} installation ISO to the target host.

. Configure the boot drive order in the server BIOS settings to boot from the attached discovery ISO and then reboot the server.

. On the administration host, monitor the installation by running the following command:
+
[source,terminal]
----
$ ./openshift-install --dir=ocp wait-for install-complete
----
+
The server restarts several times while deploying the control plane.

.Verification

* After the installation is complete, check the environment by running the following command:
+
[source,terminal]
----
$ export KUBECONFIG=ocp/auth/kubeconfig
----
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source,terminal]
----
NAME                         STATUS   ROLES           AGE     VERSION
control-plane.example.com    Ready    master,worker   10m     v1.27.3
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_sno/install-sno-installing-sno.adoc#installing-with-usb-media_install-sno-installing-sno-with-the-assisted-installer[Creating a bootable ISO image on a USB drive]

* xref:../../installing/installing_sno/install-sno-installing-sno.adoc#install-booting-from-an-iso-over-http-redfish_install-sno-installing-sno-with-the-assisted-installer[Booting from an HTTP-hosted ISO image using the Redfish API]

* xref:../../nodes/nodes/nodes-sno-worker-nodes.adoc#nodes-sno-worker-nodes[Adding worker nodes to {sno} clusters]

[id="install-sno-installing-sno-on-cloud-providers"]
== Installing {sno} on cloud providers

:leveloffset: +2

// This module is included in the following assemblies:
//
// installing/installing_sno/install-sno-preparing-to-install-sno.adoc

:_mod-docs-content-type: CONCEPT
[id="additional-requirements-for-installing-sno-on-a-cloud-provider_{context}"]
= Additional requirements for installing {sno} on a cloud provider

The documentation for installer-provisioned installation on cloud providers is based on a high availability cluster consisting of three control plane nodes. When referring to the documentation, consider the differences between the requirements for a {sno} cluster and a high availability cluster.

* A high availability cluster requires a temporary bootstrap machine, three control plane machines, and at least two compute machines. For a {sno} cluster, you need only a temporary bootstrap machine and one cloud instance for the control plane node and no worker nodes.

* The minimum resource requirements for high availability cluster installation include a control plane node with 4 vCPUs and 100GB of storage. For a {sno} cluster, you must have a minimum of 8 vCPU cores and 120GB of storage.

* The `controlPlane.replicas` setting in the `install-config.yaml` file should be set to `1`.

* The `compute.replicas` setting in the `install-config.yaml` file should be set to `0`.
This makes the control plane node schedulable.

:leveloffset!:

:leveloffset: +2

// This module is included in the following assemblies:
//
// installing/installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: REFERENCE
[id="supported-cloud-providers-for-single-node-openshift_{context}"]
= Supported cloud providers for {sno}

The following table contains a list of supported cloud providers and CPU architectures.

.Supported cloud providers
[options="header"]
|====
|Cloud provider |CPU architecture
|Amazon Web Service (AWS)|x86_64 and AArch64
|Microsoft Azure|x86_64
|Google Cloud Platform (GCP) | x86_64 and AArch64
|====

:leveloffset!:

:leveloffset: +2

// This module is included in the following assemblies:
//
// installing/installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: CONCEPT
[id="installing-sno-on-aws_{context}"]
= Installing {sno} on AWS

Installing a single-node cluster on AWS requires installer-provisioned installation using the "Installing a cluster on AWS with customizations" procedure.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_aws/installing-aws-customizations.adoc#installing-aws-customizations[Installing a cluster on AWS with customizations]


:leveloffset: +2

// This module is included in the following assemblies:
//
// installing/installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: CONCEPT
[id="installing-sno-on-azure_{context}"]
= Installing {sno} on Azure

Installing a single node cluster on Azure requires installer-provisioned installation using the "Installing a cluster on Azure with customizations" procedure.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_azure/installing-azure-customizations.adoc#installing-azure-customizations[Installing a cluster on Azure with customizations]


:leveloffset: +2

// This module is included in the following assemblies:
//
// installing/installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: CONCEPT
[id="installing-sno-on-gcp_{context}"]
= Installing {sno} on GCP

Installing a single node cluster on GCP requires installer-provisioned installation using the "Installing a cluster on GCP with customizations" procedure.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_gcp/installing-gcp-customizations.adoc#installing-gcp-customizations[Installing a cluster on GCP with customizations]


:leveloffset: +1

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-with-usb-media_{context}"]
= Creating a bootable ISO image on a USB drive

You can install software using a bootable USB drive that contains an ISO image. Booting the server with the USB drive prepares the server for the software installation.

.Procedure

. On the administration host, insert a USB drive into a USB port.

. Create a bootable USB drive, for example:
+
[source,terminal]
----
# dd if=<path_to_iso> of=<path_to_usb> status=progress
----
+
where:
+
--
<path_to_iso>:: is the relative path to the downloaded ISO file, for example, `rhcos-live.iso`.
<path_to_usb>:: is the location of the connected USB drive, for example, `/dev/sdb`.
--
+
After the ISO is copied to the USB drive, you can use the USB drive to install software on the server.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="install-booting-from-an-iso-over-http-redfish_{context}"]
= Booting from an HTTP-hosted ISO image using the Redfish API

You can provision hosts in your network using ISOs that you install using the Redfish Baseboard Management Controller (BMC) API.

.Prerequisites

. Download the installation {op-system-first} ISO.

.Procedure

. Copy the ISO file to an HTTP server accessible in your network.

. Boot the host from the hosted ISO file, for example:

.. Call the redfish API to set the hosted ISO as the `VirtualMedia` boot media by running the following command:
+
[source,terminal]
----
$ curl -k -u <bmc_username>:<bmc_password> -d '{"Image":"<hosted_iso_file>", "Inserted": true}' -H "Content-Type: application/json" -X POST <host_bmc_address>/redfish/v1/Managers/iDRAC.Embedded.1/VirtualMedia/CD/Actions/VirtualMedia.InsertMedia
----
+
Where:
+
--
<bmc_username>:<bmc_password>:: Is the username and password for the target host BMC.
<hosted_iso_file>:: Is the URL for the hosted installation ISO, for example: `http://webserver.example.com/rhcos-live-minimal.iso`. The ISO must be accessible from the target host machine.
<host_bmc_address>:: Is the BMC IP address of the target host machine.
--

.. Set the host to boot from the `VirtualMedia` device by running the following command:
+
[source,terminal]
----
$ curl -k -u <bmc_username>:<bmc_password> -X PATCH -H 'Content-Type: application/json' -d '{"Boot": {"BootSourceOverrideTarget": "Cd", "BootSourceOverrideMode": "UEFI", "BootSourceOverrideEnabled": "Once"}}' <host_bmc_address>/redfish/v1/Systems/System.Embedded.1
----

.. Reboot the host:
+
[source,terminal]
----
$ curl -k -u <bmc_username>:<bmc_password> -d '{"ResetType": "ForceRestart"}' -H 'Content-type: application/json' -X POST <host_bmc_address>/redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.Reset
----

.. Optional: If the host is powered off, you can boot it using the `{"ResetType": "On"}` switch. Run the following command:
+
[source,terminal]
----
$ curl -k -u <bmc_username>:<bmc_password> -d '{"ResetType": "On"}' -H 'Content-type: application/json' -X POST <host_bmc_address>/redfish/v1/Systems/System.Embedded.1/Actions/ComputerSystem.Reset
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_sno/install-sno-installing-sno.adoc

:_module-type: PROCEDURE
[id="create-custom-live-rhcos-iso_{context}"]
= Creating a custom live {op-system} ISO for remote server access

In some cases, you cannot attach an external disk drive to a server, however, you need to access the server remotely to provision a node.
It is recommended to enable SSH access to the server.
You can create a live {op-system} ISO with SSHd enabled and with predefined credentials so that you can access the server after it boots.

.Prerequisites

* You installed the `butane` utility.

.Procedure

. Download the `coreos-installer` binary from the `coreos-installer` image link:https://mirror.openshift.com/pub/openshift-v4/clients/coreos-installer/latest/[mirror] page.

. Download the latest live {op-system} ISO from link:https://mirror.openshift.com/pub/openshift-v4/x86_64/dependencies/rhcos/4.12/latest/[mirror.openshift.com].

. Create the `embedded.yaml` file that the `butane` utility uses to create the Ignition file:
+
[source,yaml,subs="attributes+"]
----
variant: openshift
version: {product-version}.0
metadata:
  name: sshd
  labels:
    machineconfiguration.openshift.io/role: worker
passwd:
  users:
    - name: core <1>
      ssh_authorized_keys:
        - '<ssh_key>'
----
<1> The `core` user has sudo privileges.

. Run the `butane` utility to create the Ignition file using the following command:
+
[source,terminal]
----
$ butane -pr embedded.yaml -o embedded.ign
----

. After the Ignition file is created, you can include the configuration in a new live {op-system} ISO, which is named `rhcos-sshd-{product-version}.0-x86_64-live.x86_64.iso`, with the `coreos-installer` utility:
+
[source,terminal,subs="attributes+"]
----
$ coreos-installer iso ignition embed -i embedded.ign rhcos-{product-version}.0-x86_64-live.x86_64.iso -o rhcos-sshd-{product-version}.0-x86_64-live.x86_64.iso
----

.Verification

* Check that the custom live ISO can be used to boot the server by running the following command:
+
[source,terminal,subs="attributes+"]
----
# coreos-installer iso ignition show rhcos-sshd-{product-version}.0-x86_64-live.x86_64.iso
----

+
.Example output
[source,json]
----
{
  "ignition": {
    "version": "3.2.0"
  },
  "passwd": {
    "users": [
      {
        "name": "core",
        "sshAuthorizedKeys": [
          "ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCZnG8AIzlDAhpyENpK2qKiTT8EbRWOrz7NXjRzopbPu215mocaJgjjwJjh1cYhgPhpAp6M/ttTk7I4OI7g4588Apx4bwJep6oWTU35LkY8ZxkGVPAJL8kVlTdKQviDv3XX12l4QfnDom4tm4gVbRH0gNT1wzhnLP+LKYm2Ohr9D7p9NBnAdro6k++XWgkDeijLRUTwdEyWunIdW1f8G0Mg8Y1Xzr13BUo3+8aey7HLKJMDtobkz/C8ESYA/f7HJc5FxF0XbapWWovSSDJrr9OmlL9f4TfE+cQk3s+eoKiz2bgNPRgEEwihVbGsCN4grA+RzLCAOpec+2dTJrQvFqsD alosadag@sonnelicht.local"
        ]
      }
    ]
  }
}
----

:leveloffset!:

[id="install-sno-with-ibmz"]
== Installing {sno} with {ibmzProductName} and {linuxoneProductName}

Installing a single-node cluster on {ibmzProductName} and {linuxoneProductName} requires user-provisioned installation using either the "Installing a cluster with {op-system-base} KVM on {ibmzProductName} and {linuxoneProductName}" or the "Installing a cluster with z/VM on {ibmzProductName} and {linuxoneProductName}" procedure.

[NOTE]
====
Installing a single-node cluster on {ibmzProductName} simplifies installation for development and test environments and requires less resource requirements at entry level.
====

[discrete]
=== Hardware requirements

* The equivalent of two Integrated Facilities for Linux (IFL), which are SMT2 enabled, for each cluster.
* At least one network connection to both connect to the `LoadBalancer` service and to serve data for traffic outside the cluster.

[NOTE]
====
You can use dedicated or shared IFLs to assign sufficient compute resources. Resource sharing is one of the key strengths of {ibmzProductName}. However, you must adjust capacity correctly on each hypervisor layer and ensure sufficient resources for every {product-title} cluster.
====

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_ibm_z/installing-ibm-z.adoc#installing-ibm-z[Installing a cluster with z/VM on {ibmzProductName} and {linuxoneProductName}]

* xref:../../installing/installing_ibm_z/installing-ibm-z-kvm.adoc#installing-ibm-z-kvm[Installing a cluster with {op-system-base} KVM on {ibmzProductName} and{linuxoneProductName}]

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-sno-on-ibm-z_{context}"]
= Installing {sno} with z/VM on {ibmzProductName} and {linuxoneProductName}

.Prerequisites

* You have installed `podman`.

.Procedure

. Set the {product-title} version by running the following command:
+
[source,terminal]
----
$ OCP_VERSION=<ocp_version> <1>
----
+
<1> Replace `<ocp_version>` with the current version, for example, `latest-{product-version}`

. Set the host architecture by running the following command:
+
[source,terminal]
----
$ ARCH=<architecture> <1>
----
<1> Replace `<architecture>` with the target host architecture `s390x`.

. Download the {product-title} client (`oc`) and make it available for use by entering the following commands:
+
[source,terminal]
----
$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-client-linux.tar.gz -o oc.tar.gz
----
+
[source,terminal]
----
$ tar zxf oc.tar.gz
----
+
[source,terminal]
----
$ chmod +x oc
----

. Download the {product-title} installer and make it available for use by entering the following commands:
+
[source,terminal]
----
$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-install-linux.tar.gz -o openshift-install-linux.tar.gz
----
+
[source,terminal]
----
$ tar zxvf openshift-install-linux.tar.gz
----
+
[source,terminal]
----
$ chmod +x openshift-install
----

. Prepare the `install-config.yaml` file:
+
[source,yaml]
----
apiVersion: v1
baseDomain: <domain> <1>
compute:
- name: worker
  replicas: 0 <2>
controlPlane:
  name: master
  replicas: 1 <3>
metadata:
  name: <name> <4>
networking: <5>
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16 <6>
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
bootstrapInPlace:
  installationDisk: /dev/disk/by-id/<disk_id> <7>
pullSecret: '<pull_secret>' <8>
sshKey: |
  <ssh_key> <9>
----
<1> Add the cluster domain name.
<2> Set the `compute` replicas to `0`. This makes the control plane node schedulable.
<3> Set the `controlPlane` replicas to `1`. In conjunction with the previous `compute` setting, this setting ensures the cluster runs on a single node.
<4> Set the `metadata` name to the cluster name.
<5> Set the `networking` details. OVN-Kubernetes is the only allowed network plugin type for single-node clusters.
<6> Set the `cidr` value to match the subnet of the {sno} cluster.
<7> Set the path to the installation disk drive, for example, `/dev/disk/by-id/wwn-0x64cd98f04fde100024684cf3034da5c2`.
<8> Copy the {cluster-manager-url-pull} and add the contents to this configuration setting.
<9> Add the public SSH key from the administration host so that you can log in to the cluster after installation.

. Generate {product-title} assets by running the following commands:
+
[source,terminal]
----
$ mkdir ocp
----
+
[source,terminal]
----
$ cp install-config.yaml ocp
----
+
[source,terminal]
----
$ ./openshift-install --dir=ocp create single-node-ignition-config
----

. Obtain the {op-system-base} `kernel`, `initramfs`, and `rootfs`  artifacts from the link:https://access.redhat.com/downloads/content/290[Product Downloads] page on the Red Hat Customer Portal or from the link:https://mirror.openshift.com/pub/openshift-v4/s390x/dependencies/rhcos/latest/[{op-system} image mirror] page.
+
[IMPORTANT]
====
The {op-system} images might not change with every release of {product-title}. You must download images with the highest version that is less than or equal to the {product-title} version that you install. Only use the appropriate `kernel`, `initramfs`, and `rootfs` artifacts described in the following procedure.
====
+
The file names contain the {product-title} version number. They resemble the following examples:
+
`kernel`:: `rhcos-<version>-live-kernel-<architecture>`
`initramfs`:: `rhcos-<version>-live-initramfs.<architecture>.img`
`rootfs`:: `rhcos-<version>-live-rootfs.<architecture>.img`
+
[NOTE]
====
The `rootfs` image is the same for FCP and DASD.
====

. Move the following artifacts and files to an HTTP or HTTPS server:

** Downloaded {op-system-base} live `kernel`, `initramfs`, and `rootfs` artifacts
** Ignition files

. Create parameter files for a particular virtual machine:
+
.Example parameter file
+
[source,terminal]
----
rd.neednet=1 \
console=ttysclp0 \
coreos.live.rootfs_url={rhcos_liveos}:8080/rootfs.img \// <1>
ignition.config.url={rhcos_ign}:8080/ignition/bootstrap-in-place-for-live-iso.ign \// <2>
ip=encbdd0:dhcp::02:00:00:02:34:02 <3>
rd.znet=qeth,0.0.bdd0,0.0.bdd1,0.0.bdd2,layer2=1 \
rd.dasd=0.0.4411 \// <4>
rd.zfcp=0.0.8001,0x50050763040051e3,0x4000406300000000 \// <5>
zfcp.allow_lun_scan=0 \
rd.luks.options=discard \
ignition.firstboot ignition.platform.id=metal \
console=tty1 console=ttyS1,115200n8
----
<1> For the `coreos.live.rootfs_url=` artifact, specify the matching `rootfs` artifact for the `kernel`and `initramfs` you are booting. Only HTTP and HTTPS protocols are supported.
<2> For the `ignition.config.url=` parameter, specify the Ignition file for the machine role. Only HTTP and HTTPS protocols are supported.
<3> For the `ip=` parameter, assign the IP address automatically using DHCP or manually as described in "Installing a cluster with z/VM on {ibmzProductName} and {linuxoneProductName}".
<4> For installations on DASD-type disks, use `rd.dasd=` to specify the DASD where {op-system} is to be installed. Omit this entry for FCP-type disks.
<5> For installations on FCP-type disks, use `rd.zfcp=<adapter>,<wwpn>,<lun>` to specify the FCP disk where {op-system} is to be installed. Omit this entry for DASD-type disks.
+
Leave all other parameters unchanged.

. Transfer the following artifacts, files, and images to z/VM. For example by using FTP:

** `kernel` and `initramfs` artifacts
** Parameter files
** {op-system} images
+
For details about how to transfer the files with FTP and boot from the virtual reader, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/sect-installing-zvm-s390[Installing under Z/VM].

. Punch the files to the virtual reader of the z/VM guest virtual machine that is to become your bootstrap node.

. Log in to CMS on the bootstrap machine.

. IPL the bootstrap machine from the reader by running the following command:
+
----
$ cp ipl c
----

. After the first reboot of the virtual machine, run the following commands directly after one another:

.. To boot a DASD device after first reboot, run the following commands:
+
--
[source,terminal]
----
$ cp i <devno> clear loadparm prompt
----

where:

`<devno>`:: Specifies the device number of the boot device as seen by the guest.

[source,terminal]
----
$ cp vi vmsg 0 <kernel_parameters>
----

where:

`<kernel_parameters>`:: Specifies a set of kernel parameters to be stored as system control program data (SCPDATA). When booting Linux, these kernel parameters are concatenated to the end of the existing kernel parameters that are used by your boot configuration. The combined parameter string must not exceed 896 characters.
--
.. To boot an FCP device after first reboot, run the following commands:
+
--
[source,terminal]
----
$ cp set loaddev portname <wwpn> lun <lun>
----

where:

`<wwpn>`:: Specifies the target port and `<lun>` the logical unit in hexadecimal format.

[source,terminal]
----
$ cp set loaddev bootprog <n>
----

where:

`<n>`:: Specifies the kernel to be booted.

[source,terminal]
----
$ cp set loaddev scpdata {APPEND|NEW} '<kernel_parameters>'
----

where:

`<kernel_parameters>`:: Specifies a set of kernel parameters to be stored as system control program data (SCPDATA). When booting Linux, these kernel parameters are concatenated to the end of the existing kernel parameters that are used by your boot configuration. The combined parameter string must not exceed 896 characters.

`<APPEND|NEW>`:: Optional: Specify `APPEND` to append kernel parameters to existing SCPDATA. This is the default. Specify `NEW` to replace existing SCPDATA.

.Example
[source,terminal]
----
$ cp set loaddev scpdata 'rd.zfcp=0.0.8001,0x500507630a0350a4,0x4000409D00000000
ip=encbdd0:dhcp::02:00:00:02:34:02 rd.neednet=1'
----

To start the IPL and boot process, run the following command:

[source,terminal]
----
$ cp i <devno>
----

where:

`<devno>`:: Specifies the device number of the boot device as seen by the guest.
--

:leveloffset!:

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-sno-on-ibm-z-kvm_{context}"]
= Installing {sno} with {op-system-base} KVM on {ibmzProductName} and {linuxoneProductName}

.Prerequisites

* You  have installed `podman`.

.Procedure

. Set the {product-title} version by running the following command:
+
[source,terminal]
----
$ OCP_VERSION=<ocp_version> <1>
----
+
<1> Replace `<ocp_version>` with the current version, for example, `latest-{product-version}`

. Set the host architecture by running the following command:
+
[source,terminal]
----
$ ARCH=<architecture> <1>
----
<1> Replace `<architecture>` with the target host architecture `s390x`.

. Download the {product-title} client (`oc`) and make it available for use by entering the following commands:
+
[source,terminal]
----
$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-client-linux.tar.gz -o oc.tar.gz
----
+
[source,terminal]
----
$ tar zxf oc.tar.gz
----
+
[source,terminal]
----
$ chmod +x oc
----

. Download the {product-title} installer and make it available for use by entering the following commands:
+
[source,terminal]
----
$ curl -k https://mirror.openshift.com/pub/openshift-v4/clients/ocp/$OCP_VERSION/openshift-install-linux.tar.gz -o openshift-install-linux.tar.gz
----
+
[source,terminal]
----
$ tar zxvf openshift-install-linux.tar.gz
----
+
[source,terminal]
----
$ chmod +x openshift-install
----

. Prepare the `install-config.yaml` file:
+
[source,yaml]
----
apiVersion: v1
baseDomain: <domain> <1>
compute:
- name: worker
  replicas: 0 <2>
controlPlane:
  name: master
  replicas: 1 <3>
metadata:
  name: <name> <4>
networking: <5>
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16 <6>
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
bootstrapInPlace:
  installationDisk: /dev/disk/by-id/<disk_id> <7>
pullSecret: '<pull_secret>' <8>
sshKey: |
  <ssh_key> <9>
----
<1> Add the cluster domain name.
<2> Set the `compute` replicas to `0`. This makes the control plane node schedulable.
<3> Set the `controlPlane` replicas to `1`. In conjunction with the previous `compute` setting, this setting ensures the cluster runs on a single node.
<4> Set the `metadata` name to the cluster name.
<5> Set the `networking` details. OVN-Kubernetes is the only allowed network plugin type for single-node clusters.
<6> Set the `cidr` value to match the subnet of the {sno} cluster.
<7> Set the path to the installation disk drive, for example, `/dev/disk/by-id/wwn-0x64cd98f04fde100024684cf3034da5c2`.
<8> Copy the {cluster-manager-url-pull} and add the contents to this configuration setting.
<9> Add the public SSH key from the administration host so that you can log in to the cluster after installation.

. Generate {product-title} assets by running the following commands:
+
[source,terminal]
----
$ mkdir ocp
----
+
[source,terminal]
----
$ cp install-config.yaml ocp
----
+
[source,terminal]
----
$ ./openshift-install --dir=ocp create single-node-ignition-config
----

. Obtain the {op-system-base} `kernel`, `initramfs`, and `rootfs` artifacts from the link:https://access.redhat.com/downloads/content/290[Product Downloads] page on the Red Hat Customer Portal or from the link:https://mirror.openshift.com/pub/openshift-v4/s390x/dependencies/rhcos/latest/[{op-system} image mirror] page.
+
[IMPORTANT]
====
The {op-system} images might not change with every release of {product-title}. You must download images with the highest version that is less than or equal to the {product-title} version that you install. Only use the appropriate `kernel`, `initramfs`, and `rootfs` artifacts described in the following procedure.
====
+
The file names contain the {product-title} version number. They resemble the following examples:
+
`kernel`:: `rhcos-<version>-live-kernel-<architecture>`
`initramfs`:: `rhcos-<version>-live-initramfs.<architecture>.img`
`rootfs`:: `rhcos-<version>-live-rootfs.<architecture>.img`
+
. Before you launch `virt-install`, move the following files and artifacts to an HTTP or HTTPS server:

** Downloaded {op-system-base} live `kernel`, `initramfs`, and `rootfs` artifacts
** Ignition files

. Create the KVM guest nodes by using the following components:

** {op-system-base} `kernel` and `initramfs` artifacts
** Ignition files
** The new disk image
** Adjusted parm line arguments

[source,terminal]
----
$ virt-install \
   --name {vn_name} \
   --autostart \
   --memory={memory_mb} \
   --cpu host \
   --vcpus {vcpus} \
   --location {media_location},kernel={rhcos_kernel},initrd={rhcos_initrd} \// <1>
   --disk size=100 \
   --network network={virt_network_parm} \
   --graphics none \
   --noautoconsole \
   --extra-args "ip=${IP}::${GATEWAY}:${MASK}:${VM_NAME}::none" \
   --extra-args "nameserver=${NAME_SERVER}" \
   --extra-args "ip=dhcp rd.neednet=1 ignition.platform.id=metal ignition.firstboot" \
   --extra-args "coreos.live.rootfs_url={rhcos_liveos}" \// <2>
   --extra-args "ignition.config.url={rhcos_ign}" \// <3>
   --extra-args "random.trust_cpu=on rd.luks.options=discard" \
   --extra-args "console=tty1 console=ttyS1,115200n8" \
   --wait
----
<1> For the `--location` parameter, specify the location  of the kernel/initrd on the HTTP or HTTPS server.
<2> For the `coreos.live.rootfs_url=` artifact, specify the matching `rootfs` artifact for the `kernel` and `initramfs` you are booting. Only HTTP and HTTPS protocols are supported.
<3> For the `ignition.config.url=` parameter, specify the Ignition file for the machine role. Only HTTP and HTTPS protocols are supported.

:leveloffset!:

[id="installing-sno-with-ibmpower"]
== Installing {sno} with {ibmpowerProductName}

Installing a single-node cluster on {ibmpowerProductName} requires user-provisioned installation using the "Installing a cluster with {ibmpowerProductName}" procedure.

[NOTE]
====
Installing a single-node cluster on {ibmpowerProductName} simplifies installation for development and test environments and requires less resource requirements at entry level.
====

[discrete]
=== Hardware requirements

* The equivalent of two Integrated Facilities for Linux (IFL), which are SMT2 enabled, for each cluster.
* At least one network connection to connect to the `LoadBalancer` service and to serve data for traffic outside of the cluster.

[NOTE]
====
You can use dedicated or shared IFLs to assign sufficient compute resources. Resource sharing is one of the key strengths of {ibmpowerProductName}. However, you must adjust capacity correctly on each hypervisor layer and ensure sufficient resources for every {product-title} cluster.
====

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_ibm_power/installing-ibm-power.adoc#installing-ibm-power[Installing a cluster on {ibmpowerProductName}]

:leveloffset: +2

// This module is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-up-bastion-for-sno_{context}"]
= Setting up basion for {sno} with {ibmpowerProductName}

Prior to installing {sno} on {ibmpowerProductName}, you must set up bastion. Setting up a bastion server for {sno} on {ibmpowerProductName} requires the configuration of the following services:

* PXE is used for the {sno} cluster installation. PXE requires the following services to be configured and run:
** DNS to define api, api-int, and *.apps
** DHCP service to enable PXE and assign an IP address to {sno} node
** HTTP to provide ignition and {op-system} rootfs image
** TFTP to enable PXE
* You must install `dnsmasq` to support DNS, DHCP and PXE, httpd for HTTP.

Use the following procedure to configure a bastion server that meets these requirements.

.Procedure

. Use the following command to install `grub2`, which is required to enable PXE for PowerVM:
+
[source,terminal]
----
grub2-mknetdir --net-directory=/var/lib/tftpboot
----
+
.Example `/var/lib/tftpboot/boot/grub2/grub.cfg` file
[source,terminal]
----
default=0
fallback=1
timeout=1
if [ ${net_default_mac} == fa:b0:45:27:43:20 ]; then
menuentry "CoreOS (BIOS)" {
   echo "Loading kernel"
   linux "/rhcos/kernel" ip=dhcp rd.neednet=1 ignition.platform.id=metal ignition.firstboot coreos.live.rootfs_url=http://192.168.10.5:8000/install/rootfs.img ignition.config.url=http://192.168.10.5:8000/ignition/sno.ign
   echo "Loading initrd"
   initrd  "/rhcos/initramfs.img"
}
fi
----

. Use the following commands to download {op-system} image files from the mirror repo for PXE.

.. Enter the following command to assign the `RHCOS_URL` variable the follow 4.12 URL:
+
[source,terminal]
----
$ export RHCOS_URL=https://mirror.openshift.com/pub/openshift-v4/ppc64le/dependencies/rhcos/4.12/latest/
----

.. Enter the following command to navigate to the `/var/lib/tftpboot/rhcos` directory:
+
[source,terminal]
----
$ cd /var/lib/tftpboot/rhcos
----

.. Enter the following command to download the specified {op-system} kernel file from the URL stored in the `RHCOS_URL` variable:
+
[source,terminal]
----
$ wget ${RHCOS_URL}/rhcos-live-kernel-ppc64le -o kernel
----

.. Enter the following command to download the {op-system} `initramfs` file from the URL stored in the `RHCOS_URL` variable:
+
[source,terminal]
----
$ wget ${RHCOS_URL}/rhcos-live-initramfs.ppc64le.img -o initramfs.img
----

.. Enter the following command to navigate to the `/var//var/www/html/install/` directory:
+
[source,terminal]
----
$ cd /var//var/www/html/install/
----

.. Enter the following command to download, and save, the {op-system} `root filesystem` image file from the URL stored in the `RHCOS_URL` variable:
+
[source,terminal]
----
$ wget ${RHCOS_URL}/rhcos-live-rootfs.ppc64le.img -o rootfs.img
----

. To create the ignition file for a {sno} cluster, you must create the `install-config.yaml` file.

.. Enter the following command to create the work directory that holds the file:
+
[source,terminal]
----
$ mkdir -p ~/sno-work
----

.. Enter the following command to navigate to the `~/sno-work` directory:
+
[source,terminal]
----
$ cd ~/sno-work
----

.. Use the following sample file can to create the required `install-config.yaml` in the `~/sno-work` directory:
+
[source,yaml]
----
apiVersion: v1
baseDomain: <domain> <1>
compute:
- name: worker
  replicas: 0 <2>
controlPlane:
  name: master
  replicas: 1 <3>
metadata:
  name: <name> <4>
networking: <5>
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 10.0.0.0/16 <6>
  networkType: OVNKubernetes
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
bootstrapInPlace:
  installationDisk: /dev/disk/by-id/<disk_id> <7>
pullSecret: '<pull_secret>' <8>
sshKey: |
  <ssh_key> <9>
----
<1> Add the cluster domain name.
<2> Set the `compute` replicas to `0`. This makes the control plane node schedulable.
<3> Set the `controlPlane` replicas to `1`. In conjunction with the previous `compute` setting, this setting ensures that the cluster runs on a single node.
<4> Set the `metadata` name to the cluster name.
<5> Set the `networking` details. OVN-Kubernetes is the only allowed network plugin type for single-node clusters.
<6> Set the `cidr` value to match the subnet of the {sno} cluster.
<7> Set the path to the installation disk drive, for example, `/dev/disk/by-id/wwn-0x64cd98f04fde100024684cf3034da5c2`.
<8> Copy the {cluster-manager-url-pull} and add the contents to this configuration setting.
<9> Add the public SSH key from the administration host so that you can log in to the cluster after installation.

. Download the `openshift-install` image to create the ignition file and copy it to the `http` directory.

.. Enter the following command to download the `openshift-install-linux-4.12.0` .tar file:
+
[source,terminal]
----
$ wget https://mirror.openshift.com/pub/openshift-v4/ppc64le/clients/ocp/4.12.0/openshift-install-linux-4.12.0.tar.gz
----

.. Enter the following command to unpack the `openshift-install-linux-4.12.0.tar.gz` archive:
+
[source,terminal]
----
$ tar xzvf openshift-install-linux-4.12.0.tar.gz
----

.. Enter the following command to
+
[source,terminal]
----
$ ./openshift-install --dir=~/sno-work create create single-node-ignition-config
----

.. Enter the following command to create the ignition file:
+
[source,terminal]
----
$ cp ~/sno-work/single-node-ignition-config.ign /var/www/html/ignition/sno.ign
----

.. Enter the following command to restore SELinux file for the `/var/www/html` directory:
+
[source,terminal]
----
$ restorecon -vR /var/www/html || true
----
+
Bastion now has all the required files and is properly configured in order to install {sno}.

:leveloffset!:

:leveloffset: +2

// This is included in the following assemblies:
//
// installing_sno/install-sno-installing-sno.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-sno-on-ibm-power_{context}"]
= Installing {sno} with {ibmpowerProductName}

.Prerequisites

* You have set up bastion.

.Procedure

There are two steps for the {sno} cluster installation. First the {sno} logical partition (LPAR) needs to boot up with PXE, then you need to monitor the installation progress.

. Use the following command to boot powerVM with netboot:
+
[source,terminal]
----
$ lpar_netboot -i -D -f -t ent -m <sno_mac> -s auto -d auto -S <server_ip> -C <sno_ip> -G <gateway> <lpar_name> default_profile <cec_name>
----
+
where:
+
--
sno_mac:: Specifies the MAC address of the {sno} cluster.
sno_ip:: Specifies the IP address of the {sno} cluster.
server_ip:: Specifies the IP address of bastion (PXE server).
gateway:: Specifies the Network's gateway IP.
lpar_name:: Specifies the {sno} lpar name in HMC.
cec_name:: Specifies the System name where the sno_lpar resides
--

. After the {sno} LPAR boots up with PXE, use the `openshift-install` command to monitor the progress of installation:

.. Run the following command after the bootstrap is complete:
+
[source,terminal]
----
./openshift-install wait-for bootstrap-complete
----

.. Run the following command after it returns successfully:
+
[source,terminal]
----
./openshift-install wait-for install-complete
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/install-sno-generating-the-discovery-iso-with-the-assisted-installer,modules/install-sno-installing-with-the-assisted-installer,modules/install-sno-generating-the-install-iso-manually,modules/install-sno-monitoring-the-installation-manually,modules/install-sno-additional-requirements-for-installing-sno-on-a-cloud-provider,modules/install-sno-supported-cloud-providers-for-single-node-openshift,modules/installation-aws_con_installing-sno-on-aws,modules/install-sno-installing-sno-on-azure,modules/install-sno-installing-sno-on-gcp,modules/install-sno-installing-with-usb-media,modules/install-booting-from-an-iso-over-http-redfish,modules/creating-custom-live-rhcos-iso,modules/install-sno-ibm-z,modules/install-sno-ibm-z-kvm,modules/setting-up-bastion-for-sno,modules/install-sno-ibm-power

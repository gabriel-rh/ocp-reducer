:_mod-docs-content-type: ASSEMBLY
[id="installing-with-agent-based-installer"]
= Installing an {product-title} cluster with the Agent-based Installer
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: installing-with-agent-based-installer

toc::[]

Use the following procedures to install an {product-title} cluster using the Agent-based Installer.

[id="prerequisites_installing-with-agent-based-installer"]
== Prerequisites

* You reviewed details about the xref:../../architecture/architecture-installation.adoc#architecture-installation[{product-title} installation and update] processes.
* You read the documentation on xref:../../installing/installing-preparing.adoc#installing-preparing[selecting a cluster installation method and preparing it for users].

// This anchor ID is extracted/replicated from the former installing-ocp-agent.adoc module to preserve links.
[id="installing-ocp-agent_installing-with-agent-based-installer"]
== Installing {product-title} with the Agent-based Installer

The following procedures deploy a single-node {product-title} in a disconnected environment. You can use these procedures as a basis and modify according to your requirements.

// Downloading the Agent-based Installer
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc
// * installing/installing_with_agent_based_installer/prepare-pxe-infra-agent.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-retrieve_{context}"]
= Downloading the Agent-based Installer

Use this procedure to download the Agent-based Installer and the CLI needed for your installation.

.Procedure

. Log in to the {product-title} web console using your login credentials.

. Navigate to link:https://console.redhat.com/openshift/create/datacenter[Datacenter].

. Click *Run Agent-based Installer locally*.

. Select the operating system and architecture for the *OpenShift Installer* and *Command line interface*.

. Click *Download Installer* to download and extract the install program.

. You can either download or copy the pull secret by clicking on *Download pull secret* or *Copy pull secret*.

. Click *Download command-line tools* and place the `openshift-install` binary in a directory that is on your `PATH`.

:leveloffset!:

// Creating the preferred configuration inputs
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc
// *installing/installing_with_agent_based_installer/prepare-pxe-infra-agent.adoc


:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-inputs_{context}"]
= Creating the preferred configuration inputs

Use this procedure to create the preferred configuration inputs used to create the agent image.

.Procedure

. Install `nmstate` dependency by running the following command:
+
[source,terminal]
----
$ sudo dnf install /usr/bin/nmstatectl -y
----

. Place the `openshift-install` binary in a directory that is on your PATH.

. Create a directory to store the install configuration by running the following command:
+
[source,terminal]
----
$ mkdir ~/<directory_name>
----

+
[NOTE]
====
This is the preferred method for the Agent-based installation. Using {ztp} manifests is optional.
====

. Create the `install-config.yaml` file:
+
[source,terminal]
----
$ cat << EOF > ./my-cluster/install-config.yaml
apiVersion: v1
baseDomain: test.example.com
compute:
- architecture: amd64 <1>
  hyperthreading: Enabled
  name: worker
  replicas: 0
controlPlane:
  architecture: amd64
  hyperthreading: Enabled
  name: master
  replicas: 1
metadata:
  name: sno-cluster <2>
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  machineNetwork:
  - cidr: 192.168.0.0/16
  networkType: OVNKubernetes <3>
  serviceNetwork:
  - 172.30.0.0/16
platform:
  none: {}
pullSecret: '<pull_secret>' <4>
sshKey: '<ssh_pub_key>' <5>
EOF
----
+
<1> Specify the system architecture, valid values are `amd64` and `arm64`.
<2> Required. Specify your cluster name.
<3> Specify the cluster network plugin to install. The supported values are `OVNKubernetes` and `OpenShiftSDN`. The default value is `OVNKubernetes`.
<4> Specify your pull secret.
<5> Specify your SSH public key.

+
[NOTE]
====
If you set the platform to `vSphere` or `baremetal`, you can configure IP address endpoints for cluster nodes in three ways:

* IPv4
* IPv6
* IPv4 and IPv6 in parallel (dual-stack)

IPv6 is supported only on bare metal platforms.
====
+
.Example of dual-stack networking
[source,yaml]
----
networking:
  clusterNetwork:
    - cidr: 172.21.0.0/16
      hostPrefix: 23
    - cidr: fd02::/48
      hostPrefix: 64
  machineNetwork:
    - cidr: 192.168.11.0/16
    - cidr: 2001:DB8::/32
  serviceNetwork:
    - 172.22.0.0/16
    - fd03::/112
  networkType: OVNKubernetes
platform:
  baremetal:
    apiVIPs:
    - 192.168.11.3
    - 2001:DB8::4
    ingressVIPs:
    - 192.168.11.4
    - 2001:DB8::5
----

. Create the `agent-config.yaml` file:
+
[source,terminal]
----
$ cat > agent-config.yaml << EOF
apiVersion: v1beta1
kind: AgentConfig
metadata:
  name: sno-cluster
rendezvousIP: 192.168.111.80 <1>
hosts: <2>
  - hostname: master-0 <3>
    interfaces:
      - name: eno1
        macAddress: 00:ef:44:21:e6:a5
    rootDeviceHints: <4>
      deviceName: /dev/sdb
    networkConfig: <5>
      interfaces:
        - name: eno1
          type: ethernet
          state: up
          mac-address: 00:ef:44:21:e6:a5
          ipv4:
            enabled: true
            address:
              - ip: 192.168.111.80
                prefix-length: 23
            dhcp: false
      dns-resolver:
        config:
          server:
            - 192.168.111.1
      routes:
        config:
          - destination: 0.0.0.0/0
            next-hop-address: 192.168.111.2
            next-hop-interface: eno1
            table-id: 254
EOF
----
+
<1> This IP address is used to determine which node performs the bootstrapping process as well as running the `assisted-service` component.
You must provide the rendezvous IP address when you do not specify at least one host's IP address in the `networkConfig` parameter. If this address is not provided, one IP address is selected from the provided hosts' `networkConfig`.
<2> Optional: Host configuration. The number of hosts defined must not exceed the total number of hosts defined in the `install-config.yaml` file, which is the sum of the values of the `compute.replicas` and `controlPlane.replicas` parameters.
<3> Optional: Overrides the hostname obtained from either the Dynamic Host Configuration Protocol (DHCP) or a reverse DNS lookup. Each host must have a unique hostname supplied by one of these methods.
<4> Enables provisioning of the Red Hat Enterprise Linux CoreOS (RHCOS) image to a particular device. It examines the devices in the order it discovers them, and compares the discovered values with the hint values. It uses the first discovered device that matches the hint value.
<5> Optional: Configures the network interface of a host in NMState format.



:leveloffset!:

[id="installing-ocp-agent-opt-manifests_{context}"]
=== Optional: Creating additional manifest files

You can create additional manifests to further configure your cluster beyond the configurations available in the `install-config.yaml` and `agent-config.yaml` files.

// Partitioning the disk
:leveloffset: +3

// Module included in the following assemblies:
//
// * installing/installing_bare_metal/installing-bare-metal.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_bare_metal/installing-bare-metal-network-customizations.adoc
// * installing/installing_with_agent_based_installer/installing-with-agent-based-installer.adoc

:agent:

:_mod-docs-content-type: PROCEDURE

[id="installing-ocp-agent-disk-partition_{context}"]
= Disk partitioning

In general, you should use the default disk partitioning that is created during the {op-system} installation. However, there are cases where you might want to create a separate partition for a directory that you expect to grow.

{product-title} supports the addition of a single partition to attach
storage to either the `/var` directory or a subdirectory of `/var`.
For example:

* `/var/lib/containers`: Holds container-related content that can grow
as more images and containers are added to a system.
* `/var/lib/etcd`: Holds data that you might want to keep separate for purposes such as performance optimization of etcd storage.
* `/var`: Holds data that you might want to keep separate for purposes such as auditing.
+
[IMPORTANT]
====
For disk sizes larger than 100GB, and especially larger than 1TB, create a separate `/var` partition.
====

Storing the contents of a `/var` directory separately makes it easier to grow storage for those areas as needed and reinstall {product-title} at a later date and keep that data intact. With this method, you will not have to pull all your containers again, nor will you have to copy massive log files when you update systems.

The use of a separate partition for the `/var` directory or a subdirectory of `/var` also prevents data growth in the partitioned directory from filling up the root file system.

The following procedure sets up a separate `/var` partition by adding a machine config manifest that is wrapped into the Ignition config file for a node type during the preparation phase of an installation.

.Procedure


. On your installation host, create the `openshift` subdirectory within the installation directory:
+
[source,terminal]
----
$ mkdir <installation_directory>/openshift
----

. Create a Butane config that configures the additional partition. For example, name the file `$HOME/clusterconfig/98-var-partition.bu`, change the disk device name to the name of the storage device on the `worker` systems, and set the storage size as appropriate. This example places the `/var` directory on a separate partition:
+
[source,yaml,subs="attributes+"]
----
variant: openshift
version: {product-version}.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 98-var-partition
storage:
  disks:
  - device: /dev/disk/by-id/<device_name> <1>
    partitions:
    - label: var
      start_mib: <partition_start_offset> <2>
      size_mib: <partition_size> <3>
  filesystems:
    - device: /dev/disk/by-partlabel/var
      path: /var
      format: xfs
      mount_options: [defaults, prjquota] <4>
      with_mount_unit: true
----
+
<1> The storage device name of the disk that you want to partition.
<2> When adding a data partition to the boot disk, a minimum offset value of 25000 mebibytes is recommended. The root file system is automatically resized to fill all available space up to the specified offset. If no offset value is specified, or if the specified value is smaller than the recommended minimum, the resulting root file system will be too small, and future reinstalls of {op-system} might overwrite the beginning of the data partition.
<3> The size of the data partition in mebibytes.
<4> The `prjquota` mount option must be enabled for filesystems used for container storage.
+
[NOTE]
====
When creating a separate `/var` partition, you cannot use different instance types for compute nodes, if the different instance types do not have the same device name.
====

. Create a manifest from the Butane config and save it to the `clusterconfig/openshift` directory. For example, run the following command:
+
[source,terminal]
----
$ butane $HOME/clusterconfig/98-var-partition.bu -o $HOME/clusterconfig/openshift/98-var-partition.yaml
----


:!agent:

:leveloffset!:

// Optional: Using ZTP manifests
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-ztp_{context}"]
= Optional: Using ZTP manifests

You can use {ztp-first} manifests to configure your installation beyond the options available through the `install-config.yaml` and `agent-config.yaml` files.

[NOTE]
====
{ztp} manifests can be generated with or without configuring the `install-config.yaml` and `agent-config.yaml` files beforehand.
If you chose to configure the `install-config.yaml` and `agent-config.yaml` files, the configurations will be imported to the ZTP cluster manifests when they are generated.
====

.Prerequisites

* You have placed the `openshift-install` binary in a directory that is on your `PATH`.

* Optional: You have created and configured the `install-config.yaml` and `agent-config.yaml` files.

.Procedure

. Use the following command to generate ZTP cluster manifests:
+
[source,terminal]
----
$ openshift-install agent create cluster-manifests --dir <installation_directory>
----
+
[IMPORTANT]
====
If you have created the `install-config.yaml` and `agent-config.yaml` files, those files are deleted and replaced by the cluster manifests generated through this command.

Any configurations made to the `install-config.yaml` and `agent-config.yaml` files are imported to the ZTP cluster manifests when you run the `openshift-install agent create cluster-manifests` command.
====

. Navigate to the `cluster-manifests` directory:
+
[source,terminal]
----
$ cd <installation_directory>/cluster-manifests
----

. Configure the manifest files in the `cluster-manifests` directory.
For sample files, see the "Sample GitOps ZTP custom resources" section.

. Disconnected clusters: If you did not define mirror configuration in the `install-config.yaml` file before generating the ZTP manifests, perform the following steps:

.. Navigate to the `mirror` directory:
+
[source,terminal]
----
$ cd ../mirror
----

.. Configure the manifest files in the `mirror` directory.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../../installing/installing_with_agent_based_installer/installing-with-agent-based-installer.adoc#sample-ztp-custom-resources_installing-with-agent-based-installer[Sample {ztp} custom resources].

* See xref:../../scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-clusters-at-scale.adoc#ztp-deploying-far-edge-clusters-at-scale[Challenges of the network far edge] to learn more about {ztp-first}.

// Optional: Encrypting the disk
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-encrypt_{context}"]
= Optional: Encrypting the disk

Use this procedure to encrypt your disk or partition while installing {product-title} with the Agent-based Installer.

.Prerequisites

* You have created and configured the `install-config.yaml` and `agent-config.yaml` files, unless you are using ZTP manifests.

* You have placed the `openshift-install` binary in a directory that is on your `PATH`.

.Procedure

. Use the following command to generate ZTP cluster manifests:
+
[source,terminal]
----
$ openshift-install agent create cluster-manifests --dir <installation_directory>
----
+
[IMPORTANT]
====
If you have created the `install-config.yaml` and `agent-config.yaml` files, those files are deleted and replaced by the cluster manifests generated through this command.

Any configurations made to the `install-config.yaml` and `agent-config.yaml` files are imported to the ZTP cluster manifests when you run the `openshift-install agent create cluster-manifests` command.
====
+
[NOTE]
====
If you have already generated ZTP manifests, skip this step.
====

. Navigate to the `cluster-manifests` directory:
+
[source,terminal]
----
$ cd <installation_directory>/cluster-manifests
----

. Add the following section to the `agent-cluster-install.yaml` file:
+
[source,yaml]
----
diskEncryption:
    enableOn: all <1>
    mode: tang <2>
    tangServers: "server1": "http://tang-server-1.example.com:7500" <3>
----
<1> Specify which nodes to enable disk encryption on. Valid values are 'none', 'all', 'master', and 'worker'.
<2> Specify which disk encryption mode to use. Valid values are 'tpmv2' and 'tang'.
<3> Optional: If you are using Tang, specify the Tang servers.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/install_config/installing-customizing.adoc#installation-special-config-encrypt-disk_installing-customizing[About disk encryption]

// Creating and booting the agent image
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-boot_{context}"]
= Creating and booting the agent image

Use this procedure to boot the agent image on your machines.

.Procedure

. Create the agent image by running the following command:
+
[source,terminal]
----
$ openshift-install --dir <install_directory> agent create image
----
+
NOTE: Red Hat Enterprise Linux CoreOS (RHCOS) supports multipathing on the primary disk, allowing stronger resilience to hardware failure to achieve higher host availability. Multipathing is enabled by default in the agent ISO image, with a default `/etc/multipath.conf` configuration.

. Boot the `agent.x86_64.iso` or `agent.aarch64.iso` image on the bare metal machines.

:leveloffset!:

// Verifying that the current installation host can pull release images
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-tui_{context}"]
= Verifying that the current installation host can pull release images

After you boot the agent image and network services are made available to the host, the agent console application performs a pull check to verify that the current host can retrieve release images.

If the primary pull check passes, you can quit the application to continue with the installation. If the pull check fails, the application performs additional checks, as seen in the `Additional checks` section of the TUI, to help you troubleshoot the problem. A failure for any of the additional checks is not necessarily critical as long as the primary pull check succeeds.

If there are host network configuration issues that might cause an installation to fail, you can use the console application to make adjustments to your network configurations.

[IMPORTANT]
====
If the agent console application detects host network configuration issues, the installation workflow will be halted until the user manually stops the console application and signals the intention to proceed.
====

.Procedure

. Wait for the agent console application to check whether or not the configured release image can be pulled from a registry.

. If the agent console application states that the installer connectivity checks have passed, wait for the prompt to time out to continue with the installation.
+
[NOTE]
====
You can still choose to view or change network configuration settings even if the connectivity checks have passed.

However, if you choose to interact with the agent console application rather than letting it time out, you must manually quit the TUI to proceed with the installation.
====

. If the agent console application checks have failed, which is indicated by a red icon beside the `Release image URL` pull check, use the following steps to reconfigure the host's network settings:

.. Read the `Check Errors` section of the TUI.
This section displays error messages specific to the failed checks.
+
image::agent-tui-home.png[The home screen of the agent console application  displaying check errors, indicating a failed check]

.. Select *Configure network* to launch the NetworkManager TUI.

.. Select *Edit a connection* and select the connection you want to reconfigure.

.. Edit the configuration and select *OK* to save your changes.

.. Select *Back* to return to the main screen of the NetworkManager TUI.

.. Select *Activate a Connection*.

.. Select the reconfigured network to deactivate it.

.. Select the reconfigured network again to reactivate it.

.. Select *Back* and then select *Quit* to return to the agent console application.

.. Wait at least five seconds for the continuous network checks to restart using the new network configuration.

.. If the `Release image URL` pull check succeeds and displays a green icon beside the URL, select *Quit* to exit the agent console application and continue with the installation.

:leveloffset!:

// Tracking and verifying installation progress
:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-verify_{context}"]
= Tracking and verifying installation progress

Use the following procedure to track installation progress and to verify a successful installation.

.Procedure

. Optional: To know when the bootstrap host (rendezvous host) reboots, run the following command:

+
[source,terminal]
----
$ ./openshift-install --dir <install_directory> agent wait-for bootstrap-complete \ <1>
    --log-level=info <2>
----
<1> For `<install_directory>`, specify the path to the directory where the agent ISO was generated.
<2> To view different installation details, specify `warn`, `debug`, or `error` instead of `info`.

+
.Example output
[source,terminal]
----
...................................................................
...................................................................
INFO Bootstrap configMap status is complete
INFO cluster bootstrap is complete
----
+
The command succeeds when the Kubernetes API server signals that it has been bootstrapped on the control plane machines.

. To track the progress and verify successful installation, run the following command:
+
[source,terminal]
----
$ openshift-install --dir <install_directory> agent wait-for install-complete <1>
----
<1> For `<install_directory>` directory, specify the path to the directory where the agent ISO was generated.

+
.Example output
[source,terminal]
----
...................................................................
...................................................................
INFO Cluster is installed
INFO Install complete!
INFO To access the cluster as the system:admin user when using 'oc', run
INFO     export KUBECONFIG=/home/core/installer/auth/kubeconfig
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.sno-cluster.test.example.com
----


[NOTE]
====
If you are using the optional method of {ztp} manifests, you can configure IP address endpoints for cluster nodes through the `AgentClusterInstall.yaml` file in three ways:

* IPv4
* IPv6
* IPv4 and IPv6 in parallel (dual-stack)

IPv6 is supported only on bare metal platforms.
====
.Example of dual-stack networking
[source,yaml,subs="attributes+"]
----
apiVIP: 192.168.11.3
ingressVIP: 192.168.11.4
clusterDeploymentRef:
  name: mycluster
imageSetRef:
  name: openshift-{product-version}
networking:
  clusterNetwork:
  - cidr: 172.21.0.0/16
    hostPrefix: 23
  - cidr: fd02::/48
    hostPrefix: 64
  machineNetwork:
  - cidr: 192.168.11.0/16
  - cidr: 2001:DB8::/32
  serviceNetwork:
  - 172.22.0.0/16
  - fd03::/112
  networkType: OVNKubernetes
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* See xref:../../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#modifying-install-config-for-dual-stack-network_ipi-install-installation-workflow[Deploying with dual-stack networking].
* See xref:../../installing/installing_bare_metal_ipi/ipi-install-installation-workflow.adoc#configuring-the-install-config-file_ipi-install-installation-workflow[Configuring the install-config yaml file].
* See xref:../../installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc#installation-three-node-cluster_installing-restricted-networks-bare-metal[Configuring a three-node cluster] to deploy three-node clusters in bare metal environments.
* See xref:../../installing/installing_with_agent_based_installer/preparing-to-install-with-agent-based-installer.adoc#root-device-hints_preparing-to-install-with-agent-based-installer[About root device hints].
* See link:https://nmstate.io/examples.html[NMState state examples].

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: CONCEPT
[id="sample-ztp-custom-resources_{context}"]
= Sample {ztp} custom resources

Optional: You can use {ztp-first} custom resource (CR) objects to install an {product-title} cluster with the Agent-based Installer.

You can customize the following {ztp} custom resources to specify more details about your {product-title} cluster. The following sample {ztp} custom resources are for a single-node cluster.

*agent-cluster-install.yaml*

[source,yaml,subs="attributes+"]
----
  apiVersion: extensions.hive.openshift.io/v1beta1
  kind: AgentClusterInstall
  metadata:
    name: test-agent-cluster-install
    namespace: cluster0
  spec:
    clusterDeploymentRef:
      name: ostest
    imageSetRef:
      name: openshift-{product-version}
    networking:
      clusterNetwork:
      - cidr: 10.128.0.0/14
        hostPrefix: 23
      serviceNetwork:
      - 172.30.0.0/16
    provisionRequirements:
      controlPlaneAgents: 1
      workerAgents: 0
    sshPublicKey: <YOUR_SSH_PUBLIC_KEY>
----

*cluster-deployment.yaml*

[source,yaml]
----
apiVersion: hive.openshift.io/v1
kind: ClusterDeployment
metadata:
  name: ostest
  namespace: cluster0
spec:
  baseDomain: test.metalkube.org
  clusterInstallRef:
    group: extensions.hive.openshift.io
    kind: AgentClusterInstall
    name: test-agent-cluster-install
    version: v1beta1
  clusterName: ostest
  controlPlaneConfig:
    servingCertificates: {}
  platform:
    agentBareMetal:
      agentSelector:
        matchLabels:
          bla: aaa
  pullSecretRef:
    name: pull-secret
----

*cluster-image-set.yaml*

[source,yaml,subs="attributes+"]
----
apiVersion: hive.openshift.io/v1
kind: ClusterImageSet
metadata:
  name: openshift-{product-version}
spec:
  releaseImage: registry.ci.openshift.org/ocp/release:{product-version}.0-0.nightly-2022-06-06-025509
----

*infra-env.yaml*

[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
metadata:
  name: myinfraenv
  namespace: cluster0
spec:
  clusterRef:
    name: ostest
    namespace: cluster0
  cpuArchitecture: aarch64
  pullSecretRef:
    name: pull-secret
  sshAuthorizedKey: <YOUR_SSH_PUBLIC_KEY>
  nmStateConfigLabelSelector:
    matchLabels:
      cluster0-nmstate-label-name: cluster0-nmstate-label-value
----

*nmstateconfig.yaml*

[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: NMStateConfig
metadata:
  name: master-0
  namespace: openshift-machine-api
  labels:
    cluster0-nmstate-label-name: cluster0-nmstate-label-value
spec:
  config:
    interfaces:
      - name: eth0
        type: ethernet
        state: up
        mac-address: 52:54:01:aa:aa:a1
        ipv4:
          enabled: true
          address:
            - ip: 192.168.122.2
              prefix-length: 23
          dhcp: false
    dns-resolver:
      config:
        server:
          - 192.168.122.1
    routes:
      config:
        - destination: 0.0.0.0/0
          next-hop-address: 192.168.122.1
          next-hop-interface: eth0
          table-id: 254
  interfaces:
    - name: "eth0"
      macAddress: 52:54:01:aa:aa:a1
----

**pull-secret.yaml**

[source,yaml]
----
apiVersion: v1
kind: Secret
type: kubernetes.io/dockerconfigjson
metadata:
  name: pull-secret
  namespace: cluster0
stringData:
  .dockerconfigjson: 'YOUR_PULL_SECRET'
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* See xref:../../scalability_and_performance/ztp_far_edge/ztp-deploying-far-edge-clusters-at-scale.adoc#ztp-deploying-far-edge-clusters-at-scale[Challenges of the network far edge] to learn more about {ztp-first}.


// Gathering log data from a failed Agent-based installation
:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/installing-with-agent-based-installer.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-ocp-agent-gather-log_{context}"]
= Gathering log data from a failed Agent-based installation

Use the following procedure to gather log data about a failed Agent-based installation to provide for a support case.

.Procedure

. Run the following command and collect the output:
+
[source,terminal]
----
$ ./openshift-install --dir <install_directory> agent wait-for bootstrap-complete --log-level=debug
----
+
.Example error message
[source,terminal]
----
...
ERROR Bootstrap failed to complete: : bootstrap process timed out: context deadline exceeded
----

. If the output from the previous command indicates a failure, or if the bootstrap is not progressing, run the following command on node 0 and collect the output:
+
[source,terminal]
----
$ ssh core@<node-ip> sudo /usr/local/bin/agent-gather -O > <local_tmp_path>/agent-gather.tar.xz
----
+
[NOTE]
====
You only need to gather data from node 0, but gathering this data from every node can be helpful.
====

. If the bootstrap completes and the cluster nodes reboot, run the following command and collect the output:
+
[source,terminal]
----
$ ./openshift-install --dir <install_directory> agent wait-for install-complete --log-level=debug
----

. If the output from the previous command indicates a failure, perform the following steps:

.. Export the `kubeconfig` file to your environment by running the following command:
+
[source,terminal]
----
$ export KUBECONFIG=<install_directory>/auth/kubeconfig
----

.. To gather information for debugging, run the following command:
+
[source,terminal]
----
$ oc adm must-gather
----

.. Create a compressed file from the `must-gather` directory that was just created in your working directory by running the following command:
+
[source,terminal]
----
$ tar cvaf must-gather.tar.gz <must_gather_directory>
----

. Excluding the `/auth` subdirectory, attach the installation directory used during the deployment to your support case on the link:https://access.redhat.com[Red Hat Customer Portal].

. Attach all other data gathered from this procedure to your support case.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/installing-ocp-agent-download,modules/installing-ocp-agent-inputs,modules/installation-user-infra-machines-advanced,modules/installing-ocp-agent-ZTP,modules/installing-ocp-agent-encrypt,modules/installing-ocp-agent-boot,modules/installing-ocp-agent-tui,modules/installing-ocp-agent-verify,modules/sample-ztp-custom-resources,modules/installing-ocp-agent-gather-log

:_mod-docs-content-type: ASSEMBLY
[id="preparing-to-install-with-agent-based-installer"]
= Preparing to install with the Agent-based installer
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: preparing-to-install-with-agent-based-installer

toc::[]

[id="about-the-agent-based-installer"]
== About the Agent-based Installer

The Agent-based installation method provides the flexibility to boot your on-premises servers in any way that you choose. It combines the ease of use of the Assisted Installation service with the ability to run offline, including in air-gapped environments.
Agent-based installation is a subcommand of the {product-title} installer.
It generates a bootable ISO image containing all of the information required to deploy an {product-title} cluster, with an available release image.

The configuration is in the same format as for the installer-provisioned infrastructure and user-provisioned infrastructure installation methods.
The Agent-based Installer can also optionally generate or accept Zero Touch Provisioning (ZTP) custom resources. ZTP allows you to provision new edge sites with declarative configurations of bare-metal equipment.

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_with_agent_bases_installer/preparing-to-install-with-agent-based-installer.adoc

:_mod-docs-content-type: CONCEPT
[id="understanding-agent-install_{context}"]
= Understanding Agent-based Installer
As an {product-title} user, you can leverage the advantages of the Assisted Installer hosted service in disconnected environments.

The Agent-based installation comprises a bootable ISO that contains the Assisted discovery agent and the Assisted Service. Both are required to perform the cluster installation, but the latter runs on only one of the hosts.

The `openshift-install agent create image` subcommand generates an ephemeral ISO based on the inputs that you provide. You can choose to provide inputs through the following manifests:

Preferred:

* `install-config.yaml`
* `agent-config.yaml`

or

Optional: ZTP manifests

* `cluster-manifests/cluster-deployment.yaml`
* `cluster-manifests/agent-cluster-install.yaml`
* `cluster-manifests/pull-secret.yaml`
* `cluster-manifests/infraenv.yaml`
* `cluster-manifests/cluster-image-set.yaml`
* `cluster-manifests/nmstateconfig.yaml`
* `mirror/registries.conf`
* `mirror/ca-bundle.crt`

[id="agent-based-installer-workflow"]
== Agent-based Installer workflow
One of the control plane hosts runs the Assisted Service at the start of the boot process and eventually becomes the bootstrap host. This node is called the *rendezvous host* (node 0).
The Assisted Service ensures that all the hosts meet the requirements and triggers an {product-title} cluster deployment. All the nodes have the Red Hat Enterprise Linux CoreOS (RHCOS) image written to the disk. The non-bootstrap nodes reboot and initiate a cluster deployment.
Once the nodes are rebooted, the rendezvous host reboots and joins the cluster. The bootstrapping is complete and the cluster is deployed.

.Node installation workflow
image::agent-based-installer-workflow.png[Agent-based installer workflow]

You can install a disconnected {product-title} cluster through the `openshift-install agent create image` subcommand for the following topologies:

* **A single-node {product-title} cluster (SNO)**: A node that is both a master and worker.
* **A three-node {product-title} cluster** : A compact cluster that has three master nodes that are also worker nodes.
* **Highly available {product-title} cluster (HA)**: Three master nodes with any number of worker nodes.

== Recommended resources for topologies

Recommended cluster resources for the following topologies:

.Recommended cluster resources
[options="header"]
|====
|Topology|Number of master nodes|Number of worker nodes|vCPU|Memory|Storage
|Single-node cluster|1|0|8 vCPU cores|16GB of RAM| 120GB
|Compact cluster|3|0 or 1|8 vCPU cores|16GB of RAM|120GB
|HA cluster|3|2 and above |8 vCPU cores|16GB of RAM|120GB
|====


The following platforms are supported:

* `baremetal`
* `vsphere`
* `none`
+
[IMPORTANT]
====
The `none` option requires the provision of DNS name resolution and load balancing infrastructure in your cluster. See the _Requirements for a cluster using the platform "none" option_ section for more information.
====

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../installing/installing_with_agent_based_installer/preparing-to-install-with-agent-based-installer.adoc#installation-requirements-platform-none_preparing-to-install-with-agent-based-installer[Requirements for a cluster using the platform "none" option]

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_with_agent_bases_installer/preparing-to-install-with-agent-based-installer.adoc


:_mod-docs-content-type: CONCEPT
[id="agent-installer-fips-compliance_{context}"]
= About FIPS compliance

For many {product-title} customers, regulatory readiness, or compliance, on some level is required before any systems can be put into production. That regulatory readiness can be imposed by national standards, industry standards or the organization's corporate governance framework.
Federal Information Processing Standards (FIPS) compliance is one of the most critical components required in highly secure environments to ensure that only supported cryptographic technologies are allowed on nodes.

[IMPORTANT]
====
To enable FIPS mode for your cluster, you must run the installation program from a {op-system-base-full} computer configured to operate in FIPS mode. For more information about configuring FIPS mode on RHEL, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/security_hardening/assembly_installing-the-system-in-fips-mode_security-hardening[Installing the system in FIPS mode]. When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_with_agent_bases_installer/preparing-to-install-with-agent-based-installer.adoc


:_mod-docs-content-type: PROCEDURE
[id="agent-installer-configuring-fips-compliance_{context}"]

= Configuring FIPS through the Agent-based Installer

During a cluster deployment, the Federal Information Processing Standards (FIPS) change is applied when the Red Hat Enterprise Linux CoreOS (RHCOS) machines are deployed in your cluster. For Red Hat Enterprise Linux (RHEL) machines, you must enable FIPS mode when you install the operating system on the machines that you plan to use as worker machines.

You can enable FIPS mode through the preferred method of `install-config.yaml` and `agent-config.yaml`:

. You must set value of the `fips` field to `True` in the `install-config.yaml` file:
+
.Sample install-config.yaml.file

[source,yaml]
----
apiVersion: v1
baseDomain: test.example.com
metadata:
  name: sno-cluster
fips: True
----

. Optional: If you are using the {ztp} manifests, you must set the value of `fips` as `True` in the `Agent-install.openshift.io/install-config-overrides` field in the `agent-cluster-install.yaml` file:

+
.Sample agent-cluster-install.yaml file
[source,yaml]
----
apiVersion: extensions.hive.openshift.io/v1beta1
kind: AgentClusterInstall
metadata:
  annotations:
    agent-install.openshift.io/install-config-overrides: '{"fips": True}'
  name: sno-cluster
  namespace: sno-cluster-test
----

:leveloffset!:

[discrete]
[role="_additional-resources"]
.Additional resources

* link:https://access.redhat.com/articles/5059881[OpenShift Security Guide Book]

* xref:../../installing/installing-fips.adoc#installing-fips[Support for FIPS cryptography]

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/preparing-to-install-with-agent-based-installer.adoc

:_mod-docs-content-type: CONCEPT
[id="agent-install-networking_{context}"]
= About networking

The *rendezvous IP* must be known at the time of generating the agent ISO, so that during the initial boot all the hosts can check in to the assisted service.
If the IP addresses are assigned using a Dynamic Host Configuration Protocol (DHCP) server, then the `rendezvousIP` field must be set to an IP address of one of the hosts that will become part of the deployed control plane.
In an environment without a DHCP server, you can define IP addresses statically.

In addition to static IP addresses, you can apply any network configuration that is in NMState format. This includes VLANs and NIC bonds.

== DHCP

.Preferred method: `install-config.yaml` and `agent-config.yaml`

You must specify the value for the `rendezvousIP` field. The `networkConfig` fields can be left blank:

.Sample agent-config.yaml.file

[source,yaml]
----
apiVersion: v1alpha1
kind: AgentConfig
metadata:
  name: sno-cluster
rendezvousIP: 192.168.111.80 <1>
----
<1> The IP address for the rendezvous host.

== Static networking

.. Preferred method: `install-config.yaml` and `agent-config.yaml`

+
.Sample agent-config.yaml.file
+
[source,yaml]
----
  cat > agent-config.yaml << EOF
  apiVersion: v1alpha1
  kind: AgentConfig
  metadata:
    name: sno-cluster
  rendezvousIP: 192.168.111.80 <1>
  hosts:
    - hostname: master-0
      interfaces:
        - name: eno1
          macAddress: 00:ef:44:21:e6:a5 <2>
      networkConfig:
        interfaces:
          - name: eno1
            type: ethernet
            state: up
            mac-address: 00:ef:44:21:e6:a5
            ipv4:
              enabled: true
              address:
                - ip: 192.168.111.80 <3>
                  prefix-length: 23 <4>
              dhcp: false
        dns-resolver:
          config:
            server:
              - 192.168.111.1 <5>
        routes:
          config:
            - destination: 0.0.0.0/0
              next-hop-address: 192.168.111.1 <6>
              next-hop-interface: eno1
              table-id: 254
  EOF
----
<1> If a value is not specified for the `rendezvousIP` field, one address will be chosen from the static IP addresses specified in the `networkConfig` fields.
<2> The MAC address of an interface on the host, used to determine which host to apply the configuration to.
<3> The static IP address of the target bare metal host.
<4> The static IP address’s subnet prefix for the target bare metal host.
<5> The DNS server for the target bare metal host.
<6> Next hop address for the node traffic. This must be in the same subnet as the IP address set for the specified interface.

+
.. Optional method: {ztp} manifests

+
The optional method of the {ztp} custom resources comprises 6 custom resources; you can configure static IPs in the `nmstateconfig.yaml` file.

+
[source,yaml]
----
apiVersion: agent-install.openshift.io/v1beta1
kind: NMStateConfig
metadata:
  name: master-0
  namespace: openshift-machine-api
  labels:
    cluster0-nmstate-label-name: cluster0-nmstate-label-value
spec:
  config:
    interfaces:
      - name: eth0
        type: ethernet
        state: up
        mac-address: 52:54:01:aa:aa:a1
        ipv4:
          enabled: true
          address:
            - ip: 192.168.122.2 <1>
              prefix-length: 23 <2>
          dhcp: false
    dns-resolver:
      config:
        server:
          - 192.168.122.1 <3>
    routes:
      config:
        - destination: 0.0.0.0/0
          next-hop-address: 192.168.122.1 <4>
          next-hop-interface: eth0
          table-id: 254
  interfaces:
    - name: eth0
      macAddress: 52:54:01:aa:aa:a1 <5>
----
<1> The static IP address of the target bare metal host.
<2> The static IP address’s subnet prefix for the target bare metal host.
<3> The DNS server for the target bare metal host.
<4> Next hop address for the node traffic. This must be in the same subnet as the IP address set for the specified interface.
<5> The MAC address of an interface on the host, used to determine which host to apply the configuration to.

The rendezvous IP is chosen from the static IP addresses specified in the `config` fields.

:leveloffset!:

[id="installation-requirements-platform-none_{context}"]
== Requirements for a cluster using the platform "none" option

This section describes the requirements for an Agent-based {product-title} installation that is configured to use the platform `none` option.

:leveloffset: +2

:_mod-docs-content-type: CONCEPT
[id="agent-install-dns-none_{context}"]
= Platform "none" DNS requirements

In {product-title} deployments, DNS name resolution is required for the following components:

* The Kubernetes API
* The {product-title} application wildcard
* The control plane and compute machines

Reverse DNS resolution is also required for the Kubernetes API, the control plane machines, and the compute machines.

DNS A/AAAA or CNAME records are used for name resolution and PTR records are used for reverse name resolution. The reverse records are important because {op-system-first} uses the reverse records to set the hostnames for all the nodes, unless the hostnames are provided by DHCP. Additionally, the reverse records are used to generate the certificate signing requests (CSR) that {product-title} needs to operate.

[NOTE]
====
It is recommended to use a DHCP server to provide the hostnames to each cluster node.
====

The following DNS records are required for an {product-title} cluster using the platform `none` option and they must be in place before installation. In each record, `<cluster_name>` is the cluster name and `<base_domain>` is the base domain that you specify in the `install-config.yaml` file. A complete DNS record takes the form: `<component>.<cluster_name>.<base_domain>.`.

.Required DNS records
[cols="1a,3a,5a",options="header"]
|===

|Component
|Record
|Description

.2+a|Kubernetes API
|`api.<cluster_name>.<base_domain>.`
|A DNS A/AAAA or CNAME record, and a DNS PTR record, to identify the API load balancer. These records must be resolvable by both clients external to the cluster and from all the nodes within the cluster.

|`api-int.<cluster_name>.<base_domain>.`
|A DNS A/AAAA or CNAME record, and a DNS PTR record, to internally identify the API load balancer. These records must be resolvable from all the nodes within the cluster.
[IMPORTANT]
====
The API server must be able to resolve the worker nodes by the hostnames
that are recorded in Kubernetes. If the API server cannot resolve the node
names, then proxied API calls can fail, and you cannot retrieve logs from pods.
====

|Routes
|`*.apps.<cluster_name>.<base_domain>.`
|A wildcard DNS A/AAAA or CNAME record that refers to the application ingress load balancer. The application ingress load balancer targets the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default. These records must be resolvable by both clients external to the cluster and from all the nodes within the cluster.

For example, `console-openshift-console.apps.<cluster_name>.<base_domain>` is used as a wildcard route to the {product-title} console.

|Control plane machines
|`<master><n>.<cluster_name>.<base_domain>.`
|DNS A/AAAA or CNAME records and DNS PTR records to identify each machine
for the control plane nodes. These records must be resolvable by the nodes within the cluster.

|Compute machines
|`<worker><n>.<cluster_name>.<base_domain>.`
|DNS A/AAAA or CNAME records and DNS PTR records to identify each machine
for the worker nodes. These records must be resolvable by the nodes within the cluster.

|===

[NOTE]
====
In {product-title} 4.4 and later, you do not need to specify etcd host and SRV records in your DNS configuration.
====

[TIP]
====
You can use the `dig` command to verify name and reverse name resolution.
====

[id="agent-install-dns-none-example_{context}"]
== Example DNS configuration for platform "none" clusters

This section provides A and PTR record configuration samples that meet the DNS requirements for deploying {product-title} using the platform `none` option. The samples are not meant to provide advice for choosing one DNS solution over another.

In the examples, the cluster name is `ocp4` and the base domain is `example.com`.

.Example DNS A record configuration for a platform "none" cluster

The following example is a BIND zone file that shows sample A records for name resolution in a cluster using the platform `none` option.

.Sample DNS zone database
[%collapsible]
====
[source,text]
----
$TTL 1W
@	IN	SOA	ns1.example.com.	root (
			2019070700	; serial
			3H		; refresh (3 hours)
			30M		; retry (30 minutes)
			2W		; expiry (2 weeks)
			1W )		; minimum (1 week)
	IN	NS	ns1.example.com.
	IN	MX 10	smtp.example.com.
;
;
ns1.example.com.		IN	A	192.168.1.5
smtp.example.com.		IN	A	192.168.1.5
;
helper.example.com.		IN	A	192.168.1.5
helper.ocp4.example.com.	IN	A	192.168.1.5
;
api.ocp4.example.com.		IN	A	192.168.1.5 <1>
api-int.ocp4.example.com.	IN	A	192.168.1.5 <2>
;
*.apps.ocp4.example.com.	IN	A	192.168.1.5 <3>
;
master0.ocp4.example.com.	IN	A	192.168.1.97 <4>
master1.ocp4.example.com.	IN	A	192.168.1.98 <4>
master2.ocp4.example.com.	IN	A	192.168.1.99 <4>
;
worker0.ocp4.example.com.	IN	A	192.168.1.11 <5>
worker1.ocp4.example.com.	IN	A	192.168.1.7 <5>
;
;EOF
----

<1> Provides name resolution for the Kubernetes API. The record refers to the IP address of the API load balancer.
<2> Provides name resolution for the Kubernetes API. The record refers to the IP address of the API load balancer and is used for internal cluster communications.
<3> Provides name resolution for the wildcard routes. The record refers to the IP address of the application ingress load balancer. The application ingress load balancer targets the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default.
+
[NOTE]
=====
In the example, the same load balancer is used for the Kubernetes API and application ingress traffic. In production scenarios, you can deploy the API and application ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.
=====
+
<4> Provides name resolution for the control plane machines.
<5> Provides name resolution for the compute machines.
====

.Example DNS PTR record configuration for a platform "none" cluster

The following example BIND zone file shows sample PTR records for reverse name resolution in a cluster using the platform `none` option.

.Sample DNS zone database for reverse records
[%collapsible]
====
[source,text]
----
$TTL 1W
@	IN	SOA	ns1.example.com.	root (
			2019070700	; serial
			3H		; refresh (3 hours)
			30M		; retry (30 minutes)
			2W		; expiry (2 weeks)
			1W )		; minimum (1 week)
	IN	NS	ns1.example.com.
;
5.1.168.192.in-addr.arpa.	IN	PTR	api.ocp4.example.com. <1>
5.1.168.192.in-addr.arpa.	IN	PTR	api-int.ocp4.example.com. <2>
;
97.1.168.192.in-addr.arpa.	IN	PTR	master0.ocp4.example.com. <3>
98.1.168.192.in-addr.arpa.	IN	PTR	master1.ocp4.example.com. <3>
99.1.168.192.in-addr.arpa.	IN	PTR	master2.ocp4.example.com. <3>
;
11.1.168.192.in-addr.arpa.	IN	PTR	worker0.ocp4.example.com. <4>
7.1.168.192.in-addr.arpa.	IN	PTR	worker1.ocp4.example.com. <4>
;
;EOF
----

<1> Provides reverse DNS resolution for the Kubernetes API. The PTR record refers to the record name of the API load balancer.
<2> Provides reverse DNS resolution for the Kubernetes API. The PTR record refers to the record name of the API load balancer and is used for internal cluster communications.
<3> Provides reverse DNS resolution for the control plane machines.
<4> Provides reverse DNS resolution for the compute machines.
====

[NOTE]
====
A PTR record is not required for the {product-title} application wildcard.
====

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: CONCEPT
[id="agent-install-load-balancing-none_{context}"]
= Platform "none" Load balancing requirements


Before you install {product-title}, you must provision the API and application Ingress load balancing infrastructure. In production scenarios, you can deploy the API and application Ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.

[NOTE]
====
These requirements do not apply to single-node OpenShift clusters using the platform `none` option.
====

[NOTE]
====
If you want to deploy the API and application Ingress load balancers with a {op-system-base-full} instance, you must purchase the {op-system-base} subscription separately.
====

The load balancing infrastructure must meet the following requirements:

. *API load balancer*: Provides a common endpoint for users, both human and machine, to interact with and configure the platform. Configure the following conditions:
+
--
  ** Layer 4 load balancing only. This can be referred to as Raw TCP, SSL Passthrough, or SSL Bridge mode. If you use SSL Bridge mode, you must enable Server Name Indication (SNI) for the API routes.
  ** A stateless load balancing algorithm. The options vary based on the load balancer implementation.
--
+
[IMPORTANT]
====
Do not configure session persistence for an API load balancer.
====
+
Configure the following ports on both the front and back of the load balancers:
+
.API load balancer
[cols="2,5,^2,^2,2",options="header"]
|===

|Port
|Back-end machines (pool members)
|Internal
|External
|Description

|`6443`
|Control plane. You must configure the `/readyz` endpoint for the API server health check probe.
|X
|X
|Kubernetes API server

|`22623`
|Control plane.
|X
|
|Machine config server

|===
+
[NOTE]
====
The load balancer must be configured to take a maximum of 30 seconds from the
time the API server turns off the `/readyz` endpoint to the removal of the API
server instance from the pool. Within the time frame after `/readyz` returns an
error or becomes healthy, the endpoint must have been removed or added. Probing
every 5 or 10 seconds, with two successful requests to become healthy and three
to become unhealthy, are well-tested values.
====
+
. *Application Ingress load balancer*: Provides an ingress point for application traffic flowing in from outside the cluster. A working configuration for the Ingress router is required for an {product-title} cluster.
+
Configure the following conditions:
+
--
  ** Layer 4 load balancing only. This can be referred to as Raw TCP, SSL Passthrough, or SSL Bridge mode. If you use SSL Bridge mode, you must enable Server Name Indication (SNI) for the ingress routes.
  ** A connection-based or session-based persistence is recommended, based on the options available and types of applications that will be hosted on the platform.
--
+
[TIP]
====
If the true IP address of the client can be seen by the application Ingress load balancer, enabling source IP-based session persistence can improve performance for applications that use end-to-end TLS encryption.
====
+
Configure the following ports on both the front and back of the load balancers:
+
.Application Ingress load balancer
[cols="2,5,^2,^2,2",options="header"]
|===

|Port
|Back-end machines (pool members)
|Internal
|External
|Description

|`443`
|The machines that run the Ingress Controller pods, compute, or worker, by default.
|X
|X
|HTTPS traffic

|`80`
|The machines that run the Ingress Controller pods, compute, or worker, by default.
|X
|X
|HTTP traffic

|===
+
[NOTE]
====
If you are deploying a three-node cluster with zero compute nodes, the Ingress Controller pods run on the control plane nodes. In three-node cluster deployments, you must configure your application Ingress load balancer to route HTTP and HTTPS traffic to the control plane nodes.
====

[id="agent-install-load-balancing-none-example_{context}"]
== Example load balancer configuration for platform "none" clusters

This section provides an example API and application Ingress load balancer configuration that meets the load balancing requirements for clusters using the platform `none` option. The sample is an `/etc/haproxy/haproxy.cfg` configuration for an HAProxy load balancer. The example is not meant to provide advice for choosing one load balancing solution over another.

In the example, the same load balancer is used for the Kubernetes API and application ingress traffic. In production scenarios, you can deploy the API and application ingress load balancers separately so that you can scale the load balancer infrastructure for each in isolation.

[NOTE]
====
If you are using HAProxy as a load balancer and SELinux is set to `enforcing`, you must ensure that the HAProxy service can bind to the configured TCP port by running `setsebool -P haproxy_connect_any=1`.
====

.Sample API and application Ingress load balancer configuration
[%collapsible]
====
[source,text]
----
global
  log         127.0.0.1 local2
  pidfile     /var/run/haproxy.pid
  maxconn     4000
  daemon
defaults
  mode                    http
  log                     global
  option                  dontlognull
  option http-server-close
  option                  redispatch
  retries                 3
  timeout http-request    10s
  timeout queue           1m
  timeout connect         10s
  timeout client          1m
  timeout server          1m
  timeout http-keep-alive 10s
  timeout check           10s
  maxconn                 3000
listen api-server-6443 <1>
  bind *:6443
  mode tcp
  server master0 master0.ocp4.example.com:6443 check inter 1s
  server master1 master1.ocp4.example.com:6443 check inter 1s
  server master2 master2.ocp4.example.com:6443 check inter 1s
listen machine-config-server-22623 <2>
  bind *:22623
  mode tcp
  server master0 master0.ocp4.example.com:22623 check inter 1s
  server master1 master1.ocp4.example.com:22623 check inter 1s
  server master2 master2.ocp4.example.com:22623 check inter 1s
listen ingress-router-443 <3>
  bind *:443
  mode tcp
  balance source
  server worker0 worker0.ocp4.example.com:443 check inter 1s
  server worker1 worker1.ocp4.example.com:443 check inter 1s
listen ingress-router-80 <4>
  bind *:80
  mode tcp
  balance source
  server worker0 worker0.ocp4.example.com:80 check inter 1s
  server worker1 worker1.ocp4.example.com:80 check inter 1s
----

<1> Port `6443` handles the Kubernetes API traffic and points to the control plane machines.
<2> Port `22623` handles the machine config server traffic and points to the control plane machines.
<3> Port `443` handles the HTTPS traffic and points to the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default.
<4> Port `80` handles the HTTP traffic and points to the machines that run the Ingress Controller pods. The Ingress Controller pods run on the compute machines by default.
+
[NOTE]
=====
If you are deploying a three-node cluster with zero compute nodes, the Ingress Controller pods run on the control plane nodes. In three-node cluster deployments, you must configure your application Ingress load balancer to route HTTP and HTTPS traffic to the control plane nodes.
=====
====

[TIP]
====
If you are using HAProxy as a load balancer, you can check that the `haproxy` process is listening on ports `6443`, `22623`, `443`, and `80` by running `netstat -nltupe` on the HAProxy node.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/preparing-to-install-with-agent-based-installer.adoc

:_mod-docs-content-type: REFERENCE
[id="agent-install-sample-config-bonds-vlans_{context}"]
= Example: Bonds and VLAN interface node network configuration

The following `agent-config.yaml` file is an example of a manifest for bond and VLAN interfaces.

[source,yaml]
----
  apiVersion: v1alpha1
  kind: AgentConfig
  rendezvousIP: 10.10.10.14
  hosts:
    - hostname: master0
      role: master
      interfaces:
       - name: enp0s4
         macAddress: 00:21:50:90:c0:10
       - name: enp0s5
         macAddress: 00:21:50:90:c0:20
      networkConfig:
        interfaces:
          - name: bond0.300 <1>
            type: vlan <2>
            state: up
            vlan:
              base-iface: bond0
              id: 300
            ipv4:
              enabled: true
              address:
                - ip: 10.10.10.14
                  prefix-length: 24
              dhcp: false
          - name: bond0 <1>
            type: bond <3>
            state: up
            mac-address: 00:21:50:90:c0:10 <4>
            ipv4:
              enabled: false
            ipv6:
              enabled: false
            link-aggregation:
              mode: active-backup <5>
              options:
                miimon: "150" <6>
              port:
               - enp0s4
               - enp0s5
        dns-resolver: <7>
          config:
            server:
              - 10.10.10.11
              - 10.10.10.12
        routes:
          config:
            - destination: 0.0.0.0/0
              next-hop-address: 10.10.10.10 <8>
              next-hop-interface: bond0.300 <9>
              table-id: 254
----
<1> Name of the interface.
<2> The type of interface. This example creates a VLAN.
<3> The type of interface. This example creates a bond.
<4> The mac address of the interface.
<5> The `mode` attribute specifies the bonding mode.
<6> Specifies the MII link monitoring frequency in milliseconds. This example inspects the bond link every 150 milliseconds.
<7> Optional: Specifies the search and server settings for the DNS server.
<8> Next hop address for the node traffic. This must be in the same subnet as the IP address set for the specified interface.
<9> Next hop interface for the node traffic.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-with-agent-based-installer/preparing-to-install-with-agent-based-installer.adoc

:_mod-docs-content-type: REFERENCE
[id="agent-install-sample-config-bond-sriov_{context}"]
= Example: Bonds and SR-IOV dual-nic node network configuration

:FeatureName: Support for Day 1 operations associated with enabling NIC partitioning for SR-IOV devices
:leveloffset: +1

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 1

The following `agent-config.yaml` file is an example of a manifest for dual port NIC with a bond and SR-IOV interfaces:

[source,yaml]
----
apiVersion: v1alpha1
kind: AgentConfig
rendezvousIP: 10.10.10.14
hosts:
  - hostname: worker-1
    interfaces:
      - name: eno1
        macAddress: 0c:42:a1:55:f3:06
      - name: eno2
        macAddress: 0c:42:a1:55:f3:07
    networkConfig: <1>
      interfaces: <2>
        - name: eno1 <3>
          type: ethernet <4>
          state: up
          mac-address: 0c:42:a1:55:f3:06
          ipv4:
            enabled: true
            dhcp: false <5>
          ethernet:
            sr-iov:
              total-vfs: 2 <6>
          ipv6:
            enabled: false
        - name: sriov:eno1:0
          type: ethernet
          state: up <7>
          ipv4:
            enabled: false <8>
          ipv6:
            enabled: false
            dhcp: false
        - name: sriov:eno1:1
          type: ethernet
          state: down
        - name: eno2
          type: ethernet
          state: up
          mac-address: 0c:42:a1:55:f3:07
          ipv4:
            enabled: true
          ethernet:
            sr-iov:
              total-vfs: 2
          ipv6:
            enabled: false
        - name: sriov:eno2:0
          type: ethernet
          state: up
          ipv4:
            enabled: false
          ipv6:
            enabled: false
        - name: sriov:eno2:1
          type: ethernet
          state: down
        - name: bond0
          type: bond
          state: up
          min-tx-rate: 100 <9>
          max-tx-rate: 200 <10>
          link-aggregation:
            mode: active-backup <11>
            options:
              primary: sriov:eno1:0 <12>
            port:
              - sriov:eno1:0
              - sriov:eno2:0
          ipv4:
            address:
              - ip: 10.19.16.57 <13>
                prefix-length: 23
            dhcp: false
            enabled: true
          ipv6:
            enabled: false
          dns-resolver:
            config:
              server:
                - 10.11.5.160
                - 10.2.70.215
          routes:
            config:
              - destination: 0.0.0.0/0
                next-hop-address: 10.19.17.254
                next-hop-interface: bond0 <14>
                table-id: 254
----
<1> The `networkConfig` field contains information about the network configuration of the host, with subfields including `interfaces`,`dns-resolver`, and `routes`.
<2> The `interfaces` field is an array of network interfaces defined for the host.
<3> The name of the interface.
<4> The type of interface. This example creates an ethernet interface.
<5> Set this to `false` to disable DHCP for the physical function (PF) if it is not strictly required.
<6> Set this to the number of SR-IOV virtual functions (VFs) to instantiate.
<7> Set this to `up`.
<8> Set this to `false` to disable IPv4 addressing for the VF attached to the bond.
<9> Sets a minimum transmission rate, in Mbps, for the VF. This sample value sets a rate of 100 Mbps.
    * This value must be less than or equal to the maximum transmission rate.
    * Intel NICs do not support the `min-tx-rate` parameter. For more information, see link:https://bugzilla.redhat.com/show_bug.cgi?id=1772847[*BZ#1772847*].
<10> Sets a maximum transmission rate, in Mbps, for the VF. This sample value sets a rate of 200 Mbps.
<11> Sets the desired bond mode.
<12> Sets the preferred port of the bonding interface. The primary device is the first of the bonding interfaces to be used and is not abandoned unless it fails. This setting is particularly useful when one NIC in the bonding interface is faster and, therefore, able to handle a bigger load. This setting is only valid when the bonding interface is in `active-backup` mode (mode 1) and `balance-tlb` (mode 5).
<13> Sets a static IP address for the bond interface. This is the node IP address.
<14> Sets `bond0` as the gateway for the default route.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_and_managing_networking/configuring-network-bonding_configuring-and-managing-networking[Configuring network bonding]

:leveloffset: +1

// Module included in the following assemblies:

// * installing/installing_with_agent_based_installer/preparing-to-install-with-agent-based-installer.adoc
// Re-used content from Sample install-config.yaml file for bare metal without conditionals

:_mod-docs-content-type: CONCEPT
[id="installation-bare-metal-agent-installer-config-yaml_{context}"]
= Sample install-config.yaml file for bare metal

You can customize the `install-config.yaml` file to specify more details about your {product-title} cluster's platform or modify the values of the required parameters.

[source,yaml]
----
apiVersion: v1
baseDomain: example.com <1>
compute: <2>
- name: worker
  replicas: 0 <3>
controlPlane: <2>
  name: master
  replicas: 1 <4>
metadata:
  name: sno-cluster <5>
networking:
  clusterNetwork:
  - cidr: 10.128.0.0/14 <6>
    hostPrefix: 23 <7>
  networkType: OVNKubernetes <8>
  serviceNetwork: <9>
  - 172.30.0.0/16
platform:
  none: {} <10>
fips: false <11>
pullSecret: '{"auths": ...}' <12>
sshKey: 'ssh-ed25519 AAAA...' <13>
----
<1> The base domain of the cluster. All DNS records must be sub-domains of this base and include the cluster name.
<2> The `controlPlane` section is a single mapping, but the `compute` section is a sequence of mappings. To meet the requirements of the different data structures, the first line of the `compute` section must begin with a hyphen, `-`, and the first line of the `controlPlane` section must not. Only one control plane pool is used.
<3> This parameter controls the number of compute machines that the Agent-based installation waits to discover before triggering the installation process. It is the number of compute machines that must be booted with the generated ISO.

+
[NOTE]
====
If you are installing a three-node cluster, do not deploy any compute machines when you install the {op-system-first} machines.
====
+
<4> The number of control plane machines that you add to the cluster. Because the cluster uses these values as the number of etcd endpoints in the cluster, the value must match the number of control plane machines that you deploy.
<5> The cluster name that you specified in your DNS records.
<6> A block of IP addresses from which pod IP addresses are allocated. This block must not overlap with existing physical networks. These IP addresses are used for the pod network. If you need to access the pods from an external network, you must configure load balancers and routers to manage the traffic.
+
[NOTE]
====
Class E CIDR range is reserved for a future use. To use the Class E CIDR range, you must ensure your networking environment accepts the IP addresses within the Class E CIDR range.
====
+
<7> The subnet prefix length to assign to each individual node. For example, if `hostPrefix` is set to `23`, then each node is assigned a `/23` subnet out of the given `cidr`, which allows for 510 (2^(32 - 23) - 2) pod IP addresses. If you are required to provide access to nodes from an external network, configure load balancers and routers to manage the traffic.
<8> The cluster network plugin to install. The supported values are `OVNKubernetes` (default value) and `OpenShiftSDN`.
<9> The IP address pool to use for service IP addresses. You can enter only one IP address pool. This block must not overlap with existing physical networks. If you need to access the services from an external network, configure load balancers and routers to manage the traffic.
<10> You must set the platform to `none` for a single-node cluster. You can set the platform to `vsphere`, `baremetal`, or `none` for multi-node clusters.
+
[NOTE]
====
If you set the platform to `vsphere` or `baremetal`, you can configure IP address endpoints for cluster nodes in three ways:

* IPv4
* IPv6
* IPv4 and IPv6 in parallel (dual-stack)

.Example of dual-stack networking
[source,yaml]
----
networking:
  clusterNetwork:
    - cidr: 172.21.0.0/16
      hostPrefix: 23
    - cidr: fd02::/48
      hostPrefix: 64
  machineNetwork:
    - cidr: 192.168.11.0/16
    - cidr: 2001:DB8::/32
  serviceNetwork:
    - 172.22.0.0/16
    - fd03::/112
  networkType: OVNKubernetes
platform:
  baremetal:
    apiVIPs:
    - 192.168.11.3
    - 2001:DB8::4
    ingressVIPs:
    - 192.168.11.4
    - 2001:DB8::5
----
====
<11> Whether to enable or disable FIPS mode. By default, FIPS mode is not enabled. If FIPS mode is enabled, the {op-system-first} machines that {product-title} runs on bypass the default Kubernetes cryptography suite and use the cryptography modules that are provided with {op-system} instead.
+
[IMPORTANT]
====
When running {op-system-base-full} or {op-system-first} booted in FIPS mode, {product-title} core components use the {op-system-base} cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.
====

<12> This pull secret allows you to authenticate with the services that are provided by the included authorities, including Quay.io, which serves the container images for {product-title} components.
<13> The SSH public key for the `core` user in {op-system-first}.
+
[NOTE]
====
For production {product-title} clusters on which you want to perform installation debugging or disaster recovery, specify an SSH key that your `ssh-agent` process uses.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing-with-agent/installing-with-agent.adoc

:_mod-docs-content-type: REFERENCE
[id="validations-before-agent-iso-creation_{context}"]
= Validation checks before agent ISO creation

The Agent-based Installer performs validation checks on user defined YAML files before the ISO is created. Once the validations are successful, the agent ISO
is created.

.`install-config.yaml`

* `baremetal`, `vsphere` and `none` platforms are supported.
* If `none` is used as a platform, the number of control plane replicas must be `1` and the total number of worker replicas must be `0`.
* The `networkType` parameter must be `OVNKubernetes` in the case of `none` platform.
* `apiVIPs` and `ingressVIPs` parameters must be set for bare metal and vSphere platforms.
* Some host-specific fields in the bare metal platform configuration that have equivalents in `agent-config.yaml` file are ignored. A warning message is logged if these fields are set.


.`agent-config.yaml`

* Each interface must have a defined MAC address. Additionally, all interfaces must have a different MAC address.
* At least one interface must be defined for each host.
* World Wide Name (WWN) vendor extensions are not supported in root device hints.
* The `role` parameter in the `host` object must have a value of either `master` or `worker`.

== ZTP manifests

.`agent-cluster-install.yaml`

* For IPv6, the only supported value for the `networkType` parameter is `OVNKubernetes`. The `OpenshiftSDN` value can be used only for IPv4.

.`cluster-image-set.yaml`

* The `ReleaseImage` parameter must match the release defined in the installer.

:leveloffset!:

:leveloffset: +1

// This is included in the following assemblies:
//
// preparing-to-install-with-agent-based-installer.adoc

:_mod-docs-content-type: REFERENCE
[id='root-device-hints_{context}']
= About root device hints

The `rootDeviceHints` parameter enables the installer to provision the {op-system-first} image to a particular device. The installer examines the devices in the order it discovers them, and compares the discovered values with the hint values. The installer uses the first discovered device that matches the hint value. The configuration can combine multiple hints, but a device must match all hints for the installer to select it.

.Subfields

|===
| Subfield | Description

| `deviceName` | A string containing a Linux device name such as `/dev/vda` or `/dev/disk/by-path/`. It is recommended to use the `/dev/disk/by-path/<device_path>` link to the storage location. The hint must match the actual value exactly.

| `hctl` | A string containing a SCSI bus address like `0:0:0:0`. The hint must match the actual value exactly.

| `model` | A string containing a vendor-specific device identifier. The hint can be a substring of the actual value.

| `vendor` | A string containing the name of the vendor or manufacturer of the device. The hint can be a sub-string of the actual value.

| `serialNumber` | A string containing the device serial number. The hint must match the actual value exactly.

| `minSizeGigabytes` | An integer representing the minimum size of the device in gigabytes.

| `wwn` | A string containing the unique storage identifier. The hint must match the actual value exactly.

| `rotational` | A boolean indicating whether the device should be a rotating disk (true) or not (false).

|===

.Example usage

[source,yaml]
----
     - name: master-0
       role: master
       rootDeviceHints:
         deviceName: "/dev/sda"
----

:leveloffset!:

[id="agent-based-installation-next-steps"]
== Next steps

* xref:../../installing/installing_with_agent_based_installer/installing-with-agent-based-installer.adoc#installing-with-agent-based-installer[Installing a cluster with the Agent-based Installer]

//# includes=_attributes/common-attributes,modules/understanding-agent-install,modules/agent-installer-fips-compliance,modules/agent-installer-configuring-fips-compliance,modules/agent-install-networking,modules/agent-install-dns-none,modules/agent-install-load-balancing-none,modules/agent-install-sample-config-bonds-vlans,modules/agent-install-sample-config-bond-sriov,modules/snippets/technology-preview,modules/installation-bare-metal-agent-installer-config-yaml,modules/validations-before-agent-iso-creation,modules/agent-install-ipi-install-root-device-hints

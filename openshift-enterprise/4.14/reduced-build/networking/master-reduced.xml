<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>Networking</title>
<date>2024-02-23</date>
<title>Networking</title>
<productname>OpenShift Container Platform</productname>
<productnumber>4.14</productnumber>
<subtitle>Enter a short description here.</subtitle>
<abstract>
    <para>A short overview and summary of the book's subject and purpose, traditionally no more than one paragraph long.</para>
</abstract>
<authorgroup>
    <orgname>Red Hat OpenShift Documentation Team</orgname>
</authorgroup>
<xi:include href="Common_Content/Legal_Notice.xml" xmlns:xi="http://www.w3.org/2001/XInclude" />
</info>
<chapter xml:id="about-networking">
<title>About networking</title>

<simpara>Red Hat OpenShift Networking is an ecosystem of features, plugins and advanced networking capabilities that extend Kubernetes networking with the advanced networking-related features that your cluster needs to manage its network traffic for one or multiple hybrid clusters. This ecosystem of networking capabilities integrates ingress, egress, load balancing, high-performance throughput, security, inter- and intra-cluster traffic management and provides role-based observability tooling to reduce its natural complexities.</simpara>
<simpara>The following list highlights some of the most commonly used Red Hat OpenShift Networking features available on your cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Primary cluster network provided by either of the following Container Network Interface (CNI) plugins:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="about-ovn-kubernetes">OVN-Kubernetes network plugin</link>, the default plugin</simpara>
</listitem>
<listitem>
<simpara><link linkend="about-openshift-sdn">OpenShift SDN network plugin</link></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Certified 3rd-party alternative primary network plugins</simpara>
</listitem>
<listitem>
<simpara>Cluster Network Operator for network plugin management</simpara>
</listitem>
<listitem>
<simpara>Ingress Operator for TLS encrypted web traffic</simpara>
</listitem>
<listitem>
<simpara>DNS Operator for name assignment</simpara>
</listitem>
<listitem>
<simpara>MetalLB Operator for traffic load balancing on bare metal clusters</simpara>
</listitem>
<listitem>
<simpara>IP failover support for high-availability</simpara>
</listitem>
<listitem>
<simpara>Additional hardware network support through multiple CNI plugins, including for macvlan, ipvlan, and SR-IOV hardware networks</simpara>
</listitem>
<listitem>
<simpara>IPv4, IPv6, and dual stack addressing</simpara>
</listitem>
<listitem>
<simpara>Hybrid Linux-Windows host clusters for Windows-based workloads</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenShift Service Mesh for discovery, load balancing, service-to-service authentication, failure recovery, metrics, and monitoring of services</simpara>
</listitem>
<listitem>
<simpara>Single-node OpenShift</simpara>
</listitem>
<listitem>
<simpara>Network Observability Operator for network debugging and insights</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://catalog.redhat.com/software/container-stacks/detail/5f0c67b7ce85fb9e399f3a12">Submariner</link> and <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_application_interconnect/1.0/html/introduction_to_application_interconnect/index">Red Hat Application Interconnect</link> technologies for inter-cluster networking</simpara>
</listitem>
</itemizedlist>
</chapter>
<chapter xml:id="understanding-networking">
<title>Understanding networking</title>

<simpara>Cluster Administrators have several options for exposing applications that run inside a cluster to external traffic and securing network connections:</simpara>
<itemizedlist>
<listitem>
<simpara>Service types, such as node ports or load balancers</simpara>
</listitem>
<listitem>
<simpara>API resources, such as <literal>Ingress</literal> and <literal>Route</literal></simpara>
</listitem>
</itemizedlist>
<simpara>By default, Kubernetes allocates each pod an internal IP address for applications running within the pod. Pods and their containers can network, but clients outside the cluster do not have networking access. When you expose your application to external traffic, giving each pod its own IP address means that pods can be treated like physical hosts or virtual machines in terms of port allocation, networking, naming, service discovery, load balancing, application configuration, and migration.</simpara>
<note>
<simpara>Some cloud platforms offer metadata APIs that listen on the 169.254.169.254 IP address, a link-local IP address in the IPv4 <literal>169.254.0.0/16</literal> CIDR block.</simpara>
<simpara>This CIDR block is not reachable from the pod network. Pods that need access to these IP addresses must be given host network access by setting the <literal>spec.hostNetwork</literal> field in the pod spec to <literal>true</literal>.</simpara>
<simpara>If you allow a pod host network access, you grant the pod privileged access to the underlying network infrastructure.</simpara>
</note>
<section xml:id="nw-ne-openshift-dns_understanding-networking">
<title>OpenShift Container Platform DNS</title>
<simpara>If you are running multiple services, such as front-end and back-end services for
use with multiple pods, environment variables are created for user names,
service IPs, and more so the front-end pods can communicate with the back-end
services. If the service is deleted and recreated, a new IP address can be
assigned to the service, and requires the front-end pods to be recreated to pick
up the updated values for the service IP environment variable. Additionally, the
back-end service must be created before any of the front-end pods to ensure that
the service IP is generated properly, and that it can be provided to the
front-end pods as an environment variable.</simpara>
<simpara>For this reason, OpenShift Container Platform has a built-in DNS so that the services can be
reached by the service DNS as well as the service IP/port.</simpara>
</section>
<section xml:id="nw-ne-openshift-ingress_understanding-networking">
<title>OpenShift Container Platform Ingress Operator</title>
<simpara>When you create your OpenShift Container Platform cluster, pods and services running on the cluster are each allocated their own IP addresses. The IP addresses are accessible to other pods and services running nearby but are not accessible to outside clients. The Ingress Operator implements the <literal>IngressController</literal> API and is the component responsible for enabling external access to OpenShift Container Platform cluster services.</simpara>
<simpara>The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based
<link xlink:href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</link> to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform <literal>Route</literal> and Kubernetes <literal>Ingress</literal> resources. Configurations within the Ingress Controller, such as the ability to define <literal>endpointPublishingStrategy</literal> type and internal load balancing, provide ways to publish Ingress Controller endpoints.</simpara>
<section xml:id="nw-ne-comparing-ingress-route_understanding-networking">
<title>Comparing routes and Ingress</title>
<simpara>The Kubernetes Ingress resource in OpenShift Container Platform implements the Ingress Controller with a shared router service that runs as a pod inside the cluster. The most common way to manage Ingress traffic is with the Ingress Controller. You can scale and replicate this pod like any other regular pod. This router service is based on <link xlink:href="http://www.haproxy.org/">HAProxy</link>, which is an open source load balancer solution.</simpara>
<simpara>The OpenShift Container Platform route provides Ingress traffic to services in the cluster. Routes provide advanced features that might not be supported by standard Kubernetes Ingress Controllers, such as TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.</simpara>
<simpara>Ingress traffic accesses services in the cluster through a route. Routes and Ingress are the main resources for handling Ingress traffic. Ingress provides features similar to a route, such as accepting external requests and delegating them based on the route. However, with Ingress you can only allow certain types of connections: HTTP/2, HTTPS and server name identification (SNI), and TLS with certificate. In OpenShift Container Platform, routes are generated to meet the conditions specified by the Ingress resource.</simpara>
</section>
</section>
<section xml:id="nw-networking-glossary-terms_understanding-networking">
<title>Glossary of common terms for OpenShift Container Platform networking</title>
<simpara>This glossary defines common terms that are used in the networking content.</simpara>
<variablelist>
<varlistentry>
<term>authentication</term>
<listitem>
<simpara>To control access to an OpenShift Container Platform cluster, a cluster administrator can configure user authentication and ensure only approved users access the cluster. To interact with an OpenShift Container Platform cluster, you must authenticate to the OpenShift Container Platform API. You can authenticate by providing an OAuth access token or an X.509 client certificate in your requests to the OpenShift Container Platform API.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>AWS Load Balancer Operator</term>
<listitem>
<simpara>The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <literal>aws-load-balancer-controller</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Cluster Network Operator</term>
<listitem>
<simpara>The Cluster Network Operator (CNO) deploys and manages the cluster network components in an OpenShift Container Platform cluster. This includes deployment of the Container Network Interface (CNI) network plugin selected for the cluster during installation.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>config map</term>
<listitem>
<simpara>A config map provides a way to inject configuration data into pods. You can reference the data stored in a config map in a volume of type <literal>ConfigMap</literal>. Applications running in a pod can use this data.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>custom resource (CR)</term>
<listitem>
<simpara>A CR is extension of the Kubernetes API. You can create custom resources.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>DNS</term>
<listitem>
<simpara>Cluster DNS is a DNS server which serves DNS records for Kubernetes services. Containers started by Kubernetes automatically include this DNS server in their DNS searches.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>DNS Operator</term>
<listitem>
<simpara>The DNS Operator deploys and manages CoreDNS to provide a name resolution service to pods. This enables DNS-based Kubernetes Service discovery in OpenShift Container Platform.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>deployment</term>
<listitem>
<simpara>A Kubernetes resource object that maintains the life cycle of an application.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>domain</term>
<listitem>
<simpara>Domain is a DNS name serviced by the Ingress Controller.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>egress</term>
<listitem>
<simpara>The process of data sharing externally through a network’s outbound traffic from a pod.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>External DNS Operator</term>
<listitem>
<simpara>The External DNS Operator deploys and manages ExternalDNS to provide the name resolution for services and routes from the external DNS provider to OpenShift Container Platform.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>HTTP-based route</term>
<listitem>
<simpara>An HTTP-based route is an unsecured route that uses the basic HTTP routing protocol and exposes a service on an unsecured application port.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ingress</term>
<listitem>
<simpara>The Kubernetes Ingress resource in OpenShift Container Platform implements the Ingress Controller with a shared router service that runs as a pod inside the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ingress Controller</term>
<listitem>
<simpara>The Ingress Operator manages Ingress Controllers. Using an Ingress Controller is the most common way to allow external access to an OpenShift Container Platform cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>installer-provisioned infrastructure</term>
<listitem>
<simpara>The installation program deploys and configures the infrastructure that the cluster runs on.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>kubelet</term>
<listitem>
<simpara>A primary node agent that runs on each node in the cluster to ensure that containers are running in a pod.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kubernetes NMState Operator</term>
<listitem>
<simpara>The Kubernetes NMState Operator provides a Kubernetes API for performing state-driven network configuration across the OpenShift Container Platform cluster’s nodes with NMState.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>kube-proxy</term>
<listitem>
<simpara>Kube-proxy is a proxy service which runs on each node and helps in making services available to the external host. It helps in forwarding the request to correct containers and is capable of performing primitive load balancing.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>load balancers</term>
<listitem>
<simpara>OpenShift Container Platform uses load balancers for communicating from outside the cluster with services running in the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>MetalLB Operator</term>
<listitem>
<simpara>As a cluster administrator, you can add the MetalLB Operator to your cluster so that when a service of type <literal>LoadBalancer</literal> is added to the cluster, MetalLB can add an external IP address for the service.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>multicast</term>
<listitem>
<simpara>With IP multicast, data is broadcast to many IP addresses simultaneously.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>namespaces</term>
<listitem>
<simpara>A namespace isolates specific system resources that are visible to all processes. Inside a namespace, only processes that are members of that namespace can see those resources.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>networking</term>
<listitem>
<simpara>Network information of a OpenShift Container Platform cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>node</term>
<listitem>
<simpara>A worker machine in the OpenShift Container Platform cluster. A node is either a virtual machine (VM) or a physical machine.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>OpenShift Container Platform Ingress Operator</term>
<listitem>
<simpara>The Ingress Operator implements the <literal>IngressController</literal> API and is the component responsible for enabling external access to OpenShift Container Platform services.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>pod</term>
<listitem>
<simpara>One or more containers with shared resources, such as volume and IP addresses, running in your OpenShift Container Platform cluster.
A pod is the smallest compute unit defined, deployed, and managed.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>PTP Operator</term>
<listitem>
<simpara>The PTP Operator creates and manages the <literal>linuxptp</literal> services.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>route</term>
<listitem>
<simpara>The OpenShift Container Platform route provides Ingress traffic to services in the cluster. Routes provide advanced features that might not be supported by standard Kubernetes Ingress Controllers, such as TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>scaling</term>
<listitem>
<simpara>Increasing or decreasing the resource capacity.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>service</term>
<listitem>
<simpara>Exposes a running application on a set of pods.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Single Root I/O Virtualization (SR-IOV) Network Operator</term>
<listitem>
<simpara>The Single Root I/O Virtualization (SR-IOV) Network Operator manages the SR-IOV network devices and network attachments in your cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>software-defined networking (SDN)</term>
<listitem>
<simpara>OpenShift Container Platform uses a software-defined networking (SDN) approach to provide a unified cluster network that enables communication between pods across the OpenShift Container Platform cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Stream Control Transmission Protocol (SCTP)</term>
<listitem>
<simpara>SCTP is a reliable message based protocol that runs on top of an IP network.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>taint</term>
<listitem>
<simpara>Taints and tolerations ensure that pods are scheduled onto appropriate nodes. You can apply one or more taints on a node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>toleration</term>
<listitem>
<simpara>You can apply tolerations to pods. Tolerations allow the scheduler to schedule pods with matching taints.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>web console</term>
<listitem>
<simpara>A user interface (UI) to manage OpenShift Container Platform.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</chapter>
<chapter xml:id="accessing-hosts">
<title>Accessing hosts</title>

<simpara>Learn how to create a bastion host to access OpenShift Container Platform instances and
access the control plane nodes with secure shell (SSH) access.</simpara>
<section xml:id="accessing-hosts-on-aws_accessing-hosts">
<title>Accessing hosts on Amazon Web Services in an installer-provisioned infrastructure cluster</title>
<simpara>The OpenShift Container Platform installer does not create any public IP addresses for any of
the Amazon Elastic Compute Cloud (Amazon EC2) instances that it provisions for
your OpenShift Container Platform cluster. To be able to SSH to your OpenShift Container Platform
hosts, you must follow this procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a security group that allows SSH access into the virtual private cloud
(VPC) created by the <literal>openshift-install</literal> command.</simpara>
</listitem>
<listitem>
<simpara>Create an Amazon EC2 instance on one of the public subnets the installer
created.</simpara>
</listitem>
<listitem>
<simpara>Associate a public IP address with the Amazon EC2 instance that you created.</simpara>
<simpara>Unlike with the OpenShift Container Platform installation, you should associate the Amazon
EC2 instance you created with an SSH keypair. It does not matter what operating
system you choose for this instance, as it will simply serve as an SSH bastion
to bridge the internet into your OpenShift Container Platform cluster&#8217;s VPC. The Amazon
Machine Image (AMI) you use does matter. With Red Hat Enterprise Linux CoreOS (RHCOS),
for example, you can provide keys via Ignition, like the installer does.</simpara>
</listitem>
<listitem>
<simpara>After you provisioned your Amazon EC2 instance and can SSH into it, you must add
the SSH key that you associated with your OpenShift Container Platform installation. This key
can be different from the key for the bastion instance, but does not have to be.</simpara>
<note>
<simpara>Direct SSH access is only recommended for disaster recovery. When the Kubernetes
API is responsive, run privileged pods instead.</simpara>
</note>
</listitem>
<listitem>
<simpara>Run <literal>oc get nodes</literal>, inspect the output, and choose one of the nodes that is a
master. The hostname looks similar to <literal>ip-10-0-1-163.ec2.internal</literal>.</simpara>
</listitem>
<listitem>
<simpara>From the bastion SSH host you manually deployed into Amazon EC2, SSH into that
control plane host. Ensure that you use the same SSH key you specified during the
installation:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh -i &lt;ssh-key-path&gt; core@&lt;master-hostname&gt;</programlisting>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="networking-operators-overview">
<title>Networking Operators overview</title>

<simpara>OpenShift Container Platform supports multiple types of networking Operators. You can manage the cluster networking using these networking Operators.</simpara>
<section xml:id="networking-operators-overview-cluster-network-operator">
<title>Cluster Network Operator</title>
<simpara>The Cluster Network Operator (CNO) deploys and manages the cluster network components in an OpenShift Container Platform cluster. This includes deployment of the Container Network Interface (CNI) network plugin selected for the cluster during installation. For more information, see <link linkend="cluster-network-operator">Cluster Network Operator in OpenShift Container Platform</link>.</simpara>
</section>
<section xml:id="networking-operators-overview-dns-operator">
<title>DNS Operator</title>
<simpara>The DNS Operator deploys and manages CoreDNS to provide a name resolution service to pods. This enables DNS-based Kubernetes Service discovery in OpenShift Container Platform. For more information, see <link linkend="dns-operator">DNS Operator in OpenShift Container Platform</link>.</simpara>
</section>
<section xml:id="networking-operators-overview-ingress-operator">
<title>Ingress Operator</title>
<simpara>When you create your OpenShift Container Platform cluster, pods and services running on the cluster are each allocated IP addresses. The IP addresses are accessible to other pods and services running nearby but are not accessible to external clients. The Ingress Operator implements the Ingress Controller API and is responsible for enabling external access to OpenShift Container Platform cluster services. For more information, see <link linkend="configuring-ingress">Ingress Operator in OpenShift Container Platform</link>.</simpara>
</section>
<section xml:id="networking-operators-overview-external-dns-operator">
<title>External DNS Operator</title>
<simpara>The External DNS Operator deploys and manages ExternalDNS to provide the name resolution for services and routes from the external DNS provider to OpenShift Container Platform. For more information, see <link linkend="external-dns-operator">Understanding the External DNS Operator</link>.</simpara>
</section>
<section xml:id="ingress-node-firewall-operator-1">
<title>Ingress Node Firewall Operator</title>
<simpara>The Ingress Node Firewall Operator uses an extended Berkley Packet Filter (eBPF) and eXpress Data Path (XDP) plugin to process node firewall rules, update statistics and generate events for dropped traffic. The operator manages ingress node firewall resources, verifies firewall configuration, does not allow incorrectly configured rules that can prevent cluster access, and loads ingress node firewall XDP programs to the selected interfaces in the rule&#8217;s object(s). For more information, see <link linkend="ingress-node-firewall-operator">Understanding the Ingress Node Firewall Operator</link></simpara>
</section>
<section xml:id="network-observability-operator-overview-operator">
<title>Network Observability Operator</title>
<simpara>The Network Observability Operator is an optional Operator that allows cluster administrators to observe the network traffic for OpenShift Container Platform clusters. The Network Observability Operator uses the eBPF technology to create network flows. The network flows are then enriched with OpenShift Container Platform information and stored in Loki. You can view and analyze the stored network flows information in the OpenShift Container Platform console for further insight and troubleshooting. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/network_observability/#dependency-network-observability">About Network Observability Operator</link>.</simpara>
</section>
</chapter>
<chapter xml:id="networking-dashboards_networking-operators-overview">
<title>Networking dashboards</title>

<simpara>Networking metrics are viewable in dashboards within the OpenShift Container Platform web console, under <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Dashboards</emphasis>.</simpara>
<section xml:id="network-observability-operator-operator-dashboards">
<title>Network Observability Operator</title>
<simpara>If you have the Network Observability Operator installed, you can view network traffic metrics dashboards by selecting the <emphasis role="strong">Netobserv</emphasis> dashboard from the <emphasis role="strong">Dashboards</emphasis> drop-down list. For more information about metrics available in this <emphasis role="strong">Dashboard</emphasis> view,</simpara>
</section>
<section xml:id="general-networking-ovnk-dashboards">
<title>Networking and OVN-Kubernetes dashboard</title>
<simpara>You can view both general networking metrics as well as OVN-Kubernetes metrics from the dashboard.</simpara>
<simpara>To view general networking metrics, select <emphasis role="strong">Networking/Linux Subsystem Stats</emphasis> from the <emphasis role="strong">Dashboards</emphasis> drop-down list. You can view the following networking metrics from the dashboard: <emphasis role="strong">Network Utilisation</emphasis>, <emphasis role="strong">Network Saturation</emphasis>, and <emphasis role="strong">Network Errors</emphasis>.</simpara>
<simpara>To view OVN-Kubernetes metrics select <emphasis role="strong">Networking/Infrastructure</emphasis> from the <emphasis role="strong">Dashboards</emphasis> drop-down list. You can view the following OVN-Kuberenetes metrics: <emphasis role="strong">Networking Configuration</emphasis>, <emphasis role="strong">TCP Latency Probes</emphasis>, <emphasis role="strong">Control Plane Resources</emphasis>, and <emphasis role="strong">Worker Resources</emphasis>.</simpara>
</section>
<section xml:id="ingress-dashboards">
<title>Ingress Operator dashboard</title>
<simpara>You can view networking metrics handled by the Ingress Operator from the dashboard. This includes metrics like the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Incoming and outgoing bandwidth</simpara>
</listitem>
<listitem>
<simpara>HTTP error rates</simpara>
</listitem>
<listitem>
<simpara>HTTP server response latency</simpara>
</listitem>
</itemizedlist>
<simpara>To view these Ingress metrics, select <emphasis role="strong">Networking/Ingress</emphasis> from the <emphasis role="strong">Dashboards</emphasis> drop-down list. You can view Ingress metrics for the following categories: <emphasis role="strong">Top 10 Per Route</emphasis>, <emphasis role="strong">Top 10 Per Namespace</emphasis>, and <emphasis role="strong">Top 10 Per Shard</emphasis>.</simpara>
</section>
</chapter>
<chapter xml:id="cluster-network-operator">
<title>Cluster Network Operator in OpenShift Container Platform</title>

<simpara>The Cluster Network Operator (CNO) deploys and manages the cluster network components on an OpenShift Container Platform cluster, including the Container Network Interface (CNI) network plugin selected for the cluster during installation.</simpara>
<section xml:id="nw-cluster-network-operator_cluster-network-operator">
<title>Cluster Network Operator</title>
<simpara>The Cluster Network Operator implements the <literal>network</literal> API from the <literal>operator.openshift.io</literal> API group.
The Operator deploys the OVN-Kubernetes network plugin, or the network provider plugin that you selected during cluster installation, by using a daemon set.</simpara>
<formalpara>
<title>Procedure</title>
<para>The Cluster Network Operator is deployed during installation as a Kubernetes
<literal>Deployment</literal>.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Run the following command to view the Deployment status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-network-operator deployment/network-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME               READY   UP-TO-DATE   AVAILABLE   AGE
network-operator   1/1     1            1           56m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command to view the state of the Cluster Network Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusteroperator/network</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
network   4.5.4     True        False         False      50m</programlisting>
</para>
</formalpara>
<simpara>The following fields provide information about the status of the operator:
<literal>AVAILABLE</literal>, <literal>PROGRESSING</literal>, and <literal>DEGRADED</literal>. The <literal>AVAILABLE</literal> field is <literal>True</literal> when
the Cluster Network Operator reports an available status condition.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-cno-view_cluster-network-operator">
<title>Viewing the cluster network configuration</title>
<simpara>Every new OpenShift Container Platform installation has a <literal>network.config</literal> object named
<literal>cluster</literal>.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Use the <literal>oc describe</literal> command to view the cluster network configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe network.config/cluster</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         cluster
Namespace:
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  config.openshift.io/v1
Kind:         Network
Metadata:
  Self Link:           /apis/config.openshift.io/v1/networks/cluster
Spec: <co xml:id="CO1-1"/>
  Cluster Network:
    Cidr:         10.128.0.0/14
    Host Prefix:  23
  Network Type:   OpenShiftSDN
  Service Network:
    172.30.0.0/16
Status: <co xml:id="CO1-2"/>
  Cluster Network:
    Cidr:               10.128.0.0/14
    Host Prefix:        23
  Cluster Network MTU:  8951
  Network Type:         OpenShiftSDN
  Service Network:
    172.30.0.0/16
Events:  &lt;none&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO1-1">
<para>The <literal>Spec</literal> field displays the configured state of the cluster network.</para>
</callout>
<callout arearefs="CO1-2">
<para>The <literal>Status</literal> field displays the current state of the cluster network
configuration.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-cno-status_cluster-network-operator">
<title>Viewing Cluster Network Operator status</title>
<simpara>You can inspect the status and view the details of the Cluster Network Operator
using the <literal>oc describe</literal> command.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to view the status of the Cluster Network Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe clusteroperators/network</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-cno-logs_cluster-network-operator">
<title>Viewing Cluster Network Operator logs</title>
<simpara>You can view Cluster Network Operator logs by using the <literal>oc logs</literal> command.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to view the logs of the Cluster Network Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs --namespace=openshift-network-operator deployment/network-operator</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-operator-cr_cluster-network-operator">
<title>Cluster Network Operator configuration</title>
<simpara>The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named <literal>cluster</literal>. The CR specifies the fields for the <literal>Network</literal> API in the <literal>operator.openshift.io</literal> API group.</simpara>
<simpara>The CNO configuration inherits the following fields during cluster installation from the <literal>Network</literal> API in the <literal>Network.config.openshift.io</literal> API group:</simpara>
<variablelist>
<varlistentry>
<term><literal>clusterNetwork</literal></term>
<listitem>
<simpara>IP address pools from which pod IP addresses are allocated.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>serviceNetwork</literal></term>
<listitem>
<simpara>IP address pool for services.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>defaultNetwork.type</literal></term>
<listitem>
<simpara>Cluster network plugin, such as OpenShift SDN or OVN-Kubernetes.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>After cluster installation, you can only modify the <literal>clusterNetwork</literal> IP address range. The default network type can only be changed from OpenShift SDN to OVN-Kubernetes through migration.</simpara>
</note>
<simpara>You can specify the cluster network plugin configuration for your cluster by setting the fields for the <literal>defaultNetwork</literal> object in the CNO object named <literal>cluster</literal>.</simpara>
<section xml:id="nw-operator-cr-cno-object_cluster-network-operator">
<title>Cluster Network Operator configuration object</title>
<simpara>The fields for the Cluster Network Operator (CNO) are described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster Network Operator configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNO object. This name is always <literal>cluster</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.clusterNetwork</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A list specifying the blocks of IP addresses from which pod IP addresses are
allocated and the subnet prefix length assigned to each individual node in the cluster. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  clusterNetwork:
  - cidr: 10.128.0.0/19
    hostPrefix: 23
  - cidr: 10.128.32.0/19
    hostPrefix: 23</programlisting></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.serviceNetwork</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A block of IP addresses for services. The OpenShift SDN and OVN-Kubernetes network plugins support only a single IP address block for the service network. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  serviceNetwork:
  - 172.30.0.0/14</programlisting>
<simpara>This value is ready-only and inherited from the <literal>Network.config.openshift.io</literal> object named <literal>cluster</literal> during cluster installation.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.defaultNetwork</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Configures the network plugin for the cluster network.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.kubeProxyConfig</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The fields for this object specify the kube-proxy configuration.
If you are using the OVN-Kubernetes cluster network plugin, the kube-proxy configuration has no effect.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<bridgehead xml:id="nw-operator-cr-defaultnetwork_cluster-network-operator" renderas="sect4">defaultNetwork object configuration</bridgehead>
<simpara>The values for the <literal>defaultNetwork</literal> object are defined in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>defaultNetwork</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Either <literal>OpenShiftSDN</literal> or <literal>OVNKubernetes</literal>. The Red Hat OpenShift Networking network plugin is selected during installation. You can change this value by migrating from OpenShift SDN to OVN-Kubernetes.</simpara>
<note>
<simpara>OpenShift Container Platform uses the OVN-Kubernetes network plugin by default.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>openshiftSDNConfig</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>This object is only valid for the OpenShift SDN network plugin.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ovnKubernetesConfig</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>This object is only valid for the OVN-Kubernetes network plugin.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<bridgehead xml:id="nw-operator-configuration-parameters-for-openshift-sdn_cluster-network-operator" renderas="sect5">Configuration for the OpenShift SDN network plugin</bridgehead>
<simpara>The following table describes the configuration fields for the OpenShift SDN network plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>openshiftSDNConfig</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>mode</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The network isolation mode for OpenShift SDN.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum transmission unit (MTU) for the VXLAN overlay network. This value is normally configured automatically.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>vxlanPort</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The port to use for all VXLAN packets. The default value is <literal>4789</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example OpenShift SDN configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">defaultNetwork:
  type: OpenShiftSDN
  openshiftSDNConfig:
    mode: NetworkPolicy
    mtu: 1450
    vxlanPort: 4789</programlisting>
</para>
</formalpara>
<bridgehead xml:id="nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator" renderas="sect5">Configuration for the OVN-Kubernetes network plugin</bridgehead>
<simpara>The following table describes the configuration fields for the OVN-Kubernetes network plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ovnKubernetesConfig</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum transmission unit (MTU) for the Geneve (Generic Network Virtualization Encapsulation) overlay network. This value is normally configured automatically.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>genevePort</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The UDP port for the Geneve overlay network.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipsecConfig</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>If the field is present, IPsec is enabled for the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>policyAuditConfig</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify a configuration object for customizing network policy audit logging. If unset, the defaults audit log settings are used.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gatewayConfig</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specify a configuration object for customizing how egress traffic is sent to the node gateway.</simpara>
<note>
<simpara>While migrating egress traffic, you can expect some disruption to workloads and service traffic until the Cluster Network Operator (CNO) successfully rolls out the changes.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v4InternalSubnet</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>If your existing network infrastructure overlaps with the <literal>100.64.0.0/16</literal> IPv4 subnet, you can specify a different IP address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. For example, if the <literal>clusterNetwork.cidr</literal> value is <literal>10.128.0.0/14</literal> and the <literal>clusterNetwork.hostPrefix</literal> value is <literal>/23</literal>, then the maximum number of nodes is <literal>2^(23-14)=512</literal>.</simpara><simpara>This field cannot be changed after installation.</simpara></entry>
<entry align="left" valign="middle"><simpara>The default value is <literal>100.64.0.0/16</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v6InternalSubnet</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>If your existing network infrastructure overlaps with the <literal>fd98::/48</literal> IPv6 subnet, you can specify a different IP address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster.</simpara><simpara>This field cannot be changed after installation.</simpara></entry>
<entry align="left" valign="middle"><simpara>The default value is <literal>fd98::/48</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>policyAuditConfig</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>rateLimit</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>integer</simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum number of messages to generate every second per node. The default value is <literal>20</literal> messages per second.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>maxFileSize</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>integer</simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum size for the audit log in bytes. The default value is <literal>50000000</literal> or 50 MB.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>destination</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>string</simpara></entry>
<entry align="left" valign="middle"><simpara>One of the following additional audit log targets:</simpara>
<variablelist>
<varlistentry>
<term><literal>libc</literal></term>
<listitem>
<simpara>The libc <literal>syslog()</literal> function of the journald process on the host.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>udp:&lt;host&gt;:&lt;port&gt;</literal></term>
<listitem>
<simpara>A syslog server. Replace <literal>&lt;host&gt;:&lt;port&gt;</literal> with the host and port of the syslog server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>unix:&lt;file&gt;</literal></term>
<listitem>
<simpara>A Unix Domain Socket file specified by <literal>&lt;file&gt;</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>null</literal></term>
<listitem>
<simpara>Do not send the audit logs to any additional target.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>syslogFacility</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>string</simpara></entry>
<entry align="left" valign="middle"><simpara>The syslog facility, such as <literal>kern</literal>, as defined by RFC5424. The default value is <literal>local0</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table xml:id="gatewayConfig-object_cluster-network-operator" frame="all" rowsep="1" colsep="1">
<title><literal>gatewayConfig</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>routingViaHost</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Set this field to <literal>true</literal> to send egress traffic from pods to the host networking stack.
For highly-specialized installations and applications that rely on manually configured routes in the kernel routing table, you might want to route egress traffic to the host networking stack.
By default, egress traffic is processed in OVN to exit the cluster and is not affected by specialized routes in the kernel routing table.
The default value is <literal>false</literal>.</simpara>
<simpara>This field has an interaction with the Open vSwitch hardware offloading feature.
If you set this field to <literal>true</literal>, you do not receive the performance benefits of the offloading because egress traffic is processed by the host networking stack.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipForwarding</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>You can control IP forwarding for all traffic on OVN-Kubernetes managed interfaces by using the <literal>ipForwarding</literal> specification in the <literal>Network</literal> resource. Specify <literal>Restricted</literal> to only allow IP forwarding for Kubernetes related traffic. Specify <literal>Global</literal> to allow forwarding of all IP traffic. For new installations, the default is <literal>Restricted</literal>. For updates to OpenShift Container Platform 4.14 or later, the default is <literal>Global</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example OVN-Kubernetes configuration with IPSec enabled</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">defaultNetwork:
  type: OVNKubernetes
  ovnKubernetesConfig:
    mtu: 1400
    genevePort: 6081
    ipsecConfig: {}</programlisting>
</para>
</formalpara>
<bridgehead xml:id="nw-operator-cr-kubeproxyconfig_cluster-network-operator" renderas="sect4">kubeProxyConfig object configuration (OpenShiftSDN container network interface only)</bridgehead>
<simpara>The values for the <literal>kubeProxyConfig</literal> object are defined in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>kubeProxyConfig</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>iptablesSyncPeriod</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The refresh period for <literal>iptables</literal> rules. The default value is <literal>30s</literal>. Valid suffixes include <literal>s</literal>, <literal>m</literal>, and <literal>h</literal> and are described in the <link xlink:href="https://golang.org/pkg/time/#ParseDuration">Go <literal>time</literal> package</link> documentation.</simpara>
<note>
<simpara>Because of performance improvements introduced in OpenShift Container Platform 4.3 and greater, adjusting the <literal>iptablesSyncPeriod</literal> parameter is no longer necessary.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>proxyArguments.iptables-min-sync-period</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The minimum duration before refreshing <literal>iptables</literal> rules. This field ensures that the refresh does not happen too frequently. Valid suffixes include <literal>s</literal>, <literal>m</literal>, and <literal>h</literal> and are described in the <link xlink:href="https://golang.org/pkg/time/#ParseDuration">Go <literal>time</literal> package</link>. The default value is:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kubeProxyConfig:
  proxyArguments:
    iptables-min-sync-period:
    - 0s</programlisting></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-operator-example-cr_cluster-network-operator">
<title>Cluster Network Operator example configuration</title>
<simpara>A complete CNO configuration is specified in the following example:</simpara>
<formalpara>
<title>Example Cluster Network Operator object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  serviceNetwork:
  - 172.30.0.0/16
  networkType: OVNKubernetes
      clusterNetworkMTU: 8900</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="cluster-network-operator-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-operator-openshift-io-v1"><literal>Network</literal> API in the <literal>operator.openshift.io</literal> API group</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-cluster-network-range-edit_configuring-cluster-network-range">Modifying the <literal>clusterNetwork</literal> IP address range</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="migrate-from-openshift-sdn">Migrating from the OpenShift SDN network plugin</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="dns-operator">
<title>DNS Operator in OpenShift Container Platform</title>

<simpara>The DNS Operator deploys and manages CoreDNS to provide a name resolution
service to pods, enabling DNS-based Kubernetes Service discovery in
OpenShift Container Platform.</simpara>
<section xml:id="nw-dns-operator_dns-operator">
<title>DNS Operator</title>
<simpara>The DNS Operator implements the <literal>dns</literal> API from the <literal>operator.openshift.io</literal> API
group. The Operator deploys CoreDNS using a daemon set, creates a service for
the daemon set, and configures the kubelet to instruct pods to use the CoreDNS
service IP address for name resolution.</simpara>
<formalpara>
<title>Procedure</title>
<para>The DNS Operator is deployed during installation with a <literal>Deployment</literal> object.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Use the <literal>oc get</literal> command to view the deployment status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-dns-operator deployment/dns-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME           READY     UP-TO-DATE   AVAILABLE   AGE
dns-operator   1/1       1            1           23h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>oc get</literal> command to view the state of the DNS Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusteroperator/dns</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      VERSION     AVAILABLE   PROGRESSING   DEGRADED   SINCE
dns       4.1.0-0.11  True        False         False      92m</programlisting>
</para>
</formalpara>
<simpara><literal>AVAILABLE</literal>, <literal>PROGRESSING</literal> and <literal>DEGRADED</literal> provide information about the status of the operator. <literal>AVAILABLE</literal> is <literal>True</literal> when at least 1 pod from the CoreDNS daemon set reports an <literal>Available</literal> status condition.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-dns-operator-managementState_dns-operator">
<title>Changing the DNS Operator managementState</title>
<simpara>DNS manages the CoreDNS component to provide a name resolution service for pods and services in the cluster. The <literal>managementState</literal> of the DNS Operator is set to <literal>Managed</literal> by default, which means that the DNS Operator is actively managing its resources. You can change it to <literal>Unmanaged</literal>, which means the DNS Operator is not managing its resources.</simpara>
<simpara>The following are use cases for changing the DNS Operator <literal>managementState</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>You are a developer and want to test a configuration change to see if it fixes an issue in CoreDNS. You can stop the DNS Operator from overwriting the fix by setting the <literal>managementState</literal> to <literal>Unmanaged</literal>.</simpara>
</listitem>
<listitem>
<simpara>You are a cluster administrator and have reported an issue with CoreDNS, but need to apply a workaround until the issue is fixed. You can set the <literal>managementState</literal> field of the DNS Operator to <literal>Unmanaged</literal> to apply the workaround.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Change <literal>managementState</literal> DNS Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc patch dns.operator.openshift.io default --type merge --patch '{"spec":{"managementState":"Unmanaged"}}'</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-controlling-dns-pod-placement_dns-operator">
<title>Controlling DNS pod placement</title>
<simpara>The DNS Operator has two daemon sets: one for CoreDNS and one for managing the <literal>/etc/hosts</literal> file. The daemon set for <literal>/etc/hosts</literal> must run on every node host to add an entry for the cluster image registry to support pulling images. Security policies can prohibit communication between pairs of nodes, which prevents the daemon set for CoreDNS from running on every node.</simpara>
<simpara>As a cluster administrator, you can use a custom node selector to configure the daemon set for CoreDNS to run or not run on certain nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To prevent communication between certain nodes, configure the <literal>spec.nodePlacement.nodeSelector</literal> API field:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Modify the DNS Operator object named <literal>default</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit dns.operator/default</programlisting>
</listitem>
<listitem>
<simpara>Specify a node selector that includes only control plane nodes in the <literal>spec.nodePlacement.nodeSelector</literal> API field:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"> spec:
   nodePlacement:
     nodeSelector:
       node-role.kubernetes.io/worker: ""</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To allow the daemon set for CoreDNS to run on nodes, configure a taint and toleration:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Modify the DNS Operator object named <literal>default</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit dns.operator/default</programlisting>
</listitem>
<listitem>
<simpara>Specify a taint key and a toleration for the taint:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"> spec:
   nodePlacement:
     tolerations:
     - effect: NoExecute
       key: "dns-only"
       operators: Equal
       value: abc
       tolerationSeconds: 3600 <co xml:id="CO2-1"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>If the taint is <literal>dns-only</literal>, it can be tolerated indefinitely. You can omit <literal>tolerationSeconds</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-dns-view_dns-operator">
<title>View the default DNS</title>
<simpara>Every new OpenShift Container Platform installation has a <literal>dns.operator</literal> named <literal>default</literal>.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Use the <literal>oc describe</literal> command to view the default <literal>dns</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe dns.operator/default</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         default
Namespace:
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  operator.openshift.io/v1
Kind:         DNS
...
Status:
  Cluster Domain:  cluster.local <co xml:id="CO3-1"/>
  Cluster IP:      172.30.0.10 <co xml:id="CO3-2"/>
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO3-1">
<para>The Cluster Domain field is the base DNS domain used to construct fully
qualified pod and service domain names.</para>
</callout>
<callout arearefs="CO3-2">
<para>The Cluster IP is the address pods query for name resolution. The IP is
defined as the 10th address in the service CIDR range.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To find the service CIDR of your cluster,
use the <literal>oc get</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networks.config/cluster -o jsonpath='{$.status.serviceNetwork}'</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">[172.30.0.0/16]</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-dns-forward_dns-operator">
<title>Using DNS forwarding</title>
<simpara>You can use DNS forwarding to override the default forwarding configuration in the <literal>/etc/resolv.conf</literal> file in the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara>Specify name servers for every zone. If the forwarded zone is the Ingress domain managed by OpenShift Container Platform, then the upstream name server must be authorized for the domain.</simpara>
</listitem>
<listitem>
<simpara>Provide a list of upstream DNS servers.</simpara>
</listitem>
<listitem>
<simpara>Change the default forwarding policy.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>A DNS forwarding configuration for the default domain can have both the default servers specified in the <literal>/etc/resolv.conf</literal> file and the upstream DNS servers.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the DNS Operator object named <literal>default</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit dns.operator/default</programlisting>
<simpara>After you issue the previous command, the Operator creates and updates the config map named <literal>dns-default</literal> with additional server configuration blocks based on <literal>Server</literal>.
If none of the servers have a zone that matches the query, then name resolution falls back to the upstream DNS servers.</simpara>
<formalpara>
<title>Configuring DNS forwarding</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: DNS
metadata:
  name: default
spec:
  servers:
  - name: example-server <co xml:id="CO4-1"/>
    zones: <co xml:id="CO4-2"/>
    - example.com
    forwardPlugin:
      policy: Random <co xml:id="CO4-3"/>
      upstreams: <co xml:id="CO4-4"/>
      - 1.1.1.1
      - 2.2.2.2:5353
  upstreamResolvers: <co xml:id="CO4-5"/>
    policy: Random <co xml:id="CO4-6"/>
    upstreams: <co xml:id="CO4-7"/>
    - type: SystemResolvConf <co xml:id="CO4-8"/>
    - type: Network
      address: 1.2.3.4 <co xml:id="CO4-9"/>
      port: 53 <co xml:id="CO4-10"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO4-1">
<para>Must comply with the <literal>rfc6335</literal> service name syntax.</para>
</callout>
<callout arearefs="CO4-2">
<para>Must conform to the definition of a subdomain in the <literal>rfc1123</literal> service name syntax. The cluster domain, <literal>cluster.local</literal>, is an invalid subdomain for the <literal>zones</literal> field.</para>
</callout>
<callout arearefs="CO4-3">
<para>Defines the policy to select upstream resolvers. Default value is <literal>Random</literal>. You can also use the values <literal>RoundRobin</literal>, and <literal>Sequential</literal>.</para>
</callout>
<callout arearefs="CO4-4">
<para>A maximum of 15 <literal>upstreams</literal> is allowed per <literal>forwardPlugin</literal>.</para>
</callout>
<callout arearefs="CO4-5">
<para>Optional. You can use it to override the default policy and forward DNS resolution to the specified DNS resolvers (upstream resolvers) for the default domain. If you do not provide any upstream resolvers, the DNS name queries go to the servers in <literal>/etc/resolv.conf</literal>.</para>
</callout>
<callout arearefs="CO4-6">
<para>Determines the order in which upstream servers are selected for querying. You can specify one of these values: <literal>Random</literal>, <literal>RoundRobin</literal>, or <literal>Sequential</literal>. The default value is <literal>Sequential</literal>.</para>
</callout>
<callout arearefs="CO4-7">
<para>Optional. You can use it to provide upstream resolvers.</para>
</callout>
<callout arearefs="CO4-8">
<para>You can specify two types of <literal>upstreams</literal> - <literal>SystemResolvConf</literal> and <literal>Network</literal>. <literal>SystemResolvConf</literal> configures the upstream to use <literal>/etc/resolv.conf</literal> and <literal>Network</literal> defines a <literal>Networkresolver</literal>. You can specify one or both.</para>
</callout>
<callout arearefs="CO4-9">
<para>If the specified type is <literal>Network</literal>, you must provide an IP address. The <literal>address</literal> field must be a valid IPv4 or IPv6 address.</para>
</callout>
<callout arearefs="CO4-10">
<para>If the specified type is <literal>Network</literal>, you can optionally provide a port. The <literal>port</literal> field must have a value between <literal>1</literal> and <literal>65535</literal>. If you do not specify a port for the upstream, by default port 853 is tried.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: When working in a highly regulated environment, you might need the ability to secure DNS traffic when forwarding requests to upstream resolvers so that you can ensure additional DNS traffic and data privacy.
Cluster administrators can configure transport layer security (TLS) for forwarded DNS queries.</simpara>
<formalpara>
<title>Configuring DNS forwarding with TLS</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: DNS
metadata:
  name: default
spec:
  servers:
  - name: example-server <co xml:id="CO5-1"/>
    zones: <co xml:id="CO5-2"/>
    - example.com
    forwardPlugin:
      transportConfig:
        transport: TLS <co xml:id="CO5-3"/>
        tls:
          caBundle:
            name: mycacert
          serverName: dnstls.example.com  <co xml:id="CO5-4"/>
      policy: Random <co xml:id="CO5-5"/>
      upstreams: <co xml:id="CO5-6"/>
      - 1.1.1.1
      - 2.2.2.2:5353
  upstreamResolvers: <co xml:id="CO5-7"/>
    transportConfig:
      transport: TLS
      tls:
        caBundle:
          name: mycacert
        serverName: dnstls.example.com
    upstreams:
    - type: Network <co xml:id="CO5-8"/>
      address: 1.2.3.4 <co xml:id="CO5-9"/>
      port: 53 <co xml:id="CO5-10"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO5-1">
<para>Must comply with the <literal>rfc6335</literal> service name syntax.</para>
</callout>
<callout arearefs="CO5-2">
<para>Must conform to the definition of a subdomain in the <literal>rfc1123</literal> service name syntax. The cluster domain, <literal>cluster.local</literal>, is an invalid subdomain for the <literal>zones</literal> field. The cluster domain, <literal>cluster.local</literal>, is an invalid <literal>subdomain</literal> for <literal>zones</literal>.</para>
</callout>
<callout arearefs="CO5-3">
<para>When configuring TLS for forwarded DNS queries, set the <literal>transport</literal> field to have the value <literal>TLS</literal>.
By default, CoreDNS caches forwarded connections for 10 seconds. CoreDNS will hold a TCP connection open for those 10 seconds if no request is issued. With large clusters, ensure that your DNS server is aware that it might get many new connections to hold open because you can initiate a connection per node. Set up your DNS hierarchy accordingly to avoid performance issues.</para>
</callout>
<callout arearefs="CO5-4">
<para>When configuring TLS for forwarded DNS queries, this is a mandatory server name used as part of the server name indication (SNI) to validate the upstream TLS server certificate.</para>
</callout>
<callout arearefs="CO5-5">
<para>Defines the policy to select upstream resolvers. Default value is <literal>Random</literal>. You can also use the values <literal>RoundRobin</literal>, and <literal>Sequential</literal>.</para>
</callout>
<callout arearefs="CO5-6">
<para>Required. You can use it to provide upstream resolvers. A maximum of 15 <literal>upstreams</literal> entries are allowed per <literal>forwardPlugin</literal> entry.</para>
</callout>
<callout arearefs="CO5-7">
<para>Optional. You can use it to override the default policy and forward DNS resolution to the specified DNS resolvers (upstream resolvers) for the default domain. If you do not provide any upstream resolvers, the DNS name queries go to the servers in <literal>/etc/resolv.conf</literal>.</para>
</callout>
<callout arearefs="CO5-8">
<para><literal>Network</literal> type indicates that this upstream resolver should handle forwarded requests separately from the upstream resolvers listed in <literal>/etc/resolv.conf</literal>. Only the <literal>Network</literal> type is allowed when using TLS and you must provide an IP address.</para>
</callout>
<callout arearefs="CO5-9">
<para>The <literal>address</literal> field must be a valid IPv4 or IPv6 address.</para>
</callout>
<callout arearefs="CO5-10">
<para>You can optionally provide a port. The <literal>port</literal> must have a value between <literal>1</literal> and <literal>65535</literal>. If you do not specify a port for the upstream, by default port 853 is tried.</para>
</callout>
</calloutlist>
<note>
<simpara>If <literal>servers</literal> is undefined or invalid, the config map only contains the default server.</simpara>
</note>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>View the config map:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap/dns-default -n openshift-dns -o yaml</programlisting>
<formalpara>
<title>Sample DNS ConfigMap based on previous sample DNS</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
data:
  Corefile: |
    example.com:5353 {
        forward . 1.1.1.1 2.2.2.2:5353
    }
    bar.com:5353 example.com:5353 {
        forward . 3.3.3.3 4.4.4.4:5454 <co xml:id="CO6-1"/>
    }
    .:5353 {
        errors
        health
        kubernetes cluster.local in-addr.arpa ip6.arpa {
            pods insecure
            upstream
            fallthrough in-addr.arpa ip6.arpa
        }
        prometheus :9153
        forward . /etc/resolv.conf 1.2.3.4:53 {
            policy Random
        }
        cache 30
        reload
    }
kind: ConfigMap
metadata:
  labels:
    dns.operator.openshift.io/owning-dns: default
  name: dns-default
  namespace: openshift-dns</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO6-1">
<para>Changes to the <literal>forwardPlugin</literal> triggers a rolling update of the CoreDNS daemon set.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information on DNS forwarding, see the <link xlink:href="https://coredns.io/plugins/forward/">CoreDNS forward documentation</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-dns-operator-status_dns-operator">
<title>DNS Operator status</title>
<simpara>You can inspect the status and view the details of the DNS Operator
using the <literal>oc describe</literal> command.</simpara>
<formalpara>
<title>Procedure</title>
<para>View the status of the DNS Operator:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe clusteroperators/dns</programlisting>
</section>
<section xml:id="nw-dns-operator-logs_dns-operator">
<title>DNS Operator logs</title>
<simpara>You can view DNS Operator logs by using the <literal>oc logs</literal> command.</simpara>
<formalpara>
<title>Procedure</title>
<para>View the logs of the DNS Operator:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-dns-operator deployment/dns-operator -c dns-operator</programlisting>
</section>
<section xml:id="nw-dns-loglevel_dns-operator">
<title>Setting the CoreDNS log level</title>
<simpara>You can configure the CoreDNS log level to determine the amount of detail in logged error messages. The valid values for CoreDNS log level are <literal>Normal</literal>, <literal>Debug</literal>, and <literal>Trace</literal>. The default <literal>logLevel</literal> is <literal>Normal</literal>.</simpara>
<note>
<simpara>The errors plugin is always enabled. The following <literal>logLevel</literal> settings report different error responses:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>logLevel</literal>: <literal>Normal</literal> enables the "errors" class: <literal>log . { class error }</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>logLevel</literal>: <literal>Debug</literal> enables the "denial" class: <literal>log . { class denial error }</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>logLevel</literal>: <literal>Trace</literal> enables the "all" class: <literal>log . { class all }</literal>.</simpara>
</listitem>
</itemizedlist>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To set <literal>logLevel</literal> to <literal>Debug</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"logLevel":"Debug"}}' --type=merge</programlisting>
</listitem>
<listitem>
<simpara>To set <literal>logLevel</literal> to <literal>Trace</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"logLevel":"Trace"}}' --type=merge</programlisting>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To ensure the desired log level was set, check the config map:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap/dns-default -n openshift-dns -o yaml</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-dns-operatorloglevel_dns-operator">
<title>Setting the CoreDNS Operator log level</title>
<simpara>Cluster administrators can configure the Operator log level to more quickly track down OpenShift DNS issues. The valid values for <literal>operatorLogLevel</literal> are <literal>Normal</literal>, <literal>Debug</literal>, and <literal>Trace</literal>. <literal>Trace</literal> has the most detailed information. The default <literal>operatorlogLevel</literal> is <literal>Normal</literal>. There are seven logging levels for issues: Trace, Debug, Info, Warning, Error, Fatal and Panic. After the logging level is set, log entries with that severity or anything above it will be logged.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>operatorLogLevel: "Normal"</literal> sets <literal>logrus.SetLogLevel("Info")</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>operatorLogLevel: "Debug"</literal> sets <literal>logrus.SetLogLevel("Debug")</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>operatorLogLevel: "Trace"</literal> sets  <literal>logrus.SetLogLevel("Trace")</literal>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To set <literal>operatorLogLevel</literal> to <literal>Debug</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"operatorLogLevel":"Debug"}}' --type=merge</programlisting>
</listitem>
<listitem>
<simpara>To set <literal>operatorLogLevel</literal> to <literal>Trace</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch dnses.operator.openshift.io/default -p '{"spec":{"operatorLogLevel":"Trace"}}' --type=merge</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-dns-cache-tuning_dns-operator">
<title>Tuning the CoreDNS cache</title>
<simpara>You can configure the maximum duration of both successful or unsuccessful caching, also known as positive or negative caching respectively, done by CoreDNS. Tuning the duration of caching of DNS query responses can reduce the load for any upstream DNS resolvers.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the DNS Operator object named <literal>default</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit dns.operator.openshift.io/default</programlisting>
</listitem>
<listitem>
<simpara>Modify the time-to-live (TTL) caching values:</simpara>
<formalpara>
<title>Configuring DNS caching</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: DNS
metadata:
  name: default
spec:
  cache:
    positiveTTL: 1h <co xml:id="CO7-1"/>
    negativeTTL: 0.5h10m <co xml:id="CO7-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO7-1">
<para>The string value <literal>1h</literal> is converted to its respective number of seconds by CoreDNS. If this field is omitted, the value is assumed to be <literal>0s</literal> and the cluster uses the internal default value of <literal>900s</literal> as a fallback.</para>
</callout>
<callout arearefs="CO7-2">
<para>The string value can be a combination of units such as <literal>0.5h10m</literal> and is converted to its respective number of seconds by CoreDNS. If this field is omitted, the value is assumed to be <literal>0s</literal> and the cluster uses the internal default value of <literal>30s</literal> as a fallback.</para>
</callout>
</calloutlist>
<warning>
<simpara>Setting TTL fields to low values could lead to an increased load on the cluster, any upstream resolvers, or both.</simpara>
</warning>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="configuring-ingress">
<title>Ingress Operator in OpenShift Container Platform</title>

<section xml:id="nw-ne-openshift-ingress_configuring-ingress">
<title>OpenShift Container Platform Ingress Operator</title>
<simpara>When you create your OpenShift Container Platform cluster, pods and services running on the cluster are each allocated their own IP addresses. The IP addresses are accessible to other pods and services running nearby but are not accessible to outside clients. The Ingress Operator implements the <literal>IngressController</literal> API and is the component responsible for enabling external access to OpenShift Container Platform cluster services.</simpara>
<simpara>The Ingress Operator makes it possible for external clients to access your service by deploying and managing one or more HAProxy-based
<link xlink:href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">Ingress Controllers</link> to handle routing. You can use the Ingress Operator to route traffic by specifying OpenShift Container Platform <literal>Route</literal> and Kubernetes <literal>Ingress</literal> resources. Configurations within the Ingress Controller, such as the ability to define <literal>endpointPublishingStrategy</literal> type and internal load balancing, provide ways to publish Ingress Controller endpoints.</simpara>
</section>
<section xml:id="nw-installation-ingress-config-asset_configuring-ingress">
<title>The Ingress configuration asset</title>
<simpara>The installation program generates an asset with an <literal>Ingress</literal> resource in the <literal>config.openshift.io</literal> API group, <literal>cluster-ingress-02-config.yml</literal>.</simpara>
<formalpara>
<title>YAML Definition of the <literal>Ingress</literal> resource</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: apps.openshiftdemos.com</programlisting>
</para>
</formalpara>
<simpara>The installation program stores this asset in the <literal>cluster-ingress-02-config.yml</literal> file in the <literal>manifests/</literal> directory. This <literal>Ingress</literal> resource defines the cluster-wide configuration for Ingress. This Ingress configuration is used as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>The Ingress Operator uses the domain from the cluster Ingress configuration as the domain for the default Ingress Controller.</simpara>
</listitem>
<listitem>
<simpara>The OpenShift API Server Operator uses the domain from the cluster Ingress configuration. This domain is also used when generating a default host for a <literal>Route</literal> resource that does not specify an explicit host.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-controller-configuration-parameters_configuring-ingress">
<title>Ingress Controller configuration parameters</title>
<simpara>The <literal>ingresscontrollers.operator.openshift.io</literal> resource offers the following
configuration parameters.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>domain</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>domain</literal> is a DNS name serviced by the Ingress Controller and is used to configure multiple features:</simpara>
<itemizedlist>
<listitem>
<simpara>For the <literal>LoadBalancerService</literal> endpoint publishing strategy, <literal>domain</literal> is used to configure DNS records. See <literal>endpointPublishingStrategy</literal>.</simpara>
</listitem>
<listitem>
<simpara>When using a generated default certificate, the certificate is valid for <literal>domain</literal> and its <literal>subdomains</literal>. See <literal>defaultCertificate</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value is published to individual Route statuses so that users know where to target external DNS records.</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>domain</literal> value must be unique among all Ingress Controllers and cannot be updated.</simpara>
<simpara>If empty, the default value is <literal>ingress.config.openshift.io/cluster</literal> <literal>.spec.domain</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>replicas</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>replicas</literal> is the desired number of Ingress Controller replicas. If not set, the default value is <literal>2</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>endpointPublishingStrategy</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>endpointPublishingStrategy</literal> is used to publish the Ingress Controller endpoints to other networks, enable load balancer integrations, and provide access to other systems.</simpara>
<simpara>On GCP, AWS, and Azure you can configure the following <literal>endpointPublishingStrategy</literal> fields:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>loadBalancer.scope</literal></simpara>
</listitem>
<listitem>
<simpara><literal>loadBalancer.allowedSourceRanges</literal></simpara>
</listitem>
</itemizedlist>
<simpara>If not set, the default value is based on <literal>infrastructure.config.openshift.io/cluster</literal> <literal>.status.platform</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>Azure: <literal>LoadBalancerService</literal> (with External scope)</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Platform (GCP): <literal>LoadBalancerService</literal> (with External scope)</simpara>
</listitem>
<listitem>
<simpara>Bare metal: <literal>NodePortService</literal></simpara>
</listitem>
<listitem>
<simpara>Other: <literal>HostNetwork</literal></simpara>
<note>
<simpara><literal>HostNetwork</literal> has a <literal>hostNetwork</literal> field with the following default values for the optional binding ports: <literal>httpPort: 80</literal>, <literal>httpsPort: 443</literal>, and <literal>statsPort: 1936</literal>.
With the binding ports, you can deploy multiple Ingress Controllers on the same node for the <literal>HostNetwork</literal> strategy.</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: internal
  namespace: openshift-ingress-operator
spec:
  domain: example.com
  endpointPublishingStrategy:
    type: HostNetwork
    hostNetwork:
      httpPort: 80
      httpsPort: 443
      statsPort: 1936</programlisting>
</para>
</formalpara>
</note>
<note>
<simpara>On Red Hat OpenStack Platform (RHOSP), the <literal>LoadBalancerService</literal> endpoint publishing strategy is only supported if a cloud provider is configured to create health monitors. For RHOSP 16.2, this strategy is only possible if you use the Amphora Octavia provider.</simpara>
<simpara>For more information, see the "Setting cloud provider options" section of the RHOSP installation documentation.</simpara>
</note>
<simpara>For most platforms, the <literal>endpointPublishingStrategy</literal> value can be updated. On GCP, you can configure the following <literal>endpointPublishingStrategy</literal> fields:</simpara>
</listitem>
<listitem>
<simpara><literal>loadBalancer.scope</literal></simpara>
</listitem>
<listitem>
<simpara><literal>loadbalancer.providerParameters.gcp.clientAccess</literal></simpara>
</listitem>
<listitem>
<simpara><literal>hostNetwork.protocol</literal></simpara>
</listitem>
<listitem>
<simpara><literal>nodePort.protocol</literal></simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>defaultCertificate</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>defaultCertificate</literal> value is a reference to a secret that contains the default certificate that is served by the Ingress Controller. When Routes do not specify their own certificate, <literal>defaultCertificate</literal> is used.</simpara>
<simpara>The secret must contain the following keys and data:
* <literal>tls.crt</literal>: certificate file contents
* <literal>tls.key</literal>: key file contents</simpara>
<simpara>If not set, a wildcard certificate is automatically generated and used. The certificate is valid for the Ingress Controller <literal>domain</literal> and <literal>subdomains</literal>, and
the generated certificate&#8217;s CA is automatically integrated with the
cluster&#8217;s trust store.</simpara>
<simpara>The in-use certificate, whether generated or user-specified, is automatically integrated with OpenShift Container Platform built-in OAuth server.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>namespaceSelector</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>namespaceSelector</literal> is used to filter the set of namespaces serviced by the
Ingress Controller. This is useful for implementing shards.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>routeSelector</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>routeSelector</literal> is used to filter the set of Routes serviced by the Ingress Controller. This is useful for implementing shards.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>nodePlacement</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>nodePlacement</literal> enables explicit control over the scheduling of the Ingress Controller.</simpara>
<simpara>If not set, the defaults values are used.</simpara>
<note>
<simpara>The <literal>nodePlacement</literal> parameter includes two parts, <literal>nodeSelector</literal> and <literal>tolerations</literal>. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">nodePlacement:
 nodeSelector:
   matchLabels:
     kubernetes.io/os: linux
 tolerations:
 - effect: NoSchedule
   operator: Exists</programlisting>
</note></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tlsSecurityProfile</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>tlsSecurityProfile</literal> specifies settings for TLS connections for Ingress Controllers.</simpara>
<simpara>If not set, the default value is based on the <literal>apiservers.config.openshift.io/cluster</literal> resource.</simpara>
<simpara>When using the <literal>Old</literal>, <literal>Intermediate</literal>, and <literal>Modern</literal> profile types, the effective profile configuration is subject to change between releases. For example, given a specification to use the <literal>Intermediate</literal> profile deployed on release <literal>X.Y.Z</literal>, an upgrade to release <literal>X.Y.Z+1</literal> may cause a new profile configuration to be applied to the Ingress Controller, resulting in a rollout.</simpara>
<simpara>The minimum TLS version for Ingress Controllers is <literal>1.1</literal>, and the maximum TLS version is <literal>1.3</literal>.</simpara>
<note>
<simpara>Ciphers and the minimum TLS version of the configured security profile are reflected in the <literal>TLSProfile</literal> status.</simpara>
</note>
<important>
<simpara>The Ingress Operator converts the TLS <literal>1.0</literal> of an <literal>Old</literal> or <literal>Custom</literal> profile to <literal>1.1</literal>.</simpara>
</important></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>clientTLS</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>clientTLS</literal> authenticates client access to the cluster and services; as a result, mutual TLS authentication is enabled. If not set, then client TLS is not enabled.</simpara>
<simpara><literal>clientTLS</literal> has the required subfields, <literal>spec.clientTLS.clientCertificatePolicy</literal> and <literal>spec.clientTLS.ClientCA</literal>.</simpara>
<simpara>The <literal>ClientCertificatePolicy</literal> subfield accepts one of the two values: <literal>Required</literal> or <literal>Optional</literal>. The <literal>ClientCA</literal> subfield specifies a config map that is in the openshift-config namespace. The config map should contain a CA certificate bundle.</simpara>
<simpara>The <literal>AllowedSubjectPatterns</literal> is an optional value that specifies a list of regular expressions, which are matched against the distinguished name on a valid client certificate to filter requests. The regular expressions must use PCRE syntax. At least one pattern must match a client certificate&#8217;s distinguished name; otherwise, the Ingress Controller rejects the certificate and denies the connection. If not specified, the Ingress Controller does not reject certificates based on the distinguished name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>routeAdmission</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>routeAdmission</literal> defines a policy for handling new route claims, such as allowing or denying claims across namespaces.</simpara>
<simpara><literal>namespaceOwnership</literal> describes how hostname claims across namespaces should be handled. The default is <literal>Strict</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Strict</literal>: does not allow routes to claim the same hostname across namespaces.</simpara>
</listitem>
<listitem>
<simpara><literal>InterNamespaceAllowed</literal>: allows routes to claim different paths of the same hostname across namespaces.</simpara>
</listitem>
</itemizedlist>
<simpara><literal>wildcardPolicy</literal> describes how routes with wildcard policies are handled by the Ingress Controller.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>WildcardsAllowed</literal>: Indicates routes with any wildcard policy are admitted by the Ingress Controller.</simpara>
</listitem>
<listitem>
<simpara><literal>WildcardsDisallowed</literal>: Indicates only routes with a wildcard policy of <literal>None</literal> are admitted by the Ingress Controller. Updating <literal>wildcardPolicy</literal> from <literal>WildcardsAllowed</literal> to <literal>WildcardsDisallowed</literal> causes admitted routes with a wildcard policy of <literal>Subdomain</literal> to stop working. These routes must be recreated to a wildcard policy of <literal>None</literal> to be readmitted by the Ingress Controller. <literal>WildcardsDisallowed</literal> is the default setting.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>IngressControllerLogging</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>logging</literal> defines parameters for what is logged where. If this field is empty, operational logs are enabled but access logs are disabled.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>access</literal> describes how client requests are logged. If this field is empty, access logging is disabled.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>destination</literal> describes a destination for log messages.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>type</literal> is the type of destination for logs:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Container</literal> specifies that logs should go to a sidecar container. The Ingress Operator configures the container, named <emphasis role="strong">logs</emphasis>, on the Ingress Controller pod and configures the Ingress Controller to write logs to the container. The expectation is that the administrator configures a custom logging solution that reads logs from this container. Using container logs means that logs may be dropped if the rate of logs exceeds the container runtime capacity or the custom logging solution capacity.</simpara>
</listitem>
<listitem>
<simpara><literal>Syslog</literal> specifies that logs are sent to a Syslog endpoint. The administrator must specify an endpoint that can receive Syslog messages. The expectation is that the administrator has configured a custom Syslog instance.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><literal>container</literal> describes parameters for the <literal>Container</literal> logging destination type. Currently there are no parameters for container logging, so this field must be empty.</simpara>
</listitem>
<listitem>
<simpara><literal>syslog</literal> describes parameters for the <literal>Syslog</literal> logging destination type:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>address</literal> is the IP address of the syslog endpoint that receives log messages.</simpara>
</listitem>
<listitem>
<simpara><literal>port</literal> is the UDP port number of the syslog endpoint that receives log messages.</simpara>
</listitem>
<listitem>
<simpara><literal>maxLength</literal> is the maximum length of the syslog message. It must be between <literal>480</literal> and <literal>4096</literal> bytes. If this field is empty, the maximum length is set to the default value of <literal>1024</literal> bytes.</simpara>
</listitem>
<listitem>
<simpara><literal>facility</literal> specifies the syslog facility of log messages. If this field is empty, the facility is <literal>local1</literal>. Otherwise, it must specify a valid syslog facility: <literal>kern</literal>, <literal>user</literal>, <literal>mail</literal>, <literal>daemon</literal>, <literal>auth</literal>, <literal>syslog</literal>, <literal>lpr</literal>, <literal>news</literal>, <literal>uucp</literal>, <literal>cron</literal>, <literal>auth2</literal>, <literal>ftp</literal>, <literal>ntp</literal>, <literal>audit</literal>, <literal>alert</literal>, <literal>cron2</literal>, <literal>local0</literal>, <literal>local1</literal>, <literal>local2</literal>, <literal>local3</literal>. <literal>local4</literal>, <literal>local5</literal>, <literal>local6</literal>, or <literal>local7</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><literal>httpLogFormat</literal> specifies the format of the log message for an HTTP request. If this field is empty, log messages use the implementation&#8217;s default HTTP log format. For HAProxy&#8217;s default HTTP log format, see <link xlink:href="http://cbonte.github.io/haproxy-dconv/2.0/configuration.html#8.2.3">the HAProxy documentation</link>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>httpHeaders</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>httpHeaders</literal> defines the policy for HTTP headers.</simpara>
<simpara>By setting the <literal>forwardedHeaderPolicy</literal> for the <literal>IngressControllerHTTPHeaders</literal>, you specify when and how the Ingress Controller sets the <literal>Forwarded</literal>, <literal>X-Forwarded-For</literal>, <literal>X-Forwarded-Host</literal>, <literal>X-Forwarded-Port</literal>, <literal>X-Forwarded-Proto</literal>, and <literal>X-Forwarded-Proto-Version</literal> HTTP headers.</simpara>
<simpara>By default, the policy is set to <literal>Append</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Append</literal> specifies that the Ingress Controller appends the headers, preserving any existing headers.</simpara>
</listitem>
<listitem>
<simpara><literal>Replace</literal> specifies that the Ingress Controller sets the headers, removing any existing headers.</simpara>
</listitem>
<listitem>
<simpara><literal>IfNone</literal> specifies that the Ingress Controller sets the headers if they are not already set.</simpara>
</listitem>
<listitem>
<simpara><literal>Never</literal> specifies that the Ingress Controller never sets the headers, preserving any existing headers.</simpara>
</listitem>
</itemizedlist>
<simpara>By setting <literal>headerNameCaseAdjustments</literal>, you can specify case adjustments that can be applied to HTTP header names. Each adjustment is specified as an HTTP header name with the desired capitalization. For example, specifying <literal>X-Forwarded-For</literal> indicates that the <literal>x-forwarded-for</literal> HTTP header should be adjusted to have the specified capitalization.</simpara>
<simpara>These adjustments are only applied to cleartext, edge-terminated, and re-encrypt routes, and only when using HTTP/1.</simpara>
<simpara>For request headers, these adjustments are applied only for routes that have the <literal>haproxy.router.openshift.io/h1-adjust-case=true</literal> annotation. For response headers, these adjustments are applied to all HTTP responses. If this field is empty, no request headers are adjusted.</simpara>
<simpara><literal>actions</literal> specifies options for performing certain actions on headers. Headers cannot be set or deleted for TLS passthrough connections. The <literal>actions</literal> field has additional subfields <literal>spec.httpHeader.actions.response</literal> and <literal>spec.httpHeader.actions.request</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>response</literal> subfield specifies a list of HTTP response headers to set or delete.</simpara>
</listitem>
<listitem>
<simpara>The <literal>request</literal> subfield specifies a list of HTTP request headers to set or delete.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>httpCompression</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>httpCompression</literal> defines the policy for HTTP traffic compression.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>mimeTypes</literal> defines a list of MIME types to which compression should be applied. For example, <literal>text/css; charset=utf-8</literal>, <literal>text/html</literal>, <literal>text/*</literal>, <literal>image/svg+xml</literal>, <literal>application/octet-stream</literal>, <literal>X-custom/customsub</literal>, using the format pattern, <literal>type/subtype; [;attribute=value]</literal>. The <literal>types</literal> are: application, image, message, multipart, text, video, or a custom type prefaced by <literal>X-</literal>; e.g. To see the full notation for MIME types and subtypes, see <link xlink:href="https://datatracker.ietf.org/doc/html/rfc1341#page-7">RFC1341</link></simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>httpErrorCodePages</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>httpErrorCodePages</literal> specifies custom HTTP error code response pages. By default, an IngressController uses error pages built into the IngressController image.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>httpCaptureCookies</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>httpCaptureCookies</literal> specifies HTTP cookies that you want to capture in access logs. If the <literal>httpCaptureCookies</literal> field is empty, the access logs do not capture the cookies.</simpara>
<simpara>For any cookie that you want to capture, the following parameters must be in your <literal>IngressController</literal> configuration:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>name</literal> specifies the name of the cookie.</simpara>
</listitem>
<listitem>
<simpara><literal>maxLength</literal> specifies tha maximum length of the cookie.</simpara>
</listitem>
<listitem>
<simpara><literal>matchType</literal> specifies if the field <literal>name</literal> of the cookie exactly matches the capture cookie setting or is a prefix of the capture cookie setting. The <literal>matchType</literal> field uses the <literal>Exact</literal> and <literal>Prefix</literal> parameters.</simpara>
</listitem>
</itemizedlist>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  httpCaptureCookies:
  - matchType: Exact
    maxLength: 128
    name: MYCOOKIE</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>httpCaptureHeaders</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>httpCaptureHeaders</literal> specifies the HTTP headers that you want to capture in the access logs. If the <literal>httpCaptureHeaders</literal> field is empty, the access logs do not capture the headers.</simpara>
<simpara><literal>httpCaptureHeaders</literal> contains two lists of headers to capture in the access logs. The two lists of header fields are <literal>request</literal> and <literal>response</literal>. In both lists, the <literal>name</literal> field must specify the header name and the <literal>maxlength</literal> field must specify the maximum length of the header. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  httpCaptureHeaders:
    request:
    - maxLength: 256
      name: Connection
    - maxLength: 128
      name: User-Agent
    response:
    - maxLength: 256
      name: Content-Type
    - maxLength: 256
      name: Content-Length</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tuningOptions</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>tuningOptions</literal> specifies options for tuning the performance of Ingress Controller pods.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>clientFinTimeout</literal> specifies how long a connection is held open while waiting for the client response to the server closing the connection. The default timeout is <literal>1s</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>clientTimeout</literal> specifies how long a connection is held open while waiting for a client response. The default timeout is <literal>30s</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>headerBufferBytes</literal> specifies how much memory is reserved, in bytes, for Ingress Controller connection sessions. This value must be at least <literal>16384</literal> if HTTP/2 is enabled for the Ingress Controller. If not set, the default value is <literal>32768</literal> bytes. Setting this field not recommended because <literal>headerBufferBytes</literal> values that are too small can break the Ingress Controller, and <literal>headerBufferBytes</literal> values that are too large could cause the Ingress Controller to use significantly more memory than necessary.</simpara>
</listitem>
<listitem>
<simpara><literal>headerBufferMaxRewriteBytes</literal> specifies how much memory should be reserved, in bytes, from <literal>headerBufferBytes</literal> for HTTP header rewriting and appending for Ingress Controller connection sessions. The minimum value for <literal>headerBufferMaxRewriteBytes</literal> is <literal>4096</literal>. <literal>headerBufferBytes</literal> must be greater than <literal>headerBufferMaxRewriteBytes</literal> for incoming HTTP requests. If not set, the default value is <literal>8192</literal> bytes. Setting this field not recommended because <literal>headerBufferMaxRewriteBytes</literal> values that are too small can break the Ingress Controller and <literal>headerBufferMaxRewriteBytes</literal> values that are too large could cause the Ingress Controller to use significantly more memory than necessary.</simpara>
</listitem>
<listitem>
<simpara><literal>healthCheckInterval</literal> specifies how long the router waits between health checks. The default is <literal>5s</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>serverFinTimeout</literal> specifies how long a connection is held open while waiting for the server response to the client that is closing the connection. The default timeout is <literal>1s</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>serverTimeout</literal> specifies how long a connection is held open while waiting for a server response. The default timeout is <literal>30s</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>threadCount</literal> specifies the number of threads to create per HAProxy process. Creating more threads allows each Ingress Controller pod to handle more connections, at the cost of more system resources being used. HAProxy
supports up to <literal>64</literal> threads. If this field is empty, the Ingress Controller uses the default value of <literal>4</literal> threads. The default value can change in future releases. Setting this field is not recommended because increasing the number of HAProxy threads allows Ingress Controller pods to use more CPU time under load, and prevent other pods from receiving the CPU resources they need to perform. Reducing the number of threads can cause the Ingress Controller to perform poorly.</simpara>
</listitem>
<listitem>
<simpara><literal>tlsInspectDelay</literal> specifies how long the router can hold data to find a matching route. Setting this value too short can cause the router to fall back to the default certificate for edge-terminated, reencrypted, or passthrough routes, even when using a better matched certificate. The default inspect delay is <literal>5s</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>tunnelTimeout</literal> specifies how long a tunnel connection, including websockets, remains open while the tunnel is idle. The default timeout is <literal>1h</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>maxConnections</literal> specifies the maximum number of simultaneous connections that can be established per HAProxy process. Increasing this value allows each ingress controller pod to handle more connections at the cost of additional system resources. Permitted values are <literal>0</literal>, <literal>-1</literal>, any value within the range <literal>2000</literal> and <literal>2000000</literal>, or the field can be left empty.</simpara>
<itemizedlist>
<listitem>
<simpara>If this field is left empty or has the value <literal>0</literal>, the Ingress Controller will use the default value of <literal>50000</literal>. This value is subject to change in future releases.</simpara>
</listitem>
<listitem>
<simpara>If the field has the value of <literal>-1</literal>, then HAProxy will dynamically compute a maximum value based on the available <literal>ulimits</literal> in the running container. This process results in a large computed value that will incur significant memory usage compared to the current default value of <literal>50000</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the field has a value that is greater than the current operating system limit, the HAProxy process will not start.</simpara>
</listitem>
<listitem>
<simpara>If you choose a discrete value and the router pod is migrated to a new node, it is possible the new node does not have an identical <literal>ulimit</literal> configured. In such cases, the pod fails to start.</simpara>
</listitem>
<listitem>
<simpara>If you have nodes with different <literal>ulimits</literal> configured, and you choose a discrete value, it is recommended to use the value of <literal>-1</literal> for this field so that the maximum number of connections is calculated at runtime.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>logEmptyRequests</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>logEmptyRequests</literal> specifies connections for which no request is received and logged. These empty requests come from load balancer health probes or web browser speculative connections (preconnect) and logging these requests can be undesirable. However, these requests can be caused by network errors, in which case logging empty requests can be useful for diagnosing the errors. These requests can be caused by port scans, and logging empty requests can aid in detecting intrusion attempts. Allowed values for this field are <literal>Log</literal> and <literal>Ignore</literal>. The default value is <literal>Log</literal>.</simpara>
<simpara>The <literal>LoggingPolicy</literal> type accepts either one of two values:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Log</literal>: Setting this value to <literal>Log</literal> indicates that an event should be logged.</simpara>
</listitem>
<listitem>
<simpara><literal>Ignore</literal>: Setting this value to <literal>Ignore</literal> sets the <literal>dontlognull</literal> option in the HAproxy configuration.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>HTTPEmptyRequestsPolicy</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>HTTPEmptyRequestsPolicy</literal> describes how HTTP connections are handled if the connection times out before a request is received. Allowed values for this field are <literal>Respond</literal> and <literal>Ignore</literal>. The default value is <literal>Respond</literal>.</simpara>
<simpara>The <literal>HTTPEmptyRequestsPolicy</literal> type accepts either one of two values:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Respond</literal>: If the field is set to <literal>Respond</literal>, the Ingress Controller sends an HTTP <literal>400</literal> or <literal>408</literal> response, logs the connection if access logging is enabled, and counts the connection in the appropriate metrics.</simpara>
</listitem>
<listitem>
<simpara><literal>Ignore</literal>: Setting this option to <literal>Ignore</literal> adds the <literal>http-ignore-probes</literal> parameter in the HAproxy configuration. If the field is set to <literal>Ignore</literal>, the Ingress Controller closes the connection without sending a response, then logs the connection, or incrementing metrics.</simpara>
</listitem>
</itemizedlist>
<simpara>These connections come from load balancer health probes or web browser speculative connections (preconnect) and can be safely ignored. However, these requests can be caused by network errors, so setting this field to <literal>Ignore</literal> can impede detection and diagnosis of problems. These requests can be caused by port scans, in which case logging empty requests can aid in detecting intrusion attempts.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<note>
<simpara>All parameters are optional.</simpara>
</note>
<section xml:id="configuring-ingress-controller-tls">
<title>Ingress Controller TLS security profiles</title>
<simpara>TLS security profiles provide a way for servers to regulate which ciphers a connecting client can use when connecting to the server.</simpara>
<section xml:id="tls-profiles-understanding_configuring-ingress">
<title>Understanding TLS security profiles</title>
<simpara>You can use a TLS (Transport Layer Security) security profile to define which TLS ciphers are required by various OpenShift Container Platform components. The OpenShift Container Platform TLS security profiles are based on <link xlink:href="https://wiki.mozilla.org/Security/Server_Side_TLS">Mozilla recommended configurations</link>.</simpara>
<simpara>You can specify one of the following TLS security profiles for each component:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>TLS security profiles</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="66.6667*"/>
<thead>
<row>
<entry align="left" valign="top">Profile</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>Old</literal></simpara></entry>
<entry align="left" valign="top"><simpara>This profile is intended for use with legacy clients or libraries. The profile is based on the <link xlink:href="https://wiki.mozilla.org/Security/Server_Side_TLS#Old_backward_compatibility">Old backward compatibility</link> recommended configuration.</simpara>
<simpara>The <literal>Old</literal> profile requires a minimum TLS version of 1.0.</simpara>
<note>
<simpara>For the Ingress Controller, the minimum TLS version is converted from 1.0 to 1.1.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Intermediate</literal></simpara></entry>
<entry align="left" valign="top"><simpara>This profile is the recommended configuration for the majority of clients. It is the  default TLS security profile for the Ingress Controller, kubelet, and control plane. The profile is based on the <link xlink:href="https://wiki.mozilla.org/Security/Server_Side_TLS#Intermediate_compatibility_.28recommended.29">Intermediate compatibility</link> recommended configuration.</simpara>
<simpara>The <literal>Intermediate</literal> profile requires a minimum TLS version of 1.2.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Modern</literal></simpara></entry>
<entry align="left" valign="top"><simpara>This profile is intended for use with modern clients that have no need for backwards compatibility. This profile is based on the <link xlink:href="https://wiki.mozilla.org/Security/Server_Side_TLS#Modern_compatibility">Modern compatibility</link> recommended configuration.</simpara>
<simpara>The <literal>Modern</literal> profile requires a minimum TLS version of 1.3.</simpara>
<note>
<simpara>In OpenShift Container Platform 4.6, 4.7, and 4.8, the <literal>Modern</literal> profile is unsupported. If selected, the <literal>Intermediate</literal> profile is enabled.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Custom</literal></simpara></entry>
<entry align="left" valign="top"><simpara>This profile allows you to define the TLS version and ciphers to use.</simpara>
<warning>
<simpara>Use caution when using a <literal>Custom</literal> profile, because invalid configurations can cause problems.</simpara>
</warning></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>When using one of the predefined profile types, the effective profile configuration is subject to change between releases. For example, given a specification to use the Intermediate profile deployed on release X.Y.Z, an upgrade to release X.Y.Z+1 might cause a new profile configuration to be applied, resulting in a rollout.</simpara>
</note>
</section>
<section xml:id="tls-profiles-ingress-configuring_configuring-ingress">
<title>Configuring the TLS security profile for the Ingress Controller</title>
<simpara>To configure a TLS security profile for an Ingress Controller, edit the <literal>IngressController</literal> custom resource (CR) to specify a predefined or custom TLS security profile. If a TLS security profile is not configured, the default value is based on the TLS security profile set for the API server.</simpara>
<formalpara>
<title>Sample <literal>IngressController</literal> CR that configures the <literal>Old</literal> TLS security profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
 ...
spec:
  tlsSecurityProfile:
    old: {}
    type: Old
 ...</programlisting>
</para>
</formalpara>
<simpara>The TLS security profile defines the minimum TLS version and the TLS ciphers for TLS connections for Ingress Controllers.</simpara>
<simpara>You can see the ciphers and the minimum TLS version of the configured TLS security profile in the <literal>IngressController</literal> custom resource (CR) under <literal>Status.Tls Profile</literal> and the configured TLS security profile under <literal>Spec.Tls Security Profile</literal>. For the <literal>Custom</literal> TLS security profile, the specific ciphers and minimum TLS version are listed under both parameters.</simpara>
<note>
<simpara>The HAProxy Ingress Controller image supports TLS <literal>1.3</literal> and the <literal>Modern</literal> profile.</simpara>
<simpara>The Ingress Operator also converts the TLS <literal>1.0</literal> of an <literal>Old</literal> or <literal>Custom</literal> profile to <literal>1.1</literal>.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>IngressController</literal> CR in the <literal>openshift-ingress-operator</literal> project to configure the TLS security profile:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit IngressController default -n openshift-ingress-operator</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>spec.tlsSecurityProfile</literal> field:</simpara>
<formalpara>
<title>Sample <literal>IngressController</literal> CR for a <literal>Custom</literal> profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
 ...
spec:
  tlsSecurityProfile:
    type: Custom <co xml:id="CO8-1"/>
    custom: <co xml:id="CO8-2"/>
      ciphers: <co xml:id="CO8-3"/>
      - ECDHE-ECDSA-CHACHA20-POLY1305
      - ECDHE-RSA-CHACHA20-POLY1305
      - ECDHE-RSA-AES128-GCM-SHA256
      - ECDHE-ECDSA-AES128-GCM-SHA256
      minTLSVersion: VersionTLS11
 ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO8-1">
<para>Specify the TLS security profile type (<literal>Old</literal>, <literal>Intermediate</literal>, or <literal>Custom</literal>). The default is <literal>Intermediate</literal>.</para>
</callout>
<callout arearefs="CO8-2">
<para>Specify the appropriate field for the selected type:</para>
<itemizedlist>
<listitem>
<simpara><literal>old: {}</literal></simpara>
</listitem>
<listitem>
<simpara><literal>intermediate: {}</literal></simpara>
</listitem>
<listitem>
<simpara><literal>custom:</literal></simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO8-3">
<para>For the <literal>custom</literal> type, specify a list of TLS ciphers and minimum accepted TLS version.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the profile is set in the <literal>IngressController</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe IngressController default -n openshift-ingress-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         default
Namespace:    openshift-ingress-operator
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  operator.openshift.io/v1
Kind:         IngressController
 ...
Spec:
 ...
  Tls Security Profile:
    Custom:
      Ciphers:
        ECDHE-ECDSA-CHACHA20-POLY1305
        ECDHE-RSA-CHACHA20-POLY1305
        ECDHE-RSA-AES128-GCM-SHA256
        ECDHE-ECDSA-AES128-GCM-SHA256
      Min TLS Version:  VersionTLS11
    Type:               Custom
 ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-mutual-tls-auth_configuring-ingress">
<title>Configuring mutual TLS authentication</title>
<simpara>You can configure the Ingress Controller to enable mutual TLS (mTLS) authentication by setting a <literal>spec.clientTLS</literal> value. The <literal>clientTLS</literal> value configures the Ingress Controller to verify client certificates. This configuration includes setting a <literal>clientCA</literal> value, which is a reference to a config map. The config map contains the PEM-encoded CA certificate bundle that is used to verify a client&#8217;s certificate. Optionally, you can also configure a list of certificate subject filters.</simpara>
<simpara>If the <literal>clientCA</literal> value specifies an X509v3 certificate revocation list (CRL) distribution point, the Ingress Operator downloads and manages a CRL config map based on the HTTP URI X509v3 <literal>CRL Distribution Point</literal> specified in each provided certificate. The Ingress Controller uses this config map during mTLS/TLS negotiation. Requests that do not provide valid certificates are rejected.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have a PEM-encoded CA certificate bundle.</simpara>
</listitem>
<listitem>
<simpara>If your CA bundle references a CRL distribution point, you must have also included the end-entity or leaf certificate to the client CA bundle. This certificate must have included an HTTP URI under <literal>CRL Distribution Points</literal>, as described in RFC 5280. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> Issuer: C=US, O=Example Inc, CN=Example Global G2 TLS RSA SHA256 2020 CA1
         Subject: SOME SIGNED CERT            X509v3 CRL Distribution Points:
                Full Name:
                  URI:http://crl.example.com/example.crl</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <literal>openshift-config</literal> namespace, create a config map from your CA bundle:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap \
   router-ca-certs-default \
   --from-file=ca-bundle.pem=client-ca.crt \<co xml:id="CO9-1"/>
   -n openshift-config</programlisting>
<calloutlist>
<callout arearefs="CO9-1">
<para>The config map data key must be <literal>ca-bundle.pem</literal>, and the data value must be a CA certificate in PEM format.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>IngressController</literal> resource in the <literal>openshift-ingress-operator</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit IngressController default -n openshift-ingress-operator</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>spec.clientTLS</literal> field and subfields to configure mutual TLS:</simpara>
<formalpara>
<title>Sample <literal>IngressController</literal> CR for a <literal>clientTLS</literal> profile that specifies filtering patterns</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">  apiVersion: operator.openshift.io/v1
  kind: IngressController
  metadata:
    name: default
    namespace: openshift-ingress-operator
  spec:
    clientTLS:
      clientCertificatePolicy: Required
      clientCA:
        name: router-ca-certs-default
      allowedSubjectPatterns:
      - "^/CN=example.com/ST=NC/C=US/O=Security/OU=OpenShift$"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="nw-ingress-view_configuring-ingress">
<title>View the default Ingress Controller</title>
<simpara>The Ingress Operator is a core feature of OpenShift Container Platform and is enabled out of the
box.</simpara>
<simpara>Every new OpenShift Container Platform installation has an <literal>ingresscontroller</literal> named default. It
can be supplemented with additional Ingress Controllers. If the default
<literal>ingresscontroller</literal> is deleted, the Ingress Operator will automatically recreate it
within a minute.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>View the default Ingress Controller:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe --namespace=openshift-ingress-operator ingresscontroller/default</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-operator-status_configuring-ingress">
<title>View Ingress Operator status</title>
<simpara>You can view and inspect the status of your Ingress Operator.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>View your Ingress Operator status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe clusteroperators/ingress</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-operator-logs_configuring-ingress">
<title>View Ingress Controller logs</title>
<simpara>You can view your Ingress Controller logs.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>View your Ingress Controller logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs --namespace=openshift-ingress-operator deployments/ingress-operator -c &lt;container_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-controller-status_configuring-ingress">
<title>View Ingress Controller status</title>
<simpara>Your can view the status of a particular Ingress Controller.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>View the status of an Ingress Controller:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe --namespace=openshift-ingress-operator ingresscontroller/&lt;name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-ingress-controller">
<title>Configuring the Ingress Controller</title>
<section xml:id="nw-ingress-setting-a-custom-default-certificate_configuring-ingress">
<title>Setting a custom default certificate</title>
<simpara>As an administrator, you can configure an Ingress Controller to use a custom
certificate by creating a Secret resource and editing the <literal>IngressController</literal>
custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a certificate/key pair in PEM-encoded files, where the
certificate is signed by a trusted certificate authority or by a private trusted
certificate authority that you configured in a custom PKI.</simpara>
</listitem>
<listitem>
<simpara>Your certificate meets the following requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>The certificate is valid for the ingress domain.</simpara>
</listitem>
<listitem>
<simpara>The certificate uses the <literal>subjectAltName</literal> extension to specify a wildcard domain, such as <literal>*.apps.ocp4.example.com</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You must have an <literal>IngressController</literal> CR. You may use the default one:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc --namespace openshift-ingress-operator get ingresscontrollers</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      AGE
default   10m</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<note>
<simpara>If you have intermediate certificates, they must be included in the <literal>tls.crt</literal>
file of the secret containing a custom default certificate. Order matters when
specifying a certificate; list your intermediate certificate(s) after any server
certificate(s).</simpara>
</note>
<formalpara>
<title>Procedure</title>
<para>The following assumes that the custom certificate and key pair are in the
<literal>tls.crt</literal> and <literal>tls.key</literal> files in the current working directory. Substitute the
actual path names for <literal>tls.crt</literal> and <literal>tls.key</literal>. You also may substitute another
name for <literal>custom-certs-default</literal> when creating the Secret resource and
referencing it in the IngressController CR.</para>
</formalpara>
<note>
<simpara>This action will cause the Ingress Controller to be redeployed, using a rolling deployment strategy.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a Secret resource containing the custom certificate in the
<literal>openshift-ingress</literal> namespace using the <literal>tls.crt</literal> and <literal>tls.key</literal> files.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc --namespace openshift-ingress create secret tls custom-certs-default --cert=tls.crt --key=tls.key</programlisting>
</listitem>
<listitem>
<simpara>Update the IngressController CR to reference the new certificate secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch --type=merge --namespace openshift-ingress-operator ingresscontrollers/default \
  --patch '{"spec":{"defaultCertificate":{"name":"custom-certs-default"}}}'</programlisting>
</listitem>
<listitem>
<simpara>Verify the update was effective:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo Q |\
  openssl s_client -connect console-openshift-console.apps.&lt;domain&gt;:443 -showcerts 2&gt;/dev/null |\
  openssl x509 -noout -subject -issuer -enddate</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;domain&gt;</literal></term>
<listitem>
<simpara>Specifies the base domain name for your cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">subject=C = US, ST = NC, L = Raleigh, O = RH, OU = OCP4, CN = *.apps.example.com
issuer=C = US, ST = NC, L = Raleigh, O = RH, OU = OCP4, CN = example.com
notAfter=May 10 08:32:45 2022 GM</programlisting>
</para>
</formalpara>
<tip>
<simpara>You can alternatively apply the following YAML to set a custom default certificate:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  defaultCertificate:
    name: custom-certs-default</programlisting>
</tip>
<simpara>The certificate secret name should match the value used to update the CR.</simpara>
</listitem>
</orderedlist>
<simpara>Once the IngressController CR has been modified, the Ingress Operator
updates the Ingress Controller&#8217;s deployment to use the custom certificate.</simpara>
</section>
<section xml:id="nw-ingress-custom-default-certificate-remove_configuring-ingress">
<title>Removing a custom default certificate</title>
<simpara>As an administrator, you can remove a custom certificate that you configured an Ingress Controller to use.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You previously configured a custom default certificate for the Ingress Controller.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To remove the custom certificate and restore the certificate that ships with OpenShift Container Platform, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch -n openshift-ingress-operator ingresscontrollers/default \
  --type json -p $'- op: remove\n  path: /spec/defaultCertificate'</programlisting>
<simpara>There can be a delay while the cluster reconciles the new certificate configuration.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To confirm that the original cluster certificate is restored, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo Q | \
  openssl s_client -connect console-openshift-console.apps.&lt;domain&gt;:443 -showcerts 2&gt;/dev/null | \
  openssl x509 -noout -subject -issuer -enddate</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;domain&gt;</literal></term>
<listitem>
<simpara>Specifies the base domain name for your cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">subject=CN = *.apps.&lt;domain&gt;
issuer=CN = ingress-operator@1620633373
notAfter=May 10 10:44:36 2023 GMT</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-autoscaling-ingress-controller_configuring-ingress">
<title>Autoscaling an Ingress Controller</title>
<simpara>Automatically scale an Ingress Controller to dynamically meet routing performance or availability requirements such as the requirement to increase throughput. The following procedure provides an example for scaling up the default <literal>IngressController</literal>.</simpara>
<orderedlist numeration="arabic">
<title>Prerequisites</title>
<listitem>
<simpara>You have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have the Custom Metrics Autoscaler Operator installed.</simpara>
</listitem>
<listitem>
<simpara>You are in the <literal>openshift-ingress-operator</literal> project namespace.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a service account to authenticate with Thanos by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create serviceaccount thanos &amp;&amp; oc describe serviceaccount thanos</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:                thanos
Namespace:           openshift-ingress-operator
Labels:              &lt;none&gt;
Annotations:         &lt;none&gt;
Image pull secrets:  thanos-dockercfg-b4l9s
Mountable secrets:   thanos-dockercfg-b4l9s
Tokens:              thanos-token-c422q
Events:              &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Define a <literal>TriggerAuthentication</literal> object within the <literal>openshift-ingress-operator</literal> namespace using the service account&#8217;s token.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Define the variable <literal>secret</literal> that contains the secret by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ secret=$(oc get secret | grep thanos-token | head -n 1 | awk '{ print $1 }')</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>TriggerAuthentication</literal> object and pass the value of the <literal>secret</literal> variable to the <literal>TOKEN</literal> parameter:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc process TOKEN="$secret" -f - &lt;&lt;EOF | oc apply -f -
apiVersion: template.openshift.io/v1
kind: Template
parameters:
- name: TOKEN
objects:
- apiVersion: keda.sh/v1alpha1
  kind: TriggerAuthentication
  metadata:
    name: keda-trigger-auth-prometheus
  spec:
    secretTargetRef:
    - parameter: bearerToken
      name: \${TOKEN}
      key: token
    - parameter: ca
      name: \${TOKEN}
      key: ca.crt
EOF</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create and apply a role for reading metrics from Thanos:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a new role, <literal>thanos-metrics-reader.yaml</literal>, that reads metrics from pods and nodes:</simpara>
<formalpara>
<title>thanos-metrics-reader.yaml</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: thanos-metrics-reader
rules:
- apiGroups:
  - ""
  resources:
  - pods
  - nodes
  verbs:
  - get
- apiGroups:
  - metrics.k8s.io
  resources:
  - pods
  - nodes
  verbs:
  - get
  - list
  - watch
- apiGroups:
  - ""
  resources:
  - namespaces
  verbs:
  - get</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the new role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f thanos-metrics-reader.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Add the new role to the service account by entering the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-role-to-user thanos-metrics-reader -z thanos --role-namespace=openshift-ingress-operator</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy -n openshift-ingress-operator add-cluster-role-to-user cluster-monitoring-view -z thanos</programlisting>
<note>
<simpara>The argument <literal>add-cluster-role-to-user</literal> is only required if you use cross-namespace queries. The following step uses a query from the <literal>kube-metrics</literal> namespace which requires this argument.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create a new <literal>ScaledObject</literal> YAML file, <literal>ingress-autoscaler.yaml</literal>, that targets the default Ingress Controller deployment:</simpara>
<formalpara>
<title>Example <literal>ScaledObject</literal> definition</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: keda.sh/v1alpha1
kind: ScaledObject
metadata:
  name: ingress-scaler
spec:
  scaleTargetRef: <co xml:id="CO10-1"/>
    apiVersion: operator.openshift.io/v1
    kind: IngressController
    name: default
    envSourceContainerName: ingress-operator
  minReplicaCount: 1
  maxReplicaCount: 20 <co xml:id="CO10-2"/>
  cooldownPeriod: 1
  pollingInterval: 1
  triggers:
  - type: prometheus
    metricType: AverageValue
    metadata:
      serverAddress: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091 <co xml:id="CO10-3"/>
      namespace: openshift-ingress-operator <co xml:id="CO10-4"/>
      metricName: 'kube-node-role'
      threshold: '1'
      query: 'sum(kube_node_role{role="worker",service="kube-state-metrics"})' <co xml:id="CO10-5"/>
      authModes: "bearer"
    authenticationRef:
      name: keda-trigger-auth-prometheus</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO10-1">
<para>The custom resource that you are targeting. In this case, the Ingress Controller.</para>
</callout>
<callout arearefs="CO10-2">
<para>Optional: The maximum number of replicas. If you omit this field, the default maximum is set to 100 replicas.</para>
</callout>
<callout arearefs="CO10-3">
<para>The Thanos service endpoint in the <literal>openshift-monitoring</literal> namespace.</para>
</callout>
<callout arearefs="CO10-4">
<para>The Ingress Operator namespace.</para>
</callout>
<callout arearefs="CO10-5">
<para>This expression evaluates to however many worker nodes are present in the deployed cluster.</para>
</callout>
</calloutlist>
<important>
<simpara>If you are using cross-namespace queries, you must target port 9091 and not port 9092 in the <literal>serverAddress</literal> field. You also must have elevated privileges to read metrics from this port.</simpara>
</important>
</listitem>
<listitem>
<simpara>Apply the custom resource definition by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ingress-autoscaler.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the default Ingress Controller is scaled out to match the value returned by the <literal>kube-state-metrics</literal> query by running the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>Use the <literal>grep</literal> command to search the Ingress Controller YAML file for replicas:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ingresscontroller/default -o yaml | grep replicas:</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">replicas: 3</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the pods in the <literal>openshift-ingress</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ingress</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
router-default-7b5df44ff-l9pmm   2/2     Running   0          17h
router-default-7b5df44ff-s5sl5   2/2     Running   0          3d22h
router-default-7b5df44ff-wwsth   2/2     Running   0          66s</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#enabling-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-cma-autoscaling-custom-install">Installing the custom metrics autoscaler</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-cma-autoscaling-custom-trigger-auth">Understanding custom metrics autoscaler trigger authentications</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-cma-autoscaling-custom-prometheus">Configuring the custom metrics autoscaler to use OpenShift Container Platform monitoring</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-cma-autoscaling-custom-adding">Understanding how to add custom metrics autoscalers</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-controller-configuration_configuring-ingress">
<title>Scaling an Ingress Controller</title>
<simpara>Manually scale an Ingress Controller to meeting routing performance or
availability requirements such as the requirement to increase throughput. <literal>oc</literal>
commands are used to scale the <literal>IngressController</literal> resource. The following
procedure provides an example for scaling up the default <literal>IngressController</literal>.</simpara>
<note>
<simpara>Scaling is not an immediate action, as it takes time to create the desired number of replicas.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the current number of available replicas for the default <literal>IngressController</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-ingress-operator ingresscontrollers/default -o jsonpath='{$.status.availableReplicas}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Scale the default <literal>IngressController</literal> to the desired number of replicas using
the <literal>oc patch</literal> command. The following example scales the default <literal>IngressController</literal>
to 3 replicas:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{"spec":{"replicas": 3}}' --type=merge</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ingresscontroller.operator.openshift.io/default patched</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the default <literal>IngressController</literal> scaled to the number of replicas
that you specified:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-ingress-operator ingresscontrollers/default -o jsonpath='{$.status.availableReplicas}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">3</programlisting>
</para>
</formalpara>
<tip>
<simpara>You can alternatively apply the following YAML to scale an Ingress Controller to three replicas:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 3               <co xml:id="CO11-1"/></programlisting>
</tip>
<calloutlist>
<callout arearefs="CO11-1">
<para>If you need a different amount of replicas, change the <literal>replicas</literal> value.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-configure-ingress-access-logging_configuring-ingress">
<title>Configuring Ingress access logging</title>
<simpara>You can configure the Ingress Controller to enable access logs. If you have clusters that do not receive much traffic, then you can log to a sidecar. If you have high traffic clusters, to avoid exceeding the capacity of the logging stack or  to integrate with a logging infrastructure outside of OpenShift Container Platform, you can forward logs to a custom syslog endpoint. You can also specify the format for access logs.</simpara>
<simpara>Container logging is useful to enable access logs on low-traffic clusters when there is no existing Syslog logging infrastructure, or for short-term use while diagnosing problems with the Ingress Controller.</simpara>
<simpara>Syslog is needed for high-traffic clusters where access logs could exceed the OpenShift Logging stack&#8217;s capacity, or for environments where any logging solution needs to integrate with an existing Syslog logging infrastructure. The Syslog use-cases can overlap.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>Configure Ingress access logging to a sidecar.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>To configure Ingress access logging, you must specify a destination using <literal>spec.logging.access.destination</literal>. To specify logging to a sidecar container, you must specify <literal>Container</literal> <literal>spec.logging.access.destination.type</literal>. The following example is an Ingress Controller definition that logs to a <literal>Container</literal> destination:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Container</programlisting>
</listitem>
<listitem>
<simpara>When you configure the Ingress Controller to log to a sidecar, the operator creates a container named <literal>logs</literal> inside the Ingress Controller Pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress logs deployment.apps/router-default -c logs</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2020-05-11T19:11:50.135710+00:00 router-default-57dfc6cd95-bpmk6 router-default-57dfc6cd95-bpmk6 haproxy[108]: 174.19.21.82:39654 [11/May/2020:19:11:50.133] public be_http:hello-openshift:hello-openshift/pod:hello-openshift:hello-openshift:10.128.2.12:8080 0/0/1/0/1 200 142 - - --NI 1/1/0/0/0 0/0 "GET / HTTP/1.1"</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<simpara>Configure Ingress access logging to a Syslog endpoint.</simpara>
<itemizedlist>
<listitem>
<simpara>To configure Ingress access logging, you must specify a destination using <literal>spec.logging.access.destination</literal>. To specify logging to a Syslog endpoint destination, you must specify <literal>Syslog</literal> for <literal>spec.logging.access.destination.type</literal>. If the destination type is <literal>Syslog</literal>, you must also specify a destination endpoint using <literal>spec.logging.access.destination.syslog.endpoint</literal> and you can specify a facility using <literal>spec.logging.access.destination.syslog.facility</literal>. The following example is an Ingress Controller definition that logs to a <literal>Syslog</literal> destination:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Syslog
        syslog:
          address: 1.2.3.4
          port: 10514</programlisting>
<note>
<simpara>The <literal>syslog</literal> destination port must be UDP.</simpara>
</note>
</listitem>
</itemizedlist>
<simpara>Configure Ingress access logging with a specific log format.</simpara>
<itemizedlist>
<listitem>
<simpara>You can specify <literal>spec.logging.access.httpLogFormat</literal> to customize the log format. The following example is an Ingress Controller definition that logs to a <literal>syslog</literal> endpoint with IP address 1.2.3.4 and port 10514:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Syslog
        syslog:
          address: 1.2.3.4
          port: 10514
      httpLogFormat: '%ci:%cp [%t] %ft %b/%s %B %bq %HM %HU %HV'</programlisting>
</listitem>
</itemizedlist>
<simpara>Disable Ingress access logging.</simpara>
<itemizedlist>
<listitem>
<simpara>To disable Ingress access logging, leave <literal>spec.logging</literal> or <literal>spec.logging.access</literal> empty:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access: null</programlisting>
</listitem>
</itemizedlist>
<simpara>Allow the Ingress Controller to modify the HAProxy log length when using a sidecar.</simpara>
<itemizedlist>
<listitem>
<simpara>Use <literal>spec.logging.access.destination.syslog.maxLength</literal> if you are using <literal>spec.logging.access.destination.type: Syslog</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Syslog
        syslog:
          address: 1.2.3.4
          maxLength: 4096
          port: 10514</programlisting>
</listitem>
<listitem>
<simpara>Use <literal>spec.logging.access.destination.container.maxLength</literal> if you are using <literal>spec.logging.access.destination.type: Container</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  replicas: 2
  logging:
    access:
      destination:
        type: Container
        container:
          maxLength: 8192</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-setting-thread-count_configuring-ingress">
<title>Setting Ingress Controller thread count</title>
<simpara>A cluster administrator can set the thread count to increase the amount of incoming connections a cluster can handle. You can patch an existing Ingress Controller to increase the amount of threads.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The following assumes that you already created an Ingress Controller.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Update the Ingress Controller to increase the number of threads:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge -p '{"spec":{"tuningOptions": {"threadCount": 8}}}'</programlisting>
<note>
<simpara>If you have a node that is capable of running large amounts of resources, you can configure <literal>spec.nodePlacement.nodeSelector</literal> with labels that match the capacity of the intended node, and configure <literal>spec.tuningOptions.threadCount</literal> to an appropriately high value.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-setting-internal-lb_configuring-ingress">
<title>Configuring an Ingress Controller to use an internal load balancer</title>
<simpara>When creating an Ingress Controller on cloud platforms, the Ingress Controller is published by a public cloud load balancer by default.
As an administrator, you can create an Ingress Controller that uses an internal cloud load balancer.</simpara>
<warning>
<simpara>If your cloud provider is Microsoft Azure, you must have at least one public load balancer that points to your nodes.
If you do not, all of your nodes will lose egress connectivity to the internet.</simpara>
</warning>
<important>
<simpara>If you want to change the <literal>scope</literal> for an <literal>IngressController</literal>, you can change the <literal>.spec.endpointPublishingStrategy.loadBalancer.scope</literal> parameter after the custom resource (CR) is created.</simpara>
</important>
<figure>
<title>Diagram of LoadBalancer</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/202_OpenShift_Ingress_0222_load_balancer.png"/>
</imageobject>
<textobject><phrase>OpenShift Container Platform Ingress LoadBalancerService endpoint publishing strategy</phrase></textobject>
</mediaobject>
</figure>
<simpara>The preceding graphic shows the following concepts pertaining to OpenShift Container Platform Ingress LoadBalancerService endpoint publishing strategy:</simpara>
<itemizedlist>
<listitem>
<simpara>You can load balance externally, using the cloud provider load balancer, or internally, using the OpenShift Ingress Controller Load Balancer.</simpara>
</listitem>
<listitem>
<simpara>You can use the single IP address of the load balancer and more familiar ports, such as 8080 and 4200 as shown on the cluster depicted in the graphic.</simpara>
</listitem>
<listitem>
<simpara>Traffic from the external load balancer is directed at the pods, and managed by the load balancer, as depicted in the instance of a down node.
See the <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/service/#internal-load-balancer">Kubernetes Services documentation</link>
for implementation details.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>IngressController</literal> custom resource (CR) in a file named <literal>&lt;name&gt;-ingress-controller.yaml</literal>, such as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: &lt;name&gt; <co xml:id="CO12-1"/>
spec:
  domain: &lt;domain&gt; <co xml:id="CO12-2"/>
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: Internal <co xml:id="CO12-3"/></programlisting>
<calloutlist>
<callout arearefs="CO12-1">
<para>Replace <literal>&lt;name&gt;</literal> with a name for the <literal>IngressController</literal> object.</para>
</callout>
<callout arearefs="CO12-2">
<para>Specify the <literal>domain</literal> for the application published by the controller.</para>
</callout>
<callout arearefs="CO12-3">
<para>Specify a value of <literal>Internal</literal> to use an internal load balancer.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the Ingress Controller defined in the previous step by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;-ingress-controller.yaml <co xml:id="CO13-1"/></programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>Replace <literal>&lt;name&gt;</literal> with the name of the <literal>IngressController</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: Confirm that the Ingress Controller was created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc --all-namespaces=true get ingresscontrollers</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-controller-configuration-gcp-global-access_configuring-ingress">
<title>Configuring global access for an Ingress Controller on GCP</title>
<simpara>An Ingress Controller created on GCP with an internal load balancer generates an internal IP address for the service. A cluster administrator can specify the global access option, which enables clients in any region within the same VPC network and compute region as the load balancer, to reach the workloads running on your cluster.</simpara>
<simpara>For more information, see the GCP documentation for <link xlink:href="https://cloud.google.com/kubernetes-engine/docs/how-to/internal-load-balancing#global_access">global access</link>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You deployed an OpenShift Container Platform cluster on GCP infrastructure.</simpara>
</listitem>
<listitem>
<simpara>You configured an Ingress Controller to use an internal load balancer.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the Ingress Controller resource to allow global access.</simpara>
<note>
<simpara>You can also create an Ingress Controller and specify the global access option.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Configure the Ingress Controller resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator edit ingresscontroller/default</programlisting>
</listitem>
<listitem>
<simpara>Edit the YAML file:</simpara>
<formalpara>
<title>Sample <literal>clientAccess</literal> configuration to <literal>Global</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">  spec:
    endpointPublishingStrategy:
      loadBalancer:
        providerParameters:
          gcp:
            clientAccess: Global <co xml:id="CO14-1"/>
          type: GCP
        scope: Internal
      type: LoadBalancerService</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO14-1">
<para>Set <literal>gcp.clientAccess</literal> to <literal>Global</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Run the following command to verify that the service allows global access:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress edit svc/router-default -o yaml</programlisting>
<simpara>The output shows that global access is enabled for GCP with the annotation, <literal>networking.gke.io/internal-load-balancer-allow-global-access</literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-controller-config-tuningoptions-healthcheckinterval_configuring-ingress">
<title>Setting the Ingress Controller health check interval</title>
<simpara>A cluster administrator can set the health check interval to define how long the router waits between two consecutive health checks. This value is applied globally as a default for all routes. The default value is 5 seconds.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The following assumes that you already created an Ingress Controller.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Update the Ingress Controller to change the interval between back end health checks:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge -p '{"spec":{"tuningOptions": {"healthCheckInterval": "8s"}}}'</programlisting>
<note>
<simpara>To override the <literal>healthCheckInterval</literal> for a single route, use the route annotation <literal>router.openshift.io/haproxy.health.check.interval</literal></simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-default-internal_configuring-ingress">
<title>Configuring the default Ingress Controller for your cluster to be internal</title>
<simpara>You can configure the <literal>default</literal> Ingress Controller for your cluster to be internal by deleting and recreating it.</simpara>
<warning>
<simpara>If your cloud provider is Microsoft Azure, you must have at least one public load balancer that points to your nodes.
If you do not, all of your nodes will lose egress connectivity to the internet.</simpara>
</warning>
<important>
<simpara>If you want to change the <literal>scope</literal> for an <literal>IngressController</literal>, you can change the <literal>.spec.endpointPublishingStrategy.loadBalancer.scope</literal> parameter after the custom resource (CR) is created.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the <literal>default</literal> Ingress Controller for your cluster to be internal by deleting and recreating it.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace --force --wait --filename - &lt;&lt;EOF
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: default
spec:
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: Internal
EOF</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-route-admission-policy_configuring-ingress">
<title>Configuring the route admission policy</title>
<simpara>Administrators and application developers can run applications in multiple namespaces with the same domain name. This is for organizations where multiple teams develop microservices that are exposed on the same hostname.</simpara>
<warning>
<simpara>Allowing claims across namespaces should only be enabled for clusters with trust between namespaces, otherwise a malicious user could take over a hostname. For this reason, the default admission policy disallows hostname claims across namespaces.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>.spec.routeAdmission</literal> field of the <literal>ingresscontroller</literal> resource variable using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{"spec":{"routeAdmission":{"namespaceOwnership":"InterNamespaceAllowed"}}}' --type=merge</programlisting>
<formalpara>
<title>Sample Ingress Controller configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed
...</programlisting>
</para>
</formalpara>
<tip>
<simpara>You can alternatively apply the following YAML to configure the route admission policy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="using-wildcard-routes_configuring-ingress">
<title>Using wildcard routes</title>
<simpara>The HAProxy Ingress Controller has support for wildcard routes. The Ingress Operator uses <literal>wildcardPolicy</literal> to configure the <literal>ROUTER_ALLOW_WILDCARD_ROUTES</literal> environment variable of the Ingress Controller.</simpara>
<simpara>The default behavior of the Ingress Controller is to admit routes with a wildcard policy of <literal>None</literal>, which is backwards compatible with existing <literal>IngressController</literal> resources.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the wildcard policy.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Use the following command to edit the <literal>IngressController</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit IngressController</programlisting>
</listitem>
<listitem>
<simpara>Under <literal>spec</literal>, set the <literal>wildcardPolicy</literal> field to <literal>WildcardsDisallowed</literal> or <literal>WildcardsAllowed</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  routeAdmission:
    wildcardPolicy: WildcardsDisallowed # or WildcardsAllowed</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-http-header-configuration_configuring-ingress">
<title>HTTP header configuration</title>
<simpara>OpenShift Container Platform provides different methods for working with HTTP headers. When setting or deleting headers, you can use specific fields in the Ingress Controller or an individual route to modify request and response headers. You can also set certain headers by using route annotations. The various ways of configuring headers can present challenges when working together.</simpara>
<note>
<simpara>You can only set or delete headers within an <literal>IngressController</literal> or <literal>Route</literal> CR, you cannot append them. If an HTTP header is set with a value, that value must be complete and not require appending in the future. In situations where it makes sense to append a header, such as the X-Forwarded-For header, use the <literal>spec.httpHeaders.forwardedHeaderPolicy</literal> field, instead of <literal>spec.httpHeaders.actions</literal>.</simpara>
</note>
<section xml:id="nw-http-header-configuration-order_configuring-ingress">
<title>Order of precedence</title>
<simpara>When the same HTTP header is modified both in the Ingress Controller and in a route, HAProxy prioritizes the actions in certain ways depending on whether it is a request or response header.</simpara>
<itemizedlist>
<listitem>
<simpara>For HTTP response headers, actions specified in the Ingress Controller are executed after the actions specified in a route. This means that the actions specified in the Ingress Controller take precedence.</simpara>
</listitem>
<listitem>
<simpara>For HTTP request headers, actions specified in a route are executed after the actions specified in the Ingress Controller. This means that the actions specified in the route take precedence.</simpara>
</listitem>
</itemizedlist>
<simpara>For example, a cluster administrator sets the X-Frame-Options response header with the value <literal>DENY</literal> in the Ingress Controller using the following configuration:</simpara>
<formalpara>
<title>Example <literal>IngressController</literal> spec</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
# ...
spec:
  httpHeaders:
    actions:
      response:
      - name: X-Frame-Options
        action:
          type: Set
          set:
            value: DENY</programlisting>
</para>
</formalpara>
<simpara>A route owner sets the same response header that the cluster administrator set in the Ingress Controller, but with the value <literal>SAMEORIGIN</literal> using the following configuration:</simpara>
<formalpara>
<title>Example <literal>Route</literal> spec</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
# ...
spec:
  httpHeaders:
    actions:
      response:
      - name: X-Frame-Options
        action:
          type: Set
          set:
            value: SAMEORIGIN</programlisting>
</para>
</formalpara>
<simpara>When both the <literal>IngressController</literal> spec and <literal>Route</literal> spec are configuring the X-Frame-Options header, then the value set for this header at the global level in the Ingress Controller will take precedence, even if a specific route allows frames.</simpara>
<simpara>This prioritzation occurs because the <literal>haproxy.config</literal> file uses the following logic, where the Ingress Controller is considered the front end and individual routes are considered the back end. The header value <literal>DENY</literal> applied to the front end configurations overrides the same header with the value <literal>SAMEORIGIN</literal> that is set in the back end:</simpara>
<programlisting language="text" linenumbering="unnumbered">frontend public
  http-response set-header X-Frame-Options 'DENY'

frontend fe_sni
  http-response set-header X-Frame-Options 'DENY'

frontend fe_no_sni
  http-response set-header X-Frame-Options 'DENY'

backend be_secure:openshift-monitoring:alertmanager-main
  http-response set-header X-Frame-Options 'SAMEORIGIN'</programlisting>
<simpara>Additionally, any actions defined in either the Ingress Controller or a route override values set using route annotations.</simpara>
</section>
<section xml:id="nw-http-header-configuration-special-cases_configuring-ingress">
<title>Special case headers</title>
<simpara>The following headers are either prevented entirely from being set or deleted, or allowed under specific circumstances:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Special case header configuration options</title>
<tgroup cols="5">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<colspec colname="col_5" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Header name</entry>
<entry align="left" valign="top">Configurable using <literal>IngressController</literal> spec</entry>
<entry align="left" valign="top">Configurable using <literal>Route</literal> spec</entry>
<entry align="left" valign="top">Reason for disallowment</entry>
<entry align="left" valign="top">Configurable using another method</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>proxy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>proxy</literal> HTTP request header can be used to exploit vulnerable CGI applications by injecting the header value into the <literal>HTTP_PROXY</literal> environment variable. The <literal>proxy</literal> HTTP request header is also non-standard and prone to error during configuration.</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>host</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
<entry align="left" valign="top"><simpara>When the <literal>host</literal> HTTP request header is set using the <literal>IngressController</literal> CR, HAProxy can fail when looking up the correct route.</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>strict-transport-security</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>strict-transport-security</literal> HTTP response header is already handled using route annotations and does not need a separate implementation.</simpara></entry>
<entry align="left" valign="top"><simpara>Yes: the <literal>haproxy.router.openshift.io/hsts_header</literal> route annotation</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cookie</literal> and <literal>set-cookie</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>The cookies that HAProxy sets are used for session tracking to map client connections to particular back-end servers. Allowing these headers to be set could interfere with HAProxy&#8217;s session affinity and restrict HAProxy&#8217;s ownership of a cookie.</simpara></entry>
<entry align="left" valign="top"><simpara>Yes:</simpara>
<itemizedlist>
<listitem>
<simpara>the <literal>haproxy.router.openshift.io/disable_cookie</literal> route annotation</simpara>
</listitem>
<listitem>
<simpara>the <literal>haproxy.router.openshift.io/cookie_name</literal> route annotation</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="nw-ingress-set-or-delete-http-headers_configuring-ingress">
<title>Setting or deleting HTTP request and response headers in an Ingress Controller</title>
<simpara>You can set or delete certain HTTP request and response headers for compliance purposes or other reasons. You can set or delete these headers either for all routes served by an Ingress Controller or for specific routes.</simpara>
<simpara>For example, you might want to migrate an application running on your cluster to use mutual TLS, which requires that your application checks for an X-Forwarded-Client-Cert request header, but the OpenShift Container Platform default Ingress Controller provides an X-SSL-Client-Der request header.</simpara>
<simpara>The following procedure modifies the Ingress Controller to set the X-Forwarded-Client-Cert request header, and delete the X-SSL-Client-Der request header.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the Ingress Controller resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator edit ingresscontroller/default</programlisting>
</listitem>
<listitem>
<simpara>Replace the X-SSL-Client-Der HTTP request header with the X-Forwarded-Client-Cert HTTP request header:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpHeaders:
    actions: <co xml:id="CO15-1"/>
      request: <co xml:id="CO15-2"/>
      - name: X-Forwarded-Client-Cert <co xml:id="CO15-3"/>
        action:
          type: Set <co xml:id="CO15-4"/>
          set:
           value: "%{+Q}[ssl_c_der,base64]" <co xml:id="CO15-5"/>
      - name: X-SSL-Client-Der
        action:
          type: Delete</programlisting>
<calloutlist>
<callout arearefs="CO15-1">
<para>The list of actions you want to perform on the HTTP headers.</para>
</callout>
<callout arearefs="CO15-2">
<para>The type of header you want to change. In this case, a request header.</para>
</callout>
<callout arearefs="CO15-3">
<para>The name of the header you want to change. For a list of available headers you can set or delete, see <emphasis>HTTP header configuration</emphasis>.</para>
</callout>
<callout arearefs="CO15-4">
<para>The type of action being taken on the header. This field can have the value <literal>Set</literal> or <literal>Delete</literal>.</para>
</callout>
<callout arearefs="CO15-5">
<para>When setting HTTP headers, you must provide a <literal>value</literal>. The value can be a string from a list of available directives for that header, for example <literal>DENY</literal>, or it can be a dynamic value that will be interpreted using HAProxy&#8217;s dynamic value syntax. In this case, a dynamic value is added.</para>
</callout>
</calloutlist>
<note>
<simpara>For setting dynamic header values for HTTP responses, allowed sample fetchers are <literal>res.hdr</literal> and <literal>ssl_c_der</literal>. For setting dynamic header values for HTTP requests, allowed sample fetchers are <literal>req.hdr</literal> and <literal>ssl_c_der</literal>. Both request and response dynamic values can use the <literal>lower</literal> and <literal>base64</literal> converters.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-using-ingress-forwarded_configuring-ingress">
<title>Using X-Forwarded headers</title>
<simpara>You configure the HAProxy Ingress Controller to specify a policy for how to handle HTTP headers including <literal>Forwarded</literal> and <literal>X-Forwarded-For</literal>. The Ingress Operator uses the <literal>HTTPHeaders</literal> field to configure the <literal>ROUTER_SET_FORWARDED_HEADERS</literal> environment variable of the Ingress Controller.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the <literal>HTTPHeaders</literal> field for the Ingress Controller.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Use the following command to edit the <literal>IngressController</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit IngressController</programlisting>
</listitem>
<listitem>
<simpara>Under <literal>spec</literal>, set the <literal>HTTPHeaders</literal> policy field to <literal>Append</literal>, <literal>Replace</literal>, <literal>IfNone</literal>, or <literal>Never</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpHeaders:
    forwardedHeaderPolicy: Append</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<bridgehead xml:id="_example-use-cases" renderas="sect4">Example use cases</bridgehead>
<simpara><emphasis role="strong">As a cluster administrator, you can:</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>Configure an external proxy that injects the <literal>X-Forwarded-For</literal> header into each request before forwarding it to an Ingress Controller.</simpara>
<simpara>To configure the Ingress Controller to pass the header through unmodified, you specify the <literal>never</literal> policy. The Ingress Controller then never sets the headers, and applications receive only the headers that the external proxy provides.</simpara>
</listitem>
<listitem>
<simpara>Configure the Ingress Controller to pass the <literal>X-Forwarded-For</literal> header that your external proxy sets on external cluster requests through unmodified.</simpara>
<simpara>To configure the Ingress Controller to set the <literal>X-Forwarded-For</literal> header on internal cluster requests, which do not go through the external proxy, specify the <literal>if-none</literal> policy. If an HTTP request already has the header set through the external proxy, then the Ingress Controller preserves it. If the header is absent because the request did not come through the proxy, then the Ingress Controller adds the header.</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">As an application developer, you can:</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>Configure an application-specific external proxy that injects the <literal>X-Forwarded-For</literal> header.</simpara>
<simpara>To configure an Ingress Controller to pass the header through unmodified for an application&#8217;s Route, without affecting the policy for other Routes, add an annotation <literal>haproxy.router.openshift.io/set-forwarded-headers: if-none</literal> or <literal>haproxy.router.openshift.io/set-forwarded-headers: never</literal> on the Route for the application.</simpara>
<note>
<simpara>You can set the <literal>haproxy.router.openshift.io/set-forwarded-headers</literal> annotation on a per route basis, independent from the globally set value for the Ingress Controller.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-http2-haproxy_configuring-ingress">
<title>Enabling HTTP/2 Ingress connectivity</title>
<simpara>You can enable transparent end-to-end HTTP/2 connectivity in HAProxy. It allows application owners to make use of HTTP/2 protocol capabilities, including single connection, header compression, binary streams, and more.</simpara>
<simpara>You can enable HTTP/2 connectivity for an individual Ingress Controller or for the entire cluster.</simpara>
<simpara>To enable the use of HTTP/2 for the connection from the client to HAProxy, a route must specify a custom certificate. A route that uses the default certificate cannot use HTTP/2. This restriction is necessary to avoid problems from connection coalescing, where the client re-uses a connection for different routes that use the same certificate.</simpara>
<simpara>The connection from HAProxy to the application pod can use HTTP/2 only for re-encrypt routes and not for edge-terminated or insecure routes. This restriction is because HAProxy uses Application-Level Protocol Negotiation (ALPN), which is a TLS extension, to negotiate the use of HTTP/2 with the back-end. The implication is that end-to-end HTTP/2 is possible with passthrough and re-encrypt and not with insecure or edge-terminated routes.</simpara>
<warning>
<simpara>Using WebSockets with a re-encrypt route and with HTTP/2 enabled on an Ingress Controller requires WebSocket support over HTTP/2. WebSockets over HTTP/2 is a feature of HAProxy 2.4, which is unsupported in OpenShift Container Platform at this time.</simpara>
</warning>
<important>
<simpara>For non-passthrough routes, the Ingress Controller negotiates its connection to the application independently of the connection from the client. This means a client may connect to the Ingress Controller and negotiate HTTP/1.1, and the Ingress Controller may then connect to the application, negotiate HTTP/2, and forward the request from the client HTTP/1.1 connection using the HTTP/2 connection to the application. This poses a problem if the client subsequently tries to upgrade its connection from HTTP/1.1 to the WebSocket protocol, because the Ingress Controller cannot forward WebSocket to HTTP/2 and cannot upgrade its HTTP/2 connection to WebSocket. Consequently, if you have an application that is intended to accept WebSocket connections, it must not allow negotiating the HTTP/2 protocol or else clients will fail to upgrade to the WebSocket protocol.</simpara>
</important>
<formalpara>
<title>Procedure</title>
<para>Enable HTTP/2 on a single Ingress Controller.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>To enable HTTP/2 on an Ingress Controller, enter the <literal>oc annotate</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator annotate ingresscontrollers/&lt;ingresscontroller_name&gt; ingress.operator.openshift.io/default-enable-http2=true</programlisting>
<simpara>Replace <literal>&lt;ingresscontroller_name&gt;</literal> with the name of the Ingress Controller to annotate.</simpara>
</listitem>
</itemizedlist>
<simpara>Enable HTTP/2 on the entire cluster.</simpara>
<itemizedlist>
<listitem>
<simpara>To enable HTTP/2 for the entire cluster, enter the <literal>oc annotate</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate ingresses.config/cluster ingress.operator.openshift.io/default-enable-http2=true</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add the annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
  annotations:
    ingress.operator.openshift.io/default-enable-http2: "true"</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-controller-configuration-proxy-protocol_configuring-ingress">
<title>Configuring the PROXY protocol for an Ingress Controller</title>
<simpara>A cluster administrator can configure <link xlink:href="https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt">the PROXY protocol</link> when an Ingress Controller uses either the <literal>HostNetwork</literal> or <literal>NodePortService</literal> endpoint publishing strategy types. The PROXY protocol enables the load balancer to preserve the original client addresses for connections that the Ingress Controller receives. The original client addresses are useful for logging, filtering, and injecting HTTP headers. In the default configuration, the connections that the Ingress Controller receives only contain the source address that is associated with the load balancer.</simpara>
<simpara>This feature is not supported in cloud deployments. This restriction is because when OpenShift Container Platform runs in a cloud platform, and an IngressController specifies that a service load balancer should be used, the Ingress Operator configures the load balancer service and enables the PROXY protocol based on the platform requirement for preserving source addresses.</simpara>
<important>
<simpara>You must configure both OpenShift Container Platform and the external load balancer to either use the PROXY protocol or to use TCP.</simpara>
</important>
<warning>
<simpara>The PROXY protocol is unsupported for the default Ingress Controller with installer-provisioned clusters on non-cloud platforms that use a Keepalived Ingress VIP.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created an Ingress Controller.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the Ingress Controller resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator edit ingresscontroller/default</programlisting>
</listitem>
<listitem>
<simpara>Set the PROXY configuration:</simpara>
<itemizedlist>
<listitem>
<simpara>If your Ingress Controller uses the hostNetwork endpoint publishing strategy type, set the <literal>spec.endpointPublishingStrategy.hostNetwork.protocol</literal> subfield to <literal>PROXY</literal>:</simpara>
<formalpara>
<title>Sample <literal>hostNetwork</literal> configuration to <literal>PROXY</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">  spec:
    endpointPublishingStrategy:
      hostNetwork:
        protocol: PROXY
      type: HostNetwork</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If your Ingress Controller uses the NodePortService endpoint publishing strategy type, set the <literal>spec.endpointPublishingStrategy.nodePort.protocol</literal> subfield to <literal>PROXY</literal>:</simpara>
<formalpara>
<title>Sample <literal>nodePort</literal> configuration to <literal>PROXY</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">  spec:
    endpointPublishingStrategy:
      nodePort:
        protocol: PROXY
      type: NodePortService</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-configuring-application-domain_configuring-ingress">
<title>Specifying an alternative cluster domain using the appsDomain option</title>
<simpara>As a cluster administrator, you can specify an alternative to the default cluster domain for user-created routes by configuring the <literal>appsDomain</literal> field. The <literal>appsDomain</literal> field is an optional domain for OpenShift Container Platform to use instead of the default, which is specified in the <literal>domain</literal> field. If you specify an alternative domain, it overrides the default cluster domain for the purpose of determining the default host for a new route.</simpara>
<simpara>For example, you can use the DNS domain for your company as the default domain for routes and ingresses for applications running on your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You deployed an OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>You installed the <literal>oc</literal> command line interface.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the <literal>appsDomain</literal> field by specifying an alternative default domain for user-created routes.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the ingress <literal>cluster</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit ingresses.config/cluster -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Edit the YAML file:</simpara>
<formalpara>
<title>Sample <literal>appsDomain</literal> configuration to <literal>test.example.com</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: apps.example.com            <co xml:id="CO16-1"/>
  appsDomain: &lt;test.example.com&gt;      <co xml:id="CO16-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO16-1">
<para>Specifies the default domain. You cannot modify the default domain after installation.</para>
</callout>
<callout arearefs="CO16-2">
<para>Optional: Domain for OpenShift Container Platform infrastructure to use for application routes. Instead of the default prefix, <literal>apps</literal>, you can use an alternative prefix like <literal>test</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that an existing route contains the domain name specified in the <literal>appsDomain</literal> field by exposing the route and verifying the route domain change:</simpara>
<note>
<simpara>Wait for the <literal>openshift-apiserver</literal> finish rolling updates before exposing the route.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Expose the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose service hello-openshift
route.route.openshift.io/hello-openshift exposed</programlisting>
<formalpara>
<title>Example output:</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes
NAME              HOST/PORT                                   PATH   SERVICES          PORT       TERMINATION   WILDCARD
hello-openshift   hello_openshift-&lt;my_project&gt;.test.example.com
hello-openshift   8080-tcp                 None</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-converting-http-header-case_configuring-ingress">
<title>Converting HTTP header case</title>
<simpara>HAProxy 2.2 lowercases HTTP header names by default, for example, changing <literal>Host: xyz.com</literal> to <literal>host: xyz.com</literal>. If legacy applications are sensitive to the capitalization of HTTP header names, use the Ingress Controller <literal>spec.httpHeaders.headerNameCaseAdjustments</literal> API field for a solution to accommodate legacy applications until they can be fixed.</simpara>
<important>
<simpara>Because OpenShift Container Platform includes HAProxy 2.2, make sure to add the necessary configuration by using <literal>spec.httpHeaders.headerNameCaseAdjustments</literal> before upgrading.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>As a cluster administrator, you can convert the HTTP header case by entering the <literal>oc patch</literal> command or by setting the <literal>HeaderNameCaseAdjustments</literal> field in the Ingress Controller YAML file.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Specify an HTTP header to be capitalized by entering the <literal>oc patch</literal> command.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Enter the <literal>oc patch</literal> command to change the HTTP <literal>host</literal> header to <literal>Host</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"httpHeaders":{"headerNameCaseAdjustments":["Host"]}}}'</programlisting>
</listitem>
<listitem>
<simpara>Annotate the route of the application:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate routes/my-application haproxy.router.openshift.io/h1-adjust-case=true</programlisting>
<simpara>The Ingress Controller then adjusts the <literal>host</literal> request header as specified.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<simpara>Specify adjustments using the <literal>HeaderNameCaseAdjustments</literal> field by configuring the Ingress Controller YAML file.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The following example Ingress Controller YAML adjusts the <literal>host</literal> header to <literal>Host</literal> for HTTP/1 requests to appropriately annotated routes:</simpara>
<formalpara>
<title>Example Ingress Controller YAML</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpHeaders:
    headerNameCaseAdjustments:
    - Host</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>The following example route enables HTTP response header name case adjustments using the <literal>haproxy.router.openshift.io/h1-adjust-case</literal> annotation:</simpara>
<formalpara>
<title>Example route YAML</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/h1-adjust-case: true <co xml:id="CO17-1"/>
  name: my-application
  namespace: my-application
spec:
  to:
    kind: Service
    name: my-application</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO17-1">
<para>Set <literal>haproxy.router.openshift.io/h1-adjust-case</literal> to true.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-configuring-router-compression_configuring-ingress">
<title>Using router compression</title>
<simpara>You configure the HAProxy Ingress Controller to specify router compression globally for specific MIME types. You can use the <literal>mimeTypes</literal> variable to define the formats of MIME types to which compression is applied. The types are: application, image, message, multipart, text, video, or a custom type prefaced by "X-". To see the full notation for MIME types and subtypes, see <link xlink:href="https://datatracker.ietf.org/doc/html/rfc1341#page-7">RFC1341</link>.</simpara>
<note>
<simpara>Memory allocated for compression can affect the max connections. Additionally, compression of large buffers can cause latency, like heavy regex or long lists of regex.</simpara>
<simpara>Not all MIME types benefit from compression, but HAProxy still uses resources to try to compress if instructed to.  Generally, text formats, such as html, css, and js, formats benefit from compression, but formats that are already compressed, such as image, audio, and video, benefit little in exchange for the time and resources spent on compression.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the <literal>httpCompression</literal> field for the Ingress Controller.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Use the following command to edit the <literal>IngressController</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit -n openshift-ingress-operator ingresscontrollers/default</programlisting>
</listitem>
<listitem>
<simpara>Under <literal>spec</literal>, set the <literal>httpCompression</literal> policy field to <literal>mimeTypes</literal> and specify a list of MIME types that should have compression applied:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  httpCompression:
    mimeTypes:
    - "text/html"
    - "text/css; charset=utf-8"
    - "application/json"
   ...</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-exposing-router-metrics_configuring-ingress">
<title>Exposing router metrics</title>
<simpara>You can expose the HAProxy router metrics by default in Prometheus format on the default stats port, 1936. The external metrics collection and aggregation systems such as Prometheus can access the HAProxy router metrics. You can view the HAProxy router metrics in a browser in the HTML and comma separated values (CSV) format.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured your firewall to access the default stats port, 1936.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the router pod name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ingress</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                              READY   STATUS    RESTARTS   AGE
router-default-76bfffb66c-46qwp   1/1     Running   0          11h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the router&#8217;s username and password, which the router pod stores in the <literal>/var/lib/haproxy/conf/metrics-auth/statsUsername</literal> and <literal>/var/lib/haproxy/conf/metrics-auth/statsPassword</literal> files:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the username by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh &lt;router_pod_name&gt; cat metrics-auth/statsUsername</programlisting>
</listitem>
<listitem>
<simpara>Get the password by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh &lt;router_pod_name&gt; cat metrics-auth/statsPassword</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Get the router IP and metrics certificates by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pod &lt;router_pod&gt;</programlisting>
</listitem>
<listitem>
<simpara>Get the raw statistics in Prometheus format by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -u &lt;user&gt;:&lt;password&gt; http://&lt;router_IP&gt;:&lt;stats_port&gt;/metrics</programlisting>
</listitem>
<listitem>
<simpara>Access the metrics securely by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -u user:password https://&lt;router_IP&gt;:&lt;stats_port&gt;/metrics -k</programlisting>
</listitem>
<listitem>
<simpara>Access the default stats port, 1936, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -u &lt;user&gt;:&lt;password&gt; http://&lt;router_IP&gt;:&lt;stats_port&gt;/metrics</programlisting>
<example>
<title>Example output</title>
<programlisting language="terminal" linenumbering="unnumbered">...
# HELP haproxy_backend_connections_total Total number of connections.
# TYPE haproxy_backend_connections_total gauge
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route"} 0
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route-alt"} 0
haproxy_backend_connections_total{backend="http",namespace="default",route="hello-route01"} 0
...
# HELP haproxy_exporter_server_threshold Number of servers tracked and the current threshold value.
# TYPE haproxy_exporter_server_threshold gauge
haproxy_exporter_server_threshold{type="current"} 11
haproxy_exporter_server_threshold{type="limit"} 500
...
# HELP haproxy_frontend_bytes_in_total Current total of incoming bytes.
# TYPE haproxy_frontend_bytes_in_total gauge
haproxy_frontend_bytes_in_total{frontend="fe_no_sni"} 0
haproxy_frontend_bytes_in_total{frontend="fe_sni"} 0
haproxy_frontend_bytes_in_total{frontend="public"} 119070
...
# HELP haproxy_server_bytes_in_total Current total of incoming bytes.
# TYPE haproxy_server_bytes_in_total gauge
haproxy_server_bytes_in_total{namespace="",pod="",route="",server="fe_no_sni",service=""} 0
haproxy_server_bytes_in_total{namespace="",pod="",route="",server="fe_sni",service=""} 0
haproxy_server_bytes_in_total{namespace="default",pod="docker-registry-5-nk5fz",route="docker-registry",server="10.130.0.89:5000",service="docker-registry"} 0
haproxy_server_bytes_in_total{namespace="default",pod="hello-rc-vkjqx",route="hello-route",server="10.130.0.90:8080",service="hello-svc-1"} 0
...</programlisting>
</example>
</listitem>
<listitem>
<simpara>Launch the stats window by entering the following URL in a browser:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">http://&lt;user&gt;:&lt;password&gt;@&lt;router_IP&gt;:&lt;stats_port&gt;</programlisting>
</listitem>
<listitem>
<simpara>Optional: Get the stats in CSV format by entering the following URL in a browser:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">http://&lt;user&gt;:&lt;password&gt;@&lt;router_ip&gt;:1936/metrics;csv</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-customize-ingress-error-pages_configuring-ingress">
<title>Customizing HAProxy error code response pages</title>
<simpara>As a cluster administrator, you can specify a custom error code response page for either 503, 404, or both error pages. The HAProxy router serves a 503 error page when the application pod is not running or a 404 error page when the requested URL does not exist. For example, if you customize the 503 error code response page, then the page is served when the application pod is not running, and the default 404 error code HTTP response page is served by the HAProxy router for an incorrect route or a non-existing route.</simpara>
<simpara>Custom error code response pages are specified in a config map then patched to the Ingress Controller. The config map keys have two available file names as follows:
<literal>error-page-503.http</literal> and <literal>error-page-404.http</literal>.</simpara>
<simpara>Custom HTTP error code response pages must follow the <link xlink:href="https://www.haproxy.com/documentation/hapee/latest/configuration/config-sections/http-errors/">HAProxy HTTP error page configuration guidelines</link>. Here is an example of the default OpenShift Container Platform HAProxy router <link xlink:href="https://raw.githubusercontent.com/openshift/router/master/images/router/haproxy/conf/error-page-503.http">http 503 error code response page</link>. You can use the default content as a template for creating your own custom page.</simpara>
<simpara>By default, the HAProxy router serves only a 503 error page when the application is not running or when the route is incorrect or non-existent. This default behavior is the same as the behavior on OpenShift Container Platform 4.8 and earlier. If a config map for the customization of an HTTP error code response is not provided, and you are using a custom HTTP error code response page, the router serves a default 404 or 503 error code response page.</simpara>
<note>
<simpara>If you use the OpenShift Container Platform default 503 error code page as a template for your customizations, the headers in the file require an editor that can use CRLF line endings.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a config map named <literal>my-custom-error-code-pages</literal> in the <literal>openshift-config</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-config create configmap my-custom-error-code-pages \
--from-file=error-page-503.http \
--from-file=error-page-404.http</programlisting>
</listitem>
<listitem>
<simpara>Patch the Ingress Controller to reference the <literal>my-custom-error-code-pages</literal> config map by name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch -n openshift-ingress-operator ingresscontroller/default --patch '{"spec":{"httpErrorCodePages":{"name":"my-custom-error-code-pages"}}}' --type=merge</programlisting>
<simpara>The Ingress Operator copies the <literal>my-custom-error-code-pages</literal> config map from the <literal>openshift-config</literal> namespace to the <literal>openshift-ingress</literal> namespace. The Operator names the config map according to the pattern, <literal>&lt;your_ingresscontroller_name&gt;-errorpages</literal>, in the <literal>openshift-ingress</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>Display the copy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get cm default-errorpages -n openshift-ingress</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                       DATA   AGE
default-errorpages         2      25s  <co xml:id="CO18-1"/></screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO18-1">
<para>The example config map name is <literal>default-errorpages</literal> because the <literal>default</literal> Ingress Controller custom resource (CR) was patched.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Confirm that the config map containing the custom error response page mounts on the router volume where the config map key is the filename that has the custom HTTP error code response:</simpara>
<itemizedlist>
<listitem>
<simpara>For 503 custom HTTP custom error code response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress rsh &lt;router_pod&gt; cat /var/lib/haproxy/conf/error_code_pages/error-page-503.http</programlisting>
</listitem>
<listitem>
<simpara>For 404 custom HTTP custom error code response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress rsh &lt;router_pod&gt; cat /var/lib/haproxy/conf/error_code_pages/error-page-404.http</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>Verify your custom error code HTTP response:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a test project and application:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ oc new-project test-ingress</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-app django-psql-example</programlisting>
</listitem>
<listitem>
<simpara>For 503 custom http error code response:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Stop all the pods for the application.</simpara>
</listitem>
<listitem>
<simpara>Run the following curl command or visit the route hostname in the browser:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -vk &lt;route_hostname&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For 404 custom http error code response:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Visit a non-existent route or an incorrect route.</simpara>
</listitem>
<listitem>
<simpara>Run the following curl command or visit the route hostname in the browser:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -vk &lt;route_hostname&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Check if the <literal>errorfile</literal> attribute is properly in the <literal>haproxy.config</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress rsh &lt;router&gt; cat /var/lib/haproxy/conf/haproxy.config | grep errorfile</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-setting-max-connections_configuring-ingress">
<title>Setting the Ingress Controller maximum connections</title>
<simpara>A cluster administrator can set the maximum number of simultaneous connections for OpenShift router deployments. You can patch an existing Ingress Controller to increase the maximum number of connections.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The following assumes that you already created an Ingress Controller</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Update the Ingress Controller to change the maximum number of connections for HAProxy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default --type=merge -p '{"spec":{"tuningOptions": {"maxConnections": 7500}}}'</programlisting>
<warning>
<simpara>If you set the <literal>spec.tuningOptions.maxConnections</literal> value greater than the current operating system limit, the HAProxy process will not start. See the table in the "Ingress Controller configuration parameters" section for more information about this parameter.</simpara>
</warning>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-a-custom-pki">Configuring a custom PKI</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="ingress-sharding">
<title>Ingress sharding in OpenShift Container Platform</title>

<simpara>In OpenShift Container Platform, an Ingress Controller can serve all routes, or it can serve a subset of routes. By default, the Ingress Controller serves any route created in any namespace in the cluster. You can add additional Ingress Controllers to your cluster to optimize routing by creating <emphasis>shards</emphasis>, which are subsets of routes based on selected characteristics. To mark a route as a member of a shard, use labels in the route or namespace <literal>metadata</literal> field. The Ingress Controller uses <emphasis>selectors</emphasis>, also known as a <emphasis>selection expression</emphasis>, to select a subset of routes from the entire pool of routes to serve.</simpara>
<simpara>Ingress sharding is useful in cases where you want to load balance incoming traffic across multiple Ingress Controllers, when you want to isolate traffic to be routed to a specific Ingress Controller, or for a variety of other reasons described in the next section.</simpara>
<simpara>By default, each route uses the default domain of the cluster. However, routes can be configured to use the domain of the router instead. For more information, see <link linkend="nw-ingress-sharding-route-configuration_ingress-sharding">Creating a route for Ingress Controller Sharding</link>.</simpara>
<section xml:id="nw-ingress-sharding_ingress-sharding">
<title>Ingress Controller sharding</title>
<simpara>You can use Ingress sharding, also known as router sharding, to distribute a set of routes across multiple routers by adding labels to routes, namespaces, or both. The Ingress Controller uses a corresponding set of selectors to admit only the routes that have a specified label. Each Ingress shard comprises the routes that are filtered using a given selection expression.</simpara>
<simpara>As the primary mechanism for traffic to enter the cluster, the demands on the Ingress Controller can be significant. As a cluster administrator, you can shard the routes to:</simpara>
<itemizedlist>
<listitem>
<simpara>Balance Ingress Controllers, or routers, with several routes to speed up responses to changes.</simpara>
</listitem>
<listitem>
<simpara>Allocate certain routes to have different reliability guarantees than other routes.</simpara>
</listitem>
<listitem>
<simpara>Allow certain Ingress Controllers to have different policies defined.</simpara>
</listitem>
<listitem>
<simpara>Allow only specific routes to use additional features.</simpara>
</listitem>
<listitem>
<simpara>Expose different routes on different addresses so that internal and external users can see different routes, for example.</simpara>
</listitem>
<listitem>
<simpara>Transfer traffic from one version of an application to another during a blue green deployment.</simpara>
</listitem>
</itemizedlist>
<simpara>When Ingress Controllers are sharded, a given route is admitted to zero or more Ingress Controllers in the group. A route&#8217;s status describes whether an Ingress Controller has admitted it or not. An Ingress Controller will only admit a route if it is unique to its shard.</simpara>
<simpara>An Ingress Controller can use three sharding methods:</simpara>
<itemizedlist>
<listitem>
<simpara>Adding only a namespace selector to the Ingress Controller, so that all routes in a namespace with labels that match the namespace selector are in the Ingress shard.</simpara>
</listitem>
<listitem>
<simpara>Adding only a route selector to the Ingress Controller, so that all routes with labels that match the route selector are in the Ingress shard.</simpara>
</listitem>
<listitem>
<simpara>Adding both a namespace selector and route selector to the Ingress Controller, so that routes with labels that match the route selector in a namespace with labels that match the namespace selector are in the Ingress shard.</simpara>
</listitem>
</itemizedlist>
<simpara>With sharding, you can distribute subsets of routes over multiple Ingress Controllers. These subsets can be non-overlapping, also called <emphasis>traditional</emphasis> sharding, or overlapping, otherwise known as <emphasis>overlapped</emphasis> sharding.</simpara>
<section xml:id="_traditional-sharding-example">
<title>Traditional sharding example</title>
<simpara>An Ingress Controller <literal>finops-router</literal> is configured with the label selector <literal>spec.namespaceSelector.matchLabels.name</literal> set to <literal>finance</literal> and <literal>ops</literal>:</simpara>
<formalpara>
<title>Example YAML definition for <literal>finops-router</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: finops-router
  namespace: openshift-ingress-operator
spec:
  namespaceSelector:
    matchLabels:
      name:
        - finance
        - ops</programlisting>
</para>
</formalpara>
<simpara>A second Ingress Controller <literal>dev-router</literal> is configured with the label selector <literal>spec.namespaceSelector.matchLabels.name</literal> set to <literal>dev</literal>:</simpara>
<formalpara>
<title>Example YAML definition for <literal>dev-router</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: dev-router
  namespace: openshift-ingress-operator
spec:
  namespaceSelector:
    matchLabels:
      name: dev</programlisting>
</para>
</formalpara>
<simpara>If all application routes are in separate namespaces, each labeled with <literal>name:finance</literal>, <literal>name:ops</literal>, and <literal>name:dev</literal> respectively, this configuration effectively distributes your routes between the two Ingress Controllers. OpenShift Container Platform routes for console, authentication, and other purposes should not be handled.</simpara>
<simpara>In the above scenario, sharding becomes a special case of partitioning, with no overlapping subsets. Routes are divided between router shards.</simpara>
<warning>
<simpara>The <literal>default</literal> Ingress Controller continues to serve all routes unless the <literal>namespaceSelector</literal> or <literal>routeSelector</literal> fields contain routes that are meant for exclusion. See this <link xlink:href="https://access.redhat.com/solutions/5097511">Red Hat Knowledgebase solution</link> and the section "Sharding the default Ingress Controller" for more information on how to exclude routes from the default Ingress Controller.</simpara>
</warning>
</section>
<section xml:id="_overlapped-sharding-example">
<title>Overlapped sharding example</title>
<simpara>In addition to <literal>finops-router</literal> and <literal>dev-router</literal> in the example above, you also have <literal>devops-router</literal>, which is configured with the label selector <literal>spec.namespaceSelector.matchLabels.name</literal> set to <literal>dev</literal> and <literal>ops</literal>:</simpara>
<formalpara>
<title>Example YAML definition for <literal>devops-router</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: devops-router
  namespace: openshift-ingress-operator
spec:
  namespaceSelector:
    matchLabels:
      name:
        - dev
        - ops</programlisting>
</para>
</formalpara>
<simpara>The routes in the namespaces labeled <literal>name:dev</literal> and <literal>name:ops</literal> are now serviced by two different Ingress Controllers. With this configuration, you have overlapping subsets of routes.</simpara>
<simpara>With overlapping subsets of routes you can create more complex routing rules. For example, you can divert higher priority traffic to the dedicated <literal>finops-router</literal> while sending lower priority traffic to <literal>devops-router</literal>.</simpara>
</section>
<section xml:id="nw-ingress-sharding-default_ingress-sharding">
<title>Sharding the default Ingress Controller</title>
<simpara>After creating a new Ingress shard, there might be routes that are admitted to your new Ingress shard that are also admitted by the default Ingress Controller. This is because the default Ingress Controller has no selectors and admits all routes by default.</simpara>
<simpara>You can restrict an Ingress Controller from servicing routes with specific labels using either namespace selectors or route selectors. The following procedure restricts the default Ingress Controller from servicing your newly sharded <literal>finance</literal>, <literal>ops</literal>, and <literal>dev</literal>, routes using a namespace selector. This adds further isolation to Ingress shards.</simpara>
<important>
<simpara>You must keep all of OpenShift Container Platform&#8217;s administration routes on the same Ingress Controller. Therefore, avoid adding additional selectors to the default Ingress Controller that exclude these essential routes.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a project administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the default Ingress Controller by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit ingresscontroller -n openshift-ingress-operator default</programlisting>
</listitem>
<listitem>
<simpara>Edit the Ingress Controller to contain a <literal>namespaceSelector</literal> that excludes the routes with any of the <literal>finance</literal>, <literal>ops</literal>, and <literal>dev</literal> labels:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  namespaceSelector:
    matchExpressions:
      - key: type
        operator: NotIn
        values:
          - finance
          - ops
          - dev</programlisting>
</listitem>
</orderedlist>
<simpara>The default Ingress Controller will no longer serve the namespaces labeled <literal>name:finance</literal>, <literal>name:ops</literal>, and <literal>name:dev</literal>.</simpara>
</section>
<section xml:id="nw-ingress-sharding-dns_ingress-sharding">
<title>Ingress sharding and DNS</title>
<simpara>The cluster administrator is responsible for making a separate DNS entry for each router in a project. A router will not forward unknown routes to another router.</simpara>
<simpara>Consider the following example:</simpara>
<itemizedlist>
<listitem>
<simpara>Router A lives on host 192.168.0.5 and has routes with <literal>*.foo.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>Router B lives on host 192.168.1.9 and has routes with <literal>*.example.com</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>Separate DNS entries must resolve <literal>*.foo.com</literal> to the node hosting Router A and <literal>*.example.com</literal> to the node hosting Router B:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>*.foo.com A IN 192.168.0.5</literal></simpara>
</listitem>
<listitem>
<simpara><literal>*.example.com A IN 192.168.1.9</literal></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-sharding-route-labels_ingress-sharding">
<title>Configuring Ingress Controller sharding by using route labels</title>
<simpara>Ingress Controller sharding by using route labels means that the Ingress
Controller serves any route in any namespace that is selected by the route
selector.</simpara>
<figure>
<title>Ingress sharding using route labels</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-sharding-route-labels.png"/>
</imageobject>
<textobject><phrase>A diagram showing multiple Ingress Controllers with different route selectors serving any route containing a label that matches a given route selector regardless of the namespace a route belongs to</phrase></textobject>
</mediaobject>
</figure>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among
a set of Ingress Controllers and when isolating traffic to a specific Ingress
Controller. For example, company A goes to one Ingress Controller and company B
to another.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat router-internal.yaml
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: sharded
  namespace: openshift-ingress-operator
spec:
  domain: &lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO19-1"/>
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/worker: ""
  routeSelector:
    matchLabels:
      type: sharded</programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the Ingress Controller <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc apply -f router-internal.yaml</programlisting>
<simpara>The Ingress Controller selects routes in any namespace that have the label
<literal>type: sharded</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create a new route using the domain configured in the <literal>router-internal.yaml</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-sharding-namespace-labels_ingress-sharding">
<title>Configuring Ingress Controller sharding by using namespace labels</title>
<simpara>Ingress Controller sharding by using namespace labels means that the Ingress
Controller serves any route in any namespace that is selected by the namespace
selector.</simpara>
<figure>
<title>Ingress sharding using namespace labels</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-sharding-namespace-labels.png"/>
</imageobject>
<textobject><phrase>A diagram showing multiple Ingress Controllers with different namespace selectors serving routes that belong to the namespace containing a label that matches a given namespace selector</phrase></textobject>
</mediaobject>
</figure>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among
a set of Ingress Controllers and when isolating traffic to a specific Ingress
Controller. For example, company A goes to one Ingress Controller and company B
to another.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat router-internal.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: sharded
  namespace: openshift-ingress-operator
spec:
  domain: &lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO20-1"/>
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/worker: ""
  namespaceSelector:
    matchLabels:
      type: sharded</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO20-1">
<para>Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the Ingress Controller <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc apply -f router-internal.yaml</programlisting>
<simpara>The Ingress Controller selects routes in any namespace that is selected by the
namespace selector that have the label <literal>type: sharded</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create a new route using the domain configured in the <literal>router-internal.yaml</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-ingress-sharding-route-configuration_ingress-sharding">
<title>Creating a route for Ingress Controller sharding</title>
<simpara>A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.</simpara>
<simpara>The following procedure describes how to create a route for Ingress Controller sharding, using the <literal>hello-openshift</literal> application as an example.</simpara>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a project administrator.</simpara>
</listitem>
<listitem>
<simpara>You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.</simpara>
</listitem>
<listitem>
<simpara>You have configured the Ingress Controller for sharding.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a project called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a pod in the project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</programlisting>
</listitem>
<listitem>
<simpara>Create a service called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose pod/hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a route definition called <literal>hello-openshift-route.yaml</literal>:</simpara>
<formalpara>
<title>YAML definition of the created route for sharding:</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <co xml:id="CO21-1"/>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <co xml:id="CO21-2"/>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO21-1">
<para>Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value <literal>type: sharded</literal>.</para>
</callout>
<callout arearefs="CO21-2">
<para>The route will be exposed using the value of the <literal>subdomain</literal> field. When you specify the <literal>subdomain</literal> field, you must leave the hostname unset. If you specify both the <literal>host</literal> and <literal>subdomain</literal> fields, then the route will use the value of the <literal>host</literal> field, and ignore the <literal>subdomain</literal> field.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Use <literal>hello-openshift-route.yaml</literal> to create a route to the <literal>hello-openshift</literal> application by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-openshift create -f hello-openshift-route.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Get the status of the route with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml</programlisting>
<simpara>The resulting <literal>Route</literal> resource should look similar to the following:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.&lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO22-1"/>
    routerCanonicalHostname: router-sharded.&lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO22-2"/>
    routerName: sharded <co xml:id="CO22-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO22-1">
<para>The hostname the Ingress Controller, or router, uses to expose the route. The value of the <literal>host</literal> field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is <literal>&lt;apps-sharded.basedomain.example.net&gt;</literal>.</para>
</callout>
<callout arearefs="CO22-2">
<para>The hostname of the Ingress Controller.</para>
</callout>
<callout arearefs="CO22-3">
<para>The name of the Ingress Controller. In this example, the Ingress Controller has the name <literal>sharded</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<bridgehead xml:id="additional-resources_ingress-sharding" role="_additional-resources" renderas="sect2">Additional Resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#baseline-router-performance_routing-optimization">Baseline Ingress Controller (router) performance</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="ingress-node-firewall-operator">
<title>Ingress Node Firewall Operator in OpenShift Container Platform</title>

<simpara>The Ingress Node Firewall Operator allows administrators to manage firewall configurations at the node level.</simpara>
<section xml:id="installing-infw-operator_ingress-node-firewall-operator">
<title>Installing the Ingress Node Firewall Operator</title>
<simpara>As a cluster administrator, you can install the Ingress Node Firewall Operator by using the OpenShift Container Platform CLI or the web console.</simpara>
<section xml:id="install-operator-cli_ingress-node-firewall-operator">
<title>Installing the Ingress Node Firewall Operator using the CLI</title>
<simpara>As a cluster administrator, you can install the Operator using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have an account with administrator privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create the <literal>openshift-ingress-node-firewall</literal> namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: v1
kind: Namespace
metadata:
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/enforce-version: v1.24
  name: openshift-ingress-node-firewall
EOF</programlisting>
</listitem>
<listitem>
<simpara>To create an <literal>OperatorGroup</literal> CR, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: ingress-node-firewall-operators
  namespace: openshift-ingress-node-firewall
EOF</programlisting>
</listitem>
<listitem>
<simpara>Subscribe to the Ingress Node Firewall Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To create a <literal>Subscription</literal> CR for the Ingress Node Firewall Operator, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ingress-node-firewall-sub
  namespace: openshift-ingress-node-firewall
spec:
  name: ingress-node-firewall
  channel: stable
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To verify that the Operator is installed, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ip -n openshift-ingress-node-firewall</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME            CSV                                         APPROVAL    APPROVED
install-5cvnz   ingress-node-firewall.4.14.0-202211122336   Automatic   true</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To verify the version of the Operator, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-ingress-node-firewall</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        DISPLAY                          VERSION               REPLACES                                    PHASE
ingress-node-firewall.4.14.0-202211122336   Ingress Node Firewall Operator   4.14.0-202211122336   ingress-node-firewall.4.14.0-202211102047   Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="install-operator-web-console_ingress-node-firewall-operator">
<title>Installing the Ingress Node Firewall Operator using the web console</title>
<simpara>As a cluster administrator, you can install the Operator using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have an account with administrator privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the Ingress Node Firewall Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Ingress Node Firewall Operator</emphasis> from the list of available Operators, and then click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, under <emphasis role="strong">Installed Namespace</emphasis>, select <emphasis role="strong">Operator recommended Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the Ingress Node Firewall Operator is installed successfully:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Ensure that <emphasis role="strong">Ingress Node Firewall Operator</emphasis> is listed in the <emphasis role="strong">openshift-ingress-node-firewall</emphasis> project with a <emphasis role="strong">Status</emphasis> of <emphasis role="strong">InstallSucceeded</emphasis>.</simpara>
<note>
<simpara>During installation an Operator might display a <emphasis role="strong">Failed</emphasis> status.
If the installation later succeeds with an <emphasis role="strong">InstallSucceeded</emphasis> message, you can ignore the <emphasis role="strong">Failed</emphasis> message.</simpara>
</note>
<simpara>If the Operator does not have a <emphasis role="strong">Status</emphasis> of <emphasis role="strong">InstallSucceeded</emphasis>, troubleshoot using the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara>Inspect the <emphasis role="strong">Operator Subscriptions</emphasis> and <emphasis role="strong">Install Plans</emphasis> tabs for any failures or errors under <emphasis role="strong">Status</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and check the logs for pods in the <literal>openshift-ingress-node-firewall</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Check the namespace of the YAML file. If the annotation is missing, you can add the annotation <literal>workload.openshift.io/allowed=management</literal> to the Operator namespace with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate ns/openshift-ingress-node-firewall workload.openshift.io/allowed=management</programlisting>
<note>
<simpara>For single-node OpenShift clusters, the <literal>openshift-ingress-node-firewall</literal> namespace requires the <literal>workload.openshift.io/allowed=management</literal> annotation.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-infw-operator-cr_ingress-node-firewall-operator">
<title>Ingress Node Firewall Operator</title>
<simpara>The Ingress Node Firewall Operator provides ingress firewall rules at a node level by deploying the daemon set to nodes you specify and manage in the firewall configurations. To deploy the daemon set, you create an <literal>IngressNodeFirewallConfig</literal> custom resource (CR). The Operator applies the <literal>IngressNodeFirewallConfig</literal> CR to create ingress node firewall daemon set <literal>daemon</literal>, which run on all nodes that match the <literal>nodeSelector</literal>.</simpara>
<simpara>You configure <literal>rules</literal> of the <literal>IngressNodeFirewall</literal> CR and apply them to clusters using the <literal>nodeSelector</literal> and setting values to "true".</simpara>
<important>
<simpara>The Ingress Node Firewall Operator supports only stateless firewall rules.</simpara>
<simpara>Network interface controllers (NICs) that do not support native XDP drivers will run at a lower performance.</simpara>
<simpara>For OpenShift Container Platform 4.14 or later, you must run Ingress Node Firewall Operator on RHEL 9.0 or later.</simpara>
</important>
</section>
<section xml:id="nw-infw-operator-deploying_ingress-node-firewall-operator">
<title>Deploying Ingress Node Firewall Operator</title>
<itemizedlist>
<title>Prerequisite</title>
<listitem>
<simpara>The Ingress Node Firewall Operator is installed.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To deploy the Ingress Node Firewall Operator, create a <literal>IngressNodeFirewallConfig</literal> custom resource that will deploy the Operator&#8217;s daemon set. You can deploy one or multiple <literal>IngressNodeFirewall</literal> CRDs to nodes by applying firewall rules.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create the <literal>IngressNodeFirewallConfig</literal> inside the <literal>openshift-ingress-node-firewall</literal> namespace named <literal>ingressnodefirewallconfig</literal>.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to deploy Ingress Node Firewall Operator rules:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f rule.yaml</programlisting>
</listitem>
</orderedlist>
<section xml:id="nw-infw-operator-config-object_ingress-node-firewall-operator">
<title>Ingress Node Firewall configuration object</title>
<simpara>The fields for the Ingress Node Firewall configuration object are described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Ingress Node Firewall Configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CR object. The name of the firewall rules object must be <literal>ingressnodefirewallconfig</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Namespace for the Ingress Firewall Operator CR object. The <literal>IngressNodeFirewallConfig</literal> CR must be created inside the <literal>openshift-ingress-node-firewall</literal> namespace.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.nodeSelector</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A node selection constraint used to target nodes through specified node labels. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  nodeSelector:
    node-role.kubernetes.io/worker: ""</programlisting>
<note>
<simpara>One label used in <literal>nodeSelector</literal> must match a label on the nodes in order for the daemon set to start. For example, if the node labels <literal>node-role.kubernetes.io/worker</literal> and <literal>node-type.kubernetes.io/vm</literal> are applied to a node, then at least one label must be set using <literal>nodeSelector</literal> for the daemon set to start.</simpara>
</note></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>The Operator consumes the CR and creates an ingress node firewall daemon set on all the nodes that match the <literal>nodeSelector</literal>.</simpara>
</note>
<bridgehead xml:id="nw-ingress-node-firewall-example-cr-2_ingress-node-firewall-operator" renderas="sect3">Ingress Node Firewall Operator example configuration</bridgehead>
<simpara>A complete Ingress Node Firewall Configuration is specified in the following example:</simpara>
<formalpara>
<title>Example Ingress Node Firewall Configuration object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ingressnodefirewall.openshift.io/v1alpha1
kind: IngressNodeFirewallConfig
metadata:
  name: ingressnodefirewallconfig
  namespace: openshift-ingress-node-firewall
spec:
  nodeSelector:
    node-role.kubernetes.io/worker: ""</programlisting>
</para>
</formalpara>
<note>
<simpara>The Operator consumes the CR and creates an ingress node firewall daemon set on all the nodes that match the <literal>nodeSelector</literal>.</simpara>
</note>
</section>
<section xml:id="nw-ingress-node-firewall-operator-rules-object_ingress-node-firewall-operator">
<title>Ingress Node Firewall rules object</title>
<simpara>The fields for the Ingress Node Firewall rules object are described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Ingress Node Firewall rules object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CR object.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>interfaces</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The fields for this object specify the interfaces to apply the firewall rules to. For example, <literal>- en0</literal> and
<literal>- en1</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>nodeSelector</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>You can use <literal>nodeSelector</literal> to select the nodes to apply the firewall rules to. Set the value of your named <literal>nodeselector</literal> labels to <literal>true</literal> to apply the rule.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ingress</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>ingress</literal> allows you to configure the rules that allow outside access to the services on your cluster.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<bridgehead xml:id="nw-infw-ingress-rules-object_ingress-node-firewall-operator" renderas="sect5">Ingress object configuration</bridgehead>
<simpara>The values for the <literal>ingress</literal> object are defined in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ingress</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>sourceCIDRs</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Allows you to set the CIDR block. You can configure multiple CIDRs from different address families.</simpara>
<note>
<simpara>Different CIDRs allow you to use the same order rule. In the case that there are multiple <literal>IngressNodeFirewall</literal> objects for the same nodes and interfaces with overlapping CIDRs, the <literal>order</literal> field will specify which rule is applied first. Rules are applied in ascending order.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>rules</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Ingress firewall <literal>rules.order</literal> objects are ordered starting at <literal>1</literal> for each <literal>source.CIDR</literal> with up to 100 rules per CIDR. Lower order rules are executed first.</simpara>
<simpara><literal>rules.protocolConfig.protocol</literal> supports the following protocols: TCP, UDP, SCTP, ICMP and ICMPv6. ICMP and ICMPv6 rules can match against ICMP and ICMPv6 types or codes. TCP, UDP, and SCTP rules can match against a single destination port or a range of ports using <literal>&lt;start : end-1&gt;</literal> format.</simpara>
<simpara>Set <literal>rules.action</literal> to <literal>allow</literal> to apply the rule or <literal>deny</literal> to disallow the rule.</simpara>
<note>
<simpara>Ingress firewall rules are verified using a verification webhook that blocks any invalid configuration. The verification webhook prevents you from blocking any critical cluster services such as the API server or SSH.</simpara>
</note></entry>
</row>
</tbody>
</tgroup>
</table>
<bridgehead xml:id="nw-ingress-node-firewall-example-cr_ingress-node-firewall-operator" renderas="sect4">Ingress Node Firewall rules object example</bridgehead>
<simpara>A complete Ingress Node Firewall configuration is specified in the following example:</simpara>
<formalpara>
<title>Example Ingress Node Firewall configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ingressnodefirewall.openshift.io/v1alpha1
kind: IngressNodeFirewall
metadata:
  name: ingressnodefirewall
spec:
  interfaces:
  - eth0
  nodeSelector:
    matchLabels:
      &lt;ingress_firewall_label_name&gt;: &lt;label_value&gt; <co xml:id="CO23-1"/>
  ingress:
  - sourceCIDRs:
       - 172.16.0.0/12
    rules:
    - order: 10
      protocolConfig:
        protocol: ICMP
        icmp:
          icmpType: 8 #ICMP Echo request
      action: Deny
    - order: 20
      protocolConfig:
        protocol: TCP
        tcp:
          ports: "8000-9000"
      action: Deny
  - sourceCIDRs:
       - fc00:f853:ccd:e793::0/64
    rules:
    - order: 10
      protocolConfig:
        protocol: ICMPv6
        icmpv6:
          icmpType: 128 #ICMPV6 Echo request
      action: Deny</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO23-1">
<para>A &lt;label_name&gt; and a &lt;label_value&gt; must exist on the node and must match the <literal>nodeselector</literal> label and value applied to the nodes you want the <literal>ingressfirewallconfig</literal> CR to run on. The &lt;label_value&gt; can be <literal>true</literal> or <literal>false</literal>. By using <literal>nodeSelector</literal> labels, you can target separate groups of nodes to apply different rules to using the <literal>ingressfirewallconfig</literal> CR.</para>
</callout>
</calloutlist>
<bridgehead xml:id="nw-ingress-node-firewall-zero-trust-example-cr_ingress-node-firewall-operator" renderas="sect4">Zero trust Ingress Node Firewall rules object example</bridgehead>
<simpara>Zero trust Ingress Node Firewall rules can provide additional security to multi-interface clusters. For example, you can use zero trust Ingress Node Firewall rules to drop all traffic on a specific interface except for SSH.</simpara>
<simpara>A complete configuration of a zero trust Ingress Node Firewall rule set is specified in the following example:</simpara>
<important>
<simpara>Users need to add all ports their application will use to their allowlist in the following case to ensure proper functionality.</simpara>
</important>
<formalpara>
<title>Example zero trust Ingress Node Firewall rules</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ingressnodefirewall.openshift.io/v1alpha1
kind: IngressNodeFirewall
metadata:
 name: ingressnodefirewall-zero-trust
spec:
 interfaces:
 - eth1 <co xml:id="CO24-1"/>
 nodeSelector:
   matchLabels:
     &lt;ingress_firewall_label_name&gt;: &lt;label_value&gt; <co xml:id="CO24-2"/>
 ingress:
 - sourceCIDRs:
      - 0.0.0.0/0 <co xml:id="CO24-3"/>
   rules:
   - order: 10
     protocolConfig:
       protocol: TCP
       tcp:
         ports: 22
     action: Allow
   - order: 20
     action: Deny <co xml:id="CO24-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO24-1">
<para>Network-interface cluster</para>
</callout>
<callout arearefs="CO24-2">
<para>The &lt;label_name&gt; and &lt;label_value&gt; needs to match the <literal>nodeSelector</literal> label and value applied to the specific nodes with which you wish to apply the <literal>ingressfirewallconfig</literal> CR.</para>
</callout>
<callout arearefs="CO24-3">
<para><literal>0.0.0.0/0</literal> set to match any CIDR</para>
</callout>
<callout arearefs="CO24-4">
<para><literal>action</literal> set to <literal>Deny</literal></para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="nw-infw-operator-viewing_ingress-node-firewall-operator">
<title>Viewing Ingress Node Firewall Operator rules</title>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to view all current rules :</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ingressnodefirewall</programlisting>
</listitem>
<listitem>
<simpara>Choose one of the returned <literal>&lt;resource&gt;</literal> names and run the following command to view the rules or configs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get &lt;resource&gt; &lt;name&gt; -o yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-infw-operator-troubleshooting_ingress-node-firewall-operator">
<title>Troubleshooting the Ingress Node Firewall Operator</title>
<itemizedlist>
<listitem>
<simpara>Run the following command to list installed Ingress Node Firewall custom resource definitions (CRD):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crds | grep ingressnodefirewall</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME               READY   UP-TO-DATE   AVAILABLE   AGE
ingressnodefirewallconfigs.ingressnodefirewall.openshift.io       2022-08-25T10:03:01Z
ingressnodefirewallnodestates.ingressnodefirewall.openshift.io    2022-08-25T10:03:00Z
ingressnodefirewalls.ingressnodefirewall.openshift.io             2022-08-25T10:03:00Z</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command to view the state of the Ingress Node Firewall Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ingress-node-firewall</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       READY  STATUS         RESTARTS  AGE
ingress-node-firewall-controller-manager   2/2    Running        0         5d21h
ingress-node-firewall-daemon-pqx56         3/3    Running        0         5d21h</programlisting>
</para>
</formalpara>
<simpara>The following fields provide information about the status of the Operator:
<literal>READY</literal>, <literal>STATUS</literal>, <literal>AGE</literal>, and <literal>RESTARTS</literal>. The <literal>STATUS</literal> field is <literal>Running</literal> when the Ingress Node Firewall Operator is deploying a daemon set to the assigned nodes.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to collect all ingress firewall node pods' logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather – gather_ingress_node_firewall</programlisting>
<simpara>The logs are available in the sos node&#8217;s report containing eBPF <literal>bpftool</literal> outputs at <literal>/sos_commands/ebpf</literal>. These reports include lookup tables used or updated as the ingress firewall XDP handles packet processing, updates statistics, and emits events.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="ingress-controller-dnsmgt">
<title>Configuring an Ingress Controller for manual DNS Management</title>

<simpara>As a cluster administrator, when you create an Ingress Controller, the Operator manages the DNS records automatically. This has some limitations when the required DNS zone is different from the cluster DNS zone or when the DNS zone is hosted outside the cloud provider.</simpara>
<simpara>As a cluster administrator, you can configure an Ingress Controller to stop automatic DNS management and start manual DNS management. Set <literal>dnsManagementPolicy</literal> to specify when it should be automatically or manually managed.</simpara>
<simpara>When you change an Ingress Controller from <literal>Managed</literal> to <literal>Unmanaged</literal> DNS management policy, the Operator does not clean up the previous wildcard DNS record provisioned on the cloud.
When you change an Ingress Controller from <literal>Unmanaged</literal> to <literal>Managed</literal> DNS management policy, the Operator attempts to create the DNS record on the cloud provider if it does not exist or updates the DNS record if it already exists.</simpara>
<important>
<simpara>When you set <literal>dnsManagementPolicy</literal> to <literal>unmanaged</literal>, you have to manually manage the lifecycle of the wildcard DNS record on the cloud provider.</simpara>
</important>
<section xml:id="_managed-dns-management-policy">
<title><literal>Managed</literal> DNS management policy</title>
<simpara>The <literal>Managed</literal> DNS management policy for Ingress Controllers ensures that the lifecycle of the wildcard DNS record on the cloud provider is automatically managed by the Operator.</simpara>
</section>
<section xml:id="_unmanaged-dns-management-policy">
<title><literal>Unmanaged</literal> DNS management policy</title>
<simpara>The <literal>Unmanaged</literal> DNS management policy for Ingress Controllers ensures that the lifecycle of the wildcard DNS record on the cloud provider is not automatically managed, instead it becomes the responsibility of the cluster administrator.</simpara>
<note>
<simpara>On the AWS cloud platform, if the domain on the Ingress Controller does not match with <literal>dnsConfig.Spec.BaseDomain</literal> then the DNS management policy is automatically set to <literal>Unmanaged</literal>.</simpara>
</note>
</section>
<section xml:id="creating-a-custom-ingress-controller_ingress-controller-dnsmgt">
<title>Creating a custom Ingress Controller with the <literal>Unmanaged</literal> DNS management policy</title>
<simpara>As a cluster administrator, you can create a new custom Ingress Controller with the <literal>Unmanaged</literal> DNS management policy.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a custom resource (CR) file named <literal>sample-ingress.yaml</literal> containing the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: &lt;name&gt; <co xml:id="CO25-1"/>
spec:
  domain: &lt;domain&gt; <co xml:id="CO25-2"/>
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: External <co xml:id="CO25-3"/>
      dnsManagementPolicy: Unmanaged <co xml:id="CO25-4"/></programlisting>
<calloutlist>
<callout arearefs="CO25-1">
<para>Specify the <literal>&lt;name&gt;</literal> with a name for the <literal>IngressController</literal> object.</para>
</callout>
<callout arearefs="CO25-2">
<para>Specify the <literal>domain</literal> based on the DNS record that was created as a prerequisite.</para>
</callout>
<callout arearefs="CO25-3">
<para>Specify the <literal>scope</literal> as <literal>External</literal> to expose the load balancer externally.</para>
</callout>
<callout arearefs="CO25-4">
<para><literal>dnsManagementPolicy</literal> indicates if the Ingress Controller is managing the lifecycle of the wildcard DNS record associated with the load balancer.
The valid values are <literal>Managed</literal> and <literal>Unmanaged</literal>. The default value is <literal>Managed</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc apply -f &lt;name&gt;.yaml <co xml:id="CO26-1"/></programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying-an-existing-ingress-controller_ingress-controller-dnsmgt">
<title>Modifying an existing Ingress Controller</title>
<simpara>As a cluster administrator, you can modify an existing Ingress Controller to manually manage the DNS record lifecycle.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the chosen <literal>IngressController</literal> to set <literal>dnsManagementPolicy</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">SCOPE=$(oc -n openshift-ingress-operator get ingresscontroller &lt;name&gt; -o=jsonpath="{.status.endpointPublishingStrategy.loadBalancer.scope}")

oc -n openshift-ingress-operator patch ingresscontrollers/&lt;name&gt; --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"dnsManagementPolicy":"Unmanaged", "scope":"${SCOPE}"}}}}'</programlisting>
</listitem>
<listitem>
<simpara>Optional: You can delete the associated DNS record in the cloud provider.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-ingress-controller-dns-management-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration parameters</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="nw-ingress-controller-endpoint-publishing-strategies">
<title>Configuring the Ingress Controller endpoint publishing strategy</title>

<section xml:id="nw-ingress-controller-endpoint-publishing-strategies_nw-ingress-controller-endpoint-publishing-strategies">
<title>Ingress Controller endpoint publishing strategy</title>
<simpara><emphasis role="strong"><literal>NodePortService</literal> endpoint publishing strategy</emphasis></simpara>
<simpara>The <literal>NodePortService</literal> endpoint publishing strategy publishes the Ingress Controller using a Kubernetes NodePort service.</simpara>
<simpara>In this configuration, the Ingress Controller deployment uses container networking. A <literal>NodePortService</literal> is created to publish the deployment. The specific node ports are dynamically allocated by OpenShift Container Platform; however, to support static port allocations, your changes to the node port field of the managed <literal>NodePortService</literal> are preserved.</simpara>
<figure>
<title>Diagram of NodePortService</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/202_OpenShift_Ingress_0222_node_port.png"/>
</imageobject>
<textobject><phrase>OpenShift Container Platform Ingress NodePort endpoint publishing strategy</phrase></textobject>
</mediaobject>
</figure>
<simpara>The preceding graphic shows the following concepts pertaining to OpenShift Container Platform Ingress NodePort endpoint publishing strategy:</simpara>
<itemizedlist>
<listitem>
<simpara>All the available nodes in the cluster have their own, externally accessible IP addresses. The service running in the cluster is bound to the unique NodePort for all the nodes.</simpara>
</listitem>
<listitem>
<simpara>When the client connects to a node that is down, for example, by connecting the <literal>10.0.128.4</literal> IP address in the graphic, the node port directly connects the client to an available node that is running the service. In this scenario, no load balancing is required. As the image shows, the <literal>10.0.128.4</literal> address is down and another IP address must be used instead.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The Ingress Operator ignores any updates to <literal>.spec.ports[].nodePort</literal> fields of the service.</simpara>
<simpara>By default, ports are allocated automatically and you can access the port allocations for integrations. However, sometimes static port allocations are necessary to integrate with existing infrastructure which may not be easily reconfigured in response to dynamic ports. To achieve integrations with static node ports, you can update the managed service resource directly.</simpara>
</note>
<simpara>For more information, see the <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/service/#nodeport">Kubernetes Services documentation on <literal>NodePort</literal></link>.</simpara>
<simpara><emphasis role="strong"><literal>HostNetwork</literal> endpoint publishing strategy</emphasis></simpara>
<simpara>The <literal>HostNetwork</literal> endpoint publishing strategy publishes the Ingress Controller on node ports where the Ingress Controller is deployed.</simpara>
<simpara>An Ingress Controller with the <literal>HostNetwork</literal> endpoint publishing strategy can have only one pod replica per node. If you want <emphasis>n</emphasis> replicas, you must use at least <emphasis>n</emphasis> nodes where those replicas can be scheduled. Because each pod replica requests ports <literal>80</literal> and <literal>443</literal> on the node host where it is scheduled, a replica cannot be scheduled to a node if another pod on the same node is using those ports.</simpara>
<section xml:id="nw-ingresscontroller-change-internal_nw-ingress-controller-endpoint-publishing-strategies">
<title>Configuring the Ingress Controller endpoint publishing scope to Internal</title>
<simpara>When a cluster administrator installs a new cluster without specifying that the cluster is private, the default Ingress Controller is created with a <literal>scope</literal> set to <literal>External</literal>. Cluster administrators can change an <literal>External</literal> scoped Ingress Controller to <literal>Internal</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To change an <literal>External</literal> scoped Ingress Controller to <literal>Internal</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"scope":"Internal"}}}}'</programlisting>
</listitem>
<listitem>
<simpara>To check the status of the Ingress Controller, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator get ingresscontrollers/default -o yaml</programlisting>
<itemizedlist>
<listitem>
<simpara>The <literal>Progressing</literal> status condition indicates whether you must take further action. For example, the status condition can indicate that you need to delete the service by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress delete services/router-default</programlisting>
<simpara>If you delete the service, the Ingress Operator recreates it as <literal>Internal</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingresscontroller-change-external_nw-ingress-controller-endpoint-publishing-strategies">
<title>Configuring the Ingress Controller endpoint publishing scope to External</title>
<simpara>When a cluster administrator installs a new cluster without specifying that the cluster is private, the default Ingress Controller is created with a <literal>scope</literal> set to <literal>External</literal>.</simpara>
<simpara>The Ingress Controller&#8217;s scope can be configured to be <literal>Internal</literal> during installation or after, and cluster administrators can change an <literal>Internal</literal> Ingress Controller to <literal>External</literal>.</simpara>
<important>
<simpara>On some platforms, it is necessary to delete and recreate the service.</simpara>
<simpara>Changing the scope can cause disruption to Ingress traffic, potentially for several minutes. This applies to platforms where it is necessary to delete and recreate the service, because the procedure can cause OpenShift Container Platform to deprovision the existing service load balancer, provision a new one, and update DNS.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To change an <literal>Internal</literal> scoped Ingress Controller to <literal>External</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontrollers/private --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"scope":"External"}}}}'</programlisting>
</listitem>
<listitem>
<simpara>To check the status of the Ingress Controller, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator get ingresscontrollers/default -o yaml</programlisting>
<itemizedlist>
<listitem>
<simpara>The <literal>Progressing</literal> status condition indicates whether you must take further action. For example, the status condition can indicate that you need to delete the service by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress delete services/router-default</programlisting>
<simpara>If you delete the service, the Ingress Operator recreates it as <literal>External</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_nw-ingress-controller-endpoint-publishing-strategies" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration parameters</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="verifying-connectivity-endpoint">
<title>Verifying connectivity to an endpoint</title>

<simpara>The Cluster Network Operator (CNO) runs a controller, the connectivity check controller, that performs a connection health check between resources within your cluster.
By reviewing the results of the health checks, you can diagnose connection problems or eliminate network connectivity as the cause of an issue that you are investigating.</simpara>
<section xml:id="nw-pod-network-connectivity-checks_verifying-connectivity-endpoint">
<title>Connection health checks performed</title>
<simpara>To verify that cluster resources are reachable, a TCP connection is made to each of the following cluster API services:</simpara>
<itemizedlist>
<listitem>
<simpara>Kubernetes API server service</simpara>
</listitem>
<listitem>
<simpara>Kubernetes API server endpoints</simpara>
</listitem>
<listitem>
<simpara>OpenShift API server service</simpara>
</listitem>
<listitem>
<simpara>OpenShift API server endpoints</simpara>
</listitem>
<listitem>
<simpara>Load balancers</simpara>
</listitem>
</itemizedlist>
<simpara>To verify that services and service endpoints are reachable on every node in the cluster, a TCP connection is made to each of the following targets:</simpara>
<itemizedlist>
<listitem>
<simpara>Health check target service</simpara>
</listitem>
<listitem>
<simpara>Health check target endpoints</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-pod-network-connectivity-implementation_verifying-connectivity-endpoint">
<title>Implementation of connection health checks</title>
<simpara>The connectivity check controller orchestrates connection verification checks in your cluster. The results for the connection tests are stored in <literal>PodNetworkConnectivity</literal> objects in the <literal>openshift-network-diagnostics</literal> namespace. Connection tests are performed every minute in parallel.</simpara>
<simpara>The Cluster Network Operator (CNO) deploys several resources to the cluster to send and receive connectivity health checks:</simpara>
<variablelist>
<varlistentry>
<term>Health check source</term>
<listitem>
<simpara>This program deploys in a single pod replica set managed by a <literal>Deployment</literal> object. The program consumes <literal>PodNetworkConnectivity</literal> objects and connects to the <literal>spec.targetEndpoint</literal> specified in each object.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Health check target</term>
<listitem>
<simpara>A pod deployed as part of a daemon set on every node in the cluster. The pod listens for inbound health checks. The presence of this pod on every node allows for the testing of connectivity to each node.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="nw-pod-network-connectivity-check-object_verifying-connectivity-endpoint">
<title>PodNetworkConnectivityCheck object fields</title>
<simpara>The <literal>PodNetworkConnectivityCheck</literal> object fields are described in the following tables.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>PodNetworkConnectivityCheck object fields</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="50.0001*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the object in the following format: <literal>&lt;source&gt;-to-&lt;target&gt;</literal>. The destination described by <literal>&lt;target&gt;</literal> includes one of following strings:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>load-balancer-api-external</literal></simpara>
</listitem>
<listitem>
<simpara><literal>load-balancer-api-internal</literal></simpara>
</listitem>
<listitem>
<simpara><literal>kubernetes-apiserver-endpoint</literal></simpara>
</listitem>
<listitem>
<simpara><literal>kubernetes-apiserver-service-cluster</literal></simpara>
</listitem>
<listitem>
<simpara><literal>network-check-target</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift-apiserver-endpoint</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift-apiserver-service-cluster</literal></simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The namespace that the object is associated with. This value is always <literal>openshift-network-diagnostics</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.sourcePod</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the pod where the connection check originates, such as <literal>network-check-source-596b4c6566-rgh92</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.targetEndpoint</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The target of the connection check, such as <literal>api.devcluster.example.com:6443</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.tlsClientCert</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configuration for the TLS certificate to use.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.tlsClientCert.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the TLS certificate used, if any. The default value is an empty string.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>status</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="top"><simpara>An object representing the condition of the connection test and logs of recent connection successes and failures.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>status.conditions</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The latest status of the connection check and any previous statuses.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>status.failures</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connection test logs from unsuccessful attempts.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>status.outages</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connect test logs covering the time periods of any outages.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>status.successes</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connection test logs from successful attempts.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The following table describes the fields for objects in the <literal>status.conditions</literal> array:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>status.conditions</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="50.0001*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>lastTransitionTime</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The time that the condition of the connection transitioned from one status to another.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>message</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The details about last transition in a human readable format.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>reason</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The last status of the transition in a machine readable format.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>status</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The status of the condition.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The type of the condition.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The following table describes the fields for objects in the <literal>status.conditions</literal> array:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>status.outages</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="50.0001*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>end</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The timestamp from when the connection failure is resolved.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>endLogs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connection log entries, including the log entry related to the successful end of the outage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>message</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A summary of outage details in a human readable format.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>start</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The timestamp from when the connection failure is first detected.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>startLogs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connection log entries, including the original failure.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<bridgehead xml:id="_connection-log-fields" renderas="sect3">Connection log fields</bridgehead>
<simpara>The fields for a connection log entry are described in the following table. The object is used in the following fields:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>status.failures[]</literal></simpara>
</listitem>
<listitem>
<simpara><literal>status.successes[]</literal></simpara>
</listitem>
<listitem>
<simpara><literal>status.outages[].startLogs[]</literal></simpara>
</listitem>
<listitem>
<simpara><literal>status.outages[].endLogs[]</literal></simpara>
</listitem>
</itemizedlist>
<table frame="all" rowsep="1" colsep="1">
<title>Connection log object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="50.0001*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>latency</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Records the duration of the action.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>message</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides the status in a human readable format.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>reason</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides the reason for status in a machine readable format. The value is one of <literal>TCPConnect</literal>, <literal>TCPConnectError</literal>, <literal>DNSResolve</literal>, <literal>DNSError</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>success</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates if the log entry is a success or failure.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>time</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The start time of connection check.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-pod-network-connectivity-verify_verifying-connectivity-endpoint">
<title>Verifying network connectivity for an endpoint</title>
<simpara>As a cluster administrator, you can verify the connectivity of an endpoint, such as an API server, load balancer, service, or pod.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To list the current <literal>PodNetworkConnectivityCheck</literal> objects, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get podnetworkconnectivitycheck -n openshift-network-diagnostics</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                                                                                                AGE
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0   75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-1   73m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-2   75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-service-cluster                               75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-default-service-cluster                                 75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-load-balancer-api-external                                         75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-load-balancer-api-internal                                         75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-master-0            75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-master-1            75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-master-2            75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh      74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-worker-c-n8mbf      74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-ci-ln-x5sv9rb-f76d1-4rzrp-worker-d-4hnrz      74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-network-check-target-service-cluster                               75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0    75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-1    75m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-2    74m
network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-openshift-apiserver-service-cluster                                75m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the connection test logs:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>From the output of the previous command, identify the endpoint that you want to review the connectivity logs for.</simpara>
</listitem>
<listitem>
<simpara>To view the object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get podnetworkconnectivitycheck &lt;name&gt; \
  -n openshift-network-diagnostics -o yaml</programlisting>
<simpara>where <literal>&lt;name&gt;</literal> specifies the name of the <literal>PodNetworkConnectivityCheck</literal> object.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: controlplane.operator.openshift.io/v1alpha1
kind: PodNetworkConnectivityCheck
metadata:
  name: network-check-source-ci-ln-x5sv9rb-f76d1-4rzrp-worker-b-6xdmh-to-kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0
  namespace: openshift-network-diagnostics
  ...
spec:
  sourcePod: network-check-source-7c88f6d9f-hmg2f
  targetEndpoint: 10.0.0.4:6443
  tlsClientCert:
    name: ""
status:
  conditions:
  - lastTransitionTime: "2021-01-13T20:11:34Z"
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnectSuccess
    status: "True"
    type: Reachable
  failures:
  - latency: 2.241775ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: failed
      to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443: connect:
      connection refused'
    reason: TCPConnectError
    success: false
    time: "2021-01-13T20:10:34Z"
  - latency: 2.582129ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: failed
      to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443: connect:
      connection refused'
    reason: TCPConnectError
    success: false
    time: "2021-01-13T20:09:34Z"
  - latency: 3.483578ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: failed
      to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443: connect:
      connection refused'
    reason: TCPConnectError
    success: false
    time: "2021-01-13T20:08:34Z"
  outages:
  - end: "2021-01-13T20:11:34Z"
    endLogs:
    - latency: 2.032018ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        tcp connection to 10.0.0.4:6443 succeeded'
      reason: TCPConnect
      success: true
      time: "2021-01-13T20:11:34Z"
    - latency: 2.241775ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:10:34Z"
    - latency: 2.582129ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:09:34Z"
    - latency: 3.483578ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:08:34Z"
    message: Connectivity restored after 2m59.999789186s
    start: "2021-01-13T20:08:34Z"
    startLogs:
    - latency: 3.483578ms
      message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0:
        failed to establish a TCP connection to 10.0.0.4:6443: dial tcp 10.0.0.4:6443:
        connect: connection refused'
      reason: TCPConnectError
      success: false
      time: "2021-01-13T20:08:34Z"
  successes:
  - latency: 2.845865ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:14:34Z"
  - latency: 2.926345ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:13:34Z"
  - latency: 2.895796ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:12:34Z"
  - latency: 2.696844ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:11:34Z"
  - latency: 1.502064ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:10:34Z"
  - latency: 1.388857ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:09:34Z"
  - latency: 1.906383ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:08:34Z"
  - latency: 2.089073ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:07:34Z"
  - latency: 2.156994ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:06:34Z"
  - latency: 1.777043ms
    message: 'kubernetes-apiserver-endpoint-ci-ln-x5sv9rb-f76d1-4rzrp-master-0: tcp
      connection to 10.0.0.4:6443 succeeded'
    reason: TCPConnect
    success: true
    time: "2021-01-13T21:05:34Z"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="changing-cluster-network-mtu">
<title>Changing the MTU for the cluster network</title>

<simpara role="_abstract">As a cluster administrator, you can change the MTU for the cluster network after cluster installation. This change is disruptive as cluster nodes must be rebooted to finalize the MTU change. You can change the MTU only for clusters using the OVN-Kubernetes or OpenShift SDN network plugins.</simpara>
<section xml:id="nw-cluster-mtu-change-about_changing-cluster-network-mtu">
<title>About the cluster MTU</title>
<simpara>During installation the maximum transmission unit (MTU) for the cluster network is detected automatically based on the MTU of the primary network interface of nodes in the cluster. You do not usually need to override the detected MTU.</simpara>
<simpara>You might want to change the MTU of the cluster network for several reasons:</simpara>
<itemizedlist>
<listitem>
<simpara>The MTU detected during cluster installation is not correct for your infrastructure.</simpara>
</listitem>
<listitem>
<simpara>Your cluster infrastructure now requires a different MTU, such as from the addition of nodes that need a different MTU for optimal performance.</simpara>
</listitem>
</itemizedlist>
<simpara>You can change the cluster MTU for only the OVN-Kubernetes and OpenShift SDN cluster network plugins.</simpara>
<section xml:id="service-interruption-considerations_changing-cluster-network-mtu">
<title>Service interruption considerations</title>
<simpara>When you initiate an MTU change on your cluster the following effects might impact service availability:</simpara>
<itemizedlist>
<listitem>
<simpara>At least two rolling reboots are required to complete the migration to a new MTU. During this time, some nodes are not available as they restart.</simpara>
</listitem>
<listitem>
<simpara>Specific applications deployed to the cluster with shorter timeout intervals than the absolute TCP timeout interval might experience disruption during the MTU change.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mtu-value-selection_changing-cluster-network-mtu">
<title>MTU value selection</title>
<simpara>When planning your MTU migration there are two related but distinct MTU values to consider.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Hardware MTU</emphasis>: This MTU value is set based on the specifics of your network infrastructure.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cluster network MTU</emphasis>: This MTU value is always less than your hardware MTU to account for the cluster network overlay overhead. The specific overhead is determined by your network plugin:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">OVN-Kubernetes</emphasis>: <literal>100</literal> bytes</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OpenShift SDN</emphasis>: <literal>50</literal> bytes</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>If your cluster requires different MTU values for different nodes, you must subtract the overhead value for your network plugin from the lowest MTU value that is used by any node in your cluster. For example, if some nodes in your cluster have an MTU of <literal>9001</literal>, and some have an MTU of <literal>1500</literal>, you must set this value to <literal>1400</literal>.</simpara>
</section>
<section xml:id="how-the-migration-process-works_changing-cluster-network-mtu">
<title>How the migration process works</title>
<simpara>The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Live migration of the cluster MTU</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">User-initiated steps</entry>
<entry align="left" valign="top">OpenShift Container Platform activity</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Set the following values in the Cluster Network Operator configuration:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>spec.migration.mtu.machine.to</literal></simpara>
</listitem>
<listitem>
<simpara><literal>spec.migration.mtu.network.from</literal></simpara>
</listitem>
<listitem>
<simpara><literal>spec.migration.mtu.network.to</literal></simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cluster Network Operator (CNO)</emphasis>: Confirms that each field is set to a valid value.</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>mtu.machine.to</literal> must be set to either the new hardware MTU or to the current hardware MTU if the MTU for the hardware is not changing. This value is transient and is used as part of the migration process. Separately, if you specify a hardware MTU that is different from your existing hardware MTU value, you must manually configure the MTU to persist by other means, such as with a machine config, DHCP setting, or a Linux kernel command line.</simpara>
</listitem>
<listitem>
<simpara>The <literal>mtu.network.from</literal> field must equal the <literal>network.status.clusterNetworkMTU</literal> field, which is the current MTU of the cluster network.</simpara>
</listitem>
<listitem>
<simpara>The <literal>mtu.network.to</literal> field must be set to the target cluster network MTU and must be lower than the hardware MTU to allow for the overlay overhead of the network plugin. For OVN-Kubernetes, the overhead is <literal>100</literal> bytes and for OpenShift SDN the overhead is <literal>50</literal> bytes.</simpara>
</listitem>
</itemizedlist>
<simpara>If the values provided are valid, the CNO writes out a new temporary configuration with the MTU for the cluster network set to the value of the <literal>mtu.network.to</literal> field.</simpara>
<simpara><emphasis role="strong">Machine Config Operator (MCO)</emphasis>: Performs a rolling reboot of each node in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reconfigure the MTU of the primary network interface for the nodes on the cluster. You can use a variety of methods to accomplish this, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Deploying a new NetworkManager connection profile with the MTU change</simpara>
</listitem>
<listitem>
<simpara>Changing the MTU through a DHCP server setting</simpara>
</listitem>
<listitem>
<simpara>Changing the MTU through boot parameters</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara>N/A</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Set the <literal>mtu</literal> value in the CNO configuration for the network plugin and set <literal>spec.migration</literal> to <literal>null</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Machine Config Operator (MCO)</emphasis>: Performs a rolling reboot of each node in the cluster with the new MTU configuration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="nw-cluster-mtu-change_changing-cluster-network-mtu">
<title>Changing the cluster MTU</title>
<simpara>As a cluster administrator, you can change the maximum transmission unit (MTU) for your cluster. The migration is disruptive and nodes in your cluster might be temporarily unavailable as the MTU update rolls out.</simpara>
<simpara>The following procedure describes how to change the cluster MTU by using either machine configs, DHCP, or an ISO. If you use the DHCP or ISO approach, you must refer to configuration artifacts that you kept after installing your cluster to complete the procedure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You identified the target MTU for your cluster. The correct MTU varies depending on the network plugin that your cluster uses:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">OVN-Kubernetes</emphasis>: The cluster MTU must be set to <literal>100</literal> less than the lowest hardware MTU value in your cluster.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OpenShift SDN</emphasis>: The cluster MTU must be set to <literal>50</literal> less than the lowest hardware MTU value in your cluster.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To increase or decrease the MTU for the cluster network complete the following procedure.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To obtain the current MTU for the cluster network, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe network.config cluster</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">...
Status:
  Cluster Network:
    Cidr:               10.217.0.0/22
    Host Prefix:        23
  Cluster Network MTU:  1400
  Network Type:         OpenShiftSDN
  Service Network:
    10.217.4.0/23
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Prepare your configuration for the hardware MTU:</simpara>
<itemizedlist>
<listitem>
<simpara>If your hardware MTU is specified with DHCP, update your DHCP configuration such as with the following dnsmasq configuration:</simpara>
<programlisting language="text" linenumbering="unnumbered">dhcp-option-force=26,&lt;mtu&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;mtu&gt;</literal></term>
<listitem>
<simpara>Specifies the hardware MTU for the DHCP server to advertise.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>If your hardware MTU is specified with a kernel command line with PXE, update that configuration accordingly.</simpara>
</listitem>
<listitem>
<simpara>If your hardware MTU is specified in a NetworkManager connection configuration, complete the following steps. This approach is the default for OpenShift Container Platform if you do not explicitly specify your network configuration with DHCP, a kernel command line, or some other method. Your cluster nodes must all use the same underlying network configuration for the following procedure to work unmodified.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Find the primary network interface:</simpara>
<itemizedlist>
<listitem>
<simpara>If you are using the OpenShift SDN network plugin, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt; -- chroot /host ip route list match 0.0.0.0/0 | awk '{print $5 }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;node_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a node in your cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>If you are using the OVN-Kubernetes network plugin, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt; -- chroot /host nmcli -g connection.interface-name c show ovs-if-phys0</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;node_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a node in your cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the following NetworkManager configuration in the <literal>&lt;interface&gt;-mtu.conf</literal> file:</simpara>
<formalpara>
<title>Example NetworkManager connection configuration</title>
<para>
<programlisting language="ini" linenumbering="unnumbered">[connection-&lt;interface&gt;-mtu]
match-device=interface-name:&lt;interface&gt;
ethernet.mtu=&lt;mtu&gt;</programlisting>
</para>
</formalpara>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;mtu&gt;</literal></term>
<listitem>
<simpara>Specifies the new hardware MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;interface&gt;</literal></term>
<listitem>
<simpara>Specifies the primary network interface name.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Create two <literal>MachineConfig</literal> objects, one for the control plane nodes and another for the worker nodes in your cluster:</simpara>
<orderedlist numeration="upperalpha">
<listitem>
<simpara>Create the following Butane config in the <literal>control-plane-interface.bu</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">variant: openshift
version: 4.14.0
metadata:
  name: 01-control-plane-interface
  labels:
    machineconfiguration.openshift.io/role: master
storage:
  files:
    - path: /etc/NetworkManager/conf.d/99-&lt;interface&gt;-mtu.conf <co xml:id="CO26-2"/>
      contents:
        local: &lt;interface&gt;-mtu.conf <co xml:id="CO26-3"/>
      mode: 0600</programlisting>
<calloutlist>
<callout arearefs="CO26-1 CO26-2">
<para>Specify the NetworkManager connection name for the primary network interface.</para>
</callout>
<callout arearefs="CO26-3">
<para>Specify the local filename for the updated NetworkManager configuration file from the previous step.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the following Butane config in the <literal>worker-interface.bu</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">variant: openshift
version: 4.14.0
metadata:
  name: 01-worker-interface
  labels:
    machineconfiguration.openshift.io/role: worker
storage:
  files:
    - path: /etc/NetworkManager/conf.d/99-&lt;interface&gt;-mtu.conf <co xml:id="CO27-1"/>
      contents:
        local: &lt;interface&gt;-mtu.conf <co xml:id="CO27-2"/>
      mode: 0600</programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para>Specify the NetworkManager connection name for the primary network interface.</para>
</callout>
<callout arearefs="CO27-2">
<para>Specify the local filename for the updated NetworkManager configuration file from the previous step.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create <literal>MachineConfig</literal> objects from the Butane configs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for manifest in control-plane-interface worker-interface; do
    butane --files-dir . $manifest.bu &gt; $manifest.yaml
  done</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To begin the MTU migration, specify the migration configuration by entering the following command. The Machine Config Operator performs a rolling reboot of the nodes in the cluster in preparation for the MTU change.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": &lt;overlay_from&gt;, "to": &lt;overlay_to&gt; } , "machine": { "to" : &lt;machine_to&gt; } } } } }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;overlay_from&gt;</literal></term>
<listitem>
<simpara>Specifies the current cluster network MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;overlay_to&gt;</literal></term>
<listitem>
<simpara>Specifies the target MTU for the cluster network. This value is set relative to the value for <literal>&lt;machine_to&gt;</literal> and for OVN-Kubernetes must be <literal>100</literal> less and for OpenShift SDN must be <literal>50</literal> less.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;machine_to&gt;</literal></term>
<listitem>
<simpara>Specifies the MTU for the primary network interface on the underlying host network.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example that increases the cluster MTU</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": 1400, "to": 9000 } , "machine": { "to" : 9100} } } } }'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
<note>
<simpara>By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
<simpara>The machine config must include the following update to the systemd configuration:</simpara>
<programlisting language="plain" linenumbering="unnumbered">ExecStart=/usr/local/bin/mtu-migration.sh</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Update the underlying network interface MTU value:</simpara>
<itemizedlist>
<listitem>
<simpara>If you are specifying the new MTU with a NetworkManager connection configuration, enter the following command. The MachineConfig Operator automatically performs a rolling reboot of the nodes in your cluster.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for manifest in control-plane-interface worker-interface; do
    oc create -f $manifest.yaml
  done</programlisting>
</listitem>
<listitem>
<simpara>If you are specifying the new MTU with a DHCP server option or a kernel command line and PXE, make the necessary changes for your infrastructure.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
<note>
<simpara>By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep path:</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
<simpara>If the machine config is successfully deployed, the previous output contains the <literal>/etc/NetworkManager/system-connections/&lt;connection_name&gt;</literal> file path.</simpara>
<simpara>The machine config must not contain the <literal>ExecStart=/usr/local/bin/mtu-migration.sh</literal> line.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To finalize the MTU migration, enter one of the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>If you are using the OVN-Kubernetes network plugin:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "ovnKubernetesConfig": { "mtu": &lt;mtu&gt; }}}}'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;mtu&gt;</literal></term>
<listitem>
<simpara>Specifies the new cluster network MTU that you specified with <literal>&lt;overlay_to&gt;</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>If you are using the OpenShift SDN network plugin:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "openshiftSDNConfig": { "mtu": &lt;mtu&gt; }}}}'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;mtu&gt;</literal></term>
<listitem>
<simpara>Specifies the new cluster network MTU that you specified with <literal>&lt;overlay_to&gt;</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After finalizing the MTU migration, each MCP node is rebooted one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>You can verify that a node in your cluster uses an MTU that you specified in the previous procedure.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To get the current MTU for the cluster network, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe network.config cluster</programlisting>
</listitem>
<listitem>
<simpara>Get the current MTU for the primary network interface of a node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the nodes in your cluster, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>To obtain the current MTU setting for the primary network interface on a node, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node&gt; -- chroot /host ip address show &lt;interface&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;node&gt;</literal></term>
<listitem>
<simpara>Specifies a node from the output from the previous step.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;interface&gt;</literal></term>
<listitem>
<simpara>Specifies the primary network interface name for the node.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8051</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="changing-cluster-network-mtu-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installation-user-infra-machines-advanced_network_installing-bare-metal">Using advanced networking options for PXE and ISO installations</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_networking/index#proc_manually-creating-a-networkmanager-profile-in-keyfile-format_assembly_networkmanager-connection-profiles-in-keyfile-format">Manually creating NetworkManager profiles in key file format</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/configuring_and_managing_networking/index#configuring-a-dynamic-ethernet-connection-using-nmcli_configuring-an-ethernet-connection">Configuring a dynamic Ethernet connection using nmcli</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring-node-port-service-range">
<title>Configuring the node port service range</title>

<simpara>As a cluster administrator, you can expand the available node port range. If your cluster uses of a large number of node ports, you might need to increase the number of available ports.</simpara>
<simpara>The default port range is <literal>30000-32767</literal>. You can never reduce the port range, even if you first expand it beyond the default range.</simpara>
<section xml:id="configuring-node-port-service-range-prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your cluster infrastructure must allow access to the ports that you specify within the expanded range. For example, if you expand the node port range to <literal>30000-32900</literal>, the inclusive port range of <literal>32768-32900</literal> must be allowed by your firewall or packet filtering configuration.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-nodeport-service-range-edit_configuring-node-port-service-range">
<title>Expanding the node port range</title>
<simpara>You can expand the node port range for the cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To expand the node port range, enter the following command. Replace <literal>&lt;port&gt;</literal> with the largest port number in the new range.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch network.config.openshift.io cluster --type=merge -p \
  '{
    "spec":
      { "serviceNodePortRange": "30000-&lt;port&gt;" }
  }'</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to update the node port range:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNodePortRange: "30000-&lt;port&gt;"</programlisting>
</tip>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">network.config.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmaps -n openshift-kube-apiserver config \
  -o jsonpath="{.data['config\.yaml']}" | \
  grep -Eo '"service-node-port-range":["[[:digit:]]+-[[:digit:]]+"]'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"service-node-port-range":["30000-33000"]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-node-port-service-range-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-ingress-cluster-traffic-nodeport">Configuring ingress cluster traffic using a NodePort</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-config-openshift-io-v1">Network [config.openshift.io/v1</link>]</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#service-v1">Service [core/v1</link>]</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring-cluster-network-range">
<title>Configuring the cluster network range</title>

<simpara>As a cluster administrator, you can expand the cluster network range after cluster installation. You might want to expand the cluster network range if you need more IP addresses for additional nodes.</simpara>
<simpara>For example, if you deployed a cluster and specified <literal>10.128.0.0/19</literal> as the cluster network range and a host prefix of <literal>23</literal>, you are limited to 16 nodes. You can expand that to 510 nodes by changing the CIDR mask on a cluster to <literal>/14</literal>.</simpara>
<simpara>When expanding the cluster network address range, your cluster must use the <link linkend="about-ovn-kubernetes">OVN-Kubernetes network plugin</link>. Other network plugins are not supported.</simpara>
<simpara>The following limitations apply when modifying the cluster network IP address range:</simpara>
<itemizedlist>
<listitem>
<simpara>The CIDR mask size specified must always be smaller than the currently configured CIDR mask size, because you can only increase IP space by adding more nodes to an installed cluster</simpara>
</listitem>
<listitem>
<simpara>The host prefix cannot be modified</simpara>
</listitem>
<listitem>
<simpara>Pods that are configured with an overridden default gateway must be recreated after the cluster network expands</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-cluster-network-range-edit_configuring-cluster-network-range">
<title>Expanding the cluster network IP address range</title>
<simpara>You can expand the IP address range for the cluster network. Because this change requires rolling out a new Operator configuration across the cluster, it can take up to 30 minutes to take effect.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Ensure that the cluster uses the OVN-Kubernetes network plugin.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To obtain the cluster network range and host prefix for your cluster, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.operator.openshift.io \
  -o jsonpath="{.items[0].spec.clusterNetwork}"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">[{"cidr":"10.217.0.0/22","hostPrefix":23}]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To expand the cluster network IP address range, enter the following command. Use the CIDR IP address range and host prefix returned from the output of the previous command.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.config.openshift.io cluster --type='merge' --patch \
  '{
    "spec":{
      "clusterNetwork": [ {"cidr":"&lt;network&gt;/&lt;cidr&gt;","hostPrefix":&lt;prefix&gt;} ],
      "networkType": "OVNKubernetes"
    }
  }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;network&gt;</literal></term>
<listitem>
<simpara>Specifies the network part of the <literal>cidr</literal> field that you obtained from the previous step. You cannot change this value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;cidr&gt;</literal></term>
<listitem>
<simpara>Specifies the network prefix length. For example, <literal>14</literal>. Change this value to a smaller number than the value from the output in the previous step to expand the cluster network range.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;prefix&gt;</literal></term>
<listitem>
<simpara>Specifies the current host prefix for your cluster. This value must be the same value for the <literal>hostPrefix</literal> field that you obtained from the previous step.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example command</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.config.openshift.io cluster --type='merge' --patch \
  '{
    "spec":{
      "clusterNetwork": [ {"cidr":"10.217.0.0/14","hostPrefix": 23} ],
      "networkType": "OVNKubernetes"
    }
  }'</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">network.config.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To confirm that the configuration is active, enter the following command. It can take up to 30 minutes for this change to take effect.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.operator.openshift.io \
  -o jsonpath="{.items[0].spec.clusterNetwork}"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">[{"cidr":"10.217.0.0/14","hostPrefix":23}]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-cluster-network-range-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="about-ovn-kubernetes">About the OVN-Kubernetes network plugin</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring-ipfailover">
<title>Configuring IP failover</title>

<simpara>This topic describes configuring IP failover for pods and services on your OpenShift Container Platform cluster.</simpara>
<simpara>IP failover manages a pool of Virtual IP (VIP) addresses on a set of nodes. Every VIP in the set is serviced by a node selected from the set. As long a single node is available, the VIPs are served. There is no way to explicitly distribute the VIPs over the nodes, so there can be nodes with no VIPs and other nodes with many VIPs. If there is only one node, all VIPs are on it.</simpara>
<note>
<simpara>The VIPs must be routable from outside the cluster.</simpara>
</note>
<simpara>IP failover monitors a port on each VIP to determine whether the port is reachable on the node. If the port is not reachable, the VIP is not assigned to the node. If the port is set to <literal>0</literal>, this check is suppressed. The check script does the needed testing.</simpara>
<simpara>IP failover uses <link xlink:href="http://www.keepalived.org/">Keepalived</link> to host a set of externally accessible VIP addresses on a set of hosts. Each VIP is only serviced by a single host at a time. Keepalived uses the Virtual Router Redundancy Protocol (VRRP) to determine which host, from the set of hosts, services which VIP. If a host becomes unavailable, or if the service that Keepalived is watching does not respond, the VIP is switched to another host from the set. This means a VIP is always serviced as long as a host is available.</simpara>
<simpara>When a node running Keepalived passes the check script, the VIP on that node can enter the <literal>master</literal> state based on its priority and the priority of the current master and as determined by the preemption strategy.</simpara>
<simpara>A cluster administrator can provide a script through the <literal>OPENSHIFT_HA_NOTIFY_SCRIPT</literal> variable, and this script is called whenever the state of the VIP on the node changes. Keepalived uses the <literal>master</literal> state when it is servicing the VIP, the <literal>backup</literal> state when another node is servicing the VIP, or in the <literal>fault</literal> state when the check script fails. The notify script is called with the new state whenever the state changes.</simpara>
<simpara>You can create an IP failover deployment configuration on OpenShift Container Platform. The IP failover deployment configuration specifies the set of VIP addresses, and the set of nodes on which to service them. A cluster can have multiple IP failover deployment configurations, with each managing its own set of unique VIP addresses. Each node in the IP failover configuration runs an IP failover pod, and this pod runs Keepalived.</simpara>
<simpara>When using VIPs to access a pod with host networking, the application pod runs on all nodes that are running the IP failover pods. This enables any of the IP failover nodes to become the master and service the VIPs when needed. If application pods are not running on all nodes with IP failover, either some IP failover nodes never service the VIPs or some application pods never receive any traffic. Use the same selector and replication count, for both IP failover and the application pods, to avoid this mismatch.</simpara>
<simpara>While using VIPs to access a service, any of the nodes can be in the IP failover set of nodes, since the service is reachable on all nodes, no matter where the application pod is running. Any of the IP failover nodes can become master at any time. The service can either use external IPs and a service port or it can use a <literal>NodePort</literal>.</simpara>
<simpara>When using external IPs in the service definition, the VIPs are set to the external IPs, and the IP failover monitoring port is set to the service port. When using a node port, the port is open on every node in the cluster, and the service load-balances traffic from whatever node currently services the VIP. In this case, the IP failover monitoring port is set to the <literal>NodePort</literal> in the service definition.</simpara>
<important>
<simpara>Setting up a <literal>NodePort</literal> is a privileged operation.</simpara>
</important>
<important>
<simpara>Even though a service VIP is highly available, performance can still be affected. Keepalived makes sure that each of the VIPs is serviced by some node in the configuration, and several VIPs can end up on the same node even when other nodes have none. Strategies that externally load-balance across a set of VIPs can be thwarted when IP failover puts multiple VIPs on the same node.</simpara>
</important>
<simpara>When you use <literal>ingressIP</literal>, you can set up IP failover to have the same VIP range as the <literal>ingressIP</literal> range. You can also disable the monitoring port. In this case, all the VIPs appear on same node in the cluster. Any user can set up a service with an <literal>ingressIP</literal> and have it highly available.</simpara>
<important>
<simpara>There are a maximum of 254 VIPs in the cluster.</simpara>
</important>
<section xml:id="nw-ipfailover-environment-variables_configuring-ipfailover">
<title>IP failover environment variables</title>
<simpara>The following table contains the variables used to configure IP failover.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>IP failover environment variables</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="37.5*"/>
<colspec colname="col_2" colwidth="12.5*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Variable Name</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_MONITOR_PORT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>80</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The IP failover pod tries to open a TCP connection to this port on each Virtual IP (VIP). If connection is established, the service is considered to be running. If this port is set to <literal>0</literal>, the test always passes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_NETWORK_INTERFACE</literal></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>The interface name that IP failover uses to send Virtual Router Redundancy Protocol (VRRP) traffic. The default value is <literal>eth0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_REPLICA_COUNT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>2</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The number of replicas to create. This must match <literal>spec.replicas</literal> value in IP failover deployment configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_VIRTUAL_IPS</literal></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>The list of IP address ranges to replicate. This must be provided. For example, <literal>1.2.3.4-6,1.2.3.9</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_VRRP_ID_OFFSET</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The offset value used to set the virtual router IDs. Using different offset values allows multiple IP failover configurations to exist within the same cluster. The default offset is <literal>0</literal>, and the allowed range is <literal>0</literal> through <literal>255</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_VIP_GROUPS</literal></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>The number of groups to create for VRRP. If not set, a group is created for each virtual IP range specified with the <literal>OPENSHIFT_HA_VIP_GROUPS</literal> variable.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_IPTABLES_CHAIN</literal></simpara></entry>
<entry align="left" valign="top"><simpara>INPUT</simpara></entry>
<entry align="left" valign="top"><simpara>The name of the iptables chain, to automatically add an <literal>iptables</literal> rule to allow the VRRP traffic on. If the value is not set, an <literal>iptables</literal> rule is not added. If the chain does not exist, it is not created.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_CHECK_SCRIPT</literal></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>The full path name in the pod file system of a script that is periodically run to verify the application is operating.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_CHECK_INTERVAL</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>2</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The period, in seconds, that the check script is run.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_NOTIFY_SCRIPT</literal></simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara>The full path name in the pod file system of a script that is run whenever the state changes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>OPENSHIFT_HA_PREEMPTION</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>preempt_nodelay 300</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The strategy for handling a new higher priority host. The <literal>nopreempt</literal> strategy does not move master from the lower priority host to the higher priority host.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-ipfailover-configuration_configuring-ipfailover">
<title>Configuring IP failover</title>
<simpara>As a cluster administrator, you can configure IP failover on an entire cluster, or on a subset of nodes, as defined by the label selector. You can also configure multiple IP failover deployment configurations in your cluster, where each one is independent of the others.</simpara>
<simpara>The IP failover deployment configuration ensures that a failover pod runs on each of the nodes matching the constraints or the label used.</simpara>
<simpara>This pod runs Keepalived, which can monitor an endpoint and use Virtual Router Redundancy Protocol (VRRP) to fail over the virtual IP (VIP) from one node to another if the first node cannot reach the service or endpoint.</simpara>
<simpara>For production use, set a <literal>selector</literal> that selects at least two nodes, and set <literal>replicas</literal> equal to the number of selected nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You created a pull secret.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP failover service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create sa ipfailover</programlisting>
</listitem>
<listitem>
<simpara>Update security context constraints (SCC) for <literal>hostNetwork</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-scc-to-user privileged -z ipfailover
$ oc adm policy add-scc-to-user hostnetwork -z ipfailover</programlisting>
</listitem>
<listitem>
<simpara>Create a deployment YAML file to configure IP failover:</simpara>
<formalpara>
<title>Example deployment YAML for IP failover configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: ipfailover-keepalived <co xml:id="CO28-1"/>
  labels:
    ipfailover: hello-openshift
spec:
  strategy:
    type: Recreate
  replicas: 2
  selector:
    matchLabels:
      ipfailover: hello-openshift
  template:
    metadata:
      labels:
        ipfailover: hello-openshift
    spec:
      serviceAccountName: ipfailover
      privileged: true
      hostNetwork: true
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      containers:
      - name: openshift-ipfailover
        image: quay.io/openshift/origin-keepalived-ipfailover
        ports:
        - containerPort: 63000
          hostPort: 63000
        imagePullPolicy: IfNotPresent
        securityContext:
          privileged: true
        volumeMounts:
        - name: lib-modules
          mountPath: /lib/modules
          readOnly: true
        - name: host-slash
          mountPath: /host
          readOnly: true
          mountPropagation: HostToContainer
        - name: etc-sysconfig
          mountPath: /etc/sysconfig
          readOnly: true
        - name: config-volume
          mountPath: /etc/keepalive
        env:
        - name: OPENSHIFT_HA_CONFIG_NAME
          value: "ipfailover"
        - name: OPENSHIFT_HA_VIRTUAL_IPS <co xml:id="CO28-2"/>
          value: "1.1.1.1-2"
        - name: OPENSHIFT_HA_VIP_GROUPS <co xml:id="CO28-3"/>
          value: "10"
        - name: OPENSHIFT_HA_NETWORK_INTERFACE <co xml:id="CO28-4"/>
          value: "ens3" #The host interface to assign the VIPs
        - name: OPENSHIFT_HA_MONITOR_PORT <co xml:id="CO28-5"/>
          value: "30060"
        - name: OPENSHIFT_HA_VRRP_ID_OFFSET <co xml:id="CO28-6"/>
          value: "0"
        - name: OPENSHIFT_HA_REPLICA_COUNT <co xml:id="CO28-7"/>
          value: "2" #Must match the number of replicas in the deployment
        - name: OPENSHIFT_HA_USE_UNICAST
          value: "false"
        #- name: OPENSHIFT_HA_UNICAST_PEERS
          #value: "10.0.148.40,10.0.160.234,10.0.199.110"
        - name: OPENSHIFT_HA_IPTABLES_CHAIN <co xml:id="CO28-8"/>
          value: "INPUT"
        #- name: OPENSHIFT_HA_NOTIFY_SCRIPT <co xml:id="CO28-9"/>
        #  value: /etc/keepalive/mynotifyscript.sh
        - name: OPENSHIFT_HA_CHECK_SCRIPT <co xml:id="CO28-10"/>
          value: "/etc/keepalive/mycheckscript.sh"
        - name: OPENSHIFT_HA_PREEMPTION <co xml:id="CO28-11"/>
          value: "preempt_delay 300"
        - name: OPENSHIFT_HA_CHECK_INTERVAL <co xml:id="CO28-12"/>
          value: "2"
        livenessProbe:
          initialDelaySeconds: 10
          exec:
            command:
            - pgrep
            - keepalived
      volumes:
      - name: lib-modules
        hostPath:
          path: /lib/modules
      - name: host-slash
        hostPath:
          path: /
      - name: etc-sysconfig
        hostPath:
          path: /etc/sysconfig
      # config-volume contains the check script
      # created with `oc create configmap keepalived-checkscript --from-file=mycheckscript.sh`
      - configMap:
          defaultMode: 0755
          name: keepalived-checkscript
        name: config-volume
      imagePullSecrets:
        - name: openshift-pull-secret <co xml:id="CO28-13"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO28-1">
<para>The name of the IP failover deployment.</para>
</callout>
<callout arearefs="CO28-2">
<para>The list of IP address ranges to replicate. This must be provided. For example, <literal>1.2.3.4-6,1.2.3.9</literal>.</para>
</callout>
<callout arearefs="CO28-3">
<para>The number of groups to create for VRRP. If not set, a group is created for each virtual IP range specified with the <literal>OPENSHIFT_HA_VIP_GROUPS</literal> variable.</para>
</callout>
<callout arearefs="CO28-4">
<para>The interface name that IP failover uses to send VRRP traffic. By default, <literal>eth0</literal> is used.</para>
</callout>
<callout arearefs="CO28-5">
<para>The IP failover pod tries to open a TCP connection to this port on each VIP. If connection is established, the service is considered to be running. If this port is set to <literal>0</literal>, the test always passes. The default value is <literal>80</literal>.</para>
</callout>
<callout arearefs="CO28-6">
<para>The offset value used to set the virtual router IDs. Using different offset values allows multiple IP failover configurations to exist within the same cluster. The default offset is <literal>0</literal>, and the allowed range is <literal>0</literal> through <literal>255</literal>.</para>
</callout>
<callout arearefs="CO28-7">
<para>The number of replicas to create. This must match <literal>spec.replicas</literal> value in IP failover deployment configuration. The default value is <literal>2</literal>.</para>
</callout>
<callout arearefs="CO28-8">
<para>The name of the <literal>iptables</literal> chain to automatically add an <literal>iptables</literal> rule to allow the VRRP traffic on. If the value is not set, an <literal>iptables</literal> rule is not added. If the chain does not exist, it is not created, and Keepalived operates in unicast mode. The default is <literal>INPUT</literal>.</para>
</callout>
<callout arearefs="CO28-9">
<para>The full path name in the pod file system of a script that is run whenever the state changes.</para>
</callout>
<callout arearefs="CO28-10">
<para>The full path name in the pod file system of a script that is periodically run to verify the application is operating.</para>
</callout>
<callout arearefs="CO28-11">
<para>The strategy for handling a new higher priority host. The default value is <literal>preempt_delay 300</literal>, which causes a Keepalived instance to take over a VIP after 5 minutes if a lower-priority master is holding the VIP.</para>
</callout>
<callout arearefs="CO28-12">
<para>The period, in seconds, that the check script is run. The default value is <literal>2</literal>.</para>
</callout>
<callout arearefs="CO28-13">
<para>Create the pull secret before creating the deployment, otherwise you will get an error when creating the deployment.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ipfailover-virtual-ip-addresses-concept_configuring-ipfailover">
<title>About virtual IP addresses</title>
<simpara>Keepalived manages a set of virtual IP addresses (VIP). The administrator must make sure that all of these addresses:</simpara>
<itemizedlist>
<listitem>
<simpara>Are accessible on the configured hosts from outside the cluster.</simpara>
</listitem>
<listitem>
<simpara>Are not used for any other purpose within the cluster.</simpara>
</listitem>
</itemizedlist>
<simpara>Keepalived on each node determines whether the needed service is running. If it is, VIPs are supported and Keepalived participates in the negotiation to determine which node serves the VIP. For a node to participate, the service must be listening on the watch port on a VIP or the check must be disabled.</simpara>
<note>
<simpara>Each VIP in the set may end up being served by a different node.</simpara>
</note>
</section>
<section xml:id="nw-ipfailover-configuring-check-notify-scripts_configuring-ipfailover">
<title>Configuring check and notify scripts</title>
<simpara>Keepalived monitors the health of the application by periodically running an optional user supplied check script. For example, the script can test a web server by issuing a request and verifying the response.</simpara>
<simpara>When a check script is not provided, a simple default script is run that tests the TCP connection. This default test is suppressed when the monitor port is <literal>0</literal>.</simpara>
<simpara>Each IP failover pod manages a Keepalived daemon that manages one or more virtual IPs (VIP) on the node where the pod is running. The Keepalived daemon keeps the state of each VIP for that node. A particular VIP on a particular node may be in <literal>master</literal>, <literal>backup</literal>, or <literal>fault</literal> state.</simpara>
<simpara>When the check script for that VIP on the node that is in <literal>master</literal> state fails, the VIP on that node enters the <literal>fault</literal> state, which triggers a renegotiation. During renegotiation, all VIPs on a node that are not in the <literal>fault</literal> state participate in deciding which node takes over the VIP. Ultimately, the VIP enters the <literal>master</literal> state on some node, and the VIP stays in the <literal>backup</literal> state on the other nodes.</simpara>
<simpara>When a node with a VIP in <literal>backup</literal> state fails, the VIP on that node enters the <literal>fault</literal> state. When the check script passes again for a VIP on a node in the <literal>fault</literal> state, the VIP on that node exits the <literal>fault</literal> state and negotiates to enter the <literal>master</literal> state. The VIP on that node may then enter either the <literal>master</literal> or the <literal>backup</literal> state.</simpara>
<simpara>As cluster administrator, you can provide an optional notify script, which is called whenever the state changes. Keepalived passes the following three parameters to the script:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>$1</literal> - <literal>group</literal> or <literal>instance</literal></simpara>
</listitem>
<listitem>
<simpara><literal>$2</literal> - Name of the <literal>group</literal> or <literal>instance</literal></simpara>
</listitem>
<listitem>
<simpara><literal>$3</literal> - The new state: <literal>master</literal>, <literal>backup</literal>, or <literal>fault</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The check and notify scripts run in the IP failover pod and use the pod file system, not the host file system. However, the IP failover pod makes the host file system available under the <literal>/hosts</literal> mount path. When configuring a check or notify script, you must provide the full path to the script. The recommended approach for providing the scripts is to use a config map.</simpara>
<simpara>The full path names of the check and notify scripts are added to the Keepalived configuration file, <literal>_/etc/keepalived/keepalived.conf</literal>, which is loaded every time Keepalived starts. The scripts can be added to the pod with a config map as follows.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the desired script and create a config map to hold it. The script has no input arguments and must return <literal>0</literal> for <literal>OK</literal> and <literal>1</literal> for <literal>fail</literal>.</simpara>
<simpara>The check script, <literal><emphasis>mycheckscript.sh</emphasis></literal>:</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
    # Whatever tests are needed
    # E.g., send request and verify response
exit 0</programlisting>
</listitem>
<listitem>
<simpara>Create the config map:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap mycustomcheck --from-file=mycheckscript.sh</programlisting>
</listitem>
<listitem>
<simpara>Add the script to the pod. The <literal>defaultMode</literal> for the mounted config map files must able to run by using <literal>oc</literal> commands or by editing the deployment configuration. A value of <literal>0755</literal>, <literal>493</literal> decimal, is typical:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set env deploy/ipfailover-keepalived \
    OPENSHIFT_HA_CHECK_SCRIPT=/etc/keepalive/mycheckscript.sh</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set volume deploy/ipfailover-keepalived --add --overwrite \
    --name=config-volume \
    --mount-path=/etc/keepalive \
    --source='{"configMap": { "name": "mycustomcheck", "defaultMode": 493}}'</programlisting>
<note>
<simpara>The <literal>oc set env</literal> command is whitespace sensitive. There must be no whitespace on either side of the <literal>=</literal> sign.</simpara>
</note>
<tip>
<simpara>You can alternatively edit the <literal>ipfailover-keepalived</literal> deployment configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit deploy ipfailover-keepalived</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">    spec:
      containers:
      - env:
        - name: OPENSHIFT_HA_CHECK_SCRIPT  <co xml:id="CO29-1"/>
          value: /etc/keepalive/mycheckscript.sh
...
        volumeMounts: <co xml:id="CO29-2"/>
        - mountPath: /etc/keepalive
          name: config-volume
      dnsPolicy: ClusterFirst
...
      volumes: <co xml:id="CO29-3"/>
      - configMap:
          defaultMode: 0755 <co xml:id="CO29-4"/>
          name: customrouter
        name: config-volume
...</programlisting>
<calloutlist>
<callout arearefs="CO29-1">
<para>In the <literal>spec.container.env</literal> field, add the <literal>OPENSHIFT_HA_CHECK_SCRIPT</literal> environment variable to point to the mounted script file.</para>
</callout>
<callout arearefs="CO29-2">
<para>Add the <literal>spec.container.volumeMounts</literal> field to create the mount point.</para>
</callout>
<callout arearefs="CO29-3">
<para>Add a new <literal>spec.volumes</literal> field to mention the config map.</para>
</callout>
<callout arearefs="CO29-4">
<para>This sets run permission on the files. When read back, it is displayed in decimal, <literal>493</literal>.</para>
</callout>
</calloutlist>
<simpara>Save the changes and exit the editor. This restarts <literal>ipfailover-keepalived</literal>.</simpara>
</tip>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ipfailover-configuring-vrrp-preemption_configuring-ipfailover">
<title>Configuring VRRP preemption</title>
<simpara>When a Virtual IP (VIP) on a node leaves the <literal>fault</literal> state by passing the check script, the VIP on the node enters the <literal>backup</literal> state if it has lower priority than the VIP on the node that is currently in the <literal>master</literal> state. However, if the VIP on the node that is leaving <literal>fault</literal> state has a higher priority, the preemption strategy determines its role in the cluster.</simpara>
<simpara>The <literal>nopreempt</literal> strategy does not move <literal>master</literal> from the lower priority VIP on the host to the higher priority VIP on the host. With <literal>preempt_delay 300</literal>, the default, Keepalived waits the specified 300 seconds and moves <literal>master</literal> to the higher priority VIP on the host.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To specify preemption enter <literal>oc edit deploy ipfailover-keepalived</literal> to edit the router deployment configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit deploy ipfailover-keepalived</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">...
    spec:
      containers:
      - env:
        - name: OPENSHIFT_HA_PREEMPTION  <co xml:id="CO30-1"/>
          value: preempt_delay 300
...</programlisting>
<calloutlist>
<callout arearefs="CO30-1">
<para>Set the <literal>OPENSHIFT_HA_PREEMPTION</literal> value:</para>
<itemizedlist>
<listitem>
<simpara><literal>preempt_delay 300</literal>: Keepalived waits the specified 300 seconds and moves <literal>master</literal> to the higher priority VIP on the host. This is the default value.</simpara>
</listitem>
<listitem>
<simpara><literal>nopreempt</literal>: does not move <literal>master</literal> from the lower priority VIP on the host to the higher priority VIP on the host.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ipfailover-vrrp-ip-offset_configuring-ipfailover">
<title>About VRRP ID offset</title>
<simpara>Each IP failover pod managed by the IP failover deployment configuration, <literal>1</literal> pod per node or replica, runs a Keepalived daemon. As more IP failover deployment configurations are configured, more pods are created and more daemons join into the common Virtual Router Redundancy Protocol (VRRP) negotiation. This negotiation is done by all the Keepalived daemons and it determines which nodes service which virtual IPs (VIP).</simpara>
<simpara>Internally, Keepalived assigns a unique <literal>vrrp-id</literal> to each VIP. The negotiation uses this set of <literal>vrrp-ids</literal>, when a decision is made, the VIP corresponding to the winning <literal>vrrp-id</literal> is serviced on the winning node.</simpara>
<simpara>Therefore, for every VIP defined in the IP failover deployment configuration, the IP failover pod must assign a corresponding <literal>vrrp-id</literal>. This is done by starting at <literal>OPENSHIFT_HA_VRRP_ID_OFFSET</literal> and sequentially assigning the <literal>vrrp-ids</literal> to the list of VIPs. The <literal>vrrp-ids</literal> can have values in the range <literal>1..255</literal>.</simpara>
<simpara>When there are multiple IP failover deployment configurations, you must specify <literal>OPENSHIFT_HA_VRRP_ID_OFFSET</literal> so that there is room to increase the number of VIPs in the deployment configuration and none of the <literal>vrrp-id</literal> ranges overlap.</simpara>
</section>
<section xml:id="nw-ipfailover-configuring-more-than-254_configuring-ipfailover">
<title>Configuring IP failover for more than 254 addresses</title>
<simpara>IP failover management is limited to 254 groups of Virtual IP (VIP) addresses. By default OpenShift Container Platform assigns one IP address to each group. You can use the <literal>OPENSHIFT_HA_VIP_GROUPS</literal> variable to change this so multiple IP addresses are in each group and define the number of VIP groups available for each Virtual Router Redundancy Protocol (VRRP) instance when configuring IP failover.</simpara>
<simpara>Grouping VIPs creates a wider range of allocation of VIPs per VRRP in the case of VRRP failover events, and is useful when all hosts in the cluster have access to a service locally. For example, when a service is being exposed with an <literal>ExternalIP</literal>.</simpara>
<note>
<simpara>As a rule for failover, do not limit services, such as the router, to one specific host. Instead, services should be replicated to each host so that in the case of IP failover, the services do not have to be recreated on the new host.</simpara>
</note>
<note>
<simpara>If you are using OpenShift Container Platform health checks, the nature of IP failover and groups means that all instances in the group are not checked. For that reason, <link xlink:href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">the Kubernetes health checks</link> must be used to ensure that services are live.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To change the number of IP addresses assigned to each group, change the value for the <literal>OPENSHIFT_HA_VIP_GROUPS</literal> variable, for example:</simpara>
<formalpara>
<title>Example <literal>Deployment</literal> YAML for IP failover configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">...
    spec:
        env:
        - name: OPENSHIFT_HA_VIP_GROUPS <co xml:id="CO31-1"/>
          value: "3"
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO31-1">
<para>If <literal>OPENSHIFT_HA_VIP_GROUPS</literal> is set to <literal>3</literal> in an environment with seven VIPs, it creates three groups, assigning three VIPs to the first group, and two VIPs to the two remaining groups.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>If the number of groups set by <literal>OPENSHIFT_HA_VIP_GROUPS</literal> is fewer than the number of IP addresses set to fail over, the group contains more than one IP address, and all of the addresses move as a single unit.</simpara>
</note>
</section>
<section xml:id="nw-ipfailover-cluster-ha-ingress_configuring-ipfailover">
<title>High availability For ingressIP</title>
<simpara>In non-cloud clusters, IP failover and <literal>ingressIP</literal> to a service can be combined. The result is high availability services for users that create services using <literal>ingressIP</literal>.</simpara>
<simpara>The approach is to specify an <literal>ingressIPNetworkCIDR</literal> range and then use the same range in creating the ipfailover configuration.</simpara>
<simpara>Because IP failover can support up to a maximum of 255 VIPs for the entire cluster, the <literal>ingressIPNetworkCIDR</literal> needs to be <literal>/24</literal> or smaller.</simpara>
</section>
<section xml:id="nw-ipfailover-remove_configuring-ipfailover">
<title>Removing IP failover</title>
<simpara>When IP failover is initially configured, the worker nodes in the cluster are modified with an <literal>iptables</literal> rule that explicitly allows multicast packets on <literal>224.0.0.18</literal> for Keepalived. Because of the change to the nodes, removing IP failover requires running a job to remove the <literal>iptables</literal> rule and removing the virtual IP addresses used by Keepalived.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: Identify and delete any check and notify scripts that are stored as config maps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Identify whether any pods for IP failover use a config map as a volume:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -l ipfailover \
  -o jsonpath="\
{range .items[?(@.spec.volumes[*].configMap)]}
{'Namespace: '}{.metadata.namespace}
{'Pod:       '}{.metadata.name}
{'Volumes that use config maps:'}
{range .spec.volumes[?(@.configMap)]}  {'volume:    '}{.name}
  {'configMap: '}{.configMap.name}{'\n'}{end}
{end}"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>Namespace: default
Pod:       keepalived-worker-59df45db9c-2x9mn
Volumes that use config maps:
  volume:    config-volume
  configMap: mycustomcheck</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If the preceding step provided the names of config maps that are used as volumes, delete the config maps:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete configmap &lt;configmap_name&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Identify an existing deployment for IP failover:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -l ipfailover</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE   NAME         READY   UP-TO-DATE   AVAILABLE   AGE
default     ipfailover   2/2     2            2           105d</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete deployment &lt;ipfailover_deployment_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>ipfailover</literal> service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete sa ipfailover</programlisting>
</listitem>
<listitem>
<simpara>Run a job that removes the IP tables rule that was added when IP failover was initially configured:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file such as <literal>remove-ipfailover-job.yaml</literal> with contents that are similar to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: batch/v1
kind: Job
metadata:
  generateName: remove-ipfailover-
  labels:
    app: remove-ipfailover
spec:
  template:
    metadata:
      name: remove-ipfailover
    spec:
      containers:
      - name: remove-ipfailover
        image: quay.io/openshift/origin-keepalived-ipfailover:4.14
        command: ["/var/lib/ipfailover/keepalived/remove-failover.sh"]
      nodeSelector:
        kubernetes.io/hostname: &lt;host_name&gt;  <co xml:id="CO32-1"/>
      restartPolicy: Never</programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>Run the job for each node in your cluster that was configured for IP failover and replace the hostname each time.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Run the job:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f remove-ipfailover-job.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>job.batch/remove-ipfailover-2h8dm created</screen>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Confirm that the job removed the initial configuration for IP failover.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs job/remove-ipfailover-2h8dm</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">remove-failover.sh: OpenShift IP Failover service terminating.
  - Removing ip_vs module ...
  - Cleaning up ...
  - Releasing VIPs  (interface eth0) ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configure-syscontrols-interface-tuning-cni">
<title>Configuring system controls and interface attributes using the tuning plugin</title>

<simpara>In Linux, sysctl allows an administrator to modify kernel parameters at runtime. You can modify interface-level network sysctls using the tuning Container Network Interface (CNI) meta plugin. The tuning CNI meta plugin operates in a chain with a main CNI plugin as illustrated.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/264_OpenShift_CNI_plugin_chain_0722.png"/>
</imageobject>
<textobject><phrase>CNI plugin</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The main CNI plugin assigns the interface and passes this interface to the tuning CNI meta plugin at runtime. You can change some sysctls and several interface attributes such as promiscuous mode, all-multicast mode, MTU, and MAC address in the network namespace by using the tuning CNI meta plugin.</simpara>
<section xml:id="nw-configuring-tuning-cni_configure-syscontrols-interface-tuning-cni">
<title>Configuring system controls by using the tuning CNI</title>
<simpara>The following procedure configures the tuning CNI to change the interface-level network <literal>net.ipv4.conf.IFNAME.accept_redirects</literal> sysctl. This example enables accepting and sending ICMP-redirected packets. In the tuning CNI meta plugin configuration, the interface name is represented by the <literal>IFNAME</literal> token and is replaced with the actual name of the interface at runtime.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a network attachment definition, such as <literal>tuning-example.yaml</literal>, with the following content:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: &lt;name&gt; <co xml:id="CO33-1"/>
  namespace: default <co xml:id="CO33-2"/>
spec:
  config: '{
    "cniVersion": "0.4.0", <co xml:id="CO33-3"/>
    "name": "&lt;name&gt;", <co xml:id="CO33-4"/>
    "plugins": [{
       "type": "&lt;main_CNI_plugin&gt;" <co xml:id="CO33-5"/>
      },
      {
       "type": "tuning", <co xml:id="CO33-6"/>
       "sysctl": {
            "net.ipv4.conf.IFNAME.accept_redirects": "1" <co xml:id="CO33-7"/>
        }
      }
     ]
}</programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>Specifies the name for the additional network attachment to create. The name must be unique within the specified namespace.</para>
</callout>
<callout arearefs="CO33-2">
<para>Specifies the namespace that the object is associated with.</para>
</callout>
<callout arearefs="CO33-3">
<para>Specifies the CNI specification version.</para>
</callout>
<callout arearefs="CO33-4">
<para>Specifies the name for the configuration. It is recommended to match the configuration name to the name value of the network attachment definition.</para>
</callout>
<callout arearefs="CO33-5">
<para>Specifies the name of the main CNI plugin to configure.</para>
</callout>
<callout arearefs="CO33-6">
<para>Specifies the name of the CNI meta plugin.</para>
</callout>
<callout arearefs="CO33-7">
<para>Specifies the sysctl to set. The interface name is represented by the <literal>IFNAME</literal> token and is replaced with the actual name of the interface at runtime.</para>
</callout>
</calloutlist>
<simpara>An example YAML file is shown here:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: tuningnad
  namespace: default
spec:
  config: '{
    "cniVersion": "0.4.0",
    "name": "tuningnad",
    "plugins": [{
      "type": "bridge"
      },
      {
      "type": "tuning",
      "sysctl": {
         "net.ipv4.conf.IFNAME.accept_redirects": "1"
        }
    }
  ]
}'</programlisting>
</listitem>
<listitem>
<simpara>Apply the YAML by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f tuning-example.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkattachmentdefinition.k8.cni.cncf.io/tuningnad created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a pod such as <literal>examplepod.yaml</literal> with the network attachment definition similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: tunepod
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/networks: tuningnad <co xml:id="CO34-1"/>
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000 <co xml:id="CO34-2"/>
      runAsGroup: 3000 <co xml:id="CO34-3"/>
      allowPrivilegeEscalation: false <co xml:id="CO34-4"/>
      capabilities: <co xml:id="CO34-5"/>
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true <co xml:id="CO34-6"/>
    seccompProfile: <co xml:id="CO34-7"/>
      type: RuntimeDefault</programlisting>
<calloutlist>
<callout arearefs="CO34-1">
<para>Specify the name of the configured <literal>NetworkAttachmentDefinition</literal>.</para>
</callout>
<callout arearefs="CO34-2">
<para><literal>runAsUser</literal> controls which user ID the container is run with.</para>
</callout>
<callout arearefs="CO34-3">
<para><literal>runAsGroup</literal> controls which primary group ID the containers is run with.</para>
</callout>
<callout arearefs="CO34-4">
<para><literal>allowPrivilegeEscalation</literal> determines if a pod can request to allow privilege escalation. If unspecified, it defaults to true. This boolean directly controls whether the <literal>no_new_privs</literal> flag gets set on the container process.</para>
</callout>
<callout arearefs="CO34-5">
<para><literal>capabilities</literal> permit privileged actions without giving full root access. This policy ensures all capabilities are dropped from the pod.</para>
</callout>
<callout arearefs="CO34-6">
<para><literal>runAsNonRoot: true</literal> requires that the container will run with a user with any UID other than 0.</para>
</callout>
<callout arearefs="CO34-7">
<para><literal>RuntimeDefault</literal> enables the default seccomp profile for a pod or container workload.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the yaml by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f examplepod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      READY   STATUS    RESTARTS   AGE
tunepod   1/1     Running   0          47s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Log in to the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh tunepod</programlisting>
</listitem>
<listitem>
<simpara>Verify the values of the configured sysctl flags. For example, find the value <literal>net.ipv4.conf.net1.accept_redirects</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# sysctl net.ipv4.conf.net1.accept_redirects</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">net.ipv4.conf.net1.accept_redirects = 1</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-enabling-all-multi-cni_configure-syscontrols-interface-tuning-cni">
<title>Enabling all-multicast mode by using the tuning CNI</title>
<simpara>You can enable all-multicast mode by using the tuning Container Network Interface (CNI) meta plugin.</simpara>
<simpara>The following procedure describes how to configure the tuning CNI to enable the all-multicast mode.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a network attachment definition, such as <literal>tuning-example.yaml</literal>, with the following content:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: &lt;name&gt; <co xml:id="CO35-1"/>
  namespace: default <co xml:id="CO35-2"/>
spec:
  config: '{
    "cniVersion": "0.4.0", <co xml:id="CO35-3"/>
    "name": "&lt;name&gt;", <co xml:id="CO35-4"/>
    "plugins": [{
       "type": "&lt;main_CNI_plugin&gt;" <co xml:id="CO35-5"/>
      },
      {
       "type": "tuning", <co xml:id="CO35-6"/>
       "allmulti": true <co xml:id="CO35-7"/>
        }
      }
     ]
}</programlisting>
<calloutlist>
<callout arearefs="CO35-1">
<para>Specifies the name for the additional network attachment to create. The name must be unique within the specified namespace.</para>
</callout>
<callout arearefs="CO35-2">
<para>Specifies the namespace that the object is associated with.</para>
</callout>
<callout arearefs="CO35-3">
<para>Specifies the CNI specification version.</para>
</callout>
<callout arearefs="CO35-4">
<para>Specifies the name for the configuration. Match the configuration name to the name value of the network attachment definition.</para>
</callout>
<callout arearefs="CO35-5">
<para>Specifies the name of the main CNI plugin to configure.</para>
</callout>
<callout arearefs="CO35-6">
<para>Specifies the name of the CNI meta plugin.</para>
</callout>
<callout arearefs="CO35-7">
<para>Changes the all-multicast mode of interface. If enabled, all multicast packets on the network will be received by the interface.</para>
</callout>
</calloutlist>
<simpara>An example YAML file is shown here:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: setallmulti
  namespace: default
spec:
  config: '{
    "cniVersion": "0.4.0",
    "name": "setallmulti",
    "plugins": [
      {
        "type": "bridge"
      },
      {
        "type": "tuning",
        "allmulti": true
      }
    ]
  }'</programlisting>
</listitem>
<listitem>
<simpara>Apply the settings specified in the YAML file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f tuning-allmulti.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkattachmentdefinition.k8s.cni.cncf.io/setallmulti created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a pod with a network attachment definition similar to that specified in the following <literal>examplepod.yaml</literal> sample file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: allmultipod
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/networks: setallmulti <co xml:id="CO36-1"/>
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000 <co xml:id="CO36-2"/>
      runAsGroup: 3000 <co xml:id="CO36-3"/>
      allowPrivilegeEscalation: false <co xml:id="CO36-4"/>
      capabilities: <co xml:id="CO36-5"/>
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true <co xml:id="CO36-6"/>
    seccompProfile: <co xml:id="CO36-7"/>
      type: RuntimeDefault</programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>Specifies the name of the configured <literal>NetworkAttachmentDefinition</literal>.</para>
</callout>
<callout arearefs="CO36-2">
<para>Specifies the user ID the container is run with.</para>
</callout>
<callout arearefs="CO36-3">
<para>Specifies which primary group ID the containers is run with.</para>
</callout>
<callout arearefs="CO36-4">
<para>Specifies if a pod can request privilege escalation. If unspecified, it defaults to <literal>true</literal>. This boolean directly controls whether the <literal>no_new_privs</literal> flag gets set on the container process.</para>
</callout>
<callout arearefs="CO36-5">
<para>Specifies the container capabilities. The <literal>drop: ["ALL"]</literal> statement indicates that all Linux capabilities are dropped from the pod, providing a more restrictive security profile.</para>
</callout>
<callout arearefs="CO36-6">
<para>Specifies that the container will run with a user with any UID other than 0.</para>
</callout>
<callout arearefs="CO36-7">
<para>Specifies the container&#8217;s seccomp profile. In this case, the type is set to <literal>RuntimeDefault</literal>. Seccomp is a Linux kernel feature that restricts the system calls available to a process, enhancing security by minimizing the attack surface.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the settings specified in the YAML file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f examplepod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME          READY   STATUS    RESTARTS   AGE
allmultipod   1/1     Running   0          23s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Log in to the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh allmultipod</programlisting>
</listitem>
<listitem>
<simpara>List all the interfaces associated with the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# ip link</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0@if22: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8901 qdisc noqueue state UP mode DEFAULT group default
    link/ether 0a:58:0a:83:00:10 brd ff:ff:ff:ff:ff:ff link-netnsid 0 <co xml:id="CO37-1"/>
3: net1@if24: &lt;BROADCAST,MULTICAST,ALLMULTI,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:9b:66:a4:ec:1d brd ff:ff:ff:ff:ff:ff link-netnsid 0 <co xml:id="CO37-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO37-1">
<para><literal>eth0@if22</literal> is the primary interface</para>
</callout>
<callout arearefs="CO37-2">
<para><literal>net1@if24</literal> is the secondary interface configured with the network-attachment-definition that supports the all-multicast mode (ALLMULTI flag)</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_nodes-setting-interface-level-network-sysctls" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-containers-sysctls">Using sysctls in containers</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-sriov-networknodepolicy-object_configuring-sriov-device">SR-IOV network node configuration object</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-interface-level-sysctl-settings-sriov-device">Configuring interface-level network sysctl settings and all-multicast mode for SR-IOV networks</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="using-sctp">
<title>Using the Stream Control Transmission Protocol (SCTP) on a bare metal cluster</title>

<simpara>As a cluster administrator, you can use the Stream Control Transmission Protocol (SCTP) on a cluster.</simpara>
<section xml:id="nw-sctp-about_using-sctp">
<title>Support for Stream Control Transmission Protocol (SCTP) on OpenShift Container Platform</title>
<simpara>As a cluster administrator, you can enable SCTP on the hosts in the cluster.
On Red Hat Enterprise Linux CoreOS (RHCOS), the SCTP module is disabled by default.</simpara>
<simpara>SCTP is a reliable message based protocol that runs on top of an IP network.</simpara>
<simpara>When enabled, you can use SCTP as a protocol with pods, services, and network policy.
A <literal>Service</literal> object must be defined with the <literal>type</literal> parameter set to either the <literal>ClusterIP</literal> or <literal>NodePort</literal> value.</simpara>
<section xml:id="example_configurations_using-sctp">
<title>Example configurations using SCTP protocol</title>
<simpara>You can configure a pod or service to use SCTP by setting the <literal>protocol</literal> parameter to the <literal>SCTP</literal> value in the pod or service object.</simpara>
<simpara>In the following example, a pod is configured to use SCTP:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  namespace: project1
  name: example-pod
spec:
  containers:
    - name: example-pod
...
      ports:
        - containerPort: 30100
          name: sctpserver
          protocol: SCTP</programlisting>
<simpara>In the following example, a service is configured to use SCTP:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  namespace: project1
  name: sctpserver
spec:
...
  ports:
    - name: sctpserver
      protocol: SCTP
      port: 30100
      targetPort: 30100
  type: ClusterIP</programlisting>
<simpara>In the following example, a <literal>NetworkPolicy</literal> object is configured to apply to SCTP network traffic on port <literal>80</literal> from any pods with a specific label:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-sctp-on-http
spec:
  podSelector:
    matchLabels:
      role: web
  ingress:
  - ports:
    - protocol: SCTP
      port: 80</programlisting>
</section>
</section>
<section xml:id="nw-sctp-enabling_using-sctp">
<title>Enabling Stream Control Transmission Protocol (SCTP)</title>
<simpara>As a cluster administrator, you can load and enable the blacklisted SCTP kernel module on worker nodes in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file named <literal>load-sctp-module.yaml</literal> that contains the following YAML definition:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: load-sctp-module
  labels:
    machineconfiguration.openshift.io/role: worker
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
        - path: /etc/modprobe.d/sctp-blacklist.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,
        - path: /etc/modules-load.d/sctp-load.conf
          mode: 0644
          overwrite: true
          contents:
            source: data:,sctp</programlisting>
</listitem>
<listitem>
<simpara>To create the <literal>MachineConfig</literal> object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f load-sctp-module.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To watch the status of the nodes while the MachineConfig Operator applies the configuration change, enter the following command. When the status of a node transitions to <literal>Ready</literal>, the configuration update is applied.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-sctp-verifying_using-sctp">
<title>Verifying Stream Control Transmission Protocol (SCTP) is enabled</title>
<simpara>You can verify that SCTP is working on a cluster by creating a pod with an application that listens for SCTP traffic, associating it with a service, and then connecting to the exposed service.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the internet from the cluster to install the <literal>nc</literal> package.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a pod starts an SCTP listener:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file named <literal>sctp-server.yaml</literal> that defines a pod with the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: sctpserver
  labels:
    app: sctpserver
spec:
  containers:
    - name: sctpserver
      image: registry.access.redhat.com/ubi9/ubi
      command: ["/bin/sh", "-c"]
      args:
        ["dnf install -y nc &amp;&amp; sleep inf"]
      ports:
        - containerPort: 30102
          name: sctpserver
          protocol: SCTP</programlisting>
</listitem>
<listitem>
<simpara>Create the pod by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sctp-server.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a service for the SCTP listener pod.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file named <literal>sctp-service.yaml</literal> that defines a service with the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: sctpservice
  labels:
    app: sctpserver
spec:
  type: NodePort
  selector:
    app: sctpserver
  ports:
    - name: sctpserver
      protocol: SCTP
      port: 30102
      targetPort: 30102</programlisting>
</listitem>
<listitem>
<simpara>To create the service, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sctp-service.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a pod for the SCTP client.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file named <literal>sctp-client.yaml</literal> with the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: sctpclient
  labels:
    app: sctpclient
spec:
  containers:
    - name: sctpclient
      image: registry.access.redhat.com/ubi9/ubi
      command: ["/bin/sh", "-c"]
      args:
        ["dnf install -y nc &amp;&amp; sleep inf"]</programlisting>
</listitem>
<listitem>
<simpara>To create the <literal>Pod</literal> object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f sctp-client.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Run an SCTP listener on the server.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To connect to the server pod, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh sctpserver</programlisting>
</listitem>
<listitem>
<simpara>To start the SCTP listener, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ nc -l 30102 --sctp</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Connect to the SCTP listener on the server.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Open a new terminal window or tab in your terminal program.</simpara>
</listitem>
<listitem>
<simpara>Obtain the IP address of the <literal>sctpservice</literal> service. Enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get services sctpservice -o go-template='{{.spec.clusterIP}}{{"\n"}}'</programlisting>
</listitem>
<listitem>
<simpara>To connect to the client pod, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh sctpclient</programlisting>
</listitem>
<listitem>
<simpara>To start the SCTP client, enter the following command. Replace <literal>&lt;cluster_IP&gt;</literal> with the cluster IP address of the <literal>sctpservice</literal> service.</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># nc &lt;cluster_IP&gt; 30102 --sctp</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="_using-ptp-hardware">
<title>Using PTP hardware</title>
<section xml:id="about-ptp">
<title>About PTP in OpenShift Container Platform cluster nodes</title>

<simpara>Precision Time Protocol (PTP) is used to synchronize clocks in a network.
When used in conjunction with hardware support, PTP is capable of sub-microsecond accuracy, and is more accurate than Network Time Protocol (NTP).</simpara>
<simpara>You can configure <literal>linuxptp</literal> services and use PTP-capable hardware in OpenShift Container Platform cluster nodes.</simpara>
<simpara>Use the OpenShift Container Platform web console or OpenShift CLI (<literal>oc</literal>) to install PTP by deploying the PTP Operator. The PTP Operator creates and manages the <literal>linuxptp</literal> services and provides the following features:</simpara>
<itemizedlist>
<listitem>
<simpara>Discovery of the PTP-capable devices in the cluster.</simpara>
</listitem>
<listitem>
<simpara>Management of the configuration of <literal>linuxptp</literal> services.</simpara>
</listitem>
<listitem>
<simpara>Notification of PTP clock events that negatively affect the performance and reliability of your application with the PTP Operator <literal>cloud-event-proxy</literal> sidecar.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The PTP Operator works with PTP-capable devices on clusters provisioned only on bare-metal infrastructure.</simpara>
</note>
<section xml:id="ptp-elements_about-ptp">
<title>Elements of a PTP domain</title>
<simpara>PTP is used to synchronize multiple nodes connected in a network, with clocks for each node.
The clocks synchronized by PTP are organized in a leader-follower hierarchy.
The hierarchy is created and updated automatically by the best master clock (BMC) algorithm, which runs on every clock.
Follower clocks are synchronized to leader clocks, and follower clocks can themselves be the source for other downstream clocks.</simpara>
<figure>
<title>PTP nodes in the network</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/319_OpenShift_PTP_bare-metal_OCP_nodes_1123_PTP_network.png" contentwidth="boundary clock" contentdepth="and ordinary clock syncing from a GPS satellite that is connected to the PTP grandmaster clock. The boundary and ordinary clocks are synced to the grandmaster clock."/>
</imageobject>
<textobject><phrase>Diagram showing a PTP grandmaster clock</phrase></textobject>
</mediaobject>
</figure>
<simpara>The three primary types of PTP clocks are described below.</simpara>
<variablelist>
<varlistentry>
<term>Grandmaster clock</term>
<listitem>
<simpara>The grandmaster clock provides standard time information to other clocks across the network and ensures accurate and stable synchronisation. It writes time stamps and responds to time requests from other clocks. Grandmaster clocks synchronize to a Global Navigation Satellite System (GNSS) time source. The Grandmaster clock is the authoritative source of time in the network and is responsible for providing time synchronization to all other devices.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Boundary clock</term>
<listitem>
<simpara>The boundary clock has ports in two or more communication paths and can be a source and a destination to other destination clocks at the same time. The boundary clock works as a destination clock upstream. The destination clock receives the timing message, adjusts for delay, and then creates a new source time signal to pass down the network. The boundary clock produces a new timing packet that is still correctly synced with the source clock and can reduce the number of connected devices reporting directly to the source clock.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Ordinary clock</term>
<listitem>
<simpara>The ordinary clock has a single port connection that can play the role of source or destination clock, depending on its position in the network. The ordinary clock can read and write timestamps.</simpara>
</listitem>
</varlistentry>
</variablelist>
<bridgehead xml:id="ptp-advantages-over-ntp_about-ptp" renderas="sect4">Advantages of PTP over NTP</bridgehead>
<simpara>One of the main advantages that PTP has over NTP is the hardware support present in various network interface controllers (NIC) and network switches. The specialized hardware allows PTP to account for delays in message transfer and improves the accuracy of time synchronization. To achieve the best possible accuracy, it is recommended that all networking components between PTP clocks are PTP hardware enabled.</simpara>
<simpara>Hardware-based PTP provides optimal accuracy, since the NIC can timestamp the PTP packets at the exact moment they are sent and received. Compare this to software-based PTP, which requires additional processing of the PTP packets by the operating system.</simpara>
<important>
<simpara>Before enabling PTP, ensure that NTP is disabled for the required nodes. You can disable the chrony time service (<literal>chronyd</literal>) using a <literal>MachineConfig</literal> custom resource. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/postinstallation_configuration/#cnf-disable-chronyd_post-install-machine-configuration-tasks">Disabling chrony time service</link>.</simpara>
</important>
</section>
<section xml:id="ptp-dual-nics_about-ptp">
<title>Using dual Intel E810 NIC hardware with PTP</title>
<simpara>OpenShift Container Platform supports single and dual NIC Intel E810 hardware for precision PTP timing in grandmaster clocks (T-GM) and boundary clocks (T-BC).</simpara>
<variablelist>
<varlistentry>
<term>Dual NIC grandmaster clock</term>
<listitem>
<simpara>You can use a cluster host that has dual NIC hardware as PTP grandmaster clock.
One NIC receives timing information from the global navigation satellite system (GNSS).
The second NIC receives the timing information from the first using the SMA1 Tx/Rx connections on the E810 NIC faceplate.
The system clock on the cluster host is synchronized from the NIC that is connected to the GNSS satellite.</simpara>
<simpara>Dual NIC grandmaster clocks are a feature of distributed RAN (D-RAN) configurations where the Remote Radio Unit (RRU) and Baseband Unit (BBU) are located at the same radio cell site.
D-RAN distributes radio functions across multiple sites, with backhaul connections linking them to the core network.</simpara>
<figure>
<title>Dual NIC grandmaster clock</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/561_OpenShift_Using_PTP_network_0124.png"/>
</imageobject>
<textobject><phrase>Dual NIC PTP grandmaster clock connected to GNSS timing source and downstream PTP boundary and ordinary clocks</phrase></textobject>
</mediaobject>
</figure>
<note>
<simpara>In a dual NIC T-GM configuration, a single <literal>ts2phc</literal> process reports as two <literal>ts2phc</literal> instances in the system.</simpara>
</note>
</listitem>
</varlistentry>
<varlistentry>
<term>Dual NIC boundary clock</term>
<listitem>
<simpara>For 5G telco networks that deliver mid-band spectrum coverage, each virtual distributed unit (vDU) requires connections to 6 radio units (RUs). To make these connections, each vDU host requires 2 NICs configured as boundary clocks.</simpara>
<simpara>Dual NIC hardware allows you to connect each NIC to the same upstream leader clock with separate <literal>ptp4l</literal> instances for each NIC feeding the downstream clocks.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="ptp-linuxptp-introduction_about-ptp">
<title>Overview of linuxptp and gpsd in OpenShift Container Platform nodes</title>
<simpara>OpenShift Container Platform uses the PTP Operator with <literal>linuxptp</literal> and <literal>gpsd</literal> packages for high precision network synchronization.
The <literal>linuxptp</literal> package provides tools and daemons for PTP timing in networks.
Cluster hosts with Global Navigation Satellite System (GNSS) capable NICs use <literal>gpsd</literal> to interface with GNSS clock sources.</simpara>
<simpara>The <literal>linuxptp</literal> package includes the <literal>ts2phc</literal>, <literal>pmc</literal>, <literal>ptp4l</literal>, and <literal>phc2sys</literal> programs for system clock synchronization.</simpara>
<variablelist>
<varlistentry>
<term>ts2phc</term>
<listitem>
<simpara><literal>ts2phc</literal> synchronizes the PTP hardware clock (PHC) across PTP devices with a high degree of precision.
<literal>ts2phc</literal> is used in grandmaster clock configurations.
It receives the precision timing signal a high precision clock source such as Global Navigation Satellite System (GNSS).
GNSS provides an accurate and reliable source of synchronized time for use in large distributed networks.
GNSS clocks typically provide time information with a precision of a few nanoseconds.</simpara>
<simpara>The <literal>ts2phc</literal> system daemon sends timing information from the grandmaster clock to other PTP devices in the network by reading time information from the grandmaster clock and converting it to PHC format.
PHC time is used by other devices in the network to synchronize their clocks with the grandmaster clock.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>pmc</term>
<listitem>
<simpara><literal>pmc</literal> implements a PTP management client (<literal>pmc</literal>) according to IEEE standard 1588.1588.
<literal>pmc</literal> provides basic management access for the <literal>ptp4l</literal> system daemon.
<literal>pmc</literal> reads from standard input and sends the output over the selected transport, printing any replies it receives.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>ptp4l</term>
<listitem>
<simpara><literal>ptp4l</literal> implements the PTP boundary clock and ordinary clock and runs as a system daemon.
<literal>ptp4l</literal> does the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Synchronizes the PHC to the source clock with hardware time stamping</simpara>
</listitem>
<listitem>
<simpara>Synchronizes the system clock to the source clock with software time stamping</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>phc2sys</term>
<listitem>
<simpara><literal>phc2sys</literal> synchronizes the system clock to the PHC on the network interface controller (NIC).
The <literal>phc2sys</literal> system daemon continuously monitors the PHC for timing information.
When it detects a timing error, the PHC corrects the system clock.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The <literal>gpsd</literal> package includes the <literal>ubxtool</literal>, <literal>gspipe</literal>, <literal>gpsd</literal>, programs for GNSS clock synchronization with the host clock.</simpara>
<variablelist>
<varlistentry>
<term>ubxtool</term>
<listitem>
<simpara><literal>ubxtool</literal> CLI allows you to communicate with a u-blox GPS system. The <literal>ubxtool</literal> CLI uses the u-blox binary protocol to communicate with the GPS.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>gpspipe</term>
<listitem>
<simpara><literal>gpspipe</literal> connects to <literal>gpsd</literal> output and pipes it to <literal>stdout</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>gpsd</term>
<listitem>
<simpara><literal>gpsd</literal> is a service daemon that monitors one or more GPS or AIS receivers connected to the host.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="ptp-overview-of-gnss-grandmaster-clock_about-ptp">
<title>Overview of GNSS timing for PTP grandmaster clocks</title>
<simpara>OpenShift Container Platform supports receiving precision PTP timing from Global Navigation Satellite System (GNSS) sources and grandmaster clocks (T-GM) in the cluster.</simpara>
<important>
<simpara>OpenShift Container Platform supports PTP timing from GNSS sources with Intel E810 Westport Channel NICs only.</simpara>
</important>
<figure>
<title>Overview of Synchronization with GNSS and T-GM</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/319_OpenShift_PTP_bare-metal_OCP_nodes_1023_PTP.png"/>
</imageobject>
<textobject><phrase>GNSS and T-GM system architecture</phrase></textobject>
</mediaobject>
</figure>
<variablelist>
<varlistentry>
<term>Global Navigation Satellite System (GNSS)</term>
<listitem>
<simpara>GNSS is a satellite-based system used to provide positioning, navigation, and timing information to receivers around the globe.
In PTP, GNSS receivers are often used as a highly accurate and stable reference clock source.
These receivers receive signals from multiple GNSS satellites, allowing them to calculate precise time information.
The timing information obtained from GNSS is used as a reference by the PTP grandmaster clock.</simpara>
<simpara>By using GNSS as a reference, the grandmaster clock in the PTP network can provide highly accurate timestamps to other devices, enabling precise synchronization across the entire network.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Digital Phase-Locked Loop (DPLL)</term>
<listitem>
<simpara>DPLL provides clock synchronization between different PTP nodes in the network.
DPLL compares the phase of the local system clock signal with the phase of the incoming synchronization signal, for example, PTP messages from the PTP grandmaster clock.
The DPLL continuously adjusts the local clock frequency and phase to minimize the phase difference between the local clock and the reference clock.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="configuring-ptp">
<title>Configuring PTP devices</title>

<simpara>The PTP Operator adds the <literal>NodePtpDevice.ptp.openshift.io</literal> custom resource definition (CRD) to OpenShift Container Platform.</simpara>
<simpara>When installed, the PTP Operator searches your cluster for PTP-capable network devices on each node. It creates and updates a <literal>NodePtpDevice</literal> custom resource (CR) object for each node that provides a compatible PTP-capable network device.</simpara>
<section xml:id="install-ptp-operator-cli_configuring-ptp">
<title>Installing the PTP Operator using the CLI</title>
<simpara>As a cluster administrator, you can install the Operator by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster installed on bare-metal hardware with nodes that have hardware that supports PTP.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a namespace for the PTP Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>ptp-namespace.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-ptp
  annotations:
    workload.openshift.io/allowed: management
  labels:
    name: openshift-ptp
    openshift.io/cluster-monitoring: "true"</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>Namespace</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ptp-namespace.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an Operator group for the PTP Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>ptp-operatorgroup.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: ptp-operators
  namespace: openshift-ptp
spec:
  targetNamespaces:
  - openshift-ptp</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>OperatorGroup</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ptp-operatorgroup.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Subscribe to the PTP Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>ptp-sub.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ptp-operator-subscription
  namespace: openshift-ptp
spec:
  channel: "stable"
  name: ptp-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>Subscription</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ptp-sub.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To verify that the Operator is installed, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-ptp -o custom-columns=Name:.metadata.name,Phase:.status.phase</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name                         Phase
4.14.0-202301261535          Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="install-ptp-operator-web-console_configuring-ptp">
<title>Installing the PTP Operator by using the web console</title>
<simpara>As a cluster administrator, you can install the PTP Operator by using the web console.</simpara>
<note>
<simpara>You have to create the namespace and Operator group as mentioned
in the previous section.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the PTP Operator using the OpenShift Container Platform web console:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Choose  <emphasis role="strong">PTP Operator</emphasis> from the list of available Operators, and then click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, under <emphasis role="strong">A specific namespace on the cluster</emphasis> select <emphasis role="strong">openshift-ptp</emphasis>. Then, click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: Verify that the PTP Operator installed successfully:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Switch to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Ensure that <emphasis role="strong">PTP Operator</emphasis> is listed in the <emphasis role="strong">openshift-ptp</emphasis> project with a <emphasis role="strong">Status</emphasis> of <emphasis role="strong">InstallSucceeded</emphasis>.</simpara>
<note>
<simpara>During installation an Operator might display a <emphasis role="strong">Failed</emphasis> status.
If the installation later succeeds with an <emphasis role="strong">InstallSucceeded</emphasis> message, you can ignore the <emphasis role="strong">Failed</emphasis> message.</simpara>
</note>
<simpara>If the Operator does not appear as installed, to troubleshoot further:</simpara>
<itemizedlist>
<listitem>
<simpara>Go to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page and inspect the <emphasis role="strong">Operator Subscriptions</emphasis> and <emphasis role="strong">Install Plans</emphasis> tabs for any failure or errors under <emphasis role="strong">Status</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Go to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and check the logs for pods in the <literal>openshift-ptp</literal> project.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="discover-ptp-devices_configuring-ptp">
<title>Discovering PTP capable network devices in your cluster</title>
<itemizedlist>
<listitem>
<simpara>To return a complete list of PTP capable network devices in your cluster, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get NodePtpDevice -n openshift-ptp -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: v1
items:
- apiVersion: ptp.openshift.io/v1
  kind: NodePtpDevice
  metadata:
    creationTimestamp: "2022-01-27T15:16:28Z"
    generation: 1
    name: dev-worker-0 <co xml:id="CO38-1"/>
    namespace: openshift-ptp
    resourceVersion: "6538103"
    uid: d42fc9ad-bcbf-4590-b6d8-b676c642781a
  spec: {}
  status:
    devices: <co xml:id="CO38-2"/>
    - name: eno1
    - name: eno2
    - name: eno3
    - name: eno4
    - name: enp5s0f0
    - name: enp5s0f1
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO38-1">
<para>The value for the <literal>name</literal> parameter is the same as the name of the parent node.</para>
</callout>
<callout arearefs="CO38-2">
<para>The <literal>devices</literal> collection includes a list of the PTP capable devices that the PTP Operator discovers for the node.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="ptp-using-hardware-specific-nic-features_configuring-ptp">
<title>Using hardware-specific NIC features with the PTP Operator</title>
<simpara>NIC hardware with built-in PTP capabilities sometimes require device-specific configuration.
You can use hardware-specific NIC features for supported hardware with the PTP Operator by configuring a plugin in the <literal>PtpConfig</literal> custom resource (CR).
The <literal>linuxptp-daemon</literal> service uses the named parameters in the <literal>plugin</literal> stanza to start <literal>linuxptp</literal> processes (<literal>ptp4l</literal> and <literal>phc2sys</literal>) based on the specific hardware configuration.</simpara>
<important>
<simpara>In OpenShift Container Platform 4.14, the Intel E810 NIC is supported with a <literal>PtpConfig</literal> plugin.</simpara>
</important>
</section>
<section xml:id="configuring-linuxptp-services-as-grandmaster-clock_configuring-ptp">
<title>Configuring linuxptp services as a grandmaster clock</title>
<simpara>You can configure the <literal>linuxptp</literal> services (<literal>ptp4l</literal>, <literal>phc2sys</literal>, <literal>ts2phc</literal>) as grandmaster clock (T-GM) by creating a <literal>PtpConfig</literal> custom resource (CR) that configures the host NIC.</simpara>
<simpara>The <literal>ts2phc</literal> utility allows you to synchronize the system clock with the PTP grandmaster clock so that the node can stream precision clock signal to downstream PTP ordinary clocks and boundary clocks.</simpara>
<note>
<simpara>Use the following example <literal>PtpConfig</literal> CR as the basis to configure <literal>linuxptp</literal> services as T-GM for an Intel Westport Channel E810-XXVDA4T network interface.</simpara>
<simpara>To configure PTP fast events, set appropriate values for <literal>ptp4lOpts</literal>, <literal>ptp4lConf</literal>, and <literal>ptpClockThreshold</literal>.
<literal>ptpClockThreshold</literal> is used only when events are enabled.
See "Configuring the PTP fast event notifications publisher" for more information.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>For T-GM clocks in production environments, install an Intel E810 Westport Channel NIC in the bare-metal cluster host.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>PtpConfig</literal> CR. For example:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Depending on your requirements, use one of the following T-GM configurations for your deployment.
Save the YAML in the <literal>grandmaster-clock-ptp-config.yaml</literal> file:</simpara>
<example>
<title>Example PTP grandmaster clock configuration</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: grandmaster
  namespace: openshift-ptp
spec:
  profile:
  - name: "grandmaster"
    ptp4lOpts: "-2 --summary_interval -4"
    phc2sysOpts: -r -u 0 -m -O -37 -N 8 -R 16 -s $iface_master -n 24
    ptpSchedulingPolicy: SCHED_FIFO
    ptpSchedulingPriority: 10
    ptpSettings:
      logReduce: "true"
    plugins:
      e810:
        enableDefaultConfig: false
        settings:
          LocalMaxHoldoverOffSet: 1500
          LocalHoldoverTimeout: 14400
          MaxInSpecOffset: 100
        pins: $e810_pins
        #  "$iface_master":
        #    "U.FL2": "0 2"
        #    "U.FL1": "0 1"
        #    "SMA2": "0 2"
        #    "SMA1": "0 1"
        ublxCmds:
          - args: #ubxtool -P 29.20 -z CFG-HW-ANT_CFG_VOLTCTRL,1
              - "-P"
              - "29.20"
              - "-z"
              - "CFG-HW-ANT_CFG_VOLTCTRL,1"
            reportOutput: false
          - args: #ubxtool -P 29.20 -e GPS
              - "-P"
              - "29.20"
              - "-e"
              - "GPS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d Galileo
              - "-P"
              - "29.20"
              - "-d"
              - "Galileo"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d GLONASS
              - "-P"
              - "29.20"
              - "-d"
              - "GLONASS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d BeiDou
              - "-P"
              - "29.20"
              - "-d"
              - "BeiDou"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d SBAS
              - "-P"
              - "29.20"
              - "-d"
              - "SBAS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -t -w 5 -v 1 -e SURVEYIN,600,50000
              - "-P"
              - "29.20"
              - "-t"
              - "-w"
              - "5"
              - "-v"
              - "1"
              - "-e"
              - "SURVEYIN,600,50000"
            reportOutput: true
          - args: #ubxtool -P 29.20 -p MON-HW
              - "-P"
              - "29.20"
              - "-p"
              - "MON-HW"
            reportOutput: true
    ts2phcOpts: " "
    ts2phcConf: |
      [nmea]
      ts2phc.master 1
      [global]
      use_syslog  0
      verbose 1
      logging_level 7
      ts2phc.pulsewidth 100000000
      ts2phc.nmea_serialport $gnss_serialport
      leapfile  /usr/share/zoneinfo/leap-seconds.list
      [$iface_master]
      ts2phc.extts_polarity rising
      ts2phc.extts_correction 0
    ptp4lConf: |
      [$iface_master]
      masterOnly 1
      [$iface_master_1]
      masterOnly 1
      [$iface_master_2]
      masterOnly 1
      [$iface_master_3]
      masterOnly 1
      [global]
      #
      # Default Data Set
      #
      twoStepFlag 1
      priority1 128
      priority2 128
      domainNumber 24
      #utc_offset 37
      clockClass 6
      clockAccuracy 0x27
      offsetScaledLogVariance 0xFFFF
      free_running 0
      freq_est_interval 1
      dscp_event 0
      dscp_general 0
      dataset_comparison G.8275.x
      G.8275.defaultDS.localPriority 128
      #
      # Port Data Set
      #
      logAnnounceInterval -3
      logSyncInterval -4
      logMinDelayReqInterval -4
      logMinPdelayReqInterval 0
      announceReceiptTimeout 3
      syncReceiptTimeout 0
      delayAsymmetry 0
      fault_reset_interval -4
      neighborPropDelayThresh 20000000
      masterOnly 0
      G.8275.portDS.localPriority 128
      #
      # Run time options
      #
      assume_two_step 0
      logging_level 6
      path_trace_enabled 0
      follow_up_info 0
      hybrid_e2e 0
      inhibit_multicast_service 0
      net_sync_monitor 0
      tc_spanning_tree 0
      tx_timestamp_timeout 50
      unicast_listen 0
      unicast_master_table 0
      unicast_req_duration 3600
      use_syslog 1
      verbose 0
      summary_interval -4
      kernel_leap 1
      check_fup_sync 0
      clock_class_threshold 7
      #
      # Servo Options
      #
      pi_proportional_const 0.0
      pi_integral_const 0.0
      pi_proportional_scale 0.0
      pi_proportional_exponent -0.3
      pi_proportional_norm_max 0.7
      pi_integral_scale 0.0
      pi_integral_exponent 0.4
      pi_integral_norm_max 0.3
      step_threshold 2.0
      first_step_threshold 0.00002
      clock_servo pi
      sanity_freq_limit  200000000
      ntpshm_segment 0
      #
      # Transport options
      #
      transportSpecific 0x0
      ptp_dst_mac 01:1B:19:00:00:00
      p2p_dst_mac 01:80:C2:00:00:0E
      udp_ttl 1
      udp6_scope 0x0E
      uds_address /var/run/ptp4l
      #
      # Default interface options
      #
      clock_type BC
      network_transport L2
      delay_mechanism E2E
      time_stamping hardware
      tsproc_mode filter
      delay_filter moving_median
      delay_filter_length 10
      egressLatency 0
      ingressLatency 0
      boundary_clock_jbod 0
      #
      # Clock description
      #
      productDescription ;;
      revisionData ;;
      manufacturerIdentity 00:00:00
      userDescription ;
      timeSource 0x20
  recommend:
  - profile: "grandmaster"
    priority: 4
    match:
    - nodeLabel: "node-role.kubernetes.io/$mcp"</programlisting>
<note>
<simpara>The example PTP grandmaster clock configuration is for test purposes only and is not intended for production.</simpara>
</note>
</example>
<example>
<title>PTP grandmaster clock configuration for E810 NIC</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: grandmaster
  namespace: openshift-ptp
spec:
  profile:
  - name: "grandmaster"
    ptp4lOpts: "-2 --summary_interval -4"
    phc2sysOpts: -r -u 0 -m -O -37 -N 8 -R 16 -s $iface_master -n 24
    ptpSchedulingPolicy: SCHED_FIFO
    ptpSchedulingPriority: 10
    ptpSettings:
      logReduce: "true"
    plugins:
      e810:
        enableDefaultConfig: false
        settings:
          LocalMaxHoldoverOffSet: 1500
          LocalHoldoverTimeout: 14400
          MaxInSpecOffset: 100
        pins: $e810_pins
        #  "$iface_master":
        #    "U.FL2": "0 2"
        #    "U.FL1": "0 1"
        #    "SMA2": "0 2"
        #    "SMA1": "0 1"
        ublxCmds:
          - args: #ubxtool -P 29.20 -z CFG-HW-ANT_CFG_VOLTCTRL,1
              - "-P"
              - "29.20"
              - "-z"
              - "CFG-HW-ANT_CFG_VOLTCTRL,1"
            reportOutput: false
          - args: #ubxtool -P 29.20 -e GPS
              - "-P"
              - "29.20"
              - "-e"
              - "GPS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d Galileo
              - "-P"
              - "29.20"
              - "-d"
              - "Galileo"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d GLONASS
              - "-P"
              - "29.20"
              - "-d"
              - "GLONASS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d BeiDou
              - "-P"
              - "29.20"
              - "-d"
              - "BeiDou"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d SBAS
              - "-P"
              - "29.20"
              - "-d"
              - "SBAS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -t -w 5 -v 1 -e SURVEYIN,600,50000
              - "-P"
              - "29.20"
              - "-t"
              - "-w"
              - "5"
              - "-v"
              - "1"
              - "-e"
              - "SURVEYIN,600,50000"
            reportOutput: true
          - args: #ubxtool -P 29.20 -p MON-HW
              - "-P"
              - "29.20"
              - "-p"
              - "MON-HW"
            reportOutput: true
    ts2phcOpts: " "
    ts2phcConf: |
      [nmea]
      ts2phc.master 1
      [global]
      use_syslog  0
      verbose 1
      logging_level 7
      ts2phc.pulsewidth 100000000
      ts2phc.nmea_serialport $gnss_serialport
      leapfile  /usr/share/zoneinfo/leap-seconds.list
      [$iface_master]
      ts2phc.extts_polarity rising
      ts2phc.extts_correction 0
    ptp4lConf: |
      [$iface_master]
      masterOnly 1
      [$iface_master_1]
      masterOnly 1
      [$iface_master_2]
      masterOnly 1
      [$iface_master_3]
      masterOnly 1
      [global]
      #
      # Default Data Set
      #
      twoStepFlag 1
      priority1 128
      priority2 128
      domainNumber 24
      #utc_offset 37
      clockClass 6
      clockAccuracy 0x27
      offsetScaledLogVariance 0xFFFF
      free_running 0
      freq_est_interval 1
      dscp_event 0
      dscp_general 0
      dataset_comparison G.8275.x
      G.8275.defaultDS.localPriority 128
      #
      # Port Data Set
      #
      logAnnounceInterval -3
      logSyncInterval -4
      logMinDelayReqInterval -4
      logMinPdelayReqInterval 0
      announceReceiptTimeout 3
      syncReceiptTimeout 0
      delayAsymmetry 0
      fault_reset_interval -4
      neighborPropDelayThresh 20000000
      masterOnly 0
      G.8275.portDS.localPriority 128
      #
      # Run time options
      #
      assume_two_step 0
      logging_level 6
      path_trace_enabled 0
      follow_up_info 0
      hybrid_e2e 0
      inhibit_multicast_service 0
      net_sync_monitor 0
      tc_spanning_tree 0
      tx_timestamp_timeout 50
      unicast_listen 0
      unicast_master_table 0
      unicast_req_duration 3600
      use_syslog 1
      verbose 0
      summary_interval -4
      kernel_leap 1
      check_fup_sync 0
      clock_class_threshold 7
      #
      # Servo Options
      #
      pi_proportional_const 0.0
      pi_integral_const 0.0
      pi_proportional_scale 0.0
      pi_proportional_exponent -0.3
      pi_proportional_norm_max 0.7
      pi_integral_scale 0.0
      pi_integral_exponent 0.4
      pi_integral_norm_max 0.3
      step_threshold 2.0
      first_step_threshold 0.00002
      clock_servo pi
      sanity_freq_limit  200000000
      ntpshm_segment 0
      #
      # Transport options
      #
      transportSpecific 0x0
      ptp_dst_mac 01:1B:19:00:00:00
      p2p_dst_mac 01:80:C2:00:00:0E
      udp_ttl 1
      udp6_scope 0x0E
      uds_address /var/run/ptp4l
      #
      # Default interface options
      #
      clock_type BC
      network_transport L2
      delay_mechanism E2E
      time_stamping hardware
      tsproc_mode filter
      delay_filter moving_median
      delay_filter_length 10
      egressLatency 0
      ingressLatency 0
      boundary_clock_jbod 0
      #
      # Clock description
      #
      productDescription ;;
      revisionData ;;
      manufacturerIdentity 00:00:00
      userDescription ;
      timeSource 0x20
  recommend:
  - profile: "grandmaster"
    priority: 4
    match:
    - nodeLabel: "node-role.kubernetes.io/$mcp"</programlisting>
</example>
<note>
<simpara>For E810 Westport Channel NICs, set the value for <literal>ts2phc.nmea_serialport</literal> to <literal>/dev/gnss0</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f grandmaster-clock-ptp-config.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check that the <literal>PtpConfig</literal> profile is applied to the node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of pods in the <literal>openshift-ptp</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                          READY   STATUS    RESTARTS   AGE     IP             NODE
linuxptp-daemon-74m2g         3/3     Running   3          4d15h   10.16.230.7    compute-1.example.com
ptp-operator-5f4f48d7c-x7zkf  1/1     Running   1          4d15h   10.128.1.145   compute-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the profile is correct. Examine the logs of the <literal>linuxptp</literal> daemon that corresponds to the node you specified in the <literal>PtpConfig</literal> profile.
Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs linuxptp-daemon-74m2g -n openshift-ptp -c linuxptp-daemon-container</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ts2phc[94980.334]: [ts2phc.0.config] nmea delay: 98690975 ns
ts2phc[94980.334]: [ts2phc.0.config] ens3f0 extts index 0 at 1676577329.999999999 corr 0 src 1676577330.901342528 diff -1
ts2phc[94980.334]: [ts2phc.0.config] ens3f0 master offset         -1 s2 freq      -1
ts2phc[94980.441]: [ts2phc.0.config] nmea sentence: GNRMC,195453.00,A,4233.24427,N,07126.64420,W,0.008,,160223,,,A,V
phc2sys[94980.450]: [ptp4l.0.config] CLOCK_REALTIME phc offset       943 s2 freq  -89604 delay    504
phc2sys[94980.512]: [ptp4l.0.config] CLOCK_REALTIME phc offset      1000 s2 freq  -89264 delay    474</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-linuxptp-services-as-grandmaster-clock-dual-nic_configuring-ptp">
<title>Configuring linuxptp services as a grandmaster clock for dual E810 Westport Channel NICs</title>
<simpara>You can configure the <literal>linuxptp</literal> services (<literal>ptp4l</literal>, <literal>phc2sys</literal>, <literal>ts2phc</literal>) as grandmaster clock (T-GM) for dual E810 Westport Channel NICs by creating a <literal>PtpConfig</literal> custom resource (CR) that configures the host NICs.</simpara>
<simpara>For distributed RAN (D-RAN) use cases, you can configure PTP for dual NICs as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>NIC one is synced to the global navigation satellite system (GNSS) time source.</simpara>
</listitem>
<listitem>
<simpara>NIC two is synced to the 1PPS timing output provided by NIC one. This configuration is provided by the PTP hardware plugin in the <literal>PtpConfig</literal> CR.</simpara>
</listitem>
</itemizedlist>
<simpara>The dual NIC PTP T-GM configuration uses a single instance of <literal>ptp4l</literal> and one <literal>ts2phc</literal> process reporting two <literal>ts2phc</literal> instances, one for each NIC.
The host system clock is synchronized from the NIC that is connected to the GNSS time source.</simpara>
<note>
<simpara>Use the following example <literal>PtpConfig</literal> CR as the basis to configure <literal>linuxptp</literal> services as T-GM for dual Intel Westport Channel E810-XXVDA4T network interfaces.</simpara>
<simpara>To configure PTP fast events, set appropriate values for <literal>ptp4lOpts</literal>, <literal>ptp4lConf</literal>, and <literal>ptpClockThreshold</literal>.
<literal>ptpClockThreshold</literal> is used only when events are enabled.
See "Configuring the PTP fast event notifications publisher" for more information.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>For T-GM clocks in production environments, install two Intel E810 Westport Channel NICs in the bare-metal cluster host.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>PtpConfig</literal> CR. For example:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>grandmaster-clock-ptp-config-dual-nics.yaml</literal> file:</simpara>
<example>
<title>PTP grandmaster clock configuration for dual E810 NICs</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: grandmaster
  namespace: openshift-ptp
  annotations:
    ran.openshift.io/ztp-deploy-wave: "10"
spec:
  profile:
  - name: "grandmaster"
    ptp4lOpts: "-2 --summary_interval -4"
    phc2sysOpts: -r -u 0 -m -O -37 -N 8 -R 16 -s $iface_nic1 -n 24
    ptpSchedulingPolicy: SCHED_FIFO
    ptpSchedulingPriority: 10
    ptpSettings:
      logReduce: "true"
    plugins:
      e810:
        enableDefaultConfig: false
        settings:
          LocalMaxHoldoverOffSet: 1500
          LocalHoldoverTimeout: 14400
          MaxInSpecOffset: 100
        pins:
         "$iface_nic1":
           "U.FL2": "0 2"
           "U.FL1": "0 1"
           "SMA2": "0 2"
           "SMA1": "2 1"
         "$iface_nic2":
           "U.FL2": "0 2"
           "U.FL1": "0 1"
           "SMA2": "0 2"
           "SMA1": "1 1"
        ublxCmds:
          - args: #ubxtool -P 29.20 -z CFG-HW-ANT_CFG_VOLTCTRL,1
              - "-P"
              - "29.20"
              - "-z"
              - "CFG-HW-ANT_CFG_VOLTCTRL,1"
            reportOutput: false
          - args: #ubxtool -P 29.20 -e GPS
              - "-P"
              - "29.20"
              - "-e"
              - "GPS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d Galileo
              - "-P"
              - "29.20"
              - "-d"
              - "Galileo"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d GLONASS
              - "-P"
              - "29.20"
              - "-d"
              - "GLONASS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d BeiDou
              - "-P"
              - "29.20"
              - "-d"
              - "BeiDou"
            reportOutput: false
          - args: #ubxtool -P 29.20 -d SBAS
              - "-P"
              - "29.20"
              - "-d"
              - "SBAS"
            reportOutput: false
          - args: #ubxtool -P 29.20 -t -w 5 -v 1 -e SURVEYIN,600,50000
              - "-P"
              - "29.20"
              - "-t"
              - "-w"
              - "5"
              - "-v"
              - "1"
              - "-e"
              - "SURVEYIN,600,50000"
            reportOutput: true
          - args: #ubxtool -P 29.20 -p MON-HW
              - "-P"
              - "29.20"
              - "-p"
              - "MON-HW"
            reportOutput: true
    ts2phcOpts: " "
    ts2phcConf: |
      [nmea]
      ts2phc.master 1
      [global]
      use_syslog  0
      verbose 1
      logging_level 7
      ts2phc.pulsewidth 100000000
      #cat /dev/GNSS to find available serial port
      #example value of gnss_serialport is /dev/ttyGNSS_1700_0
      ts2phc.nmea_serialport $gnss_serialport
      leapfile  /usr/share/zoneinfo/leap-seconds.list
      [$iface_nic1]
      ts2phc.extts_polarity rising
      ts2phc.extts_correction 0
      [$iface_nic2]
      ts2phc.master 0
      ts2phc.extts_polarity rising
      #this is a measured value in nanoseconds to compensate for SMA cable delay
      ts2phc.extts_correction -10
    ptp4lConf: |
      [$iface_nic1]
      masterOnly 1
      [$iface_nic1_1]
      masterOnly 1
      [$iface_nic1_2]
      masterOnly 1
      [$iface_nic1_3]
      masterOnly 1
      [$iface_nic2]
      masterOnly 1
      [$iface_nic2_1]
      masterOnly 1
      [$iface_nic2_2]
      masterOnly 1
      [$iface_nic2_3]
      masterOnly 1
      [global]
      #
      # Default Data Set
      #
      twoStepFlag 1
      priority1 128
      priority2 128
      domainNumber 24
      #utc_offset 37
      clockClass 6
      clockAccuracy 0x27
      offsetScaledLogVariance 0xFFFF
      free_running 0
      freq_est_interval 1
      dscp_event 0
      dscp_general 0
      dataset_comparison G.8275.x
      G.8275.defaultDS.localPriority 128
      #
      # Port Data Set
      #
      logAnnounceInterval -3
      logSyncInterval -4
      logMinDelayReqInterval -4
      logMinPdelayReqInterval 0
      announceReceiptTimeout 3
      syncReceiptTimeout 0
      delayAsymmetry 0
      fault_reset_interval -4
      neighborPropDelayThresh 20000000
      masterOnly 0
      G.8275.portDS.localPriority 128
      #
      # Run time options
      #
      assume_two_step 0
      logging_level 6
      path_trace_enabled 0
      follow_up_info 0
      hybrid_e2e 0
      inhibit_multicast_service 0
      net_sync_monitor 0
      tc_spanning_tree 0
      tx_timestamp_timeout 50
      unicast_listen 0
      unicast_master_table 0
      unicast_req_duration 3600
      use_syslog 1
      verbose 0
      summary_interval -4
      kernel_leap 1
      check_fup_sync 0
      clock_class_threshold 7
      #
      # Servo Options
      #
      pi_proportional_const 0.0
      pi_integral_const 0.0
      pi_proportional_scale 0.0
      pi_proportional_exponent -0.3
      pi_proportional_norm_max 0.7
      pi_integral_scale 0.0
      pi_integral_exponent 0.4
      pi_integral_norm_max 0.3
      step_threshold 2.0
      first_step_threshold 0.00002
      clock_servo pi
      sanity_freq_limit  200000000
      ntpshm_segment 0
      #
      # Transport options
      #
      transportSpecific 0x0
      ptp_dst_mac 01:1B:19:00:00:00
      p2p_dst_mac 01:80:C2:00:00:0E
      udp_ttl 1
      udp6_scope 0x0E
      uds_address /var/run/ptp4l
      #
      # Default interface options
      #
      clock_type BC
      network_transport L2
      delay_mechanism E2E
      time_stamping hardware
      tsproc_mode filter
      delay_filter moving_median
      delay_filter_length 10
      egressLatency 0
      ingressLatency 0
      boundary_clock_jbod 1
      #
      # Clock description
      #
      productDescription ;;
      revisionData ;;
      manufacturerIdentity 00:00:00
      userDescription ;
      timeSource 0x20
  recommend:
  - profile: "grandmaster"
    priority: 4
    match:
    - nodeLabel: "node-role.kubernetes.io/$mcp"</programlisting>
</example>
<note>
<simpara>For E810 Westport Channel NICs, set the value for <literal>ts2phc.nmea_serialport</literal> to <literal>/dev/gnss0</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f grandmaster-clock-ptp-config-dual-nics.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check that the <literal>PtpConfig</literal> profile is applied to the node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of pods in the <literal>openshift-ptp</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                          READY   STATUS    RESTARTS   AGE     IP             NODE
linuxptp-daemon-74m2g         3/3     Running   3          4d15h   10.16.230.7    compute-1.example.com
ptp-operator-5f4f48d7c-x7zkf  1/1     Running   1          4d15h   10.128.1.145   compute-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the profile is correct. Examine the logs of the <literal>linuxptp</literal> daemon that corresponds to the node you specified in the <literal>PtpConfig</literal> profile.
Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs linuxptp-daemon-74m2g -n openshift-ptp -c linuxptp-daemon-container</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ts2phc[509863.660]: [ts2phc.0.config] nmea delay: 347527248 ns
ts2phc[509863.660]: [ts2phc.0.config] ens2f0 extts index 0 at 1705516553.000000000 corr 0 src 1705516553.652499081 diff 0
ts2phc[509863.660]: [ts2phc.0.config] ens2f0 master offset          0 s2 freq      -0
I0117 18:35:16.000146 1633226 stats.go:57] state updated for ts2phc =s2
I0117 18:35:16.000163 1633226 event.go:417] dpll State s2, gnss State s2, tsphc state s2, gm state s2,
ts2phc[1705516516]:[ts2phc.0.config] ens2f0 nmea_status 1 offset 0 s2
GM[1705516516]:[ts2phc.0.config] ens2f0 T-GM-STATUS s2
ts2phc[509863.677]: [ts2phc.0.config] ens7f0 extts index 0 at 1705516553.000000010 corr -10 src 1705516553.652499081 diff 0
ts2phc[509863.677]: [ts2phc.0.config] ens7f0 master offset          0 s2 freq      -0
I0117 18:35:16.016597 1633226 stats.go:57] state updated for ts2phc =s2
phc2sys[509863.719]: [ptp4l.0.config] CLOCK_REALTIME phc offset        -6 s2 freq  +15441 delay    510
phc2sys[509863.782]: [ptp4l.0.config] CLOCK_REALTIME phc offset        -7 s2 freq  +15438 delay    502</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="cnf-configuring-the-ptp-fast-event-publisher_using-ptp-hardware-fast-events-framework">Configuring the PTP fast event notifications publisher</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-ptp-grandmaster-clock-configuration-reference_configuring-ptp">
<title>Grandmaster clock PtpConfig configuration reference</title>
<simpara>The following reference information describes the configuration options for the <literal>PtpConfig</literal> custom resource (CR) that configures the <literal>linuxptp</literal> services (<literal>ptp4l</literal>, <literal>phc2sys</literal>, <literal>ts2phc</literal>) as a grandmaster clock.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>PtpConfig configuration options for PTP Grandmaster clock</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">PtpConfig CR field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>plugins</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify an array of <literal>.exec.cmdline</literal> options that configure the NIC for grandmaster clock operation. Grandmaster clock configuration requires certain PTP pins to be disabled.</simpara><simpara>The plugin mechanism allows the PTP Operator to do automated hardware configuration.
For the Intel Westport Channel NIC, when <literal>enableDefaultConfig</literal> is true, The PTP Operator runs a hard-coded script to do the required configuration for the NIC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptp4lOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify system configuration options for the <literal>ptp4l</literal> service.
The options should not include the network interface name <literal>-i &lt;interface&gt;</literal> and service config file <literal>-f /etc/ptp4l.conf</literal> because the network interface name and the service config file are automatically appended.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptp4lConf</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the required configuration to start <literal>ptp4l</literal> as a grandmaster clock.
For example, the <literal>ens2f1</literal> interface synchronizes downstream connected devices.
For grandmaster clocks, set <literal>clockClass</literal> to <literal>6</literal> and set <literal>clockAccuracy</literal> to <literal>0x27</literal>.
Set <literal>timeSource</literal> to <literal>0x20</literal> for when receiving the timing signal from a Global navigation satellite system (GNSS).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tx_timestamp_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the maximum amount of time to wait for the transmit (TX) timestamp from the sender before discarding the data.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>boundary_clock_jbod</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the JBOD boundary clock time delay value.
This value is used to correct the time values that are passed between the network time devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>phc2sysOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify system config options for the <literal>phc2sys</literal> service.
If this field is empty the PTP Operator does not start the <literal>phc2sys</literal> service.</simpara>
<note>
<simpara>Ensure that the network interface listed here is configured as grandmaster and is referenced as required in the <literal>ts2phcConf</literal> and <literal>ptp4lConf</literal> fields.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpSchedulingPolicy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configure the scheduling policy for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes.
Default value is <literal>SCHED_OTHER</literal>.
Use <literal>SCHED_FIFO</literal> on systems that support FIFO scheduling.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpSchedulingPriority</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set an integer value from 1-65 to configure FIFO priority for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes when <literal>ptpSchedulingPolicy</literal> is set to <literal>SCHED_FIFO</literal>.
The <literal>ptpSchedulingPriority</literal> field is not used when <literal>ptpSchedulingPolicy</literal> is set to <literal>SCHED_OTHER</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpClockThreshold</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional.
If <literal>ptpClockThreshold</literal> stanza is not present, default values are used for <literal>ptpClockThreshold</literal> fields.
Stanza shows default <literal>ptpClockThreshold</literal> values. <literal>ptpClockThreshold</literal> values configure how long after the PTP master clock is disconnected before PTP events are triggered.
<literal>holdOverTimeout</literal> is the time value in seconds before the PTP clock event state changes to <literal>FREERUN</literal> when the PTP master clock is disconnected.
The <literal>maxOffsetThreshold</literal> and <literal>minOffsetThreshold</literal> settings configure offset values in nanoseconds that compare against the values for <literal>CLOCK_REALTIME</literal> (<literal>phc2sys</literal>) or master offset (<literal>ptp4l</literal>).
When the <literal>ptp4l</literal> or <literal>phc2sys</literal> offset value is outside this range, the PTP clock state is set to <literal>FREERUN</literal>. When the offset value is within this range, the PTP clock state is set to <literal>LOCKED</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ts2phcConf</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the configuration for the <literal>ts2phc</literal> command.</simpara>
<simpara><literal>leapfile</literal> is the default path to the current leap seconds definition file in the PTP Operator container image.</simpara>
<simpara><literal>ts2phc.nmea_serialport</literal> is the serial port device that is connected to the NMEA GPS clock source.
When configured, the GNSS receiver is accessible on <literal>/dev/gnss&lt;id&gt;</literal>.
If the host has multiple GNSS receivers, you can find the correct device by enumerating either of the following devices:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/sys/class/net/&lt;eth_port&gt;/device/gnss/</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/sys/class/gnss/gnss&lt;id&gt;/device/</literal></simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ts2phcOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set options for the <literal>ts2phc</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>recommend</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify an array of one or more <literal>recommend</literal> objects that define rules on how the <literal>profile</literal> should be applied to nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.profile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the <literal>.recommend.profile</literal> object name that is defined in the <literal>profile</literal> section.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.priority</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the <literal>priority</literal> with an integer value between <literal>0</literal> and <literal>99</literal>.
A larger number gets lower priority, so a priority of <literal>99</literal> is lower than a priority of <literal>10</literal>.
If a node can be matched with multiple profiles according to rules defined in the <literal>match</literal> field, the profile with the higher priority is applied to that node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify <literal>.recommend.match</literal> rules with <literal>nodeLabel</literal> or <literal>nodeName</literal> values.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match.nodeLabel</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>nodeLabel</literal> with the <literal>key</literal> of the <literal>node.Labels</literal> field from the node object by using the <literal>oc get nodes --show-labels</literal> command.
For example, <literal>node-role.kubernetes.io/worker</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match.nodeName</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>nodeName</literal> with the value of the <literal>node.Name</literal> field from the node object by using the <literal>oc get nodes</literal> command.
For example, <literal>compute-1.example.com</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-ptp-grandmaster-clock-class-reference_configuring-ptp">
<title>Grandmaster clock class sync state reference</title>
<simpara>The following table describes the PTP grandmaster clock (T-GM) <literal>gm.ClockClass</literal> states.
Clock class states categorize T-GM clocks based on their accuracy and stability with regard to the Primary Reference Time Clock (PRTC) or other timing source.</simpara>
<simpara>Holdover specification is the amount of time a PTP clock can maintain synchronization without receiving updates from the primary time source.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>T-GM clock class states</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Clock class state</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>gm.ClockClass 6</literal></simpara></entry>
<entry align="left" valign="top"><simpara>T-GM clock is connected to a PRTC in <literal>LOCKED</literal> mode.
For example, the PRTC is traceable to a GNSS time source.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>gm.ClockClass 7</literal></simpara></entry>
<entry align="left" valign="top"><simpara>T-GM clock is in <literal>HOLDOVER</literal> mode, and within holdover specification.
The clock source might not be traceable to a category 1 frequency source.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>gm.ClockClass 140</literal></simpara></entry>
<entry align="left" valign="top"><simpara>T-GM clock is in <literal>HOLDOVER</literal> mode, is out of holdover specification, but it is still traceable to the category 1 frequency source.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>gm.ClockClass 248</literal></simpara></entry>
<entry align="left" valign="top"><simpara>T-GM clock is in <literal>FREERUN</literal> mode.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For more information, see <link xlink:href="https://www.itu.int/rec/T-REC-G.8275.1-202211-I/en">"Phase/time traceability information", ITU-T G.8275.1/Y.1369.1 Recommendations</link>.</simpara>
</section>
<section xml:id="nw-ptp-wpc-hardware-pins-reference_configuring-ptp">
<title>Intel Westport Channel E810 hardware configuration reference</title>
<simpara>Use this information to understand how to use the <link xlink:href="https://github.com/openshift/linuxptp-daemon/blob/release-4.15/addons/intel/e810.go">Intel E810-XXVDA4T hardware plugin</link> to configure the E810 network interface as PTP grandmaster clock.
Hardware pin configuration determines how the network interface interacts with other components and devices in the system.
The E810-XXVDA4T NIC has four connectors for external 1PPS signals: <literal>SMA1</literal>, <literal>SMA2</literal>, <literal>U.FL1</literal>, and <literal>U.FL2</literal>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Intel E810 NIC hardware connectors configuration</title>
<?dbhtml table-width="90%"?>
<?dbfo table-width="90%"?>
<?dblatex table-width="90%"?>
<tgroup cols="3">
<colspec colname="col_1" colwidth="127.4998*"/>
<colspec colname="col_2" colwidth="127.4998*"/>
<colspec colname="col_3" colwidth="127.5002*"/>
<thead>
<row>
<entry align="left" valign="top">Hardware pin</entry>
<entry align="left" valign="top">Recommended setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>U.FL1</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0 1</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the <literal>U.FL1</literal> connector input.
The <literal>U.FL1</literal> connector is output-only.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>U.FL2</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0 2</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the <literal>U.FL2</literal> connector output.
The <literal>U.FL2</literal> connector is input-only.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>SMA1</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0 1</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the <literal>SMA1</literal> connector input.
The <literal>SMA1</literal> connector is bidirectional.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>SMA2</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0 2</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the <literal>SMA2</literal> connector output.
The <literal>SMA2</literal> connector is bidirectional.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara><literal>SMA1</literal> and <literal>U.FL1</literal> connectors share channel one.
<literal>SMA2</literal> and <literal>U.FL2</literal> connectors share channel two.</simpara>
</note>
<simpara>Set <literal>spec.profile.plugins.e810.ublxCmds</literal> parameters to configure the GNSS clock in the <literal>PtpConfig</literal> custom resource (CR).
Each of these <literal>ublxCmds</literal> stanzas correspond to a configuration that is applied to the host NIC by using <literal>ubxtool</literal> commands.
For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">ublxCmds:
  - args: #ubxtool -P 29.20 -z CFG-HW-ANT_CFG_VOLTCTRL,1
      - "-P"
      - "29.20"
      - "-z"
      - "CFG-HW-ANT_CFG_VOLTCTRL,1"
    reportOutput: false</programlisting>
<simpara>The following table describes the equivalent <literal>ubxtool</literal> commands:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Intel E810 ublxCmds configuration</title>
<?dbhtml table-width="90%"?>
<?dbfo table-width="90%"?>
<?dblatex table-width="90%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="191.25*"/>
<colspec colname="col_2" colwidth="191.25*"/>
<thead>
<row>
<entry align="left" valign="top">ubxtool command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -z CFG-HW-ANT_CFG_VOLTCTRL,1</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables antenna voltage control. Enables antenna status to be reported in the <literal>UBX-MON-RF</literal> and <literal>UBX-INF-NOTICE</literal> log messages.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -e GPS</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables the antenna to receive GPS signals.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -d Galileo</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures the antenna to receive signal from the Galileo GPS satellite.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -d GLONASS</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the antenna from receiving signal from the GLONASS GPS satellite.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -d BeiDou</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the antenna from receiving signal from the BeiDou GPS satellite.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -d SBAS</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the antenna from receiving signal from the SBAS GPS satellite.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -t -w 5 -v 1 -e SURVEYIN,600,50000</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures the GNSS receiver survey-in process to improve its initial position estimate. This can take up to 24 hours to achieve an optimal result.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ubxtool -P 29.20 -p MON-HW</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Runs a single automated scan of the hardware and reports on the NIC state and configuration settings.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The E810 plugin implements the following interfaces:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>E810 plugin interfaces</title>
<?dbhtml table-width="90%"?>
<?dbfo table-width="90%"?>
<?dblatex table-width="90%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="95.625*"/>
<colspec colname="col_2" colwidth="286.875*"/>
<thead>
<row>
<entry align="left" valign="top">Interface</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>OnPTPConfigChangeE810</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Runs whenever you update the <literal>PtpConfig</literal> CR.
The function parses the plugin options and applies the required configurations to the network device pins based on the configuration data.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>AfterRunPTPCommandE810</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Runs after launching the PTP processes and running the <literal>gpspipe</literal> PTP command.
The function processes the plugin options and runs <literal>ubxtool</literal> commands, storing the output in the plugin-specific data.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>PopulateHwConfigE810</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Populates the <literal>NodePtpDevice</literal> CR based on hardware-specific data in the <literal>PtpConfig</literal> CR.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The E810 plugin has the following structs and variables:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>E810 plugin structs and variables</title>
<?dbhtml table-width="90%"?>
<?dbfo table-width="90%"?>
<?dblatex table-width="90%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="95.625*"/>
<colspec colname="col_2" colwidth="286.875*"/>
<thead>
<row>
<entry align="left" valign="top">Struct</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>E810Opts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Represents options for the E810 plugin, including boolean flags and a map of network device pins.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>E810UblxCmds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Represents configurations for <literal>ubxtool</literal> commands with a boolean flag and a slice of strings for command arguments.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>E810PluginData</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Holds plugin-specific data used during plugin execution.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-ptp-dual-wpc-hardware-config-reference_configuring-ptp">
<title>Dual E810 Westport Channel NIC configuration reference</title>
<simpara>Use this information to understand how to use the <link xlink:href="https://github.com/openshift/linuxptp-daemon/blob/release-4.14/addons/intel/e810.go">Intel E810-XXVDA4T hardware plugin</link> to configure a pair of E810 network interfaces as PTP grandmaster clock (T-GM).</simpara>
<simpara>Before you configure the dual NIC cluster host, you must connect the two NICs with an SMA1 cable using the 1PPS faceplace connections.</simpara>
<simpara>When you configure a dual NIC T-GM, you need to compensate for the 1PPS signal delay that occurs when you connect the NICs using the SMA1 connection ports.
Various factors such as cable length, ambient temperature, and component and manufacturing tolerances can affect the signal delay.
To compensate for the delay, you must calculate the specific value that you use to offset the signal delay.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>E810 dual NIC T-GM PtpConfig CR reference</title>
<?dbhtml table-width="90%"?>
<?dbfo table-width="90%"?>
<?dblatex table-width="90%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="127.4998*"/>
<colspec colname="col_2" colwidth="255.0001*"/>
<thead>
<row>
<entry align="left" valign="top">PtpConfig field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>spec.profile.plugins.e810.pins</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configure the E810 hardware pins using the PTP Operator E810 hardware plugin.</simpara>
<itemizedlist>
<listitem>
<simpara>Pin <literal>2 1</literal> enables the <literal>1PPS OUT</literal> connection for <literal>SMA1</literal> on NIC one.</simpara>
</listitem>
<listitem>
<simpara>Pin <literal>1 1</literal> enables the <literal>1PPS IN</literal> connection for <literal>SMA1</literal> on NIC two.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.profile.ts2phcConf</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Use the <literal>ts2phcConf</literal> field to configure parameters for NIC one and NIC two.
Set <literal>ts2phc.master 0</literal> for NIC two.
This configures the timing source for NIC two from the 1PPS input, not GNSS.
Configure the <literal>ts2phc.extts_correction</literal> value for NIC two to compensate for the delay that is incurred for the specific SMA cable and cable length that you use.
The value that you configure depends on your specific measurements and SMA1 cable length.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.profile.ptp4lConf</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set the value of <literal>boundary_clock_jbod</literal> to 1 to enable support for multiple NICs.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="configuring-linuxptp-services-as-boundary-clock_configuring-ptp">
<title>Configuring linuxptp services as a boundary clock</title>
<simpara>You can configure the <literal>linuxptp</literal> services (<literal>ptp4l</literal>, <literal>phc2sys</literal>) as boundary clock by creating a <literal>PtpConfig</literal> custom resource (CR) object.</simpara>
<note>
<simpara>Use the following example <literal>PtpConfig</literal> CR as the basis to configure <literal>linuxptp</literal> services as the boundary clock for your particular hardware and environment.
This example CR does not configure PTP fast events. To configure PTP fast events, set appropriate values for <literal>ptp4lOpts</literal>, <literal>ptp4lConf</literal>, and <literal>ptpClockThreshold</literal>.
<literal>ptpClockThreshold</literal> is used only when events are enabled.
See "Configuring the PTP fast event notifications publisher" for more information.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>PtpConfig</literal> CR, and then save the YAML in the <literal>boundary-clock-ptp-config.yaml</literal> file.</simpara>
<formalpara>
<title>Example PTP boundary clock configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: boundary-clock
  namespace: openshift-ptp
  annotations: {}
spec:
  profile:
    - name: boundary-clock
      ptp4lOpts: "-2"
      phc2sysOpts: "-a -r -n 24"
      ptpSchedulingPolicy: SCHED_FIFO
      ptpSchedulingPriority: 10
      ptpSettings:
        logReduce: "true"
      ptp4lConf: |
        # The interface name is hardware-specific
        [$iface_slave]
        masterOnly 0
        [$iface_master_1]
        masterOnly 1
        [$iface_master_2]
        masterOnly 1
        [$iface_master_3]
        masterOnly 1
        [global]
        #
        # Default Data Set
        #
        twoStepFlag 1
        slaveOnly 0
        priority1 128
        priority2 128
        domainNumber 24
        #utc_offset 37
        clockClass 248
        clockAccuracy 0xFE
        offsetScaledLogVariance 0xFFFF
        free_running 0
        freq_est_interval 1
        dscp_event 0
        dscp_general 0
        dataset_comparison G.8275.x
        G.8275.defaultDS.localPriority 128
        #
        # Port Data Set
        #
        logAnnounceInterval -3
        logSyncInterval -4
        logMinDelayReqInterval -4
        logMinPdelayReqInterval -4
        announceReceiptTimeout 3
        syncReceiptTimeout 0
        delayAsymmetry 0
        fault_reset_interval -4
        neighborPropDelayThresh 20000000
        masterOnly 0
        G.8275.portDS.localPriority 128
        #
        # Run time options
        #
        assume_two_step 0
        logging_level 6
        path_trace_enabled 0
        follow_up_info 0
        hybrid_e2e 0
        inhibit_multicast_service 0
        net_sync_monitor 0
        tc_spanning_tree 0
        tx_timestamp_timeout 50
        unicast_listen 0
        unicast_master_table 0
        unicast_req_duration 3600
        use_syslog 1
        verbose 0
        summary_interval 0
        kernel_leap 1
        check_fup_sync 0
        clock_class_threshold 135
        #
        # Servo Options
        #
        pi_proportional_const 0.0
        pi_integral_const 0.0
        pi_proportional_scale 0.0
        pi_proportional_exponent -0.3
        pi_proportional_norm_max 0.7
        pi_integral_scale 0.0
        pi_integral_exponent 0.4
        pi_integral_norm_max 0.3
        step_threshold 2.0
        first_step_threshold 0.00002
        max_frequency 900000000
        clock_servo pi
        sanity_freq_limit 200000000
        ntpshm_segment 0
        #
        # Transport options
        #
        transportSpecific 0x0
        ptp_dst_mac 01:1B:19:00:00:00
        p2p_dst_mac 01:80:C2:00:00:0E
        udp_ttl 1
        udp6_scope 0x0E
        uds_address /var/run/ptp4l
        #
        # Default interface options
        #
        clock_type BC
        network_transport L2
        delay_mechanism E2E
        time_stamping hardware
        tsproc_mode filter
        delay_filter moving_median
        delay_filter_length 10
        egressLatency 0
        ingressLatency 0
        boundary_clock_jbod 0
        #
        # Clock description
        #
        productDescription ;;
        revisionData ;;
        manufacturerIdentity 00:00:00
        userDescription ;
        timeSource 0xA0
  recommend:
    - profile: boundary-clock
      priority: 4
      match:
        - nodeLabel: "node-role.kubernetes.io/$mcp"</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title>PTP boundary clock CR configuration options</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">CR field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the <literal>PtpConfig</literal> CR.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>profile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify an array of one or more <literal>profile</literal> objects.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the name of a profile object which uniquely identifies a profile object.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptp4lOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify system config options for the <literal>ptp4l</literal> service. The options should not include the network interface name <literal>-i &lt;interface&gt;</literal> and service config file <literal>-f /etc/ptp4l.conf</literal> because the network interface name and the service config file are automatically appended.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptp4lConf</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the required configuration to start <literal>ptp4l</literal> as boundary clock. For example, <literal>ens1f0</literal> synchronizes from a grandmaster clock and <literal>ens1f3</literal> synchronizes connected devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>&lt;interface_1&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The interface that receives the synchronization clock.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>&lt;interface_2&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The interface that sends the synchronization clock.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tx_timestamp_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>For Intel Columbiaville 800 Series NICs, set <literal>tx_timestamp_timeout</literal> to <literal>50</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>boundary_clock_jbod</literal></simpara></entry>
<entry align="left" valign="top"><simpara>For Intel Columbiaville 800 Series NICs, ensure <literal>boundary_clock_jbod</literal> is set to <literal>0</literal>. For Intel Fortville X710 Series NICs, ensure <literal>boundary_clock_jbod</literal> is set to <literal>1</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>phc2sysOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify system config options for the <literal>phc2sys</literal> service. If this field is empty, the PTP Operator does not start the <literal>phc2sys</literal> service.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpSchedulingPolicy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Scheduling policy for ptp4l and phc2sys processes. Default value is <literal>SCHED_OTHER</literal>. Use <literal>SCHED_FIFO</literal> on systems that support FIFO scheduling.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpSchedulingPriority</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Integer value from 1-65 used to set FIFO priority for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes when <literal>ptpSchedulingPolicy</literal> is set to <literal>SCHED_FIFO</literal>. The <literal>ptpSchedulingPriority</literal> field is not used when <literal>ptpSchedulingPolicy</literal> is set to <literal>SCHED_OTHER</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpClockThreshold</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional. If <literal>ptpClockThreshold</literal> is not present, default values are used for the <literal>ptpClockThreshold</literal> fields. <literal>ptpClockThreshold</literal> configures how long after the PTP master clock is disconnected before PTP events are triggered. <literal>holdOverTimeout</literal> is the time value in seconds before the PTP clock event state changes to <literal>FREERUN</literal> when the PTP master clock is disconnected. The <literal>maxOffsetThreshold</literal> and <literal>minOffsetThreshold</literal> settings configure offset values in nanoseconds that compare against the values for <literal>CLOCK_REALTIME</literal> (<literal>phc2sys</literal>) or master offset (<literal>ptp4l</literal>). When the <literal>ptp4l</literal> or <literal>phc2sys</literal> offset value is outside this range, the PTP clock state is set to <literal>FREERUN</literal>. When the offset value is within this range, the PTP clock state is set to <literal>LOCKED</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>recommend</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify an array of one or more <literal>recommend</literal> objects that define rules on how the <literal>profile</literal> should be applied to nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.profile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the <literal>.recommend.profile</literal> object name defined in the <literal>profile</literal> section.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.priority</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the <literal>priority</literal> with an integer value between <literal>0</literal> and <literal>99</literal>. A larger number gets lower priority, so a priority of <literal>99</literal> is lower than a priority of <literal>10</literal>. If a node can be matched with multiple profiles according to rules defined in the <literal>match</literal> field, the profile with the higher priority is applied to that node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify <literal>.recommend.match</literal> rules with <literal>nodeLabel</literal> or <literal>nodeName</literal> values.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match.nodeLabel</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>nodeLabel</literal> with the <literal>key</literal> of the <literal>node.Labels</literal> field from the node object by using the <literal>oc get nodes --show-labels</literal> command.
For example, <literal>node-role.kubernetes.io/worker</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match.nodeName</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>nodeName</literal> with the value of the <literal>node.Name</literal> field from the node object by using the <literal>oc get nodes</literal> command.
For example, <literal>compute-1.example.com</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</listitem>
<listitem>
<simpara>Create the CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f boundary-clock-ptp-config.yaml</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check that the <literal>PtpConfig</literal> profile is applied to the node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of pods in the <literal>openshift-ptp</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE
linuxptp-daemon-4xkbb           1/1     Running   0          43m   10.1.196.24      compute-0.example.com
linuxptp-daemon-tdspf           1/1     Running   0          43m   10.1.196.25      compute-1.example.com
ptp-operator-657bbb64c8-2f8sj   1/1     Running   0          43m   10.129.0.61      control-plane-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the profile is correct. Examine the logs of the <literal>linuxptp</literal> daemon that corresponds to the node you specified in the <literal>PtpConfig</literal> profile. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs linuxptp-daemon-4xkbb -n openshift-ptp -c linuxptp-daemon-container</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">I1115 09:41:17.117596 4143292 daemon.go:107] in applyNodePTPProfile
I1115 09:41:17.117604 4143292 daemon.go:109] updating NodePTPProfile to:
I1115 09:41:17.117607 4143292 daemon.go:110] ------------------------------------
I1115 09:41:17.117612 4143292 daemon.go:102] Profile Name: profile1
I1115 09:41:17.117616 4143292 daemon.go:102] Interface:
I1115 09:41:17.117620 4143292 daemon.go:102] Ptp4lOpts: -2
I1115 09:41:17.117623 4143292 daemon.go:102] Phc2sysOpts: -a -r -n 24
I1115 09:41:17.117626 4143292 daemon.go:116] ------------------------------------</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="cnf-configuring-fifo-priority-scheduling-for-ptp_configuring-ptp">Configuring FIFO priority scheduling for PTP hardware</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="cnf-configuring-the-ptp-fast-event-publisher_using-ptp-hardware-fast-events-framework">Configuring the PTP fast event notifications publisher</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="ptp-configuring-linuxptp-services-as-bc-for-dual-nic_configuring-ptp">
<title>Configuring linuxptp services as boundary clocks for dual NIC hardware</title>
<important>
<simpara>Precision Time Protocol (PTP) hardware with dual NIC configured as boundary clocks is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You can configure the <literal>linuxptp</literal> services (<literal>ptp4l</literal>, <literal>phc2sys</literal>) as boundary clocks for dual NIC hardware by creating a <literal>PtpConfig</literal> custom resource (CR) object for each NIC.</simpara>
<simpara>Dual NIC hardware allows you to connect each NIC to the same upstream leader clock with separate <literal>ptp4l</literal> instances for each NIC feeding the downstream clocks.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create two separate <literal>PtpConfig</literal> CRs, one for each NIC, using the reference CR in "Configuring linuxptp services as a boundary clock" as the basis for each CR. For example:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create <literal>boundary-clock-ptp-config-nic1.yaml</literal>, specifying values for <literal>phc2sysOpts</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: boundary-clock-ptp-config-nic1
  namespace: openshift-ptp
spec:
  profile:
  - name: "profile1"
    ptp4lOpts: "-2 --summary_interval -4"
    ptp4lConf: | <co xml:id="CO39-1"/>
      [ens5f1]
      masterOnly 1
      [ens5f0]
      masterOnly 0
    ...
    phc2sysOpts: "-a -r -m -n 24 -N 8 -R 16" <co xml:id="CO39-2"/></programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>Specify the required interfaces to start <literal>ptp4l</literal> as a boundary clock. For example, <literal>ens5f0</literal> synchronizes from a grandmaster clock and <literal>ens5f1</literal> synchronizes connected devices.</para>
</callout>
<callout arearefs="CO39-2">
<para>Required <literal>phc2sysOpts</literal> values. <literal>-m</literal> prints messages to <literal>stdout</literal>. The <literal>linuxptp-daemon</literal> <literal>DaemonSet</literal> parses the logs and generates Prometheus metrics.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create <literal>boundary-clock-ptp-config-nic2.yaml</literal>, removing the <literal>phc2sysOpts</literal> field altogether to disable the <literal>phc2sys</literal> service for the second NIC:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: boundary-clock-ptp-config-nic2
  namespace: openshift-ptp
spec:
  profile:
  - name: "profile2"
    ptp4lOpts: "-2 --summary_interval -4"
    ptp4lConf: | <co xml:id="CO40-1"/>
      [ens7f1]
      masterOnly 1
      [ens7f0]
      masterOnly 0
...</programlisting>
<calloutlist>
<callout arearefs="CO40-1">
<para>Specify the required interfaces to start <literal>ptp4l</literal> as a boundary clock on the second NIC.</para>
</callout>
</calloutlist>
<note>
<simpara>You must completely remove the <literal>phc2sysOpts</literal> field from the second <literal>PtpConfig</literal> CR to disable the <literal>phc2sys</literal> service on the second NIC.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the dual NIC <literal>PtpConfig</literal> CRs by running the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the CR that configures PTP for the first NIC:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f boundary-clock-ptp-config-nic1.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create the CR that configures PTP for the second NIC:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f boundary-clock-ptp-config-nic2.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Check that the PTP Operator has applied the <literal>PtpConfig</literal> CRs for both NICs. Examine the logs for the <literal>linuxptp</literal> daemon corresponding to the node that has the dual NIC hardware installed. For example, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs linuxptp-daemon-cvgr6 -n openshift-ptp -c linuxptp-daemon-container</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ptp4l[80828.335]: [ptp4l.1.config] master offset          5 s2 freq   -5727 path delay       519
ptp4l[80828.343]: [ptp4l.0.config] master offset         -5 s2 freq  -10607 path delay       533
phc2sys[80828.390]: [ptp4l.0.config] CLOCK_REALTIME phc offset         1 s2 freq  -87239 delay    539</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-linuxptp-services-as-ordinary-clock_configuring-ptp">
<title>Configuring linuxptp services as an ordinary clock</title>
<simpara>You can configure <literal>linuxptp</literal> services (<literal>ptp4l</literal>, <literal>phc2sys</literal>) as ordinary clock by creating a <literal>PtpConfig</literal> custom resource (CR) object.</simpara>
<note>
<simpara>Use the following example <literal>PtpConfig</literal> CR as the basis to configure <literal>linuxptp</literal> services as an ordinary clock for your particular hardware and environment.
This example CR does not configure PTP fast events.
To configure PTP fast events, set appropriate values for <literal>ptp4lOpts</literal>, <literal>ptp4lConf</literal>, and <literal>ptpClockThreshold</literal>. <literal>ptpClockThreshold</literal> is required only when events are enabled.
See "Configuring the PTP fast event notifications publisher" for more information.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>PtpConfig</literal> CR, and then save the YAML in the <literal>ordinary-clock-ptp-config.yaml</literal> file.</simpara>
<formalpara xml:id="ptp-ordinary-clock">
<title>Example PTP ordinary clock configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: ordinary-clock
  namespace: openshift-ptp
  annotations: {}
spec:
  profile:
    - name: ordinary-clock
      # The interface name is hardware-specific
      interface: $interface
      ptp4lOpts: "-2 -s"
      phc2sysOpts: "-a -r -n 24"
      ptpSchedulingPolicy: SCHED_FIFO
      ptpSchedulingPriority: 10
      ptpSettings:
        logReduce: "true"
      ptp4lConf: |
        [global]
        #
        # Default Data Set
        #
        twoStepFlag 1
        slaveOnly 1
        priority1 128
        priority2 128
        domainNumber 24
        #utc_offset 37
        clockClass 255
        clockAccuracy 0xFE
        offsetScaledLogVariance 0xFFFF
        free_running 0
        freq_est_interval 1
        dscp_event 0
        dscp_general 0
        dataset_comparison G.8275.x
        G.8275.defaultDS.localPriority 128
        #
        # Port Data Set
        #
        logAnnounceInterval -3
        logSyncInterval -4
        logMinDelayReqInterval -4
        logMinPdelayReqInterval -4
        announceReceiptTimeout 3
        syncReceiptTimeout 0
        delayAsymmetry 0
        fault_reset_interval -4
        neighborPropDelayThresh 20000000
        masterOnly 0
        G.8275.portDS.localPriority 128
        #
        # Run time options
        #
        assume_two_step 0
        logging_level 6
        path_trace_enabled 0
        follow_up_info 0
        hybrid_e2e 0
        inhibit_multicast_service 0
        net_sync_monitor 0
        tc_spanning_tree 0
        tx_timestamp_timeout 50
        unicast_listen 0
        unicast_master_table 0
        unicast_req_duration 3600
        use_syslog 1
        verbose 0
        summary_interval 0
        kernel_leap 1
        check_fup_sync 0
        clock_class_threshold 7
        #
        # Servo Options
        #
        pi_proportional_const 0.0
        pi_integral_const 0.0
        pi_proportional_scale 0.0
        pi_proportional_exponent -0.3
        pi_proportional_norm_max 0.7
        pi_integral_scale 0.0
        pi_integral_exponent 0.4
        pi_integral_norm_max 0.3
        step_threshold 2.0
        first_step_threshold 0.00002
        max_frequency 900000000
        clock_servo pi
        sanity_freq_limit 200000000
        ntpshm_segment 0
        #
        # Transport options
        #
        transportSpecific 0x0
        ptp_dst_mac 01:1B:19:00:00:00
        p2p_dst_mac 01:80:C2:00:00:0E
        udp_ttl 1
        udp6_scope 0x0E
        uds_address /var/run/ptp4l
        #
        # Default interface options
        #
        clock_type OC
        network_transport L2
        delay_mechanism E2E
        time_stamping hardware
        tsproc_mode filter
        delay_filter moving_median
        delay_filter_length 10
        egressLatency 0
        ingressLatency 0
        boundary_clock_jbod 0
        #
        # Clock description
        #
        productDescription ;;
        revisionData ;;
        manufacturerIdentity 00:00:00
        userDescription ;
        timeSource 0xA0
  recommend:
    - profile: ordinary-clock
      priority: 4
      match:
        - nodeLabel: "node-role.kubernetes.io/$mcp"</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title>PTP ordinary clock CR configuration options</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">CR field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the <literal>PtpConfig</literal> CR.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>profile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify an array of one or more <literal>profile</literal> objects. Each profile must be uniquely named.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>interface</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the network interface to be used by the <literal>ptp4l</literal> service, for example <literal>ens787f1</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptp4lOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify system config options for the <literal>ptp4l</literal> service, for example <literal>-2</literal> to select the IEEE 802.3 network transport. The options should not include the network interface name <literal>-i &lt;interface&gt;</literal> and service config file <literal>-f /etc/ptp4l.conf</literal> because the network interface name and the service config file are automatically appended. Append <literal>--summary_interval -4</literal> to use PTP fast events with this interface.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>phc2sysOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify system config options for the <literal>phc2sys</literal> service. If this field is empty, the PTP Operator does not start the <literal>phc2sys</literal> service. For Intel Columbiaville 800 Series NICs, set <literal>phc2sysOpts</literal> options to <literal>-a -r -m -n 24 -N 8 -R 16</literal>. <literal>-m</literal> prints messages to <literal>stdout</literal>. The <literal>linuxptp-daemon</literal> <literal>DaemonSet</literal> parses the logs and generates Prometheus metrics.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptp4lConf</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify a string that contains the configuration to replace the default <literal>/etc/ptp4l.conf</literal> file. To use the default configuration, leave the field empty.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tx_timestamp_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>For Intel Columbiaville 800 Series NICs, set <literal>tx_timestamp_timeout</literal> to <literal>50</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>boundary_clock_jbod</literal></simpara></entry>
<entry align="left" valign="top"><simpara>For Intel Columbiaville 800 Series NICs, set <literal>boundary_clock_jbod</literal> to <literal>0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpSchedulingPolicy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Scheduling policy for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes. Default value is <literal>SCHED_OTHER</literal>. Use <literal>SCHED_FIFO</literal> on systems that support FIFO scheduling.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpSchedulingPriority</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Integer value from 1-65 used to set FIFO priority for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes when <literal>ptpSchedulingPolicy</literal> is set to <literal>SCHED_FIFO</literal>. The <literal>ptpSchedulingPriority</literal> field is not used when <literal>ptpSchedulingPolicy</literal> is set to <literal>SCHED_OTHER</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ptpClockThreshold</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional. If <literal>ptpClockThreshold</literal> is not present, default values are used for the <literal>ptpClockThreshold</literal> fields. <literal>ptpClockThreshold</literal> configures how long after the PTP master clock is disconnected before PTP events are triggered. <literal>holdOverTimeout</literal> is the time value in seconds before the PTP clock event state changes to <literal>FREERUN</literal> when the PTP master clock is disconnected. The <literal>maxOffsetThreshold</literal> and <literal>minOffsetThreshold</literal> settings configure offset values in nanoseconds that compare against the values for <literal>CLOCK_REALTIME</literal> (<literal>phc2sys</literal>) or master offset (<literal>ptp4l</literal>). When the <literal>ptp4l</literal> or <literal>phc2sys</literal> offset value is outside this range, the PTP clock state is set to <literal>FREERUN</literal>. When the offset value is within this range, the PTP clock state is set to <literal>LOCKED</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>recommend</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify an array of one or more <literal>recommend</literal> objects that define rules on how the <literal>profile</literal> should be applied to nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.profile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify the <literal>.recommend.profile</literal> object name defined in the <literal>profile</literal> section.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.priority</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>.recommend.priority</literal> to <literal>0</literal> for ordinary clock.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify <literal>.recommend.match</literal> rules with <literal>nodeLabel</literal> or <literal>nodeName</literal> values.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match.nodeLabel</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>nodeLabel</literal> with the <literal>key</literal> of the <literal>node.Labels</literal> field from the node object by using the <literal>oc get nodes --show-labels</literal> command.
For example, <literal>node-role.kubernetes.io/worker</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>.recommend.match.nodeName</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set <literal>nodeName</literal> with the value of the <literal>node.Name</literal> field from the node object by using the <literal>oc get nodes</literal> command.
For example, <literal>compute-1.example.com</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</listitem>
<listitem>
<simpara>Create the <literal>PtpConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ordinary-clock-ptp-config.yaml</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check that the <literal>PtpConfig</literal> profile is applied to the node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of pods in the <literal>openshift-ptp</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            READY   STATUS    RESTARTS   AGE   IP               NODE
linuxptp-daemon-4xkbb           1/1     Running   0          43m   10.1.196.24      compute-0.example.com
linuxptp-daemon-tdspf           1/1     Running   0          43m   10.1.196.25      compute-1.example.com
ptp-operator-657bbb64c8-2f8sj   1/1     Running   0          43m   10.129.0.61      control-plane-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the profile is correct. Examine the logs of the <literal>linuxptp</literal> daemon that corresponds to the node you specified in the <literal>PtpConfig</literal> profile. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs linuxptp-daemon-4xkbb -n openshift-ptp -c linuxptp-daemon-container</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">I1115 09:41:17.117596 4143292 daemon.go:107] in applyNodePTPProfile
I1115 09:41:17.117604 4143292 daemon.go:109] updating NodePTPProfile to:
I1115 09:41:17.117607 4143292 daemon.go:110] ------------------------------------
I1115 09:41:17.117612 4143292 daemon.go:102] Profile Name: profile1
I1115 09:41:17.117616 4143292 daemon.go:102] Interface: ens787f1
I1115 09:41:17.117620 4143292 daemon.go:102] Ptp4lOpts: -2 -s
I1115 09:41:17.117623 4143292 daemon.go:102] Phc2sysOpts: -a -r -n 24
I1115 09:41:17.117626 4143292 daemon.go:116] ------------------------------------</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="cnf-configuring-fifo-priority-scheduling-for-ptp_configuring-ptp">Configuring FIFO priority scheduling for PTP hardware</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="cnf-configuring-the-ptp-fast-event-publisher_using-ptp-hardware-fast-events-framework">Configuring the PTP fast event notifications publisher</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-columbiaville-ptp-config-refererence_configuring-ptp">
<title>Intel Columbiaville E800 series NIC as PTP ordinary clock reference</title>
<simpara>The following table describes the changes that you must make to the reference PTP configuration to use Intel Columbiaville E800 series NICs as ordinary clocks. Make the changes in a <literal>PtpConfig</literal> custom resource (CR) that you apply to the cluster.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Recommended PTP settings for Intel Columbiaville NIC</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">PTP configuration</entry>
<entry align="left" valign="top">Recommended setting</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>phc2sysOpts</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>-a -r -m -n 24 -N 8 -R 16</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tx_timestamp_timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>50</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>boundary_clock_jbod</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>For <literal>phc2sysOpts</literal>, <literal>-m</literal> prints messages to <literal>stdout</literal>. The <literal>linuxptp-daemon</literal> <literal>DaemonSet</literal> parses the logs and generates Prometheus metrics.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For a complete example CR that configures <literal>linuxptp</literal> services as an ordinary clock with PTP fast events, see <link linkend="configuring-linuxptp-services-as-ordinary-clock_configuring-ptp">Configuring linuxptp services as ordinary clock</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cnf-configuring-fifo-priority-scheduling-for-ptp_configuring-ptp">
<title>Configuring FIFO priority scheduling for PTP hardware</title>
<simpara>In telco or other deployment types that require low latency performance, PTP daemon threads run in a constrained CPU footprint alongside the rest of the infrastructure components. By default, PTP threads run with the <literal>SCHED_OTHER</literal> policy. Under high load, these threads might not get the scheduling latency they require for error-free operation.</simpara>
<simpara>To mitigate against potential scheduling latency errors, you can configure the PTP Operator <literal>linuxptp</literal> services to allow threads to run with a <literal>SCHED_FIFO</literal> policy. If <literal>SCHED_FIFO</literal> is set for a <literal>PtpConfig</literal> CR, then <literal>ptp4l</literal> and <literal>phc2sys</literal> will run in the parent container under <literal>chrt</literal> with a priority set by the <literal>ptpSchedulingPriority</literal> field of the <literal>PtpConfig</literal> CR.</simpara>
<note>
<simpara>Setting <literal>ptpSchedulingPolicy</literal> is optional, and is only required if you are experiencing latency errors.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>PtpConfig</literal> CR profile:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit PtpConfig -n openshift-ptp</programlisting>
</listitem>
<listitem>
<simpara>Change the <literal>ptpSchedulingPolicy</literal> and <literal>ptpSchedulingPriority</literal> fields:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: &lt;ptp_config_name&gt;
  namespace: openshift-ptp
...
spec:
  profile:
  - name: "profile1"
...
    ptpSchedulingPolicy: SCHED_FIFO <co xml:id="CO41-1"/>
    ptpSchedulingPriority: 10 <co xml:id="CO41-2"/></programlisting>
<calloutlist>
<callout arearefs="CO41-1">
<para>Scheduling policy for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes. Use <literal>SCHED_FIFO</literal> on systems that support FIFO scheduling.</para>
</callout>
<callout arearefs="CO41-2">
<para>Required. Sets the integer value 1-65 used to configure FIFO priority for <literal>ptp4l</literal> and <literal>phc2sys</literal> processes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save and exit to apply the changes to the <literal>PtpConfig</literal> CR.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Get the name of the <literal>linuxptp-daemon</literal> pod and corresponding node where the <literal>PtpConfig</literal> CR has been applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-gmv2n           3/3     Running   0          1d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-lgm55           3/3     Running   0          1d17h   10.1.196.25   compute-1.example.com
ptp-operator-3r4dcvf7f4-zndk7   1/1     Running   0          1d7h    10.129.0.61   control-plane-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the <literal>ptp4l</literal> process is running with the updated <literal>chrt</literal> FIFO priority:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ptp logs linuxptp-daemon-lgm55 -c linuxptp-daemon-container|grep chrt</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">I1216 19:24:57.091872 1600715 daemon.go:285] /bin/chrt -f 65 /usr/sbin/ptp4l -f /var/run/ptp4l.0.config -2  --summary_interval -4 -m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-configuring-log-filtering-for-linuxptp_configuring-ptp">
<title>Configuring log filtering for linuxptp services</title>
<simpara>The <literal>linuxptp</literal> daemon generates logs that you can use for debugging purposes. In telco or other deployment types that feature a limited storage capacity, these logs can add to the storage demand.</simpara>
<simpara>To reduce the number log messages, you can configure the <literal>PtpConfig</literal> custom resource (CR) to exclude log messages that report the <literal>master offset</literal> value. The <literal>master offset</literal> log message reports the difference between the current node&#8217;s clock and the master clock in nanoseconds.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>PtpConfig</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit PtpConfig -n openshift-ptp</programlisting>
</listitem>
<listitem>
<simpara>In <literal>spec.profile</literal>, add the <literal>ptpSettings.logReduce</literal> specification and set the value to <literal>true</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpConfig
metadata:
  name: &lt;ptp_config_name&gt;
  namespace: openshift-ptp
...
spec:
  profile:
  - name: "profile1"
...
    ptpSettings:
      logReduce: "true"</programlisting>
<note>
<simpara>For debugging purposes, you can revert this specification to <literal>False</literal> to include the master offset messages.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save and exit to apply the changes to the <literal>PtpConfig</literal> CR.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Get the name of the <literal>linuxptp-daemon</literal> pod and corresponding node where the <literal>PtpConfig</literal> CR has been applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-gmv2n           3/3     Running   0          1d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-lgm55           3/3     Running   0          1d17h   10.1.196.25   compute-1.example.com
ptp-operator-3r4dcvf7f4-zndk7   1/1     Running   0          1d7h    10.129.0.61   control-plane-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that master offset messages are excluded from the logs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ptp logs &lt;linux_daemon_container&gt; -c linuxptp-daemon-container | grep "master offset" <co xml:id="CO42-1"/></programlisting>
<calloutlist>
<callout arearefs="CO42-1">
<para>&lt;linux_daemon_container&gt; is the name of the <literal>linuxptp-daemon</literal> pod, for example <literal>linuxptp-daemon-gmv2n</literal>.</para>
</callout>
</calloutlist>
<simpara>When you configure the <literal>logReduce</literal> specification, this command does not report any instances of <literal>master offset</literal> in the logs of the <literal>linuxptp</literal> daemon.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-troubleshooting-common-ptp-operator-issues_configuring-ptp">
<title>Troubleshooting common PTP Operator issues</title>
<simpara>Troubleshoot common problems with the PTP Operator by performing the following steps.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the PTP Operator on a bare-metal cluster with hosts that support PTP.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the Operator and operands are successfully deployed in the cluster for the configured nodes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-lmvgn           3/3     Running   0          4d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-qhfg7           3/3     Running   0          4d17h   10.1.196.25   compute-1.example.com
ptp-operator-6b8dcbf7f4-zndk7   1/1     Running   0          5d7h    10.129.0.61   control-plane-1.example.com</programlisting>
</para>
</formalpara>
<note>
<simpara>When the PTP fast event bus is enabled, the number of ready <literal>linuxptp-daemon</literal> pods is <literal>3/3</literal>. If the PTP fast event bus is not enabled, <literal>2/2</literal> is displayed.</simpara>
</note>
</listitem>
<listitem>
<simpara>Check that supported hardware is found in the cluster.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ptp get nodeptpdevices.ptp.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                  AGE
control-plane-0.example.com           10d
control-plane-1.example.com           10d
compute-0.example.com                 10d
compute-1.example.com                 10d
compute-2.example.com                 10d</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the available PTP network interfaces for a node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ptp get nodeptpdevices.ptp.openshift.io &lt;node_name&gt; -o yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;node_name&gt;</term>
<listitem>
<simpara>Specifies the node you want to query, for example, <literal>compute-0.example.com</literal>.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: NodePtpDevice
metadata:
  creationTimestamp: "2021-09-14T16:52:33Z"
  generation: 1
  name: compute-0.example.com
  namespace: openshift-ptp
  resourceVersion: "177400"
  uid: 30413db0-4d8d-46da-9bef-737bacd548fd
spec: {}
status:
  devices:
  - name: eno1
  - name: eno2
  - name: eno3
  - name: eno4
  - name: enp5s0f0
  - name: enp5s0f1</programlisting>
</para>
</formalpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Check that the PTP interface is successfully synchronized to the primary clock by accessing the <literal>linuxptp-daemon</literal> pod for the corresponding node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the name of the <literal>linuxptp-daemon</literal> pod and corresponding node you want to troubleshoot by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            READY   STATUS    RESTARTS   AGE     IP            NODE
linuxptp-daemon-lmvgn           3/3     Running   0          4d17h   10.1.196.24   compute-0.example.com
linuxptp-daemon-qhfg7           3/3     Running   0          4d17h   10.1.196.25   compute-1.example.com
ptp-operator-6b8dcbf7f4-zndk7   1/1     Running   0          5d7h    10.129.0.61   control-plane-1.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Remote shell into the required <literal>linuxptp-daemon</literal> container:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-ptp -c linuxptp-daemon-container &lt;linux_daemon_container&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;linux_daemon_container&gt;</term>
<listitem>
<simpara>is the container you want to diagnose, for example <literal>linuxptp-daemon-lmvgn</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>In the remote shell connection to the <literal>linuxptp-daemon</literal> container, use the PTP Management Client (<literal>pmc</literal>) tool to diagnose the network interface. Run the following <literal>pmc</literal> command to check the sync status of the PTP device, for example <literal>ptp4l</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># pmc -u -f /var/run/ptp4l.0.config -b 0 'GET PORT_DATA_SET'</programlisting>
<formalpara>
<title>Example output when the node is successfully synced to the primary clock</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">sending: GET PORT_DATA_SET
    40a6b7.fffe.166ef0-1 seq 0 RESPONSE MANAGEMENT PORT_DATA_SET
        portIdentity            40a6b7.fffe.166ef0-1
        portState               SLAVE
        logMinDelayReqInterval  -4
        peerMeanPathDelay       0
        logAnnounceInterval     -3
        announceReceiptTimeout  3
        logSyncInterval         -4
        delayMechanism          1
        logMinPdelayReqInterval -4
        versionNumber           2</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For GNSS-sourced grandmaster clocks, verify that the in-tree NIC ice driver is correct by running the following command, for example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-ptp -c linuxptp-daemon-container linuxptp-daemon-74m2g ethtool -i ens7f0</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">driver: ice
version: 5.14.0-356.bz2232515.el9.x86_64
firmware-version: 4.20 0x8001778b 1.3346.0</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>For GNSS-sourced grandmaster clocks, verify that the <literal>linuxptp-daemon</literal> container is receiving signal from the GNSS antenna.
If the container is not receiving the GNSS signal, the <literal>/dev/gnss0</literal> file is not populated.
To verify, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-ptp -c linuxptp-daemon-container linuxptp-daemon-jnz6r cat /dev/gnss0</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$GNRMC,125223.00,A,4233.24463,N,07126.64561,W,0.000,,300823,,,A,V*0A
$GNVTG,,T,,M,0.000,N,0.000,K,A*3D
$GNGGA,125223.00,4233.24463,N,07126.64561,W,1,12,99.99,98.6,M,-33.1,M,,*7E
$GNGSA,A,3,25,17,19,11,12,06,05,04,09,20,,,99.99,99.99,99.99,1*37
$GPGSV,3,1,10,04,12,039,41,05,31,222,46,06,50,064,48,09,28,064,42,1*62</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-about-collecting-nro-data_configuring-ptp">
<title>Collecting PTP Operator data</title>
<simpara>You can use the <literal>oc adm must-gather</literal> command to collect information about your cluster, including features and objects associated with PTP Operator.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To collect PTP Operator data with <literal>must-gather</literal>, you must specify the PTP Operator <literal>must-gather</literal> image.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather --image=registry.redhat.io/openshift4/ptp-must-gather-rhel8:v4.14</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="using-ptp-hardware-fast-events-framework">
<title>Using the PTP hardware fast event notifications framework</title>

<simpara>Cloud native applications such as virtual RAN (vRAN) require access to notifications about hardware timing events that are critical to the functioning of the overall network.
PTP clock synchronization errors can negatively affect the performance and reliability of your low-latency application, for example, a vRAN application running in a distributed unit (DU).</simpara>
<section xml:id="cnf-about-ptp-and-clock-synchronization_using-ptp-hardware-fast-events-framework">
<title>About PTP and clock synchronization error events</title>
<simpara>Loss of PTP synchronization is a critical error for a RAN network. If synchronization is lost on a node, the radio might be shut down and the network Over the Air (OTA) traffic might be shifted to another node in the wireless network. Fast event notifications mitigate against workload errors by allowing cluster nodes to communicate PTP clock sync status to the vRAN application running in the DU.</simpara>
<simpara>Event notifications are available to vRAN applications running on the same DU node. A publish/subscribe REST API passes events notifications to the messaging bus. Publish/subscribe messaging, or pub-sub messaging, is an asynchronous service-to-service communication architecture where any message published to a topic is immediately received by all of the subscribers to the topic.</simpara>
<simpara>The PTP Operator generates fast event notifications for every PTP-capable network interface. You can access the events by using a <literal>cloud-event-proxy</literal> sidecar container over an HTTP or Advanced Message Queuing Protocol (AMQP) message bus.</simpara>
<note>
<simpara>PTP fast event notifications are available for network interfaces configured to use PTP ordinary clocks or PTP boundary clocks.</simpara>
</note>
<note>
<simpara>Use HTTP transport instead of AMQP for PTP and bare-metal events where possible.
AMQ Interconnect is EOL from 30 June 2024.
Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029.
For more information see, <link xlink:href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</link>.</simpara>
</note>
</section>
<section xml:id="cnf-about-ptp-fast-event-notifications-framework_using-ptp-hardware-fast-events-framework">
<title>About the PTP fast event notifications framework</title>
<simpara>Use the Precision Time Protocol (PTP) fast event notifications framework to subscribe cluster applications to PTP events that the bare-metal cluster node generates.</simpara>
<note>
<simpara>The fast events notifications framework uses a REST API for communication. The REST API is based on the <emphasis>O-RAN O-Cloud Notification API Specification for Event Consumers 3.0</emphasis> that is available from <link xlink:href="https://orandownloadsweb.azurewebsites.net/specifications">O-RAN ALLIANCE Specifications</link>.</simpara>
</note>
<simpara>The framework consists of a publisher, subscriber, and an AMQ or HTTP messaging protocol to handle communications between the publisher and subscriber applications.
Applications run the <literal>cloud-event-proxy</literal> container in a sidecar pattern to subscribe to PTP events.
The <literal>cloud-event-proxy</literal> sidecar container can access the same resources as the primary application container without using any of the resources of the primary application and with no significant latency.</simpara>
<note>
<simpara>Use HTTP transport instead of AMQP for PTP and bare-metal events where possible.
AMQ Interconnect is EOL from 30 June 2024.
Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029.
For more information see, <link xlink:href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</link>.</simpara>
</note>
<figure>
<title>Overview of PTP fast events</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/319_OpenShift_PTP_bare-metal_OCP_nodes_0323_4.13.png"/>
</imageobject>
<textobject><phrase>Overview of PTP fast events</phrase></textobject>
</mediaobject>
</figure>
<variablelist>
<varlistentry>
<term><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-1.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> Event is generated on the cluster host</term>
<listitem>
<simpara><literal>linuxptp-daemon</literal> in the PTP Operator-managed pod runs as a Kubernetes <literal>DaemonSet</literal> and manages the various <literal>linuxptp</literal> processes (<literal>ptp4l</literal>, <literal>phc2sys</literal>, and optionally for grandmaster clocks, <literal>ts2phc</literal>).
The <literal>linuxptp-daemon</literal> passes the event to the UNIX domain socket.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-2.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> Event is passed to the cloud-event-proxy sidecar</term>
<listitem>
<simpara>The PTP plugin reads the event from the UNIX domain socket and passes it to the <literal>cloud-event-proxy</literal> sidecar in the PTP Operator-managed pod.
<literal>cloud-event-proxy</literal> delivers the event from the Kubernetes infrastructure to Cloud-Native Network Functions (CNFs) with low latency.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-3.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> Event is persisted</term>
<listitem>
<simpara>The <literal>cloud-event-proxy</literal> sidecar in the PTP Operator-managed pod processes the event and publishes the cloud-native event by using a REST API.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-4.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> Message is transported</term>
<listitem>
<simpara>The message transporter transports the event to the <literal>cloud-event-proxy</literal> sidecar in the application pod over HTTP or AMQP 1.0 QPID.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-5.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> Event is available from the REST API</term>
<listitem>
<simpara>The <literal>cloud-event-proxy</literal> sidecar in the Application pod processes the event and makes it available by using the REST API.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><inlinemediaobject>
<imageobject>
<imagedata fileref="images/darkcircle-6.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>20</phrase></textobject>
</inlinemediaobject> Consumer application requests a subscription and receives the subscribed event</term>
<listitem>
<simpara>The consumer application sends an API request to the <literal>cloud-event-proxy</literal> sidecar in the application pod to create a PTP events subscription.
The <literal>cloud-event-proxy</literal> sidecar creates an AMQ or HTTP messaging listener protocol for the resource specified in the subscription.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The <literal>cloud-event-proxy</literal> sidecar in the application pod receives the event from the PTP Operator-managed pod, unwraps the cloud events object to retrieve the data, and posts the event to the consumer application.
The consumer application listens to the address specified in the resource qualifier and receives and processes the PTP event.</simpara>
</section>
<section xml:id="cnf-configuring-the-ptp-fast-event-publisher_using-ptp-hardware-fast-events-framework">
<title>Configuring the PTP fast event notifications publisher</title>
<simpara>To start using PTP fast event notifications for a network interface in your cluster, you must enable the fast event publisher in the PTP Operator <literal>PtpOperatorConfig</literal> custom resource (CR) and configure <literal>ptpClockThreshold</literal> values in a <literal>PtpConfig</literal> CR that you create.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift Container Platform CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the default PTP Operator config to enable PTP fast events.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>ptp-operatorconfig.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ptp.openshift.io/v1
kind: PtpOperatorConfig
metadata:
  name: default
  namespace: openshift-ptp
spec:
  daemonNodeSelector:
    node-role.kubernetes.io/worker: ""
  ptpEventConfig:
    enableEventPublisher: true <co xml:id="CO43-1"/></programlisting>
<calloutlist>
<callout arearefs="CO43-1">
<para>Set <literal>enableEventPublisher</literal> to <literal>true</literal> to enable PTP fast event notifications.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<note>
<simpara>In OpenShift Container Platform 4.13 or later, you do not need to set the <literal>spec.ptpEventConfig.transportHost</literal> field in the <literal>PtpOperatorConfig</literal> resource when you use HTTP transport for PTP events.
Set <literal>transportHost</literal> only when you use AMQP transport for PTP events.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Update the <literal>PtpOperatorConfig</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ptp-operatorconfig.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>PtpConfig</literal> custom resource (CR) for the PTP enabled interface, and set the required values for <literal>ptpClockThreshold</literal> and <literal>ptp4lOpts</literal>.
The following YAML illustrates the required values that you must set in the <literal>PtpConfig</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  profile:
  - name: "profile1"
    interface: "enp5s0f0"
    ptp4lOpts: "-2 -s --summary_interval -4" <co xml:id="CO44-1"/>
    phc2sysOpts: "-a -r -m -n 24 -N 8 -R 16" <co xml:id="CO44-2"/>
    ptp4lConf: "" <co xml:id="CO44-3"/>
    ptpClockThreshold: <co xml:id="CO44-4"/>
      holdOverTimeout: 5
      maxOffsetThreshold: 100
      minOffsetThreshold: -100</programlisting>
<calloutlist>
<callout arearefs="CO44-1">
<para>Append <literal>--summary_interval -4</literal> to use PTP fast events.</para>
</callout>
<callout arearefs="CO44-2">
<para>Required <literal>phc2sysOpts</literal> values. <literal>-m</literal> prints messages to <literal>stdout</literal>. The <literal>linuxptp-daemon</literal> <literal>DaemonSet</literal> parses the logs and generates Prometheus metrics.</para>
</callout>
<callout arearefs="CO44-3">
<para>Specify a string that contains the configuration to replace the default <literal>/etc/ptp4l.conf</literal> file. To use the default configuration, leave the field empty.</para>
</callout>
<callout arearefs="CO44-4">
<para>Optional. If the <literal>ptpClockThreshold</literal> stanza is not present, default values are used for the <literal>ptpClockThreshold</literal> fields. The stanza shows default <literal>ptpClockThreshold</literal> values. The <literal>ptpClockThreshold</literal> values configure how long after the PTP master clock is disconnected before PTP events are triggered. <literal>holdOverTimeout</literal> is the time value in seconds before the PTP clock event state changes to <literal>FREERUN</literal> when the PTP master clock is disconnected. The <literal>maxOffsetThreshold</literal> and <literal>minOffsetThreshold</literal> settings configure offset values in nanoseconds that compare against the values for <literal>CLOCK_REALTIME</literal> (<literal>phc2sys</literal>) or master offset (<literal>ptp4l</literal>). When the <literal>ptp4l</literal> or <literal>phc2sys</literal> offset value is outside this range, the PTP clock state is set to <literal>FREERUN</literal>. When the offset value is within this range, the PTP clock state is set to <literal>LOCKED</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For a complete example CR that configures <literal>linuxptp</literal> services as an ordinary clock with PTP fast events, see <link linkend="configuring-linuxptp-services-as-ordinary-clock_configuring-ptp">Configuring linuxptp services as ordinary clock</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cnf-migrating-from-amqp-to-http-transport_using-ptp-hardware-fast-events-framework">
<title>Migrating consumer applications to use HTTP transport for PTP or bare-metal events</title>
<simpara>If you have previously deployed PTP or bare-metal events consumer applications, you need to update the applications to use HTTP message transport.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have updated the PTP Operator or Bare Metal Event Relay to version 4.13+ which uses HTTP transport by default.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Update your events consumer application to use HTTP transport.
Set the <literal>http-event-publishers</literal> variable for the cloud event sidecar deployment.</simpara>
<simpara>For example, in a cluster with PTP events configured, the following YAML snippet illustrates a cloud event sidecar deployment:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">containers:
  - name: cloud-event-sidecar
    image: cloud-event-sidecar
    args:
      - "--metrics-addr=127.0.0.1:9091"
      - "--store-path=/store"
      - "--transport-host=consumer-events-subscription-service.cloud-events.svc.cluster.local:9043"
      - "--http-event-publishers=ptp-event-publisher-service-NODE_NAME.openshift-ptp.svc.cluster.local:9043" <co xml:id="CO45-1"/>
      - "--api-port=8089"</programlisting>
<calloutlist>
<callout arearefs="CO45-1">
<para>The PTP Operator automatically resolves <literal>NODE_NAME</literal> to the host that is generating the PTP events.
For example, <literal>compute-1.example.com</literal>.</para>
</callout>
</calloutlist>
<simpara>In a cluster with bare-metal events configured, set the <literal>http-event-publishers</literal> field to <literal>hw-event-publisher-service.openshift-bare-metal-events.svc.cluster.local:9043</literal> in the cloud event sidecar deployment CR.</simpara>
</listitem>
<listitem>
<simpara>Deploy the <literal>consumer-events-subscription-service</literal> service alongside the events consumer application.
For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
    service.alpha.openshift.io/serving-cert-secret-name: sidecar-consumer-secret
  name: consumer-events-subscription-service
  namespace: cloud-events
  labels:
    app: consumer-service
spec:
  ports:
    - name: sub-port
      port: 9043
  selector:
    app: consumer
  clusterIP: None
  sessionAffinity: None
  type: ClusterIP</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cnf-installing-amq-interconnect-messaging-bus_using-ptp-hardware-fast-events-framework">
<title>Installing the AMQ messaging bus</title>
<simpara>To pass PTP fast event notifications between publisher and subscriber on a node, you can install and configure an AMQ messaging bus to run locally on the node.
To use AMQ messaging, you must install the AMQ Interconnect Operator.</simpara>
<note>
<simpara>Use HTTP transport instead of AMQP for PTP and bare-metal events where possible.
AMQ Interconnect is EOL from 30 June 2024.
Extended life cycle support (ELS) for AMQ Interconnect ends 29 November 2029.
For more information see, <link xlink:href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</link>.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Install the AMQ Interconnect Operator to its own <literal>amq-interconnect</literal> namespace. See <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_amq/2021.q1/html/deploying_amq_interconnect_on_openshift/adding-operator-router-ocp">Adding the Red Hat Integration - AMQ Interconnect Operator</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check that the AMQ Interconnect Operator is available and the required pods are running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n amq-interconnect</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                    READY   STATUS    RESTARTS   AGE
amq-interconnect-645db76c76-k8ghs       1/1     Running   0          23h
interconnect-operator-5cb5fc7cc-4v7qm   1/1     Running   0          23h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the required <literal>linuxptp-daemon</literal> PTP event producer pods are running in the <literal>openshift-ptp</literal> namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                     READY   STATUS    RESTARTS       AGE
linuxptp-daemon-2t78p    3/3     Running   0              12h
linuxptp-daemon-k8n88    3/3     Running   0              12h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="subscribing-du-applications-to-ptp-events-rest-api-reference_using-ptp-hardware-fast-events-framework">
<title>Subscribing DU applications to PTP events with the REST API</title>
<simpara>Subscribe applications to PTP events by using the resource address <literal>/cluster/node/&lt;node_name&gt;/ptp</literal>, where <literal>&lt;node_name&gt;</literal> is the cluster node running the DU application.</simpara>
<simpara>Deploy your <literal>cloud-event-consumer</literal> DU application container and <literal>cloud-event-proxy</literal> sidecar container in a separate DU application pod. The <literal>cloud-event-consumer</literal> DU application subscribes to the <literal>cloud-event-proxy</literal> container in the application pod.</simpara>
<simpara>Use the following API endpoints to subscribe the <literal>cloud-event-consumer</literal> DU application to PTP events posted by the <literal>cloud-event-proxy</literal> container at <literal>http://localhost:8089/api/ocloudNotifications/v1/</literal> in the DU application pod:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../networking/ptp/using-ptp-events.xml#api-ocloud-notifications-v1-subscriptions_using-ptp-hardware-fast-events-framework"><literal>/api/ocloudNotifications/v1/subscriptions</literal></link></simpara>
<itemizedlist>
<listitem>
<simpara><literal>POST</literal>: Creates a new subscription</simpara>
</listitem>
<listitem>
<simpara><literal>GET</literal>: Retrieves a list of subscriptions</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../../networking/ptp/using-ptp-events.xml#api-ocloud-notifications-v1-subscriptions-subscription_id_using-ptp-hardware-fast-events-framework"><literal>/api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;</literal></link></simpara>
<itemizedlist>
<listitem>
<simpara><literal>GET</literal>: Returns details for the specified subscription ID</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../../networking/ptp/using-ptp-events.xml#api-ocloudnotifications-v1-health_using-ptp-hardware-fast-events-framework"><literal>/api/ocloudNotifications/v1/health</literal></link></simpara>
<itemizedlist>
<listitem>
<simpara><literal>GET</literal>: Returns the health status of <literal>ocloudNotifications</literal> API</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../../networking/ptp/using-ptp-events.xml#api-ocloudnotifications-v1-publishers_using-ptp-hardware-fast-events-framework"><literal>api/ocloudNotifications/v1/publishers</literal></link></simpara>
<itemizedlist>
<listitem>
<simpara><literal>GET</literal>: Returns an array of <literal>os-clock-sync-state</literal>, <literal>ptp-clock-class-change</literal>, <literal>lock-state</literal>, and <literal>gnss-sync-status</literal> messages for the cluster node</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../../networking/ptp/using-ptp-events.xml#resource-address-current-state_using-ptp-hardware-fast-events-framework"><literal>/api/ocloudnotifications/v1/&lt;resource_address&gt;/CurrentState</literal></link></simpara>
<itemizedlist>
<listitem>
<simpara><literal>GET</literal>: Returns the current state of one the following event types: <literal>os-clock-sync-state</literal>, <literal>ptp-clock-class-change</literal>, <literal>lock-state</literal>, or <literal>gnss-state-change</literal> events</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara><literal>9089</literal> is the default port for the <literal>cloud-event-consumer</literal> container deployed in the application pod. You can configure a different port for your DU application as required.</simpara>
</note>
<section xml:id="cnf-fast-event-notifications-api-refererence_using-ptp-hardware-fast-events-framework">
<title>PTP events REST API reference</title>
<simpara>Use the PTP event notifications REST API to subscribe a cluster application to the PTP events that are generated on the parent node.</simpara>
<section xml:id="api-ocloud-notifications-v1-subscriptions_using-ptp-hardware-fast-events-framework">
<title>api/ocloudNotifications/v1/subscriptions</title>
<bridgehead xml:id="_http-method" renderas="sect6">HTTP method</bridgehead>
<simpara><literal>GET api/ocloudNotifications/v1/subscriptions</literal></simpara>
<bridgehead xml:id="_description" renderas="sect6">Description</bridgehead>
<simpara>Returns a list of subscriptions. If subscriptions exist, a <literal>200 OK</literal> status code is returned along with the list of subscriptions.</simpara>
<formalpara>
<title>Example API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">[
 {
  "id": "75b1ad8f-c807-4c23-acf5-56f4b7ee3826",
  "endpointUri": "http://localhost:9089/event",
  "uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/75b1ad8f-c807-4c23-acf5-56f4b7ee3826",
  "resource": "/cluster/node/compute-1.example.com/ptp"
 }
]</programlisting>
</para>
</formalpara>
<bridgehead xml:id="_http-method-2" renderas="sect6">HTTP method</bridgehead>
<simpara><literal>POST api/ocloudNotifications/v1/subscriptions</literal></simpara>
<bridgehead xml:id="_description-2" renderas="sect6">Description</bridgehead>
<simpara>Creates a new subscription. If a subscription is successfully created, or if it already exists, a <literal>201 Created</literal> status code is returned.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Query parameters</title>
<?dbhtml table-width="60%"?>
<?dbfo table-width="60%"?>
<?dblatex table-width="60%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="127.5*"/>
<colspec colname="col_2" colwidth="127.5*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Type</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>subscription</simpara></entry>
<entry align="left" valign="top"><simpara>data</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example payload</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions",
  "resource": "/cluster/node/compute-1.example.com/ptp"
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="api-ocloud-notifications-v1-subscriptions-subscription_id_using-ptp-hardware-fast-events-framework">
<title>api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;</title>
<bridgehead xml:id="_http-method-3" renderas="sect6">HTTP method</bridgehead>
<simpara><literal>GET api/ocloudNotifications/v1/subscriptions/&lt;subscription_id&gt;</literal></simpara>
<bridgehead xml:id="_description-3" renderas="sect6">Description</bridgehead>
<simpara>Returns details for the subscription with ID <literal>&lt;subscription_id&gt;</literal></simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Query parameters</title>
<?dbhtml table-width="60%"?>
<?dbfo table-width="60%"?>
<?dblatex table-width="60%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="127.5*"/>
<colspec colname="col_2" colwidth="127.5*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Type</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>&lt;subscription_id&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "id":"48210fb3-45be-4ce0-aa9b-41a0e58730ab",
  "endpointUri": "http://localhost:9089/event",
  "uriLocation":"http://localhost:8089/api/ocloudNotifications/v1/subscriptions/48210fb3-45be-4ce0-aa9b-41a0e58730ab",
  "resource":"/cluster/node/compute-1.example.com/ptp"
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="api-ocloudnotifications-v1-health_using-ptp-hardware-fast-events-framework">
<title>api/ocloudNotifications/v1/health</title>
<bridgehead xml:id="_http-method-4" renderas="sect6">HTTP method</bridgehead>
<simpara><literal>GET api/ocloudNotifications/v1/health/</literal></simpara>
<bridgehead xml:id="_description-4" renderas="sect6">Description</bridgehead>
<simpara>Returns the health status for the <literal>ocloudNotifications</literal> REST API.</simpara>
<formalpara>
<title>Example API response</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">OK</programlisting>
</para>
</formalpara>
</section>
<section xml:id="api-ocloudnotifications-v1-publishers_using-ptp-hardware-fast-events-framework">
<title>api/ocloudNotifications/v1/publishers</title>
<bridgehead xml:id="_http-method-5" renderas="sect6">HTTP method</bridgehead>
<simpara><literal>GET api/ocloudNotifications/v1/publishers</literal></simpara>
<bridgehead xml:id="_description-5" renderas="sect6">Description</bridgehead>
<simpara>Returns an array of <literal>os-clock-sync-state</literal>, <literal>ptp-clock-class-change</literal>, <literal>lock-state</literal>, and <literal>gnss-sync-status</literal> details for the cluster node.
The system generates notifications when the relevant equipment state changes.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>os-clock-sync-state</literal> notifications describe the host operating system clock synchronization state. Can be in <literal>LOCKED</literal> or <literal>FREERUN</literal> state.</simpara>
</listitem>
<listitem>
<simpara><literal>ptp-clock-class-change</literal> notifications describe the current state of the PTP clock class.</simpara>
</listitem>
<listitem>
<simpara><literal>lock-state</literal> notifications describe the current status of the PTP equipment lock state. Can be in <literal>LOCKED</literal>, <literal>HOLDOVER</literal> or <literal>FREERUN</literal> state.</simpara>
</listitem>
<listitem>
<simpara><literal>gnss-sync-status</literal> notifications describe the GPS synchronization state with regard to the external GNSS clock signal. Can be in <literal>LOCKED</literal> or <literal>FREERUN</literal> state.</simpara>
</listitem>
</itemizedlist>
<simpara>You can use equipment synchronization status subscriptions together to deliver a detailed view of the overall synchronization health of the system.</simpara>
<formalpara>
<title>Example API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">[
  {
    "id": "0fa415ae-a3cf-4299-876a-589438bacf75",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/0fa415ae-a3cf-4299-876a-589438bacf75",
    "resource": "/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state"
  },
  {
    "id": "28cd82df-8436-4f50-bbd9-7a9742828a71",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/28cd82df-8436-4f50-bbd9-7a9742828a71",
    "resource": "/cluster/node/compute-1.example.com/sync/ptp-status/ptp-clock-class-change"
  },
  {
    "id": "44aa480d-7347-48b0-a5b0-e0af01fa9677",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/44aa480d-7347-48b0-a5b0-e0af01fa9677",
    "resource": "/cluster/node/compute-1.example.com/sync/ptp-status/lock-state"
  },
  {
    "id": "778da345d-4567-67b0-a43f0-rty885a456",
    "endpointUri": "http://localhost:9085/api/ocloudNotifications/v1/dummy",
    "uriLocation": "http://localhost:9085/api/ocloudNotifications/v1/publishers/778da345d-4567-67b0-a43f0-rty885a456",
    "resource": "/cluster/node/compute-1.example.com/sync/gnss-status/gnss-sync-status"
  }
]</programlisting>
</para>
</formalpara>
<simpara>You can find <literal>os-clock-sync-state</literal>, <literal>ptp-clock-class-change</literal>, <literal>lock-state</literal>, and <literal>gnss-sync-status</literal> events in the logs for the <literal>cloud-event-proxy</literal> container. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -f linuxptp-daemon-cvgr6 -n openshift-ptp -c cloud-event-proxy</programlisting>
<formalpara>
<title>Example os-clock-sync-state event</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
   "id":"c8a784d1-5f4a-4c16-9a81-a3b4313affe5",
   "type":"event.sync.sync-status.os-clock-sync-state-change",
   "source":"/cluster/compute-1.example.com/ptp/CLOCK_REALTIME",
   "dataContentType":"application/json",
   "time":"2022-05-06T15:31:23.906277159Z",
   "data":{
      "version":"v1",
      "values":[
         {
            "resource":"/sync/sync-status/os-clock-sync-state",
            "dataType":"notification",
            "valueType":"enumeration",
            "value":"LOCKED"
         },
         {
            "resource":"/sync/sync-status/os-clock-sync-state",
            "dataType":"metric",
            "valueType":"decimal64.3",
            "value":"-53"
         }
      ]
   }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example ptp-clock-class-change event</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
   "id":"69eddb52-1650-4e56-b325-86d44688d02b",
   "type":"event.sync.ptp-status.ptp-clock-class-change",
   "source":"/cluster/compute-1.example.com/ptp/ens2fx/master",
   "dataContentType":"application/json",
   "time":"2022-05-06T15:31:23.147100033Z",
   "data":{
      "version":"v1",
      "values":[
         {
            "resource":"/sync/ptp-status/ptp-clock-class-change",
            "dataType":"metric",
            "valueType":"decimal64.3",
            "value":"135"
         }
      ]
   }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example lock-state event</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
   "id":"305ec18b-1472-47b3-aadd-8f37933249a9",
   "type":"event.sync.ptp-status.ptp-state-change",
   "source":"/cluster/compute-1.example.com/ptp/ens2fx/master",
   "dataContentType":"application/json",
   "time":"2022-05-06T15:31:23.467684081Z",
   "data":{
      "version":"v1",
      "values":[
         {
            "resource":"/sync/ptp-status/lock-state",
            "dataType":"notification",
            "valueType":"enumeration",
            "value":"LOCKED"
         },
         {
            "resource":"/sync/ptp-status/lock-state",
            "dataType":"metric",
            "valueType":"decimal64.3",
            "value":"62"
         }
      ]
   }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example gnss-sync-status event</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "id": "435e1f2a-6854-4555-8520-767325c087d7",
  "type": "event.sync.gnss-status.gnss-state-change",
  "source": "/cluster/node/compute-1.example.com/sync/gnss-status/gnss-sync-status",
  "dataContentType": "application/json",
  "time": "2023-09-27T19:35:33.42347206Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens2fx/master",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/ens2fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "5"
      }
    ]
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="resource-address-current-state_using-ptp-hardware-fast-events-framework">
<title>api/ocloudNotifications/v1/&lt;resource_address&gt;/CurrentState</title>
<bridgehead xml:id="_http-method-6" renderas="sect6">HTTP method</bridgehead>
<simpara><literal>GET api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state/CurrentState</literal></simpara>
<simpara><literal>GET api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state/CurrentState</literal></simpara>
<simpara><literal>GET api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change/CurrentState</literal></simpara>
<bridgehead xml:id="_description-6" renderas="sect6">Description</bridgehead>
<simpara>Configure the <literal>CurrentState</literal> API endpoint to return the current state of the <literal>os-clock-sync-state</literal>, <literal>ptp-clock-class-change</literal>, <literal>lock-state</literal> events for the cluster node.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>os-clock-sync-state</literal> notifications describe the host operating system clock synchronization state. Can be in <literal>LOCKED</literal> or <literal>FREERUN</literal> state.</simpara>
</listitem>
<listitem>
<simpara><literal>ptp-clock-class-change</literal> notifications describe the current state of the PTP clock class.</simpara>
</listitem>
<listitem>
<simpara><literal>lock-state</literal> notifications describe the current status of the PTP equipment lock state. Can be in <literal>LOCKED</literal>, <literal>HOLDOVER</literal> or <literal>FREERUN</literal> state.</simpara>
</listitem>
</itemizedlist>
<table frame="all" rowsep="1" colsep="1">
<title>Query parameters</title>
<?dbhtml table-width="60%"?>
<?dbfo table-width="60%"?>
<?dblatex table-width="60%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="127.5*"/>
<colspec colname="col_2" colwidth="127.5*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Type</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>&lt;resource_address&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example lock-state API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "id": "c1ac3aa5-1195-4786-84f8-da0ea4462921",
  "type": "event.sync.ptp-status.ptp-state-change",
  "source": "/cluster/node/compute-1.example.com/sync/ptp-status/lock-state",
  "dataContentType": "application/json",
  "time": "2023-01-10T02:41:57.094981478Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "29"
      }
    ]
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example os-clock-sync-state API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "specversion": "0.3",
  "id": "4f51fe99-feaa-4e66-9112-66c5c9b9afcb",
  "source": "/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state",
  "type": "event.sync.sync-status.os-clock-sync-state-change",
  "subject": "/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state",
  "datacontenttype": "application/json",
  "time": "2022-11-29T17:44:22.202Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/CLOCK_REALTIME",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/CLOCK_REALTIME",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "27"
      }
    ]
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example ptp-clock-class-change API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "id": "064c9e67-5ad4-4afb-98ff-189c6aa9c205",
  "type": "event.sync.ptp-status.ptp-clock-class-change",
  "source": "/cluster/node/compute-1.example.com/sync/ptp-status/ptp-clock-class-change",
  "dataContentType": "application/json",
  "time": "2023-01-10T02:41:56.785673989Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "165"
      }
    ]
  }
}</programlisting>
</para>
</formalpara>
</section>
</section>
</section>
<section xml:id="cnf-monitoring-fast-events-metrics_using-ptp-hardware-fast-events-framework">
<title>Monitoring PTP fast event metrics</title>
<simpara>You can monitor PTP fast events metrics from cluster nodes where the <literal>linuxptp-daemon</literal> is running.
You can also monitor PTP fast event metrics in the OpenShift Container Platform web console by using the preconfigured and self-updating Prometheus monitoring stack.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI <literal>oc</literal>.</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install and configure the PTP Operator on a node with PTP-capable hardware.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start a debug pod for the node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check for PTP metrics exposed by the <literal>linuxptp-daemon</literal> container. For example, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# curl http://localhost:9091/metrics</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen># HELP cne_api_events_published Metric to get number of events published by the rest api
# TYPE cne_api_events_published gauge
cne_api_events_published{address="/cluster/node/compute-1.example.com/sync/gnss-status/gnss-sync-status",status="success"} 1
cne_api_events_published{address="/cluster/node/compute-1.example.com/sync/ptp-status/lock-state",status="success"} 94
cne_api_events_published{address="/cluster/node/compute-1.example.com/sync/ptp-status/ptp-clock-class-change",status="success"} 18
cne_api_events_published{address="/cluster/node/compute-1.example.com/sync/sync-status/os-clock-sync-state",status="success"} 27</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To view the PTP event in the OpenShift Container Platform web console, copy the name of the PTP metric you want to query, for example, <literal>openshift_ptp_offset_ns</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Paste the PTP metric name into the <emphasis role="strong">Expression</emphasis> field, and click <emphasis role="strong">Run queries</emphasis>.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#managing-metrics">Managing metrics</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ptp-operator-metrics-reference_using-ptp-hardware-fast-events-framework">
<title>PTP fast event metrics reference</title>
<simpara>The following table describes the PTP fast events metrics that are available from cluster nodes where the <literal>linuxptp-daemon</literal> service is running.</simpara>
<note>
<simpara>Some of the following metrics are applicable for PTP grandmaster clocks (T-GM) only.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title>PTP fast event metrics</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="12.5*"/>
<colspec colname="col_2" colwidth="50*"/>
<colspec colname="col_3" colwidth="37.5*"/>
<thead>
<row>
<entry align="left" valign="top">Metric</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Example</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_clock_class</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the PTP clock class for the interface.
Possible values for PTP clock class are 6 (<literal>LOCKED</literal>), 7 (<literal>PRC UNLOCKED IN-SPEC</literal>), 52 (<literal>PRC UNLOCKED OUT-OF-SPEC</literal>), 187 (<literal>PRC UNLOCKED OUT-OF-SPEC</literal>), 135 (<literal>T-BC HOLDOVER IN-SPEC</literal>), 165 (<literal>T-BC HOLDOVER OUT-OF-SPEC</literal>), 248 (<literal>DEFAULT</literal>), or 255 (<literal>SLAVE ONLY CLOCK</literal>).
Applicable to T-GM clocks only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{node="compute-1.example.com",process="ptp4l"} 6</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_clock_state</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the current PTP clock state for the interface.
Possible values for PTP clock state are <literal>FREERUN</literal>, <literal>LOCKED</literal>, or <literal>HOLDOVER</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{iface="CLOCK_REALTIME", node="compute-1.example.com", process="phc2sys"} 1</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_delay_ns</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the delay in nanoseconds between the primary clock sending the timing packet and the secondary clock receiving the timing packet.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="master", iface="ens2fx", node="compute-1.example.com", process="ts2phc"} 0</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_frequency_adjustment_ns</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the frequency adjustment in nanoseconds between 2 PTP clocks.
For example, between the upstream clock and the NIC, between the system clock and the NIC, or between the PTP hardware clock (<literal>phc</literal>) and the NIC.
Applicable to T-GM clocks only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="phc", iface="CLOCK_REALTIME", node="compute-1.example.com", process="phc2sys"} -6768</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_frequency_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the current status of the digital phase-locked loop (DPLL) frequency for the NIC.
Possible values are -1 (<literal>UNKNOWN</literal>), 0 (<literal>INVALID</literal>), 1 (<literal>FREERUN</literal>), 2 (<literal>LOCKED</literal>), 3 (<literal>LOCKED_HO_ACQ</literal>), or 4 (<literal>HOLDOVER</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="dpll",iface="ens2fx",node="compute-1.example.com",process="dpll"} 3</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_phase_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the status of the DPLL phase for the NIC.
Possible values are -1 (<literal>UNKNOWN</literal>), 0 (<literal>INVALID</literal>), 1 (<literal>FREERUN</literal>), 2 (<literal>LOCKED</literal>), 3 (<literal>LOCKED_HO_ACQ</literal>), or 4 (<literal>HOLDOVER</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="dpll",iface="ens2fx",node="compute-1.example.com",process="dpll"} 3</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_interface_role</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the configured PTP clock role for the interface.
Possible values are 0 (<literal>PASSIVE</literal>), 1 (<literal>SLAVE</literal>), 2 (<literal>MASTER</literal>), 3 (<literal>FAULTY</literal>), 4 (<literal>UNKNOWN</literal>), or 5 (<literal>LISTENING</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{iface="ens2f0", node="compute-1.example.com", process="ptp4l"} 2</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_max_offset_ns</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the maximum offset in nanoseconds between 2 clocks or interfaces.
For example, between the upstream GNSS clock and the NIC (<literal>ts2phc</literal>), or between the PTP hardware clock (<literal>phc</literal>) and the system clock (<literal>phc2sys</literal>).
Applicable to T-GM clocks only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="master", iface="ens2fx", node="compute-1.example.com", process="ts2phc"} 1.038099569e+09</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_offset_ns</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the offset in nanoseconds between the DPLL clock or the GNSS clock source and the NIC hardware clock.
Applicable to T-GM clocks only.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="phc", iface="CLOCK_REALTIME", node="compute-1.example.com", process="phc2sys"} -9</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_process_restart_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns a count of the number of times the <literal>ptp4l</literal> process was restarted.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{config="ptp4l.0.config", node="compute-1.example.com",process="phc2sys"} 1</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_process_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns a status code that shows whether the PTP process is running or not.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{config="ptp4l.0.config", node="compute-1.example.com",process="phc2sys"} 1</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_threshold</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns values for <literal>HoldOverTimeout</literal>, <literal>MaxOffsetThreshold</literal>, and <literal>MinOffsetThreshold</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>holdOverTimeout</literal> is the time value in seconds before the PTP clock event state changes to <literal>FREERUN</literal> when the PTP master clock is disconnected.</simpara>
</listitem>
<listitem>
<simpara><literal>maxOffsetThreshold</literal> and <literal>minOffsetThreshold</literal> are offset values in nanoseconds that compare against the values for <literal>CLOCK_REALTIME</literal> (<literal>phc2sys</literal>) or master offset (<literal>ptp4l</literal>) values that you configure in the <literal>PtpConfig</literal> CR for the NIC.</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara><literal>{node="compute-1.example.com", profile="grandmaster", threshold="HoldOverTimeout"} 5</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_pps_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the current status of the NIC 1PPS connection.
You use the 1PPS connection to synchronize timing between connected NICs.
Possible values are 0 (<literal>UNAVAILABLE</literal>) and 1 (<literal>AVAILABLE</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="dpll",iface="ens2fx",node="compute-1.example.com",process="dpll"} 1</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_nmea_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the current status of the NMEA connection.
NMEA is the protocol that is used for 1PPS NIC connections.
Possible values are 0 (<literal>UNAVAILABLE</literal>) and 1 (<literal>AVAILABLE</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{iface="ens2fx",node="compute-1.example.com",process="ts2phc"} 1</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_ptp_gnss_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Returns the current status of the global navigation satellite system (GNSS) connection.
GNSS provides satellite-based positioning, navigation, and timing services globally.
Possible values are 0 (<literal>NOFIX</literal>), 1 (<literal>DEAD RECKONING ONLY</literal>), 2 (<literal>2D-FIX</literal>), 3 (<literal>3D-FIX</literal>), 4 (<literal>GPS+DEAD RECKONING FIX</literal>), 5, (<literal>TIME ONLY FIX</literal>).</simpara></entry>
<entry align="left" valign="top"><simpara><literal>{from="gnss",iface="ens2fx",node="compute-1.example.com",process="gnss"} 3</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="ptp-cloud-events-consumer-dev-reference">
<title>Developing PTP events consumer applications</title>

<simpara>When developing consumer applications that make use of Precision Time Protocol (PTP) events on a bare-metal cluster node, you need to deploy your consumer application and a <literal>cloud-event-proxy</literal> container in a separate application pod.
The <literal>cloud-event-proxy</literal> container receives the events from the PTP Operator pod and passes it to the consumer application.
The consumer application subscribes to the events posted in the <literal>cloud-event-proxy</literal> container by using a REST API.</simpara>
<simpara>For more information about deploying PTP events applications, see <link linkend="cnf-about-ptp-fast-event-notifications-framework_using-ptp-hardware-fast-events-framework">About the PTP fast event notifications framework</link>.</simpara>
<note>
<simpara>The following information provides general guidance for developing consumer applications that use PTP events.
A complete events consumer application example is outside the scope of this information.</simpara>
</note>
<section xml:id="ptp-events-consumer-application_ptp-consumer">
<title>PTP events consumer application reference</title>
<simpara>PTP event consumer applications require the following features:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>A web service running with a <literal>POST</literal> handler to receive the cloud native PTP events JSON payload</simpara>
</listitem>
<listitem>
<simpara>A <literal>createSubscription</literal> function to subscribe to the PTP events producer</simpara>
</listitem>
<listitem>
<simpara>A <literal>getCurrentState</literal> function to poll the current state of the PTP events producer</simpara>
</listitem>
</orderedlist>
<simpara>The following example Go snippets illustrate these requirements:</simpara>
<formalpara>
<title>Example PTP events consumer server function in Go</title>
<para>
<programlisting language="go" linenumbering="unnumbered">func server() {
  http.HandleFunc("/event", getEvent)
  http.ListenAndServe("localhost:8989", nil)
}

func getEvent(w http.ResponseWriter, req *http.Request) {
  defer req.Body.Close()
  bodyBytes, err := io.ReadAll(req.Body)
  if err != nil {
    log.Errorf("error reading event %v", err)
  }
  e := string(bodyBytes)
  if e != "" {
    processEvent(bodyBytes)
    log.Infof("received event %s", string(bodyBytes))
  } else {
    w.WriteHeader(http.StatusNoContent)
  }
}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example PTP events createSubscription function in Go</title>
<para>
<programlisting language="go" linenumbering="unnumbered">import (
"github.com/redhat-cne/sdk-go/pkg/pubsub"
"github.com/redhat-cne/sdk-go/pkg/types"
v1pubsub "github.com/redhat-cne/sdk-go/v1/pubsub"
)

// Subscribe to PTP events using REST API
s1,_:=createsubscription("/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state") <co xml:id="CO46-1"/>
s2,_:=createsubscription("/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change")
s3,_:=createsubscription("/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state")

// Create PTP event subscriptions POST
func createSubscription(resourceAddress string) (sub pubsub.PubSub, err error) {
  var status int
      apiPath:= "/api/ocloudNotifications/v1/"
      localAPIAddr:=localhost:8989 // vDU service API address
      apiAddr:= "localhost:8089" // event framework API address

  subURL := &amp;types.URI{URL: url.URL{Scheme: "http",
    Host: apiAddr
    Path: fmt.Sprintf("%s%s", apiPath, "subscriptions")}}
  endpointURL := &amp;types.URI{URL: url.URL{Scheme: "http",
    Host: localAPIAddr,
    Path: "event"}}

  sub = v1pubsub.NewPubSub(endpointURL, resourceAddress)
  var subB []byte

  if subB, err = json.Marshal(&amp;sub); err == nil {
    rc := restclient.New()
    if status, subB = rc.PostWithReturn(subURL, subB); status != http.StatusCreated {
      err = fmt.Errorf("error in subscription creation api at %s, returned status %d", subURL, status)
    } else {
      err = json.Unmarshal(subB, &amp;sub)
    }
  } else {
    err = fmt.Errorf("failed to marshal subscription for %s", resourceAddress)
  }
  return
}</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO46-1">
<para>Replace <literal>&lt;node_name&gt;</literal> with the FQDN of the node that is generating the PTP events. For example, <literal>compute-1.example.com</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example PTP events consumer getCurrentState function in Go</title>
<para>
<programlisting language="go" linenumbering="unnumbered">//Get PTP event state for the resource
func getCurrentState(resource string) {
  //Create publisher
  url := &amp;types.URI{URL: url.URL{Scheme: "http",
    Host: localhost:8989,
    Path: fmt.SPrintf("/api/ocloudNotifications/v1/%s/CurrentState",resource}}
  rc := restclient.New()
  status, event := rc.Get(url)
  if status != http.StatusOK {
    log.Errorf("CurrentState:error %d from url %s, %s", status, url.String(), event)
  } else {
    log.Debugf("Got CurrentState: %s ", event)
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="ptp-reference-deployment-and-service-crs_ptp-consumer">
<title>Reference cloud-event-proxy deployment and service CRs</title>
<simpara>Use the following example <literal>cloud-event-proxy</literal> deployment and subscriber service CRs as a reference when deploying your PTP events consumer application.</simpara>
<note>
<simpara>Use HTTP transport instead of AMQP for PTP and bare-metal events where possible.
AMQ Interconnect is EOL from 30 June 2024.
Extended life cycle support (ELS) for AMQ Interconnect ends 30 November 2030.
For more information see, <link xlink:href="https://access.redhat.com/support/policy/updates/jboss_notes#p_Interconnect">Red Hat AMQ Interconnect support status</link>.</simpara>
</note>
<formalpara>
<title>Reference cloud-event-proxy deployment with HTTP transport</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: event-consumer-deployment
  namespace: &lt;namespace&gt;
  labels:
    app: consumer
spec:
  replicas: 1
  selector:
    matchLabels:
      app: consumer
  template:
    metadata:
      labels:
        app: consumer
    spec:
      serviceAccountName: sidecar-consumer-sa
      containers:
        - name: event-subscriber
          image: event-subscriber-app
        - name: cloud-event-proxy-as-sidecar
          image: openshift4/ose-cloud-event-proxy
          args:
            - "--metrics-addr=127.0.0.1:9091"
            - "--store-path=/store"
            - "--transport-host=consumer-events-subscription-service.cloud-events.svc.cluster.local:9043"
            - "--http-event-publishers=ptp-event-publisher-service-NODE_NAME.openshift-ptp.svc.cluster.local:9043"
            - "--api-port=8089"
          env:
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: NODE_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
              volumeMounts:
                - name: pubsubstore
                  mountPath: /store
          ports:
            - name: metrics-port
              containerPort: 9091
            - name: sub-port
              containerPort: 9043
          volumes:
            - name: pubsubstore
              emptyDir: {}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Reference cloud-event-proxy deployment with AMQ transport</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: cloud-event-proxy-sidecar
  namespace: cloud-events
  labels:
    app: cloud-event-proxy
spec:
  selector:
    matchLabels:
      app: cloud-event-proxy
  template:
    metadata:
      labels:
        app: cloud-event-proxy
    spec:
      nodeSelector:
        node-role.kubernetes.io/worker: ""
      containers:
        - name: cloud-event-sidecar
          image: openshift4/ose-cloud-event-proxy
          args:
            - "--metrics-addr=127.0.0.1:9091"
            - "--store-path=/store"
            - "--transport-host=amqp://router.router.svc.cluster.local"
            - "--api-port=8089"
          env:
            - name: &lt;node_name&gt;
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            - name: &lt;node_ip&gt;
              valueFrom:
                fieldRef:
                  fieldPath: status.hostIP
          volumeMounts:
            - name: pubsubstore
              mountPath: /store
          ports:
            - name: metrics-port
              containerPort: 9091
            - name: sub-port
              containerPort: 9043
          volumes:
            - name: pubsubstore
              emptyDir: {}</programlisting>
</para>
</formalpara>
<formalpara>
<title>Reference cloud-event-proxy subscriber service</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  annotations:
    prometheus.io/scrape: "true"
    service.alpha.openshift.io/serving-cert-secret-name: sidecar-consumer-secret
  name: consumer-events-subscription-service
  namespace: cloud-events
  labels:
    app: consumer-service
spec:
  ports:
    - name: sub-port
      port: 9043
  selector:
    app: consumer
  clusterIP: None
  sessionAffinity: None
  type: ClusterIP</programlisting>
</para>
</formalpara>
</section>
<section xml:id="ptp-cloud-event-proxy-sidecar-api_ptp-consumer">
<title>PTP events available from the cloud-event-proxy sidecar REST API</title>
<simpara>PTP events consumer applications can poll the PTP events producer for the following PTP timing events.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>PTP events available from the cloud-event-proxy sidecar</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Resource URI</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Describes the current status of the PTP equipment lock state. Can be in <literal>LOCKED</literal>, <literal>HOLDOVER</literal>, or <literal>FREERUN</literal> state.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Describes the host operating system clock synchronization state. Can be in <literal>LOCKED</literal> or <literal>FREERUN</literal> state.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Describes the current state of the PTP clock class.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="ptp-subscribing-consumer-app-to-events_ptp-consumer">
<title>Subscribing the consumer application to PTP events</title>
<simpara>Before the PTP events consumer application can poll for events, you need to subscribe the application to the event producer.</simpara>
<section xml:id="ptp-sub-lock-state-events_ptp-consumer">
<title>Subscribing to PTP lock-state events</title>
<simpara>To create a subscription for PTP <literal>lock-state</literal> events, send a <literal>POST</literal> action to the cloud event API at <literal>http://localhost:8081/api/ocloudNotifications/v1/subscriptions</literal> with the following payload:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
"endpointUri": "http://localhost:8989/event",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state",
}</programlisting>
<formalpara>
<title>Example response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
"id": "e23473d9-ba18-4f78-946e-401a0caeff90",
"endpointUri": "http://localhost:8989/event",
"uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/e23473d9-ba18-4f78-946e-401a0caeff90",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state",
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="ptp-sub-os-clock-sync-state_ptp-consumer">
<title>Subscribing to PTP os-clock-sync-state events</title>
<simpara>To create a subscription for PTP <literal>os-clock-sync-state</literal> events, send a <literal>POST</literal> action to the cloud event API at <literal>http://localhost:8081/api/ocloudNotifications/v1/subscriptions</literal> with the following payload:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
"endpointUri": "http://localhost:8989/event",
"resource": "/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state",
}</programlisting>
<formalpara>
<title>Example response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
"id": "e23473d9-ba18-4f78-946e-401a0caeff90",
"endpointUri": "http://localhost:8989/event",
"uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/e23473d9-ba18-4f78-946e-401a0caeff90",
"resource": "/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state",
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="ptp-sub-ptp-clock-class-change_ptp-consumer">
<title>Subscribing to PTP ptp-clock-class-change events</title>
<simpara>To create a subscription for PTP <literal>ptp-clock-class-change</literal> events, send a <literal>POST</literal> action to the cloud event API at <literal>http://localhost:8081/api/ocloudNotifications/v1/subscriptions</literal> with the following payload:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
"endpointUri": "http://localhost:8989/event",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change",
}</programlisting>
<formalpara>
<title>Example response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
"id": "e23473d9-ba18-4f78-946e-401a0caeff90",
"endpointUri": "http://localhost:8989/event",
"uriLocation": "http://localhost:8089/api/ocloudNotifications/v1/subscriptions/e23473d9-ba18-4f78-946e-401a0caeff90",
"resource": "/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change",
}</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="ptp-getting-the-current-ptp-clock-status_ptp-consumer">
<title>Getting the current PTP clock status</title>
<simpara>To get the current PTP status for the node, send a <literal>GET</literal> action to one of the following event REST APIs:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>http://localhost:8081/api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/lock-state/CurrentState</literal></simpara>
</listitem>
<listitem>
<simpara><literal>http://localhost:8081/api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/sync-status/os-clock-sync-state/CurrentState</literal></simpara>
</listitem>
<listitem>
<simpara><literal>http://localhost:8081/api/ocloudNotifications/v1/cluster/node/&lt;node_name&gt;/sync/ptp-status/ptp-clock-class-change/CurrentState</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The response is a cloud native event JSON object. For example:</simpara>
<formalpara>
<title>Example lock-state API response</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "id": "c1ac3aa5-1195-4786-84f8-da0ea4462921",
  "type": "event.sync.ptp-status.ptp-state-change",
  "source": "/cluster/node/compute-1.example.com/sync/ptp-status/lock-state",
  "dataContentType": "application/json",
  "time": "2023-01-10T02:41:57.094981478Z",
  "data": {
    "version": "v1",
    "values": [
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "notification",
        "valueType": "enumeration",
        "value": "LOCKED"
      },
      {
        "resource": "/cluster/node/compute-1.example.com/ens5fx/master",
        "dataType": "metric",
        "valueType": "decimal64.3",
        "value": "29"
      }
    ]
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="ptp-verifying-events-consumer-app-is-receiving-events_ptp-consumer">
<title>Verifying that the PTP events consumer application is receiving events</title>
<simpara>Verify that the <literal>cloud-event-proxy</literal> container in the application pod is receiving PTP events.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed and configured the PTP Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the list of active <literal>linuxptp-daemon</literal> pods.
Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ptp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    READY   STATUS    RESTARTS   AGE
linuxptp-daemon-2t78p   3/3     Running   0          8h
linuxptp-daemon-k8n88   3/3     Running   0          8h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Access the metrics for the required consumer-side <literal>cloud-event-proxy</literal> container by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it &lt;linuxptp-daemon&gt; -n openshift-ptp -c cloud-event-proxy -- curl 127.0.0.1:9091/metrics</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;linuxptp-daemon&gt;</term>
<listitem>
<simpara>Specifies the pod you want to query, for example, <literal>linuxptp-daemon-2t78p</literal>.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># HELP cne_transport_connections_resets Metric to get number of connection resets
# TYPE cne_transport_connections_resets gauge
cne_transport_connection_reset 1
# HELP cne_transport_receiver Metric to get number of receiver created
# TYPE cne_transport_receiver gauge
cne_transport_receiver{address="/cluster/node/compute-1.example.com/ptp",status="active"} 2
cne_transport_receiver{address="/cluster/node/compute-1.example.com/redfish/event",status="active"} 2
# HELP cne_transport_sender Metric to get number of sender created
# TYPE cne_transport_sender gauge
cne_transport_sender{address="/cluster/node/compute-1.example.com/ptp",status="active"} 1
cne_transport_sender{address="/cluster/node/compute-1.example.com/redfish/event",status="active"} 1
# HELP cne_events_ack Metric to get number of events produced
# TYPE cne_events_ack gauge
cne_events_ack{status="success",type="/cluster/node/compute-1.example.com/ptp"} 18
cne_events_ack{status="success",type="/cluster/node/compute-1.example.com/redfish/event"} 18
# HELP cne_events_transport_published Metric to get number of events published by the transport
# TYPE cne_events_transport_published gauge
cne_events_transport_published{address="/cluster/node/compute-1.example.com/ptp",status="failed"} 1
cne_events_transport_published{address="/cluster/node/compute-1.example.com/ptp",status="success"} 18
cne_events_transport_published{address="/cluster/node/compute-1.example.com/redfish/event",status="failed"} 1
cne_events_transport_published{address="/cluster/node/compute-1.example.com/redfish/event",status="success"} 18
# HELP cne_events_transport_received Metric to get number of events received  by the transport
# TYPE cne_events_transport_received gauge
cne_events_transport_received{address="/cluster/node/compute-1.example.com/ptp",status="success"} 18
cne_events_transport_received{address="/cluster/node/compute-1.example.com/redfish/event",status="success"} 18
# HELP cne_events_api_published Metric to get number of events published by the rest api
# TYPE cne_events_api_published gauge
cne_events_api_published{address="/cluster/node/compute-1.example.com/ptp",status="success"} 19
cne_events_api_published{address="/cluster/node/compute-1.example.com/redfish/event",status="success"} 19
# HELP cne_events_received Metric to get number of events received
# TYPE cne_events_received gauge
cne_events_received{status="success",type="/cluster/node/compute-1.example.com/ptp"} 18
cne_events_received{status="success",type="/cluster/node/compute-1.example.com/redfish/event"} 18
# HELP promhttp_metric_handler_requests_in_flight Current number of scrapes being served.
# TYPE promhttp_metric_handler_requests_in_flight gauge
promhttp_metric_handler_requests_in_flight 1
# HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code.
# TYPE promhttp_metric_handler_requests_total counter
promhttp_metric_handler_requests_total{code="200"} 4
promhttp_metric_handler_requests_total{code="500"} 0
promhttp_metric_handler_requests_total{code="503"} 0</programlisting>
</para>
</formalpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="_external-dns-operator">
<title>External DNS Operator</title>
<section xml:id="external-dns-operator-release-notes">
<title>External DNS Operator release notes</title>

<simpara>The External DNS Operator deploys and manages <literal>ExternalDNS</literal> to provide name resolution for services and routes from the external DNS provider to OpenShift Container Platform.</simpara>
<simpara>These release notes track the development of the External DNS Operator in OpenShift Container Platform.</simpara>
<section xml:id="external-dns-operator-release-notes-1.2.0">
<title>External DNS Operator 1.2.0</title>
<simpara>The following advisory is available for the External DNS Operator version 1.2.0:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2023:7239">RHEA-2022:5867 ExternalDNS Operator 1.2 operator/operand containers</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="external-dns-operator-1.2.0-new-features">
<title>New features</title>
<itemizedlist>
<listitem>
<simpara>The External DNS Operator now supports AWS shared VPC. For more information, see <link linkend="nw-control-dns-records-public-aws-with-VPC_creating-dns-records-on-aws">Creating DNS records in a different AWS Account using a shared VPC</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="external-dns-operator-1.2.0-bug-fixes">
<title>Bug fixes</title>
<itemizedlist>
<listitem>
<simpara>The update strategy for the operand changed from <literal>Rolling</literal> to <literal>Recreate</literal>. (<link xlink:href="https://issues.redhat.com/browse/OCPBUGS-3630"><emphasis role="strong">OCPBUGS-3630</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="external-dns-operator-release-notes-1.1.1">
<title>External DNS Operator 1.1.1</title>
<simpara>The following advisory is available for the External DNS Operator version 1.1.1:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2024:0536">RHEA-2024:0536 ExternalDNS Operator 1.1 operator/operand containers</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="external-dns-operator-release-notes-1.1.0">
<title>External DNS Operator 1.1.0</title>
<simpara>This release included a rebase of the operand from the upstream project version 0.13.1. The following advisory is available for the External DNS Operator version 1.1.0:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2022:9086">RHEA-2022:9086-01 ExternalDNS Operator 1.1 operator/operand containers</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="external-dns-operator-1.1.0-bug-fix">
<title>Bug fixes</title>
<itemizedlist>
<listitem>
<simpara>Previously, the ExternalDNS Operator enforced an empty <literal>defaultMode</literal> value for volumes, which caused constant updates due to a conflict with the OpenShift API. Now, the <literal>defaultMode</literal> value is not enforced and operand deployment does not update constantly. (<link xlink:href="https://issues.redhat.com/browse/OCPBUGS-2793"><emphasis role="strong">OCPBUGS-2793</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="external-dns-operator-release-notes-1.0.1">
<title>External DNS Operator 1.0.1</title>
<simpara>The following advisory is available for the External DNS Operator version 1.0.1:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2024:0537">RHEA-2024:0537 ExternalDNS Operator 1.0 operator/operand containers</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="external-dns-operator-release-notes-1.0.0">
<title>External DNS Operator 1.0.0</title>
<simpara>The following advisory is available for the External DNS Operator version 1.0.0:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2022:5867">RHEA-2022:5867 ExternalDNS Operator 1.0 operator/operand containers</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="external-dns-operator-1.0.0-bug-fixes">
<title>Bug fixes</title>
<itemizedlist>
<listitem>
<simpara>Previously, the External DNS Operator issued a warning about the violation of the restricted SCC policy during ExternalDNS operand pod deployments. This issue has been resolved. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=2086408"><emphasis role="strong">BZ#2086408</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="external-dns-operator">
<title>External DNS Operator in OpenShift Container Platform</title>

<simpara>The External DNS Operator deploys and manages <literal>ExternalDNS</literal> to provide the name resolution for services and routes from the external DNS provider to OpenShift Container Platform.</simpara>
<section xml:id="nw-external-dns-operator_external-dns-operator">
<title>External DNS Operator</title>
<simpara>The External DNS Operator implements the External DNS API from the <literal>olm.openshift.io</literal> API group. The External DNS Operator deploys the <literal>ExternalDNS</literal> using a deployment resource. The ExternalDNS deployment watches the resources such as services and routes in the cluster and updates the external DNS providers.</simpara>
<formalpara>
<title>Procedure</title>
<para>You can deploy the ExternalDNS Operator on demand from the OperatorHub, this creates a <literal>Subscription</literal> object.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check the name of an install plan:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator get sub external-dns-operator -o yaml | yq '.status.installplan.name'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">install-zcvlr</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the status of an install plan, the status of an install plan must be <literal>Complete</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator get ip &lt;install_plan_name&gt; -o yaml | yq '.status.phase'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Complete</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>oc get</literal> command to view the <literal>Deployment</literal> status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n external-dns-operator deployment/external-dns-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    READY     UP-TO-DATE   AVAILABLE   AGE
external-dns-operator   1/1       1            1           23h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-external-dns-operator-logs_external-dns-operator">
<title>External DNS Operator logs</title>
<simpara>You can view External DNS Operator logs by using the <literal>oc logs</literal> command.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the logs of the External DNS Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n external-dns-operator deployment/external-dns-operator -c external-dns-operator</programlisting>
</listitem>
</orderedlist>
<section xml:id="_external-dns-operator-domain-name-limitations">
<title>External DNS Operator domain name limitations</title>
<simpara>External DNS Operator uses the TXT registry, which follows the new format and adds the prefix for the TXT records. This reduces the maximum length of the domain name for the TXT records. A DNS record cannot be present without a corresponding TXT record, so the domain name of the DNS record must follow the same limit as the TXT records. For example, DNS record is <literal>&lt;domain-name-from-source&gt;</literal> and the TXT record is <literal>external-dns-&lt;record-type&gt;-&lt;domain-name-from-source&gt;</literal>.</simpara>
<simpara>The domain name of the DNS records generated by External DNS Operator has the following limitations:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Record type</entry>
<entry align="left" valign="top">Number of characters</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>CNAME</simpara></entry>
<entry align="left" valign="top"><simpara>44</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Wildcard CNAME records on AzureDNS</simpara></entry>
<entry align="left" valign="top"><simpara>42</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>A</simpara></entry>
<entry align="left" valign="top"><simpara>48</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Wildcard A records on AzureDNS</simpara></entry>
<entry align="left" valign="top"><simpara>46</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>If the domain name generated by External DNS exceeds the domain name limitation, the External DNS instance gives the following error:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator logs external-dns-aws-7ddbd9c7f8-2jqjh <co xml:id="CO47-1"/></programlisting>
<calloutlist>
<callout arearefs="CO47-1">
<para>The <literal>external-dns-aws-7ddbd9c7f8-2jqjh</literal> parameter specifies the name of the External DNS pod.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">time="2022-09-02T08:53:57Z" level=info msg="Desired change: CREATE external-dns-cname-hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc.test.example.io TXT [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=info msg="Desired change: CREATE external-dns-hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc.test.example.io TXT [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=info msg="Desired change: CREATE hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc.test.example.io A [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=error msg="Failure in zone test.example.io. [Id: /hostedzone/Z06988883Q0H0RL6UMXXX]"
time="2022-09-02T08:53:57Z" level=error msg="InvalidChangeBatch: [FATAL problem: DomainLabelTooLong (Domain label is too long) encountered with 'external-dns-a-hello-openshift-aaaaaaaaaa-bbbbbbbbbb-ccccccc']\n\tstatus code: 400, request id: e54dfd5a-06c6-47b0-bcb9-a4f7c3a4e0c6"</programlisting>
</para>
</formalpara>
</section>
</section>
</section>
<section xml:id="installing-external-dns-on-cloud-providers">
<title>Installing External DNS Operator on cloud providers</title>

<simpara>You can install External DNS Operator on cloud providers such as AWS, Azure and GCP.</simpara>
<section xml:id="nw-installing-external-dns-operator_installing-external-dns-on-cloud-providers">
<title>Installing the External DNS Operator</title>
<simpara>You can install the External DNS Operator using the OpenShift Container Platform OperatorHub.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">OperatorHub</emphasis> in the OpenShift Container Platform Web Console.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">External DNS Operator</emphasis>.
You can use the <emphasis role="strong">Filter by keyword</emphasis> text box or the filter list to search for External DNS Operator from the list of Operators.</simpara>
</listitem>
<listitem>
<simpara>Select the <literal>external-dns-operator</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>On the External DNS Operator page, click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, ensure that you selected the following options:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Update the channel as <emphasis role="strong">stable-v1</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Installation mode as <emphasis role="strong">A specific name on the cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Installed namespace as <literal>external-dns-operator</literal>. If namespace <literal>external-dns-operator</literal> does not exist, it gets created during the Operator installation.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Approval Strategy</emphasis> as <emphasis role="strong">Automatic</emphasis> or <emphasis role="strong">Manual</emphasis>. Approval Strategy is set to <emphasis role="strong">Automatic</emphasis> by default.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>If you select <emphasis role="strong">Automatic</emphasis> updates, the Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without any intervention.</simpara>
<simpara>If you select <emphasis role="strong">Manual</emphasis> updates, the OLM creates an update request. As a cluster administrator, you must then manually approve that update request to have the Operator updated to the new version.</simpara>
<formalpara>
<title>Verification</title>
<para>Verify that External DNS Operator shows the <emphasis role="strong">Status</emphasis> as <emphasis role="strong">Succeeded</emphasis> on the Installed Operators dashboard.</para>
</formalpara>
</section>
</section>
<section xml:id="external-dns-operator-configuration-parameters">
<title>External DNS Operator configuration parameters</title>

<simpara>The External DNS Operators includes the following configuration parameters:</simpara>
<section xml:id="nw-external-dns-operator-configuration-parameters_external-dns-operator-configuration-parameters">
<title>External DNS Operator configuration parameters</title>
<simpara>The External DNS Operator includes the following configuration parameters:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>spec</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables the type of a cloud provider.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  provider:
    type: AWS <co xml:id="CO48-1"/>
    aws:
      credentials:
        name: aws-access-key <co xml:id="CO48-2"/></programlisting>
<calloutlist>
<callout arearefs="CO48-1">
<para>Defines available options such as AWS, GCP and Azure.</para>
</callout>
<callout arearefs="CO48-2">
<para>Defines a name of the <literal>secret</literal> which contains credentials for your cloud provider.</para>
</callout>
</calloutlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>zones</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables you to specify DNS zones by their domains. If you do not specify zones, <literal>ExternalDNS</literal> discovers all the zones present in your cloud provider account.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">zones:
- "myzoneid" <co xml:id="CO49-1"/></programlisting>
<calloutlist>
<callout arearefs="CO49-1">
<para>Specifies the IDs of DNS zones.</para>
</callout>
</calloutlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>domains</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables you to specify AWS zones by their domains. If you do not specify domains, <literal>ExternalDNS</literal> discovers all the zones present in your cloud provider account.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">domains:
- filterType: Include <co xml:id="CO50-1"/>
  matchType: Exact <co xml:id="CO50-2"/>
  name: "myzonedomain1.com" <co xml:id="CO50-3"/>
- filterType: Include
  matchType: Pattern <co xml:id="CO50-4"/>
  pattern: ".*\\.otherzonedomain\\.com" <co xml:id="CO50-5"/></programlisting>
<calloutlist>
<callout arearefs="CO50-1">
<para>Instructs <literal>ExternalDNS</literal> to include the domain specified.</para>
</callout>
<callout arearefs="CO50-2">
<para>Instructs <literal>ExtrnalDNS</literal> that the domain matching has to be exact as opposed to regular expression match.</para>
</callout>
<callout arearefs="CO50-3">
<para>Defines the exact domain name by which <literal>ExternalDNS</literal> filters.</para>
</callout>
<callout arearefs="CO50-4">
<para>Sets <literal>regex-domain-filter</literal> flag in <literal>ExternalDNS</literal>. You can limit possible domains by using a Regex filter.</para>
</callout>
<callout arearefs="CO50-5">
<para>Defines the regex pattern to be used by <literal>ExternalDNS</literal> to filter the domains of the target zones.</para>
</callout>
</calloutlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>source</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables you to specify the source for the DNS records, <literal>Service</literal> or <literal>Route</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">source: <co xml:id="CO51-1"/>
  type: Service <co xml:id="CO51-2"/>
  service:
    serviceType:<co xml:id="CO51-3"/>
      - LoadBalancer
      - ClusterIP
  labelFilter: <co xml:id="CO51-4"/>
    matchLabels:
      external-dns.mydomain.org/publish: "yes"
  hostnameAnnotation: "Allow" <co xml:id="CO51-5"/>
  fqdnTemplate:
  - "{{.Name}}.myzonedomain.com" <co xml:id="CO51-6"/></programlisting>
<calloutlist>
<callout arearefs="CO51-1">
<para>Defines the settings for the source of DNS records.</para>
</callout>
<callout arearefs="CO51-2">
<para>The <literal>ExternalDNS</literal> uses <literal>Service</literal> type as source for creating dns records.</para>
</callout>
<callout arearefs="CO51-3">
<para>Sets <literal>service-type-filter</literal> flag in <literal>ExternalDNS</literal>. The <literal>serviceType</literal> contains the following fields:</para>
<itemizedlist>
<listitem>
<simpara><literal>default</literal>: <literal>LoadBalancer</literal></simpara>
</listitem>
<listitem>
<simpara><literal>expected</literal>: <literal>ClusterIP</literal></simpara>
</listitem>
<listitem>
<simpara><literal>NodePort</literal></simpara>
</listitem>
<listitem>
<simpara><literal>LoadBalancer</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ExternalName</literal></simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO51-4">
<para>Ensures that the controller considers only those resources which matches with label filter.</para>
</callout>
<callout arearefs="CO51-5">
<para>The default value for <literal>hostnameAnnotation</literal> is <literal>Ignore</literal> which instructs <literal>ExternalDNS</literal> to generate DNS records using the templates specified in the field <literal>fqdnTemplates</literal>. When the value is <literal>Allow</literal> the DNS records get generated based on the value specified in the <literal>external-dns.alpha.kubernetes.io/hostname</literal> annotation.</para>
</callout>
<callout arearefs="CO51-6">
<para>External DNS Operator uses a string to generate DNS names from sources that don&#8217;t define a hostname, or to add a hostname suffix when paired with the fake source.</para>
</callout>
</calloutlist>
<programlisting language="yaml" linenumbering="unnumbered">source:
  type: OpenShiftRoute <co xml:id="CO52-1"/>
  openshiftRouteOptions:
    routerName: default <co xml:id="CO52-2"/>
    labelFilter:
      matchLabels:
        external-dns.mydomain.org/publish: "yes"</programlisting>
<calloutlist>
<callout arearefs="CO52-1">
<para>ExternalDNS` uses type <literal>route</literal> as source for creating dns records.</para>
</callout>
<callout arearefs="CO52-2">
<para>If the source is <literal>OpenShiftRoute</literal>, then you can pass the Ingress Controller name. The <literal>ExternalDNS</literal> uses canonical name of Ingress Controller as the target for CNAME record.</para>
</callout>
</calloutlist></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="creating-dns-records-on-aws">
<title>Creating DNS records on AWS</title>

<simpara>You can create DNS records on AWS and AWS GovCloud by using External DNS Operator.</simpara>
<section xml:id="nw-control-dns-records-public-hosted-zone-aws_creating-dns-records-on-aws">
<title>Creating DNS records on an public hosted zone for AWS by using Red Hat External DNS Operator</title>
<simpara>You can create DNS records on a public hosted zone for AWS by using the Red Hat External DNS Operator. You can use the same instructions to create DNS records on a hosted zone for AWS GovCloud.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the user. The user must have access to the <literal>kube-system</literal> namespace. If you don’t have the credentials, as you can fetch the credentials from the <literal>kube-system</literal> namespace to use the cloud provider client:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc whoami</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">system:admin</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Fetch the values from aws-creds secret present in <literal>kube-system</literal> namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export AWS_ACCESS_KEY_ID=$(oc get secrets aws-creds -n kube-system  --template={{.data.aws_access_key_id}} | base64 -d)
$ export AWS_SECRET_ACCESS_KEY=$(oc get secrets aws-creds -n kube-system  --template={{.data.aws_secret_access_key}} | base64 -d)</programlisting>
</listitem>
<listitem>
<simpara>Get the routes to check the domain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes --all-namespaces | grep console</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">openshift-console          console             console-openshift-console.apps.testextdnsoperator.apacshift.support                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.testextdnsoperator.apacshift.support                     downloads           http    edge/Redirect          None</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the list of dns zones to find the one which corresponds to the previously found route&#8217;s domain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws route53 list-hosted-zones | grep testextdnsoperator.apacshift.support</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">HOSTEDZONES	terraform	/hostedzone/Z02355203TNN1XXXX1J6O	testextdnsoperator.apacshift.support.	5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create <literal>ExternalDNS</literal> resource for <literal>route</literal> source:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt;EOF | oc create -f -
apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-aws <co xml:id="CO53-1"/>
spec:
  domains:
  - filterType: Include   <co xml:id="CO53-2"/>
    matchType: Exact   <co xml:id="CO53-3"/>
    name: testextdnsoperator.apacshift.support <co xml:id="CO53-4"/>
  provider:
    type: AWS <co xml:id="CO53-5"/>
  source:  <co xml:id="CO53-6"/>
    type: OpenShiftRoute <co xml:id="CO53-7"/>
    openshiftRouteOptions:
      routerName: default <co xml:id="CO53-8"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO53-1">
<para>Defines the name of external DNS resource.</para>
</callout>
<callout arearefs="CO53-2">
<para>By default all hosted zones are selected as potential targets. You can include a hosted zone that you need.</para>
</callout>
<callout arearefs="CO53-3">
<para>The matching of the target zone&#8217;s domain has to be exact (as opposed to regular expression match).</para>
</callout>
<callout arearefs="CO53-4">
<para>Specify the exact domain of the zone you want to update. The hostname of the routes must be subdomains of the specified domain.</para>
</callout>
<callout arearefs="CO53-5">
<para>Defines the <literal>AWS Route53</literal> DNS provider.</para>
</callout>
<callout arearefs="CO53-6">
<para>Defines options for the source of DNS records.</para>
</callout>
<callout arearefs="CO53-7">
<para>Defines OpenShift <literal>route</literal> resource as the source for the DNS records which gets created in the previously specified DNS provider.</para>
</callout>
<callout arearefs="CO53-8">
<para>If the source is <literal>OpenShiftRoute</literal>, then you can pass the OpenShift Ingress Controller name. External DNS Operator selects the canonical hostname of that router as the target while creating CNAME record.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the records created for OCP routes using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws route53 list-resource-record-sets --hosted-zone-id Z02355203TNN1XXXX1J6O --query "ResourceRecordSets[?Type == 'CNAME']" | grep console</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-control-dns-records-public-aws-with-VPC_creating-dns-records-on-aws">
<title>Creating DNS records in a different AWS Account using a shared VPC</title>
<simpara>You can use the ExternalDNS Operator to create DNS records in a different AWS account using a shared Virtual Private Cloud (VPC). By using a shared VPC, an organization can connect resources from multiple projects to a common VPC network. Organizations can then use VPC sharing to use a single Route 53 instance across multiple AWS accounts.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created two Amazon AWS accounts: one with a VPC and a Route 53 private hosted zone configured (Account A), and another for installing a cluster (Account B).</simpara>
</listitem>
<listitem>
<simpara>You have created an IAM Policy and IAM Role with the appropriate permissions in Account A for Account B to create DNS records in the Route 53 hosted zone of Account A.</simpara>
</listitem>
<listitem>
<simpara>You have installed a cluster in Account B into the existing VPC for Account A.</simpara>
</listitem>
<listitem>
<simpara>You have installed the ExternalDNS Operator in the cluster in Account B.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the Role ARN of the IAM Role that you created to allow Account B to access Account A&#8217;s Route 53 hosted zone by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws --profile account-a iam get-role --role-name user-rol1 | head -1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ROLE	arn:aws:iam::1234567890123:role/user-rol1	2023-09-14T17:21:54+00:00	3600	/	AROA3SGB2ZRKRT5NISNJN	user-rol1</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Locate the private hosted zone to use with Account A&#8217;s credentials by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws --profile account-a route53 list-hosted-zones | grep testextdnsoperator.apacshift.support</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">HOSTEDZONES	terraform	/hostedzone/Z02355203TNN1XXXX1J6O	testextdnsoperator.apacshift.support. 5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create the <literal>ExternalDNS</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF | oc create -f -
apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-aws
spec:
  domains:
  - filterType: Include
    matchType: Exact
    name: testextdnsoperator.apacshift.support
  provider:
    type: AWS
    aws:
      assumeRole:
        arn: arn:aws:iam::12345678901234:role/user-rol1 <co xml:id="CO54-1"/>
  source:
    type: OpenShiftRoute
    openshiftRouteOptions:
      routerName: default
EOF</programlisting>
<calloutlist>
<callout arearefs="CO54-1">
<para>Specify the Role ARN  to have DNS records created in Account A.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the records created for OpenShift Container Platform (OCP) routes by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws --profile account-a route53 list-resource-record-sets --hosted-zone-id Z02355203TNN1XXXX1J6O --query "ResourceRecordSets[?Type == 'CNAME']" | grep console-openshift-console</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="creating-dns-records-on-azure">
<title>Creating DNS records on Azure</title>

<simpara>You can create DNS records on Azure using External DNS Operator.</simpara>
<section xml:id="nw-control-dns-records-public-hosted-zone-azure_creating-dns-records-on-azure">
<title>Creating DNS records on an public DNS zone for Azure by using Red Hat External DNS Operator</title>
<simpara>You can create DNS records on a public DNS zone for Azure by using Red Hat External DNS Operator.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the user. The user must have access to the <literal>kube-system</literal> namespace. If you don’t have the credentials, as you can fetch the credentials from the <literal>kube-system</literal> namespace to use the cloud provider client:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc whoami</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">system:admin</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Fetch the values from azure-credentials secret present in <literal>kube-system</literal> namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CLIENT_ID=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_client_id}} | base64 -d)
$ CLIENT_SECRET=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_client_secret}} | base64 -d)
$ RESOURCE_GROUP=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_resourcegroup}} | base64 -d)
$ SUBSCRIPTION_ID=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_subscription_id}} | base64 -d)
$ TENANT_ID=$(oc get secrets azure-credentials  -n kube-system  --template={{.data.azure_tenant_id}} | base64 -d)</programlisting>
</listitem>
<listitem>
<simpara>Login to azure with base64 decoded values:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az login --service-principal -u "${CLIENT_ID}" -p "${CLIENT_SECRET}" --tenant "${TENANT_ID}"</programlisting>
</listitem>
<listitem>
<simpara>Get the routes to check the domain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes --all-namespaces | grep console</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">openshift-console          console             console-openshift-console.apps.test.azure.example.com                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.test.azure.example.com                     downloads           http    edge/Redirect          None</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the list of dns zones to find the one which corresponds to the previously found route&#8217;s domain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az network dns zone list --resource-group "${RESOURCE_GROUP}"</programlisting>
</listitem>
<listitem>
<simpara>Create <literal>ExternalDNS</literal> resource for <literal>route</literal> source:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-azure <co xml:id="CO55-1"/>
spec:
  zones:
  - "/subscriptions/1234567890/resourceGroups/test-azure-xxxxx-rg/providers/Microsoft.Network/dnszones/test.azure.example.com" <co xml:id="CO55-2"/>
  provider:
    type: Azure <co xml:id="CO55-3"/>
  source:
    openshiftRouteOptions: <co xml:id="CO55-4"/>
      routerName: default <co xml:id="CO55-5"/>
    type: OpenShiftRoute <co xml:id="CO55-6"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO55-1">
<para>Specifies the name of External DNS CR.</para>
</callout>
<callout arearefs="CO55-2">
<para>Define the zone ID.</para>
</callout>
<callout arearefs="CO55-3">
<para>Defines the Azure DNS provider.</para>
</callout>
<callout arearefs="CO55-4">
<para>You can define options for the source of DNS records.</para>
</callout>
<callout arearefs="CO55-5">
<para>If the source is <literal>OpenShiftRoute</literal> then you can pass the OpenShift Ingress Controller name. External DNS selects the canonical hostname of that router as the target while creating CNAME record.</para>
</callout>
<callout arearefs="CO55-6">
<para>Defines OpenShift <literal>route</literal> resource as the source for the DNS records which gets created in the previously specified DNS provider.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the records created for OCP routes using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az network dns record-set list -g "${RESOURCE_GROUP}"  -z test.azure.example.com | grep console</programlisting>
<note>
<simpara>To create records on private hosted zones on private Azure dns, you need to specify the private zone under <literal>zones</literal> which populates the provider type to <literal>azure-private-dns</literal> in the <literal>ExternalDNS</literal> container args.</simpara>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="creating-dns-records-on-gcp">
<title>Creating DNS records on GCP</title>

<simpara>You can create DNS records on GCP using External DNS Operator.</simpara>
<section xml:id="nw-control-dns-records-public-managed-zone-gcp_creating-dns-records-on-gcp">
<title>Creating DNS records on an public managed zone for GCP by using Red Hat External DNS Operator</title>
<simpara>You can create DNS records on a public managed zone for GCP by using Red Hat External DNS Operator.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the user. The user must have access to the <literal>kube-system</literal> namespace. If you don’t have the credentials, as you can fetch the credentials from the <literal>kube-system</literal> namespace to use the cloud provider client:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc whoami</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">system:admin</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Copy the value of service_account.json in gcp-credentials secret in a file encoded-gcloud.json by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret gcp-credentials -n kube-system --template='{{$v := index .data "service_account.json"}}{{$v}}' | base64 -d - &gt; decoded-gcloud.json</programlisting>
</listitem>
<listitem>
<simpara>Export Google credentials:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export GOOGLE_CREDENTIALS=decoded-gcloud.json</programlisting>
</listitem>
<listitem>
<simpara>Activate your account by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud auth activate-service-account  &lt;client_email as per decoded-gcloud.json&gt; --key-file=decoded-gcloud.json</programlisting>
</listitem>
<listitem>
<simpara>Set your project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud config set project &lt;project_id as per decoded-gcloud.json&gt;</programlisting>
</listitem>
<listitem>
<simpara>Get the routes to check the domain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes --all-namespaces | grep console</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">openshift-console          console             console-openshift-console.apps.test.gcp.example.com                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.test.gcp.example.com                     downloads           http    edge/Redirect          None</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the list of managed zones to find the zone which corresponds to the previously found route’s domain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud dns managed-zones list | grep test.gcp.example.com
qe-cvs4g-private-zone test.gcp.example.com</programlisting>
</listitem>
<listitem>
<simpara>Create <literal>ExternalDNS</literal> resource for <literal>route</literal> source:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-gcp <co xml:id="CO56-1"/>
spec:
  domains:
    - filterType: Include <co xml:id="CO56-2"/>
      matchType: Exact <co xml:id="CO56-3"/>
      name: test.gcp.example.com <co xml:id="CO56-4"/>
  provider:
    type: GCP <co xml:id="CO56-5"/>
  source:
    openshiftRouteOptions: <co xml:id="CO56-6"/>
      routerName: default <co xml:id="CO56-7"/>
    type: OpenShiftRoute <co xml:id="CO56-8"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO56-1">
<para>Specifies the name of External DNS CR.</para>
</callout>
<callout arearefs="CO56-2">
<para>By default all hosted zones are selected as potential targets. You can include a hosted zone that you need.</para>
</callout>
<callout arearefs="CO56-3">
<para>The matching of the target zone&#8217;s domain has to be exact (as opposed to regular expression match).</para>
</callout>
<callout arearefs="CO56-4">
<para>Specify the exact domain of the zone you want to update. The hostname of the routes must be subdomains of the specified domain.</para>
</callout>
<callout arearefs="CO56-5">
<para>Defines Google Cloud DNS provider.</para>
</callout>
<callout arearefs="CO56-6">
<para>You can define options for the source of DNS records.</para>
</callout>
<callout arearefs="CO56-7">
<para>If the source is <literal>OpenShiftRoute</literal> then you can pass the OpenShift Ingress Controller name. External DNS selects the canonical hostname of that router as the target while creating CNAME record.</para>
</callout>
<callout arearefs="CO56-8">
<para>Defines OpenShift <literal>route</literal> resource as the source for the DNS records which gets created in the previously specified DNS provider.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the records created for OCP routes using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud dns record-sets list --zone=qe-cvs4g-private-zone | grep console</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="creating-dns-records-on-infoblox">
<title>Creating DNS records on Infoblox</title>

<simpara>You can create DNS records on Infoblox using the Red Hat External DNS Operator.</simpara>
<section xml:id="nw-control-dns-records-public-dns-zone-infoblox_creating-dns-records-on-infoblox">
<title>Creating DNS records on a public DNS zone on Infoblox</title>
<simpara>You can create DNS records on a public DNS zone on Infoblox by using the Red Hat External DNS Operator.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the Infoblox UI.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>secret</literal> object with Infoblox credentials by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator create secret generic infoblox-credentials --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_USERNAME=&lt;infoblox_username&gt; --from-literal=EXTERNAL_DNS_INFOBLOX_WAPI_PASSWORD=&lt;infoblox_password&gt;</programlisting>
</listitem>
<listitem>
<simpara>Get the routes objects to check your cluster domain by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes --all-namespaces | grep console</programlisting>
<formalpara>
<title>Example Output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">openshift-console          console             console-openshift-console.apps.test.example.com                       console             https   reencrypt/Redirect     None
openshift-console          downloads           downloads-openshift-console.apps.test.example.com                     downloads           http    edge/Redirect          None</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create an <literal>ExternalDNS</literal> resource YAML file, for example, sample-infoblox.yaml, as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: externaldns.olm.openshift.io/v1beta1
kind: ExternalDNS
metadata:
  name: sample-infoblox
spec:
  provider:
    type: Infoblox
    infoblox:
      credentials:
        name: infoblox-credentials
      gridHost: ${INFOBLOX_GRID_PUBLIC_IP}
      wapiPort: 443
      wapiVersion: "2.3.1"
  domains:
  - filterType: Include
    matchType: Exact
    name: test.example.com
  source:
    type: OpenShiftRoute
    openshiftRouteOptions:
      routerName: default</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>ExternalDNS</literal> resource on Infoblox by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sample-infoblox.yaml</programlisting>
</listitem>
<listitem>
<simpara>From the Infoblox UI, check the DNS records created for <literal>console</literal> routes:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Data Management</emphasis> &#8594; <emphasis role="strong">DNS</emphasis> &#8594; <emphasis role="strong">Zones</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the zone name.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="external-dns-operator-cluster-wide-proxy">
<title>Configuring the cluster-wide proxy on the External DNS Operator</title>

<simpara>You can configure the cluster-wide proxy in the External DNS Operator. After configuring the cluster-wide proxy in the External DNS Operator, Operator Lifecycle Manager (OLM) automatically updates all the deployments of the Operators with the environment variables such as <literal>HTTP_PROXY</literal>, <literal>HTTPS_PROXY</literal>, and <literal>NO_PROXY</literal>.</simpara>
<section xml:id="nw-configuring-cluster-wide-proxy_external-dns-operator-cluster-wide-proxy">
<title>Configuring the External DNS Operator to trust the certificate authority of the cluster-wide proxy</title>
<simpara>You can configure the External DNS Operator to trust the certificate authority of the cluster-wide proxy.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the config map to contain the CA bundle in the <literal>external-dns-operator</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator create configmap trusted-ca</programlisting>
</listitem>
<listitem>
<simpara>To inject the trusted CA bundle into the config map, add the <literal>config.openshift.io/inject-trusted-cabundle=true</literal> label to the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator label cm trusted-ca config.openshift.io/inject-trusted-cabundle=true</programlisting>
</listitem>
<listitem>
<simpara>Update the subscription of the External DNS Operator by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator patch subscription external-dns-operator --type='json' -p='[{"op": "add", "path": "/spec/config", "value":{"env":[{"name":"TRUSTED_CA_CONFIGMAP_NAME","value":"trusted-ca"}]}}]'</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After the deployment of the External DNS Operator is completed, verify that the trusted CA environment variable is added to the <literal>external-dns-operator</literal> deployment by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n external-dns-operator exec deploy/external-dns-operator -c external-dns-operator -- printenv TRUSTED_CA_CONFIGMAP_NAME</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">trusted-ca</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_network-policy">
<title>Network policy</title>
<section xml:id="about-network-policy">
<title>About network policy</title>

<simpara>As a cluster administrator, you can define network policies that restrict traffic to pods in your cluster.</simpara>
<section xml:id="nw-networkpolicy-about_about-network-policy">
<title>About network policy</title>
<simpara>In a cluster using a network plugin that supports Kubernetes network policy, network isolation is controlled entirely by <literal>NetworkPolicy</literal> objects.
In OpenShift Container Platform 4.14, OpenShift SDN supports using network policy in its default network isolation mode.</simpara>
<warning>
<simpara>Network policy does not apply to the host network namespace. Pods with host networking enabled are unaffected by network policy rules. However, pods connecting to the host-networked pods might be affected by the network policy rules.</simpara>
<simpara>Network policies cannot block traffic from localhost or from their resident nodes.</simpara>
</warning>
<simpara>By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create <literal>NetworkPolicy</literal> objects in that project to indicate the allowed incoming connections. Project administrators can create and delete <literal>NetworkPolicy</literal> objects within their own project.</simpara>
<simpara>If a pod is matched by selectors in one or more <literal>NetworkPolicy</literal> objects, then the pod will accept only connections that are allowed by at least one of those <literal>NetworkPolicy</literal> objects. A pod that is not selected by any <literal>NetworkPolicy</literal> objects is fully accessible.</simpara>
<simpara>A network policy applies to only the TCP, UDP, ICMP, and SCTP protocols. Other protocols are not affected.</simpara>
<simpara>The following example <literal>NetworkPolicy</literal> objects demonstrate supporting different scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>Deny all traffic:</simpara>
<simpara>To make a project deny by default, add a <literal>NetworkPolicy</literal> object that matches all pods but accepts no traffic:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector: {}
  ingress: []</programlisting>
</listitem>
<listitem>
<simpara>Only allow connections from the OpenShift Container Platform Ingress Controller:</simpara>
<simpara>To make a project allow only connections from the OpenShift Container Platform Ingress Controller, add the following <literal>NetworkPolicy</literal> object.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress</programlisting>
</listitem>
<listitem>
<simpara>Only accept connections from pods within a project:</simpara>
<simpara>To make pods accept connections from other pods in the same project, but reject all other connections from pods in other projects, add the following <literal>NetworkPolicy</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}</programlisting>
</listitem>
<listitem>
<simpara>Only allow HTTP and HTTPS traffic based on pod labels:</simpara>
<simpara>To enable only HTTP and HTTPS access to the pods with a specific label (<literal>role=frontend</literal> in following example), add a <literal>NetworkPolicy</literal> object similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-http-and-https
spec:
  podSelector:
    matchLabels:
      role: frontend
  ingress:
  - ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443</programlisting>
</listitem>
<listitem>
<simpara>Accept connections by using both namespace and pod selectors:</simpara>
<simpara>To match network traffic by combining namespace and pod selectors, you can use a <literal>NetworkPolicy</literal> object similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-pod-and-namespace-both
spec:
  podSelector:
    matchLabels:
      name: test-pods
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            project: project_name
        podSelector:
          matchLabels:
            name: test-pods</programlisting>
</listitem>
</itemizedlist>
<simpara><literal>NetworkPolicy</literal> objects are additive, which means you can combine multiple <literal>NetworkPolicy</literal> objects together to satisfy complex network requirements.</simpara>
<simpara>For example, for the <literal>NetworkPolicy</literal> objects defined in previous samples, you can define both <literal>allow-same-namespace</literal> and <literal>allow-http-and-https</literal> policies within the same project. Thus allowing the pods with the label <literal>role=frontend</literal>, to accept any connection allowed by each policy. That is, connections on any port from pods in the same namespace, and connections on ports <literal>80</literal> and <literal>443</literal> from pods in any namespace.</simpara>
<section xml:id="nw-networkpolicy-allow-from-router_about-network-policy">
<title>Using the allow-from-router network policy</title>
<simpara>Use the following <literal>NetworkPolicy</literal> to allow external traffic regardless of the router configuration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-router
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""<co xml:id="CO57-1"/>
  podSelector: {}
  policyTypes:
  - Ingress</programlisting>
<calloutlist>
<callout arearefs="CO57-1">
<para><literal>policy-group.network.openshift.io/ingress:""</literal> label supports both OpenShift-SDN and OVN-Kubernetes.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-networkpolicy-allow-from-hostnetwork_about-network-policy">
<title>Using the allow-from-hostnetwork network policy</title>
<simpara>Add the following <literal>allow-from-hostnetwork</literal> <literal>NetworkPolicy</literal> object to direct traffic from the host network pods:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-hostnetwork
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/host-network: ""
  podSelector: {}
  policyTypes:
  - Ingress</programlisting>
</section>
</section>
<section xml:id="nw-networkpolicy-optimize-sdn_about-network-policy">
<title>Optimizations for network policy with OpenShift SDN</title>
<simpara>Use a network policy to isolate pods that are differentiated from one another by labels within a namespace.</simpara>
<simpara>It is inefficient to apply <literal>NetworkPolicy</literal> objects to large numbers of individual pods in a single namespace. Pod labels do not exist at the IP address level, so a network policy generates a separate Open vSwitch (OVS) flow rule for every possible link between every pod selected with a <literal>podSelector</literal>.</simpara>
<simpara>For example, if the spec <literal>podSelector</literal> and the ingress <literal>podSelector</literal> within a <literal>NetworkPolicy</literal> object each match 200 pods, then 40,000 (200*200) OVS flow rules are generated. This might slow down a node.</simpara>
<simpara>When designing your network policy, refer to the following guidelines:</simpara>
<itemizedlist>
<listitem>
<simpara>Reduce the number of OVS flow rules by using namespaces to contain groups of pods that need to be isolated.</simpara>
<simpara><literal>NetworkPolicy</literal> objects that select a whole namespace, by using the <literal>namespaceSelector</literal> or an empty <literal>podSelector</literal>, generate only a single OVS flow rule that matches the VXLAN virtual network ID (VNID) of the namespace.</simpara>
</listitem>
<listitem>
<simpara>Keep the pods that do not need to be isolated in their original namespace, and move the pods that require isolation into one or more different namespaces.</simpara>
</listitem>
<listitem>
<simpara>Create additional targeted cross-namespace network policies to allow the specific traffic that you do want to allow from the isolated pods.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-networkpolicy-optimize-ovn_about-network-policy">
<title>Optimizations for network policy with OVN-Kubernetes network plugin</title>
<simpara>When designing your network policy, refer to the following guidelines:</simpara>
<itemizedlist>
<listitem>
<simpara>For network policies with the same <literal>spec.podSelector</literal> spec, it is more efficient to use one network policy with multiple <literal>ingress</literal> or <literal>egress</literal> rules, than multiple network policies with subsets of <literal>ingress</literal> or <literal>egress</literal> rules.</simpara>
</listitem>
<listitem>
<simpara>Every <literal>ingress</literal> or <literal>egress</literal> rule based on the <literal>podSelector</literal> or <literal>namespaceSelector</literal> spec generates the number of OVS flows proportional to <literal>number of pods selected by network policy + number of pods selected by ingress or egress rule</literal>. Therefore, it is preferable to use the <literal>podSelector</literal> or <literal>namespaceSelector</literal> spec that can select as many pods as you need in one rule, instead of creating individual rules for every pod.</simpara>
<simpara>For example, the following policy contains two rules:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
  - from:
    - podSelector:
        matchLabels:
          role: backend</programlisting>
<simpara>The following policy expresses those same two rules as one:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: test-network-policy
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector:
        matchExpressions:
        - {key: role, operator: In, values: [frontend, backend]}</programlisting>
<simpara>The same guideline applies to the <literal>spec.podSelector</literal> spec. If you have the same <literal>ingress</literal> or <literal>egress</literal> rules for different network policies, it might be more efficient to create one network policy with a common <literal>spec.podSelector</literal> spec. For example, the following two policies have different rules:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy1
spec:
  podSelector:
    matchLabels:
      role: db
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy2
spec:
  podSelector:
    matchLabels:
      role: client
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend</programlisting>
<simpara>The following network policy expresses those same two rules as one:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: policy3
spec:
  podSelector:
    matchExpressions:
    - {key: role, operator: In, values: [db, client]}
  ingress:
  - from:
    - podSelector:
        matchLabels:
          role: frontend</programlisting>
<simpara>You can apply this optimization when only multiple selectors are expressed as one. In cases where selectors are based on different labels, it may not be possible to apply this optimization. In those cases, consider applying some new labels for network policy optimization specifically.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-network-policy-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="creating-network-policy">Creating a network policy</link></simpara>
</listitem>
<listitem>
<simpara>Optional: <link linkend="default-network-policy">Defining a default network policy</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-network-policy-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#rbac-projects-namespaces_using-rbac">Projects and namespaces</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="multitenant-network-policy">Configuring multitenant network policy</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#networkpolicy-networking-k8s-io-v1">NetworkPolicy API</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-network-policy">
<title>Creating a network policy</title>

<simpara>As a user with the <literal>admin</literal> role, you can create a network policy for a namespace.</simpara>
<section xml:id="nw-networkpolicy-object_creating-network-policy">
<title>Example NetworkPolicy object</title>
<simpara>The following annotates an example NetworkPolicy object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <co xml:id="CO58-1"/>
spec:
  podSelector: <co xml:id="CO58-2"/>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <co xml:id="CO58-3"/>
        matchLabels:
          app: app
    ports: <co xml:id="CO58-4"/>
    - protocol: TCP
      port: 27017</programlisting>
<calloutlist>
<callout arearefs="CO58-1">
<para>The name of the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO58-2">
<para>A selector that describes the pods to which the policy applies. The policy object can
only select pods in the project that defines the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO58-3">
<para>A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.</para>
</callout>
<callout arearefs="CO58-4">
<para>A list of one or more destination ports on which to accept traffic.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-networkpolicy-create-cli_creating-network-policy">
<title>Creating a network policy using the CLI</title>
<simpara>To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a network policy.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy rule:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>&lt;policy_name&gt;.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ touch &lt;policy_name&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the network policy file name.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Define a network policy in the file that you just created, such as in the following examples:</simpara>
<formalpara>
<title>Deny ingress from all pods in all namespaces</title>
<para>This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector:
  ingress: []</programlisting>
<simpara>+
.Allow ingress from all pods in the same namespace</simpara>
<simpara>+</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<screen>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</screen>
<simpara>+
.Allow ingress traffic to one pod from a particular namespace</simpara>
<simpara>+
This policy allows traffic to pods labelled <literal>pod-a</literal> from pods running in <literal>namespace-y</literal>.</simpara>
<simpara>+</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-traffic-pod
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y</programlisting>
<simpara>+</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To create the network policy object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;policy_name&gt;.yaml -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the network policy file name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/deny-by-default created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-deny-all-multi-network-policy_creating-network-policy">
<title>Creating a default deny all network policy</title>
<simpara>This is a fundamental policy, blocking all cross-pod networking other than network traffic allowed by the configuration of other deployed network policies. This procedure enforces a default <literal>deny-by-default</literal> policy.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following YAML that defines a <literal>deny-by-default</literal> policy to deny ingress from all pods in all namespaces. Save the YAML in the <literal>deny-by-default.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: default <co xml:id="CO59-1"/>
spec:
  podSelector: {} <co xml:id="CO59-2"/>
  ingress: [] <co xml:id="CO59-3"/></programlisting>
<calloutlist>
<callout arearefs="CO59-1">
<para><literal>namespace: default</literal> deploys this policy to the <literal>default</literal> namespace.</para>
</callout>
<callout arearefs="CO59-2">
<para><literal>podSelector:</literal> is empty, this means it matches all the pods. Therefore, the policy applies to all pods in the default namespace.</para>
</callout>
<callout arearefs="CO59-3">
<para>There are no <literal>ingress</literal> rules specified. This causes incoming traffic to be dropped to all pods.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f deny-by-default.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/deny-by-default created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-networkpolicy-allow-external-clients_creating-network-policy">
<title>Creating a network policy to allow traffic from external clients</title>
<simpara>With the <literal>deny-by-default</literal> policy in place you can proceed to configure a policy that allows traffic from external clients to a pod with the label <literal>app=web</literal>.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<simpara>Follow this procedure to configure a policy that allows external service from the public Internet directly or by using a Load Balancer to access the pod. Traffic is only allowed to a pod with the label <literal>app=web</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy that allows traffic from the public Internet directly or by using a load balancer to access the pod. Save the YAML in the <literal>web-allow-external.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-external
  namespace: default
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: web
  ingress:
    - {}</programlisting>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-external.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/web-allow-external created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<simpara>This policy allows traffic from all resources, including external traffic as illustrated in the following diagram:</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/292_OpenShift_Configuring_multi-network_policy_1122.png"/>
</imageobject>
<textobject><phrase>Allow traffic from external clients</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="nw-networkpolicy-allow-traffic-from-all-applications_creating-network-policy">
<title>Creating a network policy allowing traffic to an application from all namespaces</title>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<simpara>Follow this procedure to configure a policy that allows traffic from all pods in all namespaces to a particular application.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy that allows traffic from all pods in all namespaces to a particular application. Save the YAML in the <literal>web-allow-all-namespaces.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-all-namespaces
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web <co xml:id="CO60-1"/>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {} <co xml:id="CO60-2"/></programlisting>
<calloutlist>
<callout arearefs="CO60-1">
<para>Applies the policy only to <literal>app:web</literal> pods in default namespace.</para>
</callout>
<callout arearefs="CO60-2">
<para>Selects all pods in all namespaces.</para>
</callout>
</calloutlist>
<note>
<simpara>By default, if you omit specifying a <literal>namespaceSelector</literal> it does not select any namespaces, which means the policy allows traffic only from the namespace the network policy is deployed to.</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-all-namespaces.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/web-allow-all-namespaces created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Start a web service in the <literal>default</literal> namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>secondary</literal> namespace and to start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Run the following command in the shell and observe that the request is allowed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-networkpolicy-allow-traffic-from-a-namespace_creating-network-policy">
<title>Creating a network policy allowing traffic to an application from a namespace</title>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<simpara>Follow this procedure to configure a policy that allows traffic to a pod with the label <literal>app=web</literal> from a particular namespace. You might want to do this to:</simpara>
<itemizedlist>
<listitem>
<simpara>Restrict traffic to a production database only to namespaces where production workloads are deployed.</simpara>
</listitem>
<listitem>
<simpara>Enable monitoring tools deployed to a particular namespace to scrape metrics from the current namespace.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy that allows traffic from all pods in a particular namespaces with a label <literal>purpose=production</literal>. Save the YAML in the <literal>web-allow-prod.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-prod
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web <co xml:id="CO61-1"/>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production <co xml:id="CO61-2"/></programlisting>
<calloutlist>
<callout arearefs="CO61-1">
<para>Applies the policy only to <literal>app:web</literal> pods in the default namespace.</para>
</callout>
<callout arearefs="CO61-2">
<para>Restricts traffic to only pods in namespaces that have the label <literal>purpose=production</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-prod.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/web-allow-prod created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Start a web service in the <literal>default</literal> namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to create the <literal>prod</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace prod</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to label the <literal>prod</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace/prod purpose=production</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to create the <literal>dev</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace dev</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to label the <literal>dev</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace/dev purpose=testing</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>dev</literal> namespace and to start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-$RANDOM --namespace=dev --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Run the following command in the shell and observe that the request is blocked:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">wget: download timed out</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>prod</literal> namespace and start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-$RANDOM --namespace=prod --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Run the following command in the shell and observe that the request is allowed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="_additional-resources-2" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/web_console/#web-console">Accessing the web console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="logging-network-policy">Logging for egress firewall and network policy rules</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="viewing-network-policy">
<title>Viewing a network policy</title>

<simpara>As a user with the <literal>admin</literal> role, you can view a network policy for a namespace.</simpara>
<section xml:id="nw-networkpolicy-object_viewing-network-policy">
<title>Example NetworkPolicy object</title>
<simpara>The following annotates an example NetworkPolicy object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <co xml:id="CO62-1"/>
spec:
  podSelector: <co xml:id="CO62-2"/>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <co xml:id="CO62-3"/>
        matchLabels:
          app: app
    ports: <co xml:id="CO62-4"/>
    - protocol: TCP
      port: 27017</programlisting>
<calloutlist>
<callout arearefs="CO62-1">
<para>The name of the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO62-2">
<para>A selector that describes the pods to which the policy applies. The policy object can
only select pods in the project that defines the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO62-3">
<para>A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.</para>
</callout>
<callout arearefs="CO62-4">
<para>A list of one or more destination ports on which to accept traffic.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-networkpolicy-view-cli_viewing-network-policy">
<title>Viewing network policies using the CLI</title>
<simpara>You can examine the network policies in a namespace.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can view any network policy in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace where the network policy exists.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>List network policies in a namespace:</simpara>
<itemizedlist>
<listitem>
<simpara>To view network policy objects defined in a namespace, enter the following
command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networkpolicy</programlisting>
</listitem>
<listitem>
<simpara>Optional: To examine a specific network policy, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the network policy to inspect.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe networkpolicy allow-same-namespace</programlisting>
<formalpara>
<title>Output for <literal>oc describe</literal> command</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name:         allow-same-namespace
Namespace:    ns1
Created on:   2021-05-24 22:28:56 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      PodSelector: &lt;none&gt;
  Not affecting egress traffic
  Policy Types: Ingress</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of viewing a network policy in any namespace in the cluster directly in YAML or from a form in the web console.</simpara>
</note>
</section>
</section>
<section xml:id="editing-network-policy">
<title>Editing a network policy</title>

<simpara>As a user with the <literal>admin</literal> role, you can edit an existing network policy for a namespace.</simpara>
<section xml:id="nw-networkpolicy-edit_editing-network-policy">
<title>Editing a network policy</title>
<simpara>You can edit a network policy in a namespace.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can edit a network policy in any namespace in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace where the network policy exists.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To list the network policy objects in a namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networkpolicy</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Edit the network policy object.</simpara>
<itemizedlist>
<listitem>
<simpara>If you saved the network policy definition in a file, edit the file and make any necessary changes, and then enter the following command.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;namespace&gt; -f &lt;policy_file&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;policy_file&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the file containing the network policy.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>If you need to update the network policy object directly, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the network policy.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Confirm that the network policy object is updated.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the network policy.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of editing a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <emphasis role="strong">Actions</emphasis> menu.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-object_editing-network-policy">
<title>Example NetworkPolicy object</title>
<simpara>The following annotates an example NetworkPolicy object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <co xml:id="CO63-1"/>
spec:
  podSelector: <co xml:id="CO63-2"/>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <co xml:id="CO63-3"/>
        matchLabels:
          app: app
    ports: <co xml:id="CO63-4"/>
    - protocol: TCP
      port: 27017</programlisting>
<calloutlist>
<callout arearefs="CO63-1">
<para>The name of the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO63-2">
<para>A selector that describes the pods to which the policy applies. The policy object can
only select pods in the project that defines the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO63-3">
<para>A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.</para>
</callout>
<callout arearefs="CO63-4">
<para>A list of one or more destination ports on which to accept traffic.</para>
</callout>
</calloutlist>
</section>
<section xml:id="editing-network-policy-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="creating-network-policy">Creating a network policy</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deleting-network-policy">
<title>Deleting a network policy</title>

<simpara>As a user with the <literal>admin</literal> role, you can delete a network policy from a namespace.</simpara>
<section xml:id="nw-networkpolicy-delete-cli_deleting-network-policy">
<title>Deleting a network policy using the CLI</title>
<simpara>You can delete a network policy in a namespace.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can delete any network policy in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace where the network policy exists.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To delete a network policy object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the network policy.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">networkpolicy.networking.k8s.io/default-deny deleted</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of deleting a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <emphasis role="strong">Actions</emphasis> menu.</simpara>
</note>
</section>
</section>
<section xml:id="default-network-policy">
<title>Defining a default network policy for projects</title>

<simpara>As a cluster administrator, you can modify the new project template to
automatically include network policies when you create a new project.
If you do not yet have a customized template for new projects, you must first create one.</simpara>
<section xml:id="modifying-template-for-new-projects_default-network-policy">
<title>Modifying the template for new projects</title>
<simpara>As a cluster administrator, you can modify the default project template so that
new projects are created using your custom requirements.</simpara>
<simpara>To create your own custom project template:</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Generate the default project template:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm create-bootstrap-project-template -o yaml &gt; template.yaml</programlisting>
</listitem>
<listitem>
<simpara>Use a text editor to modify the generated <literal>template.yaml</literal> file by adding
objects or modifying existing objects.</simpara>
</listitem>
<listitem>
<simpara>The project template must be created in the <literal>openshift-config</literal> namespace. Load
your modified template:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f template.yaml -n openshift-config</programlisting>
</listitem>
<listitem>
<simpara>Edit the project configuration resource using the web console or CLI.</simpara>
<itemizedlist>
<listitem>
<simpara>Using the web console:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Cluster Settings</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Configuration</emphasis> to view all configuration resources.</simpara>
</listitem>
<listitem>
<simpara>Find the entry for <emphasis role="strong">Project</emphasis> and click <emphasis role="strong">Edit YAML</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Using the CLI:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Edit the <literal>project.config.openshift.io/cluster</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit project.config.openshift.io/cluster</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Update the <literal>spec</literal> section to include the <literal>projectRequestTemplate</literal> and <literal>name</literal>
parameters, and set the name of your uploaded project template. The default name
is <literal>project-request</literal>.</simpara>
<formalpara>
<title>Project configuration resource with custom project template</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Project
metadata:
  ...
spec:
  projectRequestTemplate:
    name: &lt;template_name&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>After you save your changes, create a new project to verify that your changes
were successfully applied.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-networkpolicy-project-defaults_default-network-policy">
<title>Adding network policies to the new project template</title>
<simpara>As a cluster administrator, you can add network policies to the default template for new projects.
OpenShift Container Platform will automatically create all the <literal>NetworkPolicy</literal> objects specified in the template in the project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a default CNI network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You must have created a custom default project template for new projects.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the default template for a new project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit template &lt;project_template&gt; -n openshift-config</programlisting>
<simpara>Replace <literal>&lt;project_template&gt;</literal> with the name of the default template that you
configured for your cluster. The default template name is <literal>project-request</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the template, add each <literal>NetworkPolicy</literal> object as an element to the <literal>objects</literal> parameter. The <literal>objects</literal> parameter accepts a collection of one or more objects.</simpara>
<simpara>In the following example, the <literal>objects</literal> parameter collection includes several <literal>NetworkPolicy</literal> objects.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">objects:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-same-namespace
  spec:
    podSelector: {}
    ingress:
    - from:
      - podSelector: {}
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-openshift-ingress
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
    podSelector: {}
    policyTypes:
    - Ingress
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-kube-apiserver-operator
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: openshift-kube-apiserver-operator
        podSelector:
          matchLabels:
            app: kube-apiserver-operator
    policyTypes:
    - Ingress
...</programlisting>
</listitem>
<listitem>
<simpara>Optional: Create a new project to confirm that your network policy objects are created successfully by running the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a new project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project &lt;project&gt; <co xml:id="CO64-1"/></programlisting>
<calloutlist>
<callout arearefs="CO64-1">
<para>Replace <literal>&lt;project&gt;</literal> with the name for the project you are creating.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Confirm that the network policy objects in the new project template exist in the new project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networkpolicy
NAME                           POD-SELECTOR   AGE
allow-from-openshift-ingress   &lt;none&gt;         7s
allow-from-same-namespace      &lt;none&gt;         7s</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="multitenant-network-policy">
<title>Configuring multitenant isolation with network policy</title>

<simpara>As a cluster administrator, you can configure your network policies to provide multitenant network isolation.</simpara>
<note>
<simpara>If you are using the OpenShift SDN network plugin, configuring network policies as described in this section provides network isolation similar to multitenant mode but with network policy mode set.</simpara>
</note>
<section xml:id="nw-networkpolicy-multitenant-isolation_multitenant-network-policy">
<title>Configuring multitenant isolation by using network policy</title>
<simpara>You can configure your project to isolate it from pods and services in other
project namespaces.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.
This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>NetworkPolicy</literal> objects:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>A policy named <literal>allow-from-openshift-ingress</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""
  podSelector: {}
  policyTypes:
  - Ingress
EOF</programlisting>
<note>
<simpara><literal>policy-group.network.openshift.io/ingress: ""</literal> is the preferred namespace selector label for OpenShift SDN. You can use the <literal>network.openshift.io/policy-group: ingress</literal> namespace selector label, but this is a legacy label.</simpara>
</note>
</listitem>
<listitem>
<simpara>A policy named <literal>allow-from-openshift-monitoring</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
EOF</programlisting>
</listitem>
<listitem>
<simpara>A policy named <literal>allow-same-namespace</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
EOF</programlisting>
</listitem>
<listitem>
<simpara>A policy named <literal>allow-from-kube-apiserver-operator</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-kube-apiserver-operator
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: openshift-kube-apiserver-operator
      podSelector:
        matchLabels:
          app: kube-apiserver-operator
  policyTypes:
  - Ingress
EOF</programlisting>
<simpara>For more details, see <link xlink:href="https://access.redhat.com/solutions/6964520">New <literal>kube-apiserver-operator</literal> webhook controller validating health of webhook</link>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: To confirm that the network policies exist in your current project, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe networkpolicy</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name:         allow-from-openshift-ingress
Namespace:    example1
Created on:   2020-06-09 00:28:17 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: ingress
  Not affecting egress traffic
  Policy Types: Ingress


Name:         allow-from-openshift-monitoring
Namespace:    example1
Created on:   2020-06-09 00:29:57 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: monitoring
  Not affecting egress traffic
  Policy Types: Ingress</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="multitenant-network-policy-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="default-network-policy">Defining a default network policy</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="multitenant-network-policy-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-openshift-sdn-modes_about-openshift-sdn">OpenShift SDN network isolation modes</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="cidr-range-definitions">
<title>CIDR range definitions</title>

<simpara>You must specify non-overlapping ranges for the following CIDR ranges.</simpara>
<note>
<simpara>Machine CIDR ranges cannot be changed after creating your cluster.</simpara>
</note>
<important>
<simpara>OVN-Kubernetes, the default network provider in OpenShift Container Platform 4.11 and later, uses the <literal>100.64.0.0/16</literal> IP address range internally. If your cluster uses OVN-Kubernetes, do not include the <literal>100.64.0.0/16</literal> IP address range in any other CIDR definitions in your cluster.</simpara>
</important>
<section xml:id="machine-cidr-description">
<title>Machine CIDR</title>
<simpara>In the Machine CIDR field, you must specify the IP address range for machines or cluster nodes.</simpara>
<simpara>The default is <literal>10.0.0.0/16</literal>. This range must not conflict with any connected networks.</simpara>
</section>
<section xml:id="service-cidr-description">
<title>Service CIDR</title>
<simpara>In the Service CIDR field, you must specify the IP address range for services.
The range must be large enough to accommodate your workload. The address block must not overlap with any external service accessed from within the cluster. The default is <literal>172.30.0.0/16</literal>.</simpara>
</section>
<section xml:id="pod-cidr-description">
<title>Pod CIDR</title>
<simpara>In the pod CIDR field, you must specify the IP address range for pods.</simpara>
<simpara>The pod CIDR is the same as the <literal>clusterNetwork</literal> CIDR and the cluster CIDR.
The range must be large enough to accommodate your workload. The address block must not overlap with any external service accessed from within the cluster. The default is <literal>10.128.0.0/14</literal>.
You can expand the range after cluster installation.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-operator-cr-cno-object_cluster-network-operator">Cluster Network Operator Configuration</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-cluster-network-range">Configuring the cluster network range</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="host-prefix-description">
<title>Host Prefix</title>
<simpara>In the Host Prefix field, you must specify the subnet prefix length assigned to pods scheduled to individual machines. The host prefix determines the pod IP address pool for each machine.</simpara>
<simpara>For example, if the host prefix is set to <literal>/23</literal>, each machine is assigned a <literal>/23</literal> subnet from the pod CIDR address range. The default is <literal>/23</literal>, allowing 510 cluster nodes, and 510 pod IP addresses per node.</simpara>
</section>
</chapter>
<chapter xml:id="_aws-load-balancer-operator">
<title>AWS Load Balancer Operator</title>
<section xml:id="aws-load-balancer-operator-release-notes">
<title>AWS Load Balancer Operator release notes</title>

<simpara>The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <literal>AWSLoadBalancerController</literal> resource.</simpara>
<simpara>These release notes track the development of the AWS Load Balancer Operator in OpenShift Container Platform.</simpara>
<simpara>For an overview of the AWS Load Balancer Operator, see <link linkend="aws-load-balancer-operator">AWS Load Balancer Operator in OpenShift Container Platform</link>.</simpara>
<section xml:id="aws-load-balancer-operator-release-notes-1.1.1">
<title>AWS Load Balancer Operator 1.1.1</title>
<simpara>The following advisory is available for the AWS Load Balancer Operator version 1.1.1:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2024:0555">RHEA-2024:0555 Release of AWS Load Balancer Operator 1.1.z on OperatorHub</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="aws-load-balancer-operator-release-notes-1.1.0">
<title>AWS Load Balancer Operator 1.1.0</title>
<simpara>The AWS Load Balancer Operator version 1.1.0 supports the AWS Load Balancer Controller version 2.4.4.</simpara>
<simpara>The following advisory is available for the AWS Load Balancer Operator version 1.1.0:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2023:6218">RHEA-2023:6218 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="aws-load-balancer-operator-1.1.0-notable-changes">
<title>Notable changes</title>
<itemizedlist>
<listitem>
<simpara>This release uses the Kubernetes API version 0.27.2.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="aws-load-balancer-operator-1.1.0-new-features">
<title>New features</title>
<itemizedlist>
<listitem>
<simpara>The AWS Load Balancer Operator now supports a standardized Security Token Service (STS) flow by using the Cloud Credential Operator.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="aws-load-balancer-operator-1.1.0-bug-fixes">
<title>Bug fixes</title>
<itemizedlist>
<listitem>
<simpara>A FIPS-compliant cluster must use TLS version 1.2. Previously, webhooks for the AWS Load Balancer Controller only accepted TLS 1.3 as the minimum version, resulting in an error such as the following on a FIPS-compliant cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">remote error: tls: protocol version not supported</programlisting>
<simpara>Now, the AWS Load Balancer Controller accepts TLS 1.2 as the minimum TLS version, resolving this issue. (<link xlink:href="https://issues.redhat.com/browse/OCPBUGS-14846"><emphasis role="strong">OCPBUGS-14846</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="aws-load-balancer-operator-release-notes-1.0.1">
<title>AWS Load Balancer Operator 1.0.1</title>
<simpara>The following advisory is available for the AWS Load Balancer Operator version 1.0.1:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2024:0556">Release of AWS Load Balancer Operator 1.0.1 on OperatorHub</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="aws-load-balancer-operator-release-notes-1.0.0">
<title>AWS Load Balancer Operator 1.0.0</title>
<simpara>The AWS Load Balancer Operator is now generally available with this release. The AWS Load Balancer Operator version 1.0.0 supports the AWS Load Balancer Controller version 2.4.4.</simpara>
<simpara>The following advisory is available for the AWS Load Balancer Operator version 1.0.0:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2023:1954">RHEA-2023:1954 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="aws-load-balancer-operator-1.0.0-notable-changes">
<title>Notable changes</title>
<itemizedlist>
<listitem>
<simpara>This release uses the new <literal>v1</literal> API version.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="aws-load-balancer-operator-1.0.0-bug-fixes">
<title>Bug fixes</title>
<itemizedlist>
<listitem>
<simpara>Previously, the controller provisioned by the AWS Load Balancer Operator did not properly use the configuration for the cluster-wide proxy.
These settings are now applied appropriately to the controller.
(<link xlink:href="https://issues.redhat.com/browse/OCPBUGS-4052"><emphasis role="strong">OCPBUGS-4052</emphasis></link>, <link xlink:href="https://issues.redhat.com/browse/OCPBUGS-5295"><emphasis role="strong">OCPBUGS-5295</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="aws-load-balancer-operator-release-notes-earlier-versions">
<title>Earlier versions</title>
<simpara>The two earliest versions of the AWS Load Balancer Operator are available as a Technology Preview.
These versions should not be used in a production cluster.
For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
<simpara>The following advisory is available for the AWS Load Balancer Operator version 0.2.0:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2022:9084">RHEA-2022:9084 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</link></simpara>
</listitem>
</itemizedlist>
<simpara>The following advisory is available for the AWS Load Balancer Operator version 0.0.1:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/errata/RHEA-2022:5780">RHEA-2022:5780 Release of AWS Load Balancer Operator on OperatorHub Enhancement Advisory Update</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="aws-load-balancer-operator">
<title>AWS Load Balancer Operator in OpenShift Container Platform</title>

<simpara>The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <literal>aws-load-balancer-controller</literal>. You can install the ALB Operator from the OperatorHub by using OpenShift Container Platform web console or CLI.</simpara>
<section xml:id="nw-aws-load-balancer-operator-considerations_aws-load-balancer-operator">
<title>AWS Load Balancer Operator considerations</title>
<simpara>Review the following limitations before installing and using the AWS Load Balancer Operator.</simpara>
<itemizedlist>
<listitem>
<simpara>The IP traffic mode only works on AWS Elastic Kubernetes Service (EKS). The AWS Load Balancer Operator disables the IP traffic mode for the AWS Load Balancer Controller. As a result of disabling the IP traffic mode, the AWS Load Balancer Controller cannot use the pod readiness gate.</simpara>
</listitem>
<listitem>
<simpara>The AWS Load Balancer Operator adds command-line flags such as <literal>--disable-ingress-class-annotation</literal> and <literal>--disable-ingress-group-name-annotation</literal> to the AWS Load Balancer Controller. Therefore, the AWS Load Balancer Operator does not allow using the <literal>kubernetes.io/ingress.class</literal> and <literal>alb.ingress.kubernetes.io/group.name</literal> annotations in the <literal>Ingress</literal> resource.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-aws-load-balancer-operator_aws-load-balancer-operator">
<title>AWS Load Balancer Operator</title>
<simpara>The AWS Load Balancer Operator can tag the public subnets if the <literal>kubernetes.io/role/elb</literal> tag is missing. Also, the AWS Load Balancer Operator detects the following from the underlying AWS cloud:</simpara>
<itemizedlist>
<listitem>
<simpara>The ID of the virtual private cloud (VPC) on which the cluster hosting the Operator is deployed in.</simpara>
</listitem>
<listitem>
<simpara>Public and private subnets of the discovered VPC.</simpara>
</listitem>
</itemizedlist>
<simpara>The AWS Load Balancer Operator supports the Kubernetes service resource of type <literal>LoadBalancer</literal> by using Network Load Balancer (NLB) with the <literal>instance</literal> target type only.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>You can deploy the AWS Load Balancer Operator on demand from the OperatorHub, by creating a <literal>Subscription</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator get sub aws-load-balancer-operator --template='{{.status.installplan.name}}{{"\n"}}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">install-zlfbt</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the status of an install plan. The status of an install plan must be <literal>Complete</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator get ip &lt;install_plan_name&gt; --template='{{.status.phase}}{{"\n"}}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Complete</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>oc get</literal> command to view the <literal>Deployment</literal> status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n aws-load-balancer-operator deployment/aws-load-balancer-operator-controller-manager</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                           READY     UP-TO-DATE   AVAILABLE   AGE
aws-load-balancer-operator-controller-manager  1/1       1            1           23h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-aws-load-balancer-operator-logs_aws-load-balancer-operator">
<title>AWS Load Balancer Operator logs</title>
<simpara>Use the <literal>oc logs</literal> command to view the AWS Load Balancer Operator logs.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>View the logs of the AWS Load Balancer Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n aws-load-balancer-operator deployment/aws-load-balancer-operator-controller-manager -c manager</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-aws-load-balancer-operator">
<title>Understanding AWS Load Balancer Operator</title>

<simpara>The AWS Load Balancer (ALB) Operator deploys and manages an instance of the <literal>aws-load-balancer-controller</literal> resource. You can install the AWS Load Balancer Operator from the OperatorHub by using OpenShift Container Platform web console or CLI.</simpara>
<section xml:id="nw-installing-aws-load-balancer-operator_aws-load-balancer-operator">
<title>Installing the AWS Load Balancer Operator</title>
<simpara>You can install the AWS Load Balancer Operator from the OperatorHub by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have logged in to the OpenShift Container Platform web console as a user with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>Your cluster is configured with AWS as the platform type and cloud provider.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">OperatorHub</emphasis> in the OpenShift Container Platform web console.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">AWS Load Balancer Operator</emphasis>. You can use the <emphasis role="strong">Filter by keyword</emphasis> text box or use the filter list to search for the AWS Load Balancer Operator from the list of Operators.</simpara>
</listitem>
<listitem>
<simpara>Select the <literal>aws-load-balancer-operator</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>Follow the instructions to prepare the Operator for installation.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">AWS Load Balancer Operator</emphasis> page, click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, select the following options:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara><emphasis role="strong">Update the channel</emphasis> as <emphasis role="strong">stable-v1</emphasis>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Installation mode</emphasis> as <emphasis role="strong">A specific namespace on the cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Installed Namespace</emphasis> as <literal>aws-load-balancer-operator</literal>. If the <literal>aws-load-balancer-operator</literal> namespace does not exist, it gets created during the Operator installation.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Update approval</emphasis> as <emphasis role="strong">Automatic</emphasis> or <emphasis role="strong">Manual</emphasis>. By default, the <emphasis role="strong">Update approval</emphasis> is set to <emphasis role="strong">Automatic</emphasis>. If you select automatic updates, the Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without any intervention. If you select manual updates, the OLM creates an update request. As a cluster administrator, you must then manually approve that update request to update the Operator updated to the new version.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the AWS Load Balancer Operator shows the <emphasis role="strong">Status</emphasis> as <emphasis role="strong">Succeeded</emphasis> on the Installed Operators dashboard.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="albo-sts-cluster">
<title>Installing AWS Load Balancer Operator on a Security Token Service cluster</title>

<simpara>You can install the AWS Load Balancer (ALB) Operator on a Security Token Service (STS) cluster.</simpara>
<simpara>The ALB Operator relies on a <literal>CredentialsRequest</literal> to bootstrap the Operator and for the <literal>AWSLoadBalancerController</literal> instance. The ALB Operator waits until the required secrets are created and available.</simpara>
<section xml:id="creating-iam-role-albo-operator_albo-sts-cluster">
<title>Creating an IAM role for the AWS Load Balancer Operator</title>
<simpara>An additional IAM role is required to successfully install the ALB Operator on a Security Token Service (STS) cluster. The IAM role is required to interact with subnets and Virtual Private Clouds (VPCs). The Operator generates a <literal>CredentialsRequest</literal> with this role to bootstrap itself.</simpara>
<simpara>There are two options for creating the IAM role:</simpara>
<itemizedlist>
<listitem>
<simpara>Using <literal>ccoctl</literal> and a predefined <literal>CredentialsRequest</literal>.</simpara>
</listitem>
<listitem>
<simpara>Using the AWS CLI and predefined AWS manifests.</simpara>
</listitem>
</itemizedlist>
<simpara>Use the AWS CLI if your environment does not support the <literal>ccoctl</literal> command.</simpara>
<section xml:id="using-ccoctl-create-iam-role-alb-operator_albo-sts-cluster">
<title>Using ccoctl to create an IAM role for the Operator</title>
<simpara>You can use the <literal>ccoctl</literal> binary to create an IAM role for the AWS Load Balancer (ALB) Operator. The created IAM role is used to interact with subnets and Virtual Private Clouds (VPCs).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must extract and prepare the <literal>ccoctl</literal> binary.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Download the <literal>CredentialsRequest</literal> custom resource (CR) of the ALB Operator and create a directory to store it in by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --create-dirs -o &lt;credrequests-dir&gt;/operator.yaml https://raw.githubusercontent.com/openshift/aws-load-balancer-operator/main/hack/operator-credentials-request.yaml</programlisting>
</listitem>
<listitem>
<simpara>Use <literal>ccoctl</literal> to create an IAM role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ccoctl aws create-iam-roles \
    --name &lt;name&gt; \
    --region=&lt;aws_region&gt; \
    --credentials-requests-dir=&lt;credrequests-dir&gt; \
    --identity-provider-arn &lt;oidc-arn&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2023/09/12 11:38:57 Role arn:aws:iam::777777777777:role/&lt;name&gt;-aws-load-balancer-operator-aws-load-balancer-operator created <co xml:id="CO65-1"/>
2023/09/12 11:38:57 Saved credentials configuration to: /home/user/&lt;credrequests-dir&gt;/manifests/aws-load-balancer-operator-aws-load-balancer-operator-credentials.yaml
2023/09/12 11:38:58 Updated Role policy for Role &lt;name&gt;-aws-load-balancer-operator-aws-load-balancer-operator created</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO65-1">
<para>Note the ARN of the created IAM role.</para>
</callout>
</calloutlist>
<note>
<simpara>The length of the role name must be less than or equal to 12 characters.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="using-aws-cli-create-iam-role-alb-operator_albo-sts-cluster">
<title>Using the AWS CLI to create an IAM role for the Operator</title>
<simpara>You can use the <literal>aws</literal> command line interface to create an IAM role for the AWS Load Balancer (ALB) Operator. The created IAM role is used to interact with subnets and Virtual Private Clouds (VPCs).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to the <literal>aws</literal> command line interface.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Generate a trust policy file using your identity provider by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF &gt; albo-operator-trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::777777777777:oidc-provider/&lt;oidc-provider-id&gt;" <co xml:id="CO66-1"/>
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "&lt;oidc-provider-id&gt;:sub": "system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-operator-controller-manager" <co xml:id="CO66-2"/>
                }
            }
        }
    ]
}
EOF</programlisting>
<calloutlist>
<callout arearefs="CO66-1">
<para>Specifies the ARN of the identity provider.</para>
</callout>
<callout arearefs="CO66-2">
<para>Specifies the service account for the Operator.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the IAM role with the generated trust policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam create-role --role-name albo-operator --assume-role-policy-document file://albo-operator-trusted-policy.json</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ROLE	arn:aws:iam::777777777777:role/albo-operator	2023-08-02T12:13:22Z <co xml:id="CO67-1"/>
ASSUMEROLEPOLICYDOCUMENT	2012-10-17
STATEMENT	sts:AssumeRoleWithWebIdentity	Allow
STRINGEQUALS	system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-controller-manager
PRINCIPAL	arn:aws:iam:777777777777:oidc-provider/&lt;oidc-provider-id&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO67-1">
<para>Note the ARN of the created IAM role.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Download the permission policy for the ALB Operator by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -o albo-controller-permission-policy.json https://raw.githubusercontent.com/openshift/aws-load-balancer-operator/main/assets/iam-policy.json</programlisting>
</listitem>
<listitem>
<simpara>Attach the permission policy for the <literal>AWSLoadBalancerController</literal> to the IAM role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam put-role-policy --role-name albo-controller --policy-name perms-policy-albo-controller --policy-document file://albo-controller-permission-policy.json</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="specifying-role-arn-albo-sts_albo-sts-cluster">
<title>Specifying the role ARN for the ALB Operator on an STS cluster</title>
<simpara>The role Amazon Resource Name (ARN) needs to be passed to the AWS Load Balancer (ALB) Operator as an environment variable. You can use the dedicated input box in the OperatorHub web UI or specify it in the <literal>Subscription</literal> resource when installing the Operator by using the OpenShift CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>aws-load-balancer-operator</literal> project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project aws-load-balancer-operator</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>OperatorGroup</literal> for the ALB Operator by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: aws-load-balancer-operator
  namespace: aws-load-balancer-operator
spec:
  targetNamespaces: []
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Subscription</literal> object for the ALB Operator with the role ARN by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: aws-load-balancer-operator
  namespace: aws-load-balancer-operator
spec:
  channel: stable-v1
  name: aws-load-balancer-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  config:
    env:
    - name: ROLEARN
      value: "&lt;role-arn&gt;" <co xml:id="CO68-1"/>
EOF</programlisting>
<calloutlist>
<callout arearefs="CO68-1">
<para>Specifies the role ARN to be used in the <literal>CredentialsRequest</literal> to provision the AWS credentials for the Operator.</para>
</callout>
</calloutlist>
<note>
<simpara>The ALB Operator waits until the creation of the required secret before moving to the available state.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-iam-role-albo-controller_albo-sts-cluster">
<title>Creating an IAM role for the AWS Load Balancer Controller</title>
<simpara>The <literal>CredentialsRequest</literal> for the AWS Load Balancer Controller must be set with a manually provisioned IAM role.</simpara>
<simpara>There are two options for creating the IAM role:</simpara>
<itemizedlist>
<listitem>
<simpara>Using <literal>ccoctl</literal> and a predefined <literal>CredentialsRequest</literal>.</simpara>
</listitem>
<listitem>
<simpara>Using the AWS CLI and predefined AWS manifests.</simpara>
</listitem>
</itemizedlist>
<simpara>Use the AWS CLI if your environment does not support the <literal>ccoctl</literal> command.</simpara>
<section xml:id="using-ccoctl-create-iam-role-alb-controller_albo-sts-cluster">
<title>Using ccoctl to create an IAM role for the Controller</title>
<simpara>You can use the <literal>ccoctl</literal> binary to create an IAM role for the <literal>AWSLoadBalancerController</literal>. The created IAM role is used to interact with subnets and Virtual Private Clouds (VPCs).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must extract and prepare the <literal>ccoctl</literal> binary.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Download the <literal>CredentialsRequest</literal> custom resource (CR) of the AWS Load Balancer Operator and create a directory to store it in by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --create-dirs -o &lt;credrequests-dir&gt;/controller.yaml https://raw.githubusercontent.com/openshift/aws-load-balancer-operator/main/hack/controller/controller-credentials-request.yaml</programlisting>
</listitem>
<listitem>
<simpara>Use <literal>ccoctl</literal> to create an IAM role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ccoctl aws create-iam-roles \
    --name &lt;name&gt; \
    --region=&lt;aws_region&gt; \
    --credentials-requests-dir=&lt;credrequests-dir&gt; \
    --identity-provider-arn &lt;oidc-arn&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2023/09/12 11:38:57 Role arn:aws:iam::777777777777:role/&lt;name&gt;-aws-load-balancer-operator-aws-load-balancer-controller created <co xml:id="CO69-1"/>
2023/09/12 11:38:57 Saved credentials configuration to: /home/user/&lt;credrequests-dir&gt;/manifests/aws-load-balancer-operator-aws-load-balancer-controller-credentials.yaml
2023/09/12 11:38:58 Updated Role policy for Role &lt;name&gt;-aws-load-balancer-operator-aws-load-balancer-controller created</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO69-1">
<para>Note the ARN of the created IAM role.</para>
</callout>
</calloutlist>
<note>
<simpara>The length of the role name must be less than or equal to 12 characters.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="using-aws-cli-create-iam-role-alb-controller_albo-sts-cluster">
<title>Using the AWS CLI to create an IAM role for the Controller</title>
<simpara>You can use the <literal>aws</literal> command line interface to create an IAM role for the <literal>AWSLoadBalancerController</literal>. The created IAM role is used to interact with subnets and Virtual Private Clouds (VPCs).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to the <literal>aws</literal> command line interface.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Generate a trust policy file using your identity provider by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF &gt; albo-controller-trust-policy.json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Federated": "arn:aws:iam::777777777777:oidc-provider/&lt;oidc-provider-id&gt;" <co xml:id="CO70-1"/>
            },
            "Action": "sts:AssumeRoleWithWebIdentity",
            "Condition": {
                "StringEquals": {
                    "&lt;oidc-provider-id&gt;:sub": "system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-controller-cluster" <co xml:id="CO70-2"/>
                }
            }
        }
    ]
}
EOF</programlisting>
<calloutlist>
<callout arearefs="CO70-1">
<para>Specifies the ARN of the identity provider.</para>
</callout>
<callout arearefs="CO70-2">
<para>Specifies the service account for the <literal>AWSLoadBalancerController</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the IAM role with the generated trust policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam create-role --role-name albo-controller --assume-role-policy-document file://albo-controller-trusted-policy.json</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ROLE	arn:aws:iam::777777777777:role/albo-controller	2023-08-02T12:13:22Z <co xml:id="CO71-1"/>
ASSUMEROLEPOLICYDOCUMENT	2012-10-17
STATEMENT	sts:AssumeRoleWithWebIdentity	Allow
STRINGEQUALS	system:serviceaccount:aws-load-balancer-operator:aws-load-balancer-controller-cluster
PRINCIPAL	arn:aws:iam:777777777777:oidc-provider/&lt;oidc-provider-id&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO71-1">
<para>Note the ARN of the created IAM role.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Download the permission policy for the <literal>AWSLoadBalancerController</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -o albo-controller-permission-policy.json https://raw.githubusercontent.com/openshift/aws-load-balancer-operator/main/assets/iam-policy.json</programlisting>
</listitem>
<listitem>
<simpara>Attach the permission policy for the <literal>AWSLoadBalancerController</literal> to the IAM role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam put-role-policy --role-name albo-controller --policy-name perms-policy-albo-controller --policy-document file://albo-controller-permission-policy.json</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>AWSLoadBalancerController</literal> resource file named <literal>example-sts-iam-role.yaml</literal> with contents such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController <co xml:id="CO72-1"/>
metadata:
  name: cluster <co xml:id="CO72-2"/>
spec:
  credentialsRequestConfig:
    stsIAMRoleARN: &lt;role-arn&gt; <co xml:id="CO72-3"/></programlisting>
<calloutlist>
<callout arearefs="CO72-1">
<para>Defines the <literal>AWSLoadBalancerController</literal> resource.</para>
</callout>
<callout arearefs="CO72-2">
<para>Defines the instance name for the <literal>AWSLoadBalancerController</literal>. All related resources use this instance name as a suffix.</para>
</callout>
<callout arearefs="CO72-3">
<para>Specifies the role ARN to be used in a <literal>CredentialsRequest</literal> to provision the AWS credentials for the controller.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="_additional-resources-3" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#cco-ccoctl-configuring_installing-aws-customizations">Configuring the Cloud Credential Operator utility</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-create-instance-aws-load-balancer">
<title>Creating an instance of AWS Load Balancer Controller</title>

<simpara>After installing the Operator, you can create an instance of the AWS Load Balancer Controller.</simpara>
<section xml:id="nw-creating-instance-aws-load-balancer-controller_create-instance-aws-load-balancer">
<title>Creating an instance of the AWS Load Balancer Controller using AWS Load Balancer Operator</title>
<simpara>You can install only a single instance of the <literal>aws-load-balancer-controller</literal> in a cluster. You can create the AWS Load Balancer Controller by using CLI. The AWS Load Balancer(ALB) Operator reconciles only the resource with the name <literal>cluster</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created the <literal>echoserver</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>aws-load-balancer-controller</literal> resource YAML file, for example, <literal>sample-aws-lb.yaml</literal>, as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController <co xml:id="CO73-1"/>
metadata:
  name: cluster <co xml:id="CO73-2"/>
spec:
  subnetTagging: Auto <co xml:id="CO73-3"/>
  additionalResourceTags: <co xml:id="CO73-4"/>
  - key: example.org/security-scope
    value: staging
  ingressClass: alb <co xml:id="CO73-5"/>
  config:
    replicas: 2 <co xml:id="CO73-6"/>
  enabledAddons: <co xml:id="CO73-7"/>
    - AWSWAFv2 <co xml:id="CO73-8"/></programlisting>
<calloutlist>
<callout arearefs="CO73-1">
<para>Defines the <literal>aws-load-balancer-controller</literal> resource.</para>
</callout>
<callout arearefs="CO73-2">
<para>Defines the AWS Load Balancer Controller instance name. This instance name gets added as a suffix to all related resources.</para>
</callout>
<callout arearefs="CO73-3">
<para>Valid options are <literal>Auto</literal> and <literal>Manual</literal>. When the value is set to <literal>Auto</literal>, the Operator attempts to determine the subnets that belong to the cluster and tags them appropriately. The Operator cannot determine the role correctly if the internal subnet tags are not present on internal subnet. If you installed your cluster on user-provided infrastructure, you can manually tag the subnets with the appropriate role tags and set the subnet tagging policy to <literal>Manual</literal>.</para>
</callout>
<callout arearefs="CO73-4">
<para>Defines the tags used by the controller when it provisions AWS resources.</para>
</callout>
<callout arearefs="CO73-5">
<para>The default value for this field is <literal>alb</literal>. The Operator provisions an <literal>IngressClass</literal> resource with the same name if it does not exist.</para>
</callout>
<callout arearefs="CO73-6">
<para>Specifies the number of replicas of the controller.</para>
</callout>
<callout arearefs="CO73-7">
<para>Specifies add-ons for AWS load balancers, which get specified through annotations.</para>
</callout>
<callout arearefs="CO73-8">
<para>Enables the <literal>alb.ingress.kubernetes.io/wafv2-acl-arn</literal> annotation.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>aws-load-balancer-controller</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sample-aws-lb.yaml</programlisting>
</listitem>
<listitem>
<simpara>After the AWS Load Balancer Controller is running, create a <literal>deployment</literal> resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment <co xml:id="CO74-1"/>
metadata:
  name: &lt;echoserver&gt; <co xml:id="CO74-2"/>
  namespace: echoserver
spec:
  selector:
    matchLabels:
      app: echoserver
  replicas: 3 <co xml:id="CO74-3"/>
  template:
    metadata:
      labels:
        app: echoserver
    spec:
      containers:
        - image: openshift/origin-node
          command:
           - "/bin/socat"
          args:
            - TCP4-LISTEN:8080,reuseaddr,fork
            - EXEC:'/bin/bash -c \"printf \\\"HTTP/1.0 200 OK\r\n\r\n\\\"; sed -e \\\"/^\r/q\\\"\"'
          imagePullPolicy: Always
          name: echoserver
          ports:
            - containerPort: 8080</programlisting>
<calloutlist>
<callout arearefs="CO74-1">
<para>Defines the deployment resource.</para>
</callout>
<callout arearefs="CO74-2">
<para>Specifies the deployment name.</para>
</callout>
<callout arearefs="CO74-3">
<para>Specifies the number of replicas of the deployment.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>service</literal> resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service <co xml:id="CO75-1"/>
metadata:
  name: &lt;echoserver&gt; <co xml:id="CO75-2"/>
  namespace: echoserver
spec:
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
  type: NodePort
  selector:
    app: echoserver</programlisting>
<calloutlist>
<callout arearefs="CO75-1">
<para>Defines the service resource.</para>
</callout>
<callout arearefs="CO75-2">
<para>Specifies the name of the service.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Deploy an ALB-backed <literal>Ingress</literal> resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: Ingress <co xml:id="CO76-1"/>
metadata:
  name: &lt;echoserver&gt; <co xml:id="CO76-2"/>
  namespace: echoserver
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Exact
            backend:
              service:
                name: &lt;echoserver&gt; <co xml:id="CO76-3"/>
                port:
                  number: 80</programlisting>
<calloutlist>
<callout arearefs="CO76-1">
<para>Defines the ingress resource.</para>
</callout>
<callout arearefs="CO76-2">
<para>Specifies the name of the ingress resource.</para>
</callout>
<callout arearefs="CO76-3">
<para>Specifies the name of the service resource.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify the status of the <literal>Ingress</literal> resource to show the host of the provisioned AWS Load Balancer (ALB) by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ HOST=$(oc get ingress -n echoserver echoserver --template='{{(index .status.loadBalancer.ingress 0).hostname}}')</programlisting>
</listitem>
<listitem>
<simpara>Verify the status of the provisioned AWS Load Balancer (ALB) host by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl $HOST</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-multiple-ingress-through-single-alb">
<title>Creating multiple ingresses</title>

<simpara>You can route the traffic to different services that are part of a single domain through a single AWS Load Balancer (ALB). Each Ingress resource provides different endpoints of the domain.</simpara>
<section xml:id="nw-creating-multiple-ingress-through-single-alb_multiple-ingress-through-single-alb">
<title>Creating multiple ingresses through a single AWS Load Balancer</title>
<simpara>You can route the traffic to multiple Ingresses through a single AWS Load Balancer (ALB) by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have an access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>IngressClassParams</literal> resource YAML file, for example, <literal>sample-single-lb-params.yaml</literal>, as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: elbv2.k8s.aws/v1beta1 <co xml:id="CO77-1"/>
kind: IngressClassParams
metadata:
  name: single-lb-params <co xml:id="CO77-2"/>
spec:
  group:
    name: single-lb <co xml:id="CO77-3"/></programlisting>
<calloutlist>
<callout arearefs="CO77-1">
<para>Defines the API group and version of the <literal>IngressClassParams</literal> resource.</para>
</callout>
<callout arearefs="CO77-2">
<para>Specifies the name of the <literal>IngressClassParams</literal> resource.</para>
</callout>
<callout arearefs="CO77-3">
<para>Specifies the name of the <literal>IngressGroup</literal>. All Ingresses of this class belong to this <literal>IngressGroup</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an <literal>IngressClassParams</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sample-single-lb-params.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>IngressClass</literal> resource YAML file, for example, <literal>sample-single-lb-class.yaml</literal>, as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1 <co xml:id="CO78-1"/>
kind: IngressClass
metadata:
  name: single-lb <co xml:id="CO78-2"/>
spec:
  controller: ingress.k8s.aws/alb <co xml:id="CO78-3"/>
  parameters:
    apiGroup: elbv2.k8s.aws <co xml:id="CO78-4"/>
    kind: IngressClassParams <co xml:id="CO78-5"/>
    name: single-lb-params <co xml:id="CO78-6"/></programlisting>
<calloutlist>
<callout arearefs="CO78-1">
<para>Defines the API group and version of the <literal>IngressClass</literal> resource.</para>
</callout>
<callout arearefs="CO78-2">
<para>Specifies the name of the <literal>IngressClass</literal>.</para>
</callout>
<callout arearefs="CO78-3">
<para>Defines the controller name. <literal>ingress.k8s.aws/alb</literal> denotes that all Ingresses of this class should be managed by the <literal>aws-load-balancer-controller</literal>.</para>
</callout>
<callout arearefs="CO78-4">
<para>Defines the API group of the <literal>IngressClassParams</literal> resource.</para>
</callout>
<callout arearefs="CO78-5">
<para>Defines the resource type of the <literal>IngressClassParams</literal> resource.</para>
</callout>
<callout arearefs="CO78-6">
<para>Defines the name of the <literal>IngressClassParams</literal> resource.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an <literal>IngressClass</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sample-single-lb-class.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>AWSLoadBalancerController</literal> resource YAML file, for example, <literal>sample-single-lb.yaml</literal>, as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController
metadata:
  name: cluster
spec:
  subnetTagging: Auto
  ingressClass: single-lb <co xml:id="CO79-1"/></programlisting>
<calloutlist>
<callout arearefs="CO79-1">
<para>Defines the name of the <literal>IngressClass</literal> resource.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an <literal>AWSLoadBalancerController</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sample-single-lb.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>Ingress</literal> resource YAML file, for example, <literal>sample-multiple-ingress.yaml</literal>, as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-1 <co xml:id="CO80-1"/>
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing <co xml:id="CO80-2"/>
    alb.ingress.kubernetes.io/group.order: "1" <co xml:id="CO80-3"/>
    alb.ingress.kubernetes.io/target-type: instance <co xml:id="CO80-4"/>
spec:
  ingressClassName: single-lb <co xml:id="CO80-5"/>
  rules:
  - host: example.com <co xml:id="CO80-6"/>
    http:
        paths:
        - path: /blog <co xml:id="CO80-7"/>
          pathType: Prefix
          backend:
            service:
              name: example-1 <co xml:id="CO80-8"/>
              port:
                number: 80 <co xml:id="CO80-9"/>
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-2
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.order: "2"
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: single-lb
  rules:
  - host: example.com
    http:
        paths:
        - path: /store
          pathType: Prefix
          backend:
            service:
              name: example-2
              port:
                number: 80
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: example-3
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing
    alb.ingress.kubernetes.io/group.order: "3"
    alb.ingress.kubernetes.io/target-type: instance
spec:
  ingressClassName: single-lb
  rules:
  - host: example.com
    http:
        paths:
        - path: /
          pathType: Prefix
          backend:
            service:
              name: example-3
              port:
                number: 80</programlisting>
<calloutlist>
<callout arearefs="CO80-1">
<para>Specifies the name of an ingress.</para>
</callout>
<callout arearefs="CO80-2">
<para>Indicates the load balancer to provision in the public subnet and makes it accessible over the internet.</para>
</callout>
<callout arearefs="CO80-3">
<para>Specifies the order in which the rules from the Ingresses are matched when the request is received at the load balancer.</para>
</callout>
<callout arearefs="CO80-4">
<para>Indicates the load balancer will target OpenShift nodes to reach the service.</para>
</callout>
<callout arearefs="CO80-5">
<para>Specifies the Ingress Class that belongs to this ingress.</para>
</callout>
<callout arearefs="CO80-6">
<para>Defines the name of a domain used for request routing.</para>
</callout>
<callout arearefs="CO80-7">
<para>Defines the path that must route to the service.</para>
</callout>
<callout arearefs="CO80-8">
<para>Defines the name of the service that serves the endpoint configured in the ingress.</para>
</callout>
<callout arearefs="CO80-9">
<para>Defines the port on the service that serves the endpoint.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>Ingress</literal> resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sample-multiple-ingress.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-adding-tls-termination">
<title>Adding TLS termination</title>

<simpara>You can add TLS termination on the AWS Load Balancer.</simpara>
<section xml:id="nw-adding-tls-termination_adding-tls-termination">
<title>Adding TLS termination on the AWS Load Balancer</title>
<simpara>You can route the traffic for the domain to pods of a service and add TLS termination on the AWS Load Balancer.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have an access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the Operator and create an instance of the <literal>aws-load-balancer-controller</literal> resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.olm.openshift.io/v1
kind: AWSLoadBalancerController
metadata:
  name: cluster
spec:
  subnetTagging: Auto
  ingressClass: tls-termination <co xml:id="CO81-1"/></programlisting>
<calloutlist>
<callout arearefs="CO81-1">
<para>Defines the name of an <literal>ingressClass</literal> resource reconciled by the AWS Load Balancer Controller. This <literal>ingressClass</literal> resource gets created if it is not present. You can add additional <literal>ingressClass</literal> values. The controller reconciles the <literal>ingressClass</literal> values if the <literal>spec.controller</literal> is set to <literal>ingress.k8s.aws/alb</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an <literal>Ingress</literal> resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: &lt;example&gt; <co xml:id="CO82-1"/>
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing <co xml:id="CO82-2"/>
    alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-west-2:xxxxx <co xml:id="CO82-3"/>
spec:
  ingressClassName: tls-termination <co xml:id="CO82-4"/>
  rules:
  - host: &lt;example.com&gt; <co xml:id="CO82-5"/>
    http:
        paths:
          - path: /
            pathType: Exact
            backend:
              service:
                name: &lt;example-service&gt; <co xml:id="CO82-6"/>
                port:
                  number: 80</programlisting>
<calloutlist>
<callout arearefs="CO82-1">
<para>Specifies the name of an ingress.</para>
</callout>
<callout arearefs="CO82-2">
<para>The controller provisions the load balancer for this <literal>Ingress</literal> resource in a public subnet so that the load balancer is reachable over the internet.</para>
</callout>
<callout arearefs="CO82-3">
<para>The Amazon Resource Name of the certificate that you attach to the load balancer.</para>
</callout>
<callout arearefs="CO82-4">
<para>Defines the ingress class name.</para>
</callout>
<callout arearefs="CO82-5">
<para>Defines the domain for traffic routing.</para>
</callout>
<callout arearefs="CO82-6">
<para>Defines the service for traffic routing.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-aws-load-balancer-operator-cluster-wide-proxy">
<title>Configuring cluster-wide proxy</title>

<simpara>You can configure the cluster-wide proxy in the AWS Load Balancer Operator. After configuring the cluster-wide proxy in the AWS Load Balancer Operator, Operator Lifecycle Manager (OLM) automatically updates all the deployments of the Operators with the environment variables such as <literal>HTTP_PROXY</literal>, <literal>HTTPS_PROXY</literal>, and <literal>NO_PROXY</literal>. These variables are populated to the managed controller by the AWS Load Balancer Operator.</simpara>
<section xml:id="nw-configuring-cluster-wide-proxy_aws-load-balancer-operator">
<title>Configuring the AWS Load Balancer Operator to trust the certificate authority of the cluster-wide proxy</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create the config map to contain the certificate authority (CA) bundle in the <literal>aws-load-balancer-operator</literal> namespace and inject a CA bundle that is trusted by OpenShift Container Platform into a config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator create configmap trusted-ca</programlisting>
</listitem>
<listitem>
<simpara>To inject the trusted CA bundle into the config map, add the <literal>config.openshift.io/inject-trusted-cabundle=true</literal> label to the config map by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator label cm trusted-ca config.openshift.io/inject-trusted-cabundle=true</programlisting>
</listitem>
<listitem>
<simpara>Update the subscription of the AWS Load Balancer Operator to access the config map in the deployment of the AWS Load Balancer Operator by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator patch subscription aws-load-balancer-operator --type='merge' -p '{"spec":{"config":{"env":[{"name":"TRUSTED_CA_CONFIGMAP_NAME","value":"trusted-ca"}],"volumes":[{"name":"trusted-ca","configMap":{"name":"trusted-ca"}}],"volumeMounts":[{"name":"trusted-ca","mountPath":"/etc/pki/tls/certs/albo-tls-ca-bundle.crt","subPath":"ca-bundle.crt"}]}}}'</programlisting>
</listitem>
<listitem>
<simpara>After the deployment of the AWS Load Balancer Operator is completed, verify that the CA bundle is added to the <literal>aws-load-balancer-operator-controller-manager</literal> deployment by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator exec deploy/aws-load-balancer-operator-controller-manager -c manager -- bash -c "ls -l /etc/pki/tls/certs/albo-tls-ca-bundle.crt; printenv TRUSTED_CA_CONFIGMAP_NAME"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">-rw-r--r--. 1 root 1000690000 5875 Jan 11 12:25 /etc/pki/tls/certs/albo-tls-ca-bundle.crt
trusted-ca</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: Restart deployment of the AWS Load Balancer Operator every time the config map changes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n aws-load-balancer-operator rollout restart deployment/aws-load-balancer-operator-controller-manager</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="_additional-resources-4" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="certificate-injection-using-operators_configuring-a-custom-pki">Certificate injection using Operators</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_multiple-networks">
<title>Multiple networks</title>
<section xml:id="understanding-multiple-networks">
<title>Understanding multiple networks</title>

<simpara>In Kubernetes, container networking is delegated to networking plugins that
implement the Container Network Interface (CNI).</simpara>
<simpara>OpenShift Container Platform uses the Multus CNI plugin to allow chaining of CNI plugins.
During cluster installation, you configure your <emphasis>default</emphasis> pod network. The
default network handles all ordinary network traffic for the cluster. You can
define an <emphasis>additional network</emphasis> based on the available CNI plugins and attach
one or more of these networks to your pods. You can define more than one
additional network for your cluster, depending on your needs. This gives you
flexibility when you configure pods that deliver network functionality, such as
switching or routing.</simpara>
<section xml:id="additional-network-considerations">
<title>Usage scenarios for an additional network</title>
<simpara>You can use an additional network in situations where network isolation is
needed, including data plane and control plane separation. Isolating network
traffic is useful for the following performance and security reasons:</simpara>
<variablelist>
<varlistentry>
<term>Performance</term>
<listitem>
<simpara>You can send traffic on two different planes to manage
how much traffic is along each plane.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Security</term>
<listitem>
<simpara>You can send sensitive traffic onto a network plane that is managed
specifically for security considerations, and you can separate private data that
must not be shared between tenants or customers.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>All of the pods in the cluster still use the cluster-wide default network
to maintain connectivity across the cluster. Every pod has an <literal>eth0</literal> interface
that is attached to the cluster-wide pod network. You can view the interfaces
for a pod by using the <literal>oc exec -it &lt;pod_name&gt; -- ip a</literal> command. If you
add additional network interfaces that use Multus CNI, they are named <literal>net1</literal>,
<literal>net2</literal>, &#8230;&#8203;, <literal>netN</literal>.</simpara>
<simpara>To attach additional network interfaces to a pod, you must create configurations that define how the interfaces are attached. You specify each interface by using a <literal>NetworkAttachmentDefinition</literal> custom resource (CR). A CNI configuration inside each of these CRs defines how that interface is created.</simpara>
</section>
<section xml:id="additional-networks-provided">
<title>Additional networks in OpenShift Container Platform</title>
<simpara>OpenShift Container Platform provides the following CNI plugins for creating additional
networks in your cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">bridge</emphasis>: <link linkend="nw-multus-bridge-object_configuring-additional-network">Configure a bridge-based additional network</link>
to allow pods on the same host to communicate with each other and the host.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">host-device</emphasis>: <link linkend="nw-multus-host-device-object_configuring-additional-network">Configure a host-device additional network</link> to allow pods access to a physical Ethernet network device on the host system.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">ipvlan</emphasis>: <link linkend="nw-multus-ipvlan-object_configuring-additional-network">Configure an ipvlan-based additional network</link> to allow pods on a host to communicate with other hosts and pods on those hosts, similar to a macvlan-based additional network. Unlike a macvlan-based additional network, each pod shares the same MAC address as the parent physical network interface.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">vlan</emphasis>: <link linkend="nw-multus-vlan-object_configuring-additional-network">Configure a vlan-based additional network</link> to allow VLAN-based network isolation and connectivity for pods.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">macvlan</emphasis>: <link linkend="nw-multus-macvlan-object_configuring-additional-network">Configure a macvlan-based additional network</link> to allow pods on a host to communicate with other hosts and pods on those hosts by using a physical network interface. Each pod that is attached to a macvlan-based additional network is provided a unique MAC address.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">tap</emphasis>: <link linkend="nw-multus-tap-object_configuring-additional-network">Configure a tap-based additional network</link> to create a tap device inside the container namespace. A tap device enables user space programs to send and receive network packets.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">SR-IOV</emphasis>: <link linkend="about-sriov">Configure an SR-IOV based additional network</link> to allow pods to attach to a virtual function (VF) interface on SR-IOV capable hardware on the host system.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-additional-network">
<title>Configuring an additional network</title>

<simpara>As a cluster administrator, you can configure an additional network for your cluster. The following network types are supported:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-multus-bridge-object_configuring-additional-network">Bridge</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-host-device-object_configuring-additional-network">Host device</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-vlan-object_configuring-additional-network">VLAN</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-ipvlan-object_configuring-additional-network">IPVLAN</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-macvlan-object_configuring-additional-network">MACVLAN</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-tap-object_configuring-additional-network">TAP</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuration-ovnk-additional-networks_configuring-additional-network">OVN-Kubernetes</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="configuring-additional-network_approaches-managing-additional-network">
<title>Approaches to managing an additional network</title>
<simpara>You can manage the life cycle of an additional network by two approaches. Each approach is mutually exclusive and you can only use one approach for managing an additional network at a time. For either approach, the additional network is managed by a Container Network Interface (CNI) plugin that you configure.</simpara>
<simpara>For an additional network, IP addresses are provisioned through an IP Address Management (IPAM) CNI plugin that you configure as part of the additional network. The IPAM plugin supports a variety of IP address assignment approaches including DHCP and static assignment.</simpara>
<itemizedlist>
<listitem>
<simpara>Modify the Cluster Network Operator (CNO) configuration: The CNO automatically creates and manages the <literal>NetworkAttachmentDefinition</literal> object. In addition to managing the object lifecycle the CNO ensures a DHCP is available for an additional network that uses a DHCP assigned IP address.</simpara>
</listitem>
<listitem>
<simpara>Applying a YAML manifest: You can manage the additional network directly by creating an <literal>NetworkAttachmentDefinition</literal> object. This approach allows for the chaining of CNI plugins.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>When deploying OpenShift Container Platform nodes with multiple network interfaces on Red Hat OpenStack Platform (RHOSP) with OVN SDN, DNS configuration of the secondary interface might take precedence over the DNS configuration of the primary interface. In this case, remove the DNS nameservers for the subnet id that is attached to the secondary interface:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack subnet set --dns-nameserver 0.0.0.0 &lt;subnet_id&gt;</programlisting>
</note>
</section>
<section xml:id="configuring-additional-network_configuration-additional-network-attachment">
<title>Configuration for an additional network attachment</title>
<simpara>An additional network is configured by using the <literal>NetworkAttachmentDefinition</literal> API in the <literal>k8s.cni.cncf.io</literal> API group.</simpara>
<important>
<simpara>Do not store any sensitive information or a secret in the <literal>NetworkAttachmentDefinition</literal> object because this information is accessible by the project administration user.</simpara>
</important>
<simpara>The configuration for the API is described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>NetworkAttachmentDefinition</literal> API fields</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name for the additional network.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The namespace that the object is associated with.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.config</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI plugin configuration in JSON format.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="configuring-additional-network_configuration-additional-network-cno">
<title>Configuration of an additional network through the Cluster Network Operator</title>
<simpara>The configuration for an additional network attachment is specified as part of the Cluster Network Operator (CNO) configuration.</simpara>
<simpara>The following YAML describes the configuration parameters for managing an additional network with the CNO:</simpara>
<formalpara>
<title>Cluster Network Operator configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  # ...
  additionalNetworks: <co xml:id="CO83-1"/>
  - name: &lt;name&gt; <co xml:id="CO83-2"/>
    namespace: &lt;namespace&gt; <co xml:id="CO83-3"/>
    rawCNIConfig: |- <co xml:id="CO83-4"/>
      {
        ...
      }
    type: Raw</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO83-1">
<para>An array of one or more additional network configurations.</para>
</callout>
<callout arearefs="CO83-2">
<para>The name for the additional network attachment that you are
creating. The name must be unique within the specified <literal>namespace</literal>.</para>
</callout>
<callout arearefs="CO83-3">
<para>The namespace to create the network attachment in. If
you do not specify a value, then the <literal>default</literal> namespace is used.</para>
</callout>
<callout arearefs="CO83-4">
<para>A CNI plugin configuration in JSON format.</para>
</callout>
</calloutlist>
</section>
<section xml:id="configuring-additional-network_configuration-additional-network-yaml">
<title>Configuration of an additional network from a YAML manifest</title>
<simpara>The configuration for an additional network is specified from a YAML configuration file, such as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: &lt;name&gt; <co xml:id="CO84-1"/>
spec:
  config: |- <co xml:id="CO84-2"/>
    {
      ...
    }</programlisting>
<calloutlist>
<callout arearefs="CO84-1">
<para>The name for the additional network attachment that you are
creating.</para>
</callout>
<callout arearefs="CO84-2">
<para>A CNI plugin configuration in JSON format.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="configuring-additional-network_configuration-additional-network-types">
<title>Configurations for additional network types</title>
<simpara>The specific configuration fields for additional networks is described in the following sections.</simpara>
<section xml:id="nw-multus-bridge-object_configuring-additional-network">
<title>Configuration for a bridge additional network</title>
<simpara>The following object describes the configuration parameters for the bridge CNI
plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Bridge CNI plugin JSON configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The <literal>0.3.1</literal> value is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The value for the <literal>name</literal> parameter you provided previously for the CNO configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure: <literal>bridge</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipam</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>bridge</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specify the name of the virtual bridge to use. If the bridge interface does not exist on the host, it is created. The default value is <literal>cni0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipMasq</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to enable IP masquerading for traffic that leaves the virtual network. The source IP address for all traffic is rewritten to the bridge&#8217;s IP address. If the bridge does not have an IP address, this setting has no effect. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>isGateway</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to assign an IP address to the bridge. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>isDefaultGateway</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to configure the bridge as the default gateway for the virtual network. The default value is <literal>false</literal>. If <literal>isDefaultGateway</literal> is set to <literal>true</literal>, then <literal>isGateway</literal> is also set to <literal>true</literal> automatically.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>forceAddress</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to allow assignment of a previously assigned IP address to the virtual bridge. When set to <literal>false</literal>, if an IPv4 address or an IPv6 address from overlapping subsets is assigned to the virtual bridge, an error occurs. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>hairpinMode</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to allow the virtual bridge to send an Ethernet frame back through the virtual port it was received on. This mode is also known as <emphasis>reflective relay</emphasis>. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>promiscMode</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to enable promiscuous mode on the bridge. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>vlan</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specify a virtual LAN (VLAN) tag as an integer value. By default, no VLAN tag is assigned.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>preserveDefaultVlan</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Indicates whether the default vlan must be preserved on the <literal>veth</literal> end connected to the bridge. Defaults to true.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>vlanTrunk</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>list</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Assign a VLAN trunk tag. The default value is <literal>none</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>enabledad</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Enables duplicate address detection for the container side <literal>veth</literal>. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>macspoofchk</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Enables mac spoof check, limiting the traffic originating from the container to the mac address of the interface. The default value is <literal>false</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>The VLAN parameter configures the VLAN tag on the host end of the <literal>veth</literal> and also enables the <literal>vlan_filtering</literal> feature on the bridge interface.</simpara>
</note>
<note>
<simpara>To configure uplink for a L2 network you need to allow the vlan on the uplink interface by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  bridge vlan add vid VLAN_ID dev DEV</programlisting>
</note>
<section xml:id="nw-multus-bridge-config-example_configuring-additional-network">
<title>bridge configuration example</title>
<simpara>The following example configures an additional network named <literal>bridge-net</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "bridge-net",
  "type": "bridge",
  "isGateway": true,
  "vlan": 2,
  "ipam": {
    "type": "dhcp"
    }
}</programlisting>
</section>
</section>
<section xml:id="nw-multus-host-device-object_configuring-additional-network">
<title>Configuration for a host device additional network</title>
<note>
<simpara>Specify your network device by setting only one of the following parameters: <literal>device</literal>,<literal>hwaddr</literal>, <literal>kernelpath</literal>, or <literal>pciBusID</literal>.</simpara>
</note>
<simpara>The following object describes the configuration parameters for the host-device CNI plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Host device CNI plugin JSON configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The <literal>0.3.1</literal> value is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The value for the <literal>name</literal> parameter you provided previously for the CNO configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure: <literal>host-device</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>device</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The name of the device, such as <literal>eth0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>hwaddr</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The device hardware MAC address.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>kernelpath</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The Linux kernel device path, such as <literal>/sys/devices/pci0000:00/0000:00:1f.6</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>pciBusID</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The PCI address of the network device, such as <literal>0000:00:1f.6</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="nw-multus-hostdev-config-example_configuring-additional-network">
<title>host-device configuration example</title>
<simpara>The following example configures an additional network named <literal>hostdev-net</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "hostdev-net",
  "type": "host-device",
  "device": "eth1"
}</programlisting>
</section>
</section>
<section xml:id="nw-multus-vlan-object_configuring-additional-network">
<title>Configuration for an VLAN additional network</title>
<simpara>The following object describes the configuration parameters for the VLAN CNI plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>VLAN CNI plugin JSON configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The <literal>0.3.1</literal> value is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The value for the <literal>name</literal> parameter you provided previously for the CNO configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure: <literal>vlan</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>master</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The Ethernet interface to associate with the network attachment. If a <literal>master</literal> is not specified, the interface for the default network route is used.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>vlanId</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Set the id of the vlan.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipam</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>dns</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: DNS information to return, for example, a priority-ordered list of DNS nameservers.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>linkInContainer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specifies whether the master interface is in the container network namespace or the main network namespace. Set the value to <literal>true</literal> to request the use of a container namespace master interface.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="nw-multus-vlan-config-example_configuring-additional-network">
<title>vlan configuration example</title>
<simpara>The following example configures an additional network named <literal>vlan-net</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "name": "vlan-net",
  "cniVersion": "0.3.1",
  "type": "vlan",
  "master": "eth0",
  "mtu": 1500,
  "vlanId": 5,
  "linkInContainer": false,
  "ipam": {
      "type": "host-local",
      "subnet": "10.1.1.0/24"
  },
  "dns": {
      "nameservers": [ "10.1.1.1", "8.8.8.8" ]
  }
}</programlisting>
</section>
</section>
<section xml:id="nw-multus-ipvlan-object_configuring-additional-network">
<title>Configuration for an IPVLAN additional network</title>
<simpara>The following object describes the configuration parameters for the IPVLAN CNI plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>IPVLAN CNI plugin JSON configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The <literal>0.3.1</literal> value is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The value for the <literal>name</literal> parameter you provided previously for the CNO configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure: <literal>ipvlan</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipam</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition. This is required unless the plugin is chained.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mode</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The operating mode for the virtual network. The value must be <literal>l2</literal>, <literal>l3</literal>, or <literal>l3s</literal>. The default value is <literal>l2</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>master</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The Ethernet interface to associate with the network attachment. If a <literal>master</literal> is not specified, the interface for the default network route is used.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>linkInContainer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specifies whether the master interface is in the container network namespace or the main network namespace. Set the value to <literal>true</literal> to request the use of a container namespace master interface.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<itemizedlist>
<listitem>
<simpara>The <literal>ipvlan</literal> object does not allow virtual interfaces to communicate with the <literal>master</literal> interface. Therefore the container will not be able to reach the host by using the <literal>ipvlan</literal> interface. Be sure that the container joins a network that provides connectivity to the host, such as a network supporting the Precision Time Protocol (<literal>PTP</literal>).</simpara>
</listitem>
<listitem>
<simpara>A single <literal>master</literal> interface cannot simultaneously be configured to use both <literal>macvlan</literal> and <literal>ipvlan</literal>.</simpara>
</listitem>
<listitem>
<simpara>For IP allocation schemes that cannot be interface agnostic, the <literal>ipvlan</literal> plugin can be chained with an earlier plugin that handles this logic. If the <literal>master</literal> is omitted, then the previous result must contain a single interface name for the <literal>ipvlan</literal> plugin to enslave. If <literal>ipam</literal> is omitted, then the previous result is used to configure the <literal>ipvlan</literal> interface.</simpara>
</listitem>
</itemizedlist>
</note>
<section xml:id="nw-multus-ipvlan-config-example_configuring-additional-network">
<title>ipvlan configuration example</title>
<simpara>The following example configures an additional network named <literal>ipvlan-net</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "ipvlan-net",
  "type": "ipvlan",
  "master": "eth1",
  "linkInContainer": false,
  "mode": "l3",
  "ipam": {
    "type": "static",
    "addresses": [
       {
         "address": "192.168.10.10/24"
       }
    ]
  }
}</programlisting>
</section>
</section>
<section xml:id="nw-multus-macvlan-object_configuring-additional-network">
<title>Configuration for a MACVLAN additional network</title>
<simpara>The following object describes the configuration parameters for the macvlan CNI plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MACVLAN CNI plugin JSON configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The <literal>0.3.1</literal> value is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The value for the <literal>name</literal> parameter you provided previously for the CNO configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure: <literal>macvlan</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>ipam</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The configuration object for the IPAM CNI plugin. The plugin manages IP address assignment for the attachment definition.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mode</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Configures traffic visibility on the virtual network. Must be either <literal>bridge</literal>, <literal>passthru</literal>, <literal>private</literal>, or <literal>vepa</literal>. If a value is not provided, the default value is <literal>bridge</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>master</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The host network interface to associate with the newly created macvlan interface. If a value is not specified, then the default route interface is used.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>linkInContainer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specifies whether the master interface is in the container network namespace or the main network namespace. Set the value to <literal>true</literal> to request the use of a container namespace master interface.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>If you specify the <literal>master</literal> key for the plugin configuration, use a different physical network interface than the one that is associated with your primary network plugin to avoid possible conflicts.</simpara>
</note>
<section xml:id="nw-multus-macvlan-config-example_configuring-additional-network">
<title>macvlan configuration example</title>
<simpara>The following example configures an additional network named <literal>macvlan-net</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "macvlan-net",
  "type": "macvlan",
  "master": "eth1",
  "linkInContainer": false,
  "mode": "bridge",
  "ipam": {
    "type": "dhcp"
    }
}</programlisting>
</section>
</section>
<section xml:id="nw-multus-tap-object_configuring-additional-network">
<title>Configuration for a TAP additional network</title>
<simpara>The following object describes the configuration parameters for the TAP CNI
plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>TAP CNI plugin JSON configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The <literal>0.3.1</literal> value is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The value for the <literal>name</literal> parameter you provided previously for the CNO configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure: <literal>tap</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mac</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Request the specified MAC address for the interface.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set the maximum transmission unit (MTU) to the specified value. The default value is automatically set by the kernel.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>selinuxcontext</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The SELinux context to associate with the tap device.</simpara>
<note>
<simpara>The value <literal>system_u:system_r:container_t:s0</literal> is required for OpenShift Container Platform.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>multiQueue</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set to <literal>true</literal> to enable multi-queue.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>owner</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The user owning the tap device.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>group</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: The group owning the tap device.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>bridge</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Set the tap device as a port of an already existing bridge.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="nw-multus-tap-config-example_configuring-additional-network">
<title>Tap configuration example</title>
<simpara>The following example configures an additional network named <literal>mynet</literal>:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
 "name": "mynet",
 "cniVersion": "0.3.1",
 "type": "tap",
 "mac": "00:11:22:33:44:55",
 "mtu": 1500,
 "selinuxcontext": "system_u:system_r:container_t:s0",
 "multiQueue": true,
 "owner": 0,
 "group": 0
 "bridge": "br1"
}</programlisting>
</section>
<section xml:id="nw-multus-enable-container_use_devices_configuring-additional-network">
<title>Setting SELinux boolean for the TAP CNI plugin</title>
<simpara>To create the tap device with the <literal>container_t</literal> SELinux context, enable the <literal>container_use_devices</literal> boolean on the host by using the Machine Config Operator (MCO).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new YAML file named, such as <literal>setsebool-container-use-devices.yaml</literal>, with the following details:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 99-worker-setsebool
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
      - enabled: true
        name: setsebool.service
        contents: |
          [Unit]
          Description=Set SELinux boolean for the TAP CNI plugin
          Before=kubelet.service

          [Service]
          Type=oneshot
          ExecStart=/usr/sbin/setsebool container_use_devices=on
          RemainAfterExit=true

          [Install]
          WantedBy=multi-user.target graphical.target</programlisting>
</listitem>
<listitem>
<simpara>Create the new <literal>MachineConfig</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f setsebool-container-use-devices.yaml</programlisting>
<note>
<simpara>Applying any changes to the <literal>MachineConfig</literal> object causes all affected nodes to gracefully reboot after the change is applied. This update can take some time to be applied.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the change is applied by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpools</programlisting>
<formalpara role="white-space-pre">
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        CONFIG                                                UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master      rendered-master-e5e0c8e8be9194e7c5a882e047379cfa      True      False      False      3              3                   3                     0                      7d2h
worker      rendered-worker-d6c9ca107fba6cd76cdcbfcedcafa0f2      True      False      False      3              3                   3                     0                      7d</programlisting>
</para>
</formalpara>
<note>
<simpara>All nodes should be in the updated and ready state.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information about enabling an SELinux boolean on a node, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-nodes-working-setting-booleans_nodes-nodes-managing">Setting SELinux booleans</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuration-ovnk-additional-networks_configuring-additional-network">
<title>Configuration for an OVN-Kubernetes additional network</title>
<simpara>The Red Hat OpenShift Networking OVN-Kubernetes network plugin allows the configuration of secondary network interfaces for pods. To configure secondary network interfaces, you must define the configurations in the <literal>NetworkAttachmentDefinition</literal> custom resource (CR).</simpara>
<note>
<simpara>Pod and multi-network policy creation might remain in a pending state until the OVN-Kubernetes control plane agent in the nodes processes the associated <literal>network-attachment-definition</literal> CR.</simpara>
</note>
<simpara>You can configure an OVN-Kubernetes additional network in either <emphasis>layer 2</emphasis> or <emphasis>localnet</emphasis> topologies.</simpara>
<itemizedlist>
<listitem>
<simpara>A layer 2 topology supports east-west cluster traffic, but does not allow access to the underlying physical network.</simpara>
</listitem>
<listitem>
<simpara>A localnet topology allows connections to the physical network, but requires additional configuration of the underlying Open vSwitch (OVS) bridge on cluster nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>The following sections provide example configurations for each of the topologies that OVN-Kubernetes currently allows for secondary networks.</simpara>
<note>
<simpara>Networks names must be unique. For example, creating multiple <literal>NetworkAttachmentDefinition</literal> CRs with different configurations that reference the same network is unsupported.</simpara>
</note>
<section xml:id="configuration-additional-network-types-supported-platforms_configuring-additional-network">
<title>Supported platforms for OVN-Kubernetes additional network</title>
<simpara>You can use an OVN-Kubernetes additional network with the following supported platforms:</simpara>
<itemizedlist>
<listitem>
<simpara>Bare metal</simpara>
</listitem>
<listitem>
<simpara>IBM Power&#174;</simpara>
</listitem>
<listitem>
<simpara>IBM Z&#174;</simpara>
</listitem>
<listitem>
<simpara>IBM&#174; LinuxONE</simpara>
</listitem>
<listitem>
<simpara>VMware vSphere</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenStack Platform (RHOSP)</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuration-ovnk-network-plugin-json-object_configuring-additional-network">
<title>OVN-Kubernetes network plugin JSON configuration table</title>
<simpara>The following table describes the configuration parameters for the OVN-Kubernetes CNI network plugin:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>OVN-Kubernetes network plugin JSON configuration table</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>cniVersion</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The CNI specification version. The required value is <literal>0.3.1</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the network. These networks are not namespaced. For example, you can have a network named
<literal>l2-network</literal> referenced from two different <literal>NetworkAttachmentDefinitions</literal> that exist on two different
namespaces. This ensures that pods making use of the <literal>NetworkAttachmentDefinition</literal> on their own different
namespaces can communicate over the same secondary network. However, those two different <literal>NetworkAttachmentDefinitions</literal> must also share the same network specific parameters such as <literal>topology</literal>, <literal>subnets</literal>, <literal>mtu</literal>, and <literal>excludeSubnets</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNI plugin to configure. This value must be set to <literal>ovn-k8s-cni-overlay</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>topology</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The topological configuration for the network. Must be one of <literal>layer2</literal> or <literal>localnet</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>subnets</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The subnet to use for the network across the cluster. When specifying <literal>layer2</literal> for the <literal>topology</literal>, only include the CIDR for the node. For example, <literal>10.100.200.0/24</literal>.</simpara><simpara>For <literal>"topology":"layer2"</literal> deployments, IPv6 (<literal>2001:DBB::/64</literal>) and dual-stack (<literal>192.168.100.0/24,2001:DBB::/64</literal>) subnets are supported.</simpara><simpara>When omitted, the logical switch implementing the network only provides layer 2 communication, and users must configure IP addresses for the pods. Port security only prevents MAC spoofing.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>mtu</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum transmission unit (MTU). The default value, <literal>1300</literal>, is automatically set by the kernel.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>netAttachDefName</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The metadata <literal>namespace</literal> and <literal>name</literal> of the network attachment definition object where this
configuration is included. For example, if this configuration is defined in a <literal>NetworkAttachmentDefinition</literal> in namespace <literal>ns1</literal> named <literal>l2-network</literal>, this should be set to <literal>ns1/l2-network</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>excludeSubnets</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A comma-separated list of CIDRs and IP addresses. IP addresses are removed from the assignable IP address pool and are never passed to the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>vlanID</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>If topology is set to <literal>localnet</literal>, the specified VLAN tag is assigned to traffic from this additional network. The default is to not assign a VLAN tag.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="compatibility-with-multi-network-policy_configuring-additional-network">
<title>Compatibility with multi-network policy</title>
<simpara>The multi-network policy API, which is provided by the <literal>MultiNetworkPolicy</literal> custom resource definition (CRD) in the <literal>k8s.cni.cncf.io</literal> API group, is compatible with an OVN-Kubernetes secondary network. When defining a network policy, the network policy rules that can be used depend on whether the OVN-Kubernetes secondary network defines the <literal>subnets</literal> field. Refer to the following table for details:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supported multi-network policy selectors based on <literal>subnets</literal> CNI configuration</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="middle"><literal>subnets</literal> field specified</entry>
<entry align="left" valign="middle">Allowed multi-network policy selectors</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara>Yes</simpara></entry>
<entry align="left" valign="middle"><itemizedlist>
<listitem>
<simpara><literal>podSelector</literal> and <literal>namespaceSelector</literal></simpara>
</listitem>
<listitem>
<simpara><literal>ipBlock</literal></simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara>No</simpara></entry>
<entry align="left" valign="middle"><itemizedlist>
<listitem>
<simpara><literal>ipBlock</literal></simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For example, the following multi-network policy is valid only if the <literal>subnets</literal> field is defined in the additional network CNI configuration for the additional network named <literal>blue2</literal>:</simpara>
<formalpara>
<title>Example multi-network policy that uses a pod selector</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: allow-same-namespace
  annotations:
    k8s.v1.cni.cncf.io/policy-for: blue2
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</programlisting>
</para>
</formalpara>
<simpara>The following example uses the <literal>ipBlock</literal> network policy selector, which is always valid for an OVN-Kubernetes additional network:</simpara>
<formalpara>
<title>Example multi-network policy that uses an IP block selector</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name:  ingress-ipblock
  annotations:
    k8s.v1.cni.cncf.io/policy-for: default/flatl2net
spec:
  podSelector:
    matchLabels:
      name: access-control
  policyTypes:
  - Ingress
  ingress:
  - from:
    - ipBlock:
        cidr: 10.200.0.0/30</programlisting>
</para>
</formalpara>
</section>
<section xml:id="configuration-layer-two-switched-topology_configuring-additional-network">
<title>Configuration for a layer 2 switched topology</title>
<simpara>The switched (layer 2) topology networks interconnect the workloads through a cluster-wide logical switch. This configuration can be used for IPv6 and dual-stack deployments.</simpara>
<note>
<simpara>Layer 2 switched topology networks only allow for the transfer of data packets between pods within a cluster.</simpara>
</note>
<simpara>The following JSON example configures a switched secondary network:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "l2-network",
  "type": "ovn-k8s-cni-overlay",
  "topology":"layer2",
  "subnets": "10.100.200.0/24",
  "mtu": 1300,
  "netAttachDefName": "ns1/l2-network",
  "excludeSubnets": "10.100.200.0/29"
}</programlisting>
</section>
<section xml:id="configuring-additional-network_ovn-kubernetes-configuration-for-a-localnet-topology">
<title>Configuration for a localnet topology</title>
<simpara>The switched (localnet) topology interconnects the workloads through a cluster-wide logical switch to a physical network.</simpara>
<section xml:id="configuring-additional-network_configuration-additional-network-types-prerequisites">
<title>Prerequisites for configuring OVN-Kubernetes additional network</title>
<itemizedlist>
<listitem>
<simpara>The NMState Operator is installed. For more information, see <link linkend="k8s-nmstate-about-the-k8s-nmstate-operator">About the Kubernetes NMState Operator</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-additional-network_configuration-additional-network-interface">
<title>Configuration for an OVN-Kubernetes additional network mapping</title>
<simpara>You must map an additional network to the OVN bridge to use it as an OVN-Kubernetes additional network. Bridge mappings allow network traffic to reach the physical network. A bridge mapping associates a physical network name, also known as an interface label, to a bridge created with Open vSwitch (OVS).</simpara>
<simpara>You can create an <literal>NodeNetworkConfigurationPolicy</literal> object, part of the <literal>nmstate.io/v1</literal> API group, to declaratively create the mapping. This API is provided by the NMState Operator. By using this API you can apply the bridge mapping to nodes that match your specified <literal>nodeSelector</literal> expression, such as <literal>node-role.kubernetes.io/worker: ''</literal>.</simpara>
<simpara>When attaching an additional network, you can either use the existing <literal>br-ex</literal> bridge or create a new bridge. Which approach to use depends on your specific network infrastructure.</simpara>
<itemizedlist>
<listitem>
<simpara>If your nodes include only a single network interface, you must use the existing bridge. This network interface is owned and managed by OVN-Kubernetes and you must not remove it from the <literal>br-ex</literal> bridge or alter the interface configuration. If you remove or alter the network interface, your cluster network will stop working correctly.</simpara>
</listitem>
<listitem>
<simpara>If your nodes include several network interfaces, you can attach a different network interface to a new bridge, and use that for your additional network. This approach provides for traffic isolation from your primary cluster network.</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>localnet1</literal> network is mapped to the <literal>br-ex</literal> bridge in the following example:</simpara>
<formalpara>
<title>Example mapping for sharing a bridge</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: mapping <co xml:id="CO85-1"/>
spec:
  nodeSelector:
    node-role.kubernetes.io/worker: '' <co xml:id="CO85-2"/>
  desiredState:
    ovn:
      bridge-mappings:
      - localnet: localnet1 <co xml:id="CO85-3"/>
        bridge: br-ex <co xml:id="CO85-4"/>
        state: present <co xml:id="CO85-5"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO85-1">
<para>The name for the configuration object.</para>
</callout>
<callout arearefs="CO85-2">
<para>A node selector that specifies the nodes to apply the node network configuration policy to.</para>
</callout>
<callout arearefs="CO85-3">
<para>The name for the additional network from which traffic is forwarded to the OVS bridge. This additional network must match the name of the <literal>spec.config.name</literal> field of the <literal>NetworkAttachmentDefinition</literal> object that defines the OVN-Kubernetes additional network.</para>
</callout>
<callout arearefs="CO85-4">
<para>The name of the OVS bridge on the node. This value is required only if you specify <literal>state: present</literal>.</para>
</callout>
<callout arearefs="CO85-5">
<para>The state for the mapping. Must be either <literal>present</literal> to add the bridge or <literal>absent</literal> to remove the bridge. The default value is <literal>present</literal>.</para>
</callout>
</calloutlist>
<simpara>In the following example, the <literal>localnet2</literal> network interface is attached to the <literal>ovs-br1</literal> bridge. Through this attachment, the network interface is available to the OVN-Kubernetes network plugin as an additional network.</simpara>
<formalpara>
<title>Example mapping for nodes with multiple interfaces</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: ovs-br1-multiple-networks <co xml:id="CO86-1"/>
spec:
  nodeSelector:
    node-role.kubernetes.io/worker: '' <co xml:id="CO86-2"/>
  desiredState:
    interfaces:
    - name: ovs-br1 <co xml:id="CO86-3"/>
      description: |-
        A dedicated OVS bridge with eth1 as a port
        allowing all VLANs and untagged traffic
      type: ovs-bridge
      state: up
      bridge:
        options:
          stp: true
        port:
        - name: eth1 <co xml:id="CO86-4"/>
    ovn:
      bridge-mappings:
      - localnet: localnet2 <co xml:id="CO86-5"/>
        bridge: ovs-br1 <co xml:id="CO86-6"/>
        state: present <co xml:id="CO86-7"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO86-1">
<para>The name for the configuration object.</para>
</callout>
<callout arearefs="CO86-2">
<para>A node selector that specifies the nodes to apply the node network configuration policy to.</para>
</callout>
<callout arearefs="CO86-3">
<para>A new OVS bridge, separate from the default bridge used by OVN-Kubernetes for all cluster traffic.</para>
</callout>
<callout arearefs="CO86-4">
<para>A network device on the host system to associate with this new OVS bridge.</para>
</callout>
<callout arearefs="CO86-5">
<para>The name for the additional network from which traffic is forwarded to the OVS bridge. This additional network must match the name of the <literal>spec.config.name</literal> field of the <literal>NetworkAttachmentDefinition</literal> object that defines the OVN-Kubernetes additional network.</para>
</callout>
<callout arearefs="CO86-6">
<para>The name of the OVS bridge on the node. This value is required only if you specify <literal>state: present</literal>.</para>
</callout>
<callout arearefs="CO86-7">
<para>The state for the mapping. Must be either <literal>present</literal> to add the bridge or <literal>absent</literal> to remove the bridge. The default value is <literal>present</literal>.</para>
</callout>
</calloutlist>
<simpara>This declarative approach is recommended because the NMState Operator applies additional network configuration to all nodes specified by the node selector automatically and transparently.</simpara>
<simpara>The following JSON example configures a localnet secondary network:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "ns1-localnet-network",
  "type": "ovn-k8s-cni-overlay",
  "topology":"localnet",
  "subnets": "202.10.130.112/28",
  "vlanID": 33,
  "mtu": 1500,
  "netAttachDefName": "ns1/localnet-network"
  "excludeSubnets": "10.100.200.0/29"
}</programlisting>
</section>
</section>
<section xml:id="configuring-pods-secondary-network_configuring-additional-network">
<title>Configuring pods for additional networks</title>
<simpara>You must specify the secondary network attachments through the <literal>k8s.v1.cni.cncf.io/networks</literal> annotation.</simpara>
<simpara>The following example provisions a pod with two secondary attachments, one for each of the attachment configurations presented in this guide.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: l2-network
  name: tinypod
  namespace: ns1
spec:
  containers:
  - args:
    - pause
    image: k8s.gcr.io/e2e-test-images/agnhost:2.36
    imagePullPolicy: IfNotPresent
    name: agnhost-container</programlisting>
</section>
<section xml:id="configuring-pods-static-ip_configuring-additional-network">
<title>Configuring pods with a static IP address</title>
<simpara>The following example provisions a pod with a static IP address.</simpara>
<note>
<itemizedlist>
<listitem>
<simpara>You can only specify the IP address for a pod&#8217;s secondary network attachment for layer 2 attachments.</simpara>
</listitem>
<listitem>
<simpara>Specifying a static IP address for the pod is only possible when the attachment configuration does not feature subnets.</simpara>
</listitem>
</itemizedlist>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {
        "name": "l2-network", <co xml:id="CO87-1"/>
        "mac": "02:03:04:05:06:07", <co xml:id="CO87-2"/>
        "interface": "myiface1", <co xml:id="CO87-3"/>
        "ips": [
          "192.0.2.20/24"
          ] <co xml:id="CO87-4"/>
      }
    ]'
  name: tinypod
  namespace: ns1
spec:
  containers:
  - args:
    - pause
    image: k8s.gcr.io/e2e-test-images/agnhost:2.36
    imagePullPolicy: IfNotPresent
    name: agnhost-container</programlisting>
<calloutlist>
<callout arearefs="CO87-1">
<para>The name of the network. This value must be unique across all <literal>NetworkAttachmentDefinitions</literal>.</para>
</callout>
<callout arearefs="CO87-2">
<para>The MAC address to be assigned for the interface.</para>
</callout>
<callout arearefs="CO87-3">
<para>The name of the network interface to be created for the pod.</para>
</callout>
<callout arearefs="CO87-4">
<para>The IP addresses to be assigned to the network interface.</para>
</callout>
</calloutlist>
</section>
</section>
</section>
<section xml:id="nw-multus-ipam-object_configuring-additional-network">
<title>Configuration of IP address assignment for an additional network</title>
<simpara>The IP address management (IPAM) Container Network Interface (CNI) plugin provides IP addresses for other CNI plugins.</simpara>
<simpara>You can use the following IP address assignment types:</simpara>
<itemizedlist>
<listitem>
<simpara>Static assignment.</simpara>
</listitem>
<listitem>
<simpara>Dynamic assignment through a DHCP server. The DHCP server you specify must be reachable from the additional network.</simpara>
</listitem>
<listitem>
<simpara>Dynamic assignment through the Whereabouts IPAM CNI plugin.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-multus-static_configuring-additional-network">
<title>Static IP address assignment configuration</title>
<simpara>The following table describes the configuration for static IP address assignment:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> static configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>static</literal> is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>addresses</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of objects specifying IP addresses to assign to the virtual interface. Both IPv4 and IPv6 IP addresses are supported.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>routes</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of objects specifying routes to configure inside the pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>dns</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: An array of objects specifying the DNS configuration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The <literal>addresses</literal> array requires objects with the following fields:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.addresses[]</literal> array</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>address</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An IP address and network prefix that you specify. For example, if you specify <literal>10.10.21.10/24</literal>, then the additional network is assigned an IP address of <literal>10.10.21.10</literal> and the netmask is <literal>255.255.255.0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gateway</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The default gateway to route egress network traffic to.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.routes[]</literal> array</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>dst</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IP address range in CIDR format, such as <literal>192.168.17.0/24</literal> or <literal>0.0.0.0/0</literal> for the default route.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gw</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The gateway where network traffic is routed.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.dns</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>nameservers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of one or more IP addresses for to send DNS queries to.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>domain</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The default domain to append to a hostname. For example, if the
domain is set to <literal>example.com</literal>, a DNS lookup query for <literal>example-host</literal> is
rewritten as <literal>example-host.example.com</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>search</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of domain names to append to an unqualified hostname,
such as <literal>example-host</literal>, during a DNS lookup query.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Static IP address assignment configuration example</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "static",
      "addresses": [
        {
          "address": "191.168.1.7/24"
        }
      ]
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-dhcp_configuring-additional-network">
<title>Dynamic IP address (DHCP) assignment configuration</title>
<simpara>The following JSON describes the configuration for dynamic IP address address assignment with DHCP.</simpara>
<important>
<title>Renewal of DHCP leases</title>
<simpara>A pod obtains its original DHCP lease when it is created. The lease must be periodically renewed by a minimal DHCP server deployment running on the cluster.</simpara>
<simpara>To trigger the deployment of the DHCP server, you must create a shim network attachment by editing the Cluster Network Operator configuration, as in the following example:</simpara>
<formalpara>
<title>Example shim network attachment definition</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: dhcp-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
        "name": "dhcp-shim",
        "cniVersion": "0.3.1",
        "type": "bridge",
        "ipam": {
          "type": "dhcp"
        }
      }
  # ...</programlisting>
</para>
</formalpara>
</important>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> DHCP configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>dhcp</literal> is required.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Dynamic IP address (DHCP) assignment configuration example</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "dhcp"
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-whereabouts_configuring-additional-network">
<title>Dynamic IP address assignment configuration with Whereabouts</title>
<simpara>The Whereabouts CNI plugin allows the dynamic assignment of an IP address to an additional network without the use of a DHCP server.</simpara>
<simpara>The following table describes the configuration for dynamic IP address assignment with Whereabouts:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> whereabouts configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>whereabouts</literal> is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>range</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An IP address and range in CIDR notation. IP addresses are assigned from within this range of addresses.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>exclude</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: A list of zero or more IP addresses and ranges in CIDR notation. IP addresses within an excluded address range are not assigned.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Dynamic IP address assignment configuration example that uses Whereabouts</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "whereabouts",
    "range": "192.0.2.192/27",
    "exclude": [
       "192.0.2.192/30",
       "192.0.2.196/32"
    ]
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-creating-whereabouts-reconciler-daemon-set_configuring-additional-network">
<title>Creating a whereabouts-reconciler daemon set</title>
<simpara>The Whereabouts reconciler is responsible for managing dynamic IP address assignments for the pods within a cluster using the Whereabouts IP Address Management (IPAM) solution. It ensures that each pod gets a unique IP address from the specified IP address range. It also handles IP address releases when pods are deleted or scaled down.</simpara>
<note>
<simpara>You can also use a <literal>NetworkAttachmentDefinition</literal> custom resource (CR) for dynamic IP address assignment.</simpara>
</note>
<simpara>The <literal>whereabouts-reconciler</literal> daemon set is automatically created when you configure an additional network through the Cluster Network Operator. It is not automatically created when you configure an additional network from a YAML manifest.</simpara>
<simpara>To trigger the deployment of the <literal>whereabouts-reconciler</literal> daemon set, you must manually create a <literal>whereabouts-shim</literal> network attachment by editing the Cluster Network Operator custom resource (CR) file.</simpara>
<simpara>Use the following procedure to deploy the <literal>whereabouts-reconciler</literal> daemon set.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>Network.operator.openshift.io</literal> custom resource (CR) by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit network.operator.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>Modify the <literal>additionalNetworks</literal> parameter in the CR to add the <literal>whereabouts-shim</literal> network attachment definition. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    rawCNIConfig: |-
      {
       "name": "whereabouts-shim",
       "cniVersion": "0.3.1",
       "type": "bridge",
       "ipam": {
         "type": "whereabouts"
       }
      }
    type: Raw</programlisting>
</listitem>
<listitem>
<simpara>Save the file and exit the text editor.</simpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>whereabouts-reconciler</literal> daemon set deployed successfully by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-multus | grep whereabouts-reconciler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">pod/whereabouts-reconciler-jnp6g 1/1 Running 0 6s
pod/whereabouts-reconciler-k76gg 1/1 Running 0 6s
pod/whereabouts-reconciler-k86t9 1/1 Running 0 6s
pod/whereabouts-reconciler-p4sxw 1/1 Running 0 6s
pod/whereabouts-reconciler-rvfdv 1/1 Running 0 6s
pod/whereabouts-reconciler-svzw9 1/1 Running 0 6s
daemonset.apps/whereabouts-reconciler 6 6 6 6 6 kubernetes.io/os=linux 6s</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-multus-configuring-whereabouts-ip-reconciler-schedule_configuring-additional-network">
<title>Configuring the Whereabouts IP reconciler schedule</title>
<simpara>The Whereabouts IPAM CNI plugin runs the IP reconciler daily. This process cleans up any stranded IP allocations that might result in exhausting IPs and therefore prevent new pods from getting an IP allocated to them.</simpara>
<simpara>Use this procedure to change the frequency at which the IP reconciler runs.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have deployed the <literal>whereabouts-reconciler</literal> daemon set, and the <literal>whereabouts-reconciler</literal> pods are up and running.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to create a <literal>ConfigMap</literal> object named <literal>whereabouts-config</literal> in the <literal>openshift-multus</literal> namespace with a specific cron expression for the IP reconciler:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap whereabouts-config -n openshift-multus --from-literal=reconciler_cron_expression="*/15 * * * *"</programlisting>
<simpara>This cron expression indicates the IP reconciler runs every 15 minutes. Adjust the expression based on your specific requirements.</simpara>
<note>
<simpara>The <literal>whereabouts-reconciler</literal> daemon set can only consume a cron expression pattern that includes five asterisks. The sixth, which is used to denote seconds, is currently not supported.</simpara>
</note>
</listitem>
<listitem>
<simpara>Retrieve information about resources related to the <literal>whereabouts-reconciler</literal> daemon set and pods within the <literal>openshift-multus</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-multus | grep whereabouts-reconciler</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">pod/whereabouts-reconciler-2p7hw                   1/1     Running   0             4m14s
pod/whereabouts-reconciler-76jk7                   1/1     Running   0             4m14s
pod/whereabouts-reconciler-94zw6                   1/1     Running   0             4m14s
pod/whereabouts-reconciler-mfh68                   1/1     Running   0             4m14s
pod/whereabouts-reconciler-pgshz                   1/1     Running   0             4m14s
pod/whereabouts-reconciler-xn5xz                   1/1     Running   0             4m14s
daemonset.apps/whereabouts-reconciler          6         6         6       6            6           kubernetes.io/os=linux   4m16s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command to verify that the <literal>whereabouts-reconciler</literal> pod runs the IP reconciler with the configured interval:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-multus logs whereabouts-reconciler-2p7hw</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2024-02-02T16:33:54Z [debug] event not relevant: "/cron-schedule/..2024_02_02_16_33_54.1375928161": CREATE
2024-02-02T16:33:54Z [debug] event not relevant: "/cron-schedule/..2024_02_02_16_33_54.1375928161": CHMOD
2024-02-02T16:33:54Z [debug] event not relevant: "/cron-schedule/..data_tmp": RENAME
2024-02-02T16:33:54Z [verbose] using expression: */15 * * * *
2024-02-02T16:33:54Z [verbose] configuration updated to file "/cron-schedule/..data". New cron expression: */15 * * * *
2024-02-02T16:33:54Z [verbose] successfully updated CRON configuration id "00c2d1c9-631d-403f-bb86-73ad104a6817" - new cron expression: */15 * * * *
2024-02-02T16:33:54Z [debug] event not relevant: "/cron-schedule/config": CREATE
2024-02-02T16:33:54Z [debug] event not relevant: "/cron-schedule/..2024_02_02_16_26_17.3874177937": REMOVE
2024-02-02T16:45:00Z [verbose] starting reconciler run
2024-02-02T16:45:00Z [debug] NewReconcileLooper - inferred connection data
2024-02-02T16:45:00Z [debug] listing IP pools
2024-02-02T16:45:00Z [debug] no IP addresses to cleanup
2024-02-02T16:45:00Z [verbose] reconciler success</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-multus-configure-dualstack-ip-address_configuring-additional-network">
<title>Creating a configuration for assignment of dual-stack IP addresses dynamically</title>
<simpara>Dual-stack IP address assignment can be configured with the <literal>ipRanges</literal> parameter for:</simpara>
<itemizedlist>
<listitem>
<simpara>IPv4 addresses</simpara>
</listitem>
<listitem>
<simpara>IPv6 addresses</simpara>
</listitem>
<listitem>
<simpara>multiple IP address assignment</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set <literal>type</literal> to <literal>whereabouts</literal>.</simpara>
</listitem>
<listitem>
<simpara>Use <literal>ipRanges</literal> to allocate IP addresses as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">cniVersion: operator.openshift.io/v1
kind: Network
=metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
       "name": "whereabouts-dual-stack",
       "cniVersion": "0.3.1,
       "type": "bridge",
       "ipam": {
         "type": "whereabouts",
         "ipRanges": [
                  {"range": "192.168.10.0/24"},
                  {"range": "2001:db8::/64"}
              ]
       }
      }</programlisting>
</listitem>
<listitem>
<simpara>Attach network to a pod. For more information, see "Adding a pod to an additional network".</simpara>
</listitem>
<listitem>
<simpara>Verify that all IP addresses are assigned.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to ensure the IP addresses are assigned as metadata.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc exec -it mypod -- ip a</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-multus-add-pod_attaching-pod">Attaching a pod to an additional network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-multus-create-network_configuring-additional-network">
<title>Creating an additional network attachment with the Cluster Network Operator</title>
<simpara>The Cluster Network Operator (CNO) manages additional network definitions. When
you specify an additional network to create, the CNO creates the
<literal>NetworkAttachmentDefinition</literal> object automatically.</simpara>
<important>
<simpara>Do not edit the <literal>NetworkAttachmentDefinition</literal> objects that the Cluster Network
Operator manages. Doing so might disrupt network traffic on your additional
network.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: Create the namespace for the additional networks:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace &lt;namespace_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>To edit the CNO configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.operator.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>Modify the CR that you are creating by adding the configuration for the
additional network that you are creating, as in the following example CR.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  # ...
  additionalNetworks:
  - name: tertiary-net
    namespace: namespace2
    type: Raw
    rawCNIConfig: |-
      {
        "cniVersion": "0.3.1",
        "name": "tertiary-net",
        "type": "ipvlan",
        "master": "eth1",
        "mode": "l2",
        "ipam": {
          "type": "static",
          "addresses": [
            {
              "address": "192.168.1.23/24"
            }
          ]
        }
      }</programlisting>
</listitem>
<listitem>
<simpara>Save your changes and quit the text editor to commit your changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Confirm that the CNO created the <literal>NetworkAttachmentDefinition</literal> object by running the following command. There might be a delay before the CNO creates the object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Specifies the namespace for the network attachment that you added to the CNO configuration.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                 AGE
test-network-1       14m</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-multus-create-network-apply_configuring-additional-network">
<title>Creating an additional network attachment by applying a YAML manifest</title>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file with your additional network configuration, such as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: next-net
spec:
  config: |-
    {
      "cniVersion": "0.3.1",
      "name": "work-network",
      "type": "host-device",
      "device": "eth1",
      "ipam": {
        "type": "dhcp"
      }
    }</programlisting>
</listitem>
<listitem>
<simpara>To create the additional network, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;file&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;file&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the file contained the YAML manifest.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-about-configuring-master-interface-container_configuring-additional-network">
<title>About configuring the master interface in the container network namespace</title>
<simpara>In OpenShift Container Platform 4.14 and later, the ability to allow users to create a MAC-VLAN, IP-VLAN, and VLAN subinterface based on a master interface in a container namespace is now generally available.</simpara>
<simpara>This feature allows you to create the master interfaces as part of the pod network configuration in a separate network attachment definition. You can then base the VLAN, MACVLAN, or IPVLAN on this interface without requiring the knowledge of the network configuration of the node.</simpara>
<simpara>To ensure the use of a container namespace master interface, specify the <literal>linkInContainer</literal> and set the value to <literal>true</literal> in the VLAN, MACVLAN, or IPVLAN plugin configuration depending on the particular type of additional network.</simpara>
<section xml:id="nw-multus-create-multiple-vlans-sriov_configuring-additional-network">
<title>Creating multiple VLANs on SR-IOV VFs</title>
<simpara>An example use case for utilizing this feature is to create multiple VLANs based on SR-IOV VFs. To do so, begin by creating an SR-IOV network and then define the network attachments for the VLAN interfaces.</simpara>
<simpara>The following example shows how to configure the setup illustrated in this diagram.</simpara>
<figure>
<title>Creating VLANs</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/345_OpenShift_config_additional_network_0823.png"/>
</imageobject>
<textobject><phrase>Creating VLANs</phrase></textobject>
</mediaobject>
</figure>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a dedicated container namespace where you want to deploy your pod by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project test-namespace</programlisting>
</listitem>
<listitem>
<simpara>Create an SR-IOV node policy:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> object, and then save the YAML in the <literal>sriov-node-network-policy.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
 name: sriovnic
 namespace: openshift-sriov-network-operator
spec:
 deviceType: netdevice
 isRdma: false
 needVhostNet: true
 nicSelector:
   vendor: "15b3" <co xml:id="CO88-1"/>
   deviceID: "101b" <co xml:id="CO88-2"/>
   rootDevices: ["00:05.0"]
 numVfs: 10
 priority: 99
 resourceName: sriovnic
 nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"</programlisting>
<note>
<simpara>The SR-IOV network node policy configuration example, with the setting <literal>deviceType: netdevice</literal>, is tailored specifically for Mellanox Network Interface Cards (NICs).</simpara>
</note>
<calloutlist>
<callout arearefs="CO88-1">
<para>The vendor hexadecimal code of the SR-IOV network device. The value <literal>15b3</literal> is associated with a Mellanox NIC.</para>
</callout>
<callout arearefs="CO88-2">
<para>The device hexadecimal code of the SR-IOV network device.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f sriov-node-network-policy.yaml</programlisting>
<note>
<simpara>Applying this might take some time due to the node requiring a reboot.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an SR-IOV network:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> custom resource (CR) for the additional SR-IOV network attachment as in the following example CR. Save the YAML as the file <literal>sriov-network-attachment.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
 name: sriov-network
 namespace: openshift-sriov-network-operator
spec:
 networkNamespace: test-namespace
 resourceName: sriovnic
 spoofChk: "off"
 trust: "on"</programlisting>
</listitem>
<listitem>
<simpara>Apply the YAML by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f sriov-network-attachment.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the VLAN additional network:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Using the following YAML example, create a file named <literal>ipvlan100-additional-network-configuration.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: vlan-100
  namespace: test-namespace
spec:
  config: |
    {
      "cniVersion": "0.4.0",
      "name": "vlan-100",
      "plugins": [
        {
          "type": "vlan",
          "master": "ext0", <co xml:id="CO89-1"/>
          "mtu": 1500,
          "vlanId": 100,
          "linkInContainer": true, <co xml:id="CO89-2"/>
          "ipam": {"type": "whereabouts", "ipRanges": [{"range": "1.1.1.0/24"}]}
        }
      ]
    }</programlisting>
<calloutlist>
<callout arearefs="CO89-1">
<para>The VLAN configuration needs to specify the master name. This can be configured in the pod networks annotation.</para>
</callout>
<callout arearefs="CO89-2">
<para>The <literal>linkInContainer</literal> parameter must be specified.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f vlan100-additional-network-configuration.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a pod definition by using the earlier specified networks:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Using the following YAML example, create a file named <literal>pod-a.yaml</literal> file:</simpara>
<note>
<simpara>The manifest below includes 2 resources:</simpara>
<itemizedlist>
<listitem>
<simpara>Namespace with security labels</simpara>
</listitem>
<listitem>
<simpara>Pod definition with appropriate network annotation</simpara>
</listitem>
</itemizedlist>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: test-namespace
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged
    security.openshift.io/scc.podSecurityLabelSync: "false"
---
apiVersion: v1
kind: Pod
metadata:
  name: nginx-pod
  namespace: test-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {
        "name": "sriov-network",
        "namespace": "test-namespace",
        "interface": "ext0" <co xml:id="CO90-1"/>
      },
      {
        "name": "vlan-100",
        "namespace": "test-namespace",
        "interface": "ext0.100"
      }
    ]'
spec:
  securityContext:
    runAsNonRoot: true
  containers:
    - name: nginx-container
      image: nginxinc/nginx-unprivileged:latest
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop: ["ALL"]
      ports:
        - containerPort: 80
      seccompProfile:
        type: "RuntimeDefault"</programlisting>
<calloutlist>
<callout arearefs="CO90-1">
<para>The name to be used as the master for the VLAN interface.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f pod-a.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Get detailed information about the <literal>nginx-pod</literal> within the <literal>test-namespace</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pods nginx-pod -n test-namespace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         nginx-pod
Namespace:    test-namespace
Priority:     0
Node:         worker-1/10.46.186.105
Start Time:   Mon, 14 Aug 2023 16:23:13 -0400
Labels:       &lt;none&gt;
Annotations:  k8s.ovn.org/pod-networks:
                {"default":{"ip_addresses":["10.131.0.26/23"],"mac_address":"0a:58:0a:83:00:1a","gateway_ips":["10.131.0.1"],"routes":[{"dest":"10.128.0.0...
              k8s.v1.cni.cncf.io/network-status:
                [{
                    "name": "ovn-kubernetes",
                    "interface": "eth0",
                    "ips": [
                        "10.131.0.26"
                    ],
                    "mac": "0a:58:0a:83:00:1a",
                    "default": true,
                    "dns": {}
                },{
                    "name": "test-namespace/sriov-network",
                    "interface": "ext0",
                    "mac": "6e:a7:5e:3f:49:1b",
                    "dns": {},
                    "device-info": {
                        "type": "pci",
                        "version": "1.0.0",
                        "pci": {
                            "pci-address": "0000:d8:00.2"
                        }
                    }
                },{
                    "name": "test-namespace/vlan-100",
                    "interface": "ext0.100",
                    "ips": [
                        "1.1.1.1"
                    ],
                    "mac": "6e:a7:5e:3f:49:1b",
                    "dns": {}
                }]
              k8s.v1.cni.cncf.io/networks:
                [ { "name": "sriov-network", "namespace": "test-namespace", "interface": "ext0" }, { "name": "vlan-100", "namespace": "test-namespace", "i...
              openshift.io/scc: privileged
Status:       Running
IP:           10.131.0.26
IPs:
  IP:  10.131.0.26</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-multus-create-master-interface-bridge-cni_configuring-additional-network">
<title>Creating a subinterface based on a bridge master interface in a container namespace</title>
<simpara>Creating a subinterface can be applied to other types of interfaces. Follow this procedure to create a subinterface based on a bridge master interface in a container namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the OpenShift Container Platform cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a dedicated container namespace where you want to deploy your pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project test-namespace</programlisting>
</listitem>
<listitem>
<simpara>Using the following YAML example, create a bridge <literal>NetworkAttachmentDefinition</literal> custom resource (CR) file named <literal>bridge-nad.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bridge-network
spec:
  config: '{
    "cniVersion": "0.4.0",
    "name": "bridge-network",
    "type": "bridge",
    "bridge": "br-001",
    "isGateway": true,
    "ipMasq": true,
    "hairpinMode": true,
    "ipam": {
      "type": "host-local",
      "subnet": "10.0.0.0/24",
      "routes": [{"dst": "0.0.0.0/0"}]
    }
  }'</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to apply the <literal>NetworkAttachmentDefinition</literal> CR to your OpenShift Container Platform cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bridge-nad.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>NetworkAttachmentDefinition</literal> CR has been created successfully by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             AGE
bridge-network   15s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Using the following YAML example, create a file named <literal>ipvlan-additional-network-configuration.yaml</literal> for the IPVLAN additional network configuration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: ipvlan-net
  namespace: test-namespace
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "ipvlan-net",
    "type": "ipvlan",
    "master": "ext0", <co xml:id="CO91-1"/>
    "mode": "l3",
    "linkInContainer": true, <co xml:id="CO91-2"/>
    "ipam": {"type": "whereabouts", "ipRanges": [{"range": "10.0.0.0/24"}]}
  }'</programlisting>
<calloutlist>
<callout arearefs="CO91-1">
<para>Specifies the ethernet interface to associate with the network attachment. This is subsequently configured in the pod networks annotation.</para>
</callout>
<callout arearefs="CO91-2">
<para>Specifies that the master interface is in the container network namespace.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipvlan-additional-network-configuration.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>NetworkAttachmentDefinition</literal> CR has been created successfully by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME             AGE
bridge-network   87s
ipvlan-net       9s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Using the following YAML example, create a file named <literal>pod-a.yaml</literal> for the pod definition:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: pod-a
  namespace: test-namespace
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {
        "name": "bridge-network",
        "interface": "ext0" <co xml:id="CO92-1"/>
      },
      {
        "name": "ipvlan-net",
        "interface": "ext1"
      }
    ]'
spec:
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: test-pod
    image: quay.io/openshifttest/hello-sdn@sha256:c89445416459e7adea9a5a416b3365ed3d74f2491beb904d61dc8d1eb89a72a4
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop: [ALL]</programlisting>
<calloutlist>
<callout arearefs="CO92-1">
<para>Specifies the name to be used as the master for the IPVLAN interface.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f pod-a.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is running by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n test-namespace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME    READY   STATUS    RESTARTS   AGE
pod-a   1/1     Running   0          2m36s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Show network interface information about the <literal>pod-a</literal> resource within the <literal>test-namespace</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n test-namespace pod-a -- ip a</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
3: eth0@if105: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UP group default
    link/ether 0a:58:0a:d9:00:5d brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.217.0.93/23 brd 10.217.1.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::488b:91ff:fe84:a94b/64 scope link
       valid_lft forever preferred_lft forever
4: ext0@if107: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default
    link/ether be:da:bd:7e:f4:37 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.0.0.2/24 brd 10.0.0.255 scope global ext0
       valid_lft forever preferred_lft forever
    inet6 fe80::bcda:bdff:fe7e:f437/64 scope link
       valid_lft forever preferred_lft forever
5: ext1@ext0: &lt;BROADCAST,MULTICAST,NOARP,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UNKNOWN group default
    link/ether be:da:bd:7e:f4:37 brd ff:ff:ff:ff:ff:ff
    inet 10.0.0.1/24 brd 10.0.0.255 scope global ext1
       valid_lft forever preferred_lft forever
    inet6 fe80::beda:bd00:17e:f437/64 scope link
       valid_lft forever preferred_lft forever</programlisting>
</para>
</formalpara>
<simpara>This output shows that the network interface <literal>ext1</literal> is associated with the physical interface <literal>ext0</literal>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="about-virtual-routing-and-forwarding">
<title>About virtual routing and forwarding</title>

<section xml:id="cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding">
<title>About virtual routing and forwarding</title>
<simpara>Virtual routing and forwarding (VRF) devices combined with IP rules provide the ability to create virtual routing and forwarding domains. VRF reduces the number of permissions needed by CNF, and provides increased visibility of the network topology of secondary networks. VRF is used to provide multi-tenancy functionality, for example, where each tenant has its own unique routing tables and requires different default gateways.</simpara>
<simpara>Processes can bind a socket to the VRF device. Packets through the binded socket use the routing table associated with the VRF device. An important feature of VRF is that it impacts only OSI model layer 3 traffic and above so L2 tools, such as LLDP, are not affected. This allows higher priority IP rules such as policy based routing to take precedence over the VRF device rules directing specific traffic.</simpara>
<section xml:id="cnf-benefits-secondary-networks-telecommunications-operators_about-virtual-routing-and-forwarding">
<title>Benefits of secondary networks for pods for telecommunications operators</title>
<simpara>In telecommunications use cases, each CNF can potentially be connected to multiple different networks sharing the same address space. These secondary networks can potentially conflict with the cluster&#8217;s main network CIDR. Using the CNI VRF plugin, network functions can be connected to different customers' infrastructure using the same IP address, keeping different customers isolated. IP addresses are overlapped with OpenShift Container Platform IP space. The CNI VRF plugin also reduces the number of permissions needed by CNF and increases the visibility of network topologies of secondary networks.</simpara>
</section>
</section>
</section>
<section xml:id="configuring-multi-network-policy">
<title>Configuring multi-network policy</title>

<simpara>As a cluster administrator, you can configure multi-network for additional networks. You can specify multi-network policy for SR-IOV, macvlan, and OVN-Kubernetes additional networks. Macvlan additional networks are fully supported. Other types of additional networks, such as ipvlan, are not supported.</simpara>
<important>
<simpara>Support for configuring multi-network policies for SR-IOV additional networks is a Technology Preview feature and is only supported with kernel network interface cards (NICs). SR-IOV is not supported for Data Plane Development Kit (DPDK) applications.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="nw-multi-network-policy-differences_configuring-multi-network-policy">
<title>Differences between multi-network policy and network policy</title>
<simpara>Although the <literal>MultiNetworkPolicy</literal> API implements the <literal>NetworkPolicy</literal> API, there are several important differences:</simpara>
<itemizedlist>
<listitem>
<simpara>You must use the <literal>MultiNetworkPolicy</literal> API:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy</programlisting>
</listitem>
<listitem>
<simpara>You must use the <literal>multi-networkpolicy</literal> resource name when using the CLI to interact with multi-network policies. For example, you can view a multi-network policy object with the <literal>oc get multi-networkpolicy &lt;name&gt;</literal> command where <literal>&lt;name&gt;</literal> is the name of a multi-network policy.</simpara>
</listitem>
<listitem>
<simpara>You must specify an annotation with the name of the network attachment definition that defines the macvlan or SR-IOV additional network:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;network_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a network attachment definition.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-multi-network-policy-enable_configuring-multi-network-policy">
<title>Enabling multi-network policy for the cluster</title>
<simpara>As a cluster administrator, you can enable multi-network policy support on your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>multinetwork-enable-patch.yaml</literal> file with the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  useMultiNetworkPolicy: true</programlisting>
</listitem>
<listitem>
<simpara>Configure the cluster to enable multi-network policy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch network.operator.openshift.io cluster --type=merge --patch-file=multinetwork-enable-patch.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">network.operator.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-multi-network-policy-ipv6-support_configuring-multi-network-policy">
<title>Supporting multi-network policies in IPv6 networks</title>
<simpara>The ICMPv6 Neighbor Discovery Protocol (NDP) is a set of messages and processes that enable devices to discover and maintain information about neighboring nodes. NDP plays a crucial role in IPv6 networks, facilitating the interaction between devices on the same link.</simpara>
<simpara>The Cluster Network Operator (CNO) deploys the iptables implementation of multi-network policy when the <literal>useMultiNetworkPolicy</literal> parameter is set to <literal>true</literal>.</simpara>
<simpara>To support multi-network policies in IPv6 networks the Cluster Network Operator deploys the following set of rules in every pod affected by a multi-network policy:</simpara>
<formalpara>
<title>Multi-network policy custom rules</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: ConfigMap
apiVersion: v1
metadata:
  name: multi-networkpolicy-custom-rules
  namespace: openshift-multus
data:

  custom-v6-rules.txt: |
    # accept NDP
    -p icmpv6 --icmpv6-type neighbor-solicitation -j ACCEPT <co xml:id="CO93-1"/>
    -p icmpv6 --icmpv6-type neighbor-advertisement -j ACCEPT <co xml:id="CO93-2"/>
    # accept RA/RS
    -p icmpv6 --icmpv6-type router-solicitation -j ACCEPT <co xml:id="CO93-3"/>
    -p icmpv6 --icmpv6-type router-advertisement -j ACCEPT <co xml:id="CO93-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO93-1">
<para>This rule allows incoming ICMPv6 neighbor solicitation messages, which are part of the neighbor discovery protocol (NDP). These messages help determine the link-layer addresses of neighboring nodes.</para>
</callout>
<callout arearefs="CO93-2">
<para>This rule allows incoming ICMPv6 neighbor advertisement messages, which are part of NDP and provide information about the link-layer address of the sender.</para>
</callout>
<callout arearefs="CO93-3">
<para>This rule permits incoming ICMPv6 router solicitation messages. Hosts use these messages to request router configuration information.</para>
</callout>
<callout arearefs="CO93-4">
<para>This rule allows incoming ICMPv6 router advertisement messages, which give configuration information to hosts.</para>
</callout>
</calloutlist>
<note>
<simpara>You cannot edit these predefined rules.</simpara>
</note>
<simpara>These rules collectively enable essential ICMPv6 traffic for correct network functioning, including address resolution and router communication in an IPv6 environment. With these rules in place and a multi-network policy denying traffic, applications are not expected to experience connectivity issues.</simpara>
</section>
<section xml:id="configuring-multi-network-policy_working-with-multi-network-policy">
<title>Working with multi-network policy</title>
<simpara>As a cluster administrator, you can create, edit, view, and delete multi-network policies.</simpara>
<section xml:id="configuring-multi-network-policy_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You have enabled multi-network policy support for your cluster.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-networkpolicy-create-cli_configuring-multi-network-policy">
<title>Creating a multi-network policy using the CLI</title>
<simpara>To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a multi-network policy.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the multi-network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy rule:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>&lt;policy_name&gt;.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ touch &lt;policy_name&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the multi-network policy file name.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Define a multi-network policy in the file that you just created, such as in the following examples:</simpara>
<formalpara>
<title>Deny ingress from all pods in all namespaces</title>
<para>This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: deny-by-default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
  ingress: []</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;network_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a network attachment definition.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Allow ingress from all pods in the same namespace</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: allow-same-namespace
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</programlisting>
</para>
</formalpara>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;network_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a network attachment definition.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Allow ingress traffic to one pod from a particular namespace</title>
<para>This policy allows traffic to pods labelled <literal>pod-a</literal> from pods running in <literal>namespace-y</literal>.</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: allow-traffic-pod
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;network_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a network attachment definition.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Restrict traffic to a service</title>
<para>This policy when applied ensures every pod with both labels <literal>app=bookstore</literal> and <literal>role=api</literal> can only be accessed by pods with label <literal>app=bookstore</literal>. In this example the application could be a REST API server, marked with labels <literal>app=bookstore</literal> and <literal>role=api</literal>.</para>
</formalpara>
<simpara>This example addresses the following use cases:</simpara>
<itemizedlist>
<listitem>
<simpara>Restricting the traffic to a service to only the other microservices that need to use it.</simpara>
</listitem>
<listitem>
<simpara>Restricting the connections to a database to only permit the application using it.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: api-allow
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
    matchLabels:
      app: bookstore
      role: api
  ingress:
  - from:
      - podSelector:
          matchLabels:
            app: bookstore</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;network_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of a network attachment definition.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To create the multi-network policy object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;policy_name&gt;.yaml -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the multi-network policy file name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">multinetworkpolicy.k8s.cni.cncf.io/deny-by-default created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-edit_configuring-multi-network-policy">
<title>Editing a multi-network policy</title>
<simpara>You can edit a multi-network policy in a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace where the multi-network policy exists.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To list the multi-network policy objects in a namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get multi-networkpolicy</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Edit the multi-network policy object.</simpara>
<itemizedlist>
<listitem>
<simpara>If you saved the multi-network policy definition in a file, edit the file and make any necessary changes, and then enter the following command.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;namespace&gt; -f &lt;policy_file&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;policy_file&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the file containing the network policy.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>If you need to update the multi-network policy object directly, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the network policy.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Confirm that the multi-network policy object is updated.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the multi-network policy.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of editing a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <emphasis role="strong">Actions</emphasis> menu.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-view-cli_configuring-multi-network-policy">
<title>Viewing multi-network policies using the CLI</title>
<simpara>You can examine the multi-network policies in a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace where the multi-network policy exists.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>List multi-network policies in a namespace:</simpara>
<itemizedlist>
<listitem>
<simpara>To view multi-network policy objects defined in a namespace, enter the following
command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get multi-networkpolicy</programlisting>
</listitem>
<listitem>
<simpara>Optional: To examine a specific multi-network policy, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the multi-network policy to inspect.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of viewing a network policy in any namespace in the cluster directly in YAML or from a form in the web console.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-delete-cli_configuring-multi-network-policy">
<title>Deleting a multi-network policy using the CLI</title>
<simpara>You can delete a multi-network policy in a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace where the multi-network policy exists.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To delete a multi-network policy object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete multi-networkpolicy &lt;policy_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the multi-network policy.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">multinetworkpolicy.k8s.cni.cncf.io/default-deny deleted</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of deleting a network policy in any namespace in the cluster directly in YAML or from the policy in the web console through the <emphasis role="strong">Actions</emphasis> menu.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-deny-all-multi-network-policy_configuring-multi-network-policy">
<title>Creating a default deny all multi-network policy</title>
<simpara>This is a fundamental policy, blocking all cross-pod networking other than network traffic allowed by the configuration of other deployed network policies. This procedure enforces a default <literal>deny-by-default</literal> policy.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the multi-network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following YAML that defines a <literal>deny-by-default</literal> policy to deny ingress from all pods in all namespaces. Save the YAML in the <literal>deny-by-default.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: deny-by-default
  namespace: default <co xml:id="CO94-1"/>
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt; <co xml:id="CO94-2"/>
spec:
  podSelector: {} <co xml:id="CO94-3"/>
  ingress: [] <co xml:id="CO94-4"/></programlisting>
<calloutlist>
<callout arearefs="CO94-1">
<para><literal>namespace: default</literal> deploys this policy to the <literal>default</literal> namespace.</para>
</callout>
<callout arearefs="CO94-2">
<para><literal>network_name</literal>: specifies the name of a network attachment definition.</para>
</callout>
<callout arearefs="CO94-3">
<para><literal>podSelector:</literal> is empty, this means it matches all the pods. Therefore, the policy applies to all pods in the default namespace.</para>
</callout>
<callout arearefs="CO94-4">
<para>There are no <literal>ingress</literal> rules specified. This causes incoming traffic to be dropped to all pods.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f deny-by-default.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">multinetworkpolicy.k8s.cni.cncf.io/deny-by-default created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-networkpolicy-allow-external-clients_configuring-multi-network-policy">
<title>Creating a multi-network policy to allow traffic from external clients</title>
<simpara>With the <literal>deny-by-default</literal> policy in place you can proceed to configure a policy that allows traffic from external clients to a pod with the label <literal>app=web</literal>.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<simpara>Follow this procedure to configure a policy that allows external service from the public Internet directly or by using a Load Balancer to access the pod. Traffic is only allowed to a pod with the label <literal>app=web</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the multi-network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy that allows traffic from the public Internet directly or by using a load balancer to access the pod. Save the YAML in the <literal>web-allow-external.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: web-allow-external
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  policyTypes:
  - Ingress
  podSelector:
    matchLabels:
      app: web
  ingress:
    - {}</programlisting>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-external.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">multinetworkpolicy.k8s.cni.cncf.io/web-allow-external created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<simpara>This policy allows traffic from all resources, including external traffic as illustrated in the following diagram:</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/292_OpenShift_Configuring_multi-network_policy_1122.png"/>
</imageobject>
<textobject><phrase>Allow traffic from external clients</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="nw-networkpolicy-allow-traffic-from-all-applications_configuring-multi-network-policy">
<title>Creating a multi-network policy allowing traffic to an application from all namespaces</title>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<simpara>Follow this procedure to configure a policy that allows traffic from all pods in all namespaces to a particular application.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the multi-network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy that allows traffic from all pods in all namespaces to a particular application. Save the YAML in the <literal>web-allow-all-namespaces.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: web-allow-all-namespaces
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
    matchLabels:
      app: web <co xml:id="CO95-1"/>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector: {} <co xml:id="CO95-2"/></programlisting>
<calloutlist>
<callout arearefs="CO95-1">
<para>Applies the policy only to <literal>app:web</literal> pods in default namespace.</para>
</callout>
<callout arearefs="CO95-2">
<para>Selects all pods in all namespaces.</para>
</callout>
</calloutlist>
<note>
<simpara>By default, if you omit specifying a <literal>namespaceSelector</literal> it does not select any namespaces, which means the policy allows traffic only from the namespace the network policy is deployed to.</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-all-namespaces.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">multinetworkpolicy.k8s.cni.cncf.io/web-allow-all-namespaces created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Start a web service in the <literal>default</literal> namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>secondary</literal> namespace and to start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-$RANDOM --namespace=secondary --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Run the following command in the shell and observe that the request is allowed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-networkpolicy-allow-traffic-from-a-namespace_configuring-multi-network-policy">
<title>Creating a multi-network policy allowing traffic to an application from a namespace</title>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<simpara>Follow this procedure to configure a policy that allows traffic to a pod with the label <literal>app=web</literal> from a particular namespace. You might want to do this to:</simpara>
<itemizedlist>
<listitem>
<simpara>Restrict traffic to a production database only to namespaces where production workloads are deployed.</simpara>
</listitem>
<listitem>
<simpara>Enable monitoring tools deployed to a particular namespace to scrape metrics from the current namespace.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the multi-network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy that allows traffic from all pods in a particular namespaces with a label <literal>purpose=production</literal>. Save the YAML in the <literal>web-allow-prod.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1beta1
kind: MultiNetworkPolicy
metadata:
  name: web-allow-prod
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/policy-for: &lt;network_name&gt;
spec:
  podSelector:
    matchLabels:
      app: web <co xml:id="CO96-1"/>
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production <co xml:id="CO96-2"/></programlisting>
<calloutlist>
<callout arearefs="CO96-1">
<para>Applies the policy only to <literal>app:web</literal> pods in the default namespace.</para>
</callout>
<callout arearefs="CO96-2">
<para>Restricts traffic to only pods in namespaces that have the label <literal>purpose=production</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-prod.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">multinetworkpolicy.k8s.cni.cncf.io/web-allow-prod created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Start a web service in the <literal>default</literal> namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run web --namespace=default --image=nginx --labels="app=web" --expose --port=80</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to create the <literal>prod</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace prod</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to label the <literal>prod</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace/prod purpose=production</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to create the <literal>dev</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace dev</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to label the <literal>dev</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace/dev purpose=testing</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>dev</literal> namespace and to start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-$RANDOM --namespace=dev --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Run the following command in the shell and observe that the request is blocked:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">wget: download timed out</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>prod</literal> namespace and start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-$RANDOM --namespace=prod --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Run the following command in the shell and observe that the request is allowed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
html { color-scheme: light dark; }
body { width: 35em; margin: 0 auto;
font-family: Tahoma, Verdana, Arial, sans-serif; }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-multi-network-policy_additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="about-network-policy">About network policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="understanding-multiple-networks">Understanding multiple networks</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-macvlan-object_configuring-additional-network">Configuring a macvlan network</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="attaching-pod">
<title>Attaching a pod to an additional network</title>

<simpara>As a cluster user you can attach a pod to an additional network.</simpara>
<section xml:id="nw-multus-add-pod_attaching-pod">
<title>Adding a pod to an additional network</title>
<simpara>You can add a pod to an additional network. The pod continues to send normal cluster-related network traffic over the default network.</simpara>
<simpara>When a pod is created additional networks are attached to it. However, if a pod already exists, you cannot attach additional networks to it.</simpara>
<simpara>The pod must be in the same namespace as the additional network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add an annotation to the <literal>Pod</literal> object. Only one of the following annotation formats can be used:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To attach an additional network without any customization, add an annotation with the following format. Replace <literal>&lt;network&gt;</literal> with the name of the additional network to associate with the pod:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: &lt;network&gt;[,&lt;network&gt;,...] <co xml:id="CO97-1"/></programlisting>
<calloutlist>
<callout arearefs="CO97-1">
<para>To specify more than one additional network, separate each network
with a comma. Do not include whitespace between the comma. If you specify
the same additional network multiple times, that pod will have multiple network
interfaces attached to that network.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To attach an additional network with customizations, add an annotation with the following format:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "&lt;network&gt;", <co xml:id="CO98-1"/>
          "namespace": "&lt;namespace&gt;", <co xml:id="CO98-2"/>
          "default-route": ["&lt;default-route&gt;"] <co xml:id="CO98-3"/>
        }
      ]</programlisting>
<calloutlist>
<callout arearefs="CO98-1">
<para>Specify the name of the additional network defined by a <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO98-2">
<para>Specify the namespace where the <literal>NetworkAttachmentDefinition</literal> object is defined.</para>
</callout>
<callout arearefs="CO98-3">
<para>Optional: Specify an override for the default route, such as <literal>192.168.17.1</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To create the pod, enter the following command. Replace <literal>&lt;name&gt;</literal> with the name of the pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To Confirm that the annotation exists in the <literal>Pod</literal> CR, enter the following command, replacing <literal>&lt;name&gt;</literal> with the name of the pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod &lt;name&gt; -o yaml</programlisting>
<simpara>In the following example, the <literal>example-pod</literal> pod is attached to the <literal>net1</literal>
additional network:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod example-pod -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: macvlan-bridge
    k8s.v1.cni.cncf.io/network-status: |- <co xml:id="CO99-1"/>
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.128.2.14"
          ],
          "default": true,
          "dns": {}
      },{
          "name": "macvlan-bridge",
          "interface": "net1",
          "ips": [
              "20.2.2.100"
          ],
          "mac": "22:2f:60:a5:f8:00",
          "dns": {}
      }]
  name: example-pod
  namespace: default
spec:
  ...
status:
  ...</programlisting>
<calloutlist>
<callout arearefs="CO99-1">
<para>The <literal>k8s.v1.cni.cncf.io/network-status</literal> parameter is a JSON array of
objects. Each object describes the status of an additional network attached
to the pod. The annotation value is stored as a plain text value.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<section xml:id="nw-multus-advanced-annotations_attaching-pod">
<title>Specifying pod-specific addressing and routing options</title>
<simpara>When attaching a pod to an additional network, you may want to specify further properties
about that network in a particular pod. This allows you to change some aspects of routing, as well
as specify static IP addresses and MAC addresses. To accomplish this, you can use the JSON formatted annotations.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The pod must be in the same namespace as the additional network.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To add a pod to an additional network while specifying addressing and/or routing options, complete the following steps:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>Pod</literal> resource definition. If you are editing an existing <literal>Pod</literal> resource, run the
following command to edit its definition in the default editor. Replace <literal>&lt;name&gt;</literal>
with the name of the <literal>Pod</literal> resource to edit.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit pod &lt;name&gt;</programlisting>
</listitem>
<listitem>
<simpara>In the <literal>Pod</literal> resource definition, add the <literal>k8s.v1.cni.cncf.io/networks</literal>
parameter to the pod <literal>metadata</literal> mapping. The <literal>k8s.v1.cni.cncf.io/networks</literal>
accepts a JSON string of a list of objects that reference the name of <literal>NetworkAttachmentDefinition</literal> custom resource (CR) names
in addition to specifying additional properties.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: '[&lt;network&gt;[,&lt;network&gt;,...]]' <co xml:id="CO100-1"/></programlisting>
<calloutlist>
<callout arearefs="CO100-1">
<para>Replace <literal>&lt;network&gt;</literal> with a JSON object as shown in the following examples. The single quotes are required.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>In the following example the annotation specifies which network attachment will have the default route,
using the <literal>default-route</literal> parameter.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
    {
      "name": "net1"
    },
    {
      "name": "net2", <co xml:id="CO101-1"/>
      "default-route": ["192.0.2.1"] <co xml:id="CO101-2"/>
    }]'
spec:
  containers:
  - name: example-pod
    command: ["/bin/bash", "-c", "sleep 2000000000000"]
    image: centos/tools</programlisting>
<calloutlist>
<callout arearefs="CO101-1">
<para>The <literal>name</literal> key is the name of the additional network to associate
with the pod.</para>
</callout>
<callout arearefs="CO101-2">
<para>The <literal>default-route</literal> key specifies a value of a gateway for traffic to be routed over if no other
routing entry is present in the routing table. If more than one <literal>default-route</literal> key is specified,
this will cause the pod to fail to become active.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<simpara>The default route will cause any traffic that is not specified in other routes to be routed to the gateway.</simpara>
<important>
<simpara>Setting the default route to an interface other than the default network interface for OpenShift Container Platform
may cause traffic that is anticipated for pod-to-pod traffic to be routed over another interface.</simpara>
</important>
<simpara>To verify the routing properties of a pod, the <literal>oc</literal> command may be used to execute the <literal>ip</literal> command within a pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it &lt;pod_name&gt; -- ip route</programlisting>
<note>
<simpara>You may also reference the pod&#8217;s <literal>k8s.v1.cni.cncf.io/network-status</literal> to see which additional network has been
assigned the default route, by the presence of the <literal>default-route</literal> key in the JSON-formatted list of objects.</simpara>
</note>
<simpara>To set a static IP address or MAC address for a pod you can use the JSON formatted annotations. This requires you create networks that specifically allow for this functionality. This can be specified in a rawCNIConfig for the CNO.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the CNO CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.operator.openshift.io cluster</programlisting>
</listitem>
</orderedlist>
<simpara>The following YAML describes the configuration parameters for the CNO:</simpara>
<formalpara>
<title>Cluster Network Operator YAML configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">name: &lt;name&gt; <co xml:id="CO102-1"/>
namespace: &lt;namespace&gt; <co xml:id="CO102-2"/>
rawCNIConfig: '{ <co xml:id="CO102-3"/>
  ...
}'
type: Raw</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO102-1">
<para>Specify a name for the additional network attachment that you are
creating. The name must be unique within the specified <literal>namespace</literal>.</para>
</callout>
<callout arearefs="CO102-2">
<para>Specify the namespace to create the network attachment in. If
you do not specify a value, then the <literal>default</literal> namespace is used.</para>
</callout>
<callout arearefs="CO102-3">
<para>Specify the CNI plugin configuration in JSON format, which
is based on the following template.</para>
</callout>
</calloutlist>
<simpara>The following object describes the configuration parameters for utilizing static MAC address and IP address using the macvlan CNI plugin:</simpara>
<formalpara>
<title>macvlan CNI plugin JSON configuration object using static IP and MAC address</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "cniVersion": "0.3.1",
  "name": "&lt;name&gt;", <co xml:id="CO103-1"/>
  "plugins": [{ <co xml:id="CO103-2"/>
      "type": "macvlan",
      "capabilities": { "ips": true }, <co xml:id="CO103-3"/>
      "master": "eth0", <co xml:id="CO103-4"/>
      "mode": "bridge",
      "ipam": {
        "type": "static"
      }
    }, {
      "capabilities": { "mac": true }, <co xml:id="CO103-5"/>
      "type": "tuning"
    }]
}</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO103-1">
<para>Specifies the name for the additional network attachment to create. The name must be unique within the specified <literal>namespace</literal>.</para>
</callout>
<callout arearefs="CO103-2">
<para>Specifies an array of CNI plugin configurations. The first object specifies a macvlan plugin configuration and the second object specifies a tuning plugin configuration.</para>
</callout>
<callout arearefs="CO103-3">
<para>Specifies that a request is made to enable the static IP address functionality of the CNI plugin runtime configuration capabilities.</para>
</callout>
<callout arearefs="CO103-4">
<para>Specifies the interface that the macvlan plugin uses.</para>
</callout>
<callout arearefs="CO103-5">
<para>Specifies that a request is made to enable the static MAC address functionality of a CNI plugin.</para>
</callout>
</calloutlist>
<simpara>The above network attachment can be referenced in a JSON formatted annotation, along with keys to specify which static IP and MAC address will be assigned to a given pod.</simpara>
<simpara>Edit the pod with:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit pod &lt;name&gt;</programlisting>
<formalpara>
<title>macvlan CNI plugin JSON configuration object using static IP and MAC address</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: example-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {
        "name": "&lt;name&gt;", <co xml:id="CO104-1"/>
        "ips": [ "192.0.2.205/24" ], <co xml:id="CO104-2"/>
        "mac": "CA:FE:C0:FF:EE:00" <co xml:id="CO104-3"/>
      }
    ]'</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO104-1">
<para>Use the <literal>&lt;name&gt;</literal> as provided when creating the <literal>rawCNIConfig</literal> above.</para>
</callout>
<callout arearefs="CO104-2">
<para>Provide an IP address including the subnet mask.</para>
</callout>
<callout arearefs="CO104-3">
<para>Provide the MAC address.</para>
</callout>
</calloutlist>
<note>
<simpara>Static IP addresses and MAC addresses do not have to be used at the same time, you may use them individually, or together.</simpara>
</note>
<simpara>To verify the IP address and MAC properties of a pod with additional networks, use the <literal>oc</literal> command to execute the ip command within a pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it &lt;pod_name&gt; -- ip a</programlisting>
</section>
</section>
</section>
<section xml:id="removing-pod">
<title>Removing a pod from an additional network</title>

<simpara>As a cluster user you can remove a pod from an additional network.</simpara>
<section xml:id="nw-multus-remove-pod_removing-pod">
<title>Removing a pod from an additional network</title>
<simpara>You can remove a pod from an additional network only by deleting the pod.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An additional network is attached to the pod.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To delete the pod, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod &lt;name&gt; -n &lt;namespace&gt;</programlisting>
<itemizedlist>
<listitem>
<simpara><literal>&lt;name&gt;</literal> is the name of the pod.</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;namespace&gt;</literal> is the namespace that contains the pod.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="edit-additional-network">
<title>Editing an additional network</title>

<simpara>As a cluster administrator you can modify the configuration for an existing
additional network.</simpara>
<section xml:id="nw-multus-edit-network_edit-additional-network">
<title>Modifying an additional network attachment definition</title>
<simpara>As a cluster administrator, you can make changes to an existing additional
network. Any existing pods attached to the additional network will not be updated.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured an additional network for your cluster.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To edit an additional network for your cluster, complete the following steps:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Run the following command to edit the Cluster Network Operator (CNO) CR in
your default text editor:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.operator.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>In the <literal>additionalNetworks</literal> collection, update the additional network with
your changes.</simpara>
</listitem>
<listitem>
<simpara>Save your changes and quit the text editor to commit your changes.</simpara>
</listitem>
<listitem>
<simpara>Optional: Confirm that the CNO updated the <literal>NetworkAttachmentDefinition</literal> object by running the following command. Replace <literal>&lt;network-name&gt;</literal> with the name of the additional network to display. There might be a delay before the CNO updates the <literal>NetworkAttachmentDefinition</literal> object to reflect your changes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions &lt;network-name&gt; -o yaml</programlisting>
<simpara>For example, the following console output displays a <literal>NetworkAttachmentDefinition</literal> object that is named <literal>net1</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions net1 -o go-template='{{printf "%s\n" .spec.config}}'
{ "cniVersion": "0.3.1", "type": "macvlan",
"master": "ens5",
"mode": "bridge",
"ipam":       {"type":"static","routes":[{"dst":"0.0.0.0/0","gw":"10.128.2.1"}],"addresses":[{"address":"10.128.2.100/23","gateway":"10.128.2.1"}],"dns":{"nameservers":["172.30.0.10"],"domain":"us-west-2.compute.internal","search":["us-west-2.compute.internal"]}} }</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="remove-additional-network">
<title>Removing an additional network</title>

<simpara>As a cluster administrator you can remove an additional network attachment.</simpara>
<section xml:id="nw-multus-delete-network_remove-additional-network">
<title>Removing an additional network attachment definition</title>
<simpara>As a cluster administrator, you can remove an additional network from your
OpenShift Container Platform cluster. The additional network is not removed from any pods it
is attached to.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To remove an additional network from your cluster, complete the following steps:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the Cluster Network Operator (CNO) in your default text editor by running
the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.operator.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>Modify the CR by removing the configuration from the <literal>additionalNetworks</literal>
collection for the network attachment definition you are removing.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks: [] <co xml:id="CO105-1"/></programlisting>
<calloutlist>
<callout arearefs="CO105-1">
<para>If you are removing the configuration mapping for the only additional
network attachment definition in the <literal>additionalNetworks</literal> collection, you must
specify an empty collection.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save your changes and quit the text editor to commit your changes.</simpara>
</listitem>
<listitem>
<simpara>Optional: Confirm that the additional network CR was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definition --all-namespaces</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="assigning-a-secondary-network-to-a-vrf">
<title>Assigning a secondary network to a VRF</title>

<section xml:id="cnf-assigning-a-secondary-network-to-a-vrf_assigning-a-secondary-network-to-a-vrf">
<title>Assigning a secondary network to a VRF</title>
<simpara>As a cluster administrator, you can configure an additional network for your VRF domain by using the CNI VRF plugin. The virtual network created by this plugin is associated with a physical interface that you specify.</simpara>
<note>
<simpara>Applications that use VRFs need to bind to a specific device. The common usage is to use the <literal>SO_BINDTODEVICE</literal> option for a socket. <literal>SO_BINDTODEVICE</literal> binds the socket to a device that is specified in the passed interface name, for example, <literal>eth1</literal>. To use <literal>SO_BINDTODEVICE</literal>, the application must have <literal>CAP_NET_RAW</literal> capabilities.</simpara>
<simpara>Using a VRF through the <literal>ip vrf exec</literal> command is not supported in OpenShift Container Platform pods. To use VRF, bind applications directly to the VRF interface.</simpara>
</note>
<section xml:id="cnf-creating-an-additional-network-attachment-with-the-cni-vrf-plug-in_assigning-a-secondary-network-to-a-vrf">
<title>Creating an additional network attachment with the CNI VRF plugin</title>
<simpara>The Cluster Network Operator (CNO) manages additional network definitions. When you specify an additional network to create, the CNO creates the <literal>NetworkAttachmentDefinition</literal> custom resource (CR) automatically.</simpara>
<note>
<simpara>Do not edit the <literal>NetworkAttachmentDefinition</literal> CRs that the Cluster Network Operator manages. Doing so might disrupt network traffic on your additional network.</simpara>
</note>
<simpara>To create an additional network attachment with the CNI VRF plugin, perform the following procedure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI (oc).</simpara>
</listitem>
<listitem>
<simpara>Log in to the OpenShift cluster as a user with cluster-admin privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>Network</literal> custom resource (CR) for the additional network attachment and insert the <literal>rawCNIConfig</literal> configuration for the additional network, as in the following example CR. Save the YAML as the file <literal>additional-network-attachment.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
  spec:
  additionalNetworks:
  - name: test-network-1
    namespace: additional-network-1
    type: Raw
    rawCNIConfig: '{
      "cniVersion": "0.3.1",
      "name": "macvlan-vrf",
      "plugins": [  <co xml:id="CO106-1"/>
      {
        "type": "macvlan",  <co xml:id="CO106-2"/>
        "master": "eth1",
        "ipam": {
            "type": "static",
            "addresses": [
            {
                "address": "191.168.1.23/24"
            }
            ]
        }
      },
      {
        "type": "vrf",
        "vrfname": "example-vrf-name",  <co xml:id="CO106-3"/>
        "table": 1001   <co xml:id="CO106-4"/>
      }]
    }'</programlisting>
<calloutlist>
<callout arearefs="CO106-1">
<para><literal>plugins</literal> must be a list. The first item in the list must be the secondary network underpinning the VRF network. The second item in the list is the VRF plugin configuration.</para>
</callout>
<callout arearefs="CO106-2">
<para><literal>type</literal> must be set to <literal>vrf</literal>.</para>
</callout>
<callout arearefs="CO106-3">
<para><literal>vrfname</literal> is the name of the VRF that the interface is assigned to. If it does not exist in the pod, it is created.</para>
</callout>
<callout arearefs="CO106-4">
<para>Optional. <literal>table</literal> is the routing table ID. By default, the <literal>tableid</literal> parameter is used. If it is not specified, the CNI assigns a free routing table ID to the VRF.</para>
</callout>
</calloutlist>
<note>
<simpara>VRF functions correctly only when the resource is of type <literal>netdevice</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the <literal>Network</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f additional-network-attachment.yaml</programlisting>
</listitem>
<listitem>
<simpara>Confirm that the CNO created the <literal>NetworkAttachmentDefinition</literal> CR by running the following command. Replace <literal>&lt;namespace&gt;</literal> with the namespace that you specified when configuring the network attachment, for example, <literal>additional-network-1</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -n &lt;namespace&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                       AGE
additional-network-1       14m</programlisting>
</para>
</formalpara>
<note>
<simpara>There might be a delay before the CNO creates the CR.</simpara>
</note>
</listitem>
</orderedlist>
<formalpara>
<title>Verifying that the additional VRF network attachment is successful</title>
<para>To verify that the VRF CNI is correctly configured and the additional network attachment is attached, do the following:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a network that uses the VRF CNI.</simpara>
</listitem>
<listitem>
<simpara>Assign the network to a pod.</simpara>
</listitem>
<listitem>
<simpara>Verify that the pod network attachment is connected to the VRF additional network. Remote shell into the pod and run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip vrf show</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name              Table
-----------------------
red                 10</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Confirm the VRF interface is master of the secondary interface:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip link</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">5: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master red state UP mode</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_hardware-networks">
<title>Hardware networks</title>
<section xml:id="about-sriov">
<title>About Single Root I/O Virtualization (SR-IOV) hardware networks</title>

<simpara>The Single Root I/O Virtualization (SR-IOV) specification is a standard for a type of PCI device assignment that can share a single device with multiple pods.</simpara>
<simpara>SR-IOV can segment a compliant network device, recognized on the host node as a physical function (PF), into multiple virtual functions (VFs).
The VF is used like any other network device.
The SR-IOV network device driver for the device determines how the VF is exposed in the container:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>netdevice</literal> driver: A regular kernel network device in the <literal>netns</literal> of the container</simpara>
</listitem>
<listitem>
<simpara><literal>vfio-pci</literal> driver: A character device mounted in the container</simpara>
</listitem>
</itemizedlist>
<simpara>You can use SR-IOV network devices with additional networks on your OpenShift Container Platform cluster installed on bare metal or Red Hat OpenStack Platform (RHOSP) infrastructure for applications that require high bandwidth or low latency.</simpara>
<simpara>You can configure multi-network policies for SR-IOV networks. The support for this is technology preview and SR-IOV additional networks are only supported with kernel NICs. They are not supported for Data Plane Development Kit (DPDK) applications.</simpara>
<note>
<simpara>Creating multi-network policies on SR-IOV networks might not deliver the same performance to applications compared to SR-IOV networks without a multi-network policy configured.</simpara>
</note>
<important>
<simpara>Multi-network policies for SR-IOV network is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You can enable SR-IOV on a node by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node_name&gt; feature.node.kubernetes.io/network-sriov.capable="true"</programlisting>
<section xml:id="components-sr-iov-network-devices">
<title>Components that manage SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator creates and manages the components of the SR-IOV stack.
It performs the following functions:</simpara>
<itemizedlist>
<listitem>
<simpara>Orchestrates discovery and management of SR-IOV network devices</simpara>
</listitem>
<listitem>
<simpara>Generates <literal>NetworkAttachmentDefinition</literal> custom resources for the SR-IOV Container Network Interface (CNI)</simpara>
</listitem>
<listitem>
<simpara>Creates and updates the configuration of the SR-IOV network device plugin</simpara>
</listitem>
<listitem>
<simpara>Creates node specific <literal>SriovNetworkNodeState</literal> custom resources</simpara>
</listitem>
<listitem>
<simpara>Updates the <literal>spec.interfaces</literal> field in each <literal>SriovNetworkNodeState</literal> custom resource</simpara>
</listitem>
</itemizedlist>
<simpara>The Operator provisions the following components:</simpara>
<variablelist>
<varlistentry>
<term>SR-IOV network configuration daemon</term>
<listitem>
<simpara>A daemon set that is deployed on worker nodes when the SR-IOV Network Operator starts. The daemon is responsible for discovering and initializing SR-IOV network devices in the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>SR-IOV Network Operator webhook</term>
<listitem>
<simpara>A dynamic admission controller webhook that validates the Operator custom resource and sets appropriate default values for unset fields.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>SR-IOV Network resources injector</term>
<listitem>
<simpara>A dynamic admission controller webhook that provides functionality for patching Kubernetes pod specifications with requests and limits for custom network resources such as SR-IOV VFs. The SR-IOV network resources injector adds the <literal>resource</literal> field to only the first container in a pod automatically.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>SR-IOV network device plugin</term>
<listitem>
<simpara>A device plugin that discovers, advertises, and allocates SR-IOV network virtual function (VF) resources. Device plugins are used in Kubernetes to enable the use of limited resources, typically in physical devices. Device plugins give the Kubernetes scheduler awareness of resource availability, so that the scheduler can schedule pods on nodes with sufficient resources.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>SR-IOV CNI plugin</term>
<listitem>
<simpara>A CNI plugin that attaches VF interfaces allocated from the SR-IOV network device plugin directly into a pod.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>SR-IOV InfiniBand CNI plugin</term>
<listitem>
<simpara>A CNI plugin that attaches InfiniBand (IB) VF interfaces allocated from the SR-IOV network device plugin directly into a pod.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>The SR-IOV Network resources injector and SR-IOV Network Operator webhook are enabled by default and can be disabled by editing the <literal>default</literal> <literal>SriovOperatorConfig</literal> CR.
Use caution when disabling the SR-IOV Network Operator Admission Controller webhook. You can disable the webhook under specific circumstances, such as troubleshooting, or if you want to use unsupported devices.</simpara>
</note>
<section xml:id="nw-sriov-supported-platforms_about-sriov">
<title>Supported platforms</title>
<simpara>The SR-IOV Network Operator is supported on the following platforms:</simpara>
<itemizedlist>
<listitem>
<simpara>Bare metal</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenStack Platform (RHOSP)</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="supported-devices_about-sriov">
<title>Supported devices</title>
<simpara>OpenShift Container Platform supports the following network interface controllers:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supported network interface controllers</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="40*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Manufacturer</entry>
<entry align="left" valign="top">Model</entry>
<entry align="left" valign="top">Vendor ID</entry>
<entry align="left" valign="top">Device ID</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Broadcom</simpara></entry>
<entry align="left" valign="top"><simpara>BCM57414</simpara></entry>
<entry align="left" valign="top"><simpara>14e4</simpara></entry>
<entry align="left" valign="top"><simpara>16d7</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Broadcom</simpara></entry>
<entry align="left" valign="top"><simpara>BCM57508</simpara></entry>
<entry align="left" valign="top"><simpara>14e4</simpara></entry>
<entry align="left" valign="top"><simpara>1750</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Broadcom</simpara></entry>
<entry align="left" valign="top"><simpara>BCM57504</simpara></entry>
<entry align="left" valign="top"><simpara>14e4</simpara></entry>
<entry align="left" valign="top"><simpara>1751</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>X710</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1572</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>X710 Backplane</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1581</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>X710 Base T</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>15ff</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>XL710</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1583</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>XXV710</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>158b</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>E810-CQDA2</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1592</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>E810-2CQDA2</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1592</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>E810-XXVDA2</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>159b</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>E810-XXVDA4</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1593</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Intel</simpara></entry>
<entry align="left" valign="top"><simpara>E810-XXVDA4T</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1593</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT27700 Family [ConnectX&#8209;4]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1013</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT27710 Family [ConnectX&#8209;4&#160;Lx]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1015</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT27800 Family [ConnectX&#8209;5]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1017</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT28880 Family [ConnectX&#8209;5&#160;Ex]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1019</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT28908 Family [ConnectX&#8209;6]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>101b</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT2892 Family [ConnectX&#8209;6&#160;Dx]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>101d</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT2894 Family [ConnectX&#8209;6&#160;Lx]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>101f</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>Mellanox MT2910 Family [ConnectX&#8209;7]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1021</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT42822 BlueField&#8209;2 in ConnectX&#8209;6 NIC mode</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>a2d6</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Pensando <superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>DSC-25 dual-port 25G distributed services card for ionic driver</simpara></entry>
<entry align="left" valign="top"><simpara>0x1dd8</simpara></entry>
<entry align="left" valign="top"><simpara>0x1002</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Pensando <superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>DSC-100 dual-port 100G distributed services card for ionic driver</simpara></entry>
<entry align="left" valign="top"><simpara>0x1dd8</simpara></entry>
<entry align="left" valign="top"><simpara>0x1003</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Silicom</simpara></entry>
<entry align="left" valign="top"><simpara>STS Family</simpara></entry>
<entry align="left" valign="top"><simpara>8086</simpara></entry>
<entry align="left" valign="top"><simpara>1591</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>OpenShift SR-IOV is supported, but you must set a static, Virtual Function (VF) media access control (MAC) address using the SR-IOV CNI config file when using SR-IOV.</simpara>
</listitem>
</orderedlist>
</para>
<note>
<simpara>For the most up-to-date list of supported cards and compatible OpenShift Container Platform versions available, see <link xlink:href="https://access.redhat.com/articles/6954499">Openshift Single Root I/O Virtualization (SR-IOV) and PTP hardware networks Support Matrix</link>.</simpara>
</note>
</section>
<section xml:id="discover-sr-iov-devices_about-sriov">
<title>Automated discovery of SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator searches your cluster for SR-IOV capable network devices on worker nodes.
The Operator creates and updates a SriovNetworkNodeState custom resource (CR) for each worker node that provides a compatible SR-IOV network device.</simpara>
<simpara>The CR is assigned the same name as the worker node.
The <literal>status.interfaces</literal> list provides information about the network devices on a node.</simpara>
<important>
<simpara>Do not modify a <literal>SriovNetworkNodeState</literal> object.
The Operator creates and manages these resources automatically.</simpara>
</important>
<section xml:id="example-sriovnetworknodestate_about-sriov">
<title>Example SriovNetworkNodeState object</title>
<simpara>The following YAML is an example of a <literal>SriovNetworkNodeState</literal> object created by the SR-IOV Network Operator:</simpara>
<formalpara>
<title>An SriovNetworkNodeState object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodeState
metadata:
  name: node-25 <co xml:id="CO107-1"/>
  namespace: openshift-sriov-network-operator
  ownerReferences:
  - apiVersion: sriovnetwork.openshift.io/v1
    blockOwnerDeletion: true
    controller: true
    kind: SriovNetworkNodePolicy
    name: default
spec:
  dpConfigVersion: "39824"
status:
  interfaces: <co xml:id="CO107-2"/>
  - deviceID: "1017"
    driver: mlx5_core
    mtu: 1500
    name: ens785f0
    pciAddress: "0000:18:00.0"
    totalvfs: 8
    vendor: 15b3
  - deviceID: "1017"
    driver: mlx5_core
    mtu: 1500
    name: ens785f1
    pciAddress: "0000:18:00.1"
    totalvfs: 8
    vendor: 15b3
  - deviceID: 158b
    driver: i40e
    mtu: 1500
    name: ens817f0
    pciAddress: 0000:81:00.0
    totalvfs: 64
    vendor: "8086"
  - deviceID: 158b
    driver: i40e
    mtu: 1500
    name: ens817f1
    pciAddress: 0000:81:00.1
    totalvfs: 64
    vendor: "8086"
  - deviceID: 158b
    driver: i40e
    mtu: 1500
    name: ens803f0
    pciAddress: 0000:86:00.0
    totalvfs: 64
    vendor: "8086"
  syncStatus: Succeeded</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO107-1">
<para>The value of the <literal>name</literal> field is the same as the name of the worker node.</para>
</callout>
<callout arearefs="CO107-2">
<para>The <literal>interfaces</literal> stanza includes a list of all of the SR-IOV devices discovered by the Operator on the worker node.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="example-vf-use-in-pod_about-sriov">
<title>Example use of a virtual function in a pod</title>
<simpara>You can run a remote direct memory access (RDMA) or a Data Plane Development Kit (DPDK) application in a pod with SR-IOV VF attached.</simpara>
<simpara>This example shows a pod using a virtual function (VF) in RDMA mode:</simpara>
<formalpara>
<title><literal>Pod</literal> spec that uses RDMA mode</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: rdma-app
  annotations:
    k8s.v1.cni.cncf.io/networks: sriov-rdma-mlnx
spec:
  containers:
  - name: testpmd
    image: &lt;RDMA_image&gt;
    imagePullPolicy: IfNotPresent
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"]
    command: ["sleep", "infinity"]</programlisting>
</para>
</formalpara>
<simpara>The following example shows a pod with a VF in DPDK mode:</simpara>
<formalpara>
<title><literal>Pod</literal> spec that uses DPDK mode</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  annotations:
    k8s.v1.cni.cncf.io/networks: sriov-dpdk-net
spec:
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt;
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"]
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
        memory: "1Gi"
        cpu: "2"
        hugepages-1Gi: "4Gi"
      requests:
        memory: "1Gi"
        cpu: "2"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-sriov-app-netutil_about-sriov">
<title>DPDK library for use with container applications</title>
<simpara>An <link xlink:href="https://github.com/openshift/app-netutil">optional library</link>, <literal>app-netutil</literal>, provides several API methods for gathering network information about a pod from within a container running within that pod.</simpara>
<simpara>This library can assist with integrating SR-IOV virtual functions (VFs) in Data Plane Development Kit (DPDK) mode into the container.
The library provides both a Golang API and a C API.</simpara>
<simpara>Currently there are three API methods implemented:</simpara>
<variablelist>
<varlistentry>
<term><literal>GetCPUInfo()</literal></term>
<listitem>
<simpara>This function determines which CPUs are available to the container and returns the list.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>GetHugepages()</literal></term>
<listitem>
<simpara>This function determines the amount of huge page memory requested in the <literal>Pod</literal> spec for each container and returns the values.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>GetInterfaces()</literal></term>
<listitem>
<simpara>This function determines the set of interfaces in the container and returns the list. The return value includes the interface type and type-specific data for each interface.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The repository for the library includes a sample Dockerfile to build a container image, <literal>dpdk-app-centos</literal>. The container image can run one of the following DPDK sample applications, depending on an environment variable in the pod specification: <literal>l2fwd</literal>, <literal>l3wd</literal> or <literal>testpmd</literal>. The container image provides an example of integrating the <literal>app-netutil</literal> library into the container image itself. The library can also integrate into an init container. The init container can collect the required data and pass the data to an existing DPDK workload.</simpara>
</section>
<section xml:id="nw-sriov-hugepages_about-sriov">
<title>Huge pages resource injection for Downward API</title>
<simpara>When a pod specification includes a resource request or limit for huge pages, the Network Resources Injector automatically adds Downward API fields to the pod specification to provide the huge pages information to the container.</simpara>
<simpara>The Network Resources Injector adds a volume that is named <literal>podnetinfo</literal> and is mounted at <literal>/etc/podnetinfo</literal> for each container in the pod. The volume uses the Downward API and includes a file for huge pages requests and limits. The file naming convention is as follows:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_1G_request_&lt;container-name&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_1G_limit_&lt;container-name&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_2M_request_&lt;container-name&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_2M_limit_&lt;container-name&gt;</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The paths specified in the previous list are compatible with the <literal>app-netutil</literal> library. By default, the library is configured to search for resource information in the <literal>/etc/podnetinfo</literal> directory. If you choose to specify the Downward API path items yourself manually, the <literal>app-netutil</literal> library searches for the following paths in addition to the paths in the previous list.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_request</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_limit</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_1G_request</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_1G_limit</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_2M_request</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/podnetinfo/hugepages_2M_limit</literal></simpara>
</listitem>
</itemizedlist>
<simpara>As with the paths that the Network Resources Injector can create, the paths in the preceding list can optionally end with a <literal>_&lt;container-name&gt;</literal> suffix.</simpara>
</section>
</section>
<section xml:id="configure-multi-networks-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-multi-network-policy">Configuring multi-network policy</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-sriov-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="installing-sriov-operator">Installing the SR-IOV Network Operator</link></simpara>
</listitem>
<listitem>
<simpara>Optional: <link linkend="configuring-sriov-operator">Configuring the SR-IOV Network Operator</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
<listitem>
<simpara>If you use OpenShift Virtualization: <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/virtualization/#virt-connecting-vm-to-sriov">Connecting a virtual machine to an SR-IOV network</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-sriov-net-attach">Configuring an SR-IOV network attachment</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="add-pod">Adding a pod to an SR-IOV additional network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="installing-sriov-operator">
<title>Installing the SR-IOV Network Operator</title>

<simpara>You can install the Single Root I/O Virtualization (SR-IOV) Network Operator on your cluster to manage SR-IOV network devices and network attachments.</simpara>
<section xml:id="installing-sr-iov-operator_installing-sriov-operator">
<title>Installing the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can install the Single Root I/O Virtualization (SR-IOV) Network Operator by using the OpenShift Container Platform CLI or the web console.</simpara>
<section xml:id="install-operator-cli_installing-sriov-operator">
<title>CLI: Installing the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can install the Operator using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>An account with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create the <literal>openshift-sriov-network-operator</literal> namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
  annotations:
    workload.openshift.io/allowed: management
EOF</programlisting>
</listitem>
<listitem>
<simpara>To create an OperatorGroup CR, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
EOF</programlisting>
</listitem>
<listitem>
<simpara>Subscribe to the SR-IOV Network Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to get the OpenShift Container Platform major and minor version. It is required for the <literal>channel</literal> value in the next
step.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ OC_VERSION=$(oc version -o yaml | grep openshiftVersion | \
    grep -o '[0-9]*[.][0-9]*' | head -1)</programlisting>
</listitem>
<listitem>
<simpara>To create a Subscription CR for the SR-IOV Network Operator, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subscription
  namespace: openshift-sriov-network-operator
spec:
  channel: "${OC_VERSION}"
  name: sriov-network-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To verify that the Operator is installed, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sriov-network-operator \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name                                         Phase
sriov-network-operator.4.14.0-202310121402   Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="install-operator-web-console_installing-sriov-operator">
<title>Web console: Installing the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can install the Operator using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>An account with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the SR-IOV Network Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">SR-IOV Network Operator</emphasis> from the list of available Operators, and then click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, under <emphasis role="strong">Installed Namespace</emphasis>, select <emphasis role="strong">Operator recommended Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the SR-IOV Network Operator is installed successfully:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Ensure that <emphasis role="strong">SR-IOV Network Operator</emphasis> is listed in the <emphasis role="strong">openshift-sriov-network-operator</emphasis> project with a <emphasis role="strong">Status</emphasis> of <emphasis role="strong">InstallSucceeded</emphasis>.</simpara>
<note>
<simpara>During installation an Operator might display a <emphasis role="strong">Failed</emphasis> status.
If the installation later succeeds with an <emphasis role="strong">InstallSucceeded</emphasis> message, you can ignore the <emphasis role="strong">Failed</emphasis> message.</simpara>
</note>
<simpara>If the Operator does not appear as installed, to troubleshoot further:</simpara>
<itemizedlist>
<listitem>
<simpara>Inspect the <emphasis role="strong">Operator Subscriptions</emphasis> and <emphasis role="strong">Install Plans</emphasis> tabs for any failure or errors under <emphasis role="strong">Status</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and check the logs for pods in the <literal>openshift-sriov-network-operator</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Check the namespace of the YAML file. If the annotation is missing, you can add the annotation <literal>workload.openshift.io/allowed=management</literal> to the Operator namespace with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate ns/openshift-sriov-network-operator workload.openshift.io/allowed=management</programlisting>
<note>
<simpara>For single-node OpenShift clusters, the annotation <literal>workload.openshift.io/allowed=management</literal> is required for the namespace.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="installing-sriov-operator-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara>Optional: <link linkend="configuring-sriov-operator">Configuring the SR-IOV Network Operator</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-sriov-operator">
<title>Configuring the SR-IOV Network Operator</title>

<simpara>The Single Root I/O Virtualization (SR-IOV) Network Operator manages the SR-IOV network devices and network attachments in your cluster.</simpara>
<section xml:id="nw-sriov-configuring-operator_configuring-sriov-operator">
<title>Configuring the SR-IOV Network Operator</title>
<important>
<simpara>Modifying the SR-IOV Network Operator configuration is not normally necessary.
The default configuration is recommended for most use cases.
Complete the steps to modify the relevant configuration only if the default behavior of the Operator is not compatible with your use case.</simpara>
</important>
<simpara>The SR-IOV Network Operator adds the <literal>SriovOperatorConfig.sriovnetwork.openshift.io</literal> CustomResourceDefinition resource.
The Operator automatically creates a SriovOperatorConfig custom resource (CR) named <literal>default</literal> in the <literal>openshift-sriov-network-operator</literal> namespace.</simpara>
<note>
<simpara>The <literal>default</literal> CR contains the SR-IOV Network Operator configuration for your cluster.
To change the Operator configuration, you must modify this CR.</simpara>
</note>
<section xml:id="nw-sriov-operator-cr_configuring-sriov-operator">
<title>SR-IOV Network Operator config custom resource</title>
<simpara>The fields for the <literal>sriovoperatorconfig</literal> custom resource are described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>SR-IOV Network Operator config custom resource</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies the name of the SR-IOV Network Operator instance.
The default value is <literal>default</literal>.
Do not set a different value.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies the namespace of the SR-IOV Network Operator instance.
The default value is <literal>openshift-sriov-network-operator</literal>.
Do not set a different value.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.configDaemonNodeSelector</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies the node selection to control scheduling the SR-IOV Network Config Daemon on selected nodes.
By default, this field is not set and the Operator deploys the SR-IOV Network Config daemon set on worker nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.disableDrain</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies whether to disable the node draining process or enable the node draining process when you apply a new policy to configure the NIC on a node.
Setting this field to <literal>true</literal> facilitates software development and installing OpenShift Container Platform on a single node. By default, this field is not set.</simpara>
<simpara>For single-node clusters, set this field to <literal>true</literal> after installing the Operator. This field must remain set to <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.enableInjector</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies whether to enable or disable the Network Resources Injector daemon set.
By default, this field is set to <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.enableOperatorWebhook</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies whether to enable or disable the Operator Admission Controller webhook daemon set.
By default, this field is set to <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.logLevel</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies the log verbosity level of the Operator.
Set to <literal>0</literal> to show only the basic logs. Set to <literal>2</literal> to show all the available logs.
By default, this field is set to <literal>2</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="about-network-resource-injector_configuring-sriov-operator">
<title>About the Network Resources Injector</title>
<simpara>The Network Resources Injector is a Kubernetes Dynamic Admission Controller
application. It provides the following capabilities:</simpara>
<itemizedlist>
<listitem>
<simpara>Mutation of resource requests and limits in a pod specification to add an SR-IOV resource name according to an SR-IOV network attachment definition annotation.</simpara>
</listitem>
<listitem>
<simpara>Mutation of a pod specification with a Downward API volume to expose pod annotations, labels, and huge pages requests and limits. Containers that run in the pod can access the exposed information as files under the <literal>/etc/podnetinfo</literal> path.</simpara>
</listitem>
</itemizedlist>
<simpara>By default, the Network Resources Injector is enabled by the SR-IOV Network Operator and runs as a daemon set on all control plane nodes. The following is an example of Network Resources Injector pods running in a cluster with three control plane nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-sriov-network-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                      READY   STATUS    RESTARTS   AGE
network-resources-injector-5cz5p          1/1     Running   0          10m
network-resources-injector-dwqpx          1/1     Running   0          10m
network-resources-injector-lktz5          1/1     Running   0          10m</programlisting>
</para>
</formalpara>
</section>
<section xml:id="about-sr-iov-operator-admission-control-webhook_configuring-sriov-operator">
<title>About the SR-IOV Network Operator admission controller webhook</title>
<simpara>The SR-IOV Network Operator Admission Controller webhook is a Kubernetes Dynamic
Admission Controller application. It provides the following capabilities:</simpara>
<itemizedlist>
<listitem>
<simpara>Validation of the <literal>SriovNetworkNodePolicy</literal> CR when it is created or updated.</simpara>
</listitem>
<listitem>
<simpara>Mutation of the <literal>SriovNetworkNodePolicy</literal> CR by setting the default value for the <literal>priority</literal> and <literal>deviceType</literal> fields when the CR is created or updated.</simpara>
</listitem>
</itemizedlist>
<simpara>By default the SR-IOV Network Operator Admission Controller webhook is enabled by the Operator and runs as a daemon set on all control plane nodes.</simpara>
<note>
<simpara>Use caution when disabling the SR-IOV Network Operator Admission Controller webhook. You can disable the webhook under specific circumstances, such as troubleshooting, or if you want to use unsupported devices. For information about configuring unsupported devices, see <link xlink:href="https://access.redhat.com/articles/7010183">Configuring the SR-IOV Network Operator to use an unsupported NIC</link>.</simpara>
</note>
<simpara>The following is an example of the Operator Admission Controller webhook pods running in a cluster with three control plane nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-sriov-network-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                      READY   STATUS    RESTARTS   AGE
operator-webhook-9jkw6                    1/1     Running   0          16m
operator-webhook-kbr5p                    1/1     Running   0          16m
operator-webhook-rpfrl                    1/1     Running   0          16m</programlisting>
</para>
</formalpara>
</section>
<section xml:id="about-custom-node-selectors_configuring-sriov-operator">
<title>About custom node selectors</title>
<simpara>The SR-IOV Network Config daemon discovers and configures the SR-IOV network devices on cluster nodes.
By default, it is deployed to all the <literal>worker</literal> nodes in the cluster.
You can use node labels to specify on which nodes the SR-IOV Network Config daemon runs.</simpara>
</section>
<section xml:id="disable-enable-network-resource-injector_configuring-sriov-operator">
<title>Disabling or enabling the Network Resources Injector</title>
<simpara>To disable or enable the Network Resources Injector, which is enabled by default, complete the following procedure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You must have installed the SR-IOV Network Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the <literal>enableInjector</literal> field. Replace <literal>&lt;value&gt;</literal> with <literal>false</literal> to disable the feature or <literal>true</literal> to enable the feature.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch sriovoperatorconfig default \
  --type=merge -n openshift-sriov-network-operator \
  --patch '{ "spec": { "enableInjector": &lt;value&gt; } }'</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to update the Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  enableInjector: &lt;value&gt;</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="disable-enable-sr-iov-operator-admission-control-webhook_configuring-sriov-operator">
<title>Disabling or enabling the SR-IOV Network Operator admission controller webhook</title>
<simpara>To disable or enable the admission controller webhook, which is enabled by default, complete the following procedure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You must have installed the SR-IOV Network Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the <literal>enableOperatorWebhook</literal> field. Replace <literal>&lt;value&gt;</literal> with <literal>false</literal> to disable the feature or <literal>true</literal> to enable it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch sriovoperatorconfig default --type=merge \
  -n openshift-sriov-network-operator \
  --patch '{ "spec": { "enableOperatorWebhook": &lt;value&gt; } }'</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to update the Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  enableOperatorWebhook: &lt;value&gt;</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-custom-nodeselector_configuring-sriov-operator">
<title>Configuring a custom NodeSelector for the SR-IOV Network Config daemon</title>
<simpara>The SR-IOV Network Config daemon discovers and configures the SR-IOV network devices on cluster nodes. By default, it is deployed to all the <literal>worker</literal> nodes in the cluster. You can use node labels to specify on which nodes the SR-IOV Network Config daemon runs.</simpara>
<simpara>To specify the nodes where the SR-IOV Network Config daemon is deployed, complete the following procedure.</simpara>
<important>
<simpara>When you update the <literal>configDaemonNodeSelector</literal> field, the SR-IOV Network Config daemon is recreated on each selected node.
While the daemon is recreated, cluster users are unable to apply any new SR-IOV Network node policy or create new SR-IOV pods.</simpara>
</important>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To update the node selector for the operator, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch sriovoperatorconfig default --type=json \
  -n openshift-sriov-network-operator \
  --patch '[{
      "op": "replace",
      "path": "/spec/configDaemonNodeSelector",
      "value": {&lt;node_label&gt;}
    }]'</programlisting>
<simpara>Replace <literal>&lt;node_label&gt;</literal> with a label to apply as in the following example:
<literal>"node-role.kubernetes.io/worker": ""</literal>.</simpara>
<tip>
<simpara>You can alternatively apply the following YAML to update the Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  configDaemonNodeSelector:
    &lt;node_label&gt;</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="configure-sr-iov-operator-single-node_configuring-sriov-operator">
<title>Configuring the SR-IOV Network Operator for single node installations</title>
<simpara>By default, the SR-IOV Network Operator drains workloads from a node before every policy change.
The Operator performs this action to ensure that there no workloads using the virtual functions before the reconfiguration.</simpara>
<simpara>For installations on a single node, there are no other nodes to receive the workloads.
As a result, the Operator must be configured not to drain the workloads from the single node.</simpara>
<important>
<simpara>After performing the following procedure to disable draining workloads, you must remove any workload that uses an SR-IOV network interface before you change any SR-IOV network node policy.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You must have installed the SR-IOV Network Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To set the <literal>disableDrain</literal> field to <literal>true</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch sriovoperatorconfig default --type=merge \
  -n openshift-sriov-network-operator \
  --patch '{ "spec": { "disableDrain": true } }'</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to update the Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovOperatorConfig
metadata:
  name: default
  namespace: openshift-sriov-network-operator
spec:
  disableDrain: true</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="sriov-operator-hosted-control-planes_configuring-sriov-operator">
<title>Deploying the SR-IOV Operator for hosted control planes</title>
<important>
<simpara>Hosted control planes on the AWS platform is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara role="_abstract">After you configure and deploy your hosting service cluster, you can create a subscription to the SR-IOV Operator on a hosted cluster. The SR-IOV pod runs on worker machines rather than the control plane.</simpara>
<formalpara>
<title>Prerequisites</title>
<para>You must configure and deploy the hosted cluster on AWS. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.9/html/clusters/cluster_mce_overview#hosting-service-cluster-configure-aws">Configuring the hosting cluster on AWS (Technology Preview)</link>.</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a namespace and an Operator group:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator</programlisting>
</listitem>
<listitem>
<simpara>Create a subscription to the SR-IOV Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subsription
  namespace: openshift-sriov-network-operator
spec:
  channel: "4.14"
  name: sriov-network-operator
  config:
    nodeSelector:
      node-role.kubernetes.io/worker: ""
  source: s/qe-app-registry/redhat-operators
  sourceNamespace: openshift-marketplace</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>To verify that the SR-IOV Operator is ready, run the following command and view the resulting output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sriov-network-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         DISPLAY                   VERSION               REPLACES                                     PHASE
sriov-network-operator.4.14.0-202211021237   SR-IOV Network Operator   4.14.0-202211021237   sriov-network-operator.4.14.0-202210290517   Succeeded</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To verify that the SR-IOV pods are deployed, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-sriov-network-operator</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-sriov-operator-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-sriov-device">
<title>Configuring an SR-IOV network device</title>

<simpara>You can configure a Single Root I/O Virtualization (SR-IOV) device in your cluster.</simpara>
<section xml:id="nw-sriov-networknodepolicy-object_configuring-sriov-device">
<title>SR-IOV network node configuration object</title>
<simpara>You specify the SR-IOV network device configuration for a node by creating an SR-IOV network node policy. The API object for the policy is part of the <literal>sriovnetwork.openshift.io</literal> API group.</simpara>
<simpara>The following YAML describes an SR-IOV network node policy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: &lt;name&gt; <co xml:id="CO108-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO108-2"/>
spec:
  resourceName: &lt;sriov_resource_name&gt; <co xml:id="CO108-3"/>
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true" <co xml:id="CO108-4"/>
  priority: &lt;priority&gt; <co xml:id="CO108-5"/>
  mtu: &lt;mtu&gt; <co xml:id="CO108-6"/>
  needVhostNet: false <co xml:id="CO108-7"/>
  numVfs: &lt;num&gt; <co xml:id="CO108-8"/>
  nicSelector: <co xml:id="CO108-9"/>
    vendor: "&lt;vendor_code&gt;" <co xml:id="CO108-10"/>
    deviceID: "&lt;device_id&gt;" <co xml:id="CO108-11"/>
    pfNames: ["&lt;pf_name&gt;", ...] <co xml:id="CO108-12"/>
    rootDevices: ["&lt;pci_bus_id&gt;", ...] <co xml:id="CO108-13"/>
    netFilter: "&lt;filter_string&gt;" <co xml:id="CO108-14"/>
  deviceType: &lt;device_type&gt; <co xml:id="CO108-15"/>
  isRdma: false <co xml:id="CO108-16"/>
  linkType: &lt;link_type&gt; <co xml:id="CO108-17"/>
  eSwitchMode: &lt;mode&gt; <co xml:id="CO108-18"/>
  excludeTopology: false <co xml:id="CO108-19"/></programlisting>
<calloutlist>
<callout arearefs="CO108-1">
<para>The name for the custom resource object.</para>
</callout>
<callout arearefs="CO108-2">
<para>The namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO108-3">
<para>The resource name of the SR-IOV network device plugin. You can create multiple SR-IOV network node policies for a resource name.</para>
<simpara>When specifying a name, be sure to use the accepted syntax expression <literal>^[a-zA-Z0-9_]+$</literal> in the <literal>resourceName</literal>.</simpara>
</callout>
<callout arearefs="CO108-4">
<para>The node selector specifies the nodes to configure. Only SR-IOV network devices on the selected nodes are configured. The SR-IOV Container Network Interface (CNI) plugin and device plugin are deployed on selected nodes only.</para>
<important>
<simpara>The SR-IOV Network Operator applies node network configuration policies to nodes in sequence. Before applying node network configuration policies, the SR-IOV Network Operator checks if the machine config pool (MCP) for a node is in an unhealthy state such as <literal>Degraded</literal> or <literal>Updating</literal>. If a node is in an unhealthy MCP, the process of applying node network configuration policies to all targeted nodes in the cluster pauses until the MCP returns to a healthy state.</simpara>
<simpara>To avoid a node in an unhealthy MCP from blocking the application of node network configuration policies to other nodes, including nodes in other MCPs, you must create a separate node network configuration policy for each MCP.</simpara>
</important>
</callout>
<callout arearefs="CO108-5">
<para>Optional: The priority is an integer value between <literal>0</literal> and <literal>99</literal>. A smaller value receives higher priority. For example, a priority of <literal>10</literal> is a higher priority than <literal>99</literal>. The default value is <literal>99</literal>.</para>
</callout>
<callout arearefs="CO108-6">
<para>Optional: The maximum transmission unit (MTU) of the virtual function. The maximum MTU value can vary for different network interface controller (NIC) models.</para>
<important>
<simpara>If you want to create virtual function on the default network interface, ensure that the MTU is set to a value that matches the cluster MTU.</simpara>
</important>
</callout>
<callout arearefs="CO108-7">
<para>Optional: Set <literal>needVhostNet</literal> to <literal>true</literal> to mount the <literal>/dev/vhost-net</literal> device in the pod. Use the mounted <literal>/dev/vhost-net</literal> device with Data Plane Development Kit (DPDK) to forward traffic to the kernel network stack.</para>
</callout>
<callout arearefs="CO108-8">
<para>The number of the virtual functions (VF) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <literal>128</literal>.</para>
</callout>
<callout arearefs="CO108-9">
<para>The NIC selector identifies the device for the Operator to configure. You do not have to specify values for all the parameters. It is recommended to identify the network device with enough precision to avoid selecting a device unintentionally.</para>
<simpara>If you specify <literal>rootDevices</literal>, you must also specify a value for <literal>vendor</literal>, <literal>deviceID</literal>, or <literal>pfNames</literal>. If you specify both <literal>pfNames</literal> and <literal>rootDevices</literal> at the same time, ensure that they refer to the same device. If you specify a value for <literal>netFilter</literal>, then you do not need to specify any other parameter because a network ID is unique.</simpara>
</callout>
<callout arearefs="CO108-10">
<para>Optional: The vendor hexadecimal code of the SR-IOV network device. The only allowed values are <literal>8086</literal> and <literal>15b3</literal>.</para>
</callout>
<callout arearefs="CO108-11">
<para>Optional: The device hexadecimal code of the SR-IOV network device. For example, <literal>101b</literal> is the device ID for a Mellanox ConnectX-6 device.</para>
</callout>
<callout arearefs="CO108-12">
<para>Optional: An array of one or more physical function (PF) names for the device.</para>
</callout>
<callout arearefs="CO108-13">
<para>Optional: An array of one or more PCI bus addresses for the PF of the device. Provide the address in the following format: <literal>0000:02:00.1</literal>.</para>
</callout>
<callout arearefs="CO108-14">
<para>Optional: The platform-specific network filter. The only supported platform is Red Hat OpenStack Platform (RHOSP). Acceptable values use the following format: <literal>openstack/NetworkID:xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</literal>. Replace <literal>xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx</literal> with the value from the <literal>/var/config/openstack/latest/network_data.json</literal> metadata file.</para>
</callout>
<callout arearefs="CO108-15">
<para>Optional: The driver type for the virtual functions. The only allowed values are <literal>netdevice</literal> and <literal>vfio-pci</literal>. The default value is <literal>netdevice</literal>.</para>
<simpara>For a Mellanox NIC to work in DPDK mode on bare metal nodes, use the <literal>netdevice</literal> driver type and set <literal>isRdma</literal> to <literal>true</literal>.</simpara>
</callout>
<callout arearefs="CO108-16">
<para>Optional: Configures whether to enable remote direct memory access (RDMA) mode. The default value is <literal>false</literal>.</para>
<simpara>If the <literal>isRdma</literal> parameter is set to <literal>true</literal>, you can continue to use the RDMA-enabled VF as a normal network device. A device can be used in either mode.</simpara>
<simpara>Set <literal>isRdma</literal> to <literal>true</literal> and additionally set <literal>needVhostNet</literal> to <literal>true</literal> to configure a Mellanox NIC for use with Fast Datapath DPDK applications.</simpara>
</callout>
<callout arearefs="CO108-17">
<para>Optional: The link type for the VFs. The default value is <literal>eth</literal> for Ethernet. Change this value to 'ib' for InfiniBand.</para>
<simpara>When <literal>linkType</literal> is set to <literal>ib</literal>, <literal>isRdma</literal> is automatically set to <literal>true</literal> by the SR-IOV Network Operator webhook. When <literal>linkType</literal> is set to <literal>ib</literal>, <literal>deviceType</literal> should not be set to <literal>vfio-pci</literal>.</simpara>
<simpara>Do not set linkType to 'eth' for SriovNetworkNodePolicy, because this can lead to an incorrect number of available devices reported by the device plugin.</simpara>
</callout>
<callout arearefs="CO108-18">
<para>Optional: The NIC device mode. The only allowed values are <literal>legacy</literal> or <literal>switchdev</literal>.</para>
<simpara>When <literal>eSwitchMode</literal> is set to <literal>legacy</literal>, the default SR-IOV behavior is enabled.</simpara>
<simpara>When <literal>eSwitchMode</literal> is set to <literal>switchdev</literal>, hardware offloading is enabled.</simpara>
</callout>
<callout arearefs="CO108-19">
<para>Optional: To exclude advertising an SR-IOV network resource&#8217;s NUMA node to the Topology Manager, set the value to <literal>true</literal>. The default value is <literal>false</literal>.</para>
</callout>
</calloutlist>
<section xml:id="sr-iov-network-node-configuration-examples_configuring-sriov-device">
<title>SR-IOV network node configuration examples</title>
<simpara>The following example describes the configuration for an InfiniBand device:</simpara>
<formalpara>
<title>Example configuration for an InfiniBand device</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-ib-net-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: ibnic1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 4
  nicSelector:
    vendor: "15b3"
    deviceID: "101b"
    rootDevices:
      - "0000:19:00.0"
  linkType: ib
  isRdma: true</programlisting>
</para>
</formalpara>
<simpara>The following example describes the configuration for an SR-IOV network device in a RHOSP virtual machine:</simpara>
<formalpara>
<title>Example configuration for an SR-IOV device in a virtual machine</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-sriov-net-openstack-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnic1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 1 <co xml:id="CO109-1"/>
  nicSelector:
    vendor: "15b3"
    deviceID: "101b"
    netFilter: "openstack/NetworkID:ea24bd04-8674-4f69-b0ee-fa0b3bd20509" <co xml:id="CO109-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO109-1">
<para>The <literal>numVfs</literal> field is always set to <literal>1</literal> when configuring the node network policy for a virtual machine.</para>
</callout>
<callout arearefs="CO109-2">
<para>The <literal>netFilter</literal> field must refer to a network ID when the virtual machine is deployed on RHOSP. Valid values for <literal>netFilter</literal> are available from an <literal>SriovNetworkNodeState</literal> object.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-sriov-nic-partitioning_configuring-sriov-device">
<title>Virtual function (VF) partitioning for SR-IOV devices</title>
<simpara>In some cases, you might want to split virtual functions (VFs) from the same physical function (PF) into multiple resource pools.
For example, you might want some of the VFs to load with the default driver and the remaining VFs load with the <literal>vfio-pci</literal> driver.
In such a deployment, the <literal>pfNames</literal> selector in your SriovNetworkNodePolicy custom resource (CR) can be used to specify a range of VFs for a pool using the following format: <literal>&lt;pfname&gt;#&lt;first_vf&gt;-&lt;last_vf&gt;</literal>.</simpara>
<simpara>For example, the following YAML shows the selector for an interface named <literal>netpf0</literal> with VF <literal>2</literal> through <literal>7</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">pfNames: ["netpf0#2-7"]</programlisting>
<itemizedlist>
<listitem>
<simpara><literal>netpf0</literal> is the PF interface name.</simpara>
</listitem>
<listitem>
<simpara><literal>2</literal> is the first VF index (0-based) that is included in the range.</simpara>
</listitem>
<listitem>
<simpara><literal>7</literal> is the last VF index (0-based) that is included in the range.</simpara>
</listitem>
</itemizedlist>
<simpara>You can select VFs from the same PF by using different policy CRs if the following requirements are met:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>numVfs</literal> value must be identical for policies that select the same PF.</simpara>
</listitem>
<listitem>
<simpara>The VF index must be in the range of <literal>0</literal> to <literal>&lt;numVfs&gt;-1</literal>. For example, if you have a policy with <literal>numVfs</literal> set to <literal>8</literal>, then the <literal>&lt;first_vf&gt;</literal> value must not be smaller than <literal>0</literal>, and the <literal>&lt;last_vf&gt;</literal> must not be larger than <literal>7</literal>.</simpara>
</listitem>
<listitem>
<simpara>The VFs ranges in different policies must not overlap.</simpara>
</listitem>
<listitem>
<simpara>The <literal>&lt;first_vf&gt;</literal> must not be larger than the <literal>&lt;last_vf&gt;</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>The following example illustrates NIC partitioning for an SR-IOV device.</simpara>
<simpara>The policy <literal>policy-net-1</literal> defines a resource pool <literal>net-1</literal> that contains the VF <literal>0</literal> of PF <literal>netpf0</literal> with the default VF driver.
The policy <literal>policy-net-1-dpdk</literal> defines a resource pool <literal>net-1-dpdk</literal> that contains the VF <literal>8</literal> to <literal>15</literal> of PF <literal>netpf0</literal> with the <literal>vfio</literal> VF driver.</simpara>
<simpara>Policy <literal>policy-net-1</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-net-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 16
  nicSelector:
    pfNames: ["netpf0#0-0"]
  deviceType: netdevice</programlisting>
<simpara>Policy <literal>policy-net-1-dpdk</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-net-1-dpdk
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1dpdk
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 16
  nicSelector:
    pfNames: ["netpf0#8-15"]
  deviceType: vfio-pci</programlisting>
<formalpara>
<title>Verifying that the interface is successfully partitioned</title>
<para>Confirm that the interface partitioned to virtual functions (VFs) for the SR-IOV device by running the following command.</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip link show &lt;interface&gt; <co xml:id="CO110-1"/></programlisting>
<calloutlist>
<callout arearefs="CO110-1">
<para>Replace <literal>&lt;interface&gt;</literal> with the interface that you specified when partitioning to VFs for the SR-IOV device, for example, <literal>ens3f1</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">5: ens3f1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP mode DEFAULT group default qlen 1000
link/ether 3c:fd:fe:d1:bc:01 brd ff:ff:ff:ff:ff:ff

vf 0     link/ether 5a:e7:88:25:ea:a0 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 1     link/ether 3e:1d:36:d7:3d:49 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 2     link/ether ce:09:56:97:df:f9 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 3     link/ether 5e:91:cf:88:d1:38 brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off
vf 4     link/ether e6:06:a1:96:2f:de brd ff:ff:ff:ff:ff:ff, spoof checking on, link-state auto, trust off</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="nw-sriov-configuring-device_configuring-sriov-device">
<title>Configuring SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator adds the <literal>SriovNetworkNodePolicy.sriovnetwork.openshift.io</literal> CustomResourceDefinition to OpenShift Container Platform.
You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).</simpara>
<note>
<simpara>When applying the configuration specified in a <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.</simpara>
<simpara>It might take several minutes for a configuration change to apply.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have enough available nodes in your cluster to handle the evicted workload from drained nodes.</simpara>
</listitem>
<listitem>
<simpara>You have not selected any control plane nodes for SR-IOV network device configuration.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> object, and then save the YAML in the <literal>&lt;name&gt;-sriov-node-network.yaml</literal> file. Replace <literal>&lt;name&gt;</literal> with the name for this configuration.</simpara>
</listitem>
<listitem>
<simpara>Optional: Label the SR-IOV capable cluster nodes with <literal>SriovNetworkNodePolicy.Spec.NodeSelector</literal> if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;-sriov-node-network.yaml</programlisting>
<simpara>where <literal>&lt;name&gt;</literal> specifies the name for this configuration.</simpara>
<simpara>After applying the configuration update, all the pods in <literal>sriov-network-operator</literal> namespace transition to the <literal>Running</literal> status.</simpara>
</listitem>
<listitem>
<simpara>To verify that the SR-IOV network device is configured, enter the following command. Replace <literal>&lt;node_name&gt;</literal> with the name of a node with the SR-IOV network device that you just configured.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-sriov-troubleshooting_configuring-sriov-device">
<title>Troubleshooting SR-IOV configuration</title>
<simpara>After following the procedure to configure an SR-IOV network device, the following sections address some error conditions.</simpara>
<simpara>To display the state of nodes, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt;</programlisting>
<simpara>where: <literal>&lt;node_name&gt;</literal> specifies the name of a node with an SR-IOV network device.</simpara>
<formalpara>
<title>Error output: Cannot allocate memory</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"lastSyncError": "write /sys/bus/pci/devices/0000:3b:00.1/sriov_numvfs: cannot allocate memory"</programlisting>
</para>
</formalpara>
<simpara>When a node indicates that it cannot allocate memory, check the following items:</simpara>
<itemizedlist>
<listitem>
<simpara>Confirm that global SR-IOV settings are enabled in the BIOS for the node.</simpara>
</listitem>
<listitem>
<simpara>Confirm that VT-d is enabled in the BIOS for the node.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cnf-assigning-a-sriov-network-to-a-vrf_configuring-sriov-device">
<title>Assigning an SR-IOV network to a VRF</title>
<simpara>As a cluster administrator, you can assign an SR-IOV network interface to your VRF domain by using the CNI VRF plugin.</simpara>
<simpara>To do this, add the VRF configuration to the optional <literal>metaPlugins</literal> parameter of the <literal>SriovNetwork</literal> resource.</simpara>
<note>
<simpara>Applications that use VRFs need to bind to a specific device. The common usage is to use the <literal>SO_BINDTODEVICE</literal> option for a socket. <literal>SO_BINDTODEVICE</literal> binds the socket to a device that is specified in the passed interface name, for example, <literal>eth1</literal>. To use <literal>SO_BINDTODEVICE</literal>, the application must have <literal>CAP_NET_RAW</literal> capabilities.</simpara>
<simpara>Using a VRF through the <literal>ip vrf exec</literal> command is not supported in OpenShift Container Platform pods. To use VRF, bind applications directly to the VRF interface.</simpara>
</note>
<section xml:id="cnf-creating-an-additional-sriov-network-with-vrf-plug-in_configuring-sriov-device">
<title>Creating an additional SR-IOV network attachment with the CNI VRF plugin</title>
<simpara>The SR-IOV Network Operator manages additional network definitions. When you specify an additional SR-IOV network to create, the SR-IOV Network Operator creates the <literal>NetworkAttachmentDefinition</literal> custom resource (CR) automatically.</simpara>
<note>
<simpara>Do not edit <literal>NetworkAttachmentDefinition</literal> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.</simpara>
</note>
<simpara>To create an additional SR-IOV network attachment with the CNI VRF plugin, perform the following procedure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI (oc).</simpara>
</listitem>
<listitem>
<simpara>Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> custom resource (CR) for the additional SR-IOV network attachment and insert the <literal>metaPlugins</literal> configuration, as in the following example CR. Save the YAML as the file <literal>sriov-network-attachment.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: example-network
  namespace: additional-sriov-network-1
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "10.56.217.1"
    }
  vlan: 0
  resourceName: intelnics
  metaPlugins : |
    {
      "type": "vrf", <co xml:id="CO111-1"/>
      "vrfname": "example-vrf-name" <co xml:id="CO111-2"/>
    }</programlisting>
<calloutlist>
<callout arearefs="CO111-1">
<para><literal>type</literal> must be set to <literal>vrf</literal>.</para>
</callout>
<callout arearefs="CO111-2">
<para><literal>vrfname</literal> is the name of the VRF that the interface is assigned to. If it does not exist in the pod, it is created.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network-attachment.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verifying that the <literal>NetworkAttachmentDefinition</literal> CR is successfully created</title>
<listitem>
<simpara>Confirm that the SR-IOV Network Operator created the <literal>NetworkAttachmentDefinition</literal> CR by running the following command.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -n &lt;namespace&gt; <co xml:id="CO112-1"/></programlisting>
<calloutlist>
<callout arearefs="CO112-1">
<para>Replace <literal>&lt;namespace&gt;</literal> with the namespace that you specified when configuring the network attachment, for example, <literal>additional-sriov-network-1</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            AGE
additional-sriov-network-1      14m</programlisting>
</para>
</formalpara>
<note>
<simpara>There might be a delay before the SR-IOV Network Operator creates the CR.</simpara>
</note>
</listitem>
</itemizedlist>
<formalpara>
<title>Verifying that the additional SR-IOV network attachment is successful</title>
<para>To verify that the VRF CNI is correctly configured and the additional SR-IOV network attachment is attached, do the following:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create an SR-IOV network that uses the VRF CNI.</simpara>
</listitem>
<listitem>
<simpara>Assign the network to a pod.</simpara>
</listitem>
<listitem>
<simpara>Verify that the pod network attachment is connected to the SR-IOV additional network. Remote shell into the pod and run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip vrf show</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name              Table
-----------------------
red                 10</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Confirm the VRF interface is master of the secondary interface:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip link</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
5: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue master red state UP mode
...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-sriov-exclude-topology-manager_configuring-sriov-device">
<title>Exclude the SR-IOV network topology for NUMA-aware scheduling</title>
<simpara>You can exclude advertising the Non-Uniform Memory Access (NUMA) node for the SR-IOV network to the Topology Manager for more flexible SR-IOV network deployments during NUMA-aware pod scheduling.</simpara>
<simpara>In some scenarios, it is a priority to maximize CPU and memory resources for a pod on a single NUMA node. By not providing a hint to the Topology Manager about the NUMA node for the pod&#8217;s SR-IOV network resource, the Topology Manager can deploy the SR-IOV network resource and the pod CPU and memory resources to different NUMA nodes. This can add to network latency because of the data transfer between NUMA nodes. However, it is acceptable in scenarios when workloads require optimal CPU and memory performance.</simpara>
<simpara>For example, consider a compute node, <literal>compute-1</literal>, that features two NUMA nodes: <literal>numa0</literal> and <literal>numa1</literal>. The SR-IOV-enabled NIC is present on <literal>numa0</literal>. The CPUs available for pod scheduling are present on <literal>numa1</literal> only. By setting the <literal>excludeTopology</literal> specification to <literal>true</literal>, the Topology Manager can assign CPU and memory resources for the pod to <literal>numa1</literal> and can assign the SR-IOV network resource for the same pod to <literal>numa0</literal>. This is only possible when you set the <literal>excludeTopology</literal> specification to <literal>true</literal>. Otherwise, the Topology Manager attempts to place all resources on the same NUMA node.</simpara>
<section xml:id="nw-sriov-configure-exclude-topology-manager_configuring-sriov-device">
<title>Excluding the SR-IOV network topology for NUMA-aware scheduling</title>
<simpara>To exclude advertising the SR-IOV network resource&#8217;s Non-Uniform Memory Access (NUMA) node to the Topology Manager, you can configure the <literal>excludeTopology</literal> specification in the <literal>SriovNetworkNodePolicy</literal> custom resource. Use this configuration for more flexible SR-IOV network deployments during NUMA-aware pod scheduling.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have configured the CPU Manager policy to <literal>static</literal>. For more information about CPU Manager, see the <emphasis>Additional resources</emphasis> section.</simpara>
</listitem>
<listitem>
<simpara>You have configured the Topology Manager policy to <literal>single-numa-node</literal>.</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>sriov-network-node-policy.yaml</literal> file, replacing values in the YAML to match your environment:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: &lt;policy_name&gt;
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnuma0 <co xml:id="CO113-1"/>
  nodeSelector:
    kubernetes.io/hostname: &lt;node_name&gt;
  numVfs: &lt;number_of_Vfs&gt;
  nicSelector: <co xml:id="CO113-2"/>
    vendor: "&lt;vendor_ID&gt;"
    deviceID: "&lt;device_ID&gt;"
  deviceType: netdevice
  excludeTopology: true <co xml:id="CO113-3"/></programlisting>
<calloutlist>
<callout arearefs="CO113-1">
<para>The resource name of the SR-IOV network device plugin. This YAML uses a sample <literal>resourceName</literal> value.</para>
</callout>
<callout arearefs="CO113-2">
<para>Identify the device for the Operator to configure by using the NIC selector.</para>
</callout>
<callout arearefs="CO113-3">
<para>To exclude advertising the NUMA node for the SR-IOV network resource to the Topology Manager, set the value to <literal>true</literal>. The default value is <literal>false</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>If multiple <literal>SriovNetworkNodePolicy</literal> resources target the same SR-IOV network resource, the <literal>SriovNetworkNodePolicy</literal> resources must have the same value as the <literal>excludeTopology</literal> specification. Otherwise, the conflicting policy is rejected.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network-node-policy.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">sriovnetworknodepolicy.sriovnetwork.openshift.io/policy-for-numa-0 created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>sriov-network.yaml</literal> file, replacing values in the YAML to match your environment:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: sriov-numa-0-network <co xml:id="CO114-1"/>
  namespace: openshift-sriov-network-operator
spec:
  resourceName: sriovnuma0 <co xml:id="CO114-2"/>
  networkNamespace: &lt;namespace&gt; <co xml:id="CO114-3"/>
  ipam: |- <co xml:id="CO114-4"/>
    {
      "type": "&lt;ipam_type&gt;",
    }</programlisting>
<calloutlist>
<callout arearefs="CO114-1">
<para>Replace <literal>sriov-numa-0-network</literal> with the name for the SR-IOV network resource.</para>
</callout>
<callout arearefs="CO114-2">
<para>Specify the resource name for the <literal>SriovNetworkNodePolicy</literal> CR from the previous step. This YAML uses a sample <literal>resourceName</literal> value.</para>
</callout>
<callout arearefs="CO114-3">
<para>Enter the namespace for your SR-IOV network resource.</para>
</callout>
<callout arearefs="CO114-4">
<para>Enter the IP address management configuration for the SR-IOV network.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">sriovnetwork.sriovnetwork.openshift.io/sriov-numa-0-network created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a pod and assign the SR-IOV network resource from the previous step:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>sriov-network-pod.yaml</literal> file, replacing values in the YAML to match your environment:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: &lt;pod_name&gt;
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "sriov-numa-0-network", <co xml:id="CO115-1"/>
        }
      ]
spec:
  containers:
  - name: &lt;container_name&gt;
    image: &lt;image&gt;
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]</programlisting>
<calloutlist>
<callout arearefs="CO115-1">
<para>This is the name of the <literal>SriovNetwork</literal> resource that uses the <literal>SriovNetworkNodePolicy</literal> resource.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>Pod</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network-pod.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">pod/example-pod created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify the status of the pod by running the following command, replacing <literal>&lt;pod_name&gt;</literal> with the name of the pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod &lt;pod_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                     READY   STATUS    RESTARTS   AGE
test-deployment-sriov-76cbbf4756-k9v72   1/1     Running   0          45h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Open a debug session with the target pod to verify that the SR-IOV network resources are deployed to a different node than the memory and CPU resources.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Open a debug session with the pod by running the following command, replacing &lt;pod_name&gt; with the target pod name.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug pod/&lt;pod_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell. The debug pod mounts the root file system from the host in <literal>/host</literal> within the pod. By changing the root directory to <literal>/host</literal>, you can run binaries from the host file system:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chroot /host</programlisting>
</listitem>
<listitem>
<simpara>View information about the CPU allocation by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lscpu | grep NUMA</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NUMA node(s):                    2
NUMA node0 CPU(s):     0,2,4,6,8,10,12,14,16,18,...
NUMA node1 CPU(s):     1,3,5,7,9,11,13,15,17,19,...</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat /proc/self/status | grep Cpus</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Cpus_allowed:	aa
Cpus_allowed_list:	1,3,5,7</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat  /sys/class/net/net1/device/numa_node</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">0</programlisting>
</para>
</formalpara>
<simpara>In this example, CPUs 1,3,5, and 7 are allocated to <literal>NUMA node1</literal> but the SR-IOV network resource can use the NIC in <literal>NUMA node0</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<note>
<simpara>If the <literal>excludeTopology</literal> specification is set to <literal>True</literal>, it is possible that the required resources exist in the same NUMA node.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#using-cpu-manager">Using CPU Manager</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-sriov-device-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-sriov-net-attach">Configuring an SR-IOV network attachment</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-sriov-net-attach">
<title>Configuring an SR-IOV Ethernet network attachment</title>

<simpara>You can configure an Ethernet network attachment for an Single Root I/O Virtualization (SR-IOV) device in the cluster.</simpara>
<section xml:id="nw-sriov-network-object_configuring-sriov-net-attach">
<title>Ethernet device configuration object</title>
<simpara>You can configure an Ethernet network device by defining an <literal>SriovNetwork</literal> object.</simpara>
<simpara>The following YAML describes an <literal>SriovNetwork</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: &lt;name&gt; <co xml:id="CO116-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO116-2"/>
spec:
  resourceName: &lt;sriov_resource_name&gt; <co xml:id="CO116-3"/>
  networkNamespace: &lt;target_namespace&gt; <co xml:id="CO116-4"/>
  vlan: &lt;vlan&gt; <co xml:id="CO116-5"/>
  spoofChk: "&lt;spoof_check&gt;" <co xml:id="CO116-6"/>
  ipam: |- <co xml:id="CO116-7"/>
    {}
  linkState: &lt;link_state&gt; <co xml:id="CO116-8"/>
  maxTxRate: &lt;max_tx_rate&gt; <co xml:id="CO116-9"/>
  minTxRate: &lt;min_tx_rate&gt; <co xml:id="CO116-10"/>
  vlanQoS: &lt;vlan_qos&gt; <co xml:id="CO116-11"/>
  trust: "&lt;trust_vf&gt;" <co xml:id="CO116-12"/>
  capabilities: &lt;capabilities&gt; <co xml:id="CO116-13"/></programlisting>
<calloutlist>
<callout arearefs="CO116-1">
<para>A name for the object. The SR-IOV Network Operator creates a <literal>NetworkAttachmentDefinition</literal> object with same name.</para>
</callout>
<callout arearefs="CO116-2">
<para>The namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO116-3">
<para>The value for the <literal>spec.resourceName</literal> parameter from the <literal>SriovNetworkNodePolicy</literal> object that defines the SR-IOV hardware for this additional network.</para>
</callout>
<callout arearefs="CO116-4">
<para>The target namespace for the <literal>SriovNetwork</literal> object. Only pods in the target namespace can attach to the additional network.</para>
</callout>
<callout arearefs="CO116-5">
<para>Optional: A Virtual LAN (VLAN) ID for the additional network. The integer value must be from <literal>0</literal> to <literal>4095</literal>. The default value is <literal>0</literal>.</para>
</callout>
<callout arearefs="CO116-6">
<para>Optional: The spoof check mode of the VF. The allowed values are the strings <literal>"on"</literal> and <literal>"off"</literal>.</para>
<important>
<simpara>You must enclose the value you specify in quotes or the object is rejected by the SR-IOV Network Operator.</simpara>
</important>
</callout>
<callout arearefs="CO116-7">
<para>A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
<callout arearefs="CO116-8">
<para>Optional: The link state of virtual function (VF). Allowed value are <literal>enable</literal>, <literal>disable</literal> and <literal>auto</literal>.</para>
</callout>
<callout arearefs="CO116-9">
<para>Optional: A maximum transmission rate, in Mbps, for the VF.</para>
</callout>
<callout arearefs="CO116-10">
<para>Optional: A minimum transmission rate, in Mbps, for the VF. This value must be less than or equal to the maximum transmission rate.</para>
<note>
<simpara>Intel NICs do not support the <literal>minTxRate</literal> parameter. For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1772847">BZ#1772847</link>.</simpara>
</note>
</callout>
<callout arearefs="CO116-11">
<para>Optional: An IEEE 802.1p priority level for the VF. The default value is <literal>0</literal>.</para>
</callout>
<callout arearefs="CO116-12">
<para>Optional: The trust mode of the VF. The allowed values are the strings <literal>"on"</literal> and <literal>"off"</literal>.</para>
<important>
<simpara>You must enclose the value that you specify in quotes, or the SR-IOV Network Operator rejects the object.</simpara>
</important>
</callout>
<callout arearefs="CO116-13">
<para>Optional: The capabilities to configure for this additional network. You can specify <literal>'{ "ips": true }'</literal> to enable IP address support or <literal>'{ "mac": true }'</literal> to enable MAC address support.</para>
</callout>
</calloutlist>
<section xml:id="nw-multus-ipam-object_configuring-sriov-net-attach">
<title>Configuration of IP address assignment for an additional network</title>
<simpara>The IP address management (IPAM) Container Network Interface (CNI) plugin provides IP addresses for other CNI plugins.</simpara>
<simpara>You can use the following IP address assignment types:</simpara>
<itemizedlist>
<listitem>
<simpara>Static assignment.</simpara>
</listitem>
<listitem>
<simpara>Dynamic assignment through a DHCP server. The DHCP server you specify must be reachable from the additional network.</simpara>
</listitem>
<listitem>
<simpara>Dynamic assignment through the Whereabouts IPAM CNI plugin.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-multus-static_configuring-sriov-net-attach">
<title>Static IP address assignment configuration</title>
<simpara>The following table describes the configuration for static IP address assignment:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> static configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>static</literal> is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>addresses</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of objects specifying IP addresses to assign to the virtual interface. Both IPv4 and IPv6 IP addresses are supported.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>routes</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of objects specifying routes to configure inside the pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>dns</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: An array of objects specifying the DNS configuration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The <literal>addresses</literal> array requires objects with the following fields:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.addresses[]</literal> array</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>address</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An IP address and network prefix that you specify. For example, if you specify <literal>10.10.21.10/24</literal>, then the additional network is assigned an IP address of <literal>10.10.21.10</literal> and the netmask is <literal>255.255.255.0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gateway</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The default gateway to route egress network traffic to.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.routes[]</literal> array</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>dst</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IP address range in CIDR format, such as <literal>192.168.17.0/24</literal> or <literal>0.0.0.0/0</literal> for the default route.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gw</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The gateway where network traffic is routed.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.dns</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>nameservers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of one or more IP addresses for to send DNS queries to.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>domain</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The default domain to append to a hostname. For example, if the
domain is set to <literal>example.com</literal>, a DNS lookup query for <literal>example-host</literal> is
rewritten as <literal>example-host.example.com</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>search</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of domain names to append to an unqualified hostname,
such as <literal>example-host</literal>, during a DNS lookup query.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Static IP address assignment configuration example</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "static",
      "addresses": [
        {
          "address": "191.168.1.7/24"
        }
      ]
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-dhcp_configuring-sriov-net-attach">
<title>Dynamic IP address (DHCP) assignment configuration</title>
<simpara>The following JSON describes the configuration for dynamic IP address address assignment with DHCP.</simpara>
<important>
<title>Renewal of DHCP leases</title>
<simpara>A pod obtains its original DHCP lease when it is created. The lease must be periodically renewed by a minimal DHCP server deployment running on the cluster.</simpara>
<simpara>The SR-IOV Network Operator does not create a DHCP server deployment; The Cluster Network Operator is responsible for creating the minimal DHCP server deployment.</simpara>
<simpara>To trigger the deployment of the DHCP server, you must create a shim network attachment by editing the Cluster Network Operator configuration, as in the following example:</simpara>
<formalpara>
<title>Example shim network attachment definition</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: dhcp-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
        "name": "dhcp-shim",
        "cniVersion": "0.3.1",
        "type": "bridge",
        "ipam": {
          "type": "dhcp"
        }
      }
  # ...</programlisting>
</para>
</formalpara>
</important>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> DHCP configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>dhcp</literal> is required.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Dynamic IP address (DHCP) assignment configuration example</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "dhcp"
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-whereabouts_configuring-sriov-net-attach">
<title>Dynamic IP address assignment configuration with Whereabouts</title>
<simpara>The Whereabouts CNI plugin allows the dynamic assignment of an IP address to an additional network without the use of a DHCP server.</simpara>
<simpara>The following table describes the configuration for dynamic IP address assignment with Whereabouts:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> whereabouts configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>whereabouts</literal> is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>range</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An IP address and range in CIDR notation. IP addresses are assigned from within this range of addresses.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>exclude</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: A list of zero or more IP addresses and ranges in CIDR notation. IP addresses within an excluded address range are not assigned.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Dynamic IP address assignment configuration example that uses Whereabouts</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "whereabouts",
    "range": "192.0.2.192/27",
    "exclude": [
       "192.0.2.192/30",
       "192.0.2.196/32"
    ]
  }
}</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="nw-multus-configure-dualstack-ip-address_configuring-sriov-net-attach">
<title>Creating a configuration for assignment of dual-stack IP addresses dynamically</title>
<simpara>Dual-stack IP address assignment can be configured with the <literal>ipRanges</literal> parameter for:</simpara>
<itemizedlist>
<listitem>
<simpara>IPv4 addresses</simpara>
</listitem>
<listitem>
<simpara>IPv6 addresses</simpara>
</listitem>
<listitem>
<simpara>multiple IP address assignment</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set <literal>type</literal> to <literal>whereabouts</literal>.</simpara>
</listitem>
<listitem>
<simpara>Use <literal>ipRanges</literal> to allocate IP addresses as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">cniVersion: operator.openshift.io/v1
kind: Network
=metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
       "name": "whereabouts-dual-stack",
       "cniVersion": "0.3.1,
       "type": "bridge",
       "ipam": {
         "type": "whereabouts",
         "ipRanges": [
                  {"range": "192.168.10.0/24"},
                  {"range": "2001:db8::/64"}
              ]
       }
      }</programlisting>
</listitem>
<listitem>
<simpara>Attach network to a pod. For more information, see "Adding a pod to an additional network".</simpara>
</listitem>
<listitem>
<simpara>Verify that all IP addresses are assigned.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to ensure the IP addresses are assigned as metadata.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc exec -it mypod -- ip a</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-multus-add-pod_attaching-pod">Attaching a pod to an additional network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-sriov-network-attachment_configuring-sriov-net-attach">
<title>Configuring SR-IOV additional network</title>
<simpara>You can configure an additional network that uses SR-IOV hardware by creating an <literal>SriovNetwork</literal> object.
When you create an <literal>SriovNetwork</literal> object, the SR-IOV Network Operator automatically creates a <literal>NetworkAttachmentDefinition</literal> object.</simpara>
<note>
<simpara>Do not modify or delete an <literal>SriovNetwork</literal> object if it is attached to any pods in a <literal>running</literal> state.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>SriovNetwork</literal> object, and then save the YAML in the <literal>&lt;name&gt;.yaml</literal> file, where <literal>&lt;name&gt;</literal> is a name for this additional network. The object specification might resemble the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: attach1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1
  networkNamespace: project2
  ipam: |-
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "gateway": "10.56.217.1"
    }</programlisting>
</listitem>
<listitem>
<simpara>To create the object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;.yaml</programlisting>
<simpara>where <literal>&lt;name&gt;</literal> specifies the name of the additional network.</simpara>
</listitem>
<listitem>
<simpara>Optional: To confirm that the <literal>NetworkAttachmentDefinition</literal> object that is associated with the <literal>SriovNetwork</literal> object that you created in the previous step exists, enter the following command. Replace <literal>&lt;namespace&gt;</literal> with the networkNamespace you specified in the <literal>SriovNetwork</literal> object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get net-attach-def -n &lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-sriov-net-attach-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="add-pod">Adding a pod to an SR-IOV additional network</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-sriov-net-attach-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-sriov-ib-attach">
<title>Configuring an SR-IOV InfiniBand network attachment</title>

<simpara>You can configure an InfiniBand (IB) network attachment for an Single Root I/O Virtualization (SR-IOV) device in the cluster.</simpara>
<section xml:id="nw-sriov-ibnetwork-object_configuring-sriov-ib-attach">
<title>InfiniBand device configuration object</title>
<simpara>You can configure an InfiniBand (IB) network device by defining an <literal>SriovIBNetwork</literal> object.</simpara>
<simpara>The following YAML describes an <literal>SriovIBNetwork</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovIBNetwork
metadata:
  name: &lt;name&gt; <co xml:id="CO117-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO117-2"/>
spec:
  resourceName: &lt;sriov_resource_name&gt; <co xml:id="CO117-3"/>
  networkNamespace: &lt;target_namespace&gt; <co xml:id="CO117-4"/>
  ipam: |- <co xml:id="CO117-5"/>
    {}
  linkState: &lt;link_state&gt; <co xml:id="CO117-6"/>
  capabilities: &lt;capabilities&gt; <co xml:id="CO117-7"/></programlisting>
<calloutlist>
<callout arearefs="CO117-1">
<para>A name for the object. The SR-IOV Network Operator creates a <literal>NetworkAttachmentDefinition</literal> object with same name.</para>
</callout>
<callout arearefs="CO117-2">
<para>The namespace where the SR-IOV Operator is installed.</para>
</callout>
<callout arearefs="CO117-3">
<para>The value for the <literal>spec.resourceName</literal> parameter from the <literal>SriovNetworkNodePolicy</literal> object that defines the SR-IOV hardware for this additional network.</para>
</callout>
<callout arearefs="CO117-4">
<para>The target namespace for the <literal>SriovIBNetwork</literal> object. Only pods in the target namespace can attach to the network device.</para>
</callout>
<callout arearefs="CO117-5">
<para>Optional: A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
<callout arearefs="CO117-6">
<para>Optional: The link state of virtual function (VF). Allowed values are <literal>enable</literal>, <literal>disable</literal> and <literal>auto</literal>.</para>
</callout>
<callout arearefs="CO117-7">
<para>Optional: The capabilities to configure for this network. You can specify <literal>'{ "ips": true }'</literal> to enable IP address support or <literal>'{ "infinibandGUID": true }'</literal> to enable IB Global Unique Identifier (GUID) support.</para>
</callout>
</calloutlist>
<section xml:id="nw-multus-ipam-object_configuring-sriov-ib-attach">
<title>Configuration of IP address assignment for an additional network</title>
<simpara>The IP address management (IPAM) Container Network Interface (CNI) plugin provides IP addresses for other CNI plugins.</simpara>
<simpara>You can use the following IP address assignment types:</simpara>
<itemizedlist>
<listitem>
<simpara>Static assignment.</simpara>
</listitem>
<listitem>
<simpara>Dynamic assignment through a DHCP server. The DHCP server you specify must be reachable from the additional network.</simpara>
</listitem>
<listitem>
<simpara>Dynamic assignment through the Whereabouts IPAM CNI plugin.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-multus-static_configuring-sriov-ib-attach">
<title>Static IP address assignment configuration</title>
<simpara>The following table describes the configuration for static IP address assignment:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> static configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>static</literal> is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>addresses</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of objects specifying IP addresses to assign to the virtual interface. Both IPv4 and IPv6 IP addresses are supported.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>routes</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of objects specifying routes to configure inside the pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>dns</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: An array of objects specifying the DNS configuration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The <literal>addresses</literal> array requires objects with the following fields:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.addresses[]</literal> array</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>address</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An IP address and network prefix that you specify. For example, if you specify <literal>10.10.21.10/24</literal>, then the additional network is assigned an IP address of <literal>10.10.21.10</literal> and the netmask is <literal>255.255.255.0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gateway</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The default gateway to route egress network traffic to.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.routes[]</literal> array</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>dst</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IP address range in CIDR format, such as <literal>192.168.17.0/24</literal> or <literal>0.0.0.0/0</literal> for the default route.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>gw</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The gateway where network traffic is routed.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam.dns</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>nameservers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of one or more IP addresses for to send DNS queries to.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>domain</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The default domain to append to a hostname. For example, if the
domain is set to <literal>example.com</literal>, a DNS lookup query for <literal>example-host</literal> is
rewritten as <literal>example-host.example.com</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>search</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An array of domain names to append to an unqualified hostname,
such as <literal>example-host</literal>, during a DNS lookup query.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Static IP address assignment configuration example</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "static",
      "addresses": [
        {
          "address": "191.168.1.7/24"
        }
      ]
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-dhcp_configuring-sriov-ib-attach">
<title>Dynamic IP address (DHCP) assignment configuration</title>
<simpara>The following JSON describes the configuration for dynamic IP address address assignment with DHCP.</simpara>
<important>
<title>Renewal of DHCP leases</title>
<simpara>A pod obtains its original DHCP lease when it is created. The lease must be periodically renewed by a minimal DHCP server deployment running on the cluster.</simpara>
<simpara>To trigger the deployment of the DHCP server, you must create a shim network attachment by editing the Cluster Network Operator configuration, as in the following example:</simpara>
<formalpara>
<title>Example shim network attachment definition</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: dhcp-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
        "name": "dhcp-shim",
        "cniVersion": "0.3.1",
        "type": "bridge",
        "ipam": {
          "type": "dhcp"
        }
      }
  # ...</programlisting>
</para>
</formalpara>
</important>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> DHCP configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>dhcp</literal> is required.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Dynamic IP address (DHCP) assignment configuration example</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "dhcp"
  }
}</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-multus-whereabouts_configuring-sriov-ib-attach">
<title>Dynamic IP address assignment configuration with Whereabouts</title>
<simpara>The Whereabouts CNI plugin allows the dynamic assignment of an IP address to an additional network without the use of a DHCP server.</simpara>
<simpara>The following table describes the configuration for dynamic IP address assignment with Whereabouts:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>ipam</literal> whereabouts configuration object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>type</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The IPAM address type. The value <literal>whereabouts</literal> is required.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>range</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>An IP address and range in CIDR notation. IP addresses are assigned from within this range of addresses.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>exclude</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: A list of zero or more IP addresses and ranges in CIDR notation. IP addresses within an excluded address range are not assigned.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Dynamic IP address assignment configuration example that uses Whereabouts</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "ipam": {
    "type": "whereabouts",
    "range": "192.0.2.192/27",
    "exclude": [
       "192.0.2.192/30",
       "192.0.2.196/32"
    ]
  }
}</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="nw-multus-configure-dualstack-ip-address_configuring-sriov-ib-attach">
<title>Creating a configuration for assignment of dual-stack IP addresses dynamically</title>
<simpara>Dual-stack IP address assignment can be configured with the <literal>ipRanges</literal> parameter for:</simpara>
<itemizedlist>
<listitem>
<simpara>IPv4 addresses</simpara>
</listitem>
<listitem>
<simpara>IPv6 addresses</simpara>
</listitem>
<listitem>
<simpara>multiple IP address assignment</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set <literal>type</literal> to <literal>whereabouts</literal>.</simpara>
</listitem>
<listitem>
<simpara>Use <literal>ipRanges</literal> to allocate IP addresses as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">cniVersion: operator.openshift.io/v1
kind: Network
=metadata:
  name: cluster
spec:
  additionalNetworks:
  - name: whereabouts-shim
    namespace: default
    type: Raw
    rawCNIConfig: |-
      {
       "name": "whereabouts-dual-stack",
       "cniVersion": "0.3.1,
       "type": "bridge",
       "ipam": {
         "type": "whereabouts",
         "ipRanges": [
                  {"range": "192.168.10.0/24"},
                  {"range": "2001:db8::/64"}
              ]
       }
      }</programlisting>
</listitem>
<listitem>
<simpara>Attach network to a pod. For more information, see "Adding a pod to an additional network".</simpara>
</listitem>
<listitem>
<simpara>Verify that all IP addresses are assigned.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to ensure the IP addresses are assigned as metadata.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc exec -it mypod -- ip a</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-multus-add-pod_attaching-pod">Attaching a pod to an additional network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-sriov-network-attachment_configuring-sriov-ib-attach">
<title>Configuring SR-IOV additional network</title>
<simpara>You can configure an additional network that uses SR-IOV hardware by creating an <literal>SriovIBNetwork</literal> object.
When you create an <literal>SriovIBNetwork</literal> object, the SR-IOV Network Operator automatically creates a <literal>NetworkAttachmentDefinition</literal> object.</simpara>
<note>
<simpara>Do not modify or delete an <literal>SriovIBNetwork</literal> object if it is attached to any pods in a <literal>running</literal> state.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>SriovIBNetwork</literal> object, and then save the YAML in the <literal>&lt;name&gt;.yaml</literal> file, where <literal>&lt;name&gt;</literal> is a name for this additional network. The object specification might resemble the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovIBNetwork
metadata:
  name: attach1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: net1
  networkNamespace: project2
  ipam: |-
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "gateway": "10.56.217.1"
    }</programlisting>
</listitem>
<listitem>
<simpara>To create the object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;.yaml</programlisting>
<simpara>where <literal>&lt;name&gt;</literal> specifies the name of the additional network.</simpara>
</listitem>
<listitem>
<simpara>Optional: To confirm that the <literal>NetworkAttachmentDefinition</literal> object that is associated with the <literal>SriovIBNetwork</literal> object that you created in the previous step exists, enter the following command. Replace <literal>&lt;namespace&gt;</literal> with the networkNamespace you specified in the <literal>SriovIBNetwork</literal> object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get net-attach-def -n &lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-sriov-ib-attach-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="add-pod">Adding a pod to an SR-IOV additional network</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-sriov-ib-attach-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="add-pod">
<title>Adding a pod to an SR-IOV additional network</title>

<simpara>You can add a pod to an existing Single Root I/O Virtualization (SR-IOV) network.</simpara>
<section xml:id="nw-sriov-runtime-config_configuring-sr-iov">
<title>Runtime configuration for a network attachment</title>
<simpara>When attaching a pod to an additional network, you can specify a runtime configuration to make specific customizations for the pod. For example, you can request a specific MAC hardware address.</simpara>
<simpara>You specify the runtime configuration by setting an annotation in the pod specification. The annotation key is <literal>k8s.v1.cni.cncf.io/networks</literal>, and it accepts a JSON object that describes the runtime configuration.</simpara>
<section xml:id="runtime-config-ethernet_configuring-sr-iov">
<title>Runtime configuration for an Ethernet-based SR-IOV attachment</title>
<simpara>The following JSON describes the runtime configuration options for an Ethernet-based SR-IOV network attachment.</simpara>
<programlisting language="json" linenumbering="unnumbered">[
  {
    "name": "&lt;name&gt;", <co xml:id="CO118-1"/>
    "mac": "&lt;mac_address&gt;", <co xml:id="CO118-2"/>
    "ips": ["&lt;cidr_range&gt;"] <co xml:id="CO118-3"/>
  }
]</programlisting>
<calloutlist>
<callout arearefs="CO118-1">
<para>The name of the SR-IOV network attachment definition CR.</para>
</callout>
<callout arearefs="CO118-2">
<para>Optional: The MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <literal>{ "mac": true }</literal> in the <literal>SriovNetwork</literal> object.</para>
</callout>
<callout arearefs="CO118-3">
<para>Optional: IP addresses for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <literal>{ "ips": true }</literal> in the <literal>SriovNetwork</literal> object.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example runtime configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "net1",
          "mac": "20:04:0f:f1:88:01",
          "ips": ["192.168.10.1/24", "2001::1/64"]
        }
      ]
spec:
  containers:
  - name: sample-container
    image: &lt;image&gt;
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]</programlisting>
</para>
</formalpara>
</section>
<section xml:id="runtime-config-infiniband_configuring-sr-iov">
<title>Runtime configuration for an InfiniBand-based SR-IOV attachment</title>
<simpara>The following JSON describes the runtime configuration options for an InfiniBand-based SR-IOV network attachment.</simpara>
<programlisting language="json" linenumbering="unnumbered">[
  {
    "name": "&lt;network_attachment&gt;", <co xml:id="CO119-1"/>
    "infiniband-guid": "&lt;guid&gt;", <co xml:id="CO119-2"/>
    "ips": ["&lt;cidr_range&gt;"] <co xml:id="CO119-3"/>
  }
]</programlisting>
<calloutlist>
<callout arearefs="CO119-1">
<para>The name of the SR-IOV network attachment definition CR.</para>
</callout>
<callout arearefs="CO119-2">
<para>The InfiniBand GUID for the SR-IOV device. To use this feature, you also must specify <literal>{ "infinibandGUID": true }</literal> in the <literal>SriovIBNetwork</literal> object.</para>
</callout>
<callout arearefs="CO119-3">
<para>The IP addresses for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <literal>{ "ips": true }</literal> in the <literal>SriovIBNetwork</literal> object.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example runtime configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "ib1",
          "infiniband-guid": "c2:11:22:33:44:55:66:77",
          "ips": ["192.168.10.1/24", "2001::1/64"]
        }
      ]
spec:
  containers:
  - name: sample-container
    image: &lt;image&gt;
    imagePullPolicy: IfNotPresent
    command: ["sleep", "infinity"]</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="nw-multus-add-pod_configuring-sr-iov">
<title>Adding a pod to an additional network</title>
<simpara>You can add a pod to an additional network. The pod continues to send normal cluster-related network traffic over the default network.</simpara>
<simpara>When a pod is created additional networks are attached to it. However, if a pod already exists, you cannot attach additional networks to it.</simpara>
<simpara>The pod must be in the same namespace as the additional network.</simpara>
<note>
<simpara>The SR-IOV Network Resource Injector adds the <literal>resource</literal> field to the first container in a pod automatically.</simpara>
<simpara>If you are using an Intel network interface controller (NIC) in Data Plane Development Kit (DPDK) mode, only the first container in your pod is configured to access the NIC. Your SR-IOV additional network is configured for DPDK mode if the <literal>deviceType</literal> is set to <literal>vfio-pci</literal> in the <literal>SriovNetworkNodePolicy</literal> object.</simpara>
<simpara>You can work around this issue by either ensuring that the container that needs access to the NIC is the first container defined in the <literal>Pod</literal> object or by disabling the Network Resource Injector. For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1990953">BZ#1990953</link>.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster.</simpara>
</listitem>
<listitem>
<simpara>Install the SR-IOV Operator.</simpara>
</listitem>
<listitem>
<simpara>Create either an <literal>SriovNetwork</literal> object or an <literal>SriovIBNetwork</literal> object to attach the pod to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add an annotation to the <literal>Pod</literal> object. Only one of the following annotation formats can be used:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To attach an additional network without any customization, add an annotation with the following format. Replace <literal>&lt;network&gt;</literal> with the name of the additional network to associate with the pod:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: &lt;network&gt;[,&lt;network&gt;,...] <co xml:id="CO120-1"/></programlisting>
<calloutlist>
<callout arearefs="CO120-1">
<para>To specify more than one additional network, separate each network
with a comma. Do not include whitespace between the comma. If you specify
the same additional network multiple times, that pod will have multiple network
interfaces attached to that network.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To attach an additional network with customizations, add an annotation with the following format:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "&lt;network&gt;", <co xml:id="CO121-1"/>
          "namespace": "&lt;namespace&gt;", <co xml:id="CO121-2"/>
          "default-route": ["&lt;default-route&gt;"] <co xml:id="CO121-3"/>
        }
      ]</programlisting>
<calloutlist>
<callout arearefs="CO121-1">
<para>Specify the name of the additional network defined by a <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO121-2">
<para>Specify the namespace where the <literal>NetworkAttachmentDefinition</literal> object is defined.</para>
</callout>
<callout arearefs="CO121-3">
<para>Optional: Specify an override for the default route, such as <literal>192.168.17.1</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To create the pod, enter the following command. Replace <literal>&lt;name&gt;</literal> with the name of the pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To Confirm that the annotation exists in the <literal>Pod</literal> CR, enter the following command, replacing <literal>&lt;name&gt;</literal> with the name of the pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod &lt;name&gt; -o yaml</programlisting>
<simpara>In the following example, the <literal>example-pod</literal> pod is attached to the <literal>net1</literal>
additional network:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod example-pod -o yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: macvlan-bridge
    k8s.v1.cni.cncf.io/network-status: |- <co xml:id="CO122-1"/>
      [{
          "name": "openshift-sdn",
          "interface": "eth0",
          "ips": [
              "10.128.2.14"
          ],
          "default": true,
          "dns": {}
      },{
          "name": "macvlan-bridge",
          "interface": "net1",
          "ips": [
              "20.2.2.100"
          ],
          "mac": "22:2f:60:a5:f8:00",
          "dns": {}
      }]
  name: example-pod
  namespace: default
spec:
  ...
status:
  ...</programlisting>
<calloutlist>
<callout arearefs="CO122-1">
<para>The <literal>k8s.v1.cni.cncf.io/network-status</literal> parameter is a JSON array of
objects. Each object describes the status of an additional network attached
to the pod. The annotation value is stored as a plain text value.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-sriov-topology-manager_configuring-sr-iov">
<title>Creating a non-uniform memory access (NUMA) aligned SR-IOV pod</title>
<simpara>You can create a NUMA aligned SR-IOV pod by restricting SR-IOV and the CPU resources allocated from the same NUMA node with <literal>restricted</literal> or <literal>single-numa-node</literal> Topology Manager polices.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have configured the CPU Manager policy to <literal>static</literal>. For more information on CPU Manager, see the "Additional resources" section.</simpara>
</listitem>
<listitem>
<simpara>You have configured the Topology Manager policy to <literal>single-numa-node</literal>.</simpara>
<note>
<simpara>When <literal>single-numa-node</literal> is unable to satisfy the request, you can configure the Topology Manager policy to <literal>restricted</literal>. For more flexible SR-IOV network resource scheduling, see <emphasis>Excluding SR-IOV network topology during NUMA-aware scheduling</emphasis> in the <emphasis>Additional resources</emphasis> section.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following SR-IOV pod spec, and then save the YAML in the <literal>&lt;name&gt;-sriov-pod.yaml</literal> file. Replace <literal>&lt;name&gt;</literal> with a name for this pod.</simpara>
<simpara>The following example shows an SR-IOV pod spec:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: sample-pod
  annotations:
    k8s.v1.cni.cncf.io/networks: &lt;name&gt; <co xml:id="CO123-1"/>
spec:
  containers:
  - name: sample-container
    image: &lt;image&gt; <co xml:id="CO123-2"/>
    command: ["sleep", "infinity"]
    resources:
      limits:
        memory: "1Gi" <co xml:id="CO123-3"/>
        cpu: "2" <co xml:id="CO123-4"/>
      requests:
        memory: "1Gi"
        cpu: "2"</programlisting>
<calloutlist>
<callout arearefs="CO123-1">
<para>Replace <literal>&lt;name&gt;</literal> with the name of the SR-IOV network attachment definition CR.</para>
</callout>
<callout arearefs="CO123-2">
<para>Replace <literal>&lt;image&gt;</literal> with the name of the <literal>sample-pod</literal> image.</para>
</callout>
<callout arearefs="CO123-3">
<para>To create the SR-IOV pod with guaranteed QoS, set <literal>memory limits</literal> equal to <literal>memory requests</literal>.</para>
</callout>
<callout arearefs="CO123-4">
<para>To create the SR-IOV pod with guaranteed QoS, set <literal>cpu limits</literal> equals to <literal>cpu requests</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the sample SR-IOV pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;filename&gt; <co xml:id="CO124-1"/></programlisting>
<calloutlist>
<callout arearefs="CO124-1">
<para>Replace <literal>&lt;filename&gt;</literal> with the name of the file you created in the previous step.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Confirm that the <literal>sample-pod</literal> is configured with guaranteed QoS.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pod sample-pod</programlisting>
</listitem>
<listitem>
<simpara>Confirm that the <literal>sample-pod</literal> is allocated with exclusive CPUs.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec sample-pod -- cat /sys/fs/cgroup/cpuset/cpuset.cpus</programlisting>
</listitem>
<listitem>
<simpara>Confirm that the SR-IOV device and CPUs that are allocated for the <literal>sample-pod</literal> are on the same NUMA node.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec sample-pod -- cat /sys/fs/cgroup/cpuset/cpuset.cpus</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-openstack-ovs-sr-iov-testpmd-pod_configuring-sr-iov">
<title>A test pod template for clusters that use SR-IOV on OpenStack</title>
<simpara>The following <literal>testpmd</literal> pod demonstrates container creation with huge pages, reserved CPUs, and the SR-IOV port.</simpara>
<formalpara>
<title>An example <literal>testpmd</literal> pod</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: testpmd-sriov
  namespace: mynamespace
  annotations:
    cpu-load-balancing.crio.io: "disable"
    cpu-quota.crio.io: "disable"
# ...
spec:
  containers:
  - name: testpmd
    command: ["sleep", "99999"]
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.9
    securityContext:
      capabilities:
        add: ["IPC_LOCK","SYS_ADMIN"]
      privileged: true
      runAsUser: 0
    resources:
      requests:
        memory: 1000Mi
        hugepages-1Gi: 1Gi
        cpu: '2'
        openshift.io/sriov1: 1
      limits:
        hugepages-1Gi: 1Gi
        cpu: '2'
        memory: 1000Mi
        openshift.io/sriov1: 1
    volumeMounts:
      - mountPath: /dev/hugepages
        name: hugepage
        readOnly: False
  runtimeClassName: performance-cnf-performanceprofile <co xml:id="CO125-1"/>
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO125-1">
<para>This example assumes that the name of the performance profile is <literal>cnf-performance profile</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="add-pod-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV Ethernet network attachment</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-sriov-ib-attach">Configuring an SR-IOV InfiniBand network attachment</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#using-cpu-manager">Using CPU Manager</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-sriov-exclude-topology-manager_configuring-sriov-device">Exclude SR-IOV network topology for NUMA-aware scheduling</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-interface-level-sysctl-settings-sriov-device">
<title>Configuring interface-level network sysctl settings and all-multicast mode for SR-IOV networks</title>

<simpara>As a cluster administrator, you can change interface-level network sysctls and several interface attributes such as promiscuous mode, all-multicast mode, MTU, and MAC address by using the tuning Container Network Interface (CNI) meta plugin for a pod connected to a SR-IOV network device.</simpara>
<section xml:id="nw-labeling-sriov-enabled-nodes_configuring-sysctl-interface-sriov-device">
<title>Labeling nodes with an SR-IOV enabled NIC</title>
<simpara>If you want to enable SR-IOV on only SR-IOV capable nodes there are a couple of ways to do this:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Install the Node Feature Discovery (NFD) Operator. NFD detects the presence of SR-IOV enabled NICs and labels the nodes with <literal>node.alpha.kubernetes-incubator.io/nfd-network-sriov.capable = true</literal>.</simpara>
</listitem>
<listitem>
<simpara>Examine the <literal>SriovNetworkNodeState</literal> CR for each node. The <literal>interfaces</literal> stanza includes a list of all of the SR-IOV devices discovered by the SR-IOV Network Operator on the worker node. Label each node with <literal>feature.node.kubernetes.io/network-sriov.capable: "true"</literal> by using the following command:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc label node &lt;node_name&gt; feature.node.kubernetes.io/network-sriov.capable="true"</programlisting>
<note>
<simpara>You can label the nodes with whatever name you want.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-setting-one-sysctl-flag_configuring-sysctl-interface-sriov-device">
<title>Setting one sysctl flag</title>
<simpara>You can set interface-level network <literal>sysctl</literal> settings for a pod connected to a SR-IOV network device.</simpara>
<simpara>In this example, <literal>net.ipv4.conf.IFNAME.accept_redirects</literal> is set to <literal>1</literal> on the created virtual interfaces.</simpara>
<simpara>The <literal>sysctl-tuning-test</literal> is a namespace used in this example.</simpara>
<itemizedlist>
<listitem>
<simpara>Use the following command to create the <literal>sysctl-tuning-test</literal> namespace:</simpara>
<screen>$ oc create namespace sysctl-tuning-test</screen>
</listitem>
</itemizedlist>
<section xml:id="nw-basic-example-setting-one-sysctl-flag-node-policy_configuring-sysctl-interface-sriov-device">
<title>Setting one sysctl flag on nodes with SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator adds the <literal>SriovNetworkNodePolicy.sriovnetwork.openshift.io</literal> custom resource definition (CRD) to OpenShift Container Platform. You can configure an SR-IOV network device by creating a <literal>SriovNetworkNodePolicy</literal> custom resource (CR).</simpara>
<note>
<simpara>When applying the configuration specified in a <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator might drain and reboot the nodes.</simpara>
<simpara>It can take several minutes for a configuration change to apply.</simpara>
</note>
<simpara>Follow this procedure to create a <literal>SriovNetworkNodePolicy</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> custom resource (CR). For example, save the following YAML as the file <literal>policyoneflag-sriov-node-network.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policyoneflag <co xml:id="CO126-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO126-2"/>
spec:
  resourceName: policyoneflag <co xml:id="CO126-3"/>
  nodeSelector: <co xml:id="CO126-4"/>
    feature.node.kubernetes.io/network-sriov.capable="true"
  priority: 10 <co xml:id="CO126-5"/>
  numVfs: 5 <co xml:id="CO126-6"/>
  nicSelector: <co xml:id="CO126-7"/>
    pfNames: ["ens5"] <co xml:id="CO126-8"/>
  deviceType: "netdevice" <co xml:id="CO126-9"/>
  isRdma: false <co xml:id="CO126-10"/></programlisting>
<calloutlist>
<callout arearefs="CO126-1">
<para>The name for the custom resource object.</para>
</callout>
<callout arearefs="CO126-2">
<para>The namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO126-3">
<para>The resource name of the SR-IOV network device plugin. You can create multiple SR-IOV network node policies for a resource name.</para>
</callout>
<callout arearefs="CO126-4">
<para>The node selector specifies the nodes to configure. Only SR-IOV network devices on the selected nodes are configured. The SR-IOV Container Network Interface (CNI) plugin and device plugin are deployed on selected nodes only.</para>
</callout>
<callout arearefs="CO126-5">
<para>Optional: The priority is an integer value between <literal>0</literal> and <literal>99</literal>. A smaller value receives higher priority. For example, a priority of <literal>10</literal> is a higher priority than <literal>99</literal>. The default value is <literal>99</literal>.</para>
</callout>
<callout arearefs="CO126-6">
<para>The number of the virtual functions (VFs) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <literal>128</literal>.</para>
</callout>
<callout arearefs="CO126-7">
<para>The NIC selector identifies the device for the Operator to configure. You do not have to specify values for all the parameters. It is recommended to identify the network device with enough precision to avoid selecting a device unintentionally.
If you specify <literal>rootDevices</literal>, you must also specify a value for <literal>vendor</literal>, <literal>deviceID</literal>, or <literal>pfNames</literal>. If you specify both <literal>pfNames</literal> and <literal>rootDevices</literal> at the same time, ensure that they refer to the same device. If you specify a value for <literal>netFilter</literal>, then you do not need to specify any other parameter because a network ID is unique.</para>
</callout>
<callout arearefs="CO126-8">
<para>Optional: An array of one or more physical function (PF) names for the device.</para>
</callout>
<callout arearefs="CO126-9">
<para>Optional: The driver type for the virtual functions. The only allowed value is <literal>netdevice</literal>.
For a Mellanox NIC to work in DPDK mode on bare metal nodes, set <literal>isRdma</literal> to <literal>true</literal>.</para>
</callout>
<callout arearefs="CO126-10">
<para>Optional: Configures whether to enable remote direct memory access (RDMA) mode. The default value is <literal>false</literal>.
If the <literal>isRdma</literal> parameter is set to <literal>true</literal>, you can continue to use the RDMA-enabled VF as a normal network device. A device can be used in either mode.
Set <literal>isRdma</literal> to <literal>true</literal> and additionally set <literal>needVhostNet</literal> to <literal>true</literal> to configure a Mellanox NIC for use with Fast Datapath DPDK applications.</para>
</callout>
</calloutlist>
<note>
<simpara>The <literal>vfio-pci</literal> driver type is not supported.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f policyoneflag-sriov-node-network.yaml</programlisting>
<simpara>After applying the configuration update, all the pods in <literal>sriov-network-operator</literal> namespace change to the <literal>Running</literal> status.</simpara>
</listitem>
<listitem>
<simpara>To verify that the SR-IOV network device is configured, enter the following command. Replace <literal>&lt;node_name&gt;</literal> with the name of a node with the SR-IOV network device that you just configured.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-sysctl-on-sriov-network_configuring-sysctl-interface-sriov-device">
<title>Configuring sysctl on a SR-IOV network</title>
<simpara>You can set interface specific <literal>sysctl</literal> settings on virtual interfaces created by SR-IOV by adding the tuning configuration to the optional <literal>metaPlugins</literal> parameter of the <literal>SriovNetwork</literal> resource.</simpara>
<simpara>The SR-IOV Network Operator manages additional network definitions. When you specify an additional SR-IOV network to create, the SR-IOV Network Operator creates the <literal>NetworkAttachmentDefinition</literal> custom resource (CR) automatically.</simpara>
<note>
<simpara>Do not edit <literal>NetworkAttachmentDefinition</literal> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.</simpara>
</note>
<simpara>To change the interface-level network <literal>net.ipv4.conf.IFNAME.accept_redirects</literal> <literal>sysctl</literal> settings, create an additional SR-IOV network with the Container Network Interface (CNI) tuning plugin.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI (oc).</simpara>
</listitem>
<listitem>
<simpara>Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> custom resource (CR) for the additional SR-IOV network attachment and insert the <literal>metaPlugins</literal> configuration, as in the following example CR. Save the YAML as the file <literal>sriov-network-interface-sysctl.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: onevalidflag <co xml:id="CO127-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO127-2"/>
spec:
  resourceName: policyoneflag <co xml:id="CO127-3"/>
  networkNamespace: sysctl-tuning-test <co xml:id="CO127-4"/>
  ipam: '{ "type": "static" }' <co xml:id="CO127-5"/>
  capabilities: '{ "mac": true, "ips": true }' <co xml:id="CO127-6"/>
  metaPlugins : | <co xml:id="CO127-7"/>
    {
      "type": "tuning",
      "capabilities":{
        "mac":true
      },
      "sysctl":{
         "net.ipv4.conf.IFNAME.accept_redirects": "1"
      }
    }</programlisting>
<calloutlist>
<callout arearefs="CO127-1">
<para>A name for the object. The SR-IOV Network Operator creates a NetworkAttachmentDefinition object with same name.</para>
</callout>
<callout arearefs="CO127-2">
<para>The namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO127-3">
<para>The value for the <literal>spec.resourceName</literal> parameter from the <literal>SriovNetworkNodePolicy</literal> object that defines the SR-IOV hardware for this additional network.</para>
</callout>
<callout arearefs="CO127-4">
<para>The target namespace for the <literal>SriovNetwork</literal> object. Only pods in the target namespace can attach to the additional network.</para>
</callout>
<callout arearefs="CO127-5">
<para>A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
<callout arearefs="CO127-6">
<para>Optional: Set capabilities for the additional network. You can specify <literal>"{ "ips": true }"</literal> to enable IP address support or <literal>"{ "mac": true }"</literal> to enable MAC address support.</para>
</callout>
<callout arearefs="CO127-7">
<para>Optional: The metaPlugins parameter is used to add additional capabilities to the device. In this use case set the <literal>type</literal> field to <literal>tuning</literal>. Specify the interface-level network <literal>sysctl</literal> you want to set in the <literal>sysctl</literal> field.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network-interface-sysctl.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verifying that the <literal>NetworkAttachmentDefinition</literal> CR is successfully created</title>
<listitem>
<simpara>Confirm that the SR-IOV Network Operator created the <literal>NetworkAttachmentDefinition</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -n &lt;namespace&gt; <co xml:id="CO128-1"/></programlisting>
<calloutlist>
<callout arearefs="CO128-1">
<para>Replace <literal>&lt;namespace&gt;</literal> with the value for <literal>networkNamespace</literal> that you specified in the <literal>SriovNetwork</literal> object. For example, <literal>sysctl-tuning-test</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                  AGE
onevalidflag                          14m</programlisting>
</para>
</formalpara>
<note>
<simpara>There might be a delay before the SR-IOV Network Operator creates the CR.</simpara>
</note>
</listitem>
</itemizedlist>
<formalpara>
<title>Verifying that the additional SR-IOV network attachment is successful</title>
<para>To verify that the tuning CNI is correctly configured and the additional SR-IOV network attachment is attached, do the following:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a <literal>Pod</literal> CR. Save the following YAML as the file <literal>examplepod.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: tunepod
  namespace: sysctl-tuning-test
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "onevalidflag",  <co xml:id="CO129-1"/>
          "mac": "0a:56:0a:83:04:0c", <co xml:id="CO129-2"/>
          "ips": ["10.100.100.200/24"] <co xml:id="CO129-3"/>
       }
      ]
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault</programlisting>
<calloutlist>
<callout arearefs="CO129-1">
<para>The name of the SR-IOV network attachment definition CR.</para>
</callout>
<callout arearefs="CO129-2">
<para>Optional: The MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <literal>{ "mac": true }</literal> in the SriovNetwork object.</para>
</callout>
<callout arearefs="CO129-3">
<para>Optional: IP addresses for the SR-IOV device that are allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <literal>{ "ips": true }</literal> in the <literal>SriovNetwork</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>Pod</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f examplepod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n sysctl-tuning-test</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      READY   STATUS    RESTARTS   AGE
tunepod   1/1     Running   0          47s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Log in to the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n sysctl-tuning-test tunepod</programlisting>
</listitem>
<listitem>
<simpara>Verify the values of the configured sysctl flag. Find the value  <literal>net.ipv4.conf.IFNAME.accept_redirects</literal> by running the following command::</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sysctl net.ipv4.conf.net1.accept_redirects</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">net.ipv4.conf.net1.accept_redirects = 1</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-configure-sysctl-settings-flag-bonded_configuring-sysctl-interface-sriov-device">
<title>Configuring sysctl settings for pods associated with bonded SR-IOV interface flag</title>
<simpara>You can set interface-level network <literal>sysctl</literal> settings for a pod connected to a bonded SR-IOV network device.</simpara>
<simpara>In this example, the specific network interface-level <literal>sysctl</literal> settings that can be configured are set on the bonded interface.</simpara>
<simpara>The <literal>sysctl-tuning-test</literal> is a namespace used in this example.</simpara>
<itemizedlist>
<listitem>
<simpara>Use the following command to create the <literal>sysctl-tuning-test</literal> namespace:</simpara>
<screen>$ oc create namespace sysctl-tuning-test</screen>
</listitem>
</itemizedlist>
<section xml:id="nw-setting-all-sysctls-flag-node-policy-bonded_configuring-sysctl-interface-sriov-device">
<title>Setting all sysctl flag on nodes with bonded SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator adds the <literal>SriovNetworkNodePolicy.sriovnetwork.openshift.io</literal> custom resource definition (CRD) to OpenShift Container Platform. You can configure an SR-IOV network device by creating a <literal>SriovNetworkNodePolicy</literal> custom resource (CR).</simpara>
<note>
<simpara>When applying the configuration specified in a SriovNetworkNodePolicy object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.</simpara>
<simpara>It might take several minutes for a configuration change to apply.</simpara>
</note>
<simpara>Follow this procedure to create a <literal>SriovNetworkNodePolicy</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> custom resource (CR). Save the following YAML as the file <literal>policyallflags-sriov-node-network.yaml</literal>. Replace <literal>policyallflags</literal> with the name for the configuration.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policyallflags <co xml:id="CO130-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO130-2"/>
spec:
  resourceName: policyallflags <co xml:id="CO130-3"/>
  nodeSelector: <co xml:id="CO130-4"/>
    node.alpha.kubernetes-incubator.io/nfd-network-sriov.capable = `true`
  priority: 10 <co xml:id="CO130-5"/>
  numVfs: 5 <co xml:id="CO130-6"/>
  nicSelector: <co xml:id="CO130-7"/>
    pfNames: ["ens1f0"]  <co xml:id="CO130-8"/>
  deviceType: "netdevice" <co xml:id="CO130-9"/>
  isRdma: false <co xml:id="CO130-10"/></programlisting>
<calloutlist>
<callout arearefs="CO130-1">
<para>The name for the custom resource object.</para>
</callout>
<callout arearefs="CO130-2">
<para>The namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO130-3">
<para>The resource name of the SR-IOV network device plugin. You can create multiple SR-IOV network node policies for a resource name.</para>
</callout>
<callout arearefs="CO130-4">
<para>The node selector specifies the nodes to configure. Only SR-IOV network devices on the selected nodes are configured. The SR-IOV Container Network Interface (CNI) plugin and device plugin are deployed on selected nodes only.</para>
</callout>
<callout arearefs="CO130-5">
<para>Optional: The priority is an integer value between <literal>0</literal> and <literal>99</literal>. A smaller value receives higher priority. For example, a priority of <literal>10</literal> is a higher priority than <literal>99</literal>. The default value is <literal>99</literal>.</para>
</callout>
<callout arearefs="CO130-6">
<para>The number of virtual functions (VFs) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <literal>128</literal>.</para>
</callout>
<callout arearefs="CO130-7">
<para>The NIC selector identifies the device for the Operator to configure. You do not have to specify values for all the parameters. It is recommended to identify the network device with enough precision to avoid selecting a device unintentionally.
If you specify <literal>rootDevices</literal>, you must also specify a value for <literal>vendor</literal>, <literal>deviceID</literal>, or <literal>pfNames</literal>. If you specify both <literal>pfNames</literal> and <literal>rootDevices</literal> at the same time, ensure that they refer to the same device. If you specify a value for <literal>netFilter</literal>, then you do not need to specify any other parameter because a network ID is unique.</para>
</callout>
<callout arearefs="CO130-8">
<para>Optional: An array of one or more physical function (PF) names for the device.</para>
</callout>
<callout arearefs="CO130-9">
<para>Optional: The driver type for the virtual functions. The only allowed value is <literal>netdevice</literal>.
For a Mellanox NIC to work in DPDK mode on bare metal nodes, set <literal>isRdma</literal> to <literal>true</literal>.</para>
</callout>
<callout arearefs="CO130-10">
<para>Optional: Configures whether to enable remote direct memory access (RDMA) mode. The default value is <literal>false</literal>.
If the <literal>isRdma</literal> parameter is set to <literal>true</literal>, you can continue to use the RDMA-enabled VF as a normal network device. A device can be used in either mode.
Set <literal>isRdma</literal> to <literal>true</literal> and additionally set <literal>needVhostNet</literal> to <literal>true</literal> to configure a Mellanox NIC for use with Fast Datapath DPDK applications.</para>
</callout>
</calloutlist>
<note>
<simpara>The <literal>vfio-pci</literal> driver type is not supported.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the SriovNetworkNodePolicy object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f policyallflags-sriov-node-network.yaml</programlisting>
<simpara>After applying the configuration update, all the pods in sriov-network-operator namespace change to the <literal>Running</literal> status.</simpara>
</listitem>
<listitem>
<simpara>To verify that the SR-IOV network device is configured, enter the following command. Replace <literal>&lt;node_name&gt;</literal> with the name of a node with the SR-IOV network device that you just configured.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-sysctl-on-bonded-sriov-network_configuring-sysctl-interface-sriov-device">
<title>Configuring sysctl on a bonded SR-IOV network</title>
<simpara>You can set interface specific <literal>sysctl</literal> settings on a bonded interface created from two SR-IOV interfaces. Do this by adding the tuning configuration to the optional <literal>Plugins</literal> parameter of the bond network attachment definition.</simpara>
<note>
<simpara>Do not edit <literal>NetworkAttachmentDefinition</literal> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.</simpara>
</note>
<simpara>To change specific interface-level network <literal>sysctl</literal> settings create the <literal>SriovNetwork</literal> custom resource (CR)  with the Container Network Interface (CNI) tuning plugin by using the following procedure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI (oc).</simpara>
</listitem>
<listitem>
<simpara>Log in to the OpenShift Container Platform cluster as a user with cluster-admin privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> custom resource (CR) for the bonded interface as in the following example CR. Save the YAML as the file <literal>sriov-network-attachment.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: allvalidflags <co xml:id="CO131-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO131-2"/>
spec:
  resourceName: policyallflags <co xml:id="CO131-3"/>
  networkNamespace: sysctl-tuning-test <co xml:id="CO131-4"/>
  capabilities: '{ "mac": true, "ips": true }' <co xml:id="CO131-5"/></programlisting>
<calloutlist>
<callout arearefs="CO131-1">
<para>A name for the object. The SR-IOV Network Operator creates a NetworkAttachmentDefinition object with same name.</para>
</callout>
<callout arearefs="CO131-2">
<para>The namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO131-3">
<para>The value for the <literal>spec.resourceName</literal> parameter from the <literal>SriovNetworkNodePolicy</literal> object that defines the SR-IOV hardware for this additional network.</para>
</callout>
<callout arearefs="CO131-4">
<para>The target namespace for the <literal>SriovNetwork</literal> object. Only pods in the target namespace can attach to the additional network.</para>
</callout>
<callout arearefs="CO131-5">
<para>Optional: The capabilities to configure for this additional network. You can specify <literal>"{ "ips": true }"</literal> to enable IP address support or <literal>"{ "mac": true }"</literal> to enable MAC address support.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network-attachment.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a bond network attachment definition as in the following example CR. Save the YAML as the file <literal>sriov-bond-network-interface.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bond-sysctl-network
  namespace: sysctl-tuning-test
spec:
  config: '{
  "cniVersion":"0.4.0",
  "name":"bound-net",
  "plugins":[
    {
      "type":"bond", <co xml:id="CO132-1"/>
      "mode": "active-backup", <co xml:id="CO132-2"/>
      "failOverMac": 1, <co xml:id="CO132-3"/>
      "linksInContainer": true, <co xml:id="CO132-4"/>
      "miimon": "100",
      "links": [ <co xml:id="CO132-5"/>
        {"name": "net1"},
        {"name": "net2"}
      ],
      "ipam":{ <co xml:id="CO132-6"/>
        "type":"static"
      }
    },
    {
      "type":"tuning", <co xml:id="CO132-7"/>
      "capabilities":{
        "mac":true
      },
      "sysctl":{
        "net.ipv4.conf.IFNAME.accept_redirects": "0",
        "net.ipv4.conf.IFNAME.accept_source_route": "0",
        "net.ipv4.conf.IFNAME.disable_policy": "1",
        "net.ipv4.conf.IFNAME.secure_redirects": "0",
        "net.ipv4.conf.IFNAME.send_redirects": "0",
        "net.ipv6.conf.IFNAME.accept_redirects": "0",
        "net.ipv6.conf.IFNAME.accept_source_route": "1",
        "net.ipv6.neigh.IFNAME.base_reachable_time_ms": "20000",
        "net.ipv6.neigh.IFNAME.retrans_time_ms": "2000"
      }
    }
  ]
}'</programlisting>
<calloutlist>
<callout arearefs="CO132-1">
<para>The type is <literal>bond</literal>.</para>
</callout>
<callout arearefs="CO132-2">
<para>The <literal>mode</literal> attribute specifies the bonding mode. The bonding modes supported are:</para>
<itemizedlist>
<listitem>
<simpara><literal>balance-rr</literal> - 0</simpara>
</listitem>
<listitem>
<simpara><literal>active-backup</literal> - 1</simpara>
</listitem>
<listitem>
<simpara><literal>balance-xor</literal> - 2</simpara>
<simpara>For <literal>balance-rr</literal> or <literal>balance-xor</literal> modes, you must set the <literal>trust</literal> mode to <literal>on</literal> for the SR-IOV virtual function.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO132-3">
<para>The <literal>failover</literal> attribute is mandatory for active-backup mode.</para>
</callout>
<callout arearefs="CO132-4">
<para>The <literal>linksInContainer=true</literal> flag informs the Bond CNI that the required interfaces are to be found inside the container. By default, Bond CNI looks for these interfaces on the host which does not work for integration with SRIOV and Multus.</para>
</callout>
<callout arearefs="CO132-5">
<para>The <literal>links</literal> section defines which interfaces will be used to create the bond. By default, Multus names the attached interfaces as: "net", plus a consecutive number, starting with one.</para>
</callout>
<callout arearefs="CO132-6">
<para>A configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition. In this pod example IP addresses are configured manually, so in this case,<literal>ipam</literal> is set to static.</para>
</callout>
<callout arearefs="CO132-7">
<para>Add additional capabilities to the device. For example, set the <literal>type</literal> field to <literal>tuning</literal>. Specify the interface-level network <literal>sysctl</literal> you want to set in the sysctl field. This example sets all interface-level network <literal>sysctl</literal> settings that can be set.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the bond network attachment resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-bond-network-interface.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verifying that the <literal>NetworkAttachmentDefinition</literal> CR is successfully created</title>
<listitem>
<simpara>Confirm that the SR-IOV Network Operator created the <literal>NetworkAttachmentDefinition</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -n &lt;namespace&gt; <co xml:id="CO133-1"/></programlisting>
<calloutlist>
<callout arearefs="CO133-1">
<para>Replace <literal>&lt;namespace&gt;</literal> with the networkNamespace that you specified when configuring the network attachment, for example, <literal>sysctl-tuning-test</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                          AGE
bond-sysctl-network           22m
allvalidflags                 47m</programlisting>
</para>
</formalpara>
<note>
<simpara>There might be a delay before the SR-IOV Network Operator creates the CR.</simpara>
</note>
</listitem>
</itemizedlist>
<formalpara>
<title>Verifying that the additional SR-IOV network resource is successful</title>
<para>To verify that the tuning CNI is correctly configured and the additional SR-IOV network attachment is attached, do the following:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a <literal>Pod</literal> CR. For example, save the following YAML as the file <literal>examplepod.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: tunepod
  namespace: sysctl-tuning-test
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {"name": "allvalidflags"}, <co xml:id="CO134-1"/>
        {"name": "allvalidflags"},
        {
          "name": "bond-sysctl-network",
          "interface": "bond0",
          "mac": "0a:56:0a:83:04:0c", <co xml:id="CO134-2"/>
          "ips": ["10.100.100.200/24"] <co xml:id="CO134-3"/>
       }
      ]
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault</programlisting>
<calloutlist>
<callout arearefs="CO134-1">
<para>The name of the SR-IOV network attachment definition CR.</para>
</callout>
<callout arearefs="CO134-2">
<para>Optional: The MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <literal>{ "mac": true }</literal> in the SriovNetwork object.</para>
</callout>
<callout arearefs="CO134-3">
<para>Optional: IP addresses for the SR-IOV device that are allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <literal>{ "ips": true }</literal> in the <literal>SriovNetwork</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f examplepod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n sysctl-tuning-test</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      READY   STATUS    RESTARTS   AGE
tunepod   1/1     Running   0          47s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Log in to the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n sysctl-tuning-test tunepod</programlisting>
</listitem>
<listitem>
<simpara>Verify the values of the configured <literal>sysctl</literal> flag. Find the value  <literal>net.ipv6.neigh.IFNAME.base_reachable_time_ms</literal> by running the following command::</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sysctl net.ipv6.neigh.bond0.base_reachable_time_ms</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">net.ipv6.neigh.bond0.base_reachable_time_ms = 20000</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-about-all-multi-cast-mode_configuring-sysctl-interface-sriov-device">
<title>About all-multicast mode</title>
<simpara>Enabling all-multicast mode, particularly in the context of rootless applications, is critical. If you do not enable this mode, you would be required to grant the <literal>NET_ADMIN</literal> capability to the pod&#8217;s Security Context Constraints (SCC). If you were to allow the <literal>NET_ADMIN</literal> capability to grant the pod privileges to make changes that extend beyond its specific requirements, you could potentially expose security vulnerabilities.</simpara>
<simpara>The tuning CNI plugin supports changing several interface attributes, including all-multicast mode. By enabling this mode, you can allow applications running on Virtual Functions (VFs) that are configured on a SR-IOV network device to receive multicast traffic from applications on other VFs, whether attached to the same or different physical functions.</simpara>
<section xml:id="enabling-all-multicast-sriov-network_configuring-sysctl-interface-sriov-device">
<title>Enabling the all-multicast mode on an SR-IOV network</title>
<simpara>You can enable the all-multicast mode on an SR-IOV interface by:</simpara>
<itemizedlist>
<listitem>
<simpara>Adding the tuning configuration to the <literal>metaPlugins</literal> parameter of the <literal>SriovNetwork</literal> resource</simpara>
</listitem>
<listitem>
<simpara>Setting the <literal>allmulti</literal> field to <literal>true</literal> in the tuning configuration</simpara>
<note>
<simpara>Ensure that you create the virtual function (VF) with trust enabled.</simpara>
</note>
</listitem>
</itemizedlist>
<simpara>The SR-IOV Network Operator manages additional network definitions. When you specify an additional SR-IOV network to create, the SR-IOV Network Operator creates the <literal>NetworkAttachmentDefinition</literal> custom resource (CR) automatically.</simpara>
<note>
<simpara>Do not edit <literal>NetworkAttachmentDefinition</literal> custom resources that the SR-IOV Network Operator manages. Doing so might disrupt network traffic on your additional network.</simpara>
</note>
<simpara>Enable the all-multicast mode on a SR-IOV network by following this guidance.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift Container Platform CLI (oc).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the OpenShift Container Platform cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have configured an appropriate <literal>SriovNetworkNodePolicy</literal> object.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file with the following settings that defines a <literal>SriovNetworkNodePolicy</literal> object for a Mellanox ConnectX-5 device. Save the YAML file as <literal>sriovnetpolicy-mlx.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriovnetpolicy-mlx
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  nicSelector:
    deviceID: "1017"
    pfNames:
      - ens8f0np0#0-9
    rootDevices:
      - 0000:d8:00.0
    vendor: "15b3"
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 10
  priority: 99
  resourceName: resourcemlx</programlisting>
</listitem>
<listitem>
<simpara>Optional: If the SR-IOV capable cluster nodes are not already labeled, add the <literal>SriovNetworkNodePolicy.Spec.NodeSelector</literal> label. For more information about labeling nodes, see "Understanding how to update labels on nodes".</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriovnetpolicy-mlx.yaml</programlisting>
<simpara>After applying the configuration update, all the pods in the <literal>sriov-network-operator</literal> namespace automatically move to a <literal>Running</literal> status.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>enable-allmulti-test</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace enable-allmulti-test</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> custom resource (CR) for the additional SR-IOV network attachment and insert the <literal>metaPlugins</literal> configuration, as in the following example CR YAML, and save the file as <literal>sriov-enable-all-multicast.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: enableallmulti <co xml:id="CO135-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO135-2"/>
spec:
  resourceName: enableallmulti <co xml:id="CO135-3"/>
  networkNamespace: enable-allmulti-test <co xml:id="CO135-4"/>
  ipam: '{ "type": "static" }' <co xml:id="CO135-5"/>
  capabilities: '{ "mac": true, "ips": true }' <co xml:id="CO135-6"/>
  trust: "on" <co xml:id="CO135-7"/>
  metaPlugins : | <co xml:id="CO135-8"/>
    {
      "type": "tuning",
      "capabilities":{
        "mac":true
      },
      "allmulti": true
      }
    }</programlisting>
<calloutlist>
<callout arearefs="CO135-1">
<para>Specify a name for the object. The SR-IOV Network Operator creates a <literal>NetworkAttachmentDefinition</literal> object with the same name.</para>
</callout>
<callout arearefs="CO135-2">
<para>Specify the namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO135-3">
<para>Specify a value for the <literal>spec.resourceName</literal> parameter from the <literal>SriovNetworkNodePolicy</literal> object that defines the SR-IOV hardware for this additional network.</para>
</callout>
<callout arearefs="CO135-4">
<para>Specify the target namespace for the <literal>SriovNetwork</literal> object. Only pods in the target namespace can attach to the additional network.</para>
</callout>
<callout arearefs="CO135-5">
<para>Specify a configuration object for the IPAM CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
<callout arearefs="CO135-6">
<para>Optional: Set capabilities for the additional network. You can specify <literal>"{ "ips": true }"</literal> to enable IP address support or <literal>"{ "mac": true }"</literal> to enable MAC address support.</para>
</callout>
<callout arearefs="CO135-7">
<para>Specify the trust mode of the virtual function. This must be set to "on".</para>
</callout>
<callout arearefs="CO135-8">
<para>Add more capabilities to the device by using the <literal>metaPlugins</literal> parameter. In this use case, set the <literal>type</literal> field to <literal>tuning</literal>, and add the <literal>allmulti</literal> field and set it to <literal>true</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-enable-all-multicast.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification of the <literal>NetworkAttachmentDefinition</literal> CR</title>
<listitem>
<simpara>Confirm that the SR-IOV Network Operator created the <literal>NetworkAttachmentDefinition</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -n &lt;namespace&gt; <co xml:id="CO136-1"/></programlisting>
<calloutlist>
<callout arearefs="CO136-1">
<para>Replace <literal>&lt;namespace&gt;</literal> with the value for <literal>networkNamespace</literal> that you specified in the <literal>SriovNetwork</literal> object. For this example, that is <literal>enable-allmulti-test</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                  AGE
enableallmulti                        14m</programlisting>
</para>
</formalpara>
<note>
<simpara>There might be a delay before the SR-IOV Network Operator creates the CR.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Display information about the SR-IOV network resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetwork -n openshift-sriov-network-operator</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Verification of the additional SR-IOV network attachment</title>
<para>To verify that the tuning CNI is correctly configured and that the additional SR-IOV network attachment is attached, follow these steps:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a <literal>Pod</literal> CR. Save the following sample YAML in a file named <literal>examplepod.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: samplepod
  namespace: enable-allmulti-test
  annotations:
    k8s.v1.cni.cncf.io/networks: |-
      [
        {
          "name": "enableallmulti",  <co xml:id="CO137-1"/>
          "mac": "0a:56:0a:83:04:0c", <co xml:id="CO137-2"/>
          "ips": ["10.100.100.200/24"] <co xml:id="CO137-3"/>
       }
      ]
spec:
  containers:
  - name: podexample
    image: centos
    command: ["/bin/bash", "-c", "sleep INF"]
    securityContext:
      runAsUser: 2000
      runAsGroup: 3000
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault</programlisting>
<calloutlist>
<callout arearefs="CO137-1">
<para>Specify the name of the SR-IOV network attachment definition CR.</para>
</callout>
<callout arearefs="CO137-2">
<para>Optional: Specify the MAC address for the SR-IOV device that is allocated from the resource type defined in the SR-IOV network attachment definition CR. To use this feature, you also must specify <literal>{"mac": true}</literal> in the SriovNetwork object.</para>
</callout>
<callout arearefs="CO137-3">
<para>Optional: Specify the IP addresses for the SR-IOV device that are allocated from the resource type defined in the SR-IOV network attachment definition CR. Both IPv4 and IPv6 addresses are supported. To use this feature, you also must specify <literal>{ "ips": true }</literal> in the <literal>SriovNetwork</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>Pod</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f examplepod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n enable-allmulti-test</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME       READY   STATUS    RESTARTS   AGE
samplepod  1/1     Running   0          47s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Log in to the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n enable-allmulti-test samplepod</programlisting>
</listitem>
<listitem>
<simpara>List all the interfaces associated with the pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# ip link</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
2: eth0@if22: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 8901 qdisc noqueue state UP mode DEFAULT group default
    link/ether 0a:58:0a:83:00:10 brd ff:ff:ff:ff:ff:ff link-netnsid 0 <co xml:id="CO138-1"/>
3: net1@if24: &lt;BROADCAST,MULTICAST,ALLMULTI,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default
    link/ether ee:9b:66:a4:ec:1d brd ff:ff:ff:ff:ff:ff link-netnsid 0 <co xml:id="CO138-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO138-1">
<para><literal>eth0@if22</literal> is the primary interface</para>
</callout>
<callout arearefs="CO138-2">
<para><literal>net1@if24</literal> is the secondary interface configured with the network-attachment-definition that supports the all-multicast mode (<literal>ALLMULTI</literal> flag)</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="using-sriov-multicast">
<title>Using high performance multicast</title>

<simpara>You can use multicast on your Single Root I/O Virtualization (SR-IOV) hardware network.</simpara>
<section xml:id="nw-high-performance-multicast_using-sriov-multicast">
<title>High performance multicast</title>
<simpara>The OpenShift SDN network plugin supports multicast between pods on the default network. This is best used for low-bandwidth coordination or service discovery, and not high-bandwidth applications.
For applications such as streaming media, like Internet Protocol television (IPTV) and multipoint videoconferencing, you can utilize Single Root I/O Virtualization (SR-IOV) hardware to provide near-native performance.</simpara>
<simpara>When using additional SR-IOV interfaces for multicast:</simpara>
<itemizedlist>
<listitem>
<simpara>Multicast packages must be sent or received by a pod through the additional SR-IOV interface.</simpara>
</listitem>
<listitem>
<simpara>The physical network which connects the SR-IOV interfaces decides the
multicast routing and topology, which is not controlled by OpenShift Container Platform.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-using-an-sriov-interface-for-multicast_using-sriov-multicast">
<title>Configuring an SR-IOV interface for multicast</title>
<simpara>The follow procedure creates an example SR-IOV interface for multicast.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>SriovNetworkNodePolicy</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-example
  namespace: openshift-sriov-network-operator
spec:
  resourceName: example
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 4
  nicSelector:
    vendor: "8086"
    pfNames: ['ens803f0']
    rootDevices: ['0000:86:00.0']</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>SriovNetwork</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: net-example
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: default
  ipam: | <co xml:id="CO139-1"/>
    {
      "type": "host-local", <co xml:id="CO139-2"/>
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "routes": [
        {"dst": "224.0.0.0/5"},
        {"dst": "232.0.0.0/5"}
      ],
      "gateway": "10.56.217.1"
    }
  resourceName: example</programlisting>
<calloutlist>
<callout arearefs="CO139-1 CO139-2">
<para>If you choose to configure DHCP as IPAM, ensure that you provision the following default routes through your DHCP server: <literal>224.0.0.0/5</literal> and <literal>232.0.0.0/5</literal>. This is to override the static multicast route set by the default network provider.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a pod with multicast application:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: testpmd
  namespace: default
  annotations:
    k8s.v1.cni.cncf.io/networks: nic1
spec:
  containers:
  - name: example
    image: rhel7:latest
    securityContext:
      capabilities:
        add: ["NET_ADMIN"] <co xml:id="CO140-1"/>
    command: [ "sleep", "infinity"]</programlisting>
<calloutlist>
<callout arearefs="CO140-1">
<para>The <literal>NET_ADMIN</literal> capability is required only if your application needs to
assign the multicast IP address to the SR-IOV interface. Otherwise, it can be
omitted.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="using-dpdk-and-rdma">
<title>Using DPDK and RDMA</title>

<simpara>The containerized Data Plane Development Kit (DPDK) application is supported on OpenShift Container Platform. You can use Single Root I/O Virtualization (SR-IOV) network hardware with the Data Plane Development Kit (DPDK) and with remote direct memory access (RDMA).</simpara>
<simpara>For information about supported devices, see <link linkend="supported-devices_about-sriov">Supported devices</link>.</simpara>
<section xml:id="example-vf-use-in-dpdk-mode-intel_using-dpdk-and-rdma">
<title>Using a virtual function in DPDK mode with an Intel NIC</title>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Install the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>SriovNetworkNodePolicy</literal> object, and then save the YAML in the <literal>intel-dpdk-node-policy.yaml</literal> file.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: intel-dpdk-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: intelnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: &lt;priority&gt;
  numVfs: &lt;num&gt;
  nicSelector:
    vendor: "8086"
    deviceID: "158b"
    pfNames: ["&lt;pf_name&gt;", ...]
    rootDevices: ["&lt;pci_bus_id&gt;", "..."]
  deviceType: vfio-pci <co xml:id="CO141-1"/></programlisting>
<calloutlist>
<callout arearefs="CO141-1">
<para>Specify the driver type for the virtual functions to <literal>vfio-pci</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>See the <literal>Configuring SR-IOV network devices</literal> section for a detailed explanation on each option in <literal>SriovNetworkNodePolicy</literal>.</simpara>
<simpara>When applying the configuration specified in a <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes.
It may take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.</simpara>
<simpara>After the configuration update is applied, all the pods in <literal>openshift-sriov-network-operator</literal> namespace will change to a <literal>Running</literal> status.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f intel-dpdk-node-policy.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create the following <literal>SriovNetwork</literal> object, and then save the YAML in the <literal>intel-dpdk-network.yaml</literal> file.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: intel-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: &lt;target_namespace&gt;
  ipam: |-
# ... <co xml:id="CO142-1"/>
  vlan: &lt;vlan&gt;
  resourceName: intelnics</programlisting>
<calloutlist>
<callout arearefs="CO142-1">
<para>Specify a configuration object for the ipam CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
</calloutlist>
<note>
<simpara>See the "Configuring SR-IOV additional network" section for a detailed explanation on each option in <literal>SriovNetwork</literal>.</simpara>
</note>
<simpara>An optional library, app-netutil, provides several API methods for gathering network information about a container&#8217;s parent pod.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f intel-dpdk-network.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create the following <literal>Pod</literal> spec, and then save the YAML in the <literal>intel-dpdk-pod.yaml</literal> file.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: &lt;target_namespace&gt; <co xml:id="CO143-1"/>
  annotations:
    k8s.v1.cni.cncf.io/networks: intel-dpdk-network
spec:
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt; <co xml:id="CO143-2"/>
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"] <co xml:id="CO143-3"/>
    volumeMounts:
    - mountPath: /mnt/huge <co xml:id="CO143-4"/>
      name: hugepage
    resources:
      limits:
        openshift.io/intelnics: "1" <co xml:id="CO143-5"/>
        memory: "1Gi"
        cpu: "4" <co xml:id="CO143-6"/>
        hugepages-1Gi: "4Gi" <co xml:id="CO143-7"/>
      requests:
        openshift.io/intelnics: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
<calloutlist>
<callout arearefs="CO143-1">
<para>Specify the same <literal>target_namespace</literal> where the <literal>SriovNetwork</literal> object <literal>intel-dpdk-network</literal> is created. If you would like to create the pod in a different namespace, change <literal>target_namespace</literal> in both the <literal>Pod</literal> spec and the <literal>SriovNetwork</literal> object.</para>
</callout>
<callout arearefs="CO143-2">
<para>Specify the DPDK image which includes your application and the DPDK library used by application.</para>
</callout>
<callout arearefs="CO143-3">
<para>Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access.</para>
</callout>
<callout arearefs="CO143-4">
<para>Mount a hugepage volume to the DPDK pod under <literal>/mnt/huge</literal>. The hugepage volume is backed by the emptyDir volume type with the medium being <literal>Hugepages</literal>.</para>
</callout>
<callout arearefs="CO143-5">
<para>Optional: Specify the number of DPDK devices allocated to DPDK pod. This resource request and limit, if not explicitly specified, will be automatically added by the SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by the SR-IOV Operator. It is enabled by default and can be disabled by setting <literal>enableInjector</literal> option to <literal>false</literal> in the default <literal>SriovOperatorConfig</literal> CR.</para>
</callout>
<callout arearefs="CO143-6">
<para>Specify the number of CPUs. The DPDK pod usually requires exclusive CPUs to be allocated from the kubelet. This is achieved by setting CPU Manager policy to <literal>static</literal> and creating a pod with <literal>Guaranteed</literal> QoS.</para>
</callout>
<callout arearefs="CO143-7">
<para>Specify hugepage size <literal>hugepages-1Gi</literal> or <literal>hugepages-2Mi</literal> and the quantity of hugepages that will be allocated to the DPDK pod. Configure <literal>2Mi</literal> and <literal>1Gi</literal> hugepages separately. Configuring <literal>1Gi</literal> hugepage requires adding kernel arguments to Nodes. For example, adding kernel arguments <literal>default_hugepagesz=1GB</literal>, <literal>hugepagesz=1G</literal> and <literal>hugepages=16</literal> will result in <literal>16*1Gi</literal> hugepages be allocated during system boot.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the DPDK pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f intel-dpdk-pod.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="example-vf-use-in-dpdk-mode-mellanox_using-dpdk-and-rdma">
<title>Using a virtual function in DPDK mode with a Mellanox NIC</title>
<simpara>You can create a network node policy and create a Data Plane Development Kit (DPDK) pod using a virtual function in DPDK mode with a Mellanox NIC.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed the Single Root I/O Virtualization (SR-IOV) Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Save the following <literal>SriovNetworkNodePolicy</literal> YAML configuration to an <literal>mlx-dpdk-node-policy.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-dpdk-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: &lt;priority&gt;
  numVfs: &lt;num&gt;
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <co xml:id="CO144-1"/>
    pfNames: ["&lt;pf_name&gt;", ...]
    rootDevices: ["&lt;pci_bus_id&gt;", "..."]
  deviceType: netdevice <co xml:id="CO144-2"/>
  isRdma: true <co xml:id="CO144-3"/></programlisting>
<calloutlist>
<callout arearefs="CO144-1">
<para>Specify the device hex code of the SR-IOV network device.</para>
</callout>
<callout arearefs="CO144-2">
<para>Specify the driver type for the virtual functions to <literal>netdevice</literal>. A Mellanox SR-IOV Virtual Function (VF) can work in DPDK mode without using the <literal>vfio-pci</literal> device type. The VF device appears as a kernel network interface inside a container.</para>
</callout>
<callout arearefs="CO144-3">
<para>Enable Remote Direct Memory Access (RDMA) mode. This is required for Mellanox cards to work in DPDK mode.</para>
</callout>
</calloutlist>
<note>
<simpara>See <emphasis>Configuring an SR-IOV network device</emphasis> for a detailed explanation of each option in the <literal>SriovNetworkNodePolicy</literal> object.</simpara>
<simpara>When applying the configuration specified in an <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.
It might take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.</simpara>
<simpara>After the configuration update is applied, all the pods in the <literal>openshift-sriov-network-operator</literal> namespace will change to a <literal>Running</literal> status.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-dpdk-node-policy.yaml</programlisting>
</listitem>
<listitem>
<simpara>Save the following <literal>SriovNetwork</literal> YAML configuration to an <literal>mlx-dpdk-network.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-dpdk-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: &lt;target_namespace&gt;
  ipam: |- <co xml:id="CO145-1"/>
...
  vlan: &lt;vlan&gt;
  resourceName: mlxnics</programlisting>
<calloutlist>
<callout arearefs="CO145-1">
<para>Specify a configuration object for the IP Address Management (IPAM) Container Network Interface (CNI) plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
</calloutlist>
<note>
<simpara>See <emphasis>Configuring an SR-IOV network device</emphasis> for a detailed explanation on each option in the <literal>SriovNetwork</literal> object.</simpara>
</note>
<simpara>The <literal>app-netutil</literal> option library provides several API methods for gathering network information about the parent pod of a container.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-dpdk-network.yaml</programlisting>
</listitem>
<listitem>
<simpara>Save the following <literal>Pod</literal> YAML configuration to an <literal>mlx-dpdk-pod.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: &lt;target_namespace&gt; <co xml:id="CO146-1"/>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-dpdk-network
spec:
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt; <co xml:id="CO146-2"/>
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"] <co xml:id="CO146-3"/>
    volumeMounts:
    - mountPath: /mnt/huge <co xml:id="CO146-4"/>
      name: hugepage
    resources:
      limits:
        openshift.io/mlxnics: "1" <co xml:id="CO146-5"/>
        memory: "1Gi"
        cpu: "4" <co xml:id="CO146-6"/>
        hugepages-1Gi: "4Gi" <co xml:id="CO146-7"/>
      requests:
        openshift.io/mlxnics: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
<calloutlist>
<callout arearefs="CO146-1">
<para>Specify the same <literal>target_namespace</literal> where <literal>SriovNetwork</literal> object <literal>mlx-dpdk-network</literal> is created. To create the pod in a different namespace, change <literal>target_namespace</literal> in both the <literal>Pod</literal> spec and <literal>SriovNetwork</literal> object.</para>
</callout>
<callout arearefs="CO146-2">
<para>Specify the DPDK image which includes your application and the DPDK library used by the application.</para>
</callout>
<callout arearefs="CO146-3">
<para>Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access.</para>
</callout>
<callout arearefs="CO146-4">
<para>Mount the hugepage volume to the DPDK pod under <literal>/mnt/huge</literal>. The hugepage volume is backed by the <literal>emptyDir</literal> volume type with the medium being <literal>Hugepages</literal>.</para>
</callout>
<callout arearefs="CO146-5">
<para>Optional: Specify the number of DPDK devices allocated for the DPDK pod. If not explicitly specified, this resource request and limit is automatically added by the SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by SR-IOV Operator. It is enabled by default and can be disabled by setting the <literal>enableInjector</literal> option to <literal>false</literal> in the default <literal>SriovOperatorConfig</literal> CR.</para>
</callout>
<callout arearefs="CO146-6">
<para>Specify the number of CPUs. The DPDK pod usually requires that exclusive CPUs be allocated from the kubelet. To do this, set the CPU Manager policy to <literal>static</literal> and create a pod with <literal>Guaranteed</literal> Quality of Service (QoS).</para>
</callout>
<callout arearefs="CO146-7">
<para>Specify hugepage size <literal>hugepages-1Gi</literal> or <literal>hugepages-2Mi</literal> and the quantity of hugepages that will be allocated to the DPDK pod. Configure <literal>2Mi</literal> and <literal>1Gi</literal> hugepages separately. Configuring <literal>1Gi</literal> hugepages requires adding kernel arguments to Nodes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the DPDK pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-dpdk-pod.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-running-dpdk-rootless-tap_using-dpdk-and-rdma">
<title>Using the TAP CNI to run a rootless DPDK workload with kernel access</title>
<simpara>DPDK applications can use <literal>virtio-user</literal> as an exception path to inject certain types of packets, such as log messages, into the kernel for processing. For more information about this feature, see <link xlink:href="https://doc.dpdk.org/guides/howto/virtio_user_as_exception_path.html">Virtio_user as Exception Path</link>.</simpara>
<simpara>In OpenShift Container Platform version 4.14 and later, you can use non-privileged pods to run DPDK applications alongside the tap CNI plugin. To enable this functionality, you need to mount the <literal>vhost-net</literal> device by setting the <literal>needVhostNet</literal> parameter to <literal>true</literal> within the <literal>SriovNetworkNodePolicy</literal> object.</simpara>
<figure>
<title>DPDK and TAP example configuration</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/348_OpenShift_rootless_DPDK_0923.png"/>
</imageobject>
<textobject><phrase>DPDK and TAP plugin</phrase></textobject>
</mediaobject>
</figure>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Ensure that <literal>setsebools container_use_devices=on</literal> is set as root on all nodes.</simpara>
<note>
<simpara>Use the Machine Config Operator to set this SELinux boolean.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>test-namespace.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: test-namespace
  labels:
    pod-security.kubernetes.io/enforce: privileged
    pod-security.kubernetes.io/audit: privileged
    pod-security.kubernetes.io/warn: privileged
    security.openshift.io/scc.podSecurityLabelSync: "false"</programlisting>
</listitem>
<listitem>
<simpara>Create the new <literal>Namespace</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f test-namespace.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a file, such as <literal>sriov-node-network-policy.yaml</literal>, with content like the following example::</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
 name: sriovnic
 namespace: openshift-sriov-network-operator
spec:
 deviceType: netdevice <co xml:id="CO147-1"/>
 isRdma: true <co xml:id="CO147-2"/>
 needVhostNet: true <co xml:id="CO147-3"/>
 nicSelector:
   vendor: "15b3" <co xml:id="CO147-4"/>
   deviceID: "101b" <co xml:id="CO147-5"/>
   rootDevices: ["00:05.0"]
 numVfs: 10
 priority: 99
 resourceName: sriovnic
 nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"</programlisting>
<calloutlist>
<callout arearefs="CO147-1">
<para>This indicates that the profile is tailored specifically for Mellanox Network Interface Controllers (NICs).</para>
</callout>
<callout arearefs="CO147-2">
<para>Setting <literal>isRdma</literal> to <literal>true</literal> is only required for a Mellanox NIC.</para>
</callout>
<callout arearefs="CO147-3">
<para>This mounts the <literal>/dev/net/tun</literal> and <literal>/dev/vhost-net</literal> devices into the container so the application can create a tap device and connect the tap device to the DPDK workload.</para>
</callout>
<callout arearefs="CO147-4">
<para>The vendor hexadecimal code of the SR-IOV network device. The value 15b3 is associated with a Mellanox NIC.</para>
</callout>
<callout arearefs="CO147-5">
<para>The device hexadecimal code of the SR-IOV network device.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-node-network-policy.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create the following <literal>SriovNetwork</literal> object, and then save the YAML in the <literal>sriov-network-attachment.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
 name: sriov-network
 namespace: openshift-sriov-network-operator
spec:
 networkNamespace: test-namespace
 resourceName: sriovnic
 spoofChk: "off"
 trust: "on"</programlisting>
<note>
<simpara>See the "Configuring SR-IOV additional network" section for a detailed explanation on each option in <literal>SriovNetwork</literal>.</simpara>
</note>
<simpara>An optional library, <literal>app-netutil</literal>, provides several API methods for gathering network information about a container&#8217;s parent pod.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetwork</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-network-attachment.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a file, such as <literal>tap-example.yaml</literal>, that defines a network attachment definition, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
 name: tap-one
 namespace: test-namespace <co xml:id="CO148-1"/>
spec:
 config: '{
   "cniVersion": "0.4.0",
   "name": "tap",
   "plugins": [
     {
        "type": "tap",
        "multiQueue": true,
        "selinuxcontext": "system_u:system_r:container_t:s0"
     },
     {
       "type":"tuning",
       "capabilities":{
         "mac":true
       }
     }
   ]
 }'</programlisting>
<calloutlist>
<callout arearefs="CO148-1">
<para>Specify the same <literal>target_namespace</literal> where the <literal>SriovNetwork</literal> object is created.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>NetworkAttachmentDefinition</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f tap-example.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a file, such as <literal>dpdk-pod-rootless.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-app
  namespace: test-namespace <co xml:id="CO149-1"/>
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      {"name": "sriov-network", "namespace": "test-namespace"},
      {"name": "tap-one", "interface": "ext0", "namespace": "test-namespace"}]'
spec:
  nodeSelector:
    kubernetes.io/hostname: "worker-0"
  securityContext:
      fsGroup: 1001 <co xml:id="CO149-2"/>
      runAsGroup: 1001 <co xml:id="CO149-3"/>
      seccompProfile:
        type: RuntimeDefault
  containers:
  - name: testpmd
    image: &lt;DPDK_image&gt; <co xml:id="CO149-4"/>
    securityContext:
      capabilities:
        drop: ["ALL"] <co xml:id="CO149-5"/>
        add: <co xml:id="CO149-6"/>
          - IPC_LOCK
          - NET_RAW #for mlx only <co xml:id="CO149-7"/>
      runAsUser: 1001 <co xml:id="CO149-8"/>
      privileged: false <co xml:id="CO149-9"/>
      allowPrivilegeEscalation: true <co xml:id="CO149-10"/>
      runAsNonRoot: true <co xml:id="CO149-11"/>
    volumeMounts:
    - mountPath: /mnt/huge <co xml:id="CO149-12"/>
      name: hugepages
    resources:
      limits:
        openshift.io/sriovnic: "1" <co xml:id="CO149-13"/>
        memory: "1Gi"
        cpu: "4" <co xml:id="CO149-14"/>
        hugepages-1Gi: "4Gi" <co xml:id="CO149-15"/>
      requests:
        openshift.io/sriovnic: "1"
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  runtimeClassName: performance-cnf-performanceprofile <co xml:id="CO149-16"/>
  volumes:
  - name: hugepages
    emptyDir:
      medium: HugePages</programlisting>
<calloutlist>
<callout arearefs="CO149-1">
<para>Specify the same <literal>target_namespace</literal> in which the <literal>SriovNetwork</literal> object is created. If you want to create the pod in a different namespace, change <literal>target_namespace</literal> in both the <literal>Pod</literal> spec and the <literal>SriovNetwork</literal> object.</para>
</callout>
<callout arearefs="CO149-2">
<para>Sets the group ownership of volume-mounted directories and files created in those volumes.</para>
</callout>
<callout arearefs="CO149-3">
<para>Specify the primary group ID used for running the container.</para>
</callout>
<callout arearefs="CO149-4">
<para>Specify the DPDK image that contains your application and the DPDK library used by application.</para>
</callout>
<callout arearefs="CO149-5">
<para>Removing all capabilities (<literal>ALL</literal>) from the container&#8217;s securityContext means that the container has no special privileges beyond what is necessary for normal operation.</para>
</callout>
<callout arearefs="CO149-6">
<para>Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access. These capabilities must also be set in the binary file by using the <literal>setcap</literal> command.</para>
</callout>
<callout arearefs="CO149-7">
<para>Mellanox network interface controller (NIC) requires the <literal>NET_RAW</literal> capability.</para>
</callout>
<callout arearefs="CO149-8">
<para>Specify the user ID used for running the container.</para>
</callout>
<callout arearefs="CO149-9">
<para>This setting indicates that the container or containers within the pod should not be granted privileged access to the host system.</para>
</callout>
<callout arearefs="CO149-10">
<para>This setting allows a container to escalate its privileges beyond the initial non-root privileges it might have been assigned.</para>
</callout>
<callout arearefs="CO149-11">
<para>This setting ensures that the container runs with a non-root user. This helps enforce the principle of least privilege, limiting the potential impact of compromising the container and reducing the attack surface.</para>
</callout>
<callout arearefs="CO149-12">
<para>Mount a hugepage volume to the DPDK pod under <literal>/mnt/huge</literal>. The hugepage volume is backed by the emptyDir volume type with the medium being <literal>Hugepages</literal>.</para>
</callout>
<callout arearefs="CO149-13">
<para>Optional: Specify the number of DPDK devices allocated for the DPDK pod. If not explicitly specified, this resource request and limit is automatically added by the SR-IOV network resource injector. The SR-IOV network resource injector is an admission controller component managed by SR-IOV Operator. It is enabled by default and can be disabled by setting the <literal>enableInjector</literal> option to <literal>false</literal> in the default <literal>SriovOperatorConfig</literal> CR.</para>
</callout>
<callout arearefs="CO149-14">
<para>Specify the number of CPUs. The DPDK pod usually requires exclusive CPUs to be allocated from the kubelet. This is achieved by setting CPU Manager policy to <literal>static</literal> and creating a pod with <literal>Guaranteed</literal> QoS.</para>
</callout>
<callout arearefs="CO149-15">
<para>Specify hugepage size <literal>hugepages-1Gi</literal> or <literal>hugepages-2Mi</literal> and the quantity of hugepages that will be allocated to the DPDK pod. Configure <literal>2Mi</literal> and <literal>1Gi</literal> hugepages separately. Configuring <literal>1Gi</literal> hugepage requires adding kernel arguments to Nodes. For example, adding kernel arguments <literal>default_hugepagesz=1GB</literal>, <literal>hugepagesz=1G</literal> and <literal>hugepages=16</literal> will result in <literal>16*1Gi</literal> hugepages be allocated during system boot.</para>
</callout>
<callout arearefs="CO149-16">
<para>If your performance profile is not named <literal>cnf-performance profile</literal>, replace that string with the correct performance profile name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the DPDK pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f dpdk-pod-rootless.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-multus-enable-container_use_devices_configuring-additional-network">Enabling the container_use_devices boolean</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#cnf-create-performance-profiles">Creating a performance profile</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-sriov-example-dpdk-line-rate_using-dpdk-and-rdma">
<title>Overview of achieving a specific DPDK line rate</title>
<simpara>To achieve a specific Data Plane Development Kit (DPDK) line rate, deploy a Node Tuning Operator and configure Single Root I/O Virtualization (SR-IOV). You must also tune the DPDK settings for the following resources:</simpara>
<itemizedlist>
<listitem>
<simpara>Isolated CPUs</simpara>
</listitem>
<listitem>
<simpara>Hugepages</simpara>
</listitem>
<listitem>
<simpara>The topology scheduler</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>In previous versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.</simpara>
</note>
<formalpara>
<title>DPDK test environment</title>
<para>The following diagram shows the components of a traffic-testing environment:</para>
</formalpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/261_OpenShift_DPDK_0722.png"/>
</imageobject>
<textobject><phrase>DPDK test environment</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Traffic generator</emphasis>: An application that can generate high-volume packet traffic.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">SR-IOV-supporting NIC</emphasis>: A network interface card compatible with SR-IOV. The card runs a number of virtual functions on a physical interface.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Physical Function (PF)</emphasis>: A PCI Express (PCIe) function of a network adapter that supports the SR-IOV interface.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Virtual Function (VF)</emphasis>:  A lightweight PCIe function on a network adapter that supports SR-IOV. The VF is associated with the PCIe PF on the network adapter. The VF represents a virtualized instance of the network adapter.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Switch</emphasis>: A network switch. Nodes can also be connected back-to-back.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong"><literal>testpmd</literal></emphasis>: An example application included with DPDK. The <literal>testpmd</literal> application can be used to test the DPDK in a packet-forwarding mode. The <literal>testpmd</literal> application is also an example of how to build a fully-fledged application using the DPDK Software Development Kit (SDK).</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">worker 0</emphasis> and <emphasis role="strong">worker 1</emphasis>: OpenShift Container Platform nodes.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-example-dpdk-line-rate_using-dpdk-and-rdma">
<title>Using SR-IOV and the Node Tuning Operator to achieve a DPDK line rate</title>
<simpara>You can use the Node Tuning Operator to configure isolated CPUs, hugepages, and a topology scheduler.
You can then use the Node Tuning Operator with Single Root I/O Virtualization (SR-IOV) to achieve a specific Data Plane Development Kit (DPDK) line rate.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have deployed a standalone Node Tuning Operator.</simpara>
<note>
<simpara>In previous versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>PerformanceProfile</literal> object based on the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: performance
spec:
  globallyDisableIrqLoadBalancing: true
  cpu:
    isolated: 21-51,73-103 <co xml:id="CO150-1"/>
    reserved: 0-20,52-72 <co xml:id="CO150-2"/>
  hugepages:
    defaultHugepagesSize: 1G <co xml:id="CO150-3"/>
    pages:
      - count: 32
        size: 1G
  net:
    userLevelNetworking: true
  numa:
    topologyPolicy: "single-numa-node"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""</programlisting>
<calloutlist>
<callout arearefs="CO150-1">
<para>If hyperthreading is enabled on the system, allocate the relevant symbolic links to the <literal>isolated</literal> and <literal>reserved</literal> CPU groups. If the system contains multiple non-uniform memory access nodes (NUMAs), allocate CPUs from both NUMAs to both groups. You can also use the Performance Profile Creator for this task. For more information, see <emphasis>Creating a performance profile</emphasis>.</para>
</callout>
<callout arearefs="CO150-2">
<para>You can also specify a list of devices that will have their queues set to the reserved CPU count. For more information, see <emphasis>Reducing NIC queues using the Node Tuning Operator</emphasis>.</para>
</callout>
<callout arearefs="CO150-3">
<para>Allocate the number and size of hugepages needed. You can specify the NUMA configuration for the hugepages. By default, the system allocates an even number to every NUMA node on the system. If needed, you can request the use of a realtime kernel for the nodes. See <emphasis>Provisioning a worker with real-time capabilities</emphasis> for more information.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the <literal>yaml</literal> file as <literal>mlx-dpdk-perfprofile-policy.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Apply the performance profile using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-dpdk-perfprofile-policy.yaml</programlisting>
</listitem>
</orderedlist>
<section xml:id="nw-sriov-network-operator_using-dpdk-and-rdma">
<title>Example SR-IOV Network Operator for virtual functions</title>
<simpara>You can use the Single Root I/O Virtualization (SR-IOV) Network Operator to allocate and configure Virtual Functions (VFs) from SR-IOV-supporting Physical Function NICs on the nodes.</simpara>
<simpara>For more information on deploying the Operator, see <emphasis>Installing the SR-IOV Network Operator</emphasis>.
For more information on configuring an SR-IOV network device, see <emphasis>Configuring an SR-IOV network device</emphasis>.</simpara>
<simpara>There are some differences between running Data Plane Development Kit (DPDK) workloads on Intel VFs and Mellanox VFs. This section provides object configuration examples for both VF types.
The following is an example of an <literal>sriovNetworkNodePolicy</literal> object used to run DPDK applications on Intel NICs:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-1
  namespace: openshift-sriov-network-operator
spec:
  deviceType: vfio-pci <co xml:id="CO151-1"/>
  needVhostNet: true <co xml:id="CO151-2"/>
  nicSelector:
    pfNames: ["ens3f0"]
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numVfs: 10
  priority: 99
  resourceName: dpdk_nic_1
---
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-1
  namespace: openshift-sriov-network-operator
spec:
  deviceType: vfio-pci
  needVhostNet: true
  nicSelector:
    pfNames: ["ens3f1"]
  nodeSelector:
  node-role.kubernetes.io/worker-cnf: ""
  numVfs: 10
  priority: 99
  resourceName: dpdk_nic_2</programlisting>
<calloutlist>
<callout arearefs="CO151-1">
<para>For Intel NICs, <literal>deviceType</literal> must be <literal>vfio-pci</literal>.</para>
</callout>
<callout arearefs="CO151-2">
<para>If kernel communication with DPDK workloads is required, add <literal>needVhostNet: true</literal>. This mounts the <literal>/dev/net/tun</literal> and <literal>/dev/vhost-net</literal> devices into the container so the application can create a tap device and connect the tap device to the DPDK workload.</para>
</callout>
</calloutlist>
<simpara>The following is an example of an <literal>sriovNetworkNodePolicy</literal> object for Mellanox NICs:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-1
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice <co xml:id="CO152-1"/>
  isRdma: true <co xml:id="CO152-2"/>
  nicSelector:
    rootDevices:
      - "0000:5e:00.1"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numVfs: 5
  priority: 99
  resourceName: dpdk_nic_1
---
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: dpdk-nic-2
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    rootDevices:
      - "0000:5e:00.0"
  nodeSelector:
    node-role.kubernetes.io/worker-cnf: ""
  numVfs: 5
  priority: 99
  resourceName: dpdk_nic_2</programlisting>
<calloutlist>
<callout arearefs="CO152-1">
<para>For Mellanox devices the <literal>deviceType</literal> must be <literal>netdevice</literal>.</para>
</callout>
<callout arearefs="CO152-2">
<para>For Mellanox devices <literal>isRdma</literal> must be <literal>true</literal>.
Mellanox cards are connected to DPDK applications using Flow Bifurcation. This mechanism splits traffic between Linux user space and kernel space, and can enhance line rate processing capability.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-sriov-create-object_using-dpdk-and-rdma">
<title>Example SR-IOV network operator</title>
<simpara>The following is an example definition of an <literal>sriovNetwork</literal> object. In this case, Intel and Mellanox configurations are identical:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: dpdk-network-1
  namespace: openshift-sriov-network-operator
spec:
  ipam: '{"type": "host-local","ranges": [[{"subnet": "10.0.1.0/24"}]],"dataDir":
   "/run/my-orchestrator/container-ipam-state-1"}' <co xml:id="CO153-1"/>
  networkNamespace: dpdk-test <co xml:id="CO153-2"/>
  spoofChk: "off"
  trust: "on"
  resourceName: dpdk_nic_1 <co xml:id="CO153-3"/>
---
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: dpdk-network-2
  namespace: openshift-sriov-network-operator
spec:
  ipam: '{"type": "host-local","ranges": [[{"subnet": "10.0.2.0/24"}]],"dataDir":
   "/run/my-orchestrator/container-ipam-state-1"}'
  networkNamespace: dpdk-test
  spoofChk: "off"
  trust: "on"
  resourceName: dpdk_nic_2</programlisting>
<calloutlist>
<callout arearefs="CO153-1">
<para>You can use a different IP Address Management (IPAM) implementation, such as Whereabouts. For more information, see <emphasis>Dynamic IP address assignment configuration with Whereabouts</emphasis>.</para>
</callout>
<callout arearefs="CO153-2">
<para>You must request the <literal>networkNamespace</literal> where the network attachment definition will be created. You must create the <literal>sriovNetwork</literal> CR under the <literal>openshift-sriov-network-operator</literal> namespace.</para>
</callout>
<callout arearefs="CO153-3">
<para>The <literal>resourceName</literal> value must match that of the <literal>resourceName</literal> created under the <literal>sriovNetworkNodePolicy</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-sriov-dpdk-base-workload_using-dpdk-and-rdma">
<title>Example DPDK base workload</title>
<simpara>The following is an example of a Data Plane Development Kit (DPDK) container:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: dpdk-test
---
apiVersion: v1
kind: Pod
metadata:
  annotations:
    k8s.v1.cni.cncf.io/networks: '[ <co xml:id="CO154-1"/>
     {
      "name": "dpdk-network-1",
      "namespace": "dpdk-test"
     },
     {
      "name": "dpdk-network-2",
      "namespace": "dpdk-test"
     }
   ]'
    irq-load-balancing.crio.io: "disable" <co xml:id="CO154-2"/>
    cpu-load-balancing.crio.io: "disable"
    cpu-quota.crio.io: "disable"
  labels:
    app: dpdk
  name: testpmd
  namespace: dpdk-test
spec:
  runtimeClassName: performance-performance <co xml:id="CO154-3"/>
  containers:
    - command:
        - /bin/bash
        - -c
        - sleep INF
      image: registry.redhat.io/openshift4/dpdk-base-rhel8
      imagePullPolicy: Always
      name: dpdk
      resources: <co xml:id="CO154-4"/>
        limits:
          cpu: "16"
          hugepages-1Gi: 8Gi
          memory: 2Gi
        requests:
          cpu: "16"
          hugepages-1Gi: 8Gi
          memory: 2Gi
      securityContext:
        capabilities:
          add:
            - IPC_LOCK
            - SYS_RESOURCE
            - NET_RAW
            - NET_ADMIN
        runAsUser: 0
      volumeMounts:
        - mountPath: /mnt/huge
          name: hugepages
  terminationGracePeriodSeconds: 5
  volumes:
    - emptyDir:
        medium: HugePages
      name: hugepages</programlisting>
<calloutlist>
<callout arearefs="CO154-1">
<para>Request the SR-IOV networks you need. Resources for the devices will be injected automatically.</para>
</callout>
<callout arearefs="CO154-2">
<para>Disable the CPU and IRQ load balancing base. See <emphasis>Disabling interrupt processing for individual pods</emphasis> for more information.</para>
</callout>
<callout arearefs="CO154-3">
<para>Set the <literal>runtimeClass</literal> to <literal>performance-performance</literal>. Do not set the <literal>runtimeClass</literal> to <literal>HostNetwork</literal> or <literal>privileged</literal>.</para>
</callout>
<callout arearefs="CO154-4">
<para>Request an equal number of resources for requests and limits to start the pod with <literal>Guaranteed</literal> Quality of Service (QoS).</para>
</callout>
</calloutlist>
<note>
<simpara>Do not start the pod with <literal>SLEEP</literal> and then exec into the pod to start the testpmd or the DPDK workload. This can add additional interrupts as the <literal>exec</literal> process is not pinned to any CPU.</simpara>
</note>
</section>
<section xml:id="nw-sriov-dpdk-running-testpmd_using-dpdk-and-rdma">
<title>Example testpmd script</title>
<simpara>The following is an example script for running <literal>testpmd</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">#!/bin/bash
set -ex
export CPU=$(cat /sys/fs/cgroup/cpuset/cpuset.cpus)
echo ${CPU}

dpdk-testpmd -l ${CPU} -a ${PCIDEVICE_OPENSHIFT_IO_DPDK_NIC_1} -a ${PCIDEVICE_OPENSHIFT_IO_DPDK_NIC_2} -n 4 -- -i --nb-cores=15 --rxd=4096 --txd=4096 --rxq=7 --txq=7 --forward-mode=mac --eth-peer=0,50:00:00:00:00:01 --eth-peer=1,50:00:00:00:00:02</programlisting>
<simpara>This example uses two different <literal>sriovNetwork</literal> CRs. The environment variable contains the Virtual Function (VF) PCI address that was allocated for the pod. If you use the same network in the pod definition, you must split the <literal>pciAddress</literal>.
It is important to configure the correct MAC addresses of the traffic generator. This example uses custom MAC addresses.</simpara>
</section>
</section>
<section xml:id="example-vf-use-in-rdma-mode-mellanox_using-dpdk-and-rdma">
<title>Using a virtual function in RDMA mode with a Mellanox NIC</title>
<important>
<simpara>RDMA over Converged Ethernet (RoCE) is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>RDMA over Converged Ethernet (RoCE) is the only supported mode when using RDMA
on OpenShift Container Platform.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Install the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>SriovNetworkNodePolicy</literal> object, and then save the YAML in the <literal>mlx-rdma-node-policy.yaml</literal> file.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: mlx-rdma-node-policy
  namespace: openshift-sriov-network-operator
spec:
  resourceName: mlxnics
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  priority: &lt;priority&gt;
  numVfs: &lt;num&gt;
  nicSelector:
    vendor: "15b3"
    deviceID: "1015" <co xml:id="CO155-1"/>
    pfNames: ["&lt;pf_name&gt;", ...]
    rootDevices: ["&lt;pci_bus_id&gt;", "..."]
  deviceType: netdevice <co xml:id="CO155-2"/>
  isRdma: true <co xml:id="CO155-3"/></programlisting>
<calloutlist>
<callout arearefs="CO155-1">
<para>Specify the device hex code of the SR-IOV network device.</para>
</callout>
<callout arearefs="CO155-2">
<para>Specify the driver type for the virtual functions to <literal>netdevice</literal>.</para>
</callout>
<callout arearefs="CO155-3">
<para>Enable RDMA mode.</para>
</callout>
</calloutlist>
<note>
<simpara>See the <literal>Configuring SR-IOV network devices</literal> section for a detailed explanation on each option in <literal>SriovNetworkNodePolicy</literal>.</simpara>
<simpara>When applying the configuration specified in a <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator may drain the nodes, and in some cases, reboot nodes.
It may take several minutes for a configuration change to apply.
Ensure that there are enough available nodes in your cluster to handle the evicted workload beforehand.</simpara>
<simpara>After the configuration update is applied, all the pods in the <literal>openshift-sriov-network-operator</literal> namespace will change to a <literal>Running</literal> status.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-rdma-node-policy.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create the following <literal>SriovNetwork</literal> object, and then save the YAML in the <literal>mlx-rdma-network.yaml</literal> file.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: mlx-rdma-network
  namespace: openshift-sriov-network-operator
spec:
  networkNamespace: &lt;target_namespace&gt;
  ipam: |- <co xml:id="CO156-1"/>
# ...
  vlan: &lt;vlan&gt;
  resourceName: mlxnics</programlisting>
<calloutlist>
<callout arearefs="CO156-1">
<para>Specify a configuration object for the ipam CNI plugin as a YAML block scalar. The plugin manages IP address assignment for the attachment definition.</para>
</callout>
</calloutlist>
<note>
<simpara>See the "Configuring SR-IOV additional network" section for a detailed explanation on each option in <literal>SriovNetwork</literal>.</simpara>
</note>
<simpara>An optional library, app-netutil, provides several API methods for gathering network information about a container&#8217;s parent pod.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-rdma-network.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create the following <literal>Pod</literal> spec, and then save the YAML in the <literal>mlx-rdma-pod.yaml</literal> file.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: rdma-app
  namespace: &lt;target_namespace&gt; <co xml:id="CO157-1"/>
  annotations:
    k8s.v1.cni.cncf.io/networks: mlx-rdma-network
spec:
  containers:
  - name: testpmd
    image: &lt;RDMA_image&gt; <co xml:id="CO157-2"/>
    securityContext:
      runAsUser: 0
      capabilities:
        add: ["IPC_LOCK","SYS_RESOURCE","NET_RAW"] <co xml:id="CO157-3"/>
    volumeMounts:
    - mountPath: /mnt/huge <co xml:id="CO157-4"/>
      name: hugepage
    resources:
      limits:
        memory: "1Gi"
        cpu: "4" <co xml:id="CO157-5"/>
        hugepages-1Gi: "4Gi" <co xml:id="CO157-6"/>
      requests:
        memory: "1Gi"
        cpu: "4"
        hugepages-1Gi: "4Gi"
    command: ["sleep", "infinity"]
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
<calloutlist>
<callout arearefs="CO157-1">
<para>Specify the same <literal>target_namespace</literal> where <literal>SriovNetwork</literal> object <literal>mlx-rdma-network</literal> is created. If you would like to create the pod in a different namespace, change <literal>target_namespace</literal> in both <literal>Pod</literal> spec and <literal>SriovNetwork</literal> object.</para>
</callout>
<callout arearefs="CO157-2">
<para>Specify the RDMA image which includes your application and RDMA library used by application.</para>
</callout>
<callout arearefs="CO157-3">
<para>Specify additional capabilities required by the application inside the container for hugepage allocation, system resource allocation, and network interface access.</para>
</callout>
<callout arearefs="CO157-4">
<para>Mount the hugepage volume to RDMA pod under <literal>/mnt/huge</literal>. The hugepage volume is backed by the emptyDir volume type with the medium being <literal>Hugepages</literal>.</para>
</callout>
<callout arearefs="CO157-5">
<para>Specify number of CPUs. The RDMA pod usually requires exclusive CPUs be allocated from the kubelet. This is achieved by setting CPU Manager policy to <literal>static</literal> and create pod with <literal>Guaranteed</literal> QoS.</para>
</callout>
<callout arearefs="CO157-6">
<para>Specify hugepage size <literal>hugepages-1Gi</literal> or <literal>hugepages-2Mi</literal> and the quantity of hugepages that will be allocated to the RDMA pod. Configure <literal>2Mi</literal> and <literal>1Gi</literal> hugepages separately. Configuring <literal>1Gi</literal> hugepage requires adding kernel arguments to Nodes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the RDMA pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mlx-rdma-pod.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-openstack-ovs-dpdk-testpmd-pod_using-dpdk-and-rdma">
<title>A test pod template for clusters that use OVS-DPDK on OpenStack</title>
<simpara>The following <literal>testpmd</literal> pod demonstrates container creation with huge pages, reserved CPUs, and the SR-IOV port.</simpara>
<formalpara>
<title>An example <literal>testpmd</literal> pod</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: testpmd-dpdk
  namespace: mynamespace
  annotations:
    cpu-load-balancing.crio.io: "disable"
    cpu-quota.crio.io: "disable"
# ...
spec:
  containers:
  - name: testpmd
    command: ["sleep", "99999"]
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.9
    securityContext:
      capabilities:
        add: ["IPC_LOCK","SYS_ADMIN"]
      privileged: true
      runAsUser: 0
    resources:
      requests:
        memory: 1000Mi
        hugepages-1Gi: 1Gi
        cpu: '2'
        openshift.io/dpdk1: 1 <co xml:id="CO158-1"/>
      limits:
        hugepages-1Gi: 1Gi
        cpu: '2'
        memory: 1000Mi
        openshift.io/dpdk1: 1
    volumeMounts:
      - mountPath: /mnt/huge
        name: hugepage
        readOnly: False
  runtimeClassName: performance-cnf-performanceprofile <co xml:id="CO158-2"/>
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO158-1">
<para>The name <literal>dpdk1</literal> in this example is a user-created <literal>SriovNetworkNodePolicy</literal> resource. You can substitute this name for that of a resource that you create.</para>
</callout>
<callout arearefs="CO158-2">
<para>If your performance profile is not named <literal>cnf-performance profile</literal>, replace that string with the correct performance profile name.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-openstack-hw-offload-testpmd-pod_using-dpdk-and-rdma">
<title>A test pod template for clusters that use OVS hardware offloading on OpenStack</title>
<simpara>The following <literal>testpmd</literal> pod demonstrates Open vSwitch (OVS) hardware offloading on Red Hat OpenStack Platform (RHOSP).</simpara>
<formalpara>
<title>An example <literal>testpmd</literal> pod</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: testpmd-sriov
  namespace: mynamespace
  annotations:
    k8s.v1.cni.cncf.io/networks: hwoffload1
spec:
  runtimeClassName: performance-cnf-performanceprofile <co xml:id="CO159-1"/>
  containers:
  - name: testpmd
    command: ["sleep", "99999"]
    image: registry.redhat.io/openshift4/dpdk-base-rhel8:v4.9
    securityContext:
      capabilities:
        add: ["IPC_LOCK","SYS_ADMIN"]
      privileged: true
      runAsUser: 0
    resources:
      requests:
        memory: 1000Mi
        hugepages-1Gi: 1Gi
        cpu: '2'
      limits:
        hugepages-1Gi: 1Gi
        cpu: '2'
        memory: 1000Mi
    volumeMounts:
      - mountPath: /mnt/huge
        name: hugepage
        readOnly: False
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO159-1">
<para>If your performance profile is not named <literal>cnf-performance profile</literal>, replace that string with the correct performance profile name.</para>
</callout>
</calloutlist>
</section>
<section xml:id="additional-resources_using-dpdk-and-rdma" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#cnf-about-the-profile-creator-tool_cnf-create-performance-profiles">Creating a performance profile</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#adjusting-nic-queues-with-the-performance-profile_cnf-master">Reducing NIC queues using the Node Tuning Operator</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#node-tuning-operator-provisioning-worker-with-real-time-capabilities_cnf-master">Provisioning a worker with real-time capabilities</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="installing-sr-iov-operator_installing-sriov-operator">Installing the SR-IOV Network Operator</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-sriov-networknodepolicy-object_configuring-sriov-device">Configuring an SR-IOV network device</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-multus-whereabouts_configuring-additional-network">Dynamic IP address assignment configuration with Whereabouts</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#disabling_interrupt_processing_for_individual_pods_cnf-master">Disabling interrupt processing for individual pods</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-sriov-net-attach">Configuring an SR-IOV Ethernet network attachment</link></simpara>
</listitem>
<listitem>
<simpara>The <link linkend="nw-sriov-app-netutil_about-sriov">app-netutil library</link> provides several API methods for gathering network information about a container&#8217;s parent pod.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="using-pod-level-bonding">
<title>Using pod-level bonding</title>

<simpara>Bonding at the pod level is vital to enable workloads inside pods that require high availability and more throughput. With pod-level bonding, you can create a bond interface from multiple single root I/O virtualization (SR-IOV) virtual function interfaces in a kernel mode interface. The SR-IOV virtual functions are passed into the pod and attached to a kernel driver.</simpara>
<simpara>One scenario where pod level bonding is required is creating a bond interface from multiple SR-IOV virtual functions on different physical functions. Creating a bond interface from two different physical functions on the host can be used to achieve high availability and throughput at pod level.</simpara>
<simpara>For guidance on tasks such as creating a SR-IOV network, network policies, network attachment definitions and pods, see  <link linkend="configuring-sriov-device">Configuring an SR-IOV network device</link>.</simpara>
<section xml:id="nw-sriov-cfg-bond-interface-with-virtual-functions_using-pod-level-bonding">
<title>Configuring a bond interface from two SR-IOV interfaces</title>
<simpara>Bonding enables multiple network interfaces to be aggregated into a single logical "bonded" interface. Bond Container Network Interface (Bond-CNI) brings bond capability into containers.</simpara>
<simpara>Bond-CNI can be created using Single Root I/O Virtualization (SR-IOV) virtual functions and placing them in the container network namespace.</simpara>
<simpara>OpenShift Container Platform only supports Bond-CNI using SR-IOV virtual functions. The SR-IOV Network Operator provides the SR-IOV CNI plugin needed to manage the virtual functions. Other CNIs or types of interfaces are not supported.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The SR-IOV Network Operator must be installed and configured to obtain virtual functions in a container.</simpara>
</listitem>
<listitem>
<simpara>To configure SR-IOV interfaces, an SR-IOV network and policy must be created for each interface.</simpara>
</listitem>
<listitem>
<simpara>The SR-IOV Network Operator creates a network attachment definition for each SR-IOV interface, based on the SR-IOV network and policy defined.</simpara>
</listitem>
<listitem>
<simpara>The <literal>linkState</literal> is set to the default value <literal>auto</literal> for the SR-IOV virtual function.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-sriov-cfg-creating-bond-network-attachment-definition_using-pod-level-bonding">
<title>Creating a bond network attachment definition</title>
<simpara>Now that the SR-IOV virtual functions are available, you can create a bond network attachment definition.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
    kind: NetworkAttachmentDefinition
    metadata:
      name: bond-net1
      namespace: demo
    spec:
      config: '{
      "type": "bond", <co xml:id="CO160-1"/>
      "cniVersion": "0.3.1",
      "name": "bond-net1",
      "mode": "active-backup", <co xml:id="CO160-2"/>
      "failOverMac": 1, <co xml:id="CO160-3"/>
      "linksInContainer": true, <co xml:id="CO160-4"/>
      "miimon": "100",
      "mtu": 1500,
      "links": [ <co xml:id="CO160-5"/>
            {"name": "net1"},
            {"name": "net2"}
        ],
      "ipam": {
            "type": "host-local",
            "subnet": "10.56.217.0/24",
            "routes": [{
            "dst": "0.0.0.0/0"
            }],
            "gateway": "10.56.217.1"
        }
      }'</programlisting>
<calloutlist>
<callout arearefs="CO160-1">
<para>The cni-type is always set to <literal>bond</literal>.</para>
</callout>
<callout arearefs="CO160-2">
<para>The <literal>mode</literal> attribute specifies the bonding mode.</para>
<note>
<simpara>The bonding modes supported are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>balance-rr</literal> - 0</simpara>
</listitem>
<listitem>
<simpara><literal>active-backup</literal> - 1</simpara>
</listitem>
<listitem>
<simpara><literal>balance-xor</literal> - 2</simpara>
</listitem>
</itemizedlist>
<simpara>For <literal>balance-rr</literal> or <literal>balance-xor</literal> modes, you must set the <literal>trust</literal> mode to <literal>on</literal> for the SR-IOV virtual function.</simpara>
</note>
</callout>
<callout arearefs="CO160-3">
<para>The <literal>failover</literal> attribute is mandatory for active-backup mode and must be set to 1.</para>
</callout>
<callout arearefs="CO160-4">
<para>The <literal>linksInContainer=true</literal> flag informs the Bond CNI that the required interfaces are to be found inside the container. By default, Bond CNI looks for these interfaces on the host which does not work for integration with SRIOV and Multus.</para>
</callout>
<callout arearefs="CO160-5">
<para>The <literal>links</literal> section defines which interfaces will be used to create the bond. By default, Multus names the attached interfaces as: "net", plus a consecutive number, starting with one.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-sriov-cfg-creating-pod-using-interface_using-pod-level-bonding">
<title>Creating a pod using a bond interface</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Test the setup by creating a pod with a YAML file named for example <literal>podbonding.yaml</literal> with content similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
    kind: Pod
    metadata:
      name: bondpod1
      namespace: demo
      annotations:
        k8s.v1.cni.cncf.io/networks: demo/sriovnet1, demo/sriovnet2, demo/bond-net1 <co xml:id="CO161-1"/>
    spec:
      containers:
      - name: podexample
        image: quay.io/openshift/origin-network-interface-bond-cni:4.11.0
        command: ["/bin/bash", "-c", "sleep INF"]</programlisting>
<calloutlist>
<callout arearefs="CO161-1">
<para>Note the network annotation: it contains two SR-IOV network attachments, and one bond network attachment. The bond attachment uses the two SR-IOV interfaces as bonded port interfaces.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the yaml by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f podbonding.yaml</programlisting>
</listitem>
<listitem>
<simpara>Inspect the pod interfaces with the following command:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc rsh -n demo bondpod1
sh-4.4#
sh-4.4# ip a
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN qlen 1000
link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
inet 127.0.0.1/8 scope host lo
valid_lft forever preferred_lft forever
3: eth0@if150: &lt;BROADCAST,MULTICAST,UP,LOWER_UP,M-DOWN&gt; mtu 1450 qdisc noqueue state UP
link/ether 62:b1:b5:c8:fb:7a brd ff:ff:ff:ff:ff:ff
inet 10.244.1.122/24 brd 10.244.1.255 scope global eth0
valid_lft forever preferred_lft forever
4: net3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP400&gt; mtu 1500 qdisc noqueue state UP qlen 1000
link/ether 9e:23:69:42:fb:8a brd ff:ff:ff:ff:ff:ff <co xml:id="CO162-1"/>
inet 10.56.217.66/24 scope global bond0
valid_lft forever preferred_lft forever
43: net1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP800&gt; mtu 1500 qdisc mq master bond0 state UP qlen 1000
link/ether 9e:23:69:42:fb:8a brd ff:ff:ff:ff:ff:ff <co xml:id="CO162-2"/>
44: net2: &lt;BROADCAST,MULTICAST,UP,LOWER_UP800&gt; mtu 1500 qdisc mq master bond0 state UP qlen 1000
link/ether 9e:23:69:42:fb:8a brd ff:ff:ff:ff:ff:ff <co xml:id="CO162-3"/></programlisting>
<calloutlist>
<callout arearefs="CO162-1">
<para>The bond interface is automatically named <literal>net3</literal>. To set a specific interface name add <literal>@name</literal> suffix to the pod’s <literal>k8s.v1.cni.cncf.io/networks</literal> annotation.</para>
</callout>
<callout arearefs="CO162-2">
<para>The <literal>net1</literal> interface is based on an SR-IOV virtual function.</para>
</callout>
<callout arearefs="CO162-3">
<para>The <literal>net2</literal> interface is based on an SR-IOV virtual function.</para>
</callout>
</calloutlist>
<note>
<simpara>If no interface names are configured in the pod annotation, interface names are assigned automatically as <literal>net&lt;n&gt;</literal>, with <literal>&lt;n&gt;</literal> starting at <literal>1</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: If you want to set a specific interface name for example <literal>bond0</literal>, edit the <literal>k8s.v1.cni.cncf.io/networks</literal> annotation and set <literal>bond0</literal> as the interface name as follows:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">annotations:
        k8s.v1.cni.cncf.io/networks: demo/sriovnet1, demo/sriovnet2, demo/bond-net1@bond0</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="configuring-hardware-offloading">
<title>Configuring hardware offloading</title>

<simpara>As a cluster administrator, you can configure hardware offloading on compatible nodes to increase data processing performance and reduce load on host CPUs.</simpara>
<section xml:id="about-hardware-offloading_configuring-hardware-offloading">
<title>About hardware offloading</title>
<simpara>Open vSwitch hardware offloading is a method of processing network tasks by diverting them away from the CPU and offloading them to a dedicated processor on a network interface controller.
As a result, clusters can benefit from faster data transfer speeds, reduced CPU workloads, and lower computing costs.</simpara>
<simpara>The key element for this feature is a modern class of network interface controllers known as SmartNICs.
A SmartNIC is a network interface controller that is able to handle computationally-heavy network processing tasks.
In the same way that a dedicated graphics card can improve graphics performance, a SmartNIC can improve network performance.
In each case, a dedicated processor improves performance for a specific type of processing task.</simpara>
<simpara>In OpenShift Container Platform, you can configure hardware offloading for bare metal nodes that have a compatible SmartNIC.
Hardware offloading is configured and enabled by the SR-IOV Network Operator.</simpara>
<simpara>Hardware offloading is not compatible with all workloads or application types.
Only the following two communication types are supported:</simpara>
<itemizedlist>
<listitem>
<simpara>pod-to-pod</simpara>
</listitem>
<listitem>
<simpara>pod-to-service, where the service is a ClusterIP service backed by a regular pod</simpara>
</listitem>
</itemizedlist>
<simpara>In all cases, hardware offloading takes place only when those pods and services are assigned to nodes that have a compatible SmartNIC.
Suppose, for example, that a pod on a node with hardware offloading tries to communicate with a service on a regular node.
On the regular node, all the processing takes place in the kernel, so the overall performance of the pod-to-service communication is limited to the maximum performance of that regular node.
Hardware offloading is not compatible with DPDK applications.</simpara>
<simpara>Enabling hardware offloading on a node, but not configuring pods to use, it can result in decreased throughput performance for pod traffic. You cannot configure hardware offloading for pods that are managed by OpenShift Container Platform.</simpara>
</section>
<section xml:id="supported_devices_configuring-hardware-offloading">
<title>Supported devices</title>
<simpara>Hardware offloading is supported on the following network interface controllers:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supported network interface controllers</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="40*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Manufacturer</entry>
<entry align="left" valign="top">Model</entry>
<entry align="left" valign="top">Vendor ID</entry>
<entry align="left" valign="top">Device ID</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT27800 Family [ConnectX&#8209;5]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1017</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT28880 Family [ConnectX&#8209;5&#160;Ex]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>1019</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT2892 Family [ConnectX&#8209;6 Dx]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>101d</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT2894 Family [ConnectX-6 Lx]</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>101f</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Mellanox</simpara></entry>
<entry align="left" valign="top"><simpara>MT42822 BlueField-2 in ConnectX-6 NIC mode</simpara></entry>
<entry align="left" valign="top"><simpara>15b3</simpara></entry>
<entry align="left" valign="top"><simpara>a2d6</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="configuring-hardware-offloading-prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your cluster has at least one bare metal machine with a network interface controller that is supported for hardware offloading.</simpara>
</listitem>
<listitem>
<simpara>You <link linkend="installing-sr-iov-operator_installing-sriov-operator">installed the SR-IOV Network Operator</link>.</simpara>
</listitem>
<listitem>
<simpara>Your cluster uses the <link linkend="about-ovn-kubernetes">OVN-Kubernetes network plugin</link>.</simpara>
</listitem>
<listitem>
<simpara>In your <link linkend="gatewayConfig-object_cluster-network-operator">OVN-Kubernetes network plugin configuration</link>, the <literal>gatewayConfig.routingViaHost</literal> field is set to <literal>false</literal>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-machine-config-pool_configuring-hardware-offloading">
<title>Configuring a machine config pool for hardware offloading</title>
<simpara>To enable hardware offloading, you must first create a dedicated machine config pool and configure it to work with the SR-IOV Network Operator.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a machine config pool for machines you want to use hardware offloading on.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>mcp-offloading.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: mcp-offloading <co xml:id="CO163-1"/>
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,mcp-offloading]} <co xml:id="CO163-2"/>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/mcp-offloading: "" <co xml:id="CO163-3"/></programlisting>
<calloutlist>
<callout arearefs="CO163-1 CO163-2">
<para>The name of your machine config pool for hardware offloading.</para>
</callout>
<callout arearefs="CO163-3">
<para>This node role label is used to add nodes to the machine config pool.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mcp-offloading.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Add nodes to the machine config pool. Label each node with the node role label of your pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node worker-2 node-role.kubernetes.io/mcp-offloading=""</programlisting>
</listitem>
<listitem>
<simpara>Optional: To verify that the new pool is created, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME       STATUS   ROLES                   AGE   VERSION
master-0   Ready    master                  2d    v1.28.5
master-1   Ready    master                  2d    v1.28.5
master-2   Ready    master                  2d    v1.28.5
worker-0   Ready    worker                  2d    v1.28.5
worker-1   Ready    worker                  2d    v1.28.5
worker-2   Ready    mcp-offloading,worker   47h   v1.28.5
worker-3   Ready    mcp-offloading,worker   47h   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Add this machine config pool to the <literal>SriovNetworkPoolConfig</literal> custom resource:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>sriov-pool-config.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkPoolConfig
metadata:
  name: sriovnetworkpoolconfig-offload
  namespace: openshift-sriov-network-operator
spec:
  ovsHardwareOffloadConfig:
    name: mcp-offloading <co xml:id="CO164-1"/></programlisting>
<calloutlist>
<callout arearefs="CO164-1">
<para>The name of your machine config pool for hardware offloading.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;SriovNetworkPoolConfig_name&gt;.yaml</programlisting>
<note>
<simpara>When you apply the configuration specified in a <literal>SriovNetworkPoolConfig</literal> object, the SR-IOV Operator drains and restarts the nodes in the machine config pool.</simpara>
<simpara>It might take several minutes for a configuration changes to apply.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="configure-sriov-node-policy_configuring-hardware-offloading">
<title>Configuring the SR-IOV network node policy</title>
<simpara>You can create an SR-IOV network device configuration for a node by creating an SR-IOV network node policy.
To enable hardware offloading, you must define the <literal>.spec.eSwitchMode</literal> field with the value <literal>"switchdev"</literal>.</simpara>
<simpara>The following procedure creates an SR-IOV interface for a network interface controller with hardware offloading.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>sriov-node-policy.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-policy <co xml:id="CO165-1"/>
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice <co xml:id="CO165-2"/>
  eSwitchMode: "switchdev" <co xml:id="CO165-3"/>
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"
  numVfs: 6
  priority: 5
  resourceName: mlxnics</programlisting>
<calloutlist>
<callout arearefs="CO165-1">
<para>The name for the custom resource object.</para>
</callout>
<callout arearefs="CO165-2">
<para>Required. Hardware offloading is not supported with <literal>vfio-pci</literal>.</para>
</callout>
<callout arearefs="CO165-3">
<para>Required.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the policy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-node-policy.yaml</programlisting>
<note>
<simpara>When you apply the configuration specified in a <literal>SriovNetworkPoolConfig</literal> object, the SR-IOV Operator drains and restarts the nodes in the machine config pool.</simpara>
<simpara>It might take several minutes for a configuration change to apply.</simpara>
</note>
</listitem>
</orderedlist>
<section xml:id="nw-sriov-hwol-ref-openstack-sriov-policy_configuring-hardware-offloading">
<title>An example SR-IOV network node policy for OpenStack</title>
<simpara>The following example describes an SR-IOV interface for a network interface controller (NIC) with hardware offloading on Red Hat OpenStack Platform (RHOSP).</simpara>
<formalpara>
<title>An SR-IOV interface for a NIC with hardware offloading on RHOSP</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: ${name}
  namespace: openshift-sriov-network-operator
spec:
  deviceType: switchdev
  isRdma: true
  nicSelector:
    netFilter: openstack/NetworkID:${net_id}
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: ${name}</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="improving-network-traffic-performance-using-vf_configuring-hardware-offloading">
<title>Improving network traffic performance using a virtual function</title>
<simpara>Follow this procedure to assign a virtual function to the OVN-Kubernetes management port and increase its network traffic performance.</simpara>
<simpara>This procedure results in the creation of two pools: the first has a virtual function used by OVN-Kubernetes, and the second comprises the remaining virtual functions.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the <literal>network.operator.openshift.io/smart-nic</literal> label to each worker node with a SmartNIC present by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node-name&gt; network.operator.openshift.io/smart-nic=</programlisting>
<simpara>Use the <literal>oc get nodes</literal> command to get a list of the available nodes.</simpara>
</listitem>
<listitem>
<simpara>Create a policy named <literal>sriov-node-mgmt-vf-policy.yaml</literal> for the management port with content such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-mgmt-vf-policy
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  eSwitchMode: "switchdev"
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0#0-0 <co xml:id="CO166-1"/>
  nodeSelector:
    network.operator.openshift.io/smart-nic: ""
  numVfs: 6 <co xml:id="CO166-2"/>
  priority: 5
  resourceName: mgmtvf</programlisting>
<calloutlist>
<callout arearefs="CO166-1">
<para>Replace this device with the appropriate network device for your use case. The <literal>#0-0</literal> part of the <literal>pfNames</literal> value reserves a single virtual function used by OVN-Kubernetes.</para>
</callout>
<callout arearefs="CO166-2">
<para>The value provided here is an example. Replace this value with one that meets your requirements. For more information, see <emphasis>SR-IOV network node configuration object</emphasis> in the <emphasis>Additional resources</emphasis> section.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a policy named <literal>sriov-node-policy.yaml</literal> with content such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: sriov-node-policy
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  eSwitchMode: "switchdev"
  nicSelector:
    deviceID: "1019"
    rootDevices:
    - 0000:d8:00.0
    vendor: "15b3"
    pfNames:
    - ens8f0#1-5 <co xml:id="CO167-1"/>
  nodeSelector:
    network.operator.openshift.io/smart-nic: ""
  numVfs: 6 <co xml:id="CO167-2"/>
  priority: 5
  resourceName: mlxnics</programlisting>
<calloutlist>
<callout arearefs="CO167-1">
<para>Replace this device with the appropriate network device for your use case.</para>
</callout>
<callout arearefs="CO167-2">
<para>The value provided here is an example. Replace this value with the value specified in the <literal>sriov-node-mgmt-vf-policy.yaml</literal> file. For more information, see <emphasis>SR-IOV network node configuration object</emphasis> in the <emphasis>Additional resources</emphasis> section.</para>
</callout>
</calloutlist>
<note>
<simpara>The <literal>sriov-node-mgmt-vf-policy.yaml</literal> file has different values for the <literal>pfNames</literal> and <literal>resourceName</literal> keys than the <literal>sriov-node-policy.yaml</literal> file.</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the configuration for both policies:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-node-policy.yaml</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f sriov-node-mgmt-vf-policy.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a Cluster Network Operator (CNO) ConfigMap in the cluster for the management configuration:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a ConfigMap named <literal>hardware-offload-config.yaml</literal> with the following contents:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
    name: hardware-offload-config
    namespace: openshift-network-operator
data:
    mgmt-port-resource-name: openshift.io/mgmtvf</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the ConfigMap:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f hardware-offload-config.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist xml:id="additional-resources_using-vf-improve-network-traffic-performance" role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-sriov-networknodepolicy-object_configuring-sriov-device">SR-IOV network node configuration object</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="create-network-attachment-definition_configuring-hardware-offloading">
<title>Creating a network attachment definition</title>
<simpara>After you define the machine config pool and the SR-IOV network node policy, you can create a network attachment definition for the network interface card you specified.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>net-attach-def.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: net-attach-def <co xml:id="CO168-1"/>
  namespace: net-attach-def <co xml:id="CO168-2"/>
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/mlxnics <co xml:id="CO168-3"/>
spec:
  config: '{"cniVersion":"0.3.1","name":"ovn-kubernetes","type":"ovn-k8s-cni-overlay","ipam":{},"dns":{}}'</programlisting>
<calloutlist>
<callout arearefs="CO168-1">
<para>The name for your network attachment definition.</para>
</callout>
<callout arearefs="CO168-2">
<para>The namespace for your network attachment definition.</para>
</callout>
<callout arearefs="CO168-3">
<para>This is the value of the <literal>spec.resourceName</literal> field you specified in the <literal>SriovNetworkNodePolicy</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the network attachment definition:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f net-attach-def.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Run the following command to see whether the new definition is present:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get net-attach-def -A</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE         NAME             AGE
net-attach-def    net-attach-def   43h</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="adding-network-attachment-definition-to-pods_configuring-hardware-offloading">
<title>Adding the network attachment definition to your pods</title>
<simpara>After you create the machine config pool, the <literal>SriovNetworkPoolConfig</literal> and <literal>SriovNetworkNodePolicy</literal> custom resources, and the network attachment definition, you can apply these configurations to your pods by adding the network attachment definition to your pod specifications.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>In the pod specification, add the <literal>.metadata.annotations.k8s.v1.cni.cncf.io/networks</literal> field and specify the network attachment definition you created for hardware offloading:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">....
metadata:
  annotations:
    v1.multus-cni.io/default-network: net-attach-def/net-attach-def <co xml:id="CO169-1"/></programlisting>
<calloutlist>
<callout arearefs="CO169-1">
<para>The value must be the name and namespace of the network attachment definition you created for hardware offloading.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="switching-bf2-nic-dpu">
<title>Switching Bluefield-2 from DPU to NIC</title>

<simpara>You can switch the Bluefield-2 network device from data processing unit (DPU) mode to network interface controller (NIC) mode.</simpara>
<section xml:id="proc-switching-bf2-nic_switching-bf2-nic-dpu">
<title>Switching Bluefield-2 from DPU mode to NIC mode</title>
<simpara>Use the following procedure to switch Bluefield-2 from data processing units (DPU) mode to network interface controller (NIC) mode.</simpara>
<important>
<simpara>Currently, only switching Bluefield-2 from DPU to NIC mode is supported. Switching from NIC mode to DPU mode is unsupported.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the SR-IOV Network Operator. For more information, see "Installing SR-IOV Network Operator".</simpara>
</listitem>
<listitem>
<simpara>You have updated Bluefield-2 to the latest firmware. For more information, see <link xlink:href="https://network.nvidia.com/support/firmware/bluefield2/">Firmware for NVIDIA BlueField-2</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the following labels to each of your worker nodes by entering the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;example_node_name_one&gt; node-role.kubernetes.io/sriov=</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;example_node_name_two&gt; node-role.kubernetes.io/sriov=</programlisting>
</listitem>
<listitem>
<simpara>Create a machine config pool for the SR-IOV Network Operator, for example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: sriov
spec:
  machineConfigSelector:
    matchExpressions:
    - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,sriov]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/sriov: ""</programlisting>
</listitem>
<listitem>
<simpara>Apply the following <literal>machineconfig.yaml</literal> file to the worker nodes:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: sriov
  name: 99-bf2-dpu
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - contents:
          source: data:text/plain;charset=utf-8;base64,ZmluZF9jb250YWluZXIoKSB7CiAgY3JpY3RsIHBzIC1vIGpzb24gfCBqcSAtciAnLmNvbnRhaW5lcnNbXSB8IHNlbGVjdCgubWV0YWRhdGEubmFtZT09InNyaW92LW5ldHdvcmstY29uZmlnLWRhZW1vbiIpIHwgLmlkJwp9CnVudGlsIG91dHB1dD0kKGZpbmRfY29udGFpbmVyKTsgW1sgLW4gIiRvdXRwdXQiIF1dOyBkbwogIGVjaG8gIndhaXRpbmcgZm9yIGNvbnRhaW5lciB0byBjb21lIHVwIgogIHNsZWVwIDE7CmRvbmUKISBzdWRvIGNyaWN0bCBleGVjICRvdXRwdXQgL2JpbmRhdGEvc2NyaXB0cy9iZjItc3dpdGNoLW1vZGUuc2ggIiRAIgo=
        mode: 0755
        overwrite: true
        path: /etc/default/switch_in_sriov_config_daemon.sh
    systemd:
      units:
      - name: dpu-switch.service
        enabled: true
        contents: |
          [Unit]
          Description=Switch BlueField2 card to NIC/DPU mode
          RequiresMountsFor=%t/containers
          Wants=network.target
          After=network-online.target kubelet.service
          [Service]
          SuccessExitStatus=0 120
          RemainAfterExit=True
          ExecStart=/bin/bash -c '/etc/default/switch_in_sriov_config_daemon.sh nic || shutdown -r now' <co xml:id="CO170-1"/>
          Type=oneshot
          [Install]
          WantedBy=multi-user.target</programlisting>
<calloutlist>
<callout arearefs="CO170-1">
<para>Optional: The PCI address of a specific card can optionally be specified, for example <literal>ExecStart=/bin/bash -c '/etc/default/switch_in_sriov_config_daemon.sh nic 0000:5e:00.0 || echo done'</literal>. By default, the first device is selected. If there is more than one device, you must specify which PCI address to be used. The PCI address must be the same on all nodes that are switching Bluefield-2 from DPU mode to NIC mode.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Wait for the worker nodes to restart. After restarting, the Bluefield-2 network device on the worker nodes is switched into NIC mode.</simpara>
</listitem>
</orderedlist>
<formalpara role="_additional-resources">
<title>Additional resources</title>
<para><link linkend="installing-sr-iov-operator_installing-sriov-operator">Installing SR-IOV Network Operator</link></para>
</formalpara>
</section>
</section>
<section xml:id="uninstalling-sriov-operator">
<title>Uninstalling the SR-IOV Network Operator</title>

<simpara>To uninstall the SR-IOV Network Operator, you must delete any running SR-IOV workloads, uninstall the Operator, and delete the webhooks that the Operator used.</simpara>
<section xml:id="nw-sriov-operator-uninstall_uninstalling-sr-iov-operator">
<title>Uninstalling the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can uninstall the SR-IOV Network Operator.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster using an account with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have the SR-IOV Network Operator installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete all SR-IOV custom resources (CRs):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete sriovnetwork -n openshift-sriov-network-operator --all</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete sriovnetworknodepolicy -n openshift-sriov-network-operator --all</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete sriovibnetwork -n openshift-sriov-network-operator --all</programlisting>
</listitem>
<listitem>
<simpara>Follow the instructions in the "Deleting Operators from a cluster" section to remove the SR-IOV Network Operator from your cluster.</simpara>
</listitem>
<listitem>
<simpara>Delete the SR-IOV custom resource definitions that remain in the cluster after the SR-IOV Network Operator is uninstalled:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd sriovibnetworks.sriovnetwork.openshift.io</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd sriovnetworknodepolicies.sriovnetwork.openshift.io</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd sriovnetworknodestates.sriovnetwork.openshift.io</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd sriovnetworkpoolconfigs.sriovnetwork.openshift.io</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd sriovnetworks.sriovnetwork.openshift.io</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd sriovoperatorconfigs.sriovnetwork.openshift.io</programlisting>
</listitem>
<listitem>
<simpara>Delete the SR-IOV webhooks:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete mutatingwebhookconfigurations network-resources-injector-config</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete MutatingWebhookConfiguration sriov-operator-webhook-config</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete ValidatingWebhookConfiguration sriov-operator-webhook-config</programlisting>
</listitem>
<listitem>
<simpara>Delete the SR-IOV Network Operator namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-sriov-network-operator</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-deleting-operators-from-a-cluster">Deleting Operators from a cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_ovn-kubernetes-network-plugin">
<title>OVN-Kubernetes network plugin</title>
<section xml:id="about-ovn-kubernetes">
<title>About the OVN-Kubernetes network plugin</title>

<simpara>The OpenShift Container Platform cluster uses a virtualized network for pod and service networks.</simpara>
<simpara>Part of Red Hat OpenShift Networking, the OVN-Kubernetes network plugin is the default network provider for OpenShift Container Platform.
OVN-Kubernetes is based on Open Virtual Network (OVN) and provides an overlay-based networking implementation.
A cluster that uses the OVN-Kubernetes plugin also runs Open vSwitch (OVS) on each node.
OVN configures OVS on each node to implement the declared network configuration.</simpara>
<note>
<simpara>OVN-Kubernetes is the default networking solution for OpenShift Container Platform and single-node OpenShift deployments.</simpara>
</note>
<simpara>OVN-Kubernetes, which arose from the OVS project, uses many of the same constructs, such as open flow rules, to determine how packets travel through the network.
For more information, see the <link xlink:href="https://www.ovn.org/en/">Open Virtual Network website</link>.</simpara>
<simpara>OVN-Kubernetes is a series of daemons for OVS that translate virtual network configurations into <literal>OpenFlow</literal> rules.
<literal>OpenFlow</literal> is a protocol for communicating with network switches and routers, providing a means for remotely controlling the flow of network traffic on a network device, allowing network administrators to configure, manage, and monitor the flow of network traffic.</simpara>
<simpara>OVN-Kubernetes provides more of the advanced functionality not available with <literal>OpenFlow</literal>.
OVN supports distributed virtual routing, distributed logical switches, access control, DHCP and DNS.
OVN implements distributed virtual routing within logic flows which equate to open flows.
So for example if you have a pod that sends out a DHCP request on the network, it sends out that broadcast looking for DHCP address there will be a logic flow rule that matches that packet, and it responds giving it a gateway, a DNS server an IP address and so on.</simpara>
<simpara>OVN-Kubernetes runs a daemon on each node. There are daemon sets for the databases and for the OVN controller that run on every node.
The OVN controller programs the Open vSwitch daemon on the nodes to support the network provider features; egress IPs, firewalls, routers, hybrid networking, IPSEC encryption, IPv6, network policy, network policy logs, hardware offloading and multicast.</simpara>
<section xml:id="nw-ovn-kubernetes-purpose_about-ovn-kubernetes">
<title>OVN-Kubernetes purpose</title>
<simpara>The OVN-Kubernetes network plugin is an open-source, fully-featured Kubernetes CNI plugin that uses Open Virtual Network (OVN) to manage network traffic flows. OVN is a community developed, vendor-agnostic network virtualization solution. The OVN-Kubernetes network plugin:</simpara>
<itemizedlist>
<listitem>
<simpara>Uses OVN (Open Virtual Network) to manage network traffic flows. OVN is a community developed, vendor-agnostic network virtualization solution.</simpara>
</listitem>
<listitem>
<simpara>Implements Kubernetes network policy support, including ingress and egress rules.</simpara>
</listitem>
<listitem>
<simpara>Uses the Geneve (Generic Network Virtualization Encapsulation) protocol rather than VXLAN to create an overlay network between nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>The OVN-Kubernetes network plugin provides the following advantages over OpenShift SDN.</simpara>
<itemizedlist>
<listitem>
<simpara>Full support for IPv6 single-stack and IPv4/IPv6 dual-stack networking on supported platforms</simpara>
</listitem>
<listitem>
<simpara>Support for hybrid clusters with both Linux and Microsoft Windows workloads</simpara>
</listitem>
<listitem>
<simpara>Optional IPsec encryption of intra-cluster communications</simpara>
</listitem>
<listitem>
<simpara>Offload of network data processing from host CPU to compatible network cards and data processing units (DPUs)</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ovn-kubernetes-matrix_about-ovn-kubernetes">
<title>Supported network plugin feature matrix</title>
<simpara>Red Hat OpenShift Networking offers two options for the network plugin, OpenShift SDN and OVN-Kubernetes, for the network plugin. The following table summarizes the current feature support for both network plugins:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Default CNI network plugin feature comparison</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Feature</entry>
<entry align="left" valign="top">OVN-Kubernetes</entry>
<entry align="left" valign="top">OpenShift SDN</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Egress IPs</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Egress firewall <superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Egress router</simpara></entry>
<entry align="left" valign="top"><simpara>Supported <superscript>[2]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hybrid networking</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IPsec encryption for intra-cluster communication</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IPv6</simpara></entry>
<entry align="left" valign="top"><simpara>Supported <superscript>[3]</superscript> <superscript>[4]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes network policy</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes network policy logs</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hardware offloading</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Multicast</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>Egress firewall is also known as egress network policy in OpenShift SDN. This is not the same as network policy egress.</simpara>
</listitem>
<listitem>
<simpara>Egress router for OVN-Kubernetes supports only redirect mode.</simpara>
</listitem>
<listitem>
<simpara>IPv6 is supported only on bare metal, vSphere, IBM Power&#174;, and IBM Z&#174; clusters.</simpara>
</listitem>
<listitem>
<simpara>IPv6 single stack is not supported on IBM Power&#174; and IBM Z&#174; clusters.</simpara>
</listitem>
</orderedlist>
</para>
</section>
<section xml:id="nw-ovn-kubernetes-limitations_about-ovn-kubernetes">
<title>OVN-Kubernetes IPv6 and dual-stack limitations</title>
<simpara>The OVN-Kubernetes network plugin has the following limitations:</simpara>
<itemizedlist>
<listitem>
<simpara>For clusters configured for dual-stack networking, both IPv4 and IPv6 traffic must use the same network interface as the default gateway.
If this requirement is not met, pods on the host in the <literal>ovnkube-node</literal> daemon set enter the <literal>CrashLoopBackOff</literal> state.
If you display a pod with a command such as <literal>oc get pod -n openshift-ovn-kubernetes -l app=ovnkube-node -o yaml</literal>, the <literal>status</literal> field contains more than one message about the default gateway, as shown in the following output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">I1006 16:09:50.985852   60651 helper_linux.go:73] Found default gateway interface br-ex 192.168.127.1
I1006 16:09:50.985923   60651 helper_linux.go:73] Found default gateway interface ens4 fe80::5054:ff:febe:bcd4
F1006 16:09:50.985939   60651 ovnkube.go:130] multiple gateway interfaces detected: br-ex ens4</programlisting>
<simpara>The only resolution is to reconfigure the host networking so that both IP families use the same network interface for the default gateway.</simpara>
</listitem>
<listitem>
<simpara>For clusters configured for dual-stack networking, both the IPv4 and IPv6 routing tables must contain the default gateway.
If this requirement is not met, pods on the host in the <literal>ovnkube-node</literal> daemon set enter the <literal>CrashLoopBackOff</literal> state.
If you display a pod with a command such as <literal>oc get pod -n openshift-ovn-kubernetes -l app=ovnkube-node -o yaml</literal>, the <literal>status</literal> field contains more than one message about the default gateway, as shown in the following output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">I0512 19:07:17.589083  108432 helper_linux.go:74] Found default gateway interface br-ex 192.168.123.1
F0512 19:07:17.589141  108432 ovnkube.go:133] failed to get default gateway interface</programlisting>
<simpara>The only resolution is to reconfigure the host networking so that both IP families contain the default gateway.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ovn-kubernetes-session-affinity_about-ovn-kubernetes">
<title>Session affinity</title>
<simpara>Session affinity is a feature that applies to Kubernetes <literal>Service</literal> objects. You can use <emphasis>session affinity</emphasis> if you want to ensure that each time you connect to a &lt;service_VIP&gt;:&lt;Port&gt;, the traffic is always load balanced to the same back end. For more information, including how to set session affinity based on a client&#8217;s IP address, see <link xlink:href="https://kubernetes.io/docs/reference/networking/virtual-ips/#session-affinity">Session affinity</link>.</simpara>
<bridgehead xml:id="nw-ovn-kubernetes-session-affinity-stickyness-timeout_about-ovn-kubernetes" renderas="sect4">Stickiness timeout for session affinity</bridgehead>
<simpara>The OVN-Kubernetes network plugin for OpenShift Container Platform calculates the stickiness timeout for a session from a client based on the last packet. For example, if you run a <literal>curl</literal> command 10 times, the sticky session timer starts from the tenth packet not the first. As a result, if the client is continuously contacting the service, then the session never times out. The timeout starts when the service has not received a packet for the amount of time set by the <link xlink:href="https://kubernetes.io/docs/reference/networking/virtual-ips/#session-stickiness-timeout"><literal>timeoutSeconds</literal></link> parameter.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="configuring-egress-firewall-ovn">Configuring an egress firewall for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="about-network-policy">About network policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="logging-network-policy">Logging network policy events</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-ovn-kubernetes-enabling-multicast">Enabling multicast for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-ipsec-ovn">Configuring IPsec encryption</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1]</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="ovn-kubernetes-architecture-assembly">
<title>OVN-Kubernetes architecture</title>

<section xml:id="ovn-kubernetes-architecture-con">
<title>Introduction to OVN-Kubernetes architecture</title>
<simpara>The following diagram shows the OVN-Kubernetes architecture.</simpara>
<figure>
<title>OVK-Kubernetes architecture</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/299_OpenShift_OVN-Kubernetes_arch_1023_1.png"/>
</imageobject>
<textobject><phrase>OVN-Kubernetes architecture</phrase></textobject>
</mediaobject>
</figure>
<simpara>The key components are:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Cloud Management System (CMS)</emphasis> - A platform specific client for OVN that provides a CMS specific plugin for OVN integration. The plugin translates the cloud management system&#8217;s concept of the logical network configuration, stored in the CMS configuration database in a  CMS-specific  format, into an intermediate representation understood by OVN.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OVN Northbound database (<literal>nbdb</literal>) container</emphasis> - Stores the logical network configuration passed by the CMS plugin.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OVN Southbound database (<literal>sbdb</literal>) container</emphasis> - Stores the physical and logical network configuration state for Open vSwitch (OVS) system on each node, including tables that bind them.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OVN north daemon (<literal>ovn-northd</literal>)</emphasis> - This is the intermediary client between <literal>nbdb</literal> container and <literal>sbdb</literal> container. It translates  the logical network configuration in terms of conventional network concepts, taken from the <literal>nbdb</literal> container, into  logical data path flows in the <literal>sbdb</literal> container. The container name for <literal>ovn-northd</literal> daemon is <literal>northd</literal> and it runs in the <literal>ovnkube-node</literal> pods.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">ovn-controller</emphasis> - This is the OVN agent that interacts with OVS and hypervisors, for any information or update that is needed for <literal>sbdb</literal> container. The <literal>ovn-controller</literal> reads logical flows from the <literal>sbdb</literal> container, translates them into <literal>OpenFlow</literal> flows and sends them to the node’s OVS daemon. The container name is <literal>ovn-controller</literal> and it runs in the <literal>ovnkube-node</literal> pods.</simpara>
</listitem>
</itemizedlist>
<simpara>The OVN northd, northbound database, and southbound database run on each node in the cluster and mostly contain and process information that is local to that node.</simpara>
<simpara>The OVN northbound database has the logical network configuration passed down to it by the cloud management system (CMS).
The OVN northbound database contains the current desired state of the network, presented as a collection of logical ports, logical switches, logical routers, and more.
The <literal>ovn-northd</literal> (<literal>northd</literal> container) connects to the OVN northbound database and the OVN southbound database.
It translates the logical network configuration in terms of conventional network concepts, taken from the OVN northbound database, into logical data path flows in the OVN southbound database.</simpara>
<simpara>The OVN southbound database has physical and logical representations of the network and binding tables that link them together. It contains the chassis information of the node and other constructs like remote transit switch ports that are required to connect to the other nodes in the cluster. The OVN southbound database also contains all the logic flows. The logic flows are shared with the <literal>ovn-controller</literal> process that runs on each node and the <literal>ovn-controller</literal> turns those into <literal>OpenFlow</literal> rules to program <literal>Open vSwitch</literal>(OVS).</simpara>
<simpara>The Kubernetes control plane nodes each contain an <literal>ovnkube-control-plane</literal> pod which does the central IP address management (IPAM) allocation for each node in the cluster. At any given time a single <literal>ovnkube-control-plane</literal> pod is the leader.</simpara>
</section>
<section xml:id="nw-ovn-kubernetes-list-resources_ovn-kubernetes-architecture">
<title>Listing all resources in the OVN-Kubernetes project</title>
<simpara>Finding the resources and containers that run in the OVN-Kubernetes project is important to help you understand the OVN-Kubernetes networking implementation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>The OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to get all resources, endpoints, and <literal>ConfigMaps</literal> in the OVN-Kubernetes project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all,ep,cm -n openshift-ovn-kubernetes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Warning: apps.openshift.io/v1 DeploymentConfig is deprecated in v4.14+, unavailable in v4.10000+
NAME                                         READY   STATUS    RESTARTS       AGE
pod/ovnkube-control-plane-65c6f55656-6d55h   2/2     Running   0              114m
pod/ovnkube-control-plane-65c6f55656-fd7vw   2/2     Running   2 (104m ago)   114m
pod/ovnkube-control-plane-65c6f55656-vtqtm   2/2     Running   0              114m
pod/ovnkube-node-bcvts                       8/8     Running   0              113m
pod/ovnkube-node-drgvv                       8/8     Running   0              113m
pod/ovnkube-node-f2pxt                       8/8     Running   0              113m
pod/ovnkube-node-frqsb                       8/8     Running   0              105m
pod/ovnkube-node-lbxkk                       8/8     Running   0              105m
pod/ovnkube-node-tt7bx                       8/8     Running   1 (102m ago)   105m

NAME                                   TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)             AGE
service/ovn-kubernetes-control-plane   ClusterIP   None         &lt;none&gt;        9108/TCP            114m
service/ovn-kubernetes-node            ClusterIP   None         &lt;none&gt;        9103/TCP,9105/TCP   114m

NAME                          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR                 AGE
daemonset.apps/ovnkube-node   6         6         6       6            6           beta.kubernetes.io/os=linux   114m

NAME                                    READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/ovnkube-control-plane   3/3     3            3           114m

NAME                                               DESIRED   CURRENT   READY   AGE
replicaset.apps/ovnkube-control-plane-65c6f55656   3         3         3       114m

NAME                                     ENDPOINTS                                               AGE
endpoints/ovn-kubernetes-control-plane   10.0.0.3:9108,10.0.0.4:9108,10.0.0.5:9108               114m
endpoints/ovn-kubernetes-node            10.0.0.3:9105,10.0.0.4:9105,10.0.0.5:9105 + 9 more...   114m

NAME                                 DATA   AGE
configmap/control-plane-status       1      113m
configmap/kube-root-ca.crt           1      114m
configmap/openshift-service-ca.crt   1      114m
configmap/ovn-ca                     1      114m
configmap/ovnkube-config             1      114m
configmap/signer-ca                  1      114m</programlisting>
</para>
</formalpara>
<simpara>There is one <literal>ovnkube-node</literal> pod for each node in the cluster.
The <literal>ovnkube-config</literal> config map has the OpenShift Container Platform OVN-Kubernetes configurations.</simpara>
</listitem>
<listitem>
<simpara>List all of the containers in the <literal>ovnkube-node</literal> pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods ovnkube-node-bcvts -o jsonpath='{.spec.containers[*].name}' -n openshift-ovn-kubernetes</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovn-controller ovn-acl-logging kube-rbac-proxy-node kube-rbac-proxy-ovn-metrics northd nbdb sbdb ovnkube-controller</programlisting>
</para>
</formalpara>
<simpara>The <literal>ovnkube-node</literal> pod is made up of several containers. It is responsible for hosting the northbound database (<literal>nbdb</literal> container), the southbound database (<literal>sbdb</literal> container), the north daemon (<literal>northd</literal> container), <literal>ovn-controller</literal> and the <literal>ovnkube-controller`container. The `ovnkube-controller</literal> container watches for API objects like pods, egress IPs, namespaces, services, endpoints, egress firewall, and network policies. It is also responsible for allocating pod IP from the available subnet pool for that node.</simpara>
</listitem>
<listitem>
<simpara>List all the containers in the <literal>ovnkube-control-plane</literal> pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods ovnkube-control-plane-65c6f55656-6d55h -o jsonpath='{.spec.containers[*].name}' -n openshift-ovn-kubernetes</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kube-rbac-proxy ovnkube-cluster-manager</programlisting>
</para>
</formalpara>
<simpara>The <literal>ovnkube-control-plane</literal> pod has a container (<literal>ovnkube-cluster-manager</literal>) that resides on each OpenShift Container Platform node. The <literal>ovnkube-cluster-manager</literal> container allocates pod subnet, transit switch subnet IP and join switch subnet IP to each node in the cluster. The <literal>kube-rbac-proxy</literal> container monitors metrics for the <literal>ovnkube-cluster-manager</literal> container.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-list-database-contents_ovn-kubernetes-architecture">
<title>Listing the OVN-Kubernetes northbound database contents</title>
<simpara>Each node is controlled by the <literal>ovnkube-controller</literal> container running in the <literal>ovnkube-node</literal> pod on that node. To understand the OVN logical networking entities you need to examine the northbound database that is running as a container inside the <literal>ovnkube-node</literal> pod on that node to see what objects are in the node you wish to see.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>The OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
</itemizedlist>
<note>
<title>Procedure</title>
<simpara>To run ovn <literal>nbctl</literal> or <literal>sbctl</literal> commands in a cluster you must open a remote shell into the <literal>nbdb</literal> or <literal>sbdb</literal> containers on the relevant node</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>List pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get po -n openshift-ovn-kubernetes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                     READY   STATUS    RESTARTS      AGE
ovnkube-control-plane-8444dff7f9-4lh9k   2/2     Running   0             27m
ovnkube-control-plane-8444dff7f9-5rjh9   2/2     Running   0             27m
ovnkube-control-plane-8444dff7f9-k64b7   2/2     Running   2 (11m ago)   27m
ovnkube-node-55xs2                       8/8     Running   0             26m
ovnkube-node-7r84r                       8/8     Running   0             16m
ovnkube-node-bqq8p                       8/8     Running   0             17m
ovnkube-node-mkj4f                       8/8     Running   0             26m
ovnkube-node-mlr8k                       8/8     Running   0             26m
ovnkube-node-wqn2m                       8/8     Running   0             16m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: To list the pods with node information, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ovn-kubernetes -owide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                     READY   STATUS    RESTARTS      AGE   IP           NODE                                       NOMINATED NODE   READINESS GATES
ovnkube-control-plane-8444dff7f9-4lh9k   2/2     Running   0             27m   10.0.0.3     ci-ln-t487nnb-72292-mdcnq-master-1         &lt;none&gt;           &lt;none&gt;
ovnkube-control-plane-8444dff7f9-5rjh9   2/2     Running   0             27m   10.0.0.4     ci-ln-t487nnb-72292-mdcnq-master-2         &lt;none&gt;           &lt;none&gt;
ovnkube-control-plane-8444dff7f9-k64b7   2/2     Running   2 (12m ago)   27m   10.0.0.5     ci-ln-t487nnb-72292-mdcnq-master-0         &lt;none&gt;           &lt;none&gt;
ovnkube-node-55xs2                       8/8     Running   0             26m   10.0.0.4     ci-ln-t487nnb-72292-mdcnq-master-2         &lt;none&gt;           &lt;none&gt;
ovnkube-node-7r84r                       8/8     Running   0             17m   10.0.128.3   ci-ln-t487nnb-72292-mdcnq-worker-b-wbz7z   &lt;none&gt;           &lt;none&gt;
ovnkube-node-bqq8p                       8/8     Running   0             17m   10.0.128.2   ci-ln-t487nnb-72292-mdcnq-worker-a-lh7ms   &lt;none&gt;           &lt;none&gt;
ovnkube-node-mkj4f                       8/8     Running   0             27m   10.0.0.5     ci-ln-t487nnb-72292-mdcnq-master-0         &lt;none&gt;           &lt;none&gt;
ovnkube-node-mlr8k                       8/8     Running   0             27m   10.0.0.3     ci-ln-t487nnb-72292-mdcnq-master-1         &lt;none&gt;           &lt;none&gt;
ovnkube-node-wqn2m                       8/8     Running   0             17m   10.0.128.4   ci-ln-t487nnb-72292-mdcnq-worker-c-przlm   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Navigate into a pod to look at the northbound database by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -c nbdb -n openshift-ovn-kubernetes ovnkube-node-55xs2</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to show all the objects in the northbound database:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ovn-nbctl show</programlisting>
<simpara>The output is too long to list here. The list includes the NAT rules, logical switches, load balancers and so on.</simpara>
<simpara>You can narrow down and focus on specific components by using some of the following optional commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to show the list of logical routers:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-node-55xs2 \
-c northd -- ovn-nbctl lr-list</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">45339f4f-7d0b-41d0-b5f9-9fca9ce40ce6 (GR_ci-ln-t487nnb-72292-mdcnq-master-2)
96a0a0f0-e7ed-4fec-8393-3195563de1b8 (ovn_cluster_router)</programlisting>
</para>
</formalpara>
<note>
<simpara>From this output you can see there is router on each node plus an <literal>ovn_cluster_router</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Run the following command to show the list of logical switches:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-node-55xs2 \
-c nbdb -- ovn-nbctl ls-list</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">bdd7dc3d-d848-4a74-b293-cc15128ea614 (ci-ln-t487nnb-72292-mdcnq-master-2)
b349292d-ee03-4914-935f-1940b6cb91e5 (ext_ci-ln-t487nnb-72292-mdcnq-master-2)
0aac0754-ea32-4e33-b086-35eeabf0a140 (join)
992509d7-2c3f-4432-88db-c179e43592e5 (transit_switch)</programlisting>
</para>
</formalpara>
<note>
<simpara>From this output you can see there is an ext switch for each node plus switches with the node name itself and a join switch.</simpara>
</note>
</listitem>
<listitem>
<simpara>Run the following command to show the list of load balancers:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-node-55xs2 \
-c nbdb -- ovn-nbctl lb-list</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">UUID                                    LB                  PROTO      VIP                     IPs
7c84c673-ed2a-4436-9a1f-9bc5dd181eea    Service_default/    tcp        172.30.0.1:443          10.0.0.3:6443,169.254.169.2:6443,10.0.0.5:6443
4d663fd9-ddc8-4271-b333-4c0e279e20bb    Service_default/    tcp        172.30.0.1:443          10.0.0.3:6443,10.0.0.4:6443,10.0.0.5:6443
292eb07f-b82f-4962-868a-4f541d250bca    Service_openshif    tcp        172.30.105.247:443      10.129.0.12:8443
034b5a7f-bb6a-45e9-8e6d-573a82dc5ee3    Service_openshif    tcp        172.30.192.38:443       10.0.0.3:10259,10.0.0.4:10259,10.0.0.5:10259
a68bb53e-be84-48df-bd38-bdd82fcd4026    Service_openshif    tcp        172.30.161.125:8443     10.129.0.32:8443
6cc21b3d-2c54-4c94-8ff5-d8e017269c2e    Service_openshif    tcp        172.30.3.144:443        10.129.0.22:8443
37996ffd-7268-4862-a27f-61cd62e09c32    Service_openshif    tcp        172.30.181.107:443      10.129.0.18:8443
81d4da3c-f811-411f-ae0c-bc6713d0861d    Service_openshif    tcp        172.30.228.23:443       10.129.0.29:8443
ac5a4f3b-b6ba-4ceb-82d0-d84f2c41306e    Service_openshif    tcp        172.30.14.240:9443      10.129.0.36:9443
c88979fb-1ef5-414b-90ac-43b579351ac9    Service_openshif    tcp        172.30.231.192:9001     10.128.0.5:9001,10.128.2.5:9001,10.129.0.5:9001,10.129.2.4:9001,10.130.0.3:9001,10.131.0.3:9001
fcb0a3fb-4a77-4230-a84a-be45dce757e8    Service_openshif    tcp        172.30.189.92:443       10.130.0.17:8440
67ef3e7b-ceb9-4bf0-8d96-b43bde4c9151    Service_openshif    tcp        172.30.67.218:443       10.129.0.9:8443
d0032fba-7d5e-424a-af25-4ab9b5d46e81    Service_openshif    tcp        172.30.102.137:2379     10.0.0.3:2379,10.0.0.4:2379,10.0.0.5:2379
                                                            tcp        172.30.102.137:9979     10.0.0.3:9979,10.0.0.4:9979,10.0.0.5:9979
7361c537-3eec-4e6c-bc0c-0522d182abd4    Service_openshif    tcp        172.30.198.215:9001     10.0.0.3:9001,10.0.0.4:9001,10.0.0.5:9001,10.0.128.2:9001,10.0.128.3:9001,10.0.128.4:9001
0296c437-1259-410b-a6fd-81c310ad0af5    Service_openshif    tcp        172.30.198.215:9001     10.0.0.3:9001,169.254.169.2:9001,10.0.0.5:9001,10.0.128.2:9001,10.0.128.3:9001,10.0.128.4:9001
5d5679f5-45b8-479d-9f7c-08b123c688b8    Service_openshif    tcp        172.30.38.253:17698     10.128.0.52:17698,10.129.0.84:17698,10.130.0.60:17698
2adcbab4-d1c9-447d-9573-b5dc9f2efbfa    Service_openshif    tcp        172.30.148.52:443       10.0.0.4:9202,10.0.0.5:9202
                                                            tcp        172.30.148.52:444       10.0.0.4:9203,10.0.0.5:9203
                                                            tcp        172.30.148.52:445       10.0.0.4:9204,10.0.0.5:9204
                                                            tcp        172.30.148.52:446       10.0.0.4:9205,10.0.0.5:9205
2a33a6d7-af1b-4892-87cc-326a380b809b    Service_openshif    tcp        172.30.67.219:9091      10.129.2.16:9091,10.131.0.16:9091
                                                            tcp        172.30.67.219:9092      10.129.2.16:9092,10.131.0.16:9092
                                                            tcp        172.30.67.219:9093      10.129.2.16:9093,10.131.0.16:9093
                                                            tcp        172.30.67.219:9094      10.129.2.16:9094,10.131.0.16:9094
f56f59d7-231a-4974-99b3-792e2741ec8d    Service_openshif    tcp        172.30.89.212:443       10.128.0.41:8443,10.129.0.68:8443,10.130.0.44:8443
08c2c6d7-d217-4b96-b5d8-c80c4e258116    Service_openshif    tcp        172.30.102.137:2379     10.0.0.3:2379,169.254.169.2:2379,10.0.0.5:2379
                                                            tcp        172.30.102.137:9979     10.0.0.3:9979,169.254.169.2:9979,10.0.0.5:9979
60a69c56-fc6a-4de6-bd88-3f2af5ba5665    Service_openshif    tcp        172.30.10.193:443       10.129.0.25:8443
ab1ef694-0826-4671-a22c-565fc2d282ec    Service_openshif    tcp        172.30.196.123:443      10.128.0.33:8443,10.129.0.64:8443,10.130.0.37:8443
b1fb34d3-0944-4770-9ee3-2683e7a630e2    Service_openshif    tcp        172.30.158.93:8443      10.129.0.13:8443
95811c11-56e2-4877-be1e-c78ccb3a82a9    Service_openshif    tcp        172.30.46.85:9001       10.130.0.16:9001
4baba1d1-b873-4535-884c-3f6fc07a50fd    Service_openshif    tcp        172.30.28.87:443        10.129.0.26:8443
6c2e1c90-f0ca-484e-8a8e-40e71442110a    Service_openshif    udp        172.30.0.10:53          10.128.0.13:5353,10.128.2.6:5353,10.129.0.39:5353,10.129.2.6:5353,10.130.0.11:5353,10.131.0.9:5353</programlisting>
</para>
</formalpara>
<note>
<simpara>From this truncated output you can see there are many OVN-Kubernetes load balancers. Load balancers in OVN-Kubernetes are representations of services.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Run the following command to display the options available with the command <literal>ovn-nbctl</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-node-55xs2 \
-c nbdb ovn-nbctl --help</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-examine-nb-database-contents-ref_ovn-kubernetes-architecture">
<title>Command line arguments for ovn-nbctl to examine northbound database contents</title>
<simpara>The following table describes the command line arguments that can be used with <literal>ovn-nbctl</literal> to examine the contents of the northbound database.</simpara>
<note>
<simpara>Open a remote shell in the pod you want to view the contents of and then run the <literal>ovn-nbctl</literal> commands.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title>Command line arguments to examine northbound database contents</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="top">Argument</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl show</literal></simpara></entry>
<entry align="left" valign="top"><simpara>An overview of the northbound database contents as seen from a specific node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl show &lt;switch_or_router&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Show the details associated with the specified switch or router.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl lr-list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Show the logical routers.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl lrp-list &lt;router&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Using the router information from <literal>ovn-nbctl lr-list</literal> to show the router ports.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl lr-nat-list &lt;router&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Show network address translation details for the specified router.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl ls-list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Show the logical switches</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl lsp-list  &lt;switch&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Using the switch information from <literal>ovn-nbctl ls-list</literal> to show the switch port.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl lsp-get-type &lt;port&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Get the type for the logical port.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-nbctl lb-list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Show the load balancers.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-ovn-kubernetes-list-southbound-database-contents_ovn-kubernetes-architecture">
<title>Listing the OVN-Kubernetes southbound database contents</title>
<simpara>Each node is controlled by the <literal>ovnkube-controller</literal> container running in the <literal>ovnkube-node</literal> pod on that node. To understand the OVN logical networking entities you need to examine the northbound database that is running as a container inside the <literal>ovnkube-node</literal> pod on that node to see what objects are in the node you wish to see.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>The OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
</itemizedlist>
<note>
<title>Procedure</title>
<simpara>To run ovn <literal>nbctl</literal> or <literal>sbctl</literal> commands in a cluster you must open a remote shell into the <literal>nbdb</literal> or <literal>sbdb</literal> containers on the relevant node</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>List the pods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get po -n openshift-ovn-kubernetes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                     READY   STATUS    RESTARTS      AGE
ovnkube-control-plane-8444dff7f9-4lh9k   2/2     Running   0             27m
ovnkube-control-plane-8444dff7f9-5rjh9   2/2     Running   0             27m
ovnkube-control-plane-8444dff7f9-k64b7   2/2     Running   2 (11m ago)   27m
ovnkube-node-55xs2                       8/8     Running   0             26m
ovnkube-node-7r84r                       8/8     Running   0             16m
ovnkube-node-bqq8p                       8/8     Running   0             17m
ovnkube-node-mkj4f                       8/8     Running   0             26m
ovnkube-node-mlr8k                       8/8     Running   0             26m
ovnkube-node-wqn2m                       8/8     Running   0             16m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: To list the pods with node information, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ovn-kubernetes -owide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                     READY   STATUS    RESTARTS      AGE   IP           NODE                                       NOMINATED NODE   READINESS GATES
ovnkube-control-plane-8444dff7f9-4lh9k   2/2     Running   0             27m   10.0.0.3     ci-ln-t487nnb-72292-mdcnq-master-1         &lt;none&gt;           &lt;none&gt;
ovnkube-control-plane-8444dff7f9-5rjh9   2/2     Running   0             27m   10.0.0.4     ci-ln-t487nnb-72292-mdcnq-master-2         &lt;none&gt;           &lt;none&gt;
ovnkube-control-plane-8444dff7f9-k64b7   2/2     Running   2 (12m ago)   27m   10.0.0.5     ci-ln-t487nnb-72292-mdcnq-master-0         &lt;none&gt;           &lt;none&gt;
ovnkube-node-55xs2                       8/8     Running   0             26m   10.0.0.4     ci-ln-t487nnb-72292-mdcnq-master-2         &lt;none&gt;           &lt;none&gt;
ovnkube-node-7r84r                       8/8     Running   0             17m   10.0.128.3   ci-ln-t487nnb-72292-mdcnq-worker-b-wbz7z   &lt;none&gt;           &lt;none&gt;
ovnkube-node-bqq8p                       8/8     Running   0             17m   10.0.128.2   ci-ln-t487nnb-72292-mdcnq-worker-a-lh7ms   &lt;none&gt;           &lt;none&gt;
ovnkube-node-mkj4f                       8/8     Running   0             27m   10.0.0.5     ci-ln-t487nnb-72292-mdcnq-master-0         &lt;none&gt;           &lt;none&gt;
ovnkube-node-mlr8k                       8/8     Running   0             27m   10.0.0.3     ci-ln-t487nnb-72292-mdcnq-master-1         &lt;none&gt;           &lt;none&gt;
ovnkube-node-wqn2m                       8/8     Running   0             17m   10.0.128.4   ci-ln-t487nnb-72292-mdcnq-worker-c-przlm   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Navigate into a pod to look at the southbound database:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -c sbdb -n openshift-ovn-kubernetes ovnkube-node-55xs2</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to show all the objects in the southbound database:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ovn-sbctl show</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Chassis "5db31703-35e9-413b-8cdf-69e7eecb41f7"
    hostname: ci-ln-9gp362t-72292-v2p94-worker-a-8bmwz
    Encap geneve
        ip: "10.0.128.4"
        options: {csum="true"}
    Port_Binding tstor-ci-ln-9gp362t-72292-v2p94-worker-a-8bmwz
Chassis "070debed-99b7-4bce-b17d-17e720b7f8bc"
    hostname: ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Encap geneve
        ip: "10.0.128.2"
        options: {csum="true"}
    Port_Binding k8s-ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Port_Binding rtoe-GR_ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Port_Binding openshift-monitoring_alertmanager-main-1
    Port_Binding rtoj-GR_ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Port_Binding etor-GR_ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Port_Binding cr-rtos-ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Port_Binding openshift-e2e-loki_loki-promtail-qcrcz
    Port_Binding jtor-GR_ci-ln-9gp362t-72292-v2p94-worker-b-svmp6
    Port_Binding openshift-multus_network-metrics-daemon-mkd4t
    Port_Binding openshift-ingress-canary_ingress-canary-xtvj4
    Port_Binding openshift-ingress_router-default-6c76cbc498-pvlqk
    Port_Binding openshift-dns_dns-default-zz582
    Port_Binding openshift-monitoring_thanos-querier-57585899f5-lbf4f
    Port_Binding openshift-network-diagnostics_network-check-target-tn228
    Port_Binding openshift-monitoring_prometheus-k8s-0
    Port_Binding openshift-image-registry_image-registry-68899bd877-xqxjj
Chassis "179ba069-0af1-401c-b044-e5ba90f60fea"
    hostname: ci-ln-9gp362t-72292-v2p94-master-0
    Encap geneve
        ip: "10.0.0.5"
        options: {csum="true"}
    Port_Binding tstor-ci-ln-9gp362t-72292-v2p94-master-0
Chassis "68c954f2-5a76-47be-9e84-1cb13bd9dab9"
    hostname: ci-ln-9gp362t-72292-v2p94-worker-c-mjf9w
    Encap geneve
        ip: "10.0.128.3"
        options: {csum="true"}
    Port_Binding tstor-ci-ln-9gp362t-72292-v2p94-worker-c-mjf9w
Chassis "2de65d9e-9abf-4b6e-a51d-a1e038b4d8af"
    hostname: ci-ln-9gp362t-72292-v2p94-master-2
    Encap geneve
        ip: "10.0.0.4"
        options: {csum="true"}
    Port_Binding tstor-ci-ln-9gp362t-72292-v2p94-master-2
Chassis "1d371cb8-5e21-44fd-9025-c4b162cc4247"
    hostname: ci-ln-9gp362t-72292-v2p94-master-1
    Encap geneve
        ip: "10.0.0.3"
        options: {csum="true"}
    Port_Binding tstor-ci-ln-9gp362t-72292-v2p94-master-1</programlisting>
</para>
</formalpara>
<simpara>This detailed output shows the chassis and the ports that are attached to the chassis which in this case are all of the router ports and anything that runs like host networking.
Any pods communicate out to the wider network using source network address translation (SNAT).
Their IP address is translated into the IP address of the node that the pod is running on and then sent out into the network.</simpara>
<simpara>In addition to the chassis information the southbound database has all the logic flows and those logic flows are then sent to the <literal>ovn-controller</literal> running on each of the nodes.
The <literal>ovn-controller</literal> translates the logic flows into open flow rules and ultimately programs <literal>OpenvSwitch</literal> so that your pods can then follow open flow rules and make it out of the network.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to display the options available with the command <literal>ovn-sbctl</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-ovn-kubernetes -it ovnkube-node-55xs2 \
-c sbdb ovn-sbctl --help</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-examine-sb-database-contents-ref_ovn-kubernetes-architecture">
<title>Command line arguments for ovn-sbctl to examine southbound database contents</title>
<simpara>The following table describes the command line arguments that can be used with <literal>ovn-sbctl</literal> to examine the contents of the southbound database.</simpara>
<note>
<simpara>Open a remote shell in the pod you wish to view the contents of and then run the <literal>ovn-sbctl</literal> commands.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title>Command line arguments to examine southbound database contents</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="top">Argument</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-sbctl show</literal></simpara></entry>
<entry align="left" valign="top"><simpara>An overview of the southbound database contents as seen from a specific node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-sbctl list Port_Binding &lt;port&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>List the contents of southbound database for a the specified port .</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ovn-sbctl dump-flows</literal></simpara></entry>
<entry align="left" valign="top"><simpara>List the logical flows.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="ovn-kubernetes-logical-architecture-con_ovn-kubernetes-architecture">
<title>OVN-Kubernetes logical architecture</title>
<simpara>OVN is a network virtualization solution. It creates logical switches and routers. These switches and routers are interconnected to create any network topologies. When you run <literal>ovnkube-trace</literal> with the log level set to 2 or 5 the OVN-Kubernetes logical components are exposed. The following diagram shows how the routers and switches are connected in OpenShift Container Platform.</simpara>
<figure>
<title>OVN-Kubernetes router and switch components</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/299_OpenShift_OVN-Kubernetes_arch_1023_2.png"/>
</imageobject>
<textobject><phrase>OVN-Kubernetes logical architecture</phrase></textobject>
</mediaobject>
</figure>
<simpara>The key components involved in packet processing are:</simpara>
<variablelist>
<varlistentry>
<term>Gateway routers</term>
<listitem>
<simpara>Gateway routers sometimes called L3 gateway routers, are typically used between the distributed routers and the physical network. Gateway routers including their logical patch ports are bound to a physical location (not distributed), or chassis. The patch ports on this router are known as l3gateway ports in the ovn-southbound database (<literal>ovn-sbdb</literal>).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Distributed logical routers</term>
<listitem>
<simpara>Distributed logical routers and the logical switches behind them, to which virtual machines and containers attach, effectively reside on each hypervisor.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Join local switch</term>
<listitem>
<simpara>Join local switches are used to connect the distributed router and gateway routers. It reduces the number of IP addresses needed on the distributed router.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Logical switches with patch ports</term>
<listitem>
<simpara>Logical switches with patch ports are used to virtualize the network stack. They connect remote logical ports through tunnels.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Logical switches with localnet ports</term>
<listitem>
<simpara>Logical switches with localnet ports are used to connect OVN to the physical network. They connect remote logical ports by bridging the packets to directly connected physical L2 segments using localnet ports.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Patch ports</term>
<listitem>
<simpara>Patch ports represent connectivity between logical switches and logical routers and between peer logical routers. A single connection has a pair of patch ports at each such point of connectivity, one on each side.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>l3gateway ports</term>
<listitem>
<simpara>l3gateway ports are the port binding entries in the <literal>ovn-sbdb</literal> for logical patch ports used in the gateway routers. They are called l3gateway ports rather than patch ports just to portray the fact that these ports are bound to a chassis just like the gateway router itself.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>localnet ports</term>
<listitem>
<simpara>localnet ports are present on the bridged logical switches that allows a connection to a locally accessible network from each <literal>ovn-controller</literal> instance. This helps model the direct connectivity to the physical network from the logical switches. A logical switch can only have a single localnet port attached to it.</simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="nw-ovn-kubernetes-installing-network-tools_ovn-kubernetes-architecture">
<title>Installing network-tools on local host</title>
<simpara>Install <literal>network-tools</literal> on your local host to make a collection of tools available for debugging OpenShift Container Platform cluster network issues.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Clone the <literal>network-tools</literal> repository onto your workstation with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ git clone git@github.com:openshift/network-tools.git</programlisting>
</listitem>
<listitem>
<simpara>Change into the directory for the repository you just cloned:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cd network-tools</programlisting>
</listitem>
<listitem>
<simpara>Optional: List all available commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./debug-scripts/network-tools -h</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-running-network-tools_ovn-kubernetes-architecture">
<title>Running network-tools</title>
<simpara>Get information about the logical switches and routers by running <literal>network-tools</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>network-tools</literal> on local host.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open a remote shell into a pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-ovn-kubernetes ovnkube-node-2hsbt</programlisting>
</listitem>
<listitem>
<simpara>List the routers by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./debug-scripts/network-tools ovn-db-run-command ovn-nbctl lr-list</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">944a7b53-7948-4ad2-a494-82b55eeccf87 (GR_ci-ln-54932yb-72292-kd676-worker-c-rzj99)
84bd4a4c-4b0b-4a47-b0cf-a2c32709fc53 (ovn_cluster_router)</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>List the localnet ports by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./debug-scripts/network-tools ovn-db-run-command \
ovn-sbctl find Port_Binding type=localnet</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">_uuid               : d05298f5-805b-4838-9224-1211afc2f199
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : f3c2c959-743b-4037-854d-26627902597c
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : br-ex_ci-ln-54932yb-72292-kd676-worker-c-rzj99
mac                 : [unknown]
mirror_rules        : []
nat_addresses       : []
options             : {network_name=physnet}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 2
type                : localnet
up                  : false
virtual_parent      : []

[...]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>List the <literal>l3gateway</literal> ports by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./debug-scripts/network-tools ovn-db-run-command \
ovn-sbctl find Port_Binding type=l3gateway</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">_uuid               : 5207a1f3-1cf3-42f1-83e9-387bbb06b03c
additional_chassis  : []
additional_encap    : []
chassis             : ca6eb600-3a10-4372-a83e-e0d957c4cd92
datapath            : f3c2c959-743b-4037-854d-26627902597c
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : etor-GR_ci-ln-54932yb-72292-kd676-worker-c-rzj99
mac                 : ["42:01:0a:00:80:04"]
mirror_rules        : []
nat_addresses       : ["42:01:0a:00:80:04 10.0.128.4"]
options             : {l3gateway-chassis="84737c36-b383-4c83-92c5-2bd5b3c7e772", peer=rtoe-GR_ci-ln-54932yb-72292-kd676-worker-c-rzj99}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 1
type                : l3gateway
up                  : true
virtual_parent      : []

_uuid               : 6088d647-84f2-43f2-b53f-c9d379042679
additional_chassis  : []
additional_encap    : []
chassis             : ca6eb600-3a10-4372-a83e-e0d957c4cd92
datapath            : dc9cea00-d94a-41b8-bdb0-89d42d13aa2e
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : jtor-GR_ci-ln-54932yb-72292-kd676-worker-c-rzj99
mac                 : [router]
mirror_rules        : []
nat_addresses       : []
options             : {l3gateway-chassis="84737c36-b383-4c83-92c5-2bd5b3c7e772", peer=rtoj-GR_ci-ln-54932yb-72292-kd676-worker-c-rzj99}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 2
type                : l3gateway
up                  : true
virtual_parent      : []

[...]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>List the patch ports by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./debug-scripts/network-tools ovn-db-run-command \
ovn-sbctl find Port_Binding type=patch</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">_uuid               : 785fb8b6-ee5a-4792-a415-5b1cb855dac2
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : f1ddd1cc-dc0d-43b4-90ca-12651305acec
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : stor-ci-ln-54932yb-72292-kd676-worker-c-rzj99
mac                 : [router]
mirror_rules        : []
nat_addresses       : ["0a:58:0a:80:02:01 10.128.2.1 is_chassis_resident(\"cr-rtos-ci-ln-54932yb-72292-kd676-worker-c-rzj99\")"]
options             : {peer=rtos-ci-ln-54932yb-72292-kd676-worker-c-rzj99}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 1
type                : patch
up                  : false
virtual_parent      : []

_uuid               : c01ff587-21a5-40b4-8244-4cd0425e5d9a
additional_chassis  : []
additional_encap    : []
chassis             : []
datapath            : f6795586-bf92-4f84-9222-efe4ac6a7734
encap               : []
external_ids        : {}
gateway_chassis     : []
ha_chassis_group    : []
logical_port        : rtoj-ovn_cluster_router
mac                 : ["0a:58:64:40:00:01 100.64.0.1/16"]
mirror_rules        : []
nat_addresses       : []
options             : {peer=jtor-ovn_cluster_router}
parent_port         : []
port_security       : []
requested_additional_chassis: []
requested_chassis   : []
tag                 : []
tunnel_key          : 1
type                : patch
up                  : false
virtual_parent      : []
[...]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_ovn-kubernetes-architecture" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="ovn-kubernetes-tracing-using-ovntrace">Tracing Openflow with ovnkube-trace</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://www.ovn.org/support/dist-docs/ovn-architecture.7.html">OVN architecture</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://man7.org/linux/man-pages/man8/ovn-nbctl.8.html">ovn-nbctl linux manual page</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://man7.org/linux/man-pages/man8/ovn-sbctl.8.html">ovn-sbctl linux manual page</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="ovn-kubernetes-troubleshooting-sources">
<title>Troubleshooting OVN-Kubernetes</title>

<simpara>OVN-Kubernetes has many sources of built-in health checks and logs.</simpara>
<section xml:id="nw-ovn-kubernetes-readiness-probes_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Monitoring OVN-Kubernetes health by using readiness probes</title>
<simpara>The <literal>ovnkube-control-plane</literal> and <literal>ovnkube-node</literal> pods have containers configured with readiness probes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>jq</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Review the details of the <literal>ovnkube-node</literal> readiness probe by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node \
-o json | jq '.items[0].spec.containers[] | .name,.readinessProbe'</programlisting>
<simpara>The readiness probe for the northbound and southbound database containers in the <literal>ovnkube-node</literal> pod checks for the health of the databases and the <literal>ovnkube-controller</literal> container.</simpara>
<simpara>The <literal>ovnkube-node</literal> container in the <literal>ovnkube-node</literal> pod has a readiness probe to verify the presence of the OVN-Kubernetes CNI configuration file, the absence of which would indicate that the pod is not running or is not ready to accept requests to configure pods.</simpara>
</listitem>
<listitem>
<simpara>Show all events including the probe failures, for the namespace by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get events -n openshift-ovn-kubernetes</programlisting>
</listitem>
<listitem>
<simpara>Show the events for just a specific pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pod ovnkube-node-9lqfk -n openshift-ovn-kubernetes</programlisting>
</listitem>
<listitem>
<simpara>Show the messages and statuses from the cluster network operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get co/network -o json | jq '.status.conditions[]'</programlisting>
</listitem>
<listitem>
<simpara>Show the <literal>ready</literal> status of each container in <literal>ovnkube-node</literal> pods by running the following script:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for p in $(oc get pods --selector app=ovnkube-node -n openshift-ovn-kubernetes \
-o jsonpath='{range.items[*]}{" "}{.metadata.name}'); do echo === $p ===;  \
oc get pods -n openshift-ovn-kubernetes $p -o json | jq '.status.containerStatuses[] | .name, .ready'; \
done</programlisting>
<note>
<simpara>The expectation is all container statuses are reporting as <literal>true</literal>. Failure of a readiness probe sets the status to <literal>false</literal>.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#application-health">Monitoring application health by using health checks</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ovn-kubernetes-alerts-console_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Viewing OVN-Kubernetes alerts in the console</title>
<simpara>The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure (UI)</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis>. The three main pages in the Alerting UI in this perspective are the <emphasis role="strong">Alerts</emphasis>, <emphasis role="strong">Silences</emphasis>, and <emphasis role="strong">Alerting Rules</emphasis> pages.</simpara>
</listitem>
<listitem>
<simpara>View the rules for OVN-Kubernetes alerts by selecting <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Alerting Rules</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-alerts-cli_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Viewing OVN-Kubernetes alerts in the CLI</title>
<simpara>You can get information about alerts and their governing alerting rules and silences from the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>The OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>jq</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View active or firing alerts by running the following commands.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Set the alert manager route environment variable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ALERT_MANAGER=$(oc get route alertmanager-main -n openshift-monitoring \
-o jsonpath='{@.spec.host}')</programlisting>
</listitem>
<listitem>
<simpara>Issue a <literal>curl</literal> request to the alert manager route API by running the following command, replacing <literal>$ALERT_MANAGER</literal> with the URL of your <literal>Alertmanager</literal> instance:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -s -k -H "Authorization: Bearer $(oc create token prometheus-k8s -n openshift-monitoring)" https://$ALERT_MANAGER/api/v1/alerts | jq '.data[] | "\(.labels.severity) \(.labels.alertname) \(.labels.pod) \(.labels.container) \(.labels.endpoint) \(.labels.instance)"'</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>View alerting rules by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -s 'http://localhost:9090/api/v1/rules' | jq '.data.groups[].rules[] | select(((.name|contains("ovn")) or (.name|contains("OVN")) or (.name|contains("Ovn")) or (.name|contains("North")) or (.name|contains("South"))) and .type=="alerting")'</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-logs-cli_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Viewing the OVN-Kubernetes logs using the CLI</title>
<simpara>You can view the logs for each of the pods in the <literal>ovnkube-master</literal> and <literal>ovnkube-node</literal> pods using the OpenShift CLI (<literal>oc</literal>).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>jq</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the log for a specific pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -f &lt;pod_name&gt; -c &lt;container_name&gt; -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>-f</literal></term>
<listitem>
<simpara>Optional: Specifies that the output follows what is being written into the logs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;pod_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the pod.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;container_name&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the name of a container. When a pod has more than one container, you must specify the container name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Specify the namespace the pod is running in.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs ovnkube-node-5dx44 -n openshift-ovn-kubernetes</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -f ovnkube-node-5dx44 -c ovnkube-controller -n openshift-ovn-kubernetes</programlisting>
<simpara>The contents of log files are printed out.</simpara>
</listitem>
<listitem>
<simpara>Examine the most recent entries in all the containers in the <literal>ovnkube-node</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for p in $(oc get pods --selector app=ovnkube-node -n openshift-ovn-kubernetes \
-o jsonpath='{range.items[*]}{" "}{.metadata.name}'); \
do echo === $p ===; for container in $(oc get pods -n openshift-ovn-kubernetes $p \
-o json | jq -r '.status.containerStatuses[] | .name');do echo ---$container---; \
oc logs -c $container $p -n openshift-ovn-kubernetes --tail=5; done; done</programlisting>
</listitem>
<listitem>
<simpara>View the last 5 lines of every log in every container in an <literal>ovnkube-node</literal> pod using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -l app=ovnkube-node -n openshift-ovn-kubernetes --all-containers --tail 5</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-logs-console_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Viewing the OVN-Kubernetes logs using the web console</title>
<simpara>You can view the logs for each of the pods in the <literal>ovnkube-master</literal> and <literal>ovnkube-node</literal> pods in the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, navigate to <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> or navigate to the pod through the resource you want to investigate.</simpara>
</listitem>
<listitem>
<simpara>Select the <literal>openshift-ovn-kubernetes</literal> project from the drop-down menu.</simpara>
</listitem>
<listitem>
<simpara>Click the name of the pod you want to investigate.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Logs</emphasis>. By default for the <literal>ovnkube-master</literal> the logs associated with the <literal>northd</literal> container are displayed.</simpara>
</listitem>
<listitem>
<simpara>Use the down-down menu to select logs for each container in turn.</simpara>
</listitem>
</orderedlist>
<section xml:id="nw-ovn-kubernetes-change-log-levels_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Changing the OVN-Kubernetes log levels</title>
<simpara>The default log level for OVN-Kubernetes is 4. To debug OVN-Kubernetes, set the log level to 5.
Follow this procedure to increase the log level of the OVN-Kubernetes to help you debug an issue.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have access to the OpenShift Container Platform web console.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to get detailed information for all pods in the OVN-Kubernetes project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get po -o wide -n openshift-ovn-kubernetes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                     READY   STATUS    RESTARTS       AGE    IP           NODE                                       NOMINATED NODE   READINESS GATES
ovnkube-control-plane-65497d4548-9ptdr   2/2     Running   2 (128m ago)   147m   10.0.0.3     ci-ln-3njdr9b-72292-5nwkp-master-0         &lt;none&gt;           &lt;none&gt;
ovnkube-control-plane-65497d4548-j6zfk   2/2     Running   0              147m   10.0.0.5     ci-ln-3njdr9b-72292-5nwkp-master-2         &lt;none&gt;           &lt;none&gt;
ovnkube-control-plane-65497d4548-k7xqt   2/2     Running   0              147m   10.0.0.4     ci-ln-3njdr9b-72292-5nwkp-master-1         &lt;none&gt;           &lt;none&gt;
ovnkube-node-5dx44                       8/8     Running   0              146m   10.0.0.3     ci-ln-3njdr9b-72292-5nwkp-master-0         &lt;none&gt;           &lt;none&gt;
ovnkube-node-dpfn4                       8/8     Running   0              146m   10.0.0.4     ci-ln-3njdr9b-72292-5nwkp-master-1         &lt;none&gt;           &lt;none&gt;
ovnkube-node-kwc9l                       8/8     Running   0              134m   10.0.128.2   ci-ln-3njdr9b-72292-5nwkp-worker-a-2fjcj   &lt;none&gt;           &lt;none&gt;
ovnkube-node-mcrhl                       8/8     Running   0              134m   10.0.128.4   ci-ln-3njdr9b-72292-5nwkp-worker-c-v9x5v   &lt;none&gt;           &lt;none&gt;
ovnkube-node-nsct4                       8/8     Running   0              146m   10.0.0.5     ci-ln-3njdr9b-72292-5nwkp-master-2         &lt;none&gt;           &lt;none&gt;
ovnkube-node-zrj9f                       8/8     Running   0              134m   10.0.128.3   ci-ln-3njdr9b-72292-5nwkp-worker-b-v78h7   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>ConfigMap</literal> file similar to the following example and use a filename such as <literal>env-overrides.yaml</literal>:</simpara>
<formalpara>
<title>Example <literal>ConfigMap</literal> file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: ConfigMap
apiVersion: v1
metadata:
  name: env-overrides
  namespace: openshift-ovn-kubernetes
data:
  ci-ln-3njdr9b-72292-5nwkp-master-0: | <co xml:id="CO171-1"/>
    # This sets the log level for the ovn-kubernetes node process:
    OVN_KUBE_LOG_LEVEL=5
    # You might also/instead want to enable debug logging for ovn-controller:
    OVN_LOG_LEVEL=dbg
  ci-ln-3njdr9b-72292-5nwkp-master-2: |
    # This sets the log level for the ovn-kubernetes node process:
    OVN_KUBE_LOG_LEVEL=5
    # You might also/instead want to enable debug logging for ovn-controller:
    OVN_LOG_LEVEL=dbg
  _master: | <co xml:id="CO171-2"/>
    # This sets the log level for the ovn-kubernetes master process as well as the ovn-dbchecker:
    OVN_KUBE_LOG_LEVEL=5
    # You might also/instead want to enable debug logging for northd, nbdb and sbdb on all masters:
    OVN_LOG_LEVEL=dbg</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO171-1">
<para>Specify the name of the node you want to set the debug log level on.</para>
</callout>
<callout arearefs="CO171-2">
<para>Specify <literal>_master</literal> to set the log levels of <literal>ovnkube-master</literal> components.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>ConfigMap</literal> file by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n openshift-ovn-kubernetes -f env-overrides.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">configmap/env-overrides.yaml created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Restart the <literal>ovnkube</literal> pods to apply the new log level by using the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod -n openshift-ovn-kubernetes \
--field-selector spec.nodeName=ci-ln-3njdr9b-72292-5nwkp-master-0 -l app=ovnkube-node</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod -n openshift-ovn-kubernetes \
--field-selector spec.nodeName=ci-ln-3njdr9b-72292-5nwkp-master-2 -l app=ovnkube-node</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod -n openshift-ovn-kubernetes -l app=ovnkube-node</programlisting>
</listitem>
<listitem>
<simpara>To verify that the `ConfigMap`file has been applied to all nodes for a specific pod, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-ovn-kubernetes --all-containers --prefix ovnkube-node-&lt;xxxx&gt; | grep -E -m 10 '(Logging config:|vconsole|DBG)'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;XXXX&gt;</literal></term>
<listitem>
<simpara>Specifies the random sequence of letters for a pod from the previous step.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">[pod/ovnkube-node-2cpjc/sbdb] + exec /usr/share/ovn/scripts/ovn-ctl --no-monitor '--ovn-sb-log=-vconsole:info -vfile:off -vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m' run_sb_ovsdb
[pod/ovnkube-node-2cpjc/ovnkube-controller] I1012 14:39:59.984506   35767 config.go:2247] Logging config: {File: CNIFile:/var/log/ovn-kubernetes/ovn-k8s-cni-overlay.log LibovsdbFile:/var/log/ovnkube/libovsdb.log Level:5 LogFileMaxSize:100 LogFileMaxBackups:5 LogFileMaxAge:0 ACLLoggingRateLimit:20}
[pod/ovnkube-node-2cpjc/northd] + exec ovn-northd --no-chdir -vconsole:info -vfile:off '-vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m' --pidfile /var/run/ovn/ovn-northd.pid --n-threads=1
[pod/ovnkube-node-2cpjc/nbdb] + exec /usr/share/ovn/scripts/ovn-ctl --no-monitor '--ovn-nb-log=-vconsole:info -vfile:off -vPATTERN:console:%D{%Y-%m-%dT%H:%M:%S.###Z}|%05N|%c%T|%p|%m' run_nb_ovsdb
[pod/ovnkube-node-2cpjc/ovn-controller] 2023-10-12T14:39:54.552Z|00002|hmap|DBG|lib/shash.c:114: 1 bucket with 6+ nodes, including 1 bucket with 6 nodes (32 nodes total across 32 buckets)
[pod/ovnkube-node-2cpjc/ovn-controller] 2023-10-12T14:39:54.553Z|00003|hmap|DBG|lib/shash.c:114: 1 bucket with 6+ nodes, including 1 bucket with 6 nodes (64 nodes total across 64 buckets)
[pod/ovnkube-node-2cpjc/ovn-controller] 2023-10-12T14:39:54.553Z|00004|hmap|DBG|lib/shash.c:114: 1 bucket with 6+ nodes, including 1 bucket with 7 nodes (32 nodes total across 32 buckets)
[pod/ovnkube-node-2cpjc/ovn-controller] 2023-10-12T14:39:54.553Z|00005|reconnect|DBG|unix:/var/run/openvswitch/db.sock: entering BACKOFF
[pod/ovnkube-node-2cpjc/ovn-controller] 2023-10-12T14:39:54.553Z|00007|reconnect|DBG|unix:/var/run/openvswitch/db.sock: entering CONNECTING
[pod/ovnkube-node-2cpjc/ovn-controller] 2023-10-12T14:39:54.553Z|00008|ovsdb_cs|DBG|unix:/var/run/openvswitch/db.sock: SERVER_SCHEMA_REQUESTED -&gt; SERVER_SCHEMA_REQUESTED at lib/ovsdb-cs.c:423</programlisting>
</para>
</formalpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Optional: Check the <literal>ConfigMap</literal> file has been applied by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">for f in $(oc -n openshift-ovn-kubernetes get po -l 'app=ovnkube-node' --no-headers -o custom-columns=N:.metadata.name) ; do echo "---- $f ----" ; oc -n openshift-ovn-kubernetes exec -c ovnkube-controller $f --  pgrep -a -f  init-ovnkube-controller | grep -P -o '^.*loglevel\s+\d' ; done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">---- ovnkube-node-2dt57 ----
60981 /usr/bin/ovnkube --init-ovnkube-controller xpst8-worker-c-vmh5n.c.openshift-qe.internal --init-node xpst8-worker-c-vmh5n.c.openshift-qe.internal --config-file=/run/ovnkube-config/ovnkube.conf --ovn-empty-lb-events --loglevel 4
---- ovnkube-node-4zznh ----
178034 /usr/bin/ovnkube --init-ovnkube-controller xpst8-master-2.c.openshift-qe.internal --init-node xpst8-master-2.c.openshift-qe.internal --config-file=/run/ovnkube-config/ovnkube.conf --ovn-empty-lb-events --loglevel 4
---- ovnkube-node-548sx ----
77499 /usr/bin/ovnkube --init-ovnkube-controller xpst8-worker-a-fjtnb.c.openshift-qe.internal --init-node xpst8-worker-a-fjtnb.c.openshift-qe.internal --config-file=/run/ovnkube-config/ovnkube.conf --ovn-empty-lb-events --loglevel 4
---- ovnkube-node-6btrf ----
73781 /usr/bin/ovnkube --init-ovnkube-controller xpst8-worker-b-p8rww.c.openshift-qe.internal --init-node xpst8-worker-b-p8rww.c.openshift-qe.internal --config-file=/run/ovnkube-config/ovnkube.conf --ovn-empty-lb-events --loglevel 4
---- ovnkube-node-fkc9r ----
130707 /usr/bin/ovnkube --init-ovnkube-controller xpst8-master-0.c.openshift-qe.internal --init-node xpst8-master-0.c.openshift-qe.internal --config-file=/run/ovnkube-config/ovnkube.conf --ovn-empty-lb-events --loglevel 5
---- ovnkube-node-tk9l4 ----
181328 /usr/bin/ovnkube --init-ovnkube-controller xpst8-master-1.c.openshift-qe.internal --init-node xpst8-master-1.c.openshift-qe.internal --config-file=/run/ovnkube-config/ovnkube.conf --ovn-empty-lb-events --loglevel 4</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-ovn-kubernetes-pod-connectivity-checks_ovn-kubernetes-sources-of-troubleshooting-information">
<title>Checking the OVN-Kubernetes pod network connectivity</title>
<simpara>The connectivity check controller, in OpenShift Container Platform 4.10 and later, orchestrates connection verification checks in your cluster. These include Kubernetes API, OpenShift API and individual nodes. The results for the connection tests are stored in <literal>PodNetworkConnectivity</literal> objects in the <literal>openshift-network-diagnostics</literal> namespace. Connection tests are performed every minute in parallel.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>jq</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To list the current <literal>PodNetworkConnectivityCheck</literal> objects, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics</programlisting>
</listitem>
<listitem>
<simpara>View the most recent success for each connection object by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics \
-o json | jq '.items[]| .spec.targetEndpoint,.status.successes[0]'</programlisting>
</listitem>
<listitem>
<simpara>View the most recent failures for each connection object by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics \
-o json | jq '.items[]| .spec.targetEndpoint,.status.failures[0]'</programlisting>
</listitem>
<listitem>
<simpara>View the most recent outages for each connection object by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get podnetworkconnectivitychecks -n openshift-network-diagnostics \
-o json | jq '.items[]| .spec.targetEndpoint,.status.outages[0]'</programlisting>
<simpara>The connectivity check controller also logs metrics from these checks into Prometheus.</simpara>
</listitem>
<listitem>
<simpara>View all the metrics by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec prometheus-k8s-0 -n openshift-monitoring -- \
promtool query instant  http://localhost:9090 \
'{component="openshift-network-diagnostics"}'</programlisting>
</listitem>
<listitem>
<simpara>View the latency between the source pod and the openshift api service for the last 5 minutes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec prometheus-k8s-0 -n openshift-monitoring -- \
promtool query instant  http://localhost:9090 \
'{component="openshift-network-diagnostics"}'</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_ovn-kubernetes-sources-of-troubleshooting-information" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-pod-network-connectivity-implementation_verifying-connectivity-endpoint">Implementation of connection health checks</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-pod-network-connectivity-verify_verifying-connectivity-endpoint">Verifying network connectivity for an endpoint</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="ovn-kubernetes-tracing-using-ovntrace">
<title>Tracing Openflow with ovnkube-trace</title>

<simpara>OVN and OVS traffic flows can be simulated in a single utility called <literal>ovnkube-trace</literal>. The <literal>ovnkube-trace</literal> utility runs <literal>ovn-trace</literal>, <literal>ovs-appctl ofproto/trace</literal> and <literal>ovn-detrace</literal> and correlates that information in a single output.</simpara>
<simpara>You can execute the <literal>ovnkube-trace</literal> binary from a dedicated container. For releases after OpenShift Container Platform 4.7, you can also copy the binary to a local host and execute it from that host.</simpara>
<section xml:id="nw-ovn-kubernetes-install-ovnkube-trace-local_ovn-kubernetes-tracing-with-ovnkube">
<title>Installing the ovnkube-trace on local host</title>
<simpara>The <literal>ovnkube-trace</literal> tool traces packet simulations for arbitrary UDP or TCP traffic between points in an OVN-Kubernetes driven OpenShift Container Platform cluster. Copy the <literal>ovnkube-trace</literal> binary to your local host making it available to run against the cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a pod variable by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  POD=$(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-control-plane -o name | head -1 | awk -F '/' '{print $NF}')</programlisting>
</listitem>
<listitem>
<simpara>Run the following command on your local host to copy the binary from the <literal>ovnkube-control-plane</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  oc cp -n openshift-ovn-kubernetes $POD:/usr/bin/ovnkube-trace -c ovnkube-cluster-manager ovnkube-trace</programlisting>
<note>
<simpara>If you are using Red Hat Enterprise Linux (RHEL) 8 to run the <literal>ovnkube-trace</literal> tool, you must copy the file <literal>/usr/lib/rhel8/ovnkube-trace</literal> to your local host.</simpara>
</note>
</listitem>
<listitem>
<simpara>Make <literal>ovnkube-trace</literal> executable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  chmod +x ovnkube-trace</programlisting>
</listitem>
<listitem>
<simpara>Display the options available with <literal>ovnkube-trace</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  ./ovnkube-trace -help</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Usage of ./ovnkube-trace:
  -addr-family string
    	Address family (ip4 or ip6) to be used for tracing (default "ip4")
  -dst string
    	dest: destination pod name
  -dst-ip string
    	destination IP address (meant for tests to external targets)
  -dst-namespace string
    	k8s namespace of dest pod (default "default")
  -dst-port string
    	dst-port: destination port (default "80")
  -kubeconfig string
    	absolute path to the kubeconfig file
  -loglevel string
    	loglevel: klog level (default "0")
  -ovn-config-namespace string
    	namespace used by ovn-config itself
  -service string
    	service: destination service name
  -skip-detrace
    	skip ovn-detrace command
  -src string
    	src: source pod name
  -src-namespace string
    	k8s namespace of source pod (default "default")
  -tcp
    	use tcp transport protocol
  -udp
    	use udp transport protocol</programlisting>
</para>
</formalpara>
<simpara>The command-line arguments supported are familiar Kubernetes constructs, such as namespaces, pods, services so you do not need to find the MAC address, the IP address of the destination nodes, or the ICMP type.</simpara>
<simpara>The log levels are:</simpara>
<itemizedlist>
<listitem>
<simpara>0 (minimal output)</simpara>
</listitem>
<listitem>
<simpara>2 (more verbose output showing results of trace commands)</simpara>
</listitem>
<listitem>
<simpara>5 (debug output)</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-kubernetes-running-ovnkube-trace_ovn-kubernetes-tracing-with-ovnkube">
<title>Running ovnkube-trace</title>
<simpara>Run <literal>ovn-trace</literal> to simulate packet forwarding within an OVN logical network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>ovnkube-trace</literal> on local host</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Example: Testing that DNS resolution works from a deployed pod</title>
<para>This example illustrates how to test the DNS resolution from a deployed pod to the core DNS pod that runs in the cluster.</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start a web service in the default namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run web --namespace=default --image=quay.io/openshifttest/nginx --labels="app=web" --expose --port=80</programlisting>
</listitem>
<listitem>
<simpara>List the pods running in the <literal>openshift-dns</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc get pods -n openshift-dns</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                  READY   STATUS    RESTARTS   AGE
dns-default-8s42x     2/2     Running   0          5h8m
dns-default-mdw6r     2/2     Running   0          4h58m
dns-default-p8t5h     2/2     Running   0          4h58m
dns-default-rl6nk     2/2     Running   0          5h8m
dns-default-xbgqx     2/2     Running   0          5h8m
dns-default-zv8f6     2/2     Running   0          4h58m
node-resolver-62jjb   1/1     Running   0          5h8m
node-resolver-8z4cj   1/1     Running   0          4h59m
node-resolver-bq244   1/1     Running   0          5h8m
node-resolver-hc58n   1/1     Running   0          4h59m
node-resolver-lm6z4   1/1     Running   0          5h8m
node-resolver-zfx5k   1/1     Running   0          5h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following <literal>ovnkube-trace</literal> command to verify DNS resolution is working:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./ovnkube-trace \
  -src-namespace default \ <co xml:id="CO172-1"/>
  -src web \ <co xml:id="CO172-2"/>
  -dst-namespace openshift-dns \ <co xml:id="CO172-3"/>
  -dst dns-default-p8t5h \ <co xml:id="CO172-4"/>
  -udp -dst-port 53 \ <co xml:id="CO172-5"/>
  -loglevel 0 <co xml:id="CO172-6"/></programlisting>
<calloutlist>
<callout arearefs="CO172-1">
<para>Namespace of the source pod</para>
</callout>
<callout arearefs="CO172-2">
<para>Source pod name</para>
</callout>
<callout arearefs="CO172-3">
<para>Namespace of destination pod</para>
</callout>
<callout arearefs="CO172-4">
<para>Destination pod name</para>
</callout>
<callout arearefs="CO172-5">
<para>Use the <literal>udp</literal> transport protocol. Port 53 is the port the DNS service uses.</para>
</callout>
<callout arearefs="CO172-6">
<para>Set the log level to 0 (0 is minimal and 5 is debug)</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output if the <literal>src&amp;dst</literal> pod lands on the same node:</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovn-trace source pod to destination pod indicates success from web to dns-default-p8t5h
ovn-trace destination pod to source pod indicates success from dns-default-p8t5h to web
ovs-appctl ofproto/trace source pod to destination pod indicates success from web to dns-default-p8t5h
ovs-appctl ofproto/trace destination pod to source pod indicates success from dns-default-p8t5h to web
ovn-detrace source pod to destination pod indicates success from web to dns-default-p8t5h
ovn-detrace destination pod to source pod indicates success from dns-default-p8t5h to web</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output if the <literal>src&amp;dst</literal> pod lands on a different node:</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovn-trace source pod to destination pod indicates success from web to dns-default-8s42x
ovn-trace (remote) source pod to destination pod indicates success from web to dns-default-8s42x
ovn-trace destination pod to source pod indicates success from dns-default-8s42x to web
ovn-trace (remote) destination pod to source pod indicates success from dns-default-8s42x to web
ovs-appctl ofproto/trace source pod to destination pod indicates success from web to dns-default-8s42x
ovs-appctl ofproto/trace destination pod to source pod indicates success from dns-default-8s42x to web
ovn-detrace source pod to destination pod indicates success from web to dns-default-8s42x
ovn-detrace destination pod to source pod indicates success from dns-default-8s42x to web</programlisting>
</para>
</formalpara>
<simpara>The ouput indicates success from the deployed pod to the DNS port and also indicates that it is
successful going back in the other direction. So you know bi-directional traffic is supported on UDP port 53 if my web pod wants to do dns resolution from core DNS.</simpara>
</listitem>
</orderedlist>
<simpara>If for example that did not work and you wanted to get the <literal>ovn-trace</literal>, the <literal>ovs-appctl</literal> of <literal>proto/trace</literal> and <literal>ovn-detrace</literal>, and more debug type information increase the log level to 2 and run the command again as follows:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./ovnkube-trace \
  -src-namespace default \
  -src web \
  -dst-namespace openshift-dns \
  -dst dns-default-467qw \
  -udp -dst-port 53 \
  -loglevel 2</programlisting>
<simpara>The output from this increased log level is too much to list here. In a failure situation the output of this command shows which flow is dropping that traffic. For example an egress or ingress network policy may be configured on the cluster that does not allow that traffic.</simpara>
<formalpara>
<title>Example: Verifying by using debug output a configured default deny</title>
<para>This example illustrates how to identify by using the debug output that an ingress default deny policy blocks traffic.</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following YAML that defines a <literal>deny-by-default</literal> policy to deny ingress from all pods in all namespaces. Save the YAML in the <literal>deny-by-default.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
  namespace: default
spec:
  podSelector: {}
  ingress: []</programlisting>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f deny-by-default.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/deny-by-default created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Start a web service in the <literal>default</literal> namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run web --namespace=default --image=quay.io/openshifttest/nginx --labels="app=web" --expose --port=80</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to create the <literal>prod</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace prod</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to label the <literal>prod</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace/prod purpose=production</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to deploy an <literal>alpine</literal> image in the <literal>prod</literal> namespace and start a shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc run test-6459 --namespace=prod --rm -i -t --image=alpine -- sh</programlisting>
</listitem>
<listitem>
<simpara>Open another terminal session.</simpara>
</listitem>
<listitem>
<simpara>In this new terminal session run <literal>ovn-trace</literal> to verify the failure in communication between the source pod <literal>test-6459</literal> running in namespace <literal>prod</literal> and destination pod running in the <literal>default</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./ovnkube-trace \
 -src-namespace prod \
 -src test-6459 \
 -dst-namespace default \
 -dst web \
 -tcp -dst-port 80 \
 -loglevel 0</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovn-trace source pod to destination pod indicates failure from test-6459 to web</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Increase the log level to 2 to expose the reason for the failure by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./ovnkube-trace \
 -src-namespace prod \
 -src test-6459 \
 -dst-namespace default \
 -dst web \
 -tcp -dst-port 80 \
 -loglevel 2</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
------------------------------------------------
 3. ls_out_acl_hint (northd.c:7454): !ct.new &amp;&amp; ct.est &amp;&amp; !ct.rpl &amp;&amp; ct_mark.blocked == 0, priority 4, uuid 12efc456
    reg0[8] = 1;
    reg0[10] = 1;
    next;
 5. ls_out_acl_action (northd.c:7835): reg8[30..31] == 0, priority 500, uuid 69372c5d
    reg8[30..31] = 1;
    next(4);
 5. ls_out_acl_action (northd.c:7835): reg8[30..31] == 1, priority 500, uuid 2fa0af89
    reg8[30..31] = 2;
    next(4);
 4. ls_out_acl_eval (northd.c:7691): reg8[30..31] == 2 &amp;&amp; reg0[10] == 1 &amp;&amp; (outport == @a16982411286042166782_ingressDefaultDeny), priority 2000, uuid 447d0dab
    reg8[17] = 1;
    ct_commit { ct_mark.blocked = 1; }; <co xml:id="CO173-1"/>
    next;
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO173-1">
<para>Ingress traffic is blocked due to the default deny policy being in place.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a policy that allows traffic from all pods in a particular namespaces with a label <literal>purpose=production</literal>. Save the YAML in the <literal>web-allow-prod.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: web-allow-prod
  namespace: default
spec:
  podSelector:
    matchLabels:
      app: web
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          purpose: production</programlisting>
</listitem>
<listitem>
<simpara>Apply the policy by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f web-allow-prod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Run <literal>ovnkube-trace</literal> to verify that traffic is now allowed by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./ovnkube-trace \
 -src-namespace prod \
 -src test-6459 \
 -dst-namespace default \
 -dst web \
 -tcp -dst-port 80 \
 -loglevel 0</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovn-trace source pod to destination pod indicates success from test-6459 to web
ovn-trace destination pod to source pod indicates success from web to test-6459
ovs-appctl ofproto/trace source pod to destination pod indicates success from test-6459 to web
ovs-appctl ofproto/trace destination pod to source pod indicates success from web to test-6459
ovn-detrace source pod to destination pod indicates success from test-6459 to web
ovn-detrace destination pod to source pod indicates success from web to test-6459</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command in the shell that was opened in step six to connect nginx to the web-server:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> wget -qO- --timeout=2 http://web.default</programlisting>
<formalpara>
<title>Expected output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">&lt;!DOCTYPE html&gt;
&lt;html&gt;
&lt;head&gt;
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
  body {
    width: 35em;
    margin: 0 auto;
    font-family: Tahoma, Verdana, Arial, sans-serif;
  }
&lt;/style&gt;
&lt;/head&gt;
&lt;body&gt;
&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;

&lt;p&gt;For online documentation and support please refer to
&lt;a href="http://nginx.org/"&gt;nginx.org&lt;/a&gt;.&lt;br/&gt;
Commercial support is available at
&lt;a href="http://nginx.com/"&gt;nginx.com&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;
&lt;/body&gt;
&lt;/html&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_ovn-kubernetes-tracing-with-ovnkube" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/5887511">Tracing Openflow with ovnkube-trace utility</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/ovn-org/ovn-kubernetes/blob/master/docs/ovnkube-trace.md">ovnkube-trace</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migrate-from-openshift-sdn">
<title>Migrating from the OpenShift SDN network plugin</title>

<simpara>As a cluster administrator, you can migrate to the OVN-Kubernetes network plugin from the OpenShift SDN network plugin.</simpara>
<simpara>To learn more about OVN-Kubernetes, read <link xlink:href="../../networking/ovn_kubernetes_network_provider/about-ovn-kubernetes.xml#about-ovn-kubernetes">About the OVN-Kubernetes network plugin</link>.</simpara>
<section xml:id="nw-ovn-kubernetes-migration-about_migrate-from-openshift-sdn">
<title>Migration to the OVN-Kubernetes network plugin</title>
<simpara>Migrating to the OVN-Kubernetes network plugin is a manual process that includes some downtime during which your cluster is unreachable. Although a rollback procedure is provided, the migration is intended to be a one-way process.</simpara>
<simpara>A migration to the OVN-Kubernetes network plugin is supported on the following platforms:</simpara>
<itemizedlist>
<listitem>
<simpara>Bare metal hardware</simpara>
</listitem>
<listitem>
<simpara>Amazon Web Services (AWS)</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Platform (GCP)</simpara>
</listitem>
<listitem>
<simpara>IBM Cloud&#174;</simpara>
</listitem>
<listitem>
<simpara>Microsoft Azure</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenStack Platform (RHOSP)</simpara>
</listitem>
<listitem>
<simpara>VMware vSphere</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Migrating to or from the OVN-Kubernetes network plugin is not supported for managed OpenShift cloud services such as Red Hat OpenShift Dedicated, Azure Red Hat OpenShift(ARO), and Red Hat OpenShift Service on AWS (ROSA).</simpara>
<simpara>Migrating from OpenShift SDN network plugin to OVN-Kubernetes network plugin is not supported on Nutanix.</simpara>
</important>
<section xml:id="considerations-migrating-ovn-kubernetes-network-provider_migrate-from-openshift-sdn">
<title>Considerations for migrating to the OVN-Kubernetes network plugin</title>
<simpara>If you have more than 150 nodes in your OpenShift Container Platform cluster, then open a support case for consultation on your migration to the OVN-Kubernetes network plugin.</simpara>
<simpara>The subnets assigned to nodes and the IP addresses assigned to individual pods are not preserved during the migration.</simpara>
<simpara>While the OVN-Kubernetes network plugin implements many of the capabilities present in the OpenShift SDN network plugin, the configuration is not the same.</simpara>
<itemizedlist>
<listitem>
<simpara>If your cluster uses any of the following OpenShift SDN network plugin capabilities, you must manually configure the same capability in the OVN-Kubernetes network plugin:</simpara>
<itemizedlist>
<listitem>
<simpara>Namespace isolation</simpara>
</listitem>
<listitem>
<simpara>Egress router pods</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If your cluster or surrounding network uses any part of the <literal>100.64.0.0/16</literal> address range, you must choose another unused IP range by specifying the <literal>v4InternalSubnet</literal> spec under the <literal>spec.defaultNetwork.ovnKubernetesConfig</literal> object definition. OVN-Kubernetes uses the IP range <literal>100.64.0.0/16</literal> internally by default.</simpara>
</listitem>
</itemizedlist>
<simpara>The following sections highlight the differences in configuration between the aforementioned capabilities in OVN-Kubernetes and OpenShift SDN network plugins.</simpara>
<bridgehead xml:id="namespace-isolation_migrate-from-openshift-sdn" renderas="sect5">Namespace isolation</bridgehead>
<simpara>OVN-Kubernetes supports only the network policy isolation mode.</simpara>
<important>
<simpara>If your cluster uses OpenShift SDN configured in either the multitenant or subnet isolation modes, you cannot migrate to the OVN-Kubernetes network plugin.</simpara>
</important>
<bridgehead xml:id="egress-ip-addresses_migrate-from-openshift-sdn" renderas="sect5">Egress IP addresses</bridgehead>
<simpara>OpenShift SDN supports two different Egress IP modes:</simpara>
<itemizedlist>
<listitem>
<simpara>In the <emphasis>automatically assigned</emphasis> approach, an egress IP address range is assigned to a node.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis>manually assigned</emphasis> approach, a list of one or more egress IP addresses is assigned to a node.</simpara>
</listitem>
</itemizedlist>
<simpara>The migration process supports migrating Egress IP configurations that use the automatically assigned mode.</simpara>
<simpara>The differences in configuring an egress IP address between OVN-Kubernetes and OpenShift SDN is described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Differences in egress IP address configuration</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">OVN-Kubernetes</entry>
<entry align="left" valign="top">OpenShift SDN</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Create an <literal>EgressIPs</literal> object</simpara>
</listitem>
<listitem>
<simpara>Add an annotation on a <literal>Node</literal> object</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Patch a <literal>NetNamespace</literal> object</simpara>
</listitem>
<listitem>
<simpara>Patch a <literal>HostSubnet</literal> object</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For more information on using egress IP addresses in OVN-Kubernetes, see "Configuring an egress IP address".</simpara>
<bridgehead xml:id="egress-network-policies_migrate-from-openshift-sdn" renderas="sect5">Egress network policies</bridgehead>
<simpara>The difference in configuring an egress network policy, also known as an egress firewall, between OVN-Kubernetes and OpenShift SDN is described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Differences in egress network policy configuration</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">OVN-Kubernetes</entry>
<entry align="left" valign="top">OpenShift SDN</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Create an <literal>EgressFirewall</literal> object in a namespace</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Create an <literal>EgressNetworkPolicy</literal> object in a namespace</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>Because the name of an <literal>EgressFirewall</literal> object can only be set to <literal>default</literal>, after the migration all migrated <literal>EgressNetworkPolicy</literal> objects are named <literal>default</literal>, regardless of what the name was under OpenShift SDN.</simpara>
<simpara>If you subsequently rollback to OpenShift SDN, all <literal>EgressNetworkPolicy</literal> objects are named <literal>default</literal> as the prior name is lost.</simpara>
<simpara>For more information on using an egress firewall in OVN-Kubernetes, see "Configuring an egress firewall for a project".</simpara>
</note>
<bridgehead xml:id="egress-router-pods_migrate-from-openshift-sdn" renderas="sect5">Egress router pods</bridgehead>
<simpara>OVN-Kubernetes supports egress router pods in redirect mode. OVN-Kubernetes does not support egress router pods in HTTP proxy mode or DNS proxy mode.</simpara>
<simpara>When you deploy an egress router with the Cluster Network Operator, you cannot specify a node selector to control which node is used to host the egress router pod.</simpara>
<bridgehead xml:id="multicast_migrate-from-openshift-sdn" renderas="sect5">Multicast</bridgehead>
<simpara>The difference between enabling multicast traffic on OVN-Kubernetes and OpenShift SDN is described in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Differences in multicast configuration</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">OVN-Kubernetes</entry>
<entry align="left" valign="top">OpenShift SDN</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Add an annotation on a <literal>Namespace</literal> object</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Add an annotation on a <literal>NetNamespace</literal> object</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For more information on using multicast in OVN-Kubernetes, see "Enabling multicast for a project".</simpara>
<bridgehead xml:id="network-policies_migrate-from-openshift-sdn" renderas="sect5">Network policies</bridgehead>
<simpara>OVN-Kubernetes fully supports the Kubernetes <literal>NetworkPolicy</literal> API in the <literal>networking.k8s.io/v1</literal> API group. No changes are necessary in your network policies when migrating from OpenShift SDN.</simpara>
</section>
<section xml:id="how-the-migration-process-works_migrate-from-openshift-sdn">
<title>How the migration process works</title>
<simpara>The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Migrating to OVN-Kubernetes from OpenShift SDN</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">User-initiated steps</entry>
<entry align="left" valign="top">Migration activity</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Set the <literal>migration</literal> field of the <literal>Network.operator.openshift.io</literal> custom resource (CR) named <literal>cluster</literal> to <literal>OVNKubernetes</literal>. Make sure the <literal>migration</literal> field is <literal>null</literal> before setting it to a value.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>Cluster Network Operator (CNO)</term>
<listitem>
<simpara>Updates the status of the <literal>Network.config.openshift.io</literal> CR named <literal>cluster</literal> accordingly.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Machine Config Operator (MCO)</term>
<listitem>
<simpara>Rolls out an update to the systemd configuration necessary for OVN-Kubernetes; the MCO updates a single machine per pool at a time by default, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Update the <literal>networkType</literal> field of the <literal>Network.config.openshift.io</literal> CR.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>CNO</term>
<listitem>
<simpara>Performs the following actions:</simpara>
<itemizedlist>
<listitem>
<simpara>Destroys the OpenShift SDN control plane pods.</simpara>
</listitem>
<listitem>
<simpara>Deploys the OVN-Kubernetes control plane pods.</simpara>
</listitem>
<listitem>
<simpara>Updates the Multus objects to reflect the new network plugin.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reboot each node in the cluster.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>Cluster</term>
<listitem>
<simpara>As nodes reboot, the cluster assigns IP addresses to pods on the OVN-Kubernetes cluster network.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>If a rollback to OpenShift SDN is required, the following table describes the process.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Performing a rollback to OpenShift SDN</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">User-initiated steps</entry>
<entry align="left" valign="top">Migration activity</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Suspend the MCO to ensure that it does not interrupt the migration.</simpara></entry>
<entry align="left" valign="top"><simpara>The MCO stops.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Set the <literal>migration</literal> field of the <literal>Network.operator.openshift.io</literal> custom resource (CR) named <literal>cluster</literal> to <literal>OpenShiftSDN</literal>. Make sure the <literal>migration</literal> field is <literal>null</literal> before setting it to a value.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>CNO</term>
<listitem>
<simpara>Updates the status of the <literal>Network.config.openshift.io</literal> CR named <literal>cluster</literal> accordingly.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Update the <literal>networkType</literal> field.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>CNO</term>
<listitem>
<simpara>Performs the following actions:</simpara>
<itemizedlist>
<listitem>
<simpara>Destroys the OVN-Kubernetes control plane pods.</simpara>
</listitem>
<listitem>
<simpara>Deploys the OpenShift SDN control plane pods.</simpara>
</listitem>
<listitem>
<simpara>Updates the Multus objects to reflect the new network plugin.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reboot each node in the cluster.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>Cluster</term>
<listitem>
<simpara>As nodes reboot, the cluster assigns IP addresses to pods on the OpenShift-SDN network.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Enable the MCO after all nodes in the cluster reboot.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>MCO</term>
<listitem>
<simpara>Rolls out an update to the systemd configuration necessary for OpenShift SDN; the MCO updates a single machine per pool at a time by default, so the total time the migration takes increases with the size of the cluster.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="nw-ovn-kubernetes-migration_migrate-from-openshift-sdn">
<title>Migrating to the OVN-Kubernetes network plugin</title>
<simpara>As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes.
During the migration, you must reboot every node in your cluster.</simpara>
<important>
<simpara>While performing the migration, your cluster is unavailable and workloads might be interrupted.
Perform the migration only when an interruption in service is acceptable.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>A recent backup of the etcd database is available.</simpara>
</listitem>
<listitem>
<simpara>A reboot can be triggered manually for each node.</simpara>
</listitem>
<listitem>
<simpara>The cluster is in a known good state, without any errors.</simpara>
</listitem>
<listitem>
<simpara>On all cloud platforms after updating software, a security group rule must be in place to allow UDP packets on port <literal>6081</literal> for all nodes.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To backup the configuration for the cluster network, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get Network.config.openshift.io cluster -o yaml &gt; cluster-openshift-sdn.yaml</programlisting>
</listitem>
<listitem>
<simpara>To prepare all the nodes for the migration, set the <literal>migration</literal> field on the Cluster Network Operator configuration object by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'</programlisting>
<note>
<simpara>This step does not deploy OVN-Kubernetes immediately. Instead, specifying the <literal>migration</literal> field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster in preparation for the OVN-Kubernetes deployment.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:</simpara>
<itemizedlist>
<listitem>
<simpara>Egress IPs</simpara>
</listitem>
<listitem>
<simpara>Egress firewall</simpara>
</listitem>
<listitem>
<simpara>Multicast</simpara>
</listitem>
</itemizedlist>
<simpara>To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OVNKubernetes",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</programlisting>
<simpara>where:</simpara>
<simpara><literal>bool</literal>: Specifies whether to enable migration of the feature. The default is <literal>true</literal>.</simpara>
</listitem>
<listitem>
<simpara>Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>Maximum transmission unit (MTU). Consider the following before customizing the MTU for this optional step:</simpara>
<itemizedlist>
<listitem>
<simpara>If you use the default MTU, and you want to keep the default MTU during migration, this step can be ignored.</simpara>
</listitem>
<listitem>
<simpara>If you used a custom MTU, and you want to keep the custom MTU during migration, you must declare the custom MTU value in this step.</simpara>
</listitem>
<listitem>
<simpara>This step does not work if you want to change the MTU value during migration. Instead, you must first follow the instructions for "Changing the cluster MTU". You can then keep the custom MTU value by performing this procedure and declaring the custom MTU value in this step.</simpara>
<note>
<simpara>OpenShift-SDN and OVN-Kubernetes have different overlay overhead. MTU values should be selected by following the guidelines found on the "MTU value selection" page.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Geneve (Generic Network Virtualization Encapsulation) overlay network port</simpara>
</listitem>
<listitem>
<simpara>OVN-Kubernetes IPv4 internal subnet</simpara>
</listitem>
<listitem>
<simpara>OVN-Kubernetes IPv6 internal subnet</simpara>
</listitem>
</itemizedlist>
<simpara>To customize either of the previously noted settings, enter and customize the following command. If you do not need to change the default value, omit the key from the patch.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":&lt;mtu&gt;,
          "genevePort":&lt;port&gt;,
          "v4InternalSubnet":"&lt;ipv4_subnet&gt;",
          "v6InternalSubnet":"&lt;ipv6_subnet&gt;"
    }}}}'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>mtu</literal></term>
<listitem>
<simpara>The MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <literal>100</literal> less than the smallest node MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>port</literal></term>
<listitem>
<simpara>The UDP port for the Geneve overlay network. If a value is not specified, the default is <literal>6081</literal>. The port cannot be the same as the VXLAN port that is used by OpenShift SDN. The default value for the VXLAN port is <literal>4789</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ipv4_subnet</literal></term>
<listitem>
<simpara>An IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <literal>100.64.0.0/16</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ipv6_subnet</literal></term>
<listitem>
<simpara>An IPv6 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <literal>fd98::/48</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example patch command to update <literal>mtu</literal> field</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
<note>
<simpara>By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
<simpara>The machine config must include the following update to the systemd configuration:</simpara>
<programlisting language="plain" linenumbering="unnumbered">ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes</programlisting>
</listitem>
<listitem>
<simpara>If a node is stuck in the <literal>NotReady</literal> state, investigate the machine config daemon pod logs and resolve any errors.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>To list the pods, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n openshift-machine-config-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</programlisting>
</para>
</formalpara>
<simpara>The names for the config daemon pods are in the following format: <literal>machine-config-daemon-&lt;seq&gt;</literal>. The <literal>&lt;seq&gt;</literal> value is a random five character alphanumeric sequence.</simpara>
</listitem>
<listitem>
<simpara>Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</programlisting>
<simpara>where <literal>pod</literal> is the name of a machine config daemon pod.</simpara>
</listitem>
<listitem>
<simpara>Resolve any errors in the logs shown by the output from the previous command.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>To specify the network provider without changing the cluster network IP address block, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'</programlisting>
</listitem>
<listitem>
<simpara>To specify a different cluster network IP address block, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "&lt;cidr&gt;",
          "hostPrefix": &lt;prefix&gt;
        }
      ],
      "networkType": "OVNKubernetes"
    }
  }'</programlisting>
<simpara>where <literal>cidr</literal> is a CIDR block and <literal>prefix</literal> is the slice of the CIDR block apportioned to each node in your cluster. You cannot use any CIDR block that overlaps with the <literal>100.64.0.0/16</literal> CIDR block because the OVN-Kubernetes network provider uses this block internally.</simpara>
<important>
<simpara>You cannot change the service network address block during the migration.</simpara>
</important>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-multus rollout status daemonset/multus</programlisting>
<simpara>The name of the Multus pods is in the form of <literal>multus-&lt;xxxxx&gt;</literal> where <literal>&lt;xxxxx&gt;</literal> is a random sequence of letters. It might take several moments for the pods to restart.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:</simpara>
<itemizedlist>
<listitem>
<simpara>With the <literal>oc rsh</literal> command, you can use a bash script similar to the following:</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</programlisting>
</listitem>
<listitem>
<simpara>With the <literal>ssh</literal> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Confirm that the migration succeeded:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To confirm that the network plugin is OVN-Kubernetes, enter the following command.  The value of <literal>status.networkType</literal> must be <literal>OVNKubernetes</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>To confirm that the cluster nodes are in the <literal>Ready</literal> state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>To confirm that your pods are not in an error state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</programlisting>
<simpara>If pods on a node are in an error state, reboot that node.</simpara>
</listitem>
<listitem>
<simpara>To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get co</programlisting>
<simpara>The status of every cluster Operator must be the following: <literal>AVAILABLE="True"</literal>, <literal>PROGRESSING="False"</literal>, <literal>DEGRADED="False"</literal>. If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Complete the following steps only if the migration succeeds and your cluster is in a good state:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To remove the migration configuration from the CNO configuration object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove custom configuration for the OpenShift SDN network provider, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "openshiftSDNConfig": null } } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove the OpenShift SDN network provider namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-sdn</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="migrate-from-openshift-sdn-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-operator-configuration-parameters-for-ovn-sdn_cluster-network-operator">Configuration parameters for the OVN-Kubernetes network plugin</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#backup-etcd">Backing up etcd</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="about-network-policy">About network policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-cluster-mtu-change_changing-cluster-network-mtu">Changing the cluster MTU</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="mtu-value-selection_changing-cluster-network-mtu">MTU value selection</link></simpara>
</listitem>
<listitem>
<simpara>OVN-Kubernetes capabilities</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-egress-ips-ovn">Configuring an egress IP address</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-egress-firewall-ovn">Configuring an egress firewall for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-ovn-kubernetes-enabling-multicast">Enabling multicast for a project</link></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>OpenShift SDN capabilities</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="assigning-egress-ips">Configuring egress IPs for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-egress-firewall">Configuring an egress firewall for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="enabling-multicast">Enabling multicast for a project</link></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</link>]</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="rollback-to-openshift-sdn">
<title>Rolling back to the OpenShift SDN network provider</title>

<simpara>As a cluster administrator, you can rollback to the OpenShift SDN network plugin from the OVN-Kubernetes network plugin if the migration to OVN-Kubernetes is unsuccessful.</simpara>
<section xml:id="nw-ovn-kubernetes-rollback_rollback-to-openshift-sdn">
<title>Migrating to the OpenShift SDN network plugin</title>
<simpara>As a cluster administrator, you can migrate to the OpenShift SDN Container Network Interface (CNI) network plugin.
During the migration you must reboot every node in your cluster.</simpara>
<important>
<simpara>Rollback to OpenShift SDN if the migration to OVN-Kubernetes fails.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>A cluster installed on infrastructure configured with the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>A recent backup of the etcd database is available.</simpara>
</listitem>
<listitem>
<simpara>A reboot can be triggered manually for each node.</simpara>
</listitem>
<listitem>
<simpara>The cluster is in a known good state, without any errors.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Stop all of the machine configuration pools managed by the Machine Config Operator (MCO):</simpara>
<itemizedlist>
<listitem>
<simpara>Stop the master configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": true } }'</programlisting>
</listitem>
<listitem>
<simpara>Stop the worker machine configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec":{ "paused": true } }'</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To prepare for the migration, set the migration field to <literal>null</literal> by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</programlisting>
</listitem>
<listitem>
<simpara>To start the migration, set the network plugin back to OpenShift SDN by entering the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OpenShiftSDN" } } }'

$ oc patch Network.config.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "networkType": "OpenShiftSDN" } }'</programlisting>
</listitem>
<listitem>
<simpara>Optional: You can disable automatic migration of several OVN-Kubernetes capabilities to the OpenShift SDN equivalents:</simpara>
<itemizedlist>
<listitem>
<simpara>Egress IPs</simpara>
</listitem>
<listitem>
<simpara>Egress firewall</simpara>
</listitem>
<listitem>
<simpara>Multicast</simpara>
</listitem>
</itemizedlist>
<simpara>To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OpenShiftSDN",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</programlisting>
<simpara>where:</simpara>
<simpara><literal>bool</literal>: Specifies whether to enable migration of the feature. The default is <literal>true</literal>.</simpara>
</listitem>
<listitem>
<simpara>Optional: You can customize the following settings for OpenShift SDN to meet your network infrastructure requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>Maximum transmission unit (MTU)</simpara>
</listitem>
<listitem>
<simpara>VXLAN port</simpara>
</listitem>
</itemizedlist>
<simpara>To customize either or both of the previously noted settings, customize and enter the following command. If you do not need to change the default value, omit the key from the patch.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":&lt;mtu&gt;,
          "vxlanPort":&lt;port&gt;
    }}}}'</programlisting>
<variablelist>
<varlistentry>
<term><literal>mtu</literal></term>
<listitem>
<simpara>The MTU for the VXLAN overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <literal>50</literal> less than the smallest node MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>port</literal></term>
<listitem>
<simpara>The UDP port for the VXLAN overlay network. If a value is not specified, the default is <literal>4789</literal>. The port cannot be the same as the Geneve port that is used by OVN-Kubernetes. The default value for the Geneve port is <literal>6081</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example patch command</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":1200
    }}}}'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Wait until the Multus daemon set rollout completes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-multus rollout status daemonset/multus</programlisting>
<simpara>The name of the Multus pods is in form of <literal>multus-&lt;xxxxx&gt;</literal> where <literal>&lt;xxxxx&gt;</literal> is a random sequence of letters. It might take several moments for the pods to restart.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:</simpara>
<itemizedlist>
<listitem>
<simpara>With the <literal>oc rsh</literal> command, you can use a bash script similar to the following:</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</programlisting>
</listitem>
<listitem>
<simpara>With the <literal>ssh</literal> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After the nodes in your cluster have rebooted, start all of the machine configuration pools:</simpara>
<itemizedlist>
<listitem>
<simpara>Start the master configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": false } }'</programlisting>
</listitem>
<listitem>
<simpara>Start the worker configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec": { "paused": false } }'</programlisting>
</listitem>
</itemizedlist>
<simpara>As the MCO updates machines in each config pool, it reboots each node.</simpara>
<simpara>By default the MCO updates a single machine per pool at a time, so the time that the migration requires to complete grows with the size of the cluster.</simpara>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Confirm that the migration succeeded:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To confirm that the network plugin is OpenShift SDN, enter the following command.  The value of <literal>status.networkType</literal> must be <literal>OpenShiftSDN</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>To confirm that the cluster nodes are in the <literal>Ready</literal> state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>If a node is stuck in the <literal>NotReady</literal> state, investigate the machine config daemon pod logs and resolve any errors.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>To list the pods, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n openshift-machine-config-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</programlisting>
</para>
</formalpara>
<simpara>The names for the config daemon pods are in the following format: <literal>machine-config-daemon-&lt;seq&gt;</literal>. The <literal>&lt;seq&gt;</literal> value is a random five character alphanumeric sequence.</simpara>
</listitem>
<listitem>
<simpara>To display the pod log for each machine config daemon pod shown in the previous output, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</programlisting>
<simpara>where <literal>pod</literal> is the name of a machine config daemon pod.</simpara>
</listitem>
<listitem>
<simpara>Resolve any errors in the logs shown by the output from the previous command.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To confirm that your pods are not in an error state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</programlisting>
<simpara>If pods on a node are in an error state, reboot that node.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Complete the following steps only if the migration succeeds and your cluster is in a good state:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To remove the migration configuration from the Cluster Network Operator configuration object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove the OVN-Kubernetes configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "ovnKubernetesConfig":null } } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove the OVN-Kubernetes network provider namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-ovn-kubernetes</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="converting-to-dual-stack">
<title>Converting to IPv4/IPv6 dual-stack networking</title>

<simpara>As a cluster administrator, you can convert your IPv4 single-stack cluster to a dual-network cluster network that supports IPv4 and IPv6 address families.
After converting to dual-stack, all newly created pods are dual-stack enabled.</simpara>
<note>
<itemizedlist>
<listitem>
<simpara>While using dual-stack networking, you cannot use IPv4-mapped IPv6 addresses, such as <literal>::FFFF:198.51.100.1</literal>, where IPv6 is required.</simpara>
</listitem>
<listitem>
<simpara>A dual-stack network is supported on clusters provisioned on bare metal, IBM Power&#174;, IBM Z&#174; infrastructure, single-node OpenShift, and VMware vSphere.</simpara>
</listitem>
</itemizedlist>
</note>
<section xml:id="nw-dual-stack-convert_converting-to-dual-stack">
<title>Converting to a dual-stack cluster network</title>
<simpara>As a cluster administrator, you can convert your single-stack cluster network to a dual-stack cluster network.</simpara>
<note>
<simpara>After converting to dual-stack networking only newly created pods are assigned IPv6 addresses. Any pods created before the conversion must be recreated to receive an IPv6 address.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Your cluster uses the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>The cluster nodes have IPv6 addresses.</simpara>
</listitem>
<listitem>
<simpara>You have configured an IPv6-enabled router based on your infrastructure.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To specify IPv6 address blocks for the cluster and service networks, create a file containing the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">- op: add
  path: /spec/clusterNetwork/-
  value: <co xml:id="CO174-1"/>
    cidr: fd01::/48
    hostPrefix: 64
- op: add
  path: /spec/serviceNetwork/-
  value: fd02::/112 <co xml:id="CO174-2"/></programlisting>
<calloutlist>
<callout arearefs="CO174-1">
<para>Specify an object with the <literal>cidr</literal> and <literal>hostPrefix</literal> fields. The host prefix must be <literal>64</literal> or greater. The IPv6 CIDR prefix must be large enough to accommodate the specified host prefix.</para>
</callout>
<callout arearefs="CO174-2">
<para>Specify an IPv6 CIDR with a prefix of <literal>112</literal>. Kubernetes uses only the lowest 16 bits. For a prefix of <literal>112</literal>, IP addresses are assigned from <literal>112</literal> to <literal>128</literal> bits.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To patch the cluster network configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch network.config.openshift.io cluster \
  --type='json' --patch-file &lt;file&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>file</literal></term>
<listitem>
<simpara>Specifies the name of the file you created in the previous step.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">network.config.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>Complete the following step to verify that the cluster network recognizes the IPv6 address blocks that you specified in the previous procedure.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Display the network configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe network</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Status:
  Cluster Network:
    Cidr:               10.128.0.0/14
    Host Prefix:        23
    Cidr:               fd01::/48
    Host Prefix:        64
  Cluster Network MTU:  1400
  Network Type:         OVNKubernetes
  Service Network:
    172.30.0.0/16
    fd02::/112</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-dual-stack-convert-back-single-stack_converting-to-dual-stack">
<title>Converting to a single-stack cluster network</title>
<simpara>As a cluster administrator, you can convert your dual-stack cluster network to a single-stack cluster network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Your cluster uses the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>The cluster nodes have IPv6 addresses.</simpara>
</listitem>
<listitem>
<simpara>You have enabled dual-stack networking.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>networks.config.openshift.io</literal> custom resource (CR) by running the
following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.config.openshift.io</programlisting>
</listitem>
<listitem>
<simpara>Remove the IPv6 specific configuration that you have added to the <literal>cidr</literal> and <literal>hostPrefix</literal> fields in the previous procedure.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="logging-network-policy">
<title>Logging for egress firewall and network policy rules</title>

<simpara>As a cluster administrator, you can configure audit logging for your cluster and enable logging for one or more namespaces. OpenShift Container Platform produces audit logs for both egress firewalls and network policies.</simpara>
<note>
<simpara>Audit logging is available for only the <link linkend="about-ovn-kubernetes">OVN-Kubernetes network plugin</link>.</simpara>
</note>
<section xml:id="nw-networkpolicy-audit-concept_logging-network-policy">
<title>Audit logging</title>
<simpara>The OVN-Kubernetes network plugin uses Open Virtual Network (OVN) ACLs to manage egress firewalls and network policies. Audit logging exposes allow and deny ACL events.</simpara>
<simpara>You can configure the destination for audit logs, such as a syslog server or a UNIX domain socket.
Regardless of any additional configuration, an audit log is always saved to <literal>/var/log/ovn/acl-audit-log.log</literal> on each OVN-Kubernetes pod in the cluster.</simpara>
<simpara>Audit logging is enabled per namespace by annotating the namespace with the <literal>k8s.ovn.org/acl-logging</literal> key as in the following example:</simpara>
<formalpara>
<title>Example namespace annotation</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: Namespace
apiVersion: v1
metadata:
  name: example1
  annotations:
    k8s.ovn.org/acl-logging: |-
      {
        "deny": "info",
        "allow": "info"
      }</programlisting>
</para>
</formalpara>
<simpara>The logging format is compatible with syslog as defined by RFC5424. The syslog facility is configurable and defaults to <literal>local0</literal>. An example log entry might resemble the following:</simpara>
<formalpara>
<title>Example ACL deny log entry for a network policy</title>
<para>
<programlisting language="text" linenumbering="unnumbered">2023-11-02T16:28:54.139Z|00004|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:Ingress", verdict=drop, severity=alert, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:01,dl_dst=0a:58:0a:81:02:23,nw_src=10.131.0.39,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=62,nw_frag=no,tp_src=58496,tp_dst=8080,tcp_flags=syn
2023-11-02T16:28:55.187Z|00005|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:Ingress", verdict=drop, severity=alert, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:01,dl_dst=0a:58:0a:81:02:23,nw_src=10.131.0.39,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=62,nw_frag=no,tp_src=58496,tp_dst=8080,tcp_flags=syn
2023-11-02T16:28:57.235Z|00006|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:Ingress", verdict=drop, severity=alert, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:01,dl_dst=0a:58:0a:81:02:23,nw_src=10.131.0.39,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=62,nw_frag=no,tp_src=58496,tp_dst=8080,tcp_flags=syn</programlisting>
</para>
</formalpara>
<simpara>The following table describes namespace annotation values:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Audit logging namespace annotation</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Annotation</entry>
<entry align="left" valign="middle">Value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>k8s.ovn.org/acl-logging</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>You must specify at least one of <literal>allow</literal>, <literal>deny</literal>, or both to enable audit logging for a namespace.</simpara>
<variablelist>
<varlistentry>
<term><literal>deny</literal></term>
<listitem>
<simpara>Optional: Specify <literal>alert</literal>, <literal>warning</literal>, <literal>notice</literal>, <literal>info</literal>, or <literal>debug</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>allow</literal></term>
<listitem>
<simpara>Optional: Specify <literal>alert</literal>, <literal>warning</literal>, <literal>notice</literal>, <literal>info</literal>, or <literal>debug</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="network-policy-audit-configuration-logging-network-policy">
<title>Audit configuration</title>
<simpara>The configuration for audit logging is specified as part of the OVN-Kubernetes cluster network provider configuration. The following YAML illustrates the default values for the audit logging:</simpara>
<formalpara>
<title>Audit logging configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  defaultNetwork:
    ovnKubernetesConfig:
      policyAuditConfig:
        destination: "null"
        maxFileSize: 50
        rateLimit: 20
        syslogFacility: local0</programlisting>
</para>
</formalpara>
<simpara>The following table describes the configuration fields for audit logging.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>policyAuditConfig</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>rateLimit</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>integer</simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum number of messages to generate every second per node. The default value is <literal>20</literal> messages per second.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>maxFileSize</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>integer</simpara></entry>
<entry align="left" valign="middle"><simpara>The maximum size for the audit log in bytes. The default value is <literal>50000000</literal> or 50 MB.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>destination</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>string</simpara></entry>
<entry align="left" valign="middle"><simpara>One of the following additional audit log targets:</simpara>
<variablelist>
<varlistentry>
<term><literal>libc</literal></term>
<listitem>
<simpara>The libc <literal>syslog()</literal> function of the journald process on the host.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>udp:&lt;host&gt;:&lt;port&gt;</literal></term>
<listitem>
<simpara>A syslog server. Replace <literal>&lt;host&gt;:&lt;port&gt;</literal> with the host and port of the syslog server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>unix:&lt;file&gt;</literal></term>
<listitem>
<simpara>A Unix Domain Socket file specified by <literal>&lt;file&gt;</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>null</literal></term>
<listitem>
<simpara>Do not send the audit logs to any additional target.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>syslogFacility</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>string</simpara></entry>
<entry align="left" valign="middle"><simpara>The syslog facility, such as <literal>kern</literal>, as defined by RFC5424. The default value is <literal>local0</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-networkpolicy-audit-configure_logging-network-policy">
<title>Configuring egress firewall and network policy auditing for a cluster</title>
<simpara>As a cluster administrator, you can customize audit logging for your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To customize the audit logging configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit network.operator.openshift.io/cluster</programlisting>
<tip>
<simpara>You can alternatively customize and apply the following YAML to configure audit logging:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  defaultNetwork:
    ovnKubernetesConfig:
      policyAuditConfig:
        destination: "null"
        maxFileSize: 50
        rateLimit: 20
        syslogFacility: local0</programlisting>
</tip>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>To create a namespace with network policies complete the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a namespace for verification:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -f -
kind: Namespace
apiVersion: v1
metadata:
  name: verify-audit-logging
  annotations:
    k8s.ovn.org/acl-logging: '{ "deny": "alert", "allow": "alert" }'
EOF</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">namespace/verify-audit-logging created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create network policies for the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -n verify-audit-logging -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: deny-all
spec:
  podSelector:
    matchLabels:
  policyTypes:
  - Ingress
  - Egress
---
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-same-namespace
  namespace: verify-audit-logging
spec:
  podSelector: {}
  policyTypes:
   - Ingress
   - Egress
  ingress:
    - from:
        - podSelector: {}
  egress:
    - to:
       - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: verify-audit-logging
EOF</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">networkpolicy.networking.k8s.io/deny-all created
networkpolicy.networking.k8s.io/allow-from-same-namespace created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a pod for source traffic in the <literal>default</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -n default -f -
apiVersion: v1
kind: Pod
metadata:
  name: client
spec:
  containers:
    - name: client
      image: registry.access.redhat.com/rhel7/rhel-tools
      command: ["/bin/sh", "-c"]
      args:
        ["sleep inf"]
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create two pods in the <literal>verify-audit-logging</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for name in client server; do
cat &lt;&lt;EOF| oc create -n verify-audit-logging -f -
apiVersion: v1
kind: Pod
metadata:
  name: ${name}
spec:
  containers:
    - name: ${name}
      image: registry.access.redhat.com/rhel7/rhel-tools
      command: ["/bin/sh", "-c"]
      args:
        ["sleep inf"]
EOF
done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">pod/client created
pod/server created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To generate traffic and produce network policy audit log entries, complete the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Obtain the IP address for pod named <literal>server</literal> in the <literal>verify-audit-logging</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POD_IP=$(oc get pods server -n verify-audit-logging -o jsonpath='{.status.podIP}')</programlisting>
</listitem>
<listitem>
<simpara>Ping the IP address from the previous command from the pod named <literal>client</literal> in the <literal>default</literal> namespace and confirm that all packets are dropped:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it client -n default -- /bin/ping -c 2 $POD_IP</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">PING 10.128.2.55 (10.128.2.55) 56(84) bytes of data.

--- 10.128.2.55 ping statistics ---
2 packets transmitted, 0 received, 100% packet loss, time 2041ms</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Ping the IP address saved in the <literal>POD_IP</literal> shell environment variable from the pod named <literal>client</literal> in the <literal>verify-audit-logging</literal> namespace and confirm that all packets are allowed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it client -n verify-audit-logging -- /bin/ping -c 2 $POD_IP</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">PING 10.128.0.86 (10.128.0.86) 56(84) bytes of data.
64 bytes from 10.128.0.86: icmp_seq=1 ttl=64 time=2.21 ms
64 bytes from 10.128.0.86: icmp_seq=2 ttl=64 time=0.440 ms

--- 10.128.0.86 ping statistics ---
2 packets transmitted, 2 received, 0% packet loss, time 1001ms
rtt min/avg/max/mdev = 0.440/1.329/2.219/0.890 ms</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Display the latest entries in the network policy audit log:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for pod in $(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node --no-headers=true | awk '{ print $1 }') ; do
    oc exec -it $pod -n openshift-ovn-kubernetes -- tail -4 /var/log/ovn/acl-audit-log.log
  done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">2023-11-02T16:28:54.139Z|00004|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:Ingress", verdict=drop, severity=alert, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:01,dl_dst=0a:58:0a:81:02:23,nw_src=10.131.0.39,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=62,nw_frag=no,tp_src=58496,tp_dst=8080,tcp_flags=syn
2023-11-02T16:28:55.187Z|00005|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:Ingress", verdict=drop, severity=alert, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:01,dl_dst=0a:58:0a:81:02:23,nw_src=10.131.0.39,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=62,nw_frag=no,tp_src=58496,tp_dst=8080,tcp_flags=syn
2023-11-02T16:28:57.235Z|00006|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:Ingress", verdict=drop, severity=alert, direction=to-lport: tcp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:01,dl_dst=0a:58:0a:81:02:23,nw_src=10.131.0.39,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=62,nw_frag=no,tp_src=58496,tp_dst=8080,tcp_flags=syn
2023-11-02T16:49:57.909Z|00028|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Egress:0", verdict=allow, severity=alert, direction=from-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0
2023-11-02T16:49:57.909Z|00029|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Ingress:0", verdict=allow, severity=alert, direction=to-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0
2023-11-02T16:49:58.932Z|00030|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Egress:0", verdict=allow, severity=alert, direction=from-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0
2023-11-02T16:49:58.932Z|00031|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Ingress:0", verdict=allow, severity=alert, direction=to-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-networkpolicy-audit-enable_logging-network-policy">
<title>Enabling egress firewall and network policy audit logging for a namespace</title>
<simpara>As a cluster administrator, you can enable audit logging for a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To enable audit logging for a namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate namespace &lt;namespace&gt; \
  k8s.ovn.org/acl-logging='{ "deny": "alert", "allow": "notice" }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<tip>
<simpara>You can alternatively apply the following YAML to enable audit logging:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Namespace
apiVersion: v1
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/acl-logging: |-
      {
        "deny": "alert",
        "allow": "notice"
      }</programlisting>
</tip>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">namespace/verify-audit-logging annotated</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Display the latest entries in the audit log:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for pod in $(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node --no-headers=true | awk '{ print $1 }') ; do
    oc exec -it $pod -n openshift-ovn-kubernetes -- tail -4 /var/log/ovn/acl-audit-log.log
  done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">2023-11-02T16:49:57.909Z|00028|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Egress:0", verdict=allow, severity=alert, direction=from-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0
2023-11-02T16:49:57.909Z|00029|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Ingress:0", verdict=allow, severity=alert, direction=to-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0
2023-11-02T16:49:58.932Z|00030|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Egress:0", verdict=allow, severity=alert, direction=from-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0
2023-11-02T16:49:58.932Z|00031|acl_log(ovn_pinctrl0)|INFO|name="NP:verify-audit-logging:allow-from-same-namespace:Ingress:0", verdict=allow, severity=alert, direction=to-lport: icmp,vlan_tci=0x0000,dl_src=0a:58:0a:81:02:22,dl_dst=0a:58:0a:81:02:23,nw_src=10.129.2.34,nw_dst=10.129.2.35,nw_tos=0,nw_ecn=0,nw_ttl=64,nw_frag=no,icmp_type=8,icmp_code=0</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-networkpolicy-audit-disable_logging-network-policy">
<title>Disabling egress firewall and network policy audit logging for a namespace</title>
<simpara>As a cluster administrator, you can disable audit logging for a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To disable audit logging for a namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate --overwrite namespace &lt;namespace&gt; k8s.ovn.org/acl-logging-</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<tip>
<simpara>You can alternatively apply the following YAML to disable audit logging:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Namespace
apiVersion: v1
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/acl-logging: null</programlisting>
</tip>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">namespace/verify-audit-logging annotated</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="logging-network-policy-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="about-network-policy">About network policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-egress-firewall-ovn">Configuring an egress firewall for a project</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-ipsec-ovn">
<title>Configuring IPsec encryption</title>

<simpara>With IPsec enabled, you can encrypt both internal pod-to-pod cluster traffic between nodes and external traffic between pods and IPsec endpoints external to your cluster. All pod-to-pod network traffic between nodes on the OVN-Kubernetes cluster network is encrypted with IPsec <emphasis>Transport mode</emphasis>.</simpara>
<simpara>IPsec is disabled by default. It can be enabled either during or after installing the cluster. For information about cluster installation, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#ocp-installation-overview">OpenShift Container Platform installation overview</link>. If you need to enable IPsec after cluster installation, you must first resize your cluster MTU to account for the overhead of the IPsec ESP IP header.</simpara>
<note>
<simpara>IPsec on IBM Cloud&#174; supports only NAT-T. Using ESP is not supported.</simpara>
</note>
<simpara>Use the procedures in the following documentation to:</simpara>
<itemizedlist>
<listitem>
<simpara>Enable and disable IPSec after cluster installation</simpara>
</listitem>
<listitem>
<simpara>Configure support for external IPsec endpoints outside the cluster</simpara>
</listitem>
<listitem>
<simpara>Verify that IPsec encrypts traffic between pods on different nodes</simpara>
</listitem>
</itemizedlist>
<section xml:id="configuring-ipsec-ovn-prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You have decreased the size of the cluster MTU by <literal>46</literal> bytes to allow for the additional overhead of the IPsec ESP header. For more information on resizing the MTU that your cluster uses, see <link linkend="changing-cluster-network-mtu">Changing the MTU for the cluster network</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="network-connectivity-requirements-ipsec_configuring-ipsec-ovn">
<title>Network connectivity requirements when IPsec is enabled</title>
<simpara>You must configure the network connectivity between machines to allow OpenShift Container Platform cluster components to communicate. Each machine must be able to resolve the hostnames of all other machines in the cluster.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Ports used for all-machine to all-machine communications</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="22.2222*"/>
<colspec colname="col_2" colwidth="22.2222*"/>
<colspec colname="col_3" colwidth="55.5556*"/>
<thead>
<row>
<entry align="left" valign="top">Protocol</entry>
<entry align="left" valign="top">Port</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara>UDP</simpara></entry>
<entry align="left" valign="top"><simpara><literal>500</literal></simpara></entry>
<entry align="left" valign="top"><simpara>IPsec IKE packets</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>4500</literal></simpara></entry>
<entry align="left" valign="top"><simpara>IPsec NAT-T packets</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ESP</simpara></entry>
<entry align="left" valign="top"><simpara>N/A</simpara></entry>
<entry align="left" valign="top"><simpara>IPsec Encapsulating Security Payload (ESP)</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="configuring-ipsec-ovn-pod-to-pod-ipsec">
<title>IPsec encryption for pod-to-pod traffic</title>
<simpara>OpenShift Container Platform supports IPsec encryption for network traffic between pods.</simpara>
<section xml:id="nw-ovn-ipsec-traffic_configuring-ipsec-ovn">
<title>Types of network traffic flows encrypted by pod-to-pod IPsec</title>
<simpara>With IPsec enabled, only the following network traffic flows between pods are encrypted:</simpara>
<itemizedlist>
<listitem>
<simpara>Traffic between pods on different nodes on the cluster network</simpara>
</listitem>
<listitem>
<simpara>Traffic from a pod on the host network to a pod on the cluster network</simpara>
</listitem>
</itemizedlist>
<simpara>The following traffic flows are not encrypted:</simpara>
<itemizedlist>
<listitem>
<simpara>Traffic between pods on the same node on the cluster network</simpara>
</listitem>
<listitem>
<simpara>Traffic between pods on the host network</simpara>
</listitem>
<listitem>
<simpara>Traffic from a pod on the cluster network to a pod on the host network</simpara>
</listitem>
</itemizedlist>
<simpara>The encrypted and unencrypted flows are illustrated in the following diagram:</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-ipsec-encryption.png"/>
</imageobject>
<textobject><phrase>IPsec encrypted and unencrypted traffic flows</phrase></textobject>
</mediaobject>
</informalfigure>
</section>
<section xml:id="nw-ovn-ipsec-encryption_configuring-ipsec-ovn">
<title>Encryption protocol and IPsec mode</title>
<simpara>The encrypt cipher used is <literal>AES-GCM-16-256</literal>. The integrity check value (ICV) is <literal>16</literal> bytes. The key length is <literal>256</literal> bits.</simpara>
<simpara>The IPsec mode used is <emphasis>Transport mode</emphasis>, a mode that encrypts end-to-end communication by adding an Encapsulated Security Payload (ESP) header to the IP header of the original packet and encrypts the packet data. OpenShift Container Platform does not currently use or support IPsec <emphasis>Tunnel mode</emphasis> for pod-to-pod communication.</simpara>
</section>
<section xml:id="nw-ovn-ipsec-certificates_configuring-ipsec-ovn">
<title>Security certificate generation and rotation</title>
<simpara>The Cluster Network Operator (CNO) generates a self-signed X.509 certificate authority (CA) that is used by IPsec for encryption. Certificate signing requests (CSRs) from each node are automatically fulfilled by the CNO.</simpara>
<simpara>The CA is valid for 10 years. The individual node certificates are valid for 5 years and are automatically rotated after 4 1/2 years elapse.</simpara>
</section>
<section xml:id="nw-ovn-ipsec-enable_configuring-ipsec-ovn">
<title>Enabling pod-to-pod IPsec encryption</title>
<simpara>As a cluster administrator, you can enable pod-to-pod IPsec encryption after cluster installation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have reduced the size of your cluster MTU by <literal>46</literal> bytes to allow for the overhead of the IPsec ESP header.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To enable IPsec encryption, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch networks.operator.openshift.io cluster --type=merge \
-p '{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipsecConfig":{ }}}}}'</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ovn-ipsec-verification_configuring-ipsec-ovn">
<title>Verifying that IPsec is enabled</title>
<simpara>As a cluster administrator, you can verify that IPsec is enabled.</simpara>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>To find the names of the OVN-Kubernetes data plane pods, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ovn-kubernetes -l=app=ovnkube-node</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovnkube-node-5xqbf                       8/8     Running   0              28m
ovnkube-node-6mwcx                       8/8     Running   0              29m
ovnkube-node-ck5fr                       8/8     Running   0              31m
ovnkube-node-fr4ld                       8/8     Running   0              26m
ovnkube-node-wgs4l                       8/8     Running   0              33m
ovnkube-node-zfvcl                       8/8     Running   0              34m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that IPsec is enabled on your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes -c nbdb rsh ovnkube-node-&lt;XXXXX&gt; ovn-nbctl --no-leader-only get nb_global . ipsec</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;XXXXX&gt;</literal></term>
<listitem>
<simpara>Specifies the random sequence of letters for a pod from the previous step.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ovn-ipsec-disable_configuring-ipsec-ovn">
<title>Disabling IPsec encryption</title>
<simpara>As a cluster administrator, you can disable IPsec encryption only if you enabled IPsec after cluster installation.</simpara>
<note>
<simpara>If you enabled IPsec when you installed your cluster, you cannot disable IPsec with this procedure.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To disable IPsec encryption, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch networks.operator.openshift.io/cluster --type=json \
  -p='[{"op":"remove", "path":"/spec/defaultNetwork/ovnKubernetesConfig/ipsecConfig"}]'</programlisting>
</listitem>
<listitem>
<simpara>Optional: You can increase the size of your cluster MTU by <literal>46</literal> bytes because there is no longer any overhead from the IPsec ESP header in IP packets.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-ipsec-ovn-external-traffic-ipsec">
<title>IPsec encryption for external traffic</title>
<simpara>OpenShift Container Platform supports IPsec encryption for traffic to external hosts.</simpara>
<simpara>You must supply a custom IPsec configuration, which includes the IPsec configuration file itself and TLS certificates.</simpara>
<simpara>Ensure that the following prohibitions are observed:</simpara>
<itemizedlist>
<listitem>
<simpara>The custom IPsec configuration must not include any connection specifications that might interfere with the cluster&#8217;s pod-to-pod IPsec configuration.</simpara>
</listitem>
<listitem>
<simpara>Certificate common names (CN) in the provided certificate bundle must not begin with the <literal>ovs_</literal> prefix, because this naming can collide with pod-to-pod IPsec CN names in the Network Security Services (NSS) database of each node.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>IPsec support for external endpoints is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="nw-ovn-ipsec-north-south-enable_configuring-ipsec-ovn">
<title>Enabling IPsec encryption for external IPsec endpoints</title>
<simpara>As a cluster administrator, you can enable IPsec encryption between the cluster and external IPsec endpoints. Because this procedure uses Butane to create machine configs, you must have the <literal>butane</literal> command installed.</simpara>
<note>
<simpara>After you apply the machine config, the Machine Config Operator reboots affected nodes in your cluster to rollout the new machine config.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have reduced the size of your cluster MTU by <literal>46</literal> bytes to allow for the overhead of the IPsec ESP header.</simpara>
</listitem>
<listitem>
<simpara>You have installed the <literal>butane</literal> utility.</simpara>
</listitem>
<listitem>
<simpara>You have an existing PKCS#12 certificate for the IPsec endpoint and a CA cert in PEM format.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>As a cluster administrator, you can enable IPsec support for external IPsec endpoints.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create an IPsec configuration file named <literal>ipsec-endpoint-config.conf</literal>. The configuration is consumed in the next step. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/securing_networks/configuring-a-vpn-with-ipsec_securing-networks#configuring-a-vpn-with-ipsec_securing-networks">Libreswan as an IPsec VPN implementation</link>.</simpara>
</listitem>
<listitem>
<simpara>Provide the following certificate files to add to the Network Security Services (NSS) database on each host. These files are imported as part of the Butane configuration in subsequent steps.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>left_server.p12</literal>: The certificate bundle for the IPsec endpoints</simpara>
</listitem>
<listitem>
<simpara><literal>ca.pem</literal>: The certificate authority that you signed your certificates with</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create a machine config to apply the IPsec configuration to your cluster by using the following two steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To add the IPsec configuration, create Butane config files for the control plane and worker nodes with the following contents:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for role in master worker; do
  cat &gt;&gt; "99-ipsec-${role}-endpoint-config.bu" &lt;&lt;-EOF
  variant: openshift
  version: 4.14.0
  metadata:
    name: 99-${role}-import-certs-enable-svc-os-ext
    labels:
      machineconfiguration.openshift.io/role: $role
  openshift:
    extensions:
      - ipsec
  systemd:
    units:
    - name: ipsec-import.service
      enabled: true
      contents: |
        [Unit]
        Description=Import external certs into ipsec NSS
        Before=ipsec.service

        [Service]
        Type=oneshot
        ExecStart=/usr/local/bin/ipsec-addcert.sh
        RemainAfterExit=false
        StandardOutput=journal

        [Install]
        WantedBy=multi-user.target
    - name: ipsecenabler.service
      enabled: true
      contents: |
        [Service]
        Type=oneshot
        ExecStart=systemctl enable --now ipsec.service

        [Install]
        WantedBy=multi-user.target
  storage:
    files:
    - path: /etc/ipsec.d/ipsec-endpoint-config.conf
      mode: 0400
      overwrite: true
      contents:
        local: ipsec-endpoint-config.conf
    - path: /etc/pki/certs/ca.pem
      mode: 0400
      overwrite: true
      contents:
        local: ca.pem
    - path: /etc/pki/certs/left_server.p12
      mode: 0400
      overwrite: true
      contents:
        local: left_server.p12
    - path: /usr/local/bin/ipsec-addcert.sh
      mode: 0740
      overwrite: true
      contents:
        inline: |
          #!/bin/bash -e
          echo "importing cert to NSS"
          certutil -A -n "CA" -t "CT,C,C" -d /var/lib/ipsec/nss/ -i /etc/pki/certs/ca.pem
          pk12util -W "" -i /etc/pki/certs/left_server.p12 -d /var/lib/ipsec/nss/
          certutil -M -n "left_server" -t "u,u,u" -d /var/lib/ipsec/nss/
EOF
done</programlisting>
</listitem>
<listitem>
<simpara>To transform the Butane files that you created in the previous step into machine configs, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for role in master worker; do
  butane 99-ipsec-${role}-endpoint-config.bu -o ./99-ipsec-$role-endpoint-config.yaml
done</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To apply the machine configs to your cluster, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for role in master worker; do
  oc apply -f 99-ipsec-${role}-endpoint-config.yaml
done</programlisting>
<important>
<simpara>As the Machine Config Operator (MCO) updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated before external IPsec connectivity is available.</simpara>
</important>
</listitem>
<listitem>
<simpara>Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
<note>
<simpara>By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-ipsec-ovn_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="about-ovn-kubernetes">About the OVN-Kubernetes Container Network Interface (CNI) network plugin</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="changing-cluster-network-mtu">Changing the MTU for the cluster network</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installation-special-config-butane-install_installing-customizing">Installing Butane</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</link>] API</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-secondary-external-gateway">
<title>Configure an external gateway through a secondary network interface</title>

<simpara>As a cluster administrator, you can configure an external gateway on a secondary network.</simpara>
<simpara>This feature offers the following benefits:</simpara>
<itemizedlist>
<listitem>
<simpara>Granular control over egress traffic on a per-namespace basis</simpara>
</listitem>
<listitem>
<simpara>Flexible configuration of static and dynamic external gateway IP addresses</simpara>
</listitem>
<listitem>
<simpara>Support for both IPv4 and IPv6 address families</simpara>
</listitem>
</itemizedlist>
<section xml:id="configuring-secondary-external-gateway_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your cluster uses the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>Your infrastructure is configured to route traffic from the secondary external gateway.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-secondary-ext-gw-about_configuring-secondary-external-gateway">
<title>How OpenShift Container Platform determines the external gateway IP address</title>
<simpara>You configure a secondary external gateway with the <literal>AdminPolicyBasedExternalRoute</literal> custom resource (CR) from the the <literal>k8s.ovn.org</literal> API group. The CR supports static and dynamic approaches to specifying an external gateway&#8217;s IP address.</simpara>
<simpara>Each namespace that a <literal>AdminPolicyBasedExternalRoute</literal> CR targets cannot be selected by any other <literal>AdminPolicyBasedExternalRoute</literal> CR. A namespace cannot have concurrent secondary external gateways.</simpara>
<simpara>Changes to policies are isolated in the controller. If a policy fails to apply, changes to other policies do not trigger a retry of other policies. Policies are only re-evaluated, applying any differences that might have occurred by the change, when updates to the policy itself or related objects to the policy such as target namespaces, pod gateways, or namespaces hosting them from dynamic hops are made.</simpara>
<variablelist>
<varlistentry>
<term>Static assignment</term>
<listitem>
<simpara>You specify an IP address directly.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Dynamic assignment</term>
<listitem>
<simpara>You specify an IP address indirectly, with namespace and pod selectors, and an optional network attachment definition.</simpara>
<itemizedlist>
<listitem>
<simpara>If the name of a network attachment definition is provided, the external gateway IP address of the network attachment is used.</simpara>
</listitem>
<listitem>
<simpara>If the name of a network attachment definition is not provided, the external gateway IP address for the pod itself is used. However, this approach works only if the pod is configured with <literal>hostNetwork</literal> set to <literal>true</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="nw-secondary-ext-gw-object_configuring-secondary-external-gateway">
<title>AdminPolicyBasedExternalRoute object configuration</title>
<simpara>You can define an <literal>AdminPolicyBasedExternalRoute</literal> object, which is cluster scoped, with the following properties. A namespace can be selected by only one <literal>AdminPolicyBasedExternalRoute</literal> CR at a time.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>AdminPolicyBasedExternalRoute</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies the name of the  <literal>AdminPolicyBasedExternalRoute</literal> object.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.from</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies a namespace selector that the routing polices apply to. Only <literal>namespaceSelector</literal> is supported for external traffic. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">from:
  namespaceSelector:
    matchLabels:
      kubernetes.io/metadata.name: novxlan-externalgw-ecmp-4059</programlisting>
<simpara>A namespace can only be targeted by one <literal>AdminPolicyBasedExternalRoute</literal> CR. If a namespace is selected by more than one <literal>AdminPolicyBasedExternalRoute</literal> CR, a <literal>failed</literal> error status occurs on the second and subsequent CRs that target the same namespace. To apply updates, you must change the policy itself or related objects to the policy such as target namespaces, pod gateways, or namespaces hosting them from dynamic hops in order for the policy to be re-evaluated and your changes to be applied.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.nextHops</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies the destinations where the packets are forwarded to. Must be either or both of <literal>static</literal> and <literal>dynamic</literal>. You must have at least one next hop defined.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>nextHops</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>static</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies an array of static IP addresses.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>dynamic</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies an array of pod selectors corresponding to pods configured with a network attachment definition to use as the external gateway target.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>nextHops.static</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>ip</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies either an IPv4 or IPv6 address of the next destination hop.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>bfdEnabled</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specifies whether Bi-Directional Forwarding Detection (BFD) is supported by the network. The default value is <literal>false</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>nextHops.dynamic</literal> object</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>podSelector</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies a [set-based](<link xlink:href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#set-based-requirement</link>) label selector to filter the pods in the namespace that match this network configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>namespaceSelector</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specifies a <literal>set-based</literal> selector to filter the namespaces that the <literal>podSelector</literal> applies to. You must specify a value for this field.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>bfdEnabled</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specifies whether Bi-Directional Forwarding Detection (BFD) is supported by the network. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>networkAttachmentName</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Optional: Specifies the name of a network attachment definition. The name must match the list of logical networks associated with the pod. If this field is not specified, the host network of the pod is used. However, the pod must be configure as a host network pod to use the host network.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="example-secondary-external-gateway-configurations_configuring-secondary-external-gateway">
<title>Example secondary external gateway configurations</title>
<simpara>In the following example, the <literal>AdminPolicyBasedExternalRoute</literal> object configures two static IP addresses as external gateways for pods in namespaces with the <literal>kubernetes.io/metadata.name: novxlan-externalgw-ecmp-4059</literal> label.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: AdminPolicyBasedExternalRoute
metadata:
  name: default-route-policy
spec:
  from:
    namespaceSelector:
      matchLabels:
        kubernetes.io/metadata.name: novxlan-externalgw-ecmp-4059
  nextHops:
    static:
    - ip: "172.18.0.8"
    - ip: "172.18.0.9"</programlisting>
<simpara>In the following example, the <literal>AdminPolicyBasedExternalRoute</literal> object configures a dynamic external gateway. The IP addresses used for the external gateway are derived from the additional network attachments associated with each of the selected pods.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: AdminPolicyBasedExternalRoute
metadata:
  name: shadow-traffic-policy
spec:
  from:
    namespaceSelector:
      matchLabels:
        externalTraffic: ""
  nextHops:
    dynamic:
    - podSelector:
        matchLabels:
          gatewayPod: ""
      namespaceSelector:
        matchLabels:
          shadowTraffic: ""
      networkAttachmentName: shadow-gateway
    - podSelector:
        matchLabels:
          gigabyteGW: ""
      namespaceSelector:
        matchLabels:
          gatewayNamespace: ""
      networkAttachmentName: gateway</programlisting>
<simpara>In the following example, the <literal>AdminPolicyBasedExternalRoute</literal> object configures both static and dynamic external gateways.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: AdminPolicyBasedExternalRoute
metadata:
  name: multi-hop-policy
spec:
  from:
    namespaceSelector:
      matchLabels:
        trafficType: "egress"
  nextHops:
    static:
    - ip: "172.18.0.8"
    - ip: "172.18.0.9"
    dynamic:
    - podSelector:
        matchLabels:
          gatewayPod: ""
      namespaceSelector:
        matchLabels:
          egressTraffic: ""
      networkAttachmentName: gigabyte</programlisting>
</section>
</section>
<section xml:id="nw-secondary-ext-gw-configure_configuring-secondary-external-gateway">
<title>Configure a secondary external gateway</title>
<simpara>You can configure a secondary external gateway for a namespace in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file that contains an <literal>AdminPolicyBasedExternalRoute</literal> object.</simpara>
</listitem>
<listitem>
<simpara>To create an admin policy based external route, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;file&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the YAML file that you created in the previous step.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">adminpolicybasedexternalroute.k8s.ovn.org/default-route-policy created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To confirm that the admin policy based external route was created, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe apbexternalroute &lt;name&gt; | tail -n 6</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the <literal>AdminPolicyBasedExternalRoute</literal> object.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Status:
  Last Transition Time:  2023-04-24T15:09:01Z
  Messages:
  Configured external gateway IPs: 172.18.0.8
  Status:  Success
Events:  &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-secondary-external-gateway_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>For more information about additional network attachments, see <link linkend="understanding-multiple-networks">Understanding multiple networks</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-egress-firewall-ovn">
<title>Configuring an egress firewall for a project</title>

<simpara>As a cluster administrator, you can create an egress firewall for a project that restricts egress traffic leaving your OpenShift Container Platform cluster.</simpara>
<section xml:id="nw-egressnetworkpolicy-about_configuring-egress-firewall-ovn">
<title>How an egress firewall works in a project</title>
<simpara>As a cluster administrator, you can use an <emphasis>egress firewall</emphasis> to
limit the external hosts that some or all pods can access from within the
cluster. An egress firewall supports the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>A pod can only connect to internal hosts and cannot initiate connections to
the public internet.</simpara>
</listitem>
<listitem>
<simpara>A pod can only connect to the public internet and cannot initiate connections
to internal hosts that are outside the OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>A pod cannot reach specified internal subnets or hosts outside the OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>A pod can connect to only specific external hosts.</simpara>
</listitem>
</itemizedlist>
<simpara>For example, you can allow one project access to a specified IP range but deny the same access to a different project. Or you can restrict application developers from updating from Python pip mirrors, and force updates to come only from approved sources.</simpara>
<note>
<simpara>Egress firewall does not apply to the host network namespace. Pods with host networking enabled are unaffected by egress firewall rules.</simpara>
</note>
<simpara>You configure an egress firewall policy by creating an EgressFirewall custom resource (CR) object. The egress firewall matches network traffic that meets any of the following criteria:</simpara>
<itemizedlist>
<listitem>
<simpara>An IP address range in CIDR format</simpara>
</listitem>
<listitem>
<simpara>A DNS name that resolves to an IP address</simpara>
</listitem>
<listitem>
<simpara>A port number</simpara>
</listitem>
<listitem>
<simpara>A protocol that is one of the following protocols: TCP, UDP, and SCTP</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>If your egress firewall includes a deny rule for <literal>0.0.0.0/0</literal>, access to your OpenShift Container Platform API servers is blocked. You must either add allow rules for each IP address or use the <literal>nodeSelector</literal> type allow rule in your egress policy rules to connect to API servers.</simpara>
<simpara>The following example illustrates the order of the egress firewall rules necessary to ensure API server access:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
  namespace: &lt;namespace&gt; <co xml:id="CO175-1"/>
spec:
  egress:
  - to:
      cidrSelector: &lt;api_server_address_range&gt; <co xml:id="CO175-2"/>
    type: Allow
# ...
  - to:
      cidrSelector: 0.0.0.0/0 <co xml:id="CO175-3"/>
    type: Deny</programlisting>
<calloutlist>
<callout arearefs="CO175-1">
<para>The namespace for the egress firewall.</para>
</callout>
<callout arearefs="CO175-2">
<para>The IP address range that includes your OpenShift Container Platform API servers.</para>
</callout>
<callout arearefs="CO175-3">
<para>A global deny rule prevents access to the OpenShift Container Platform API servers.</para>
</callout>
</calloutlist>
<simpara>To find the IP address for your API servers, run <literal>oc get ep kubernetes -n default</literal>.</simpara>
<simpara>For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1988324">BZ#1988324</link>.</simpara>
</important>
<warning>
<simpara>Egress firewall rules do not apply to traffic that goes through routers. Any user with permission to create a Route CR object can bypass egress firewall policy rules by creating a route that points to a forbidden destination.</simpara>
</warning>
<section xml:id="limitations-of-an-egress-firewall_configuring-egress-firewall-ovn">
<title>Limitations of an egress firewall</title>
<simpara>An egress firewall has the following limitations:</simpara>
<itemizedlist>
<listitem>
<simpara>No project can have more than one EgressFirewall object.</simpara>
</listitem>
<listitem>
<simpara>A maximum of one EgressFirewall object with a maximum of 8,000 rules can be defined per project.</simpara>
</listitem>
<listitem>
<simpara>If you are using the OVN-Kubernetes network plugin with shared gateway mode in Red Hat OpenShift Networking, return ingress replies are affected by egress firewall rules. If the egress firewall rules drop the ingress reply destination IP, the traffic is dropped.</simpara>
</listitem>
</itemizedlist>
<simpara>Violating any of these restrictions results in a broken egress firewall for the project. Consequently, all external network traffic is dropped, which can cause security risks for your organization.</simpara>
<simpara>An Egress Firewall resource can be created in the <literal>kube-node-lease</literal>, <literal>kube-public</literal>, <literal>kube-system</literal>, <literal>openshift</literal> and <literal>openshift-</literal> projects.</simpara>
</section>
<section xml:id="policy-rule-order_configuring-egress-firewall-ovn">
<title>Matching order for egress firewall policy rules</title>
<simpara>The egress firewall policy rules are evaluated in the order that they are defined, from first to last. The first rule that matches an egress connection from a pod applies. Any subsequent rules are ignored for that connection.</simpara>
</section>
<section xml:id="domain-name-server-resolution_configuring-egress-firewall-ovn">
<title>How Domain Name Server (DNS) resolution works</title>
<simpara>If you use DNS names in any of your egress firewall policy rules, proper resolution of the domain names is subject to the following restrictions:</simpara>
<itemizedlist>
<listitem>
<simpara>Domain name updates are polled based on a time-to-live (TTL) duration. By default, the duration is 30 minutes. When the egress firewall controller queries the local name servers for a domain name, if the response includes a TTL and the TTL is less than 30 minutes, the controller sets the duration for that DNS name to the returned value. Each DNS name is queried after the TTL for the DNS record expires.</simpara>
</listitem>
<listitem>
<simpara>The pod must resolve the domain from the same local name servers when necessary. Otherwise the IP addresses for the domain known by the egress firewall controller and the pod can be different. If the IP addresses for a hostname differ, the egress firewall might not be enforced consistently.</simpara>
</listitem>
<listitem>
<simpara>Because the egress firewall controller and pods asynchronously poll the same local name server, the pod might obtain the updated IP address before the egress controller does, which causes a race condition. Due to this current limitation, domain name usage in EgressFirewall objects is only recommended for domains with infrequent IP address changes.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Using DNS names in your egress firewall policy does not affect local DNS resolution through CoreDNS.</simpara>
<simpara>However, if your egress firewall policy uses domain names, and an external DNS server handles DNS resolution for an affected pod, you must include egress firewall rules that permit access to the IP addresses of your DNS server.</simpara>
</note>
</section>
</section>
<section xml:id="nw-egressnetworkpolicy-object_configuring-egress-firewall-ovn">
<title>EgressFirewall custom resource (CR) object</title>
<simpara>You can define one or more rules for an egress firewall. A rule is either an <literal>Allow</literal> rule or a <literal>Deny</literal> rule, with a specification for the traffic that the rule applies to.</simpara>
<simpara>The following YAML describes an EgressFirewall CR object:</simpara>
<formalpara>
<title>EgressFirewall object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: &lt;name&gt; <co xml:id="CO176-1"/>
spec:
  egress: <co xml:id="CO176-2"/>
    ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO176-1">
<para>The name for the object must be <literal>default</literal>.</para>
</callout>
<callout arearefs="CO176-2">
<para>A collection of one or more egress network policy rules as described in the following section.</para>
</callout>
</calloutlist>
<section xml:id="egressnetworkpolicy-rules_configuring-egress-firewall-ovn">
<title>EgressFirewall rules</title>
<simpara>The following YAML describes an egress firewall rule object. The user can select either an IP address range in CIDR format, a domain name, or use the <literal>nodeSelector</literal> to allow or deny egress traffic. The <literal>egress</literal> stanza expects an array of one or more objects.</simpara>
<formalpara>
<title>Egress policy rule stanza</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">egress:
- type: &lt;type&gt; <co xml:id="CO177-1"/>
  to: <co xml:id="CO177-2"/>
    cidrSelector: &lt;cidr&gt; <co xml:id="CO177-3"/>
    dnsName: &lt;dns_name&gt; <co xml:id="CO177-4"/>
    nodeSelector: &lt;label_name&gt;: &lt;label_value&gt; <co xml:id="CO177-5"/>
  ports: <co xml:id="CO177-6"/>
      ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO177-1">
<para>The type of rule. The value must be either <literal>Allow</literal> or <literal>Deny</literal>.</para>
</callout>
<callout arearefs="CO177-2">
<para>A stanza describing an egress traffic match rule that specifies the <literal>cidrSelector</literal> field or the <literal>dnsName</literal> field. You cannot use both fields in the same rule.</para>
</callout>
<callout arearefs="CO177-3">
<para>An IP address range in CIDR format.</para>
</callout>
<callout arearefs="CO177-4">
<para>A DNS domain name.</para>
</callout>
<callout arearefs="CO177-5">
<para>Labels are key/value pairs that the user defines. Labels are attached to objects, such as pods. The <literal>nodeSelector</literal> allows for one or more node labels to be selected and attached to pods.</para>
</callout>
<callout arearefs="CO177-6">
<para>Optional: A stanza describing a collection of network ports and protocols for the rule.</para>
</callout>
</calloutlist>
<formalpara>
<title>Ports stanza</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">ports:
- port: &lt;port&gt; <co xml:id="CO178-1"/>
  protocol: &lt;protocol&gt; <co xml:id="CO178-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO178-1">
<para>A network port, such as <literal>80</literal> or <literal>443</literal>. If you specify a value for this field, you must also specify a value for <literal>protocol</literal>.</para>
</callout>
<callout arearefs="CO178-2">
<para>A network protocol. The value must be either <literal>TCP</literal>, <literal>UDP</literal>, or <literal>SCTP</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="egressnetworkpolicy-example_configuring-egress-firewall-ovn">
<title>Example EgressFirewall CR objects</title>
<simpara>The following example defines several egress firewall policy rules:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
  egress: <co xml:id="CO179-1"/>
  - type: Allow
    to:
      cidrSelector: 1.2.3.0/24
  - type: Deny
    to:
      cidrSelector: 0.0.0.0/0</programlisting>
<calloutlist>
<callout arearefs="CO179-1">
<para>A collection of egress firewall policy rule objects.</para>
</callout>
</calloutlist>
<simpara>The following example defines a policy rule that denies traffic to the host at the <literal>172.16.1.1</literal> IP address, if the traffic is using either the TCP protocol and destination port <literal>80</literal> or any protocol and destination port <literal>443</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
  egress:
  - type: Deny
    to:
      cidrSelector: 172.16.1.1
    ports:
    - port: 80
      protocol: TCP
    - port: 443</programlisting>
</section>
<section xml:id="configuringNodeSelector-example_configuring-egress-firewall-ovn">
<title>Example nodeSelector for EgressFirewall</title>
<simpara>As a cluster administrator, you can allow or deny egress traffic to nodes in your cluster by specifying a label using <literal>nodeSelector</literal>. Labels can be applied to one or more nodes. The following is an example with the <literal>region=east</literal> label:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressFirewall
metadata:
  name: default
spec:
    egress:
    - to:
        nodeSelector:
          matchLabels:
            region: east
      type: Allow</programlisting>
<tip>
<simpara>Instead of adding manual rules per node IP address, use node selectors to create a label that allows pods behind an egress firewall to access host network pods.</simpara>
</tip>
</section>
</section>
<section xml:id="nw-networkpolicy-create_configuring-egress-firewall-ovn">
<title>Creating an egress firewall policy object</title>
<simpara>As a cluster administrator, you can create an egress firewall policy object for a project.</simpara>
<important>
<simpara>If the project already has an EgressFirewall object defined, you must edit the existing policy to make changes to the egress firewall rules.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster that uses the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy rule:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>&lt;policy_name&gt;.yaml</literal> file where <literal>&lt;policy_name&gt;</literal> describes the egress
policy rules.</simpara>
</listitem>
<listitem>
<simpara>In the file you created, define an egress policy object.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Enter the following command to create the policy object. Replace <literal>&lt;policy_name&gt;</literal> with the name of the policy and <literal>&lt;project&gt;</literal> with the project that the rule applies to.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;policy_name&gt;.yaml -n &lt;project&gt;</programlisting>
<simpara>In the following example, a new EgressFirewall object is created in a project named <literal>project1</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f default.yaml -n project1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">egressfirewall.k8s.ovn.org/v1 created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: Save the <literal>&lt;policy_name&gt;.yaml</literal> file so that you can make changes later.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="viewing-egress-firewall-ovn">
<title>Viewing an egress firewall for a project</title>

<simpara>As a cluster administrator, you can list the names of any existing egress firewalls and view the traffic rules for a specific egress firewall.</simpara>
<section xml:id="nw-egressnetworkpolicy-view_viewing-egress-firewall-ovn">
<title>Viewing an EgressFirewall object</title>
<simpara>You can view an EgressFirewall object in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster using the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift Command-line Interface (CLI), commonly known as <literal>oc</literal>.</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To view the names of the EgressFirewall objects defined in your cluster,
enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get egressfirewall --all-namespaces</programlisting>
</listitem>
<listitem>
<simpara>To inspect a policy, enter the following command. Replace <literal>&lt;policy_name&gt;</literal> with the name of the policy to inspect.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe egressfirewall &lt;policy_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:		default
Namespace:	project1
Created:	20 minutes ago
Labels:		&lt;none&gt;
Annotations:	&lt;none&gt;
Rule:		Allow to 1.2.3.0/24
Rule:		Allow to www.example.com
Rule:		Deny to 0.0.0.0/0</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="editing-egress-firewall-ovn">
<title>Editing an egress firewall for a project</title>

<simpara>As a cluster administrator, you can modify network traffic rules for an existing egress firewall.</simpara>
<section xml:id="nw-egressnetworkpolicy-edit_editing-egress-firewall-ovn">
<title>Editing an EgressFirewall object</title>
<simpara>As a cluster administrator, you can update the egress firewall for a project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster using the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Find the name of the EgressFirewall object for the project. Replace <literal>&lt;project&gt;</literal> with the name of the project.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n &lt;project&gt; egressfirewall</programlisting>
</listitem>
<listitem>
<simpara>Optional: If you did not save a copy of the EgressFirewall object when you created the egress network firewall, enter the following command to create a copy.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n &lt;project&gt; egressfirewall &lt;name&gt; -o yaml &gt; &lt;filename&gt;.yaml</programlisting>
<simpara>Replace <literal>&lt;project&gt;</literal> with the name of the project. Replace <literal>&lt;name&gt;</literal> with the name of the object. Replace <literal>&lt;filename&gt;</literal> with the name of the file to save the YAML to.</simpara>
</listitem>
<listitem>
<simpara>After making changes to the policy rules, enter the following command to replace the EgressFirewall object. Replace <literal>&lt;filename&gt;</literal> with the name of the file containing the updated EgressFirewall object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f &lt;filename&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="removing-egress-firewall-ovn">
<title>Removing an egress firewall from a project</title>

<simpara>As a cluster administrator, you can remove an egress firewall from a project to remove all restrictions on network traffic from the project that leaves the OpenShift Container Platform cluster.</simpara>
<section xml:id="nw-egressnetworkpolicy-delete_removing-egress-firewall-ovn">
<title>Removing an EgressFirewall object</title>
<simpara>As a cluster administrator, you can remove an egress firewall from a project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster using the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Find the name of the EgressFirewall object for the project. Replace <literal>&lt;project&gt;</literal> with the name of the project.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n &lt;project&gt; egressfirewall</programlisting>
</listitem>
<listitem>
<simpara>Enter the following command to delete the EgressFirewall object. Replace <literal>&lt;project&gt;</literal> with the name of the project and <literal>&lt;name&gt;</literal> with the name of the object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n &lt;project&gt; egressfirewall &lt;name&gt;</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-egress-ips-ovn">
<title>Configuring an egress IP address</title>

<simpara>As a cluster administrator, you can configure the OVN-Kubernetes Container Network Interface (CNI) network plugin to assign one or more egress IP addresses to a namespace, or to specific pods in a namespace.</simpara>
<section xml:id="nw-egress-ips-about_configuring-egress-ips-ovn">
<title>Egress IP address architectural design and implementation</title>
<simpara>The OpenShift Container Platform egress IP address functionality allows you to ensure that the traffic from one or more pods in one or more namespaces has a consistent source IP address for services outside the cluster network.</simpara>
<simpara>For example, you might have a pod that periodically queries a database that is hosted on a server outside of your cluster. To enforce access requirements for the server, a packet filtering device is configured to allow traffic only from specific IP addresses.
To ensure that you can reliably allow access to the server from only that specific pod, you can configure a specific egress IP address for the pod that makes the requests to the server.</simpara>
<simpara>An egress IP address assigned to a namespace is different from an egress router, which is used to send traffic to specific destinations.</simpara>
<simpara>In some cluster configurations,
application pods and ingress router pods run on the same node. If you configure an egress IP address for an application project in this scenario, the IP address is not used when you send a request to a route from the application project.</simpara>
<important>
<simpara>Egress IP addresses must not be configured in any Linux network configuration files, such as <literal>ifcfg-eth0</literal>.</simpara>
</important>
<section xml:id="nw-egress-ips-platform-support_configuring-egress-ips-ovn">
<title>Platform support</title>
<simpara>Support for the egress IP address functionality on various platforms is summarized in the following table:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Platform</entry>
<entry align="left" valign="top">Supported</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Bare metal</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VMware vSphere</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Red Hat OpenStack Platform (RHOSP)</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Amazon Web Services (AWS)</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Google Cloud Platform (GCP)</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Microsoft Azure</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Z&#174; and IBM&#174; LinuxONE</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Z&#174; and IBM&#174; LinuxONE for Red Hat Enterprise Linux (RHEL) KVM</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Power&#174;</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Nutanix</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<important>
<simpara>The assignment of egress IP addresses to control plane nodes with the EgressIP feature is
not supported on a cluster provisioned on Amazon Web Services (AWS). (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=2039656"><emphasis role="strong">BZ#2039656</emphasis></link>).</simpara>
</important>
</section>
<section xml:id="nw-egress-ips-public-cloud-platform-considerations_configuring-egress-ips-ovn">
<title>Public cloud platform considerations</title>
<simpara>For clusters provisioned on public cloud infrastructure, there is a constraint on the absolute number of assignable IP addresses per node. The maximum number of assignable IP addresses per node, or the <emphasis>IP capacity</emphasis>, can be described in the following formula:</simpara>
<programlisting language="text" linenumbering="unnumbered">IP capacity = public cloud default capacity - sum(current IP assignments)</programlisting>
<simpara>While the Egress IPs capability manages the IP address capacity per node, it is important to plan for this constraint in your deployments. For example, for a cluster installed on bare-metal infrastructure with 8 nodes you can configure 150 egress IP addresses. However, if a public cloud provider limits IP address capacity to 10 IP addresses per node, the total number of assignable IP addresses is only 80. To achieve the same IP address capacity in this example cloud provider, you would need to allocate 7 additional nodes.</simpara>
<simpara>To confirm the IP capacity and subnets for any node in your public cloud environment, you can enter the <literal>oc get node &lt;node_name&gt; -o yaml</literal> command. The <literal>cloud.network.openshift.io/egress-ipconfig</literal> annotation includes capacity and subnet information for the node.</simpara>
<simpara>The annotation value is an array with a single object with fields that provide the following information for the primary network interface:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>interface</literal>: Specifies the interface ID on AWS and Azure and the interface name on GCP.</simpara>
</listitem>
<listitem>
<simpara><literal>ifaddr</literal>: Specifies the subnet mask for one or both IP address families.</simpara>
</listitem>
<listitem>
<simpara><literal>capacity</literal>: Specifies the IP address capacity for the node. On AWS, the IP address capacity is provided per IP address family. On Azure and GCP, the IP address capacity includes both IPv4 and IPv6 addresses.</simpara>
</listitem>
</itemizedlist>
<simpara>Automatic attachment and detachment of egress IP addresses for traffic between nodes are available. This allows for traffic from many pods in namespaces to have a consistent source IP address to locations outside of the cluster. This also supports OpenShift SDN and OVN-Kubernetes, which is the default networking plugin in Red Hat OpenShift Networking in OpenShift Container Platform 4.14.</simpara>
<note>
<simpara>When an RHOSP cluster administrator assigns a floating IP to the reservation port, OpenShift Container Platform cannot delete the reservation port. The <literal>CloudPrivateIPConfig</literal> object cannot perform delete and move operations until an RHOSP cluster administrator unassigns the floating IP from the reservation port.</simpara>
</note>
<simpara>The following examples illustrate the annotation from nodes on several public cloud providers. The annotations are indented for readability.</simpara>
<formalpara>
<title>Example <literal>cloud.network.openshift.io/egress-ipconfig</literal> annotation on AWS</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"eni-078d267045138e436",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ipv4":14,"ipv6":15}
  }
]</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example <literal>cloud.network.openshift.io/egress-ipconfig</literal> annotation on GCP</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"nic0",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ip":14}
  }
]</programlisting>
</para>
</formalpara>
<simpara>The following sections describe the IP address capacity for supported public cloud environments for use in your capacity calculation.</simpara>
<section xml:id="nw-egress-ips-capacity-aws_configuring-egress-ips-ovn">
<title>Amazon Web Services (AWS) IP address capacity limits</title>
<simpara>On AWS, constraints on IP address assignments depend on the instance type configured. For more information, see <link xlink:href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</link></simpara>
</section>
<section xml:id="nw-egress-ips-capacity-gcp_configuring-egress-ips-ovn">
<title>Google Cloud Platform (GCP) IP address capacity limits</title>
<simpara>On GCP, the networking model implements additional node IP addresses through IP address aliasing, rather than IP address assignments. However, IP address capacity maps directly to IP aliasing capacity.</simpara>
<simpara>The following capacity limits exist for IP aliasing assignment:</simpara>
<itemizedlist>
<listitem>
<simpara>Per node, the maximum number of IP aliases, both IPv4 and IPv6, is 100.</simpara>
</listitem>
<listitem>
<simpara>Per VPC, the maximum number of IP aliases is unspecified, but OpenShift Container Platform scalability testing reveals the maximum to be approximately 15,000.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information, see <link xlink:href="https://cloud.google.com/vpc/docs/quota#per_instance">Per instance</link> quotas and <link xlink:href="https://cloud.google.com/vpc/docs/alias-ip">Alias IP ranges overview</link>.</simpara>
</section>
<section xml:id="nw-egress-ips-capacity-azure_configuring-egress-ips-ovn">
<title>Microsoft Azure IP address capacity limits</title>
<simpara>On Azure, the following capacity limits exist for IP address assignment:</simpara>
<itemizedlist>
<listitem>
<simpara>Per NIC, the maximum number of assignable IP addresses, for both IPv4 and IPv6, is 256.</simpara>
</listitem>
<listitem>
<simpara>Per virtual network, the maximum number of assigned IP addresses cannot exceed 65,536.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information, see <link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?toc=/azure/virtual-network/toc.json#networking-limits">Networking limits</link>.</simpara>
</section>
</section>
<section xml:id="nw-egress-ips-multi-nic-considerations_configuring-egress-ips-ovn">
<title>Considerations for using an egress IP on additional network interfaces</title>
<simpara>In OpenShift Container Platform, egress IPs provide administrators a way to control network traffic. Egress IPs can be used with the <literal>br-ex</literal>, or primary, network interface, which is a Linux bridge interface associated with Open vSwitch, or they can be used with additional network interfaces.</simpara>
<simpara>You can inspect your network interface type by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip -details link show</programlisting>
<simpara>The primary network interface is assigned a node IP address which also contains a subnet mask. Information for this node IP address can be retrieved from the Kubernetes node object for each node within your cluster by inspecting the <literal>k8s.ovn.org/node-primary-ifaddr</literal> annotation. In an IPv4 cluster, this annotation is similar to the following example: <literal>"k8s.ovn.org/node-primary-ifaddr: {"ipv4":"192.168.111.23/24"}"</literal>.</simpara>
<simpara>If the egress IP is not within the subnet of the primary network interface subnet, you can use an egress IP on another Linux network interface that is not of the primary network interface type. By doing so, OpenShift Container Platform administrators are provided with a greater level of control over networking aspects such as routing, addressing, segmentation, and security policies. This feature provides users with the option to route workload traffic over specific network interfaces for purposes such as traffic segmentation or meeting specialized requirements.</simpara>
<simpara>If the egress IP is not within the subnet of the primary network interface, then the selection of another network interface for egress traffic might occur if they are present on a node.</simpara>
<simpara>You can determine which other network interfaces might support egress IPs by inspecting the <literal>k8s.ovn.org/host-cidrs</literal> Kubernetes node annotation. This annotation contains the addresses and subnet mask found for the primary network interface. It also contains additional network interface addresses and subnet mask information. These addresses and subnet masks are assigned to network interfaces that use the <link xlink:href="https://networklessons.com/cisco/ccna-200-301/longest-prefix-match-routing">longest prefix match routing</link> mechanism to determine which network interface supports the egress IP.</simpara>
<note>
<simpara>OVN-Kubernetes provides a mechanism to control and direct outbound network traffic from specific namespaces and pods. This ensures that it exits the cluster through a particular network interface and with a specific egress IP address.</simpara>
</note>
<bridgehead xml:id="nw-egress-ips-multi-nic-requirements_configuring-egress-ips-ovn" renderas="sect5">Requirements for assigning an egress IP to a network interface that is not the primary network interface</bridgehead>
<simpara>For users who want an egress IP and traffic to be routed over a particular interface that is not the primary network interface, the following conditions must be met:</simpara>
<itemizedlist>
<listitem>
<simpara>OpenShift Container Platform is installed on a bare metal cluster. This feature is disabled within cloud or hypervisor environments.</simpara>
</listitem>
<listitem>
<simpara>Your OpenShift Container Platform pods are not configured as host-networked.</simpara>
</listitem>
<listitem>
<simpara>If a network interface is removed or if the IP address and subnet mask which allows the egress IP to be hosted on the interface is removed, then the egress IP is reconfigured. Consequently, it could be assigned to another node and interface.</simpara>
</listitem>
<listitem>
<simpara>The Egress IP must be IPv4. IPv6 is currently unsupported.</simpara>
</listitem>
<listitem>
<simpara>IP forwarding must be enabled for the network interface. To enable IP forwarding, you can use the <literal>oc edit network.operator</literal> command and edit the object like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  defaultNetwork:
    ovnKubernetesConfig:
      gatewayConfig:
        ipForwarding: Global
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-egress-ips-considerations_configuring-egress-ips-ovn">
<title>Assignment of egress IPs to pods</title>
<simpara>To assign one or more egress IPs to a namespace or specific pods in a namespace, the following conditions must be satisfied:</simpara>
<itemizedlist>
<listitem>
<simpara>At least one node in your cluster must have the <literal>k8s.ovn.org/egress-assignable: ""</literal> label.</simpara>
</listitem>
<listitem>
<simpara>An <literal>EgressIP</literal> object exists that defines one or more egress IP addresses to use as the source IP address for traffic leaving the cluster from pods in a namespace.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>If you create <literal>EgressIP</literal> objects prior to labeling any nodes in your cluster for egress IP assignment, OpenShift Container Platform might assign every egress IP address to the first node with the <literal>k8s.ovn.org/egress-assignable: ""</literal> label.</simpara>
<simpara>To ensure that egress IP addresses are widely distributed across nodes in the cluster, always apply the label to the nodes you intent to host the egress IP addresses before creating any <literal>EgressIP</literal> objects.</simpara>
</important>
</section>
<section xml:id="nw-egress-ips-node-assignment_configuring-egress-ips-ovn">
<title>Assignment of egress IPs to nodes</title>
<simpara>When creating an <literal>EgressIP</literal> object, the following conditions apply to nodes that are labeled with the <literal>k8s.ovn.org/egress-assignable: ""</literal> label:</simpara>
<itemizedlist>
<listitem>
<simpara>An egress IP address is never assigned to more than one node at a time.</simpara>
</listitem>
<listitem>
<simpara>An egress IP address is equally balanced between available nodes that can host the egress IP address.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>spec.EgressIPs</literal> array in an <literal>EgressIP</literal> object specifies more than one IP address, the following conditions apply:</simpara>
<itemizedlist>
<listitem>
<simpara>No node will ever host more than one of the specified IP addresses.</simpara>
</listitem>
<listitem>
<simpara>Traffic is balanced roughly equally between the specified IP addresses for a given namespace.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If a node becomes unavailable, any egress IP addresses assigned to it are automatically reassigned, subject to the previously described conditions.</simpara>
</listitem>
</itemizedlist>
<simpara>When a pod matches the selector for multiple <literal>EgressIP</literal> objects, there is no guarantee which of the egress IP addresses that are specified in the <literal>EgressIP</literal> objects is assigned as the egress IP address for the pod.</simpara>
<simpara>Additionally, if an <literal>EgressIP</literal> object specifies multiple egress IP addresses, there is no guarantee which of the egress IP addresses might be used. For example, if a pod matches a selector for an <literal>EgressIP</literal> object with two egress IP addresses, <literal>10.10.20.1</literal> and <literal>10.10.20.2</literal>, either might be used for each TCP connection or UDP conversation.</simpara>
</section>
<section xml:id="nw-egress-ips-node-architecture_configuring-egress-ips-ovn">
<title>Architectural diagram of an egress IP address configuration</title>
<simpara>The following diagram depicts an egress IP address configuration. The diagram describes four pods in two different namespaces running on three nodes in a cluster. The nodes are assigned IP addresses from the <literal>192.168.126.0/18</literal> CIDR block on the host network.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-egress-ips-diagram.svg"/>
</imageobject>
<textobject><phrase>Architectural diagram for the egress IP feature.</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>Both Node 1 and Node 3 are labeled with <literal>k8s.ovn.org/egress-assignable: ""</literal> and thus available for the assignment of egress IP addresses.</simpara>
<simpara>The dashed lines in the diagram depict the traffic flow from pod1, pod2, and pod3 traveling through the pod network to egress the cluster from Node 1 and Node 3. When an external service receives traffic from any of the pods selected by the example <literal>EgressIP</literal> object, the source IP address is either <literal>192.168.126.10</literal> or <literal>192.168.126.102</literal>. The traffic is balanced roughly equally between these two nodes.</simpara>
<simpara>The following resources from the diagram are illustrated in detail:</simpara>
<variablelist>
<varlistentry>
<term><literal>Namespace</literal> objects</term>
<listitem>
<simpara>The namespaces are defined in the following manifest:</simpara>
<formalpara>
<title>Namespace objects</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: namespace1
  labels:
    env: prod
---
apiVersion: v1
kind: Namespace
metadata:
  name: namespace2
  labels:
    env: prod</programlisting>
</para>
</formalpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>EgressIP</literal> object</term>
<listitem>
<simpara>The following <literal>EgressIP</literal> object describes a configuration that selects all pods in any namespace with the <literal>env</literal> label set to <literal>prod</literal>. The egress IP addresses for the selected pods are <literal>192.168.126.10</literal> and <literal>192.168.126.102</literal>.</simpara>
<formalpara>
<title><literal>EgressIP</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egressips-prod
spec:
  egressIPs:
  - 192.168.126.10
  - 192.168.126.102
  namespaceSelector:
    matchLabels:
      env: prod
status:
  items:
  - node: node1
    egressIP: 192.168.126.10
  - node: node3
    egressIP: 192.168.126.102</programlisting>
</para>
</formalpara>
<simpara>For the configuration in the previous example, OpenShift Container Platform assigns both egress IP addresses to the available nodes. The <literal>status</literal> field reflects whether and where the egress IP addresses are assigned.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="nw-egress-ips-object_configuring-egress-ips-ovn">
<title>EgressIP object</title>
<simpara>The following YAML describes the API for the <literal>EgressIP</literal> object. The scope of the object is cluster-wide; it is not created in a namespace.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: &lt;name&gt; <co xml:id="CO180-1"/>
spec:
  egressIPs: <co xml:id="CO180-2"/>
  - &lt;ip_address&gt;
  namespaceSelector: <co xml:id="CO180-3"/>
    ...
  podSelector: <co xml:id="CO180-4"/>
    ...</programlisting>
<calloutlist>
<callout arearefs="CO180-1">
<para>The name for the <literal>EgressIPs</literal> object.</para>
</callout>
<callout arearefs="CO180-2">
<para>An array of one or more IP addresses.</para>
</callout>
<callout arearefs="CO180-3">
<para>One or more selectors for the namespaces to associate the egress IP addresses with.</para>
</callout>
<callout arearefs="CO180-4">
<para>Optional: One or more selectors for pods in the specified namespaces to associate egress IP addresses with. Applying these selectors allows for the selection of a subset of pods within a namespace.</para>
</callout>
</calloutlist>
<simpara>The following YAML describes the stanza for the namespace selector:</simpara>
<formalpara>
<title>Namespace selector stanza</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">namespaceSelector: <co xml:id="CO181-1"/>
  matchLabels:
    &lt;label_name&gt;: &lt;label_value&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO181-1">
<para>One or more matching rules for namespaces. If more than one match rule is provided, all matching namespaces are selected.</para>
</callout>
</calloutlist>
<simpara>The following YAML describes the optional stanza for the pod selector:</simpara>
<formalpara>
<title>Pod selector stanza</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">podSelector: <co xml:id="CO182-1"/>
  matchLabels:
    &lt;label_name&gt;: &lt;label_value&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO182-1">
<para>Optional: One or more matching rules for pods in the namespaces that match the specified <literal>namespaceSelector</literal> rules. If specified, only pods that match are selected. Others pods in the namespace are not selected.</para>
</callout>
</calloutlist>
<simpara>In the following example, the <literal>EgressIP</literal> object associates the <literal>192.168.126.11</literal> and <literal>192.168.126.102</literal> egress IP addresses with pods that have the <literal>app</literal> label set to <literal>web</literal> and are in the namespaces that have the <literal>env</literal> label set to <literal>prod</literal>:</simpara>
<formalpara>
<title>Example <literal>EgressIP</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egress-group1
spec:
  egressIPs:
  - 192.168.126.11
  - 192.168.126.102
  podSelector:
    matchLabels:
      app: web
  namespaceSelector:
    matchLabels:
      env: prod</programlisting>
</para>
</formalpara>
<simpara>In the following example, the <literal>EgressIP</literal> object associates the <literal>192.168.127.30</literal> and <literal>192.168.127.40</literal> egress IP addresses with any pods that do not have the <literal>environment</literal> label set to <literal>development</literal>:</simpara>
<formalpara>
<title>Example <literal>EgressIP</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egress-group2
spec:
  egressIPs:
  - 192.168.127.30
  - 192.168.127.40
  namespaceSelector:
    matchExpressions:
    - key: environment
      operator: NotIn
      values:
      - development</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-ips-config-object_configuring-egress-ips-ovn">
<title>EgressIPconfig object</title>
<simpara>As a feature of egress IP, the <literal>reachabilityTotalTimeoutSeconds</literal> parameter configures the total timeout for checks that are sent by probes to egress IP nodes. The <literal>egressIPConfig</literal> object allows users to set the <literal>reachabilityTotalTimeoutSeconds</literal> <literal>spec</literal>. If the EgressIP node cannot be reached within this timeout, the node is declared down.</simpara>
<simpara>You can increase this value if your network is not stable enough to handle the current default value of 1 second.</simpara>
<simpara>The following YAML describes changing the <literal>reachabilityTotalTimeoutSeconds</literal> from the default 1 second probes to 5 second probes:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  defaultNetwork:
    ovnKubernetesConfig:
      egressIPConfig: <co xml:id="CO183-1"/>
        reachabilityTotalTimeoutSeconds: 5 <co xml:id="CO183-2"/>
      gatewayConfig:
        routingViaHost: false
      genevePort: 6081</programlisting>
<calloutlist>
<callout arearefs="CO183-1">
<para>The <literal>egressIPConfig</literal> holds the configurations for the options of the <literal>EgressIP</literal> object. Changing these configurations allows you to extend the <literal>EgressIP</literal> object.</para>
</callout>
<callout arearefs="CO183-2">
<para>The value for <literal>reachabilityTotalTimeoutSeconds</literal> accepts integer values from <literal>0</literal> to <literal>60</literal>. A value of 0 disables the reachability check of the egressIP node. Values of <literal>1</literal> to <literal>60</literal> correspond to the duration in seconds between probes sending the reachability check for the node.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-egress-ips-node_configuring-egress-ips-ovn">
<title>Labeling a node to host egress IP addresses</title>
<simpara>You can apply the <literal>k8s.ovn.org/egress-assignable=""</literal> label to a node in your cluster so that OpenShift Container Platform can assign one or more egress IP addresses to the node.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To label a node so that it can host one or more egress IP addresses, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label nodes &lt;node_name&gt; k8s.ovn.org/egress-assignable="" <co xml:id="CO184-1"/></programlisting>
<calloutlist>
<callout arearefs="CO184-1">
<para>The name of the node to label.</para>
</callout>
</calloutlist>
<tip>
<simpara>You can alternatively apply the following YAML to add the label to a node:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Node
metadata:
  labels:
    k8s.ovn.org/egress-assignable: ""
  name: &lt;node_name&gt;</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-egress-ips-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="assigning-egress-ips-ovn">Assigning egress IPs</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-egress-ips-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#labelselector-meta-v1">LabelSelector meta/v1</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#labelselectorrequirement-meta-v1">LabelSelectorRequirement meta/v1</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assigning-egress-ips-ovn">
<title>Assigning an egress IP address</title>

<simpara>As a cluster administrator, you can assign an egress IP address for traffic leaving the cluster from a namespace or from specific pods in a namespace.</simpara>
<section xml:id="nw-egress-ips-assign_assigning-egress-ips-ovn">
<title>Assigning an egress IP address to a namespace</title>
<simpara>You can assign one or more egress IP addresses to a namespace or to specific pods in a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster as a cluster administrator.</simpara>
</listitem>
<listitem>
<simpara>Configure at least one node to host an egress IP address.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>EgressIP</literal> object:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>&lt;egressips_name&gt;.yaml</literal> file where <literal>&lt;egressips_name&gt;</literal> is the name of the object.</simpara>
</listitem>
<listitem>
<simpara>In the file that you created, define an <literal>EgressIP</literal> object, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressIP
metadata:
  name: egress-project1
spec:
  egressIPs:
  - 192.168.127.10
  - 192.168.127.11
  namespaceSelector:
    matchLabels:
      env: qa</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To create the object, enter the following command.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;egressips_name&gt;.yaml <co xml:id="CO185-1"/></programlisting>
<calloutlist>
<callout arearefs="CO185-1">
<para>Replace <literal>&lt;egressips_name&gt;</literal> with the name of the object.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">egressips.k8s.ovn.org/&lt;egressips_name&gt; created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: Save the <literal>&lt;egressips_name&gt;.yaml</literal> file so that you can make changes later.</simpara>
</listitem>
<listitem>
<simpara>Add labels to the namespace that requires egress IP addresses. To add a label to the namespace of an <literal>EgressIP</literal> object defined in step 1, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label ns &lt;namespace&gt; env=qa <co xml:id="CO186-1"/></programlisting>
<calloutlist>
<callout arearefs="CO186-1">
<para>Replace <literal>&lt;namespace&gt;</literal> with the namespace that requires egress IP addresses.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="assigning-egress-ips-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-egress-ips-ovn">Configuring egress IP addresses</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-egress-traffic-loadbalancer-services">
<title>Configuring an egress service</title>

<simpara>As a cluster administrator, you can configure egress traffic for pods behind a load balancer service by using an egress service.</simpara>
<important>
<simpara>Egress service is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You can use the <literal>EgressService</literal> custom resource (CR) to manage egress traffic in the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara>Assign a load balancer service IP address as the source IP address for egress traffic for pods behind the load balancer service.</simpara>
<simpara>Assigning the load balancer IP address as the source IP address in this context is useful to present a single point of egress and ingress. For example, in some scenarios, an external system communicating with an application behind a load balancer service can expect the source and destination IP address for the application to be the same.</simpara>
<note>
<simpara>When you assign the load balancer service IP address to egress traffic for pods behind the service, OVN-Kubernetes restricts the ingress and egress point to a single node. This limits the load balancing of traffic that MetalLB typically provides.</simpara>
</note>
</listitem>
<listitem>
<simpara>Assign the egress traffic for pods behind a load balancer to a different network than the default node network.</simpara>
<simpara>This is useful to assign the egress traffic for applications behind a load balancer to a different network than the default network. Typically, the different network is implemented by using a VRF instance associated with a network interface.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-egress-service-ovn-cr_configuring-egress-traffic-loadbalancer-services">
<title>Egress service custom resource</title>
<simpara>Define the configuration for an egress service in an <literal>EgressService</literal> custom resource. The following YAML describes the fields for the configuration of an egress service:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressService
metadata:
  name: &lt;egress_service_name&gt; <co xml:id="CO187-1"/>
  namespace: &lt;namespace&gt; <co xml:id="CO187-2"/>
spec:
  sourceIPBy: &lt;egress_traffic_ip&gt; <co xml:id="CO187-3"/>
  nodeSelector: <co xml:id="CO187-4"/>
    matchLabels:
      node-role.kubernetes.io/&lt;role&gt;: ""
  network: &lt;egress_traffic_network&gt; <co xml:id="CO187-5"/></programlisting>
<calloutlist>
<callout arearefs="CO187-1">
<para>Specify the name for the egress service. The name of the <literal>EgressService</literal> resource must match the name of the load-balancer service that you want to modify.</para>
</callout>
<callout arearefs="CO187-2">
<para>Specify the namespace for the egress service. The namespace for the <literal>EgressService</literal> must match the namespace of the load-balancer service that you want to modify. The egress service is namespace-scoped.</para>
</callout>
<callout arearefs="CO187-3">
<para>Specify the source IP address of egress traffic for pods behind a service. Valid values are <literal>LoadBalancerIP</literal> or <literal>Network</literal>. Use the <literal>LoadBalancerIP</literal> value to assign the <literal>LoadBalancer</literal> service ingress IP address as the source IP address for egress traffic. Specify <literal>Network</literal> to assign the network interface IP address as the source IP address for egress traffic.</para>
</callout>
<callout arearefs="CO187-4">
<para>Optional: If you use the <literal>LoadBalancerIP</literal> value for the <literal>sourceIPBy</literal> specification, a single node handles the <literal>LoadBalancer</literal> service traffic. Use the <literal>nodeSelector</literal> field to limit which node can be assigned this task. When a node is selected to handle the service traffic, OVN-Kubernetes labels the node in the following format: <literal>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: ""</literal>. When the <literal>nodeSelector</literal> field is not specified, any node can manage the <literal>LoadBalancer</literal> service traffic.</para>
</callout>
<callout arearefs="CO187-5">
<para>Optional: Specify the routing table for egress traffic. If you do not include the <literal>network</literal> specification, the egress service uses the default host network.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example egress service specification</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressService
metadata:
  name: test-egress-service
  namespace: test-namespace
spec:
  sourceIPBy: "LoadBalancerIP"
  nodeSelector:
    matchLabels:
      vrf: "true"
  network: "2"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-service-ovn_configuring-egress-traffic-loadbalancer-services">
<title>Deploying an egress service</title>
<simpara>You can deploy an egress service to manage egress traffic for pods behind a <literal>LoadBalancer</literal> service.</simpara>
<simpara>The following example configures the egress traffic to have the same source IP address as the ingress IP address of the <literal>LoadBalancer</literal> service.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You configured MetalLB <literal>BGPPeer</literal> resources.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>IPAddressPool</literal> CR with the desired IP for the service:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ip-addr-pool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: example-pool
  namespace: metallb-system
spec:
  addresses:
  - 172.19.0.100/32</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ip-addr-pool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create <literal>Service</literal> and <literal>EgressService</literal> CRs:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>service-egress-service.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: example-service
  namespace: example-namespace
  annotations:
    metallb.universe.tf/address-pool: example-pool <co xml:id="CO188-1"/>
spec:
  selector:
    app: example
  ports:
    - name: http
      protocol: TCP
      port: 8080
      targetPort: 8080
  type: LoadBalancer
---
apiVersion: k8s.ovn.org/v1
kind: EgressService
metadata:
  name: example-service
  namespace: example-namespace
spec:
  sourceIPBy: "LoadBalancerIP" <co xml:id="CO188-2"/>
  nodeSelector: <co xml:id="CO188-3"/>
    matchLabels:
      node-role.kubernetes.io/worker: ""</programlisting>
<calloutlist>
<callout arearefs="CO188-1">
<para>The <literal>LoadBalancer</literal> service uses the IP address assigned by MetalLB from the <literal>example-pool</literal> IP address pool.</para>
</callout>
<callout arearefs="CO188-2">
<para>This example uses the <literal>LoadBalancerIP</literal> value to assign the ingress IP address of the <literal>LoadBalancer</literal> service as the source IP address of egress traffic.</para>
</callout>
<callout arearefs="CO188-3">
<para>When you specify the <literal>LoadBalancerIP</literal> value, a single node handles the <literal>LoadBalancer</literal> service&#8217;s traffic. In this example, only nodes with the <literal>worker</literal> label can be selected to handle the traffic. When a node is selected, OVN-Kubernetes labels the node in the following format <literal>egress-service.k8s.ovn.org/&lt;svc-namespace&gt;-&lt;svc-name&gt;: ""</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>If you use the <literal>sourceIPBy: "LoadBalancerIP"</literal> setting, you must specify the load-balancer node in the <literal>BGPAdvertisement</literal> custom resource (CR).</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the configuration for the service and egress service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f service-egress-service.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>BGPAdvertisement</literal> CR to advertise the service:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>service-bgp-advertisement.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: example-bgp-adv
  namespace: metallb-system
spec:
  ipAddressPools:
  - example-pool
  nodeSelector:
  - matchLabels:
      egress-service.k8s.ovn.org/example-namespace-example-service: "" <co xml:id="CO189-1"/></programlisting>
<calloutlist>
<callout arearefs="CO189-1">
<para>In this example, the <literal>EgressService</literal> CR configures the source IP address for egress traffic to use the load-balancer service IP address. Therefore, you must specify the load-balancer node for return traffic to use the same return path for the traffic originating from the pod.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that you can access the application endpoint of the pods running behind the MetalLB service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl &lt;external_ip_address&gt;:&lt;port_number&gt; <co xml:id="CO190-1"/></programlisting>
<calloutlist>
<callout arearefs="CO190-1">
<para>Update the external IP address and port number to suit your application endpoint.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If you assigned the <literal>LoadBalancer</literal> service&#8217;s ingress IP address as the source IP address for egress traffic, verify this configuration by using tools such as <literal>tcpdump</literal> to analyze packets received at the external client.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="nw-metallb-bgp-peer-vrf_configure-metallb-bgp-peers">Exposing a service through a network VRF</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-example-host-vrf_k8s_nmstate-updating-node-network-config">Example: Network interface with a VRF instance node network configuration policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="metallb-configure-return-traffic">Managing symmetric routing with MetalLB</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding">About virtual routing and forwarding</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="using-an-egress-router-ovn">
<title>Considerations for the use of an egress router pod</title>

<section xml:id="nw-egress-router-about_using-an-egress-router-ovn">
<title>About an egress router pod</title>
<simpara>The OpenShift Container Platform egress router pod redirects traffic to a specified remote server from a private source IP address that is not used for any other purpose. An egress router pod can send network traffic to servers that are set up to allow access only from specific IP addresses.</simpara>
<note>
<simpara>The egress router pod is not intended for every outgoing connection. Creating large numbers of egress router pods can exceed the limits of your network hardware. For example, creating an egress router pod for every project or application could exceed the number of local MAC addresses that the network interface can handle before reverting to filtering MAC addresses in software.</simpara>
</note>
<important>
<simpara>The egress router image is not compatible with Amazon AWS, Azure Cloud, or any other cloud platform that does not support layer 2 manipulations due to their incompatibility with macvlan traffic.</simpara>
</important>
<section xml:id="nw-egress-router-about-modes_using-an-egress-router-ovn">
<title>Egress router modes</title>
<simpara>In <emphasis>redirect mode</emphasis>, an egress router pod configures <literal>iptables</literal> rules to redirect traffic from its own IP address to one or more destination IP addresses. Client pods that need to use the reserved source IP address must be configured to access the service for the egress router rather than connecting directly to the destination IP. You can access the destination service and port from the application pod by using the <literal>curl</literal> command. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl &lt;router_service_IP&gt; &lt;port&gt;</programlisting>
<note>
<simpara>The egress router CNI plugin supports redirect mode only. This is a difference with the egress router implementation that you can deploy with OpenShift SDN. Unlike the egress router for OpenShift SDN, the egress router CNI plugin does not support HTTP proxy mode or DNS proxy mode.</simpara>
</note>
</section>
<section xml:id="nw-egress-router-about-router-pod-implementation_using-an-egress-router-ovn">
<title>Egress router pod implementation</title>
<simpara>The egress router implementation uses the egress router Container Network Interface (CNI) plugin. The plugin adds a secondary network interface to a pod.</simpara>
<simpara>An egress router is a pod that has two network interfaces. For example, the pod can have <literal>eth0</literal> and <literal>net1</literal> network interfaces. The <literal>eth0</literal> interface is on the cluster network and the pod continues to use the interface for ordinary cluster-related network traffic. The <literal>net1</literal> interface is on a secondary network and has an IP address and gateway for that network. Other pods in the OpenShift Container Platform cluster can access the egress router service and the service enables the pods to access external services. The egress router acts as a bridge between pods and an external system.</simpara>
<simpara>Traffic that leaves the egress router exits through a node, but the packets
have the MAC address of the <literal>net1</literal> interface from the egress router pod.</simpara>
<simpara>When you add an egress router custom resource, the Cluster Network Operator creates the following objects:</simpara>
<itemizedlist>
<listitem>
<simpara>The network attachment definition for the <literal>net1</literal> secondary network interface of the pod.</simpara>
</listitem>
<listitem>
<simpara>A deployment for the egress router.</simpara>
</listitem>
</itemizedlist>
<simpara>If you delete an egress router custom resource, the Operator deletes the two objects in the preceding list that are associated with the egress router.</simpara>
</section>
<section xml:id="nw-egress-router-about-deployments_using-an-egress-router-ovn">
<title>Deployment considerations</title>
<simpara>An egress router pod adds an additional IP address and MAC address to the primary network interface of the node. As a result, you might need to configure your hypervisor or cloud provider to allow the additional address.</simpara>
<variablelist>
<varlistentry>
<term>Red Hat OpenStack Platform (RHOSP)</term>
<listitem>
<simpara>If you deploy OpenShift Container Platform on RHOSP, you must allow traffic from the IP and MAC addresses of the egress router pod on your OpenStack environment. If you do not allow the traffic, then <link xlink:href="https://access.redhat.com/solutions/2803331">communication will fail</link>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack port set --allowed-address \
  ip_address=&lt;ip_address&gt;,mac_address=&lt;mac_address&gt; &lt;neutron_port_uuid&gt;</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>VMware vSphere</term>
<listitem>
<simpara>If you are using VMware vSphere, see the <link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-3507432E-AFEA-4B6B-B404-17A020575358.html">VMware documentation for securing vSphere standard switches</link>. View and change VMware vSphere default settings by selecting the host virtual switch from the vSphere Web Client.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Specifically, ensure that the following are enabled:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-942BD3AA-731B-4A05-8196-66F2B4BF1ACB.html">MAC Address Changes</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-7DC6486F-5400-44DF-8A62-6273798A2F80.html">Forged Transits</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-92F3AB1F-B4C5-4F25-A010-8820D7250350.html">Promiscuous Mode Operation</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-egress-router-about-failover_using-an-egress-router-ovn">
<title>Failover configuration</title>
<simpara>To avoid downtime, the Cluster Network Operator deploys the egress router pod as a deployment resource. The deployment name is <literal>egress-router-cni-deployment</literal>. The pod that corresponds to the deployment has a label of <literal>app=egress-router-cni</literal>.</simpara>
<simpara>To create a new service for the deployment, use the <literal>oc expose deployment/egress-router-cni-deployment --port &lt;port_number&gt;</literal> command or create a file like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: app-egress
spec:
  ports:
  - name: tcp-8080
    protocol: TCP
    port: 8080
  - name: tcp-8443
    protocol: TCP
    port: 8443
  - name: udp-80
    protocol: UDP
    port: 80
  type: ClusterIP
  selector:
    app: egress-router-cni</programlisting>
</section>
</section>
<section xml:id="using-an-egress-router-ovn-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="deploying-egress-router-ovn-redirection">Deploying an egress router in redirection mode</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-egress-router-ovn-redirection">
<title>Deploying an egress router pod in redirect mode</title>

<simpara>As a cluster administrator, you can deploy an egress router pod to redirect traffic to specified destination IP addresses from a reserved source IP address.</simpara>
<simpara>The egress router implementation uses the egress router Container Network Interface (CNI) plugin.</simpara>
<section xml:id="nw-egress-router-ovn-cr_deploying-egress-router-ovn-redirection">
<title>Egress router custom resource</title>
<simpara>Define the configuration for an egress router pod in an egress router custom resource. The following YAML describes the fields for the configuration of an egress router in redirect mode:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.operator.openshift.io/v1
kind: EgressRouter
metadata:
  name: &lt;egress_router_name&gt;
  namespace: &lt;namespace&gt;  <co xml:id="CO191-1"/>
spec:
  addresses: [  <co xml:id="CO191-2"/>
    {
      ip: "&lt;egress_router&gt;",  <co xml:id="CO191-3"/>
      gateway: "&lt;egress_gateway&gt;"  <co xml:id="CO191-4"/>
    }
  ]
  mode: Redirect
  redirect: {
    redirectRules: [  <co xml:id="CO191-5"/>
      {
        destinationIP: "&lt;egress_destination&gt;",
        port: &lt;egress_router_port&gt;,
        targetPort: &lt;target_port&gt;,  <co xml:id="CO191-6"/>
        protocol: &lt;network_protocol&gt;  <co xml:id="CO191-7"/>
      },
      ...
    ],
    fallbackIP: "&lt;egress_destination&gt;" <co xml:id="CO191-8"/>
  }</programlisting>
<calloutlist>
<callout arearefs="CO191-1">
<para>Optional: The <literal>namespace</literal> field specifies the namespace to create the egress router in. If you do not specify a value in the file or on the command line, the <literal>default</literal> namespace is used.</para>
</callout>
<callout arearefs="CO191-2">
<para>The <literal>addresses</literal> field specifies the IP addresses to configure on the secondary network interface.</para>
</callout>
<callout arearefs="CO191-3">
<para>The <literal>ip</literal> field specifies the reserved source IP address and netmask from the physical network that the node is on to use with egress router pod. Use CIDR notation to specify the IP address and netmask.</para>
</callout>
<callout arearefs="CO191-4">
<para>The <literal>gateway</literal> field specifies the IP address of the network gateway.</para>
</callout>
<callout arearefs="CO191-5">
<para>Optional: The <literal>redirectRules</literal> field specifies a combination of egress destination IP address, egress router port, and protocol. Incoming connections to the egress router on the specified port and protocol are routed to the destination IP address.</para>
</callout>
<callout arearefs="CO191-6">
<para>Optional: The <literal>targetPort</literal> field specifies the network port on the destination IP address. If this field is not specified, traffic is routed to the same network port that it arrived on.</para>
</callout>
<callout arearefs="CO191-7">
<para>The <literal>protocol</literal> field supports TCP, UDP, or SCTP.</para>
</callout>
<callout arearefs="CO191-8">
<para>Optional: The <literal>fallbackIP</literal> field specifies a destination IP address. If you do not specify any redirect rules, the egress router sends all traffic to this fallback IP address. If you specify redirect rules, any connections to network ports that are not defined in the rules are sent by the egress router to this fallback IP address. If you do not specify this field, the egress router rejects connections to network ports that are not defined in the rules.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example egress router specification</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.operator.openshift.io/v1
kind: EgressRouter
metadata:
  name: egress-router-redirect
spec:
  networkInterface: {
    macvlan: {
      mode: "Bridge"
    }
  }
  addresses: [
    {
      ip: "192.168.12.99/24",
      gateway: "192.168.12.1"
    }
  ]
  mode: Redirect
  redirect: {
    redirectRules: [
      {
        destinationIP: "10.0.0.99",
        port: 80,
        protocol: UDP
      },
      {
        destinationIP: "203.0.113.26",
        port: 8080,
        targetPort: 80,
        protocol: TCP
      },
      {
        destinationIP: "203.0.113.27",
        port: 8443,
        targetPort: 443,
        protocol: TCP
      }
    ]
  }</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-router-redirect-mode-ovn_deploying-egress-router-ovn-redirection">
<title>Deploying an egress router in redirect mode</title>
<simpara>You can deploy an egress router to redirect traffic from its own reserved source IP address to one or more destination IP addresses.</simpara>
<simpara>After you add an egress router, the client pods that need to use the reserved source IP address must be modified to connect to the egress router rather than connecting directly to the destination IP.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an egress router definition.</simpara>
</listitem>
<listitem>
<simpara>To ensure that other pods can find the IP address of the egress router pod, create a service that uses the egress router, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: web-app
    protocol: TCP
    port: 8080
  type: ClusterIP
  selector:
    app: egress-router-cni <co xml:id="CO192-1"/></programlisting>
<calloutlist>
<callout arearefs="CO192-1">
<para>Specify the label for the egress router. The value shown is added by the Cluster Network Operator and is not configurable.</para>
</callout>
</calloutlist>
<simpara>After you create the service, your pods can connect to the service. The egress router pod redirects traffic to the corresponding port on the destination IP address. The connections originate from the reserved source IP address.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>To verify that the Cluster Network Operator started the egress router, complete the following procedure:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>View the network attachment definition that the Operator created for the egress router:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definition egress-router-cni-nad</programlisting>
<simpara>The name of the network attachment definition is not configurable.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    AGE
egress-router-cni-nad   18m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the deployment for the egress router pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment egress-router-cni-deployment</programlisting>
<simpara>The name of the deployment is not configurable.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           READY   UP-TO-DATE   AVAILABLE   AGE
egress-router-cni-deployment   1/1     1            1           18m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the status of the egress router pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -l app=egress-router-cni</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                            READY   STATUS    RESTARTS   AGE
egress-router-cni-deployment-575465c75c-qkq6m   1/1     Running   0          18m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the logs and the routing table for the egress router pod.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the node name for the egress router pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POD_NODENAME=$(oc get pod -l app=egress-router-cni -o jsonpath="{.items[0].spec.nodeName}")</programlisting>
</listitem>
<listitem>
<simpara>Enter into a debug session on the target node. This step instantiates a debug pod called <literal>&lt;node_name&gt;-debug</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/$POD_NODENAME</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell. The debug pod mounts the root file system of the host in <literal>/host</literal> within the pod. By changing the root directory to <literal>/host</literal>, you can run binaries from the executable paths of the host:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># chroot /host</programlisting>
</listitem>
<listitem>
<simpara>From within the <literal>chroot</literal> environment console, display the egress router logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat /tmp/egress-router-log</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2021-04-26T12:27:20Z [debug] Called CNI ADD
2021-04-26T12:27:20Z [debug] Gateway: 192.168.12.1
2021-04-26T12:27:20Z [debug] IP Source Addresses: [192.168.12.99/24]
2021-04-26T12:27:20Z [debug] IP Destinations: [80 UDP 10.0.0.99/30 8080 TCP 203.0.113.26/30 80 8443 TCP 203.0.113.27/30 443]
2021-04-26T12:27:20Z [debug] Created macvlan interface
2021-04-26T12:27:20Z [debug] Renamed macvlan to "net1"
2021-04-26T12:27:20Z [debug] Adding route to gateway 192.168.12.1 on macvlan interface
2021-04-26T12:27:20Z [debug] deleted default route {Ifindex: 3 Dst: &lt;nil&gt; Src: &lt;nil&gt; Gw: 10.128.10.1 Flags: [] Table: 254}
2021-04-26T12:27:20Z [debug] Added new default route with gateway 192.168.12.1
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat PREROUTING -i eth0 -p UDP --dport 80 -j DNAT --to-destination 10.0.0.99
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat PREROUTING -i eth0 -p TCP --dport 8080 -j DNAT --to-destination 203.0.113.26:80
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat PREROUTING -i eth0 -p TCP --dport 8443 -j DNAT --to-destination 203.0.113.27:443
2021-04-26T12:27:20Z [debug] Added iptables rule: iptables -t nat -o net1 -j SNAT --to-source 192.168.12.99</programlisting>
</para>
</formalpara>
<simpara>The logging file location and logging level are not configurable when you start the egress router by creating an <literal>EgressRouter</literal> object as described in this procedure.</simpara>
</listitem>
<listitem>
<simpara>From within the <literal>chroot</literal> environment console, get the container ID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># crictl ps --name egress-router-cni-pod | awk '{print $1}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">CONTAINER
bac9fae69ddb6</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Determine the process ID of the container. In this example, the container ID is <literal>bac9fae69ddb6</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># crictl inspect -o yaml bac9fae69ddb6 | grep 'pid:' | awk '{print $2}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">68857</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter the network namespace of the container:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># nsenter -n -t 68857</programlisting>
</listitem>
<listitem>
<simpara>Display the routing table:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># ip route</programlisting>
<simpara>In the following example output, the <literal>net1</literal> network interface is the default route. Traffic for the cluster network uses the <literal>eth0</literal> network interface. Traffic for the <literal>192.168.12.0/24</literal> network uses the <literal>net1</literal> network interface and originates from the reserved source IP address <literal>192.168.12.99</literal>. The pod routes all other traffic to the gateway at IP address <literal>192.168.12.1</literal>. Routing for the service network is not shown.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">default via 192.168.12.1 dev net1
10.128.10.0/23 dev eth0 proto kernel scope link src 10.128.10.18
192.168.12.0/24 dev net1 proto kernel scope link src 192.168.12.99
192.168.12.1 dev net1</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-ovn-kubernetes-enabling-multicast">
<title>Enabling multicast for a project</title>

<section xml:id="nw-about-multicast_ovn-kubernetes-enabling-multicast">
<title>About multicast</title>
<simpara>With IP multicast, data is broadcast to many IP addresses simultaneously.</simpara>
<important>
<itemizedlist>
<listitem>
<simpara>At this time, multicast is best used for low-bandwidth coordination or service discovery and not a high-bandwidth solution.</simpara>
</listitem>
<listitem>
<simpara>By default, network policies affect all connections in a namespace. However, multicast is unaffected by network policies. If multicast is enabled in the same namespace as your network policies, it is always allowed, even if there is a <literal>deny-all</literal> network policy. Cluster administrators should consider the implications to the exemption of multicast from network policies before enabling it.</simpara>
</listitem>
</itemizedlist>
</important>
<simpara>Multicast traffic between OpenShift Container Platform pods is disabled by default. If you are using the OVN-Kubernetes network plugin, you can enable multicast on a per-project basis.</simpara>
</section>
<section xml:id="nw-enabling-multicast_ovn-kubernetes-enabling-multicast">
<title>Enabling multicast between pods</title>
<simpara>You can enable multicast between pods for your project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal>
role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to enable multicast for a project. Replace <literal>&lt;namespace&gt;</literal> with the namespace for the project you want to enable multicast for.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate namespace &lt;namespace&gt; \
    k8s.ovn.org/multicast-enabled=true</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add the annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/multicast-enabled: "true"</programlisting>
</tip>
</listitem>
</itemizedlist>
<formalpara>
<title>Verification</title>
<para>To verify that multicast is enabled for a project, complete the following procedure:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Change your current project to the project that you enabled multicast for. Replace <literal>&lt;project&gt;</literal> with the project name.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project &lt;project&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create a pod to act as a multicast receiver:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: mlistener
  labels:
    app: multicast-verify
spec:
  containers:
    - name: mlistener
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat hostname &amp;&amp; sleep inf"]
      ports:
        - containerPort: 30102
          name: mlistener
          protocol: UDP
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create a pod to act as a multicast sender:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: msender
  labels:
    app: multicast-verify
spec:
  containers:
    - name: msender
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat &amp;&amp; sleep inf"]
EOF</programlisting>
</listitem>
<listitem>
<simpara>In a new terminal window or tab, start the multicast listener.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the IP address for the Pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POD_IP=$(oc get pods mlistener -o jsonpath='{.status.podIP}')</programlisting>
</listitem>
<listitem>
<simpara>Start the multicast listener by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec mlistener -i -t -- \
    socat UDP4-RECVFROM:30102,ip-add-membership=224.1.0.1:$POD_IP,fork EXEC:hostname</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Start the multicast transmitter.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the pod network IP address range:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CIDR=$(oc get Network.config.openshift.io cluster \
    -o jsonpath='{.status.clusterNetwork[0].cidr}')</programlisting>
</listitem>
<listitem>
<simpara>To send a multicast message, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec msender -i -t -- \
    /bin/bash -c "echo | socat STDIO UDP4-DATAGRAM:224.1.0.1:30102,range=$CIDR,ip-multicast-ttl=64"</programlisting>
<simpara>If multicast is working, the previous command returns the following output:</simpara>
<programlisting language="text" linenumbering="unnumbered">mlistener</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-ovn-kubernetes-disabling-multicast">
<title>Disabling multicast for a project</title>

<section xml:id="nw-disabling-multicast_ovn-kubernetes-disabling-multicast">
<title>Disabling multicast between pods</title>
<simpara>You can disable multicast between pods for your project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Disable multicast by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate namespace &lt;namespace&gt; \ <co xml:id="CO193-1"/>
    k8s.ovn.org/multicast-enabled-</programlisting>
<calloutlist>
<callout arearefs="CO193-1">
<para>The <literal>namespace</literal> for the project you want to disable multicast for.</para>
</callout>
</calloutlist>
<tip>
<simpara>You can alternatively apply the following YAML to delete the annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: &lt;namespace&gt;
  annotations:
    k8s.ovn.org/multicast-enabled: null</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="tracking-network-flows">
<title>Tracking network flows</title>

<simpara>As a cluster administrator, you can collect information about pod network flows from your cluster to assist with the following areas:</simpara>
<itemizedlist>
<listitem>
<simpara>Monitor ingress and egress traffic on the pod network.</simpara>
</listitem>
<listitem>
<simpara>Troubleshoot performance issues.</simpara>
</listitem>
<listitem>
<simpara>Gather data for capacity planning and security audits.</simpara>
</listitem>
</itemizedlist>
<simpara>When you enable the collection of the network flows, only the metadata about the traffic is collected.
For example, packet data is not collected, but the protocol, source address, destination address, port numbers, number of bytes, and other packet-level information is collected.</simpara>
<simpara>The data is collected in one or more of the following record formats:</simpara>
<itemizedlist>
<listitem>
<simpara>NetFlow</simpara>
</listitem>
<listitem>
<simpara>sFlow</simpara>
</listitem>
<listitem>
<simpara>IPFIX</simpara>
</listitem>
</itemizedlist>
<simpara>When you configure the Cluster Network Operator (CNO) with one or more collector IP addresses and port numbers, the Operator configures Open vSwitch (OVS) on each node to send the network flows records to each collector.</simpara>
<simpara>You can configure the Operator to send records to more than one type of network flow collector. For example, you can send records to NetFlow collectors and also send records to sFlow collectors.</simpara>
<simpara>When OVS sends data to the collectors, each type of collector receives identical records. For example, if you configure two NetFlow collectors, OVS on a node sends identical records to the two collectors. If you also configure two sFlow collectors, the two sFlow collectors receive identical records. However, each collector type has a unique record format.</simpara>
<simpara>Collecting the network flows data and sending the records to collectors affects performance. Nodes process packets at a slower rate. If the performance impact is too great, you can delete the destinations for collectors to disable collecting network flows data and restore performance.</simpara>
<note>
<simpara>Enabling network flow collectors might have an impact on the overall performance of the cluster network.</simpara>
</note>
<section xml:id="nw-network-flows-object_tracking-network-flows">
<title>Network object configuration for tracking network flows</title>
<simpara>The fields for configuring network flows collectors in the Cluster Network Operator (CNO) are shown in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Network flows configuration</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Field</entry>
<entry align="left" valign="middle">Type</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>The name of the CNO object. This name is always <literal>cluster</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.exportNetworkFlows</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>object</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>One or more of <literal>netFlow</literal>, <literal>sFlow</literal>, or <literal>ipfix</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.exportNetworkFlows.netFlow.collectors</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A list of IP address and network port pairs for up to 10 collectors.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.exportNetworkFlows.sFlow.collectors</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A list of IP address and network port pairs for up to 10 collectors.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>spec.exportNetworkFlows.ipfix.collectors</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>array</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>A list of IP address and network port pairs for up to 10 collectors.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>After applying the following manifest to the CNO, the Operator configures Open vSwitch (OVS) on each node in the cluster to send network flows records to the NetFlow collector that is listening at <literal>192.168.1.99:2056</literal>.</simpara>
<formalpara>
<title>Example configuration for tracking network flows</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  exportNetworkFlows:
    netFlow:
      collectors:
        - 192.168.1.99:2056</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-network-flows-create_tracking-network-flows">
<title>Adding destinations for network flows collectors</title>
<simpara>As a cluster administrator, you can configure the Cluster Network Operator (CNO) to send network flows metadata about the pod network to a network flows collector.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have a network flows collector and know the IP address and port that it listens on.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a patch file that specifies the network flows collector type and the IP address and port information of the collectors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">spec:
  exportNetworkFlows:
    netFlow:
      collectors:
        - 192.168.1.99:2056</programlisting>
</listitem>
<listitem>
<simpara>Configure the CNO with the network flows collectors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch network.operator cluster --type merge -p "$(cat &lt;file_name&gt;.yaml)"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">network.operator.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>Verification is not typically necessary. You can run the following command to confirm that Open vSwitch (OVS) on each node is configured to send network flows records to one or more collectors.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>View the Operator configuration to confirm that the <literal>exportNetworkFlows</literal> field is configured:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.operator cluster -o jsonpath="{.spec.exportNetworkFlows}"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{"netFlow":{"collectors":["192.168.1.99:2056"]}}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the network flows configuration in OVS from each node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for pod in $(oc get pods -n openshift-ovn-kubernetes -l app=ovnkube-node -o jsonpath='{range@.items[*]}{.metadata.name}{"\n"}{end}');
  do ;
    echo;
    echo $pod;
    oc -n openshift-ovn-kubernetes exec -c ovnkube-controller $pod \
      -- bash -c 'for type in ipfix sflow netflow ; do ovs-vsctl find $type ; done';
done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovnkube-node-xrn4p
_uuid               : a4d2aaca-5023-4f3d-9400-7275f92611f9
active_timeout      : 60
add_id_to_interface : false
engine_id           : []
engine_type         : []
external_ids        : {}
targets             : ["192.168.1.99:2056"]

ovnkube-node-z4vq9
_uuid               : 61d02fdb-9228-4993-8ff5-b27f01a29bd6
active_timeout      : 60
add_id_to_interface : false
engine_id           : []
engine_type         : []
external_ids        : {}
targets             : ["192.168.1.99:2056"]-

...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-network-flows-delete_tracking-network-flows">
<title>Deleting all destinations for network flows collectors</title>
<simpara>As a cluster administrator, you can configure the Cluster Network Operator (CNO) to stop sending network flows metadata to a network flows collector.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Remove all network flows collectors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch network.operator cluster --type='json' \
    -p='[{"op":"remove", "path":"/spec/exportNetworkFlows"}]'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">network.operator.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_tracking-network-flows" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</link>]</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-hybrid-networking">
<title>Configuring hybrid networking</title>

<simpara>As a cluster administrator, you can configure the Red Hat OpenShift Networking OVN-Kubernetes network plugin to allow Linux and Windows nodes to host Linux and Windows workloads, respectively.</simpara>
<section xml:id="configuring-hybrid-ovnkubernetes_configuring-hybrid-networking">
<title>Configuring hybrid networking with OVN-Kubernetes</title>
<simpara>You can configure your cluster to use hybrid networking with the OVN-Kubernetes network plugin. This allows a hybrid cluster that supports different node networking configurations.</simpara>
<note>
<simpara>This configuration is necessary to run both Linux and Windows nodes in the same cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Ensure that the cluster uses the OVN-Kubernetes network plugin.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To configure the OVN-Kubernetes hybrid network overlay, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch networks.operator.openshift.io cluster --type=merge \
  -p '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "hybridOverlayConfig":{
            "hybridClusterNetwork":[
              {
                "cidr": "&lt;cidr&gt;",
                "hostPrefix": &lt;prefix&gt;
              }
            ],
            "hybridOverlayVXLANPort": &lt;overlay_port&gt;
          }
        }
      }
    }
  }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>cidr</literal></term>
<listitem>
<simpara>Specify the CIDR configuration used for nodes on the additional overlay network. This CIDR cannot overlap with the cluster network CIDR.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>hostPrefix</literal></term>
<listitem>
<simpara>Specifies the subnet prefix length to assign to each individual node. For example, if <literal>hostPrefix</literal> is set to <literal>23</literal>, then each node is assigned a <literal>/23</literal> subnet out of the given <literal>cidr</literal>, which allows for 510 (2^(32 - 23) - 2) pod IP addresses. If you are required to provide access to nodes from an external network, configure load balancers and routers to manage the traffic.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>hybridOverlayVXLANPort</literal></term>
<listitem>
<simpara>Specify a custom VXLAN port for the additional overlay network. This is required for running Windows nodes in a cluster installed on vSphere, and must not be configured for any other cloud provider. The custom port can be any open port excluding the default <literal>4789</literal> port. For more information on this requirement, see the Microsoft documentation on <link xlink:href="https://docs.microsoft.com/en-us/virtualization/windowscontainers/kubernetes/common-problems#pod-to-pod-connectivity-between-hosts-is-broken-on-my-kubernetes-cluster-running-on-vsphere">Pod-to-pod connectivity between hosts is broken</link>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>Windows Server Long-Term Servicing Channel (LTSC): Windows Server 2019 is not supported on clusters with a custom <literal>hybridOverlayVXLANPort</literal> value because this Windows server version does not support selecting a custom VXLAN port.</simpara>
</note>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">network.operator.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.operator.openshift.io -o jsonpath="{.items[0].spec.defaultNetwork.ovnKubernetesConfig}"</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-hybrid-networking-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/windows_container_support_for_openshift/#understanding-windows-container-workloads">Understanding Windows container workloads</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/windows_container_support_for_openshift/#enabling-windows-container-workloads">Enabling Windows container workloads</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-network-customizations">Installing a cluster on AWS with network customizations</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-azure-network-customizations">Installing a cluster on Azure with network customizations</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_openshift-sdn-network-plugin">
<title>OpenShift SDN network plugin</title>
<section xml:id="about-openshift-sdn">
<title>About the OpenShift SDN network plugin</title>

<simpara>Part of Red Hat OpenShift Networking, OpenShift SDN is a network plugin that uses a
software-defined networking (SDN) approach to provide a unified cluster network
that enables communication between pods across the OpenShift Container Platform cluster. This
pod network is established and maintained by OpenShift SDN, which configures
an overlay network using Open vSwitch (OVS).</simpara>
<section xml:id="nw-openshift-sdn-modes_about-openshift-sdn">
<title>OpenShift SDN network isolation modes</title>
<simpara>OpenShift SDN provides three SDN modes for configuring the pod network:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis>Network policy</emphasis> mode allows project administrators to configure their own
isolation policies using <literal>NetworkPolicy</literal> objects. Network policy is the default mode in OpenShift Container Platform 4.14.</simpara>
</listitem>
<listitem>
<simpara><emphasis>Multitenant</emphasis> mode provides project-level isolation for pods and services. Pods from different projects cannot send packets to or receive packets from pods and services of a different project. You can disable isolation for a project, allowing it to send network traffic to all pods and services in the entire cluster and receive network traffic from those pods and services.</simpara>
</listitem>
<listitem>
<simpara><emphasis>Subnet</emphasis> mode provides a flat pod network where every pod can communicate with every other pod and service. The network policy mode provides the same functionality as subnet mode.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ovn-kubernetes-matrix_about-openshift-sdn">
<title>Supported network plugin feature matrix</title>
<simpara>Red Hat OpenShift Networking offers two options for the network plugin, OpenShift SDN and OVN-Kubernetes, for the network plugin. The following table summarizes the current feature support for both network plugins:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Default CNI network plugin feature comparison</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Feature</entry>
<entry align="left" valign="top">OpenShift SDN</entry>
<entry align="left" valign="top">OVN-Kubernetes</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Egress IPs</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Egress firewall <superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Egress router</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported <superscript>[2]</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hybrid networking</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IPsec encryption</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IPv6</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported <superscript>[3]</superscript> <superscript>[4]</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes network policy</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes network policy logs</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Multicast</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hardware offloading</simpara></entry>
<entry align="left" valign="top"><simpara>Not supported</simpara></entry>
<entry align="left" valign="top"><simpara>Supported</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>Egress firewall is also known as egress network policy in OpenShift SDN. This is not the same as network policy egress.</simpara>
</listitem>
<listitem>
<simpara>Egress router for OVN-Kubernetes supports only redirect mode.</simpara>
</listitem>
<listitem>
<simpara>IPv6 is supported only on bare metal, vSphere, IBM Power&#174;, and IBM Z&#174; clusters.</simpara>
</listitem>
<listitem>
<simpara>IPv6 single stack is not supported on IBM Power&#174; and IBM Z&#174; clusters.</simpara>
</listitem>
</orderedlist>
</para>
</section>
</section>
<section xml:id="migrate-to-openshift-sdn">
<title>Migrating to the OpenShift SDN network plugin</title>

<simpara>As a cluster administrator, you can migrate to the OpenShift SDN network plugin from the OVN-Kubernetes network plugin.</simpara>
<simpara>To learn more about OpenShift SDN, read <link linkend="about-openshift-sdn">About the OpenShift SDN network plugin</link>.</simpara>
<section xml:id="how-the-migration-process-works_migrate-to-openshift-sdn">
<title>How the migration process works</title>
<simpara>The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Migrating to OpenShift SDN from OVN-Kubernetes</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">User-initiated steps</entry>
<entry align="left" valign="top">Migration activity</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Set the <literal>migration</literal> field of the <literal>Network.operator.openshift.io</literal> custom resource (CR) named <literal>cluster</literal> to <literal>OpenShiftSDN</literal>. Make sure the <literal>migration</literal> field is <literal>null</literal> before setting it to a value.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>Cluster Network Operator (CNO)</term>
<listitem>
<simpara>Updates the status of the <literal>Network.config.openshift.io</literal> CR named <literal>cluster</literal> accordingly.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Machine Config Operator (MCO)</term>
<listitem>
<simpara>Rolls out an update to the systemd configuration necessary for OpenShift SDN; the MCO updates a single machine per pool at a time by default, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Update the <literal>networkType</literal> field of the <literal>Network.config.openshift.io</literal> CR.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>CNO</term>
<listitem>
<simpara>Performs the following actions:</simpara>
<itemizedlist>
<listitem>
<simpara>Destroys the OVN-Kubernetes control plane pods.</simpara>
</listitem>
<listitem>
<simpara>Deploys the OpenShift SDN control plane pods.</simpara>
</listitem>
<listitem>
<simpara>Updates the Multus objects to reflect the new network plugin.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reboot each node in the cluster.</simpara></entry>
<entry align="left" valign="top"><variablelist>
<varlistentry>
<term>Cluster</term>
<listitem>
<simpara>As nodes reboot, the cluster assigns IP addresses to pods on the OpenShift SDN cluster network.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-ovn-kubernetes-rollback_migrate-to-openshift-sdn">
<title>Migrating to the OpenShift SDN network plugin</title>
<simpara>As a cluster administrator, you can migrate to the OpenShift SDN Container Network Interface (CNI) network plugin.
During the migration you must reboot every node in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>A cluster installed on infrastructure configured with the OVN-Kubernetes network plugin.</simpara>
</listitem>
<listitem>
<simpara>A recent backup of the etcd database is available.</simpara>
</listitem>
<listitem>
<simpara>A reboot can be triggered manually for each node.</simpara>
</listitem>
<listitem>
<simpara>The cluster is in a known good state, without any errors.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Stop all of the machine configuration pools managed by the Machine Config Operator (MCO):</simpara>
<itemizedlist>
<listitem>
<simpara>Stop the master configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": true } }'</programlisting>
</listitem>
<listitem>
<simpara>Stop the worker machine configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec":{ "paused": true } }'</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To prepare for the migration, set the migration field to <literal>null</literal> by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</programlisting>
</listitem>
<listitem>
<simpara>To start the migration, set the network plugin back to OpenShift SDN by entering the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OpenShiftSDN" } } }'

$ oc patch Network.config.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "networkType": "OpenShiftSDN" } }'</programlisting>
</listitem>
<listitem>
<simpara>Optional: You can disable automatic migration of several OVN-Kubernetes capabilities to the OpenShift SDN equivalents:</simpara>
<itemizedlist>
<listitem>
<simpara>Egress IPs</simpara>
</listitem>
<listitem>
<simpara>Egress firewall</simpara>
</listitem>
<listitem>
<simpara>Multicast</simpara>
</listitem>
</itemizedlist>
<simpara>To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OpenShiftSDN",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</programlisting>
<simpara>where:</simpara>
<simpara><literal>bool</literal>: Specifies whether to enable migration of the feature. The default is <literal>true</literal>.</simpara>
</listitem>
<listitem>
<simpara>Optional: You can customize the following settings for OpenShift SDN to meet your network infrastructure requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>Maximum transmission unit (MTU)</simpara>
</listitem>
<listitem>
<simpara>VXLAN port</simpara>
</listitem>
</itemizedlist>
<simpara>To customize either or both of the previously noted settings, customize and enter the following command. If you do not need to change the default value, omit the key from the patch.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":&lt;mtu&gt;,
          "vxlanPort":&lt;port&gt;
    }}}}'</programlisting>
<variablelist>
<varlistentry>
<term><literal>mtu</literal></term>
<listitem>
<simpara>The MTU for the VXLAN overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <literal>50</literal> less than the smallest node MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>port</literal></term>
<listitem>
<simpara>The UDP port for the VXLAN overlay network. If a value is not specified, the default is <literal>4789</literal>. The port cannot be the same as the Geneve port that is used by OVN-Kubernetes. The default value for the Geneve port is <literal>6081</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example patch command</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "openshiftSDNConfig":{
          "mtu":1200
    }}}}'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Wait until the Multus daemon set rollout completes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-multus rollout status daemonset/multus</programlisting>
<simpara>The name of the Multus pods is in form of <literal>multus-&lt;xxxxx&gt;</literal> where <literal>&lt;xxxxx&gt;</literal> is a random sequence of letters. It might take several moments for the pods to restart.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:</simpara>
<itemizedlist>
<listitem>
<simpara>With the <literal>oc rsh</literal> command, you can use a bash script similar to the following:</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</programlisting>
</listitem>
<listitem>
<simpara>With the <literal>ssh</literal> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After the nodes in your cluster have rebooted, start all of the machine configuration pools:</simpara>
<itemizedlist>
<listitem>
<simpara>Start the master configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool master --type='merge' --patch \
  '{ "spec": { "paused": false } }'</programlisting>
</listitem>
<listitem>
<simpara>Start the worker configuration pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineConfigPool worker --type='merge' --patch \
  '{ "spec": { "paused": false } }'</programlisting>
</listitem>
</itemizedlist>
<simpara>As the MCO updates machines in each config pool, it reboots each node.</simpara>
<simpara>By default the MCO updates a single machine per pool at a time, so the time that the migration requires to complete grows with the size of the cluster.</simpara>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Confirm that the migration succeeded:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To confirm that the network plugin is OpenShift SDN, enter the following command.  The value of <literal>status.networkType</literal> must be <literal>OpenShiftSDN</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>To confirm that the cluster nodes are in the <literal>Ready</literal> state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>If a node is stuck in the <literal>NotReady</literal> state, investigate the machine config daemon pod logs and resolve any errors.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>To list the pods, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n openshift-machine-config-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</programlisting>
</para>
</formalpara>
<simpara>The names for the config daemon pods are in the following format: <literal>machine-config-daemon-&lt;seq&gt;</literal>. The <literal>&lt;seq&gt;</literal> value is a random five character alphanumeric sequence.</simpara>
</listitem>
<listitem>
<simpara>To display the pod log for each machine config daemon pod shown in the previous output, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</programlisting>
<simpara>where <literal>pod</literal> is the name of a machine config daemon pod.</simpara>
</listitem>
<listitem>
<simpara>Resolve any errors in the logs shown by the output from the previous command.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To confirm that your pods are not in an error state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</programlisting>
<simpara>If pods on a node are in an error state, reboot that node.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Complete the following steps only if the migration succeeds and your cluster is in a good state:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To remove the migration configuration from the Cluster Network Operator configuration object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove the OVN-Kubernetes configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "ovnKubernetesConfig":null } } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove the OVN-Kubernetes network provider namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-ovn-kubernetes</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="migrate-to-openshift-sdn-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-operator-configuration-parameters-for-openshift-sdn_cluster-network-operator">Configuration parameters for the OpenShift SDN network plugin</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#backup-etcd">Backing up etcd</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="about-network-policy">About network policy</link></simpara>
</listitem>
<listitem>
<simpara>OpenShift SDN capabilities</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="assigning-egress-ips">Configuring egress IPs for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-egress-firewall">Configuring an egress firewall for a project</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="enabling-multicast">Enabling multicast for a project</link></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#network-operator-openshift-io-v1">Network [operator.openshift.io/v1</link>]</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="roll-back-to-ovn-kubernetes">
<title>Rolling back to the OVN-Kubernetes network plugin</title>

<simpara>As a cluster administrator, you can rollback to the OVN-Kubernetes network plugin from the OpenShift SDN network plugin if the migration to OpenShift SDN is unsuccessful.</simpara>
<simpara>To learn more about OVN-Kubernetes, read <link linkend="about-ovn-kubernetes">About the OVN-Kubernetes network plugin</link>.</simpara>
<section xml:id="nw-ovn-kubernetes-migration_roll-back-to-ovn-kubernetes">
<title>Migrating to the OVN-Kubernetes network plugin</title>
<simpara>As a cluster administrator, you can change the network plugin for your cluster to OVN-Kubernetes.
During the migration, you must reboot every node in your cluster.</simpara>
<important>
<simpara>While performing the migration, your cluster is unavailable and workloads might be interrupted.
Perform the migration only when an interruption in service is acceptable.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster configured with the OpenShift SDN CNI network plugin in the network policy isolation mode.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>A recent backup of the etcd database is available.</simpara>
</listitem>
<listitem>
<simpara>A reboot can be triggered manually for each node.</simpara>
</listitem>
<listitem>
<simpara>The cluster is in a known good state, without any errors.</simpara>
</listitem>
<listitem>
<simpara>On all cloud platforms after updating software, a security group rule must be in place to allow UDP packets on port <literal>6081</literal> for all nodes.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To backup the configuration for the cluster network, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get Network.config.openshift.io cluster -o yaml &gt; cluster-openshift-sdn.yaml</programlisting>
</listitem>
<listitem>
<simpara>To prepare all the nodes for the migration, set the <literal>migration</literal> field on the Cluster Network Operator configuration object by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": { "networkType": "OVNKubernetes" } } }'</programlisting>
<note>
<simpara>This step does not deploy OVN-Kubernetes immediately. Instead, specifying the <literal>migration</literal> field triggers the Machine Config Operator (MCO) to apply new machine configs to all the nodes in the cluster in preparation for the OVN-Kubernetes deployment.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: You can disable automatic migration of several OpenShift SDN capabilities to the OVN-Kubernetes equivalents:</simpara>
<itemizedlist>
<listitem>
<simpara>Egress IPs</simpara>
</listitem>
<listitem>
<simpara>Egress firewall</simpara>
</listitem>
<listitem>
<simpara>Multicast</simpara>
</listitem>
</itemizedlist>
<simpara>To disable automatic migration of the configuration for any of the previously noted OpenShift SDN features, specify the following keys:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{
    "spec": {
      "migration": {
        "networkType": "OVNKubernetes",
        "features": {
          "egressIP": &lt;bool&gt;,
          "egressFirewall": &lt;bool&gt;,
          "multicast": &lt;bool&gt;
        }
      }
    }
  }'</programlisting>
<simpara>where:</simpara>
<simpara><literal>bool</literal>: Specifies whether to enable migration of the feature. The default is <literal>true</literal>.</simpara>
</listitem>
<listitem>
<simpara>Optional: You can customize the following settings for OVN-Kubernetes to meet your network infrastructure requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>Maximum transmission unit (MTU). Consider the following before customizing the MTU for this optional step:</simpara>
<itemizedlist>
<listitem>
<simpara>If you use the default MTU, and you want to keep the default MTU during migration, this step can be ignored.</simpara>
</listitem>
<listitem>
<simpara>If you used a custom MTU, and you want to keep the custom MTU during migration, you must declare the custom MTU value in this step.</simpara>
</listitem>
<listitem>
<simpara>This step does not work if you want to change the MTU value during migration. Instead, you must first follow the instructions for "Changing the cluster MTU". You can then keep the custom MTU value by performing this procedure and declaring the custom MTU value in this step.</simpara>
<note>
<simpara>OpenShift-SDN and OVN-Kubernetes have different overlay overhead. MTU values should be selected by following the guidelines found on the "MTU value selection" page.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Geneve (Generic Network Virtualization Encapsulation) overlay network port</simpara>
</listitem>
<listitem>
<simpara>OVN-Kubernetes IPv4 internal subnet</simpara>
</listitem>
<listitem>
<simpara>OVN-Kubernetes IPv6 internal subnet</simpara>
</listitem>
</itemizedlist>
<simpara>To customize either of the previously noted settings, enter and customize the following command. If you do not need to change the default value, omit the key from the patch.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":&lt;mtu&gt;,
          "genevePort":&lt;port&gt;,
          "v4InternalSubnet":"&lt;ipv4_subnet&gt;",
          "v6InternalSubnet":"&lt;ipv6_subnet&gt;"
    }}}}'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>mtu</literal></term>
<listitem>
<simpara>The MTU for the Geneve overlay network. This value is normally configured automatically, but if the nodes in your cluster do not all use the same MTU, then you must set this explicitly to <literal>100</literal> less than the smallest node MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>port</literal></term>
<listitem>
<simpara>The UDP port for the Geneve overlay network. If a value is not specified, the default is <literal>6081</literal>. The port cannot be the same as the VXLAN port that is used by OpenShift SDN. The default value for the VXLAN port is <literal>4789</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ipv4_subnet</literal></term>
<listitem>
<simpara>An IPv4 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <literal>100.64.0.0/16</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>ipv6_subnet</literal></term>
<listitem>
<simpara>An IPv6 address range for internal use by OVN-Kubernetes. You must ensure that the IP address range does not overlap with any other subnet used by your OpenShift Container Platform installation. The IP address range must be larger than the maximum number of nodes that can be added to the cluster. The default value is <literal>fd98::/48</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example patch command to update <literal>mtu</literal> field</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge \
  --patch '{
    "spec":{
      "defaultNetwork":{
        "ovnKubernetesConfig":{
          "mtu":1200
    }}}}'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
<note>
<simpara>By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
<simpara>The machine config must include the following update to the systemd configuration:</simpara>
<programlisting language="plain" linenumbering="unnumbered">ExecStart=/usr/local/bin/configure-ovs.sh OVNKubernetes</programlisting>
</listitem>
<listitem>
<simpara>If a node is stuck in the <literal>NotReady</literal> state, investigate the machine config daemon pod logs and resolve any errors.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>To list the pods, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n openshift-machine-config-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         READY   STATUS    RESTARTS   AGE
machine-config-controller-75f756f89d-sjp8b   1/1     Running   0          37m
machine-config-daemon-5cf4b                  2/2     Running   0          43h
machine-config-daemon-7wzcd                  2/2     Running   0          43h
machine-config-daemon-fc946                  2/2     Running   0          43h
machine-config-daemon-g2v28                  2/2     Running   0          43h
machine-config-daemon-gcl4f                  2/2     Running   0          43h
machine-config-daemon-l5tnv                  2/2     Running   0          43h
machine-config-operator-79d9c55d5-hth92      1/1     Running   0          37m
machine-config-server-bsc8h                  1/1     Running   0          43h
machine-config-server-hklrm                  1/1     Running   0          43h
machine-config-server-k9rtx                  1/1     Running   0          43h</programlisting>
</para>
</formalpara>
<simpara>The names for the config daemon pods are in the following format: <literal>machine-config-daemon-&lt;seq&gt;</literal>. The <literal>&lt;seq&gt;</literal> value is a random five character alphanumeric sequence.</simpara>
</listitem>
<listitem>
<simpara>Display the pod log for the first machine config daemon pod shown in the previous output by enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs &lt;pod&gt; -n openshift-machine-config-operator</programlisting>
<simpara>where <literal>pod</literal> is the name of a machine config daemon pod.</simpara>
</listitem>
<listitem>
<simpara>Resolve any errors in the logs shown by the output from the previous command.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To start the migration, configure the OVN-Kubernetes network plugin by using one of the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>To specify the network provider without changing the cluster network IP address block, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{ "spec": { "networkType": "OVNKubernetes" } }'</programlisting>
</listitem>
<listitem>
<simpara>To specify a different cluster network IP address block, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.config.openshift.io cluster \
  --type='merge' --patch '{
    "spec": {
      "clusterNetwork": [
        {
          "cidr": "&lt;cidr&gt;",
          "hostPrefix": &lt;prefix&gt;
        }
      ],
      "networkType": "OVNKubernetes"
    }
  }'</programlisting>
<simpara>where <literal>cidr</literal> is a CIDR block and <literal>prefix</literal> is the slice of the CIDR block apportioned to each node in your cluster. You cannot use any CIDR block that overlaps with the <literal>100.64.0.0/16</literal> CIDR block because the OVN-Kubernetes network provider uses this block internally.</simpara>
<important>
<simpara>You cannot change the service network address block during the migration.</simpara>
</important>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Verify that the Multus daemon set rollout is complete before continuing with subsequent steps:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-multus rollout status daemonset/multus</programlisting>
<simpara>The name of the Multus pods is in the form of <literal>multus-&lt;xxxxx&gt;</literal> where <literal>&lt;xxxxx&gt;</literal> is a random sequence of letters. It might take several moments for the pods to restart.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Waiting for daemon set "multus" rollout to finish: 1 out of 6 new pods have been updated...
...
Waiting for daemon set "multus" rollout to finish: 5 of 6 updated pods are available...
daemon set "multus" successfully rolled out</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To complete changing the network plugin, reboot each node in your cluster. You can reboot the nodes in your cluster with either of the following approaches:</simpara>
<itemizedlist>
<listitem>
<simpara>With the <literal>oc rsh</literal> command, you can use a bash script similar to the following:</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
readarray -t POD_NODES &lt;&lt;&lt; "$(oc get pod -n openshift-machine-config-operator -o wide| grep daemon|awk '{print $1" "$7}')"

for i in "${POD_NODES[@]}"
do
  read -r POD NODE &lt;&lt;&lt; "$i"
  until oc rsh -n openshift-machine-config-operator "$POD" chroot /rootfs shutdown -r +1
    do
      echo "cannot reboot node $NODE, retry" &amp;&amp; sleep 3
    done
done</programlisting>
</listitem>
<listitem>
<simpara>With the <literal>ssh</literal> command, you can use a bash script similar to the following. The script assumes that you have configured sudo to not prompt for a password.</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash

for ip in $(oc get nodes  -o jsonpath='{.items[*].status.addresses[?(@.type=="InternalIP")].address}')
do
   echo "reboot node $ip"
   ssh -o StrictHostKeyChecking=no core@$ip sudo shutdown -r -t 3
done</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Confirm that the migration succeeded:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To confirm that the network plugin is OVN-Kubernetes, enter the following command.  The value of <literal>status.networkType</literal> must be <literal>OVNKubernetes</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network.config/cluster -o jsonpath='{.status.networkType}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>To confirm that the cluster nodes are in the <literal>Ready</literal> state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>To confirm that your pods are not in an error state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces -o wide --sort-by='{.spec.nodeName}'</programlisting>
<simpara>If pods on a node are in an error state, reboot that node.</simpara>
</listitem>
<listitem>
<simpara>To confirm that all of the cluster Operators are not in an abnormal state, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get co</programlisting>
<simpara>The status of every cluster Operator must be the following: <literal>AVAILABLE="True"</literal>, <literal>PROGRESSING="False"</literal>, <literal>DEGRADED="False"</literal>. If a cluster Operator is not available or degraded, check the logs for the cluster Operator for more information.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Complete the following steps only if the migration succeeds and your cluster is in a good state:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To remove the migration configuration from the CNO configuration object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "migration": null } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove custom configuration for the OpenShift SDN network provider, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type='merge' \
  --patch '{ "spec": { "defaultNetwork": { "openshiftSDNConfig": null } } }'</programlisting>
</listitem>
<listitem>
<simpara>To remove the OpenShift SDN network provider namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-sdn</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="assigning-egress-ips">
<title>Configuring egress IPs for a project</title>

<simpara role="_abstract">As a cluster administrator, you can configure the OpenShift SDN Container Network Interface (CNI) network plugin to assign one or more egress IP addresses to a project.</simpara>
<section xml:id="nw-egress-ips-about_egress-ips">
<title>Egress IP address architectural design and implementation</title>
<simpara>The OpenShift Container Platform egress IP address functionality allows you to ensure that the traffic from one or more pods in one or more namespaces has a consistent source IP address for services outside the cluster network.</simpara>
<simpara>For example, you might have a pod that periodically queries a database that is hosted on a server outside of your cluster. To enforce access requirements for the server, a packet filtering device is configured to allow traffic only from specific IP addresses.
To ensure that you can reliably allow access to the server from only that specific pod, you can configure a specific egress IP address for the pod that makes the requests to the server.</simpara>
<simpara>An egress IP address assigned to a namespace is different from an egress router, which is used to send traffic to specific destinations.</simpara>
<simpara>In some cluster configurations,
application pods and ingress router pods run on the same node. If you configure an egress IP address for an application project in this scenario, the IP address is not used when you send a request to a route from the application project.</simpara>
<simpara>An egress IP address is implemented as an additional IP address on the primary network interface of a node and must be in the same subnet as the primary IP address of the node. The additional IP address must not be assigned to any other node in the cluster.</simpara>
<important>
<simpara>Egress IP addresses must not be configured in any Linux network configuration files, such as <literal>ifcfg-eth0</literal>.</simpara>
</important>
<section xml:id="nw-egress-ips-platform-support_egress-ips">
<title>Platform support</title>
<simpara>Support for the egress IP address functionality on various platforms is summarized in the following table:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Platform</entry>
<entry align="left" valign="top">Supported</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Bare metal</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VMware vSphere</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Red Hat OpenStack Platform (RHOSP)</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Amazon Web Services (AWS)</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Google Cloud Platform (GCP)</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Microsoft Azure</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Z&#174; and IBM&#174; LinuxONE</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Z&#174; and IBM&#174; LinuxONE for Red Hat Enterprise Linux (RHEL) KVM</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Power&#174;</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Nutanix</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<important>
<simpara>The assignment of egress IP addresses to control plane nodes with the EgressIP feature is
not supported on a cluster provisioned on Amazon Web Services (AWS). (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=2039656"><emphasis role="strong">BZ#2039656</emphasis></link>).</simpara>
</important>
</section>
<section xml:id="nw-egress-ips-public-cloud-platform-considerations_egress-ips">
<title>Public cloud platform considerations</title>
<simpara>For clusters provisioned on public cloud infrastructure, there is a constraint on the absolute number of assignable IP addresses per node. The maximum number of assignable IP addresses per node, or the <emphasis>IP capacity</emphasis>, can be described in the following formula:</simpara>
<programlisting language="text" linenumbering="unnumbered">IP capacity = public cloud default capacity - sum(current IP assignments)</programlisting>
<simpara>While the Egress IPs capability manages the IP address capacity per node, it is important to plan for this constraint in your deployments. For example, for a cluster installed on bare-metal infrastructure with 8 nodes you can configure 150 egress IP addresses. However, if a public cloud provider limits IP address capacity to 10 IP addresses per node, the total number of assignable IP addresses is only 80. To achieve the same IP address capacity in this example cloud provider, you would need to allocate 7 additional nodes.</simpara>
<simpara>To confirm the IP capacity and subnets for any node in your public cloud environment, you can enter the <literal>oc get node &lt;node_name&gt; -o yaml</literal> command. The <literal>cloud.network.openshift.io/egress-ipconfig</literal> annotation includes capacity and subnet information for the node.</simpara>
<simpara>The annotation value is an array with a single object with fields that provide the following information for the primary network interface:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>interface</literal>: Specifies the interface ID on AWS and Azure and the interface name on GCP.</simpara>
</listitem>
<listitem>
<simpara><literal>ifaddr</literal>: Specifies the subnet mask for one or both IP address families.</simpara>
</listitem>
<listitem>
<simpara><literal>capacity</literal>: Specifies the IP address capacity for the node. On AWS, the IP address capacity is provided per IP address family. On Azure and GCP, the IP address capacity includes both IPv4 and IPv6 addresses.</simpara>
</listitem>
</itemizedlist>
<simpara>Automatic attachment and detachment of egress IP addresses for traffic between nodes are available. This allows for traffic from many pods in namespaces to have a consistent source IP address to locations outside of the cluster. This also supports OpenShift SDN and OVN-Kubernetes, which is the default networking plugin in Red Hat OpenShift Networking in OpenShift Container Platform 4.14.</simpara>
<note>
<simpara>The RHOSP egress IP address feature creates a Neutron reservation port called <literal>egressip-&lt;IP address&gt;</literal>. Using the same RHOSP user as the one used for the OpenShift Container Platform cluster installation, you can assign a floating IP address to this reservation port to have a predictable SNAT address for egress traffic. When an egress IP address on an RHOSP network is moved from one node to another, because of a node failover, for example, the Neutron reservation port is removed and recreated. This means that the floating IP association is lost and you need to manually reassign the floating IP address to the new reservation port.</simpara>
</note>
<note>
<simpara>When an RHOSP cluster administrator assigns a floating IP to the reservation port, OpenShift Container Platform cannot delete the reservation port. The <literal>CloudPrivateIPConfig</literal> object cannot perform delete and move operations until an RHOSP cluster administrator unassigns the floating IP from the reservation port.</simpara>
</note>
<simpara>The following examples illustrate the annotation from nodes on several public cloud providers. The annotations are indented for readability.</simpara>
<formalpara>
<title>Example <literal>cloud.network.openshift.io/egress-ipconfig</literal> annotation on AWS</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"eni-078d267045138e436",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ipv4":14,"ipv6":15}
  }
]</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example <literal>cloud.network.openshift.io/egress-ipconfig</literal> annotation on GCP</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">cloud.network.openshift.io/egress-ipconfig: [
  {
    "interface":"nic0",
    "ifaddr":{"ipv4":"10.0.128.0/18"},
    "capacity":{"ip":14}
  }
]</programlisting>
</para>
</formalpara>
<simpara>The following sections describe the IP address capacity for supported public cloud environments for use in your capacity calculation.</simpara>
<section xml:id="nw-egress-ips-capacity-aws_egress-ips">
<title>Amazon Web Services (AWS) IP address capacity limits</title>
<simpara>On AWS, constraints on IP address assignments depend on the instance type configured. For more information, see <link xlink:href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#AvailableIpPerENI">IP addresses per network interface per instance type</link></simpara>
</section>
<section xml:id="nw-egress-ips-capacity-gcp_egress-ips">
<title>Google Cloud Platform (GCP) IP address capacity limits</title>
<simpara>On GCP, the networking model implements additional node IP addresses through IP address aliasing, rather than IP address assignments. However, IP address capacity maps directly to IP aliasing capacity.</simpara>
<simpara>The following capacity limits exist for IP aliasing assignment:</simpara>
<itemizedlist>
<listitem>
<simpara>Per node, the maximum number of IP aliases, both IPv4 and IPv6, is 100.</simpara>
</listitem>
<listitem>
<simpara>Per VPC, the maximum number of IP aliases is unspecified, but OpenShift Container Platform scalability testing reveals the maximum to be approximately 15,000.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information, see <link xlink:href="https://cloud.google.com/vpc/docs/quota#per_instance">Per instance</link> quotas and <link xlink:href="https://cloud.google.com/vpc/docs/alias-ip">Alias IP ranges overview</link>.</simpara>
</section>
<section xml:id="nw-egress-ips-capacity-azure_egress-ips">
<title>Microsoft Azure IP address capacity limits</title>
<simpara>On Azure, the following capacity limits exist for IP address assignment:</simpara>
<itemizedlist>
<listitem>
<simpara>Per NIC, the maximum number of assignable IP addresses, for both IPv4 and IPv6, is 256.</simpara>
</listitem>
<listitem>
<simpara>Per virtual network, the maximum number of assigned IP addresses cannot exceed 65,536.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information, see <link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/azure-subscription-service-limits?toc=/azure/virtual-network/toc.json#networking-limits">Networking limits</link>.</simpara>
</section>
</section>
<section xml:id="nw-egress-ips-multi-nic-considerations_egress-ips">
<title>Considerations for using an egress IP on additional network interfaces</title>
<simpara>In OpenShift Container Platform, egress IPs provide administrators a way to control network traffic. Egress IPs can be used with the <literal>br-ex</literal>, or primary, network interface, which is a Linux bridge interface associated with Open vSwitch, or they can be used with additional network interfaces.</simpara>
<simpara>You can inspect your network interface type by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip -details link show</programlisting>
<simpara>The primary network interface is assigned a node IP address which also contains a subnet mask. Information for this node IP address can be retrieved from the Kubernetes node object for each node within your cluster by inspecting the <literal>k8s.ovn.org/node-primary-ifaddr</literal> annotation. In an IPv4 cluster, this annotation is similar to the following example: <literal>"k8s.ovn.org/node-primary-ifaddr: {"ipv4":"192.168.111.23/24"}"</literal>.</simpara>
<simpara>If the egress IP is not within the subnet of the primary network interface subnet, you can use an egress IP on another Linux network interface that is not of the primary network interface type. By doing so, OpenShift Container Platform administrators are provided with a greater level of control over networking aspects such as routing, addressing, segmentation, and security policies. This feature provides users with the option to route workload traffic over specific network interfaces for purposes such as traffic segmentation or meeting specialized requirements.</simpara>
<simpara>If the egress IP is not within the subnet of the primary network interface, then the selection of another network interface for egress traffic might occur if they are present on a node.</simpara>
<simpara>You can determine which other network interfaces might support egress IPs by inspecting the <literal>k8s.ovn.org/host-cidrs</literal> Kubernetes node annotation. This annotation contains the addresses and subnet mask found for the primary network interface. It also contains additional network interface addresses and subnet mask information. These addresses and subnet masks are assigned to network interfaces that use the <link xlink:href="https://networklessons.com/cisco/ccna-200-301/longest-prefix-match-routing">longest prefix match routing</link> mechanism to determine which network interface supports the egress IP.</simpara>
<note>
<simpara>OVN-Kubernetes provides a mechanism to control and direct outbound network traffic from specific namespaces and pods. This ensures that it exits the cluster through a particular network interface and with a specific egress IP address.</simpara>
</note>
<bridgehead xml:id="nw-egress-ips-multi-nic-requirements_egress-ips" renderas="sect5">Requirements for assigning an egress IP to a network interface that is not the primary network interface</bridgehead>
<simpara>For users who want an egress IP and traffic to be routed over a particular interface that is not the primary network interface, the following conditions must be met:</simpara>
<itemizedlist>
<listitem>
<simpara>OpenShift Container Platform is installed on a bare metal cluster. This feature is disabled within cloud or hypervisor environments.</simpara>
</listitem>
<listitem>
<simpara>Your OpenShift Container Platform pods are not configured as host-networked.</simpara>
</listitem>
<listitem>
<simpara>If a network interface is removed or if the IP address and subnet mask which allows the egress IP to be hosted on the interface is removed, then the egress IP is reconfigured. Consequently, it could be assigned to another node and interface.</simpara>
</listitem>
<listitem>
<simpara>The Egress IP must be IPv4. IPv6 is currently unsupported.</simpara>
</listitem>
<listitem>
<simpara>IP forwarding must be enabled for the network interface. To enable IP forwarding, you can use the <literal>oc edit network.operator</literal> command and edit the object like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  clusterNetwork:
  - cidr: 10.128.0.0/14
    hostPrefix: 23
  defaultNetwork:
    ovnKubernetesConfig:
      gatewayConfig:
        ipForwarding: Global
# ...</programlisting>
<simpara xml:id="nw-egress-ips-limitations_egress-ips">== Limitations</simpara>
</listitem>
</itemizedlist>
<simpara>The following limitations apply when using egress IP addresses with the OpenShift SDN network plugin:</simpara>
<itemizedlist>
<listitem>
<simpara>You cannot use manually assigned and automatically assigned egress IP addresses on the same nodes.</simpara>
</listitem>
<listitem>
<simpara>If you manually assign egress IP addresses from an IP address range, you must not make that range available for automatic IP assignment.</simpara>
</listitem>
<listitem>
<simpara>You cannot share egress IP addresses across multiple namespaces using the OpenShift SDN egress IP address implementation.</simpara>
</listitem>
</itemizedlist>
<simpara>If you need to share IP addresses across namespaces, the OVN-Kubernetes network plugin egress IP address implementation allows you to span IP addresses across multiple namespaces.</simpara>
<note>
<simpara>If you use OpenShift SDN in multitenant mode, you cannot use egress IP addresses with any namespace that is joined to another namespace by the projects that are associated with them.
For example, if <literal>project1</literal> and <literal>project2</literal> are joined by running the <literal>oc adm pod-network join-projects --to=project1 project2</literal> command, neither project can use an egress IP address. For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1645577">BZ#1645577</link>.</simpara>
</note>
</section>
<section xml:id="automatic-manual-assignment-approaches">
<title>IP address assignment approaches</title>
<simpara>You can assign egress IP addresses to namespaces by setting the <literal>egressIPs</literal> parameter of the <literal>NetNamespace</literal> object. After an egress IP address is associated with a project, OpenShift SDN allows you to assign egress IP addresses to hosts in two ways:</simpara>
<itemizedlist>
<listitem>
<simpara>In the <emphasis>automatically assigned</emphasis> approach, an egress IP address range is assigned to a node.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis>manually assigned</emphasis> approach, a list of one or more egress IP address is assigned to a node.</simpara>
</listitem>
</itemizedlist>
<simpara>Namespaces that request an egress IP address are matched with nodes that can host those egress IP addresses, and then the egress IP addresses are assigned to those nodes.
If the <literal>egressIPs</literal> parameter is set on a <literal>NetNamespace</literal> object, but no node hosts that egress IP address, then egress traffic from the namespace will be dropped.</simpara>
<simpara>High availability of nodes is automatic.
If a node that hosts an egress IP address is unreachable and there are nodes that are able to host that egress IP address, then the egress IP address will move to a new node.
When the unreachable node comes back online, the egress IP address automatically moves to balance egress IP addresses across nodes.</simpara>
<section xml:id="considerations-automatic-egress-ips">
<title>Considerations when using automatically assigned egress IP addresses</title>
<simpara>When using the automatic assignment approach for egress IP addresses the following considerations apply:</simpara>
<itemizedlist>
<listitem>
<simpara>You set the <literal>egressCIDRs</literal> parameter of each node&#8217;s <literal>HostSubnet</literal> resource to indicate the range of egress IP addresses that can be hosted by a node.
OpenShift Container Platform sets the <literal>egressIPs</literal> parameter of the <literal>HostSubnet</literal> resource based on the IP address range you specify.</simpara>
</listitem>
</itemizedlist>
<simpara>If the node hosting the namespace&#8217;s egress IP address is unreachable, OpenShift Container Platform will reassign the egress IP address to another node with a compatible egress IP address range.
The automatic assignment approach works best for clusters installed in environments with flexibility in associating additional IP addresses with nodes.</simpara>
</section>
<section xml:id="considerations-manual-egress-ips">
<title>Considerations when using manually assigned egress IP addresses</title>
<simpara>This approach allows you to control which nodes can host an egress IP address.</simpara>
<note>
<simpara>If your cluster is installed on public cloud infrastructure, you must ensure that each node that you assign egress IP addresses to has sufficient spare capacity to host the IP addresses. For more information, see "Platform considerations" in a previous section.</simpara>
</note>
<simpara>When using the manual assignment approach for egress IP addresses the following considerations apply:</simpara>
<itemizedlist>
<listitem>
<simpara>You set the <literal>egressIPs</literal> parameter of each node&#8217;s <literal>HostSubnet</literal> resource to indicate the IP addresses that can be hosted by a node.</simpara>
</listitem>
<listitem>
<simpara>Multiple egress IP addresses per namespace are supported.</simpara>
</listitem>
</itemizedlist>
<simpara>If a namespace has multiple egress IP addresses and those addresses are hosted on multiple nodes, the following additional considerations apply:</simpara>
<itemizedlist>
<listitem>
<simpara>If a pod is on a node that is hosting an egress IP address, that pod always uses the egress IP address on the node.</simpara>
</listitem>
<listitem>
<simpara>If a pod is not on a node that is hosting an egress IP address, that pod uses an egress IP address at random.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="nw-egress-ips-automatic_egress-ips">
<title>Configuring automatically assigned egress IP addresses for a namespace</title>
<simpara>In OpenShift Container Platform you can enable automatic assignment of an egress IP address
for a specific namespace across one or more nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Update the <literal>NetNamespace</literal> object with the egress IP address using the
following JSON:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ oc patch netnamespace &lt;project_name&gt; --type=merge -p \
  '{
    "egressIPs": [
      "&lt;ip_address&gt;"
    ]
  }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;project_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the project.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;ip_address&gt;</literal></term>
<listitem>
<simpara>Specifies one or more egress IP addresses for the <literal>egressIPs</literal> array.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example, to assign <literal>project1</literal> to an IP address of 192.168.1.100 and
<literal>project2</literal> to an IP address of 192.168.1.101:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch netnamespace project1 --type=merge -p \
  '{"egressIPs": ["192.168.1.100"]}'
$ oc patch netnamespace project2 --type=merge -p \
  '{"egressIPs": ["192.168.1.101"]}'</programlisting>
<note>
<simpara>Because OpenShift SDN manages the <literal>NetNamespace</literal> object, you can make changes only by modifying the existing <literal>NetNamespace</literal> object. Do not create a new <literal>NetNamespace</literal> object.</simpara>
</note>
</listitem>
<listitem>
<simpara>Indicate which nodes can host egress IP addresses by setting the <literal>egressCIDRs</literal>
parameter for each host using the following JSON:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hostsubnet &lt;node_name&gt; --type=merge -p \
  '{
    "egressCIDRs": [
      "&lt;ip_address_range&gt;", "&lt;ip_address_range&gt;"
    ]
  }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;node_name&gt;</literal></term>
<listitem>
<simpara>Specifies a node name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;ip_address_range&gt;</literal></term>
<listitem>
<simpara>Specifies an IP address range in CIDR format. You can specify more than one address range for the <literal>egressCIDRs</literal> array.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example, to set <literal>node1</literal> and <literal>node2</literal> to host egress IP addresses
in the range 192.168.1.0 to 192.168.1.255:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hostsubnet node1 --type=merge -p \
  '{"egressCIDRs": ["192.168.1.0/24"]}'
$ oc patch hostsubnet node2 --type=merge -p \
  '{"egressCIDRs": ["192.168.1.0/24"]}'</programlisting>
<simpara>OpenShift Container Platform automatically assigns specific egress IP addresses to
available nodes in a balanced way. In this case, it assigns the egress IP
address 192.168.1.100 to <literal>node1</literal> and the egress IP address 192.168.1.101 to
<literal>node2</literal> or vice versa.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-egress-ips-static_egress-ips">
<title>Configuring manually assigned egress IP addresses for a namespace</title>
<simpara>In OpenShift Container Platform you can associate one or more egress IP addresses with a namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Update the <literal>NetNamespace</literal> object by specifying the following JSON
object with the desired IP addresses:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ oc patch netnamespace &lt;project_name&gt; --type=merge -p \
  '{
    "egressIPs": [
      "&lt;ip_address&gt;"
    ]
  }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;project_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the project.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;ip_address&gt;</literal></term>
<listitem>
<simpara>Specifies one or more egress IP addresses for the <literal>egressIPs</literal> array.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example, to assign the <literal>project1</literal> project to the IP addresses <literal>192.168.1.100</literal> and <literal>192.168.1.101</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch netnamespace project1 --type=merge \
  -p '{"egressIPs": ["192.168.1.100","192.168.1.101"]}'</programlisting>
<simpara>To provide high availability, set the <literal>egressIPs</literal> value to two or more IP addresses on different nodes. If multiple egress IP addresses are set, then pods use all egress IP addresses roughly equally.</simpara>
<note>
<simpara>Because OpenShift SDN manages the <literal>NetNamespace</literal> object, you can make changes only by modifying the existing <literal>NetNamespace</literal> object. Do not create a new <literal>NetNamespace</literal> object.</simpara>
</note>
</listitem>
<listitem>
<simpara>Manually assign the egress IP address to the node hosts.</simpara>
<simpara>If your cluster is installed on public cloud infrastructure, you must confirm that the node has available IP address capacity.</simpara>
<simpara>Set the <literal>egressIPs</literal> parameter on the <literal>HostSubnet</literal> object on the node host. Using the following JSON, include as many IP addresses as you want to assign to that node host:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hostsubnet &lt;node_name&gt; --type=merge -p \
  '{
    "egressIPs": [
      "&lt;ip_address&gt;",
      "&lt;ip_address&gt;"
      ]
  }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;node_name&gt;</literal></term>
<listitem>
<simpara>Specifies a node name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;ip_address&gt;</literal></term>
<listitem>
<simpara>Specifies an IP address. You can specify more than one IP address for the <literal>egressIPs</literal> array.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example, to specify that <literal>node1</literal> should have the egress IPs <literal>192.168.1.100</literal>,
<literal>192.168.1.101</literal>, and <literal>192.168.1.102</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hostsubnet node1 --type=merge -p \
  '{"egressIPs": ["192.168.1.100", "192.168.1.101", "192.168.1.102"]}'</programlisting>
<simpara>In the previous example, all egress traffic for <literal>project1</literal> will be routed to the node hosting the specified egress IP, and then connected through Network Address Translation (NAT) to that IP address.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="egress-ips-additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>If you are configuring manual egress IP address assignment, see <link linkend="nw-egress-ips-public-cloud-platform-considerations_egress-ips">Platform considerations</link> for information about IP capacity planning.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-egress-firewall">
<title>Configuring an egress firewall for a project</title>

<simpara>As a cluster administrator, you can create an egress firewall for a project that restricts egress traffic leaving your OpenShift Container Platform cluster.</simpara>
<section xml:id="nw-egressnetworkpolicy-about_openshift-sdn-egress-firewall">
<title>How an egress firewall works in a project</title>
<simpara>As a cluster administrator, you can use an <emphasis>egress firewall</emphasis> to
limit the external hosts that some or all pods can access from within the
cluster. An egress firewall supports the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>A pod can only connect to internal hosts and cannot initiate connections to
the public internet.</simpara>
</listitem>
<listitem>
<simpara>A pod can only connect to the public internet and cannot initiate connections
to internal hosts that are outside the OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>A pod cannot reach specified internal subnets or hosts outside the OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>A pod can connect to only specific external hosts.</simpara>
</listitem>
</itemizedlist>
<simpara>For example, you can allow one project access to a specified IP range but deny the same access to a different project. Or you can restrict application developers from updating from Python pip mirrors, and force updates to come only from approved sources.</simpara>
<note>
<simpara>Egress firewall does not apply to the host network namespace. Pods with host networking enabled are unaffected by egress firewall rules.</simpara>
</note>
<simpara>You configure an egress firewall policy by creating an EgressNetworkPolicy custom resource (CR) object. The egress firewall matches network traffic that meets any of the following criteria:</simpara>
<itemizedlist>
<listitem>
<simpara>An IP address range in CIDR format</simpara>
</listitem>
<listitem>
<simpara>A DNS name that resolves to an IP address</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>If your egress firewall includes a deny rule for <literal>0.0.0.0/0</literal>, access to your OpenShift Container Platform API servers is blocked. You must either add allow rules for each IP address or use the <literal>nodeSelector</literal> type allow rule in your egress policy rules to connect to API servers.</simpara>
<simpara>The following example illustrates the order of the egress firewall rules necessary to ensure API server access:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: default
  namespace: &lt;namespace&gt; <co xml:id="CO194-1"/>
spec:
  egress:
  - to:
      cidrSelector: &lt;api_server_address_range&gt; <co xml:id="CO194-2"/>
    type: Allow
# ...
  - to:
      cidrSelector: 0.0.0.0/0 <co xml:id="CO194-3"/>
    type: Deny</programlisting>
<calloutlist>
<callout arearefs="CO194-1">
<para>The namespace for the egress firewall.</para>
</callout>
<callout arearefs="CO194-2">
<para>The IP address range that includes your OpenShift Container Platform API servers.</para>
</callout>
<callout arearefs="CO194-3">
<para>A global deny rule prevents access to the OpenShift Container Platform API servers.</para>
</callout>
</calloutlist>
<simpara>To find the IP address for your API servers, run <literal>oc get ep kubernetes -n default</literal>.</simpara>
<simpara>For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1988324">BZ#1988324</link>.</simpara>
</important>
<important>
<simpara>You must have OpenShift SDN configured to use either the network policy or multitenant mode to configure an egress firewall.</simpara>
<simpara>If you use network policy mode, an egress firewall is compatible with only one policy per namespace and will not work with projects that share a network, such as global projects.</simpara>
</important>
<warning>
<simpara>Egress firewall rules do not apply to traffic that goes through routers. Any user with permission to create a Route CR object can bypass egress firewall policy rules by creating a route that points to a forbidden destination.</simpara>
</warning>
<section xml:id="limitations-of-an-egress-firewall_openshift-sdn-egress-firewall">
<title>Limitations of an egress firewall</title>
<simpara>An egress firewall has the following limitations:</simpara>
<itemizedlist>
<listitem>
<simpara>No project can have more than one EgressNetworkPolicy object.</simpara>
<important>
<simpara>The creation of more than one EgressNetworkPolicy object is allowed, however it should not be done. When you create more than one EgressNetworkPolicy object, the following message is returned: <literal>dropping all rules</literal>. In actuality, all external traffic is dropped, which can cause security risks for your organization.</simpara>
</important>
</listitem>
<listitem>
<simpara>A maximum of one EgressNetworkPolicy object with a maximum of 1,000 rules can be defined per project.</simpara>
</listitem>
<listitem>
<simpara>The <literal>default</literal> project cannot use an egress firewall.</simpara>
</listitem>
<listitem>
<simpara>When using the OpenShift SDN network plugin in multitenant mode, the following limitations apply:</simpara>
<itemizedlist>
<listitem>
<simpara>Global projects cannot use an egress firewall. You can make a project global by using the <literal>oc adm pod-network make-projects-global</literal> command.</simpara>
</listitem>
<listitem>
<simpara>Projects merged by using the <literal>oc adm pod-network join-projects</literal> command cannot use an egress firewall in any of the joined projects.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>Violating any of these restrictions results in a broken egress firewall for the project. Consequently, all external network traffic is dropped, which can cause security risks for your organization.</simpara>
<simpara>An Egress Firewall resource can be created in the <literal>kube-node-lease</literal>, <literal>kube-public</literal>, <literal>kube-system</literal>, <literal>openshift</literal> and <literal>openshift-</literal> projects.</simpara>
</section>
<section xml:id="policy-rule-order_openshift-sdn-egress-firewall">
<title>Matching order for egress firewall policy rules</title>
<simpara>The egress firewall policy rules are evaluated in the order that they are defined, from first to last. The first rule that matches an egress connection from a pod applies. Any subsequent rules are ignored for that connection.</simpara>
</section>
<section xml:id="domain-name-server-resolution_openshift-sdn-egress-firewall">
<title>How Domain Name Server (DNS) resolution works</title>
<simpara>If you use DNS names in any of your egress firewall policy rules, proper resolution of the domain names is subject to the following restrictions:</simpara>
<itemizedlist>
<listitem>
<simpara>Domain name updates are polled based on a time-to-live (TTL) duration. By default, the duration is 30 seconds. When the egress firewall controller queries the local name servers for a domain name, if the response includes a TTL that is less than 30 seconds, the controller sets the duration to the returned value. If the TTL in the response is greater than 30 minutes, the controller sets the duration to 30 minutes. If the TTL is between 30 seconds and 30 minutes, the controller ignores the value and sets the duration to 30 seconds.</simpara>
</listitem>
<listitem>
<simpara>The pod must resolve the domain from the same local name servers when necessary. Otherwise the IP addresses for the domain known by the egress firewall controller and the pod can be different. If the IP addresses for a hostname differ, the egress firewall might not be enforced consistently.</simpara>
</listitem>
<listitem>
<simpara>Because the egress firewall controller and pods asynchronously poll the same local name server, the pod might obtain the updated IP address before the egress controller does, which causes a race condition. Due to this current limitation, domain name usage in EgressNetworkPolicy objects is only recommended for domains with infrequent IP address changes.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Using DNS names in your egress firewall policy does not affect local DNS resolution through CoreDNS.</simpara>
<simpara>However, if your egress firewall policy uses domain names, and an external DNS server handles DNS resolution for an affected pod, you must include egress firewall rules that permit access to the IP addresses of your DNS server.</simpara>
</note>
</section>
</section>
<section xml:id="nw-egressnetworkpolicy-object_openshift-sdn-egress-firewall">
<title>EgressNetworkPolicy custom resource (CR) object</title>
<simpara>You can define one or more rules for an egress firewall. A rule is either an <literal>Allow</literal> rule or a <literal>Deny</literal> rule, with a specification for the traffic that the rule applies to.</simpara>
<simpara>The following YAML describes an EgressNetworkPolicy CR object:</simpara>
<formalpara>
<title>EgressNetworkPolicy object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: &lt;name&gt; <co xml:id="CO195-1"/>
spec:
  egress: <co xml:id="CO195-2"/>
    ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO195-1">
<para>A name for your egress firewall policy.</para>
</callout>
<callout arearefs="CO195-2">
<para>A collection of one or more egress network policy rules as described in the following section.</para>
</callout>
</calloutlist>
<section xml:id="egressnetworkpolicy-rules_openshift-sdn-egress-firewall">
<title>EgressNetworkPolicy rules</title>
<simpara>The following YAML describes an egress firewall rule object. The user can select either an IP address range in CIDR format, a domain name, or use the <literal>nodeSelector</literal> to allow or deny egress traffic. The <literal>egress</literal> stanza expects an array of one or more objects.</simpara>
<formalpara>
<title>Egress policy rule stanza</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">egress:
- type: &lt;type&gt; <co xml:id="CO196-1"/>
  to: <co xml:id="CO196-2"/>
    cidrSelector: &lt;cidr&gt; <co xml:id="CO196-3"/>
    dnsName: &lt;dns_name&gt; <co xml:id="CO196-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO196-1">
<para>The type of rule. The value must be either <literal>Allow</literal> or <literal>Deny</literal>.</para>
</callout>
<callout arearefs="CO196-2">
<para>A stanza describing an egress traffic match rule. A value for either the <literal>cidrSelector</literal> field or the <literal>dnsName</literal> field for the rule. You cannot use both fields in the same rule.</para>
</callout>
<callout arearefs="CO196-3">
<para>An IP address range in CIDR format.</para>
</callout>
<callout arearefs="CO196-4">
<para>A domain name.</para>
</callout>
</calloutlist>
</section>
<section xml:id="egressnetworkpolicy-example_openshift-sdn-egress-firewall">
<title>Example EgressNetworkPolicy CR objects</title>
<simpara>The following example defines several egress firewall policy rules:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: default
spec:
  egress: <co xml:id="CO197-1"/>
  - type: Allow
    to:
      cidrSelector: 1.2.3.0/24
  - type: Allow
    to:
      dnsName: www.example.com
  - type: Deny
    to:
      cidrSelector: 0.0.0.0/0</programlisting>
<calloutlist>
<callout arearefs="CO197-1">
<para>A collection of egress firewall policy rule objects.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="nw-networkpolicy-create_openshift-sdn-egress-firewall">
<title>Creating an egress firewall policy object</title>
<simpara>As a cluster administrator, you can create an egress firewall policy object for a project.</simpara>
<important>
<simpara>If the project already has an EgressNetworkPolicy object defined, you must edit the existing policy to make changes to the egress firewall rules.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster that uses the OpenShift SDN network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy rule:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>&lt;policy_name&gt;.yaml</literal> file where <literal>&lt;policy_name&gt;</literal> describes the egress
policy rules.</simpara>
</listitem>
<listitem>
<simpara>In the file you created, define an egress policy object.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Enter the following command to create the policy object. Replace <literal>&lt;policy_name&gt;</literal> with the name of the policy and <literal>&lt;project&gt;</literal> with the project that the rule applies to.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;policy_name&gt;.yaml -n &lt;project&gt;</programlisting>
<simpara>In the following example, a new EgressNetworkPolicy object is created in a project named <literal>project1</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f default.yaml -n project1</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">egressnetworkpolicy.network.openshift.io/v1 created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: Save the <literal>&lt;policy_name&gt;.yaml</literal> file so that you can make changes later.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="openshift-sdn-viewing-egress-firewall">
<title>Editing an egress firewall for a project</title>

<simpara>As a cluster administrator, you can modify network traffic rules for an existing egress firewall.</simpara>
<section xml:id="nw-egressnetworkpolicy-view_openshift-sdn-viewing-egress-firewall">
<title>Viewing an EgressNetworkPolicy object</title>
<simpara>You can view an EgressNetworkPolicy object in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster using the OpenShift SDN network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift Command-line Interface (CLI), commonly known as <literal>oc</literal>.</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To view the names of the EgressNetworkPolicy objects defined in your cluster,
enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get egressnetworkpolicy --all-namespaces</programlisting>
</listitem>
<listitem>
<simpara>To inspect a policy, enter the following command. Replace <literal>&lt;policy_name&gt;</literal> with the name of the policy to inspect.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe egressnetworkpolicy &lt;policy_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:		default
Namespace:	project1
Created:	20 minutes ago
Labels:		&lt;none&gt;
Annotations:	&lt;none&gt;
Rule:		Allow to 1.2.3.0/24
Rule:		Allow to www.example.com
Rule:		Deny to 0.0.0.0/0</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="editing-egress-firewall">
<title>Editing an egress firewall for a project</title>

<simpara>As a cluster administrator, you can modify network traffic rules for an existing egress firewall.</simpara>
<section xml:id="nw-egressnetworkpolicy-edit_openshift-sdn-egress-firewall">
<title>Editing an EgressNetworkPolicy object</title>
<simpara>As a cluster administrator, you can update the egress firewall for a project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster using the OpenShift SDN network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Find the name of the EgressNetworkPolicy object for the project. Replace <literal>&lt;project&gt;</literal> with the name of the project.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n &lt;project&gt; egressnetworkpolicy</programlisting>
</listitem>
<listitem>
<simpara>Optional: If you did not save a copy of the EgressNetworkPolicy object when you created the egress network firewall, enter the following command to create a copy.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n &lt;project&gt; egressnetworkpolicy &lt;name&gt; -o yaml &gt; &lt;filename&gt;.yaml</programlisting>
<simpara>Replace <literal>&lt;project&gt;</literal> with the name of the project. Replace <literal>&lt;name&gt;</literal> with the name of the object. Replace <literal>&lt;filename&gt;</literal> with the name of the file to save the YAML to.</simpara>
</listitem>
<listitem>
<simpara>After making changes to the policy rules, enter the following command to replace the EgressNetworkPolicy object. Replace <literal>&lt;filename&gt;</literal> with the name of the file containing the updated EgressNetworkPolicy object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f &lt;filename&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="removing-egress-firewall">
<title>Removing an egress firewall from a project</title>

<simpara>As a cluster administrator, you can remove an egress firewall from a project to remove all restrictions on network traffic from the project that leaves the OpenShift Container Platform cluster.</simpara>
<section xml:id="nw-egressnetworkpolicy-delete_openshift-sdn-egress-firewall">
<title>Removing an EgressNetworkPolicy object</title>
<simpara>As a cluster administrator, you can remove an egress firewall from a project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster using the OpenShift SDN network plugin.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Find the name of the EgressNetworkPolicy object for the project. Replace <literal>&lt;project&gt;</literal> with the name of the project.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n &lt;project&gt; egressnetworkpolicy</programlisting>
</listitem>
<listitem>
<simpara>Enter the following command to delete the EgressNetworkPolicy object. Replace <literal>&lt;project&gt;</literal> with the name of the project and <literal>&lt;name&gt;</literal> with the name of the object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n &lt;project&gt; egressnetworkpolicy &lt;name&gt;</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="using-an-egress-router">
<title>Considerations for the use of an egress router pod</title>

<section xml:id="nw-egress-router-about_using-an-egress-router">
<title>About an egress router pod</title>
<simpara>The OpenShift Container Platform egress router pod redirects traffic to a specified remote server from a private source IP address that is not used for any other purpose. An egress router pod can send network traffic to servers that are set up to allow access only from specific IP addresses.</simpara>
<note>
<simpara>The egress router pod is not intended for every outgoing connection. Creating large numbers of egress router pods can exceed the limits of your network hardware. For example, creating an egress router pod for every project or application could exceed the number of local MAC addresses that the network interface can handle before reverting to filtering MAC addresses in software.</simpara>
</note>
<important>
<simpara>The egress router image is not compatible with Amazon AWS, Azure Cloud, or any other cloud platform that does not support layer 2 manipulations due to their incompatibility with macvlan traffic.</simpara>
</important>
<section xml:id="nw-egress-router-about-modes_using-an-egress-router">
<title>Egress router modes</title>
<simpara>In <emphasis>redirect mode</emphasis>, an egress router pod configures <literal>iptables</literal> rules to redirect traffic from its own IP address to one or more destination IP addresses. Client pods that need to use the reserved source IP address must be configured to access the service for the egress router rather than connecting directly to the destination IP. You can access the destination service and port from the application pod by using the <literal>curl</literal> command. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl &lt;router_service_IP&gt; &lt;port&gt;</programlisting>
<simpara>In <emphasis>HTTP proxy mode</emphasis>, an egress router pod runs as an HTTP proxy on port <literal>8080</literal>. This mode only works for clients that are connecting to HTTP-based or HTTPS-based services, but usually requires fewer changes to the client pods to get them to work. Many programs can be told to use an HTTP proxy by setting an environment variable.</simpara>
<simpara>In <emphasis>DNS proxy mode</emphasis>, an egress router pod runs as a DNS proxy for TCP-based services from its own IP address to one or more destination IP addresses. To make use of the reserved, source IP address, client pods must be modified to connect to the egress router pod rather than connecting directly to the destination IP address. This modification ensures that external destinations treat traffic as though it were coming from a known source.</simpara>
<simpara>Redirect mode works for all services except for HTTP and HTTPS. For HTTP and HTTPS services, use HTTP proxy mode. For TCP-based services with IP addresses or domain names, use DNS proxy mode.</simpara>
</section>
<section xml:id="nw-egress-router-about-router-pod-implementation_using-an-egress-router">
<title>Egress router pod implementation</title>
<simpara>The egress router pod setup is performed by an initialization container. That container runs in a privileged context so that it can configure the macvlan interface and set up <literal>iptables</literal> rules. After the initialization container finishes setting up the <literal>iptables</literal> rules, it exits. Next the egress router pod executes the container to handle the egress router traffic. The image used varies depending on the egress router mode.</simpara>
<simpara>The environment variables determine which addresses the egress-router image uses. The image configures the macvlan interface to use <literal>EGRESS_SOURCE</literal> as its IP address, with <literal>EGRESS_GATEWAY</literal> as the IP address for the gateway.</simpara>
<simpara>Network Address Translation (NAT) rules are set up so that connections to the cluster IP address of the pod on any TCP or UDP port are redirected to the same port on IP address specified by the <literal>EGRESS_DESTINATION</literal> variable.</simpara>
<simpara>If only some of the nodes in your cluster are capable of claiming the specified source IP address and using the specified gateway, you can specify a <literal>nodeName</literal> or <literal>nodeSelector</literal> to identify which nodes are acceptable.</simpara>
</section>
<section xml:id="nw-egress-router-about-deployments_using-an-egress-router">
<title>Deployment considerations</title>
<simpara>An egress router pod adds an additional IP address and MAC address to the primary network interface of the node. As a result, you might need to configure your hypervisor or cloud provider to allow the additional address.</simpara>
<variablelist>
<varlistentry>
<term>Red Hat OpenStack Platform (RHOSP)</term>
<listitem>
<simpara>If you deploy OpenShift Container Platform on RHOSP, you must allow traffic from the IP and MAC addresses of the egress router pod on your OpenStack environment. If you do not allow the traffic, then <link xlink:href="https://access.redhat.com/solutions/2803331">communication will fail</link>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack port set --allowed-address \
  ip_address=&lt;ip_address&gt;,mac_address=&lt;mac_address&gt; &lt;neutron_port_uuid&gt;</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>VMware vSphere</term>
<listitem>
<simpara>If you are using VMware vSphere, see the <link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-3507432E-AFEA-4B6B-B404-17A020575358.html">VMware documentation for securing vSphere standard switches</link>. View and change VMware vSphere default settings by selecting the host virtual switch from the vSphere Web Client.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Specifically, ensure that the following are enabled:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-942BD3AA-731B-4A05-8196-66F2B4BF1ACB.html">MAC Address Changes</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-7DC6486F-5400-44DF-8A62-6273798A2F80.html">Forged Transits</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.vmware.com/en/VMware-vSphere/6.0/com.vmware.vsphere.security.doc/GUID-92F3AB1F-B4C5-4F25-A010-8820D7250350.html">Promiscuous Mode Operation</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-egress-router-about-failover_using-an-egress-router">
<title>Failover configuration</title>
<simpara>To avoid downtime, you can deploy an egress router pod with a <literal>Deployment</literal> resource, as in the following example. To create a new <literal>Service</literal> object for the example deployment, use the <literal>oc expose deployment/egress-demo-controller</literal> command.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: egress-demo-controller
spec:
  replicas: 1 <co xml:id="CO198-1"/>
  selector:
    matchLabels:
      name: egress-router
  template:
    metadata:
      name: egress-router
      labels:
        name: egress-router
      annotations:
        pod.network.openshift.io/assign-macvlan: "true"
    spec: <co xml:id="CO198-2"/>
      initContainers:
        ...
      containers:
        ...</programlisting>
<calloutlist>
<callout arearefs="CO198-1">
<para>Ensure that replicas is set to <literal>1</literal>, because only one pod can use a given egress source IP address at any time. This means that only a single copy of the router runs on a node.</para>
</callout>
<callout arearefs="CO198-2">
<para>Specify the <literal>Pod</literal> object template for the egress router pod.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="using-an-egress-router-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="deploying-egress-router-layer3-redirection">Deploying an egress router in redirection mode</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="deploying-egress-router-http-redirection">Deploying an egress router in HTTP proxy mode</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="deploying-egress-router-dns-redirection">Deploying an egress router in DNS proxy mode</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-egress-router-layer3-redirection">
<title>Deploying an egress router pod in redirect mode</title>

<simpara>As a cluster administrator, you can deploy an egress router pod that is configured to redirect traffic to specified destination IP addresses.</simpara>
<section xml:id="nw-egress-router-pod_deploying-egress-router-layer3-redirection">
<title>Egress router pod specification for redirect mode</title>
<simpara>Define the configuration for an egress router pod in the <literal>Pod</literal> object. The following YAML describes the fields for the configuration of an egress router pod in redirect mode:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: egress-1
  labels:
    name: egress-1
  annotations:
    pod.network.openshift.io/assign-macvlan: "true" <co xml:id="CO199-1"/>
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE <co xml:id="CO199-2"/>
      value: &lt;egress_router&gt;
    - name: EGRESS_GATEWAY <co xml:id="CO199-3"/>
      value: &lt;egress_gateway&gt;
    - name: EGRESS_DESTINATION <co xml:id="CO199-4"/>
      value: &lt;egress_destination&gt;
    - name: EGRESS_ROUTER_MODE
      value: init
  containers:
  - name: egress-router-wait
    image: registry.redhat.io/openshift4/ose-pod</programlisting>
<calloutlist>
<callout arearefs="CO199-1">
<para>The annotation tells OpenShift Container Platform to create a macvlan network interface on the primary network interface controller (NIC) and move that macvlan interface into the pod&#8217;s network namespace. You must include the quotation marks around the <literal>"true"</literal> value. To have OpenShift Container Platform create the macvlan interface on a different NIC interface, set the annotation value to the name of that interface. For example, <literal>eth1</literal>.</para>
</callout>
<callout arearefs="CO199-2">
<para>IP address from the physical network that the node is on that is reserved for use by the egress router pod. Optional: You can include the subnet length, the <literal>/24</literal> suffix, so that a proper route to the local subnet is set. If you do not specify a subnet length, then the egress router can access only the host specified with the <literal>EGRESS_GATEWAY</literal> variable and no other hosts on the subnet.</para>
</callout>
<callout arearefs="CO199-3">
<para>Same value as the default gateway used by the node.</para>
</callout>
<callout arearefs="CO199-4">
<para>External server to direct traffic to. Using this example, connections to the pod are redirected to <literal>203.0.113.25</literal>, with a source IP address of <literal>192.168.12.99</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example egress router pod specification</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: egress-multi
  labels:
    name: egress-multi
  annotations:
    pod.network.openshift.io/assign-macvlan: "true"
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE
      value: 192.168.12.99/24
    - name: EGRESS_GATEWAY
      value: 192.168.12.1
    - name: EGRESS_DESTINATION
      value: |
        80   tcp 203.0.113.25
        8080 tcp 203.0.113.26 80
        8443 tcp 203.0.113.26 443
        203.0.113.27
    - name: EGRESS_ROUTER_MODE
      value: init
  containers:
  - name: egress-router-wait
    image: registry.redhat.io/openshift4/ose-pod</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-router-dest-var_deploying-egress-router-layer3-redirection">
<title>Egress destination configuration format</title>
<simpara>When an egress router pod is deployed in redirect mode, you can specify redirection rules by using one or more of the following formats:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>&lt;port&gt; &lt;protocol&gt; &lt;ip_address&gt;</literal> - Incoming connections to the given <literal>&lt;port&gt;</literal> should be redirected to the same port on the given <literal>&lt;ip_address&gt;</literal>. <literal>&lt;protocol&gt;</literal> is either <literal>tcp</literal> or <literal>udp</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;port&gt; &lt;protocol&gt; &lt;ip_address&gt; &lt;remote_port&gt;</literal> - As above, except that the connection is redirected to a different <literal>&lt;remote_port&gt;</literal> on <literal>&lt;ip_address&gt;</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;ip_address&gt;</literal> - If the last line is a single IP address, then any connections on any other port will be redirected to the corresponding port on that IP address. If there is no fallback IP address then connections on other ports are rejected.</simpara>
</listitem>
</itemizedlist>
<simpara>In the example that follows several rules are defined:</simpara>
<itemizedlist>
<listitem>
<simpara>The first line redirects traffic from local port <literal>80</literal> to port <literal>80</literal> on <literal>203.0.113.25</literal>.</simpara>
</listitem>
<listitem>
<simpara>The second and third lines redirect local ports <literal>8080</literal> and <literal>8443</literal> to remote ports <literal>80</literal> and <literal>443</literal> on <literal>203.0.113.26</literal>.</simpara>
</listitem>
<listitem>
<simpara>The last line matches traffic for any ports not specified in the previous rules.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Example configuration</title>
<para>
<programlisting language="text" linenumbering="unnumbered">80   tcp 203.0.113.25
8080 tcp 203.0.113.26 80
8443 tcp 203.0.113.26 443
203.0.113.27</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-router-redirect-mode_deploying-egress-router-layer3-redirection">
<title>Deploying an egress router pod in redirect mode</title>
<simpara>In <emphasis>redirect mode</emphasis>, an egress router pod sets up iptables rules to redirect traffic from its own IP address to one or more destination IP addresses. Client pods that need to use the reserved source IP address must be configured to access the service for the egress router rather than connecting directly to the destination IP. You can access the destination service and port from the application pod by using the <literal>curl</literal> command. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl &lt;router_service_IP&gt; &lt;port&gt;</programlisting>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an egress router pod.</simpara>
</listitem>
<listitem>
<simpara>To ensure that other pods can find the IP address of the egress router pod, create a service to point to the egress router pod, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: http
    port: 80
  - name: https
    port: 443
  type: ClusterIP
  selector:
    name: egress-1</programlisting>
<simpara>Your pods can now connect to this service. Their connections are redirected to
the corresponding ports on the external server, using the reserved egress IP
address.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="deploying-egress-router-layer3-redirection-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-egress-router-configmap">Configuring an egress router destination mappings with a ConfigMap</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-egress-router-http-redirection">
<title>Deploying an egress router pod in HTTP proxy mode</title>

<simpara>As a cluster administrator, you can deploy an egress router pod configured to proxy traffic to specified HTTP and HTTPS-based services.</simpara>
<section xml:id="nw-egress-router-pod_deploying-egress-router-http-redirection">
<title>Egress router pod specification for HTTP mode</title>
<simpara>Define the configuration for an egress router pod in the <literal>Pod</literal> object. The following YAML describes the fields for the configuration of an egress router pod in HTTP mode:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: egress-1
  labels:
    name: egress-1
  annotations:
    pod.network.openshift.io/assign-macvlan: "true" <co xml:id="CO200-1"/>
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE <co xml:id="CO200-2"/>
      value: &lt;egress-router&gt;
    - name: EGRESS_GATEWAY <co xml:id="CO200-3"/>
      value: &lt;egress-gateway&gt;
    - name: EGRESS_ROUTER_MODE
      value: http-proxy
  containers:
  - name: egress-router-pod
    image: registry.redhat.io/openshift4/ose-egress-http-proxy
    env:
    - name: EGRESS_HTTP_PROXY_DESTINATION <co xml:id="CO200-4"/>
      value: |-
        ...
    ...</programlisting>
<calloutlist>
<callout arearefs="CO200-1">
<para>The annotation tells OpenShift Container Platform to create a macvlan network interface on the primary network interface controller (NIC) and move that macvlan interface into the pod&#8217;s network namespace. You must include the quotation marks around the <literal>"true"</literal> value. To have OpenShift Container Platform create the macvlan interface on a different NIC interface, set the annotation value to the name of that interface. For example, <literal>eth1</literal>.</para>
</callout>
<callout arearefs="CO200-2">
<para>IP address from the physical network that the node is on that is reserved for use by the egress router pod. Optional: You can include the subnet length, the <literal>/24</literal> suffix, so that a proper route to the local subnet is set. If you do not specify a subnet length, then the egress router can access only the host specified with the <literal>EGRESS_GATEWAY</literal> variable and no other hosts on the subnet.</para>
</callout>
<callout arearefs="CO200-3">
<para>Same value as the default gateway used by the node.</para>
</callout>
<callout arearefs="CO200-4">
<para>A string or YAML multi-line string specifying how to configure the proxy. Note that this is specified as an environment variable in the HTTP proxy container, not with the other environment variables in the init container.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-egress-router-dest-var_deploying-egress-router-http-redirection">
<title>Egress destination configuration format</title>
<simpara>When an egress router pod is deployed in HTTP proxy mode, you can specify redirection rules by using one or more of the following formats. Each line in the configuration specifies one group of connections to allow or deny:</simpara>
<itemizedlist>
<listitem>
<simpara>An IP address allows connections to that IP address, such as <literal>192.168.1.1</literal>.</simpara>
</listitem>
<listitem>
<simpara>A CIDR range allows connections to that CIDR range, such as <literal>192.168.1.0/24</literal>.</simpara>
</listitem>
<listitem>
<simpara>A hostname allows proxying to that host, such as <literal>www.example.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>A domain name preceded by <literal>*.</literal> allows proxying to that domain and all of its subdomains, such as <literal>*.example.com</literal>.</simpara>
</listitem>
<listitem>
<simpara>A <literal>!</literal> followed by any of the previous match expressions denies the connection instead.</simpara>
</listitem>
<listitem>
<simpara>If the last line is <literal>*</literal>, then anything that is not explicitly denied is allowed. Otherwise, anything that is not allowed is denied.</simpara>
</listitem>
</itemizedlist>
<simpara>You can also use <literal>*</literal> to allow connections to all remote destinations.</simpara>
<formalpara>
<title>Example configuration</title>
<para>
<programlisting language="text" linenumbering="unnumbered">!*.example.com
!192.168.1.0/24
192.168.2.1
*</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-router-http-proxy-mode_deploying-egress-router-http-redirection">
<title>Deploying an egress router pod in HTTP proxy mode</title>
<simpara>In <emphasis>HTTP proxy mode</emphasis>, an egress router pod runs as an HTTP proxy on port <literal>8080</literal>. This mode only works for clients that are connecting to HTTP-based or HTTPS-based services, but usually requires fewer changes to the client pods to get them to work. Many programs can be told to use an HTTP proxy by setting an environment variable.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an egress router pod.</simpara>
</listitem>
<listitem>
<simpara>To ensure that other pods can find the IP address of the egress router pod, create a service to point to the egress router pod, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: egress-1
spec:
  ports:
  - name: http-proxy
    port: 8080 <co xml:id="CO201-1"/>
  type: ClusterIP
  selector:
    name: egress-1</programlisting>
<calloutlist>
<callout arearefs="CO201-1">
<para>Ensure the <literal>http</literal> port is set to <literal>8080</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To configure the client pod (not the egress proxy pod) to use the HTTP proxy, set the <literal>http_proxy</literal> or <literal>https_proxy</literal> variables:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: app-1
  labels:
    name: app-1
spec:
  containers:
    env:
    - name: http_proxy
      value: http://egress-1:8080/ <co xml:id="CO202-1"/>
    - name: https_proxy
      value: http://egress-1:8080/
    ...</programlisting>
<calloutlist>
<callout arearefs="CO202-1">
<para>The service created in the previous step.</para>
</callout>
</calloutlist>
<note>
<simpara>Using the <literal>http_proxy</literal> and <literal>https_proxy</literal> environment variables is not necessary for all setups. If the above does not create a working setup, then consult the documentation for the tool or software you are running in the pod.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="deploying-egress-router-http-redirection-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-egress-router-configmap">Configuring an egress router destination mappings with a ConfigMap</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deploying-egress-router-dns-redirection">
<title>Deploying an egress router pod in DNS proxy mode</title>

<simpara>As a cluster administrator, you can deploy an egress router pod configured to proxy traffic to specified DNS names and IP addresses.</simpara>
<section xml:id="nw-egress-router-pod_deploying-egress-router-dns-redirection">
<title>Egress router pod specification for DNS mode</title>
<simpara>Define the configuration for an egress router pod in the <literal>Pod</literal> object. The following YAML describes the fields for the configuration of an egress router pod in DNS mode:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: egress-1
  labels:
    name: egress-1
  annotations:
    pod.network.openshift.io/assign-macvlan: "true" <co xml:id="CO203-1"/>
spec:
  initContainers:
  - name: egress-router
    image: registry.redhat.io/openshift4/ose-egress-router
    securityContext:
      privileged: true
    env:
    - name: EGRESS_SOURCE <co xml:id="CO203-2"/>
      value: &lt;egress-router&gt;
    - name: EGRESS_GATEWAY <co xml:id="CO203-3"/>
      value: &lt;egress-gateway&gt;
    - name: EGRESS_ROUTER_MODE
      value: dns-proxy
  containers:
  - name: egress-router-pod
    image: registry.redhat.io/openshift4/ose-egress-dns-proxy
    securityContext:
      privileged: true
    env:
    - name: EGRESS_DNS_PROXY_DESTINATION <co xml:id="CO203-4"/>
      value: |-
        ...
    - name: EGRESS_DNS_PROXY_DEBUG <co xml:id="CO203-5"/>
      value: "1"
    ...</programlisting>
<calloutlist>
<callout arearefs="CO203-1">
<para>The annotation tells OpenShift Container Platform to create a macvlan network interface on the primary network interface controller (NIC) and move that macvlan interface into the pod&#8217;s network namespace. You must include the quotation marks around the <literal>"true"</literal> value. To have OpenShift Container Platform create the macvlan interface on a different NIC interface, set the annotation value to the name of that interface. For example, <literal>eth1</literal>.</para>
</callout>
<callout arearefs="CO203-2">
<para>IP address from the physical network that the node is on that is reserved for use by the egress router pod. Optional: You can include the subnet length, the <literal>/24</literal> suffix, so that a proper route to the local subnet is set. If you do not specify a subnet length, then the egress router can access only the host specified with the <literal>EGRESS_GATEWAY</literal> variable and no other hosts on the subnet.</para>
</callout>
<callout arearefs="CO203-3">
<para>Same value as the default gateway used by the node.</para>
</callout>
<callout arearefs="CO203-4">
<para>Specify a list of one or more proxy destinations.</para>
</callout>
<callout arearefs="CO203-5">
<para>Optional: Specify to output the DNS proxy log output to <literal>stdout</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-egress-router-dest-var_deploying-egress-router-dns-redirection">
<title>Egress destination configuration format</title>
<simpara>When the router is deployed in DNS proxy mode, you specify a list of port and destination mappings. A destination may be either an IP address or a DNS name.</simpara>
<simpara>An egress router pod supports the following formats for specifying port and destination mappings:</simpara>
<variablelist>
<varlistentry>
<term>Port and remote address</term>
<listitem>
<simpara>You can specify a source port and a destination host by using the two field format: <literal>&lt;port&gt; &lt;remote_address&gt;</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The host can be an IP address or a DNS name. If a DNS name is provided, DNS resolution occurs at runtime. For a given host, the proxy connects to the specified source port on the destination host when connecting to the destination host IP address.</simpara>
<formalpara>
<title>Port and remote address pair example</title>
<para>
<programlisting language="text" linenumbering="unnumbered">80 172.16.12.11
100 example.com</programlisting>
</para>
</formalpara>
<variablelist>
<varlistentry>
<term>Port, remote address, and remote port</term>
<listitem>
<simpara>You can specify a source port, a destination host, and a destination port by using the three field format: <literal>&lt;port&gt; &lt;remote_address&gt; &lt;remote_port&gt;</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The three field format behaves identically to the two field version, with the exception that the destination port can be different than the source port.</simpara>
<formalpara>
<title>Port, remote address, and remote port example</title>
<para>
<programlisting language="text" linenumbering="unnumbered">8080 192.168.60.252 80
8443 web.example.com 443</programlisting>
</para>
</formalpara>
</section>
<section xml:id="nw-egress-router-dns-mode_deploying-egress-router-dns-redirection">
<title>Deploying an egress router pod in DNS proxy mode</title>
<simpara>In <emphasis>DNS proxy mode</emphasis>, an egress router pod acts as a DNS proxy for TCP-based services from its own IP address to one or more destination IP addresses.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an egress router pod.</simpara>
</listitem>
<listitem>
<simpara>Create a service for the egress router pod:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file named <literal>egress-router-service.yaml</literal> that contains the following YAML. Set <literal>spec.ports</literal> to the list of ports that you defined previously for the <literal>EGRESS_DNS_PROXY_DESTINATION</literal> environment variable.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: egress-dns-svc
spec:
  ports:
    ...
  type: ClusterIP
  selector:
    name: egress-dns-proxy</programlisting>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: egress-dns-svc
spec:
  ports:
  - name: con1
    protocol: TCP
    port: 80
    targetPort: 80
  - name: con2
    protocol: TCP
    port: 100
    targetPort: 100
  type: ClusterIP
  selector:
    name: egress-dns-proxy</programlisting>
</listitem>
<listitem>
<simpara>To create the service, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f egress-router-service.yaml</programlisting>
<simpara>Pods can now connect to this service. The connections are proxied to the corresponding ports on the external server, using the reserved egress IP address.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="deploying-egress-router-dns-redirection-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-egress-router-configmap">Configuring an egress router destination mappings with a ConfigMap</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-egress-router-configmap">
<title>Configuring an egress router pod destination list from a config map</title>

<simpara>As a cluster administrator, you can define a <literal>ConfigMap</literal> object that specifies destination mappings for an egress router pod. The specific format of the configuration depends on the type of egress router pod. For details on the format, refer to the documentation for the specific egress router pod.</simpara>
<section xml:id="configuring-egress-router-configmap_configuring-egress-router-configmap">
<title>Configuring an egress router destination mappings with a config map</title>
<simpara>For a large or frequently-changing set of destination mappings, you can use a config map to externally maintain the list.
An advantage of this approach is that permission to edit the config map can be delegated to users without <literal>cluster-admin</literal> privileges. Because the egress router pod requires a privileged container, it is not possible for users without <literal>cluster-admin</literal> privileges to edit the pod definition directly.</simpara>
<note>
<simpara>The egress router pod does not automatically update when the config map changes.
You must restart the egress router pod to get updates.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file containing the mapping data for the egress router pod, as in the following example:</simpara>
<screen># Egress routes for Project "Test", version 3

80   tcp 203.0.113.25

8080 tcp 203.0.113.26 80
8443 tcp 203.0.113.26 443

# Fallback
203.0.113.27</screen>
<simpara>You can put blank lines and comments into this file.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>ConfigMap</literal> object from the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete configmap egress-routes --ignore-not-found</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap egress-routes \
  --from-file=destination=my-egress-destination.txt</programlisting>
<simpara>In the previous command, the <literal>egress-routes</literal> value is the name of the <literal>ConfigMap</literal> object to create and <literal>my-egress-destination.txt</literal> is the name of the file that the data is read from.</simpara>
<tip>
<simpara>You can alternatively apply the following YAML to create the config map:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: egress-routes
data:
  destination: |
    # Egress routes for Project "Test", version 3

    80   tcp 203.0.113.25

    8080 tcp 203.0.113.26 80
    8443 tcp 203.0.113.26 443

    # Fallback
    203.0.113.27</programlisting>
</tip>
</listitem>
<listitem>
<simpara>Create an egress router pod definition and specify the <literal>configMapKeyRef</literal> stanza for the <literal>EGRESS_DESTINATION</literal> field in the environment stanza:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">...
env:
- name: EGRESS_DESTINATION
  valueFrom:
    configMapKeyRef:
      name: egress-routes
      key: destination
...</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-egress-router-configmap-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-egress-router-dest-var_deploying-egress-router-layer3-redirection">Redirect mode</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-egress-router-dest-var_deploying-egress-router-http-redirection">HTTP proxy mode</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-egress-router-dest-var_deploying-egress-router-dns-redirection">DNS proxy mode</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="enabling-multicast">
<title>Enabling multicast for a project</title>

<section xml:id="nw-about-multicast_openshift-sdn-enabling-multicast">
<title>About multicast</title>
<simpara>With IP multicast, data is broadcast to many IP addresses simultaneously.</simpara>
<important>
<itemizedlist>
<listitem>
<simpara>At this time, multicast is best used for low-bandwidth coordination or service discovery and not a high-bandwidth solution.</simpara>
</listitem>
<listitem>
<simpara>By default, network policies affect all connections in a namespace. However, multicast is unaffected by network policies. If multicast is enabled in the same namespace as your network policies, it is always allowed, even if there is a <literal>deny-all</literal> network policy. Cluster administrators should consider the implications to the exemption of multicast from network policies before enabling it.</simpara>
</listitem>
</itemizedlist>
</important>
<simpara>Multicast traffic between OpenShift Container Platform pods is disabled by default. If you are using the OpenShift SDN network plugin, you can enable multicast on a per-project basis.</simpara>
<simpara>When using the OpenShift SDN network plugin in <literal>networkpolicy</literal> isolation mode:</simpara>
<itemizedlist>
<listitem>
<simpara>Multicast packets sent by a pod will be delivered to all other pods in the project, regardless of <literal>NetworkPolicy</literal> objects. Pods might be able to communicate over multicast even when they cannot communicate over unicast.</simpara>
</listitem>
<listitem>
<simpara>Multicast packets sent by a pod in one project will never be delivered to pods in any other project, even if there are <literal>NetworkPolicy</literal> objects that allow communication between the projects.</simpara>
</listitem>
</itemizedlist>
<simpara>When using the OpenShift SDN network plugin in <literal>multitenant</literal> isolation mode:</simpara>
<itemizedlist>
<listitem>
<simpara>Multicast packets sent by a pod will be delivered to all other pods in the
project.</simpara>
</listitem>
<listitem>
<simpara>Multicast packets sent by a pod in one project will be delivered to pods in
other projects only if each project is joined together and multicast is enabled
in each joined project.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-enabling-multicast_openshift-sdn-enabling-multicast">
<title>Enabling multicast between pods</title>
<simpara>You can enable multicast between pods for your project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal>
role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to enable multicast for a project. Replace <literal>&lt;namespace&gt;</literal> with the namespace for the project you want to enable multicast for.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate netnamespace &lt;namespace&gt; \
    netnamespace.network.openshift.io/multicast-enabled=true</programlisting>
</listitem>
</itemizedlist>
<formalpara>
<title>Verification</title>
<para>To verify that multicast is enabled for a project, complete the following procedure:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Change your current project to the project that you enabled multicast for. Replace <literal>&lt;project&gt;</literal> with the project name.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project &lt;project&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create a pod to act as a multicast receiver:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: mlistener
  labels:
    app: multicast-verify
spec:
  containers:
    - name: mlistener
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat hostname &amp;&amp; sleep inf"]
      ports:
        - containerPort: 30102
          name: mlistener
          protocol: UDP
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create a pod to act as a multicast sender:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF| oc create -f -
apiVersion: v1
kind: Pod
metadata:
  name: msender
  labels:
    app: multicast-verify
spec:
  containers:
    - name: msender
      image: registry.access.redhat.com/ubi9
      command: ["/bin/sh", "-c"]
      args:
        ["dnf -y install socat &amp;&amp; sleep inf"]
EOF</programlisting>
</listitem>
<listitem>
<simpara>In a new terminal window or tab, start the multicast listener.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the IP address for the Pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POD_IP=$(oc get pods mlistener -o jsonpath='{.status.podIP}')</programlisting>
</listitem>
<listitem>
<simpara>Start the multicast listener by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec mlistener -i -t -- \
    socat UDP4-RECVFROM:30102,ip-add-membership=224.1.0.1:$POD_IP,fork EXEC:hostname</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Start the multicast transmitter.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the pod network IP address range:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CIDR=$(oc get Network.config.openshift.io cluster \
    -o jsonpath='{.status.clusterNetwork[0].cidr}')</programlisting>
</listitem>
<listitem>
<simpara>To send a multicast message, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec msender -i -t -- \
    /bin/bash -c "echo | socat STDIO UDP4-DATAGRAM:224.1.0.1:30102,range=$CIDR,ip-multicast-ttl=64"</programlisting>
<simpara>If multicast is working, the previous command returns the following output:</simpara>
<programlisting language="text" linenumbering="unnumbered">mlistener</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="disabling-multicast">
<title>Disabling multicast for a project</title>

<section xml:id="nw-disabling-multicast_openshift-sdn-disabling-multicast">
<title>Disabling multicast between pods</title>
<simpara>You can disable multicast between pods for your project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Disable multicast by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate netnamespace &lt;namespace&gt; \ <co xml:id="CO204-1"/>
    netnamespace.network.openshift.io/multicast-enabled-</programlisting>
<calloutlist>
<callout arearefs="CO204-1">
<para>The <literal>namespace</literal> for the project you want to disable multicast for.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-multitenant-isolation">
<title>Configuring network isolation using OpenShift SDN</title>

<simpara>When your cluster is configured to use the multitenant isolation mode for the
OpenShift SDN network plugin, each project is isolated by default. Network traffic
is not allowed between pods or services in different projects in multitenant
isolation mode.</simpara>
<simpara>You can change the behavior of multitenant isolation for a project in two ways:</simpara>
<itemizedlist>
<listitem>
<simpara>You can join one or more projects, allowing network traffic between pods and
services in different projects.</simpara>
</listitem>
<listitem>
<simpara>You can disable network isolation for a project. It will be globally
accessible, accepting network traffic from pods and services in all other
projects. A globally accessible project can access pods and services in all
other projects.</simpara>
</listitem>
</itemizedlist>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You must have a cluster configured to use the OpenShift SDN network plugin in multitenant isolation mode.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-multitenant-joining_multitenant-isolation">
<title>Joining projects</title>
<simpara>You can join two or more projects to allow network traffic between pods and
services in different projects.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Use the following command to join projects to an existing project network:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm pod-network join-projects --to=&lt;project1&gt; &lt;project2&gt; &lt;project3&gt;</programlisting>
<simpara>Alternatively, instead of specifying specific project names, you can use the
<literal>--selector=&lt;project_selector&gt;</literal> option to specify projects based upon an
associated label.</simpara>
</listitem>
<listitem>
<simpara>Optional: Run the following command to view the pod networks that you have
joined together:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get netnamespaces</programlisting>
<simpara>Projects in the same pod-network have the same network ID in the <emphasis role="strong">NETID</emphasis> column.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-multitenant-isolation_multitenant-isolation">
<title>Isolating a project</title>
<simpara>You can isolate a project so that pods and services in other projects cannot
access its pods and services.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To isolate the projects in the cluster, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm pod-network isolate-projects &lt;project1&gt; &lt;project2&gt;</programlisting>
<simpara>Alternatively, instead of specifying specific project names, you can use the
<literal>--selector=&lt;project_selector&gt;</literal> option to specify projects based upon an
associated label.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-multitenant-global_multitenant-isolation">
<title>Disabling network isolation for a project</title>
<simpara>You can disable network isolation for a project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user that has the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command for the project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm pod-network make-projects-global &lt;project1&gt; &lt;project2&gt;</programlisting>
<simpara>Alternatively, instead of specifying specific project names, you can use the
<literal>--selector=&lt;project_selector&gt;</literal> option to specify projects based upon an
associated label.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-kube-proxy">
<title>Configuring kube-proxy</title>

<simpara>The Kubernetes network proxy (kube-proxy) runs on each node and is managed by
the Cluster Network Operator (CNO). kube-proxy maintains network rules for
forwarding connections for endpoints associated with services.</simpara>
<section xml:id="nw-kube-proxy-sync_configuring-kube-proxy">
<title>About iptables rules synchronization</title>
<simpara>The synchronization period determines how frequently the Kubernetes network
proxy (kube-proxy) syncs the iptables rules on a node.</simpara>
<simpara>A sync begins when either of the following events occurs:</simpara>
<itemizedlist>
<listitem>
<simpara>An event occurs, such as service or endpoint is added to or removed from the
cluster.</simpara>
</listitem>
<listitem>
<simpara>The time since the last sync exceeds the sync period defined for kube-proxy.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-kube-proxy-config_configuring-kube-proxy">
<title>kube-proxy configuration parameters</title>
<simpara>You can modify the following <literal>kubeProxyConfig</literal> parameters.</simpara>
<note>
<simpara>Because of performance improvements introduced in OpenShift Container Platform 4.3 and greater, adjusting the <literal>iptablesSyncPeriod</literal> parameter is no longer necessary.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title>Parameters</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="30*"/>
<colspec colname="col_3" colwidth="30*"/>
<colspec colname="col_4" colwidth="10*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Values</entry>
<entry align="left" valign="top">Default</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>iptablesSyncPeriod</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The refresh period for <literal>iptables</literal> rules.</simpara></entry>
<entry align="left" valign="top"><simpara>A time interval, such as <literal>30s</literal> or <literal>2m</literal>. Valid
suffixes include <literal>s</literal>, <literal>m</literal>, and <literal>h</literal> and are described in the
<link xlink:href="https://golang.org/pkg/time/#ParseDuration">Go time package</link> documentation.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>30s</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>proxyArguments.iptables-min-sync-period</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The minimum duration before refreshing <literal>iptables</literal> rules. This parameter ensures
that the refresh does not happen too frequently. By default, a refresh starts as soon as a change that affects <literal>iptables</literal> rules occurs.</simpara></entry>
<entry align="left" valign="top"><simpara>A time interval, such as <literal>30s</literal> or <literal>2m</literal>. Valid suffixes include <literal>s</literal>,
<literal>m</literal>, and <literal>h</literal> and are described in the
<link xlink:href="https://golang.org/pkg/time/#ParseDuration">Go time package</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>0s</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-kube-proxy-configuring_configuring-kube-proxy">
<title>Modifying the kube-proxy configuration</title>
<simpara>You can modify the Kubernetes network proxy configuration for your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to a running cluster with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>Network.operator.openshift.io</literal> custom resource (CR) by running the
following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit network.operator.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>Modify the <literal>kubeProxyConfig</literal> parameter in the CR with your changes to the
kube-proxy configuration, such as in the following example CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  kubeProxyConfig:
    iptablesSyncPeriod: 30s
    proxyArguments:
      iptables-min-sync-period: ["30s"]</programlisting>
</listitem>
<listitem>
<simpara>Save the file and exit the text editor.</simpara>
<simpara>The syntax is validated by the <literal>oc</literal> command when you save the file and exit the
editor. If your modifications contain a syntax error, the editor opens the file
and displays an error message.</simpara>
</listitem>
<listitem>
<simpara>Enter the following command to confirm the configuration update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networks.operator.openshift.io -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
items:
- apiVersion: operator.openshift.io/v1
  kind: Network
  metadata:
    name: cluster
  spec:
    clusterNetwork:
    - cidr: 10.128.0.0/14
      hostPrefix: 23
    defaultNetwork:
      type: OpenShiftSDN
    kubeProxyConfig:
      iptablesSyncPeriod: 30s
      proxyArguments:
        iptables-min-sync-period:
        - 30s
    serviceNetwork:
    - 172.30.0.0/16
  status: {}
kind: List</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: Enter the following command to confirm that the Cluster Network
Operator accepted the configuration change:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusteroperator network</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      VERSION     AVAILABLE   PROGRESSING   DEGRADED   SINCE
network   4.1.0-0.9   True        False         False      1m</programlisting>
</para>
</formalpara>
<simpara>The <literal>AVAILABLE</literal> field is <literal>True</literal> when the configuration update is applied
successfully.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="_configuring-routes">
<title>Configuring Routes</title>
<section xml:id="route-configuration">
<title>Route configuration</title>

<section xml:id="nw-creating-a-route_route-configuration">
<title>Creating an HTTP-based route</title>
<simpara>A route allows you to host your application at a public URL. It can either be secure or unsecured, depending on the network security configuration of your application. An HTTP-based route is an unsecured route that uses the basic HTTP routing protocol and exposes a service on an unsecured application port.</simpara>
<simpara>The following procedure describes how to create a simple HTTP-based route to a web application, using the <literal>hello-openshift</literal> application as an example.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as an administrator.</simpara>
</listitem>
<listitem>
<simpara>You have a web application that exposes a port and a TCP endpoint listening for traffic on the port.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a project called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a pod in the project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</programlisting>
</listitem>
<listitem>
<simpara>Create a service called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose pod/hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create an unsecured route to the <literal>hello-openshift</literal> application by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose svc hello-openshift</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To verify that the <literal>route</literal> resource that you created, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes -o yaml &lt;name of resource&gt; <co xml:id="CO205-1"/></programlisting>
<calloutlist>
<callout arearefs="CO205-1">
<para>In this example, the route is named <literal>hello-openshift</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Sample YAML definition of the created unsecured route:</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: hello-openshift
spec:
  host: hello-openshift-hello-openshift.&lt;Ingress_Domain&gt; <co xml:id="CO206-1"/>
  port:
    targetPort: 8080 <co xml:id="CO206-2"/>
  to:
    kind: Service
    name: hello-openshift</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO206-1">
<para><literal>&lt;Ingress_Domain&gt;</literal> is the default ingress domain name. The <literal>ingresses.config/cluster</literal> object is created during the installation and cannot be changed. If you want to specify a different domain, you can specify an alternative cluster domain using the <literal>appsDomain</literal> option.</para>
</callout>
<callout arearefs="CO206-2">
<para><literal>targetPort</literal> is the target port on pods that is selected by the service that this route points to.</para>
<note>
<simpara>To display your default ingress domain, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ingresses.config/cluster -o jsonpath={.spec.domain}</programlisting>
</note>
</callout>
</calloutlist>
</section>
<section xml:id="nw-ingress-sharding-route-configuration_route-configuration">
<title>Creating a route for Ingress Controller sharding</title>
<simpara>A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.</simpara>
<simpara>The following procedure describes how to create a route for Ingress Controller sharding, using the <literal>hello-openshift</literal> application as an example.</simpara>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a project administrator.</simpara>
</listitem>
<listitem>
<simpara>You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.</simpara>
</listitem>
<listitem>
<simpara>You have configured the Ingress Controller for sharding.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a project called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a pod in the project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</programlisting>
</listitem>
<listitem>
<simpara>Create a service called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose pod/hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a route definition called <literal>hello-openshift-route.yaml</literal>:</simpara>
<formalpara>
<title>YAML definition of the created route for sharding:</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <co xml:id="CO207-1"/>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <co xml:id="CO207-2"/>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO207-1">
<para>Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value <literal>type: sharded</literal>.</para>
</callout>
<callout arearefs="CO207-2">
<para>The route will be exposed using the value of the <literal>subdomain</literal> field. When you specify the <literal>subdomain</literal> field, you must leave the hostname unset. If you specify both the <literal>host</literal> and <literal>subdomain</literal> fields, then the route will use the value of the <literal>host</literal> field, and ignore the <literal>subdomain</literal> field.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Use <literal>hello-openshift-route.yaml</literal> to create a route to the <literal>hello-openshift</literal> application by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-openshift create -f hello-openshift-route.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Get the status of the route with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml</programlisting>
<simpara>The resulting <literal>Route</literal> resource should look similar to the following:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.&lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO208-1"/>
    routerCanonicalHostname: router-sharded.&lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO208-2"/>
    routerName: sharded <co xml:id="CO208-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO208-1">
<para>The hostname the Ingress Controller, or router, uses to expose the route. The value of the <literal>host</literal> field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is <literal>&lt;apps-sharded.basedomain.example.net&gt;</literal>.</para>
</callout>
<callout arearefs="CO208-2">
<para>The hostname of the Ingress Controller.</para>
</callout>
<callout arearefs="CO208-3">
<para>The name of the Ingress Controller. In this example, the Ingress Controller has the name <literal>sharded</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-configuring-route-timeouts_route-configuration">
<title>Configuring route timeouts</title>
<simpara>You can configure the default timeouts for an existing route when you
have services in need of a low timeout, which is required for Service Level
Availability (SLA) purposes, or a high timeout, for cases with a slow
back end.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You need a deployed Ingress Controller on a running cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Using the <literal>oc annotate</literal> command, add the timeout to the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route &lt;route_name&gt; \
    --overwrite haproxy.router.openshift.io/timeout=&lt;timeout&gt;&lt;time_unit&gt; <co xml:id="CO209-1"/></programlisting>
<calloutlist>
<callout arearefs="CO209-1">
<para>Supported time units are microseconds (us), milliseconds (ms), seconds (s),
minutes (m), hours (h), or days (d).</para>
</callout>
</calloutlist>
<simpara>The following example sets  a timeout of two seconds on a route named <literal>myroute</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route myroute --overwrite haproxy.router.openshift.io/timeout=2s</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-enabling-hsts_route-configuration">
<title>HTTP Strict Transport Security</title>
<simpara>HTTP Strict Transport Security (HSTS) policy is a security enhancement, which signals to the browser client that only HTTPS traffic is allowed on the route host. HSTS also optimizes web traffic by signaling HTTPS transport is required, without using HTTP redirects. HSTS is useful for speeding up interactions with websites.</simpara>
<simpara>When HSTS policy is enforced, HSTS adds a Strict Transport Security header to HTTP and HTTPS responses from the site. You can use the <literal>insecureEdgeTerminationPolicy</literal> value in a route to redirect HTTP to HTTPS. When HSTS is enforced, the client changes all requests from the HTTP URL to HTTPS before the request is sent, eliminating the need for a redirect.</simpara>
<simpara>Cluster administrators can configure HSTS to do the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Enable HSTS per-route</simpara>
</listitem>
<listitem>
<simpara>Disable HSTS per-route</simpara>
</listitem>
<listitem>
<simpara>Enforce HSTS per-domain, for a set of domains, or use namespace labels in combination with domains</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>HSTS works only with secure routes, either edge-terminated or re-encrypt. The configuration is ineffective on HTTP or passthrough routes.</simpara>
</important>
<section xml:id="nw-enabling-hsts-per-route_route-configuration">
<title>Enabling HTTP Strict Transport Security per-route</title>
<simpara>HTTP strict transport security (HSTS) is implemented in the HAProxy template and applied to edge and re-encrypt routes that have the <literal>haproxy.router.openshift.io/hsts_header</literal> annotation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster with a user with administrator privileges for the project.</simpara>
</listitem>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To enable HSTS on a route, add the <literal>haproxy.router.openshift.io/hsts_header</literal> value to the edge-terminated or re-encrypt route. You can use the <literal>oc annotate</literal> tool to do this by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route &lt;route_name&gt; -n &lt;namespace&gt; --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000;\ <co xml:id="CO210-1"/>
includeSubDomains;preload"</programlisting>
<calloutlist>
<callout arearefs="CO210-1">
<para>In this example, the maximum age is set to <literal>31536000</literal> ms, which is approximately eight and a half hours.</para>
</callout>
</calloutlist>
<note>
<simpara>In this example, the equal sign (<literal>=</literal>) is in quotes. This is required to properly execute the annotate command.</simpara>
</note>
<formalpara>
<title>Example route configured with an annotation</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/hsts_header: max-age=31536000;includeSubDomains;preload <co xml:id="CO211-1"/> <co xml:id="CO211-2"/> <co xml:id="CO211-3"/>
...
spec:
  host: def.abc.com
  tls:
    termination: "reencrypt"
    ...
  wildcardPolicy: "Subdomain"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO211-1">
<para>Required. <literal>max-age</literal> measures the length of time, in seconds, that the HSTS policy is in effect. If set to <literal>0</literal>, it negates the policy.</para>
</callout>
<callout arearefs="CO211-2">
<para>Optional. When included, <literal>includeSubDomains</literal> tells the client
that all subdomains of the host must have the same HSTS policy as the host.</para>
</callout>
<callout arearefs="CO211-3">
<para>Optional. When <literal>max-age</literal> is greater than 0, you can add <literal>preload</literal> in  <literal>haproxy.router.openshift.io/hsts_header</literal> to allow external services to include this site in their HSTS preload lists. For example, sites such as Google can construct a list of sites that have <literal>preload</literal> set. Browsers can then use these lists to determine which sites they can communicate with over HTTPS, even before they have interacted with the site. Without <literal>preload</literal> set, browsers must have interacted with the site over HTTPS, at least once, to get the header.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-disabling-hsts_route-configuration">
<title>Disabling HTTP Strict Transport Security per-route</title>
<simpara>To disable HTTP strict transport security (HSTS) per-route, you can set the <literal>max-age</literal> value in the route annotation to <literal>0</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster with a user with administrator privileges for the project.</simpara>
</listitem>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To disable HSTS, set the <literal>max-age</literal> value in the route annotation to <literal>0</literal>, by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route &lt;route_name&gt; -n &lt;namespace&gt; --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=0"</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to create the config map:</simpara>
<formalpara>
<title>Example of disabling HSTS per-route</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    haproxy.router.openshift.io/hsts_header: max-age=0</programlisting>
</para>
</formalpara>
</tip>
</listitem>
<listitem>
<simpara>To disable HSTS for every route in a namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route --all -n &lt;namespace&gt; --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=0"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>To query the annotation for all routes, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route  --all-namespaces -o go-template='{{range .items}}{{if .metadata.annotations}}{{$a := index .metadata.annotations "haproxy.router.openshift.io/hsts_header"}}{{$n := .metadata.name}}{{with $a}}Name: {{$n}} HSTS: {{$a}}{{"\n"}}{{else}}{{""}}{{end}}{{end}}{{end}}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name: routename HSTS: max-age=0</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-enforcing-hsts-per-domain_route-configuration">
<title>Enforcing HTTP Strict Transport Security per-domain</title>
<simpara>To enforce HTTP Strict Transport Security (HSTS) per-domain for secure routes, add a <literal>requiredHSTSPolicies</literal> record to the Ingress spec to capture the configuration of the HSTS policy.</simpara>
<simpara>If you configure a <literal>requiredHSTSPolicy</literal> to enforce HSTS, then any newly created route must be configured with a compliant HSTS policy annotation.</simpara>
<note>
<simpara>To handle upgraded clusters with non-compliant HSTS routes, you can update the manifests at the source and apply the updates.</simpara>
</note>
<note>
<simpara>You cannot use <literal>oc expose route</literal> or <literal>oc create route</literal> commands to add a route in a domain that enforces HSTS, because the API for these commands does not accept annotations.</simpara>
</note>
<important>
<simpara>HSTS cannot be applied to insecure, or non-TLS routes, even if HSTS is requested for all routes globally.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster with a user with administrator privileges for the project.</simpara>
</listitem>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the Ingress config file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit ingresses.config.openshift.io/cluster</programlisting>
<formalpara>
<title>Example HSTS policy</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Ingress
metadata:
  name: cluster
spec:
  domain: 'hello-openshift-default.apps.username.devcluster.openshift.com'
  requiredHSTSPolicies: <co xml:id="CO212-1"/>
  - domainPatterns: <co xml:id="CO212-2"/>
    - '*hello-openshift-default.apps.username.devcluster.openshift.com'
    - '*hello-openshift-default2.apps.username.devcluster.openshift.com'
    namespaceSelector: <co xml:id="CO212-3"/>
      matchLabels:
        myPolicy: strict
    maxAge: <co xml:id="CO212-4"/>
      smallestMaxAge: 1
      largestMaxAge: 31536000
    preloadPolicy: RequirePreload <co xml:id="CO212-5"/>
    includeSubDomainsPolicy: RequireIncludeSubDomains <co xml:id="CO212-6"/>
  - domainPatterns: <co xml:id="CO212-7"/>
    - 'abc.example.com'
    - '*xyz.example.com'
    namespaceSelector:
      matchLabels: {}
    maxAge: {}
    preloadPolicy: NoOpinion
    includeSubDomainsPolicy: RequireNoIncludeSubDomains</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO212-1">
<para>Required. <literal>requiredHSTSPolicies</literal> are validated in order, and the first matching <literal>domainPatterns</literal> applies.</para>
</callout>
<callout arearefs="CO212-2 CO212-7">
<para>Required. You must specify at least one <literal>domainPatterns</literal> hostname. Any number of domains can be listed. You can include multiple sections of enforcing options for different <literal>domainPatterns</literal>.</para>
</callout>
<callout arearefs="CO212-3">
<para>Optional. If you include <literal>namespaceSelector</literal>, it must match the labels of the project where the routes reside, to enforce the set HSTS policy on the routes. Routes that only match the <literal>namespaceSelector</literal> and not the <literal>domainPatterns</literal> are not validated.</para>
</callout>
<callout arearefs="CO212-4">
<para>Required. <literal>max-age</literal> measures the length of time, in seconds, that the HSTS policy is in effect. This policy setting allows for a smallest and largest <literal>max-age</literal> to be enforced.</para>
<itemizedlist>
<listitem>
<simpara>The <literal>largestMaxAge</literal> value must be between <literal>0</literal> and <literal>2147483647</literal>. It can be left unspecified, which means no upper limit is enforced.</simpara>
</listitem>
<listitem>
<simpara>The <literal>smallestMaxAge</literal> value must be between <literal>0</literal> and <literal>2147483647</literal>. Enter <literal>0</literal> to disable HSTS for troubleshooting, otherwise enter <literal>1</literal> if you never want HSTS to be disabled. It can be left unspecified, which means no lower limit is enforced.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO212-5">
<para>Optional. Including <literal>preload</literal> in <literal>haproxy.router.openshift.io/hsts_header</literal> allows external services to include this site in their HSTS preload lists. Browsers can then use these lists to determine which sites they can communicate with over HTTPS, before they have interacted with the site. Without <literal>preload</literal> set, browsers need to interact at least once with the site to get the header. <literal>preload</literal> can be set with one of the following:</para>
<itemizedlist>
<listitem>
<simpara><literal>RequirePreload</literal>: <literal>preload</literal> is required by the <literal>RequiredHSTSPolicy</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>RequireNoPreload</literal>: <literal>preload</literal> is forbidden by the <literal>RequiredHSTSPolicy</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>NoOpinion</literal>: <literal>preload</literal> does not matter to the <literal>RequiredHSTSPolicy</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO212-6">
<para>Optional. <literal>includeSubDomainsPolicy</literal> can be set with one of the following:</para>
<itemizedlist>
<listitem>
<simpara><literal>RequireIncludeSubDomains</literal>: <literal>includeSubDomains</literal> is required by the <literal>RequiredHSTSPolicy</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>RequireNoIncludeSubDomains</literal>: <literal>includeSubDomains</literal> is forbidden by the <literal>RequiredHSTSPolicy</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>NoOpinion</literal>: <literal>includeSubDomains</literal> does not matter to the <literal>RequiredHSTSPolicy</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>You can apply HSTS to all routes in the cluster or in a particular namespace by entering the <literal>oc annotate command</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>To apply HSTS to all routes in the cluster, enter the <literal>oc annotate command</literal>. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route --all --all-namespaces --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000"</programlisting>
</listitem>
<listitem>
<simpara>To apply HSTS to all routes in a particular namespace, enter the <literal>oc annotate command</literal>. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route --all -n my-namespace --overwrite=true "haproxy.router.openshift.io/hsts_header"="max-age=31536000"</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>You can review the HSTS policy you configured. For example:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>To review the <literal>maxAge</literal> set for required HSTS policies, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusteroperator/ingress -n openshift-ingress-operator -o jsonpath='{range .spec.requiredHSTSPolicies[*]}{.spec.requiredHSTSPolicies.maxAgePolicy.largestMaxAge}{"\n"}{end}'</programlisting>
</listitem>
<listitem>
<simpara>To review the HSTS annotations on all routes, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route  --all-namespaces -o go-template='{{range .items}}{{if .metadata.annotations}}{{$a := index .metadata.annotations "haproxy.router.openshift.io/hsts_header"}}{{$n := .metadata.name}}{{with $a}}Name: {{$n}} HSTS: {{$a}}{{"\n"}}{{else}}{{""}}{{end}}{{end}}{{end}}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name: &lt;_routename_&gt; HSTS: max-age=31536000;preload;includeSubDomains</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-throughput-troubleshoot_route-configuration">
<title>Throughput issue troubleshooting methods</title>
<simpara>Sometimes applications deployed by using OpenShift Container Platform can cause network throughput issues, such as unusually high latency between specific services.</simpara>
<simpara>If pod logs do not reveal any cause of the problem, use the following methods to analyze performance issues:</simpara>
<itemizedlist>
<listitem>
<simpara>Use a packet analyzer, such as <literal>ping</literal> or <literal>tcpdump</literal> to analyze traffic between a pod and its node.</simpara>
<simpara>For example, <link xlink:href="https://access.redhat.com/solutions/4569211">run the <literal>tcpdump</literal> tool on each pod</link> while reproducing the behavior that led to the issue. Review the captures on both sides to compare send and receive timestamps to analyze the latency of traffic to and from a pod. Latency can occur in OpenShift Container Platform if a node interface is overloaded with traffic from other pods, storage devices, or the data plane.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tcpdump -s 0 -i any -w /tmp/dump.pcap host &lt;podip 1&gt; &amp;&amp; host &lt;podip 2&gt; <co xml:id="CO213-1"/></programlisting>
<calloutlist>
<callout arearefs="CO213-1">
<para><literal>podip</literal> is the IP address for the pod. Run the <literal>oc get pod &lt;pod_name&gt; -o wide</literal> command to get the IP address of a pod.</para>
</callout>
</calloutlist>
<simpara>The <literal>tcpdump</literal> command generates a file at <literal>/tmp/dump.pcap</literal> containing all traffic between these two pods. You can run the analyzer shortly before the issue is reproduced and stop the analyzer shortly after the issue is finished reproducing to minimize the size of the file. You can also <link xlink:href="https://access.redhat.com/solutions/5074041">run a packet analyzer between the nodes</link> (eliminating the SDN from the equation) with:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tcpdump -s 0 -i any -w /tmp/dump.pcap port 4789</programlisting>
</listitem>
<listitem>
<simpara>Use a bandwidth measuring tool, such as <link xlink:href="https://access.redhat.com/solutions/6129701"><literal>iperf</literal></link>, to measure streaming throughput and UDP throughput. Locate any bottlenecks by running the tool from the pods first, and then running it from the nodes.</simpara>
<itemizedlist>
<listitem>
<simpara>For information on installing and using <literal>iperf</literal>, see this <link xlink:href="https://access.redhat.com/solutions/33103">Red Hat Solution</link>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>In some cases, the cluster may mark the node with the router pod as unhealthy due to latency issues. Use worker latency profiles to adjust the frequency that the cluster waits for a status update from the node before taking action.</simpara>
</listitem>
<listitem>
<simpara>If your cluster has designated lower-latency and higher-latency nodes, configure the <literal>spec.nodePlacement</literal> field in the Ingress Controller to control the placement of the router pod.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-edge-remote-workers-latency">Latency spikes or temporary reduction in throughput to remote workers</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../networking/ingress-operator.xml#nw-ingress-controller-configuration-parameters_configuring-ingress">Ingress Controller configuration
parameters</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-using-cookies-keep-route-statefulness_route-configuration">
<title>Using cookies to keep route statefulness</title>
<simpara>OpenShift Container Platform provides sticky sessions, which enables stateful application
traffic by ensuring all traffic hits the same endpoint. However, if the endpoint
pod terminates, whether through restart, scaling, or a change in configuration,
this statefulness can disappear.</simpara>
<simpara>OpenShift Container Platform can use cookies to configure session persistence. The Ingress
controller selects an endpoint to handle any user requests, and creates a cookie
for the session. The cookie is passed back in the response to the request and
the user sends the cookie back with the next request in the session. The cookie
tells the Ingress Controller which endpoint is handling the session, ensuring
that client requests use the cookie so that they are routed to the same pod.</simpara>
<note>
<simpara>Cookies cannot be set on passthrough routes, because the HTTP traffic cannot be seen. Instead, a number is calculated based on the source IP address, which determines the backend.</simpara>
<simpara>If backends change, the traffic can be directed to the wrong server, making it less sticky. If you are using a load balancer, which hides source IP, the same number is set for all connections and traffic is sent to the same pod.</simpara>
</note>
<section xml:id="nw-annotating-a-route-with-a-cookie-name_route-configuration">
<title>Annotating a route with a cookie</title>
<simpara>You can set a cookie name to overwrite the default, auto-generated one for the route. This allows the application receiving route traffic to know the cookie name. By deleting the cookie it can force the next request to re-choose an endpoint. So, if a server was overloaded it tries to remove the requests from the client and redistribute them.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Annotate the route with the specified cookie name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route &lt;route_name&gt; router.openshift.io/cookie_name="&lt;cookie_name&gt;"</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;route_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the route.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;cookie_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name for the cookie.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example, to annotate the route <literal>my_route</literal> with the cookie name <literal>my_cookie</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route my_route router.openshift.io/cookie_name="my_cookie"</programlisting>
</listitem>
<listitem>
<simpara>Capture the route hostname in a variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ROUTE_NAME=$(oc get route &lt;route_name&gt; -o jsonpath='{.spec.host}')</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;route_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the route.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Save the cookie, and then access the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl $ROUTE_NAME -k -c /tmp/cookie_jar</programlisting>
<simpara>Use the cookie saved by the previous command when connecting to the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl $ROUTE_NAME -k -b /tmp/cookie_jar</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-path-based-routes_route-configuration">
<title>Path-based routes</title>
<simpara>Path-based routes specify a path component that can be compared against a URL, which requires that the traffic for the route be HTTP based. Thus, multiple routes can be served using the same hostname, each with a different path. Routers should match routes based on the most specific path to the least. However, this depends on the router implementation.</simpara>
<simpara>The following table shows example routes and their accessibility:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Route availability</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Route</entry>
<entry align="left" valign="top">When Compared to</entry>
<entry align="left" valign="top">Accessible</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara><emphasis>www.example.com/test</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>www.example.com/test</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis>www.example.com</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara><emphasis>www.example.com/test</emphasis> and <emphasis>www.example.com</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>www.example.com/test</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis>www.example.com</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara><emphasis>www.example.com</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis>www.example.com/text</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Yes (Matched by the host, not the route)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis>www.example.com</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>An unsecured route with a path</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: route-unsecured
spec:
  host: www.example.com
  path: "/test" <co xml:id="CO214-1"/>
  to:
    kind: Service
    name: service-name</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO214-1">
<para>The path is the only added attribute for a path-based route.</para>
</callout>
</calloutlist>
<note>
<simpara>Path-based routing is not available when using passthrough TLS, as the router does not terminate TLS in that case and cannot read the contents of the request.</simpara>
</note>
</section>
<section xml:id="nw-http-header-configuration_route-configuration">
<title>HTTP header configuration</title>
<simpara>OpenShift Container Platform provides different methods for working with HTTP headers. When setting or deleting headers, you can use specific fields in the Ingress Controller or an individual route to modify request and response headers. You can also set certain headers by using route annotations. The various ways of configuring headers can present challenges when working together.</simpara>
<note>
<simpara>You can only set or delete headers within an <literal>IngressController</literal> or <literal>Route</literal> CR, you cannot append them. If an HTTP header is set with a value, that value must be complete and not require appending in the future. In situations where it makes sense to append a header, such as the X-Forwarded-For header, use the <literal>spec.httpHeaders.forwardedHeaderPolicy</literal> field, instead of <literal>spec.httpHeaders.actions</literal>.</simpara>
</note>
<section xml:id="nw-http-header-configuration-order_route-configuration">
<title>Order of precedence</title>
<simpara>When the same HTTP header is modified both in the Ingress Controller and in a route, HAProxy prioritizes the actions in certain ways depending on whether it is a request or response header.</simpara>
<itemizedlist>
<listitem>
<simpara>For HTTP response headers, actions specified in the Ingress Controller are executed after the actions specified in a route. This means that the actions specified in the Ingress Controller take precedence.</simpara>
</listitem>
<listitem>
<simpara>For HTTP request headers, actions specified in a route are executed after the actions specified in the Ingress Controller. This means that the actions specified in the route take precedence.</simpara>
</listitem>
</itemizedlist>
<simpara>For example, a cluster administrator sets the X-Frame-Options response header with the value <literal>DENY</literal> in the Ingress Controller using the following configuration:</simpara>
<formalpara>
<title>Example <literal>IngressController</literal> spec</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
# ...
spec:
  httpHeaders:
    actions:
      response:
      - name: X-Frame-Options
        action:
          type: Set
          set:
            value: DENY</programlisting>
</para>
</formalpara>
<simpara>A route owner sets the same response header that the cluster administrator set in the Ingress Controller, but with the value <literal>SAMEORIGIN</literal> using the following configuration:</simpara>
<formalpara>
<title>Example <literal>Route</literal> spec</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
# ...
spec:
  httpHeaders:
    actions:
      response:
      - name: X-Frame-Options
        action:
          type: Set
          set:
            value: SAMEORIGIN</programlisting>
</para>
</formalpara>
<simpara>When both the <literal>IngressController</literal> spec and <literal>Route</literal> spec are configuring the X-Frame-Options header, then the value set for this header at the global level in the Ingress Controller will take precedence, even if a specific route allows frames.</simpara>
<simpara>This prioritzation occurs because the <literal>haproxy.config</literal> file uses the following logic, where the Ingress Controller is considered the front end and individual routes are considered the back end. The header value <literal>DENY</literal> applied to the front end configurations overrides the same header with the value <literal>SAMEORIGIN</literal> that is set in the back end:</simpara>
<programlisting language="text" linenumbering="unnumbered">frontend public
  http-response set-header X-Frame-Options 'DENY'

frontend fe_sni
  http-response set-header X-Frame-Options 'DENY'

frontend fe_no_sni
  http-response set-header X-Frame-Options 'DENY'

backend be_secure:openshift-monitoring:alertmanager-main
  http-response set-header X-Frame-Options 'SAMEORIGIN'</programlisting>
<simpara>Additionally, any actions defined in either the Ingress Controller or a route override values set using route annotations.</simpara>
</section>
<section xml:id="nw-http-header-configuration-special-cases_route-configuration">
<title>Special case headers</title>
<simpara>The following headers are either prevented entirely from being set or deleted, or allowed under specific circumstances:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Special case header configuration options</title>
<tgroup cols="5">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<colspec colname="col_5" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Header name</entry>
<entry align="left" valign="top">Configurable using <literal>IngressController</literal> spec</entry>
<entry align="left" valign="top">Configurable using <literal>Route</literal> spec</entry>
<entry align="left" valign="top">Reason for disallowment</entry>
<entry align="left" valign="top">Configurable using another method</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>proxy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>proxy</literal> HTTP request header can be used to exploit vulnerable CGI applications by injecting the header value into the <literal>HTTP_PROXY</literal> environment variable. The <literal>proxy</literal> HTTP request header is also non-standard and prone to error during configuration.</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>host</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
<entry align="left" valign="top"><simpara>When the <literal>host</literal> HTTP request header is set using the <literal>IngressController</literal> CR, HAProxy can fail when looking up the correct route.</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>strict-transport-security</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>strict-transport-security</literal> HTTP response header is already handled using route annotations and does not need a separate implementation.</simpara></entry>
<entry align="left" valign="top"><simpara>Yes: the <literal>haproxy.router.openshift.io/hsts_header</literal> route annotation</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cookie</literal> and <literal>set-cookie</literal></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>The cookies that HAProxy sets are used for session tracking to map client connections to particular back-end servers. Allowing these headers to be set could interfere with HAProxy&#8217;s session affinity and restrict HAProxy&#8217;s ownership of a cookie.</simpara></entry>
<entry align="left" valign="top"><simpara>Yes:</simpara>
<itemizedlist>
<listitem>
<simpara>the <literal>haproxy.router.openshift.io/disable_cookie</literal> route annotation</simpara>
</listitem>
<listitem>
<simpara>the <literal>haproxy.router.openshift.io/cookie_name</literal> route annotation</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="nw-route-set-or-delete-http-headers_route-configuration">
<title>Setting or deleting HTTP request and response headers in a route</title>
<simpara>You can set or delete certain HTTP request and response headers for compliance purposes or other reasons. You can set or delete these headers either for all routes served by an Ingress Controller or for specific routes.</simpara>
<simpara>For example, you might want to enable a web application to serve content in alternate locations for specific routes if that content is written in multiple languages, even if there is a default global location specified by the Ingress Controller serving the routes.</simpara>
<simpara>The following procedure creates a route that sets the Content-Location HTTP request header so that the URL associated with the application, <literal>https://app.example.com</literal>, directs to the location <literal>https://app.example.com/lang/en-us</literal>. Directing application traffic to this location means that anyone using that specific route is accessing web content written in American English.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged into an OpenShift Container Platform cluster as a project administrator.</simpara>
</listitem>
<listitem>
<simpara>You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a route definition and save it in a file called <literal>app-example-route.yaml</literal>:</simpara>
<formalpara>
<title>YAML definition of the created route with HTTP header directives</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
# ...
spec:
  host: app.example.com
  tls:
    termination: edge
  to:
    kind: Service
    name: app-example
  httpHeaders:
    actions: <co xml:id="CO215-1"/>
      response: <co xml:id="CO215-2"/>
      - name: Content-Location <co xml:id="CO215-3"/>
        action:
          type: Set <co xml:id="CO215-4"/>
          set:
            value: /lang/en-us <co xml:id="CO215-5"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO215-1">
<para>The list of actions you want to perform on the HTTP headers.</para>
</callout>
<callout arearefs="CO215-2">
<para>The type of header you want to change. In this case, a response header.</para>
</callout>
<callout arearefs="CO215-3">
<para>The name of the header you want to change. For a list of available headers you can set or delete, see <emphasis>HTTP header configuration</emphasis>.</para>
</callout>
<callout arearefs="CO215-4">
<para>The type of action being taken on the header. This field can have the value <literal>Set</literal> or <literal>Delete</literal>.</para>
</callout>
<callout arearefs="CO215-5">
<para>When setting HTTP headers, you must provide a <literal>value</literal>. The value can be a string from a list of available directives for that header, for example <literal>DENY</literal>, or it can be a dynamic value that will be interpreted using HAProxy&#8217;s dynamic value syntax. In this case, the value is set to the relative location of the content.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a route to your existing web application using the newly created route definition:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n app-example create -f app-example-route.yaml</programlisting>
</listitem>
</orderedlist>
<simpara>For HTTP request headers, the actions specified in the route definitions are executed after any actions performed on HTTP request headers in the Ingress Controller. This means that any values set for those request headers in a route will take precedence over the ones set in the Ingress Controller. For more information on the processing order of HTTP headers, see <emphasis>HTTP header configuration</emphasis>.</simpara>
</section>
<section xml:id="nw-route-specific-annotations_route-configuration">
<title>Route-specific annotations</title>
<simpara>The Ingress Controller can set the default options for all the routes it exposes. An individual route can override some of these defaults by providing specific configurations in its annotations. Red Hat does not support adding a route annotation to an operator-managed route.</simpara>
<important>
<simpara>To create a whitelist with multiple source IPs or subnets, use a space-delimited list. Any other delimiter type causes the list to be ignored without a warning or error message.</simpara>
</important>
<table frame="all" rowsep="1" colsep="1">
<title>Route annotations</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Variable</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Environment variable used as default</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/balance</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the load-balancing algorithm. Available options are <literal>random</literal>, <literal>source</literal>, <literal>roundrobin</literal>, and <literal>leastconn</literal>.  The default value is <literal>source</literal> for TLS passthrough routes. For all other routes, the default is <literal>random</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>ROUTER_TCP_BALANCE_SCHEME</literal> for passthrough routes. Otherwise, use <literal>ROUTER_LOAD_BALANCE_ALGORITHM</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/disable_cookies</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disables the use of cookies to track related connections. If set to <literal>'true'</literal> or <literal>'TRUE'</literal>, the balance algorithm is used to choose which back-end serves connections for each incoming HTTP request.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>router.openshift.io/cookie_name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies an optional cookie to use for
this route. The name must consist of any combination of upper and lower case letters, digits, "_",
and "-". The default is the hashed internal key name for the route.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/pod-concurrent-connections</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the maximum number of connections that are allowed to a backing pod from a router.<?asciidoc-br?>
Note: If there are multiple pods, each can have this many connections.  If you have multiple routers, there is no coordination among them, each may connect this many times. If not set, or set to 0, there is no limit.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/rate-limit-connections</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Setting <literal>'true'</literal> or <literal>'TRUE'</literal> enables rate limiting functionality which is implemented through stick-tables on the specific backend per route.<?asciidoc-br?>
Note: Using this annotation provides basic protection against denial-of-service attacks.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/rate-limit-connections.concurrent-tcp</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Limits the number of concurrent TCP connections made through the same source IP address. It accepts a numeric value.<?asciidoc-br?>
Note: Using this annotation provides basic protection against denial-of-service attacks.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/rate-limit-connections.rate-http</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Limits the rate at which a client with the same source IP address can make HTTP requests. It accepts a numeric value. <?asciidoc-br?>
Note: Using this annotation provides basic protection against denial-of-service attacks.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/rate-limit-connections.rate-tcp</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Limits the rate at which a client with the same source IP address can make TCP connections. It accepts a numeric value. <?asciidoc-br?>
Note: Using this annotation provides basic protection against denial-of-service attacks.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets a server-side timeout for the route. (TimeUnits)</simpara></entry>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_SERVER_TIMEOUT</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/timeout-tunnel</literal></simpara></entry>
<entry align="left" valign="top"><simpara>This timeout applies to a tunnel connection, for example, WebSocket over cleartext, edge, reencrypt, or passthrough routes. With cleartext, edge, or reencrypt route types, this annotation is applied as a timeout tunnel with the existing timeout value. For the passthrough route types, the annotation takes precedence over any existing timeout value set.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_TUNNEL_TIMEOUT</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ingresses.config/cluster ingress.operator.openshift.io/hard-stop-after</literal></simpara></entry>
<entry align="left" valign="top"><simpara>You can set either an IngressController or the ingress config . This annotation redeploys the router and configures the HA proxy to emit the haproxy <literal>hard-stop-after</literal> global option, which defines the maximum time allowed to perform a clean soft-stop.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>ROUTER_HARD_STOP_AFTER</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>router.openshift.io/haproxy.health.check.interval</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the interval for the back-end health checks. (TimeUnits)</simpara></entry>
<entry align="left" valign="top"><simpara><literal>ROUTER_BACKEND_CHECK_INTERVAL</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/ip_whitelist</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets an allowlist for the route. The allowlist is a space-separated list of IP addresses and CIDR ranges for the approved source addresses. Requests from IP addresses that are not in the allowlist are dropped.</simpara><simpara>The maximum number of IP addresses and CIDR ranges directly visible in the <literal>haproxy.config</literal> file is 61. [<superscript>1</superscript>]</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/hsts_header</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets a Strict-Transport-Security header for the edge terminated or re-encrypt route.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/log-send-hostname</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the <literal>hostname</literal> field in the Syslog header. Uses the hostname of the system. <literal>log-send-hostname</literal> is enabled by default if any Ingress API logging method, such as sidecar or Syslog facility, is enabled for the router.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/rewrite-target</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the rewrite path of the request on the backend.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>router.openshift.io/cookie-same-site</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets a value to restrict cookies. The values are:</simpara><simpara><literal>Lax</literal>: cookies are transferred between the visited site and third-party sites.</simpara><simpara><literal>Strict</literal>: cookies are restricted to the visited site.</simpara><simpara><literal>None</literal>: cookies are restricted to the visited site.</simpara><simpara>This value is applicable to re-encrypt and edge routes only. For more information, see the <link xlink:href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite">SameSite cookies documentation</link>.</simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>haproxy.router.openshift.io/set-forwarded-headers</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the policy for handling the <literal>Forwarded</literal> and <literal>X-Forwarded-For</literal> HTTP headers per route. The values are:</simpara><simpara><literal>append</literal>: appends the header, preserving any existing header. This is the default value.</simpara><simpara><literal>replace</literal>: sets the header, removing any existing header.</simpara><simpara><literal>never</literal>: never sets the header, but preserves any existing header.</simpara><simpara><literal>if-none</literal>: sets the header if it is not already set.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>ROUTER_SET_FORWARDED_HEADERS</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>If the number of IP addresses and CIDR ranges in an allowlist exceeds 61, they are written into a separate file that is then referenced from <literal>haproxy.config</literal>. This file is stored in the <literal>var/lib/haproxy/router/whitelists</literal> folder.</simpara>
<note>
<simpara>To ensure that the addresses are written to the allowlist, check that the full list of CIDR ranges are listed in the Ingress Controller configuration file. The etcd object size limit restricts how large a route annotation can be. Because of this, it creates a threshold for the maximum number of IP addresses and CIDR ranges that you can include in an allowlist.</simpara>
</note>
</listitem>
</orderedlist>
</para>
<note>
<simpara>Environment variables cannot be edited.</simpara>
</note>
<formalpara>
<title>Router timeout variables</title>
<para><literal>TimeUnits</literal> are represented by a number followed by the unit: <literal>us</literal> *(microseconds), <literal>ms</literal> (milliseconds, default), <literal>s</literal> (seconds), <literal>m</literal> (minutes), <literal>h</literal> *(hours), <literal>d</literal> (days).</para>
</formalpara>
<simpara>The regular expression is: [1-9][0-9]*(<literal>us</literal>\|<literal>ms</literal>\|<literal>s</literal>\|<literal>m</literal>\|<literal>h</literal>\|<literal>d</literal>).</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="40*"/>
<thead>
<row>
<entry align="left" valign="top">Variable</entry>
<entry align="left" valign="top">Default</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_BACKEND_CHECK_INTERVAL</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>5000ms</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Length of time between subsequent liveness checks on back ends.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_CLIENT_FIN_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>1s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Controls the TCP FIN timeout period for the client connecting to the route. If the FIN sent to close the connection does not answer within the given time, HAProxy closes the connection. This is harmless if set to a low value and uses fewer resources on the router.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_CLIENT_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>30s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Length of time that a client has to acknowledge or send data.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_CONNECT_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>5s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum connection time.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_SERVER_FIN_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>1s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Controls the TCP FIN timeout from the router to the pod backing the route.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_SERVER_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>30s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Length of time that a server has to acknowledge or send data.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_DEFAULT_TUNNEL_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>1h</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Length of time for TCP or WebSocket connections to remain open. This timeout period resets whenever HAProxy reloads.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_SLOWLORIS_HTTP_KEEPALIVE</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>300s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set the maximum time to wait for a new HTTP request to appear. If this is set too low, it can cause problems with browsers and applications not expecting a small <literal>keepalive</literal> value.</simpara>
<simpara>Some effective timeout values can be the sum of certain variables, rather than the specific expected timeout. For example, <literal>ROUTER_SLOWLORIS_HTTP_KEEPALIVE</literal> adjusts <literal>timeout http-keep-alive</literal>. It is set to <literal>300s</literal> by default, but HAProxy also waits on <literal>tcp-request inspect-delay</literal>, which is set to <literal>5s</literal>. In this case, the overall timeout would be <literal>300s</literal> plus <literal>5s</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_SLOWLORIS_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>10s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Length of time the transmission of an HTTP request can take.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>RELOAD_INTERVAL</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>5s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allows the minimum frequency for the router to reload and accept new changes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ROUTER_METRICS_HAPROXY_TIMEOUT</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>5s</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Timeout for the gathering of HAProxy metrics.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<formalpara>
<title>A route setting custom timeout</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/timeout: 5500ms <co xml:id="CO216-1"/>
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO216-1">
<para>Specifies the new timeout with HAProxy supported units (<literal>us</literal>, <literal>ms</literal>, <literal>s</literal>, <literal>m</literal>, <literal>h</literal>, <literal>d</literal>). If the unit is not provided, <literal>ms</literal> is the default.</para>
</callout>
</calloutlist>
<note>
<simpara>Setting a server-side timeout value for passthrough routes too low can cause
WebSocket connections to timeout frequently on that route.</simpara>
</note>
<formalpara>
<title>A route that allows only one specific IP address</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10</programlisting>
</para>
</formalpara>
<formalpara>
<title>A route that allows several IP addresses</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.10 192.168.1.11 192.168.1.12</programlisting>
</para>
</formalpara>
<formalpara>
<title>A route that allows an IP address CIDR network</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 192.168.1.0/24</programlisting>
</para>
</formalpara>
<formalpara>
<title>A route that allows both IP an address and IP address CIDR networks</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  annotations:
    haproxy.router.openshift.io/ip_whitelist: 180.5.61.153 192.168.1.0/24 10.0.0.0/8</programlisting>
</para>
</formalpara>
<formalpara>
<title>A route specifying a rewrite target</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  annotations:
    haproxy.router.openshift.io/rewrite-target: / <co xml:id="CO217-1"/>
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO217-1">
<para>Sets <literal>/</literal> as rewrite path of the request on the backend.</para>
</callout>
</calloutlist>
<simpara>Setting the <literal>haproxy.router.openshift.io/rewrite-target</literal> annotation on a route specifies that the Ingress Controller should rewrite paths in HTTP requests using this route before forwarding the requests to the backend application.
The part of the request path that matches the path specified in <literal>spec.path</literal> is replaced with the rewrite target specified in the annotation.</simpara>
<simpara>The following table provides examples of the path rewriting behavior for various combinations of <literal>spec.path</literal>, request path, and rewrite target.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>rewrite-target examples:</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Route.spec.path</entry>
<entry align="left" valign="top">Request path</entry>
<entry align="left" valign="top">Rewrite target</entry>
<entry align="left" valign="top">Forwarded request path</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/bar</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>/bar</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/bar/</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>/bar/</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/bar</simpara></entry>
<entry align="left" valign="top"><simpara>/bar</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/</simpara></entry>
<entry align="left" valign="top"><simpara>/bar</simpara></entry>
<entry align="left" valign="top"><simpara>/bar/</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/bar</simpara></entry>
<entry align="left" valign="top"><simpara>/baz</simpara></entry>
<entry align="left" valign="top"><simpara>/baz/bar</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/bar/</simpara></entry>
<entry align="left" valign="top"><simpara>/baz</simpara></entry>
<entry align="left" valign="top"><simpara>/baz/bar/</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo/</simpara></entry>
<entry align="left" valign="top"><simpara>/foo</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>N/A (request path does not match route path)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo/</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>/foo/</simpara></entry>
<entry align="left" valign="top"><simpara>/foo/bar</simpara></entry>
<entry align="left" valign="top"><simpara>/</simpara></entry>
<entry align="left" valign="top"><simpara>/bar</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-route-admission-policy_route-configuration">
<title>Configuring the route admission policy</title>
<simpara>Administrators and application developers can run applications in multiple namespaces with the same domain name. This is for organizations where multiple teams develop microservices that are exposed on the same hostname.</simpara>
<warning>
<simpara>Allowing claims across namespaces should only be enabled for clusters with trust between namespaces, otherwise a malicious user could take over a hostname. For this reason, the default admission policy disallows hostname claims across namespaces.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>.spec.routeAdmission</literal> field of the <literal>ingresscontroller</literal> resource variable using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default --patch '{"spec":{"routeAdmission":{"namespaceOwnership":"InterNamespaceAllowed"}}}' --type=merge</programlisting>
<formalpara>
<title>Sample Ingress Controller configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed
...</programlisting>
</para>
</formalpara>
<tip>
<simpara>You can alternatively apply the following YAML to configure the route admission policy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: default
  namespace: openshift-ingress-operator
spec:
  routeAdmission:
    namespaceOwnership: InterNamespaceAllowed</programlisting>
</tip>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-creating-a-route-via-an-ingress_route-configuration">
<title>Creating a route through an Ingress object</title>
<simpara>Some ecosystem components have an integration with Ingress resources but not with route resources. To cover this case, OpenShift Container Platform automatically creates managed route objects when an Ingress object is created. These route objects are deleted when the corresponding Ingress objects are deleted.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Define an Ingress object in the OpenShift Container Platform console or by entering the <literal>oc create</literal> command:</simpara>
<formalpara>
<title>YAML Definition of an Ingress</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: "reencrypt" <co xml:id="CO218-1"/>
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert <co xml:id="CO218-2"/>
spec:
  rules:
  - host: www.example.com <co xml:id="CO218-3"/>
    http:
      paths:
      - backend:
          service:
            name: frontend
            port:
              number: 443
        path: /
        pathType: Prefix
  tls:
  - hosts:
    - www.example.com
    secretName: example-com-tls-certificate</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO218-1">
<para>The <literal>route.openshift.io/termination</literal> annotation can be used to configure the <literal>spec.tls.termination</literal> field of the <literal>Route</literal> as <literal>Ingress</literal> has  no field for this. The accepted values are <literal>edge</literal>, <literal>passthrough</literal> and <literal>reencrypt</literal>. All other values are silently ignored. When  the annotation value is unset, <literal>edge</literal> is the default route. The TLS certificate details must be defined in the template file to implement the default edge route.</para>
</callout>
<callout arearefs="CO218-3">
<para>When working with an <literal>Ingress</literal> object, you must specify an explicit hostname, unlike when working with routes. You can use the <literal>&lt;host_name&gt;.&lt;cluster_ingress_domain&gt;</literal> syntax, for example <literal>apps.openshiftdemos.com</literal>, to take advantage of the <literal>*.&lt;cluster_ingress_domain&gt;</literal> wildcard DNS record and serving certificate for the cluster. Otherwise, you must ensure that there is a DNS record for the chosen hostname.</para>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>If you specify the <literal>passthrough</literal> value in the <literal>route.openshift.io/termination</literal> annotation, set <literal>path</literal> to <literal>''</literal> and <literal>pathType</literal> to <literal>ImplementationSpecific</literal> in the spec:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  spec:
    rules:
    - host: www.example.com
      http:
        paths:
        - path: ''
          pathType: ImplementationSpecific
          backend:
            service:
              name: frontend
              port:
                number: 443</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ingress.yaml</programlisting>
</listitem>
</orderedlist>
</callout>
<callout arearefs="CO218-2">
<para>The <literal>route.openshift.io/destination-ca-certificate-secret</literal> can be used on an Ingress object to define a route with a custom destination certificate (CA). The annotation references a kubernetes secret, <literal>secret-ca-cert</literal> that will be inserted into the generated route.</para>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To specify a route object with a destination CA from an ingress object, you must create a <literal>kubernetes.io/tls</literal> or <literal>Opaque</literal> type secret with a certificate in PEM-encoded format in the <literal>data.tls.crt</literal> specifier of the secret.</simpara>
</listitem>
</orderedlist>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>List your routes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes</programlisting>
<simpara>The result includes an autogenerated route whose name starts with <literal>frontend-</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME             HOST/PORT         PATH    SERVICES    PORT    TERMINATION          WILDCARD
frontend-gnztq   www.example.com           frontend    443     reencrypt/Redirect   None</programlisting>
<simpara>If you inspect this route, it looks this:</simpara>
<formalpara>
<title>YAML Definition of an autogenerated route</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend-gnztq
  ownerReferences:
  - apiVersion: networking.k8s.io/v1
    controller: true
    kind: Ingress
    name: frontend
    uid: 4e6c59cc-704d-4f44-b390-617d879033b6
spec:
  host: www.example.com
  path: /
  port:
    targetPort: https
  tls:
    certificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    insecureEdgeTerminationPolicy: Redirect
    key: |
      -----BEGIN RSA PRIVATE KEY-----
      [...]
      -----END RSA PRIVATE KEY-----
    termination: reencrypt
    destinationCACertificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
  to:
    kind: Service
    name: frontend</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-edge-route-with-default-certificate_route-configuration">
<title>Creating a route using the default certificate through an Ingress object</title>
<simpara>If you create an Ingress object without specifying any TLS configuration, OpenShift Container Platform generates an insecure route. To create an Ingress object that generates a secure, edge-terminated route using the default ingress certificate, you can specify an empty TLS configuration as follows.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have a service that you want to expose.</simpara>
</listitem>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the Ingress object.  In this example, the file is called <literal>example-ingress.yaml</literal>:</simpara>
<formalpara>
<title>YAML definition of an Ingress object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  ...
spec:
  rules:
    ...
  tls:
  - {} <co xml:id="CO219-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO219-1">
<para>Use this exact syntax to specify TLS without specifying a custom certificate.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the Ingress object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f example-ingress.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that OpenShift Container Platform has created the expected route for the Ingress object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
items:
- apiVersion: route.openshift.io/v1
  kind: Route
  metadata:
    name: frontend-j9sdd <co xml:id="CO220-1"/>
    ...
  spec:
  ...
    tls: <co xml:id="CO220-2"/>
      insecureEdgeTerminationPolicy: Redirect
      termination: edge <co xml:id="CO220-3"/>
  ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO220-1">
<para>The name of the route includes the name of the Ingress object followed by a random suffix.</para>
</callout>
<callout arearefs="CO220-2">
<para>In order to use the default certificate, the route should not specify <literal>spec.certificate</literal>.</para>
</callout>
<callout arearefs="CO220-3">
<para>The route should specify the <literal>edge</literal> termination policy.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-re-encrypt-route-with-custom-certificate_route-configuration">
<title>Creating a route using the destination CA certificate in the Ingress annotation</title>
<simpara>The <literal>route.openshift.io/destination-ca-certificate-secret</literal> annotation can be used on an Ingress object to define a route with a custom destination CA certificate.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You may have a certificate/key pair in PEM-encoded files, where the certificate is valid for the route host.</simpara>
</listitem>
<listitem>
<simpara>You may have a separate CA certificate in a PEM-encoded file that completes the certificate chain.</simpara>
</listitem>
<listitem>
<simpara>You must have a separate destination CA certificate in a PEM-encoded file.</simpara>
</listitem>
<listitem>
<simpara>You must have a service that you want to expose.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the <literal>route.openshift.io/destination-ca-certificate-secret</literal> to the Ingress annotations:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: "reencrypt"
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert <co xml:id="CO221-1"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO221-1">
<para>The annotation references a kubernetes secret.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>The secret referenced in this annotation will be inserted into the generated route.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
  annotations:
    route.openshift.io/termination: reencrypt
    route.openshift.io/destination-ca-certificate-secret: secret-ca-cert
spec:
...
  tls:
    insecureEdgeTerminationPolicy: Redirect
    termination: reencrypt
    destinationCACertificate: |
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-router-configuring-dual-stack_route-configuration">
<title>Configuring the OpenShift Container Platform Ingress Controller for dual-stack networking</title>
<simpara>If your OpenShift Container Platform cluster is configured for IPv4 and IPv6 dual-stack networking, your cluster is externally reachable by OpenShift Container Platform routes.</simpara>
<simpara>The Ingress Controller automatically serves services that have both IPv4 and IPv6 endpoints, but you can configure the Ingress Controller for single-stack or dual-stack services.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You deployed an OpenShift Container Platform cluster on bare metal.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To have the Ingress Controller serve traffic over IPv4/IPv6 to a workload, you can create a service YAML file or modify an existing service YAML file by setting the <literal>ipFamilies</literal> and <literal>ipFamilyPolicy</literal> fields. For example:</simpara>
<formalpara>
<title>Sample service YAML file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  creationTimestamp: yyyy-mm-ddT00:00:00Z
  labels:
    name: &lt;service_name&gt;
    manager: kubectl-create
    operation: Update
    time: yyyy-mm-ddT00:00:00Z
  name: &lt;service_name&gt;
  namespace: &lt;namespace_name&gt;
  resourceVersion: "&lt;resource_version_number&gt;"
  selfLink: "/api/v1/namespaces/&lt;namespace_name&gt;/services/&lt;service_name&gt;"
  uid: &lt;uid_number&gt;
spec:
  clusterIP: 172.30.0.0/16
  clusterIPs: <co xml:id="CO222-1"/>
  - 172.30.0.0/16
  - &lt;second_IP_address&gt;
  ipFamilies: <co xml:id="CO222-2"/>
  - IPv4
  - IPv6
  ipFamilyPolicy: RequireDualStack <co xml:id="CO222-3"/>
  ports:
  - port: 8080
    protocol: TCP
    targetport: 8080
  selector:
    name: &lt;namespace_name&gt;
  sessionAffinity: None
  type: ClusterIP
status:
  loadbalancer: {}</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO222-1">
<para>In a dual-stack instance, there are two different <literal>clusterIPs</literal> provided.</para>
</callout>
<callout arearefs="CO222-2">
<para>For a single-stack instance, enter <literal>IPv4</literal> or <literal>IPv6</literal>. For a dual-stack instance, enter both <literal>IPv4</literal> and <literal>IPv6</literal>.</para>
</callout>
<callout arearefs="CO222-3">
<para>For a single-stack instance, enter <literal>SingleStack</literal>. For a dual-stack instance, enter <literal>RequireDualStack</literal>.</para>
</callout>
</calloutlist>
<simpara>These resources generate corresponding <literal>endpoints</literal>. The Ingress Controller now watches <literal>endpointslices</literal>.</simpara>
</listitem>
<listitem>
<simpara>To view <literal>endpoints</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get endpoints</programlisting>
</listitem>
<listitem>
<simpara>To view <literal>endpointslices</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get endpointslices</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="configuring-ingress">Specifying an alternative cluster domain using the appsDomain option</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-default-certificate">
<title>Secured routes</title>

<simpara>Secure routes provide the ability to use several types of TLS termination to serve certificates to the client. The following sections describe how to create re-encrypt, edge, and passthrough routes with custom certificates.</simpara>
<important>
<simpara>If you create routes in Microsoft Azure through public endpoints, the resource
names are subject to restriction. You cannot create resources that use certain
terms. For a list of terms that Azure restricts, see
<link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-manager-reserved-resource-name">Resolve reserved resource name errors</link>
in the Azure documentation.</simpara>
</important>
<section xml:id="nw-ingress-creating-a-reencrypt-route-with-a-custom-certificate_secured-routes">
<title>Creating a re-encrypt route with a custom certificate</title>
<simpara>You can configure a secure route using reencrypt TLS termination with a custom
certificate by using the <literal>oc create route</literal> command.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a certificate/key pair in PEM-encoded files, where the certificate
is valid for the route host.</simpara>
</listitem>
<listitem>
<simpara>You may have a separate CA certificate in a PEM-encoded file that completes
the certificate chain.</simpara>
</listitem>
<listitem>
<simpara>You must have a separate destination CA certificate in a PEM-encoded file.</simpara>
</listitem>
<listitem>
<simpara>You must have a service that you want to expose.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Password protected key files are not supported. To remove a passphrase from a
key file, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl rsa -in password_protected_tls.key -out tls.key</programlisting>
</note>
<formalpara>
<title>Procedure</title>
<para>This procedure creates a <literal>Route</literal> resource with a custom certificate and
reencrypt TLS termination. The following assumes that the certificate/key pair
are in the <literal>tls.crt</literal> and <literal>tls.key</literal> files in the current working directory. You
must also specify a destination CA certificate to enable the Ingress Controller
to trust the service&#8217;s certificate. You may also specify a CA certificate if
needed to complete the certificate chain. Substitute the actual path names for
<literal>tls.crt</literal>, <literal>tls.key</literal>, <literal>cacert.crt</literal>, and (optionally) <literal>ca.crt</literal>. Substitute the
name of the <literal>Service</literal> resource that you want to expose for <literal>frontend</literal>.
Substitute the appropriate hostname for <literal>www.example.com</literal>.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Create a secure <literal>Route</literal> resource using reencrypt TLS termination and a custom
certificate:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route reencrypt --service=frontend --cert=tls.crt --key=tls.key --dest-ca-cert=destca.crt --ca-cert=ca.crt --hostname=www.example.com</programlisting>
<simpara>If you examine the resulting <literal>Route</literal> resource, it should look similar to the
following:</simpara>
<formalpara>
<title>YAML Definition of the Secure Route</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
spec:
  host: www.example.com
  to:
    kind: Service
    name: frontend
  tls:
    termination: reencrypt
    key: |-
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    caCertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    destinationCACertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----</programlisting>
</para>
</formalpara>
<simpara>See <literal>oc create route reencrypt --help</literal> for more options.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-creating-an-edge-route-with-a-custom-certificate_secured-routes">
<title>Creating an edge route with a custom certificate</title>
<simpara>You can configure a secure route using edge TLS termination with a custom
certificate by using the <literal>oc create route</literal> command. With an edge route, the
Ingress Controller terminates TLS encryption before forwarding traffic to the
destination pod. The route specifies the TLS certificate and key that the
Ingress Controller uses for the route.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a certificate/key pair in PEM-encoded files, where the certificate
is valid for the route host.</simpara>
</listitem>
<listitem>
<simpara>You may have a separate CA certificate in a PEM-encoded file that completes
the certificate chain.</simpara>
</listitem>
<listitem>
<simpara>You must have a service that you want to expose.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Password protected key files are not supported. To remove a passphrase from a
key file, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openssl rsa -in password_protected_tls.key -out tls.key</programlisting>
</note>
<formalpara>
<title>Procedure</title>
<para>This procedure creates a <literal>Route</literal> resource with a custom certificate and edge TLS
termination. The following assumes that the certificate/key pair are in the
<literal>tls.crt</literal> and <literal>tls.key</literal> files in the current working directory. You may also
specify a CA certificate if needed to complete the certificate chain.
Substitute the actual path names for <literal>tls.crt</literal>, <literal>tls.key</literal>, and (optionally)
<literal>ca.crt</literal>. Substitute the name of the service that you want to expose
for <literal>frontend</literal>. Substitute the appropriate hostname for <literal>www.example.com</literal>.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Create a secure <literal>Route</literal> resource using edge TLS termination and a custom certificate.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route edge --service=frontend --cert=tls.crt --key=tls.key --ca-cert=ca.crt --hostname=www.example.com</programlisting>
<simpara>If you examine the resulting <literal>Route</literal> resource, it should look similar to the
following:</simpara>
<formalpara>
<title>YAML Definition of the Secure Route</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: frontend
spec:
  host: www.example.com
  to:
    kind: Service
    name: frontend
  tls:
    termination: edge
    key: |-
      -----BEGIN PRIVATE KEY-----
      [...]
      -----END PRIVATE KEY-----
    certificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----
    caCertificate: |-
      -----BEGIN CERTIFICATE-----
      [...]
      -----END CERTIFICATE-----</programlisting>
</para>
</formalpara>
<simpara>See <literal>oc create route edge --help</literal> for more options.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ingress-creating-a-passthrough-route_secured-routes">
<title>Creating a passthrough route</title>
<simpara>You can configure a secure route using passthrough termination by using the <literal>oc create route</literal> command. With passthrough termination, encrypted traffic is sent straight to the destination without the router providing TLS termination. Therefore no key or certificate is required on the route.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a service that you want to expose.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Route</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create route passthrough route-passthrough-secured --service=frontend --port=8080</programlisting>
<simpara>If you examine the resulting <literal>Route</literal> resource, it should look similar to the following:</simpara>
<formalpara>
<title>A Secured Route Using Passthrough Termination</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  name: route-passthrough-secured <co xml:id="CO223-1"/>
spec:
  host: www.example.com
  port:
    targetPort: 8080
  tls:
    termination: passthrough <co xml:id="CO223-2"/>
    insecureEdgeTerminationPolicy: None <co xml:id="CO223-3"/>
  to:
    kind: Service
    name: frontend</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO223-1">
<para>The name of the object, which is limited to 63 characters.</para>
</callout>
<callout arearefs="CO223-2">
<para>The <literal><emphasis role="strong">termination</emphasis></literal> field is set to <literal>passthrough</literal>. This is the only required <literal>tls</literal> field.</para>
</callout>
<callout arearefs="CO223-3">
<para>Optional <literal>insecureEdgeTerminationPolicy</literal>. The only valid values are <literal>None</literal>, <literal>Redirect</literal>, or empty for disabled.</para>
</callout>
</calloutlist>
<simpara>The destination pod is responsible for serving certificates for the
traffic at the endpoint. This is currently the only method that can support requiring client certificates, also known as two-way authentication.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_configuring-ingress-cluster-traffic">
<title>Configuring ingress cluster traffic</title>
<section xml:id="overview-traffic">
<title>Configuring ingress cluster traffic overview</title>

<simpara>OpenShift Container Platform provides the following methods for communicating from
outside the cluster with services running in the cluster.</simpara>
<simpara>The methods are recommended, in order or preference:</simpara>
<itemizedlist>
<listitem>
<simpara>If you have HTTP/HTTPS, use an Ingress Controller.</simpara>
</listitem>
<listitem>
<simpara>If you have a TLS-encrypted protocol other than HTTPS. For example, for TLS
with the SNI header, use an Ingress Controller.</simpara>
</listitem>
<listitem>
<simpara>Otherwise, use a Load Balancer, an External IP, or a <literal>NodePort</literal>.</simpara>
</listitem>
</itemizedlist>
<informaltable xml:id="external-access-options-table" frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Method</entry>
<entry align="left" valign="top">Purpose</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><link linkend="configuring-ingress-cluster-traffic-ingress-controller">Use an Ingress Controller</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows access to HTTP/HTTPS traffic and TLS-encrypted protocols other than HTTPS (for example, TLS with the SNI header).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="configuring-ingress-cluster-traffic-load-balancer">Automatically assign an external IP using a load balancer service</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows traffic to non-standard ports through an IP address assigned from a pool.
Most cloud platforms offer a method to start a service with a load-balancer IP address.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="about-metallb">About MetalLB and the MetalLB Operator</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows traffic to a specific IP address or address from a pool on the machine network.
For bare-metal installations or platforms that are like bare metal, MetalLB provides a way to start a service with a load-balancer IP address.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="configuring-ingress-cluster-traffic-service-external-ip">Manually assign an external IP to a service</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows traffic to non-standard ports through a specific IP address.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="configuring-ingress-cluster-traffic-nodeport">Configure a <literal>NodePort</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Expose a service on all nodes in the cluster.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<section xml:id="overview-traffic-comparision_overview-traffic">
<title>Comparision: Fault tolerant access to external IP addresses</title>
<simpara>For the communication methods that provide access to an external IP address, fault tolerant access to the IP address is another consideration.
The following features provide fault tolerant access to an external IP address.</simpara>
<variablelist>
<varlistentry>
<term>IP failover</term>
<listitem>
<simpara>IP failover manages a pool of virtual IP address for a set of nodes.
It is implemented with Keepalived and Virtual Router Redundancy Protocol (VRRP).
IP failover is a layer 2 mechanism only and relies on multicast.
Multicast can have disadvantages for some networks.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>MetalLB</term>
<listitem>
<simpara>MetalLB has a layer 2 mode, but it does not use multicast.
Layer 2 mode has a disadvantage that it transfers all traffic for an external IP address through one node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Manually assigning external IP addresses</term>
<listitem>
<simpara>You can configure your cluster with an IP address block that is used to assign external IP addresses to services.
By default, this feature is disabled.
This feature is flexible, but places the largest burden on the cluster or network administrator.
The cluster is prepared to receive traffic that is destined for the external IP, but each customer has to decide how they want to route traffic to nodes.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="configuring-externalip">
<title>Configuring ExternalIPs for services</title>

<simpara>As a cluster administrator, you can designate an IP address block that is external to the cluster that can send traffic to services in the cluster.</simpara>
<simpara>This functionality is generally most useful for clusters installed on bare-metal hardware.</simpara>
<section xml:id="_prerequisites-2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your network infrastructure must route traffic for the external IP addresses to your cluster.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-externalip-about_configuring-externalip">
<title>About ExternalIP</title>
<simpara>For non-cloud environments, OpenShift Container Platform supports the assignment of external IP addresses to a <literal>Service</literal> object <literal>spec.externalIPs[]</literal> field through the <emphasis role="strong">ExternalIP</emphasis> facility.
By setting this field, OpenShift Container Platform assigns an additional virtual IP address to the service. The IP address can be outside the service network defined for the cluster.
A service configured with an ExternalIP functions similarly to a service with <literal>type=NodePort</literal>, allowing you to direct traffic to a local node for load balancing.</simpara>
<simpara>You must configure your networking infrastructure to ensure that the external IP address blocks that you define are routed to the cluster.</simpara>
<simpara>OpenShift Container Platform extends the ExternalIP functionality in Kubernetes by adding the following capabilities:</simpara>
<itemizedlist>
<listitem>
<simpara>Restrictions on the use of external IP addresses by users through a configurable policy</simpara>
</listitem>
<listitem>
<simpara>Allocation of an external IP address automatically to a service upon request</simpara>
</listitem>
</itemizedlist>
<warning>
<simpara>Disabled by default, use of ExternalIP functionality can be a security risk, because in-cluster traffic to an external IP address is directed to that service.
This could allow cluster users to intercept sensitive traffic destined for external resources.</simpara>
</warning>
<important>
<simpara>This feature is supported only in non-cloud deployments.
For cloud deployments, use the load balancer services for automatic deployment of a cloud load balancer to target the endpoints of a service.</simpara>
</important>
<simpara>You can assign an external IP address in the following ways:</simpara>
<variablelist>
<varlistentry>
<term>Automatic assignment of an external IP</term>
<listitem>
<simpara>OpenShift Container Platform automatically assigns an IP address from the <literal>autoAssignCIDRs</literal> CIDR block to the <literal>spec.externalIPs[]</literal> array when you create a <literal>Service</literal> object with <literal>spec.type=LoadBalancer</literal> set.
In this case, OpenShift Container Platform implements a non-cloud version of the load balancer service type and assigns IP addresses to the services.
Automatic assignment is disabled by default and must be configured by a cluster administrator as described in the following section.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Manual assignment of an external IP</term>
<listitem>
<simpara>OpenShift Container Platform uses the IP addresses assigned to the <literal>spec.externalIPs[]</literal> array when you create a <literal>Service</literal> object. You cannot specify an IP address that is already in use by another service.</simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="configuration-externalip_configuring-externalip">
<title>Configuration for ExternalIP</title>
<simpara>Use of an external IP address in OpenShift Container Platform is governed by the following fields in the <literal>Network.config.openshift.io</literal> CR named <literal>cluster</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>spec.externalIP.autoAssignCIDRs</literal> defines an IP address block used by the load balancer when choosing an external IP address for the service. OpenShift Container Platform supports only a single IP address block for automatic assignment. This can be simpler than having to manage the port space of a limited number of shared IP addresses when manually assigning ExternalIPs to services. If automatic assignment is enabled, a <literal>Service</literal> object with <literal>spec.type=LoadBalancer</literal> is allocated an external IP address.</simpara>
</listitem>
<listitem>
<simpara><literal>spec.externalIP.policy</literal> defines the permissible IP address blocks when manually specifying an IP address. OpenShift Container Platform does not apply policy rules to IP address blocks defined by <literal>spec.externalIP.autoAssignCIDRs</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>If routed correctly, external traffic from the configured external IP address block can reach service endpoints through any TCP or UDP port that the service exposes.</simpara>
<important>
<simpara>As a cluster administrator, you must configure routing to externalIPs on both OpenShiftSDN and OVN-Kubernetes network types. You must also ensure that the IP address block you assign terminates at one or more nodes in your cluster. For more information, see <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/service/#external-ips"><emphasis role="strong">Kubernetes External IPs</emphasis></link>.</simpara>
</important>
<simpara>OpenShift Container Platform supports both the automatic and manual assignment of IP
addresses, and each address is guaranteed to be assigned to a maximum of one
service. This ensures that each service can expose its chosen ports regardless
of the ports exposed by other services.</simpara>
<note>
<simpara>To use IP address blocks defined by <literal>autoAssignCIDRs</literal> in OpenShift Container Platform, you must configure the necessary IP address assignment and routing for your host network.</simpara>
</note>
<simpara>The following YAML describes a service with an external IP address configured:</simpara>
<formalpara>
<title>Example <literal>Service</literal> object with <literal>spec.externalIPs[]</literal> set</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: http-service
spec:
  clusterIP: 172.30.163.110
  externalIPs:
  - 192.168.132.253
  externalTrafficPolicy: Cluster
  ports:
  - name: highport
    nodePort: 31903
    port: 30102
    protocol: TCP
    targetPort: 30102
  selector:
    app: web
  sessionAffinity: None
  type: LoadBalancer
status:
  loadBalancer:
    ingress:
    - ip: 192.168.132.253</programlisting>
</para>
</formalpara>
</section>
<section xml:id="restrictions-on-ip-assignment_configuring-externalip">
<title>Restrictions on the assignment of an external IP address</title>
<simpara>As a cluster administrator, you can specify IP address blocks to allow and to reject.</simpara>
<simpara>Restrictions apply only to users without <literal>cluster-admin</literal> privileges. A cluster administrator can always set the service <literal>spec.externalIPs[]</literal> field to any IP address.</simpara>
<simpara>You configure IP address policy with a <literal>policy</literal> object defined by specifying the <literal>spec.ExternalIP.policy</literal> field.
The policy object has the following shape:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "policy": {
    "allowedCIDRs": [],
    "rejectedCIDRs": []
  }
}</programlisting>
<simpara>When configuring policy restrictions, the following rules apply:</simpara>
<itemizedlist>
<listitem>
<simpara>If <literal>policy={}</literal> is set, then creating a <literal>Service</literal> object with <literal>spec.ExternalIPs[]</literal> set will fail. This is the default for OpenShift Container Platform. The behavior when <literal>policy=null</literal> is set is identical.</simpara>
</listitem>
<listitem>
<simpara>If <literal>policy</literal> is set and either <literal>policy.allowedCIDRs[]</literal> or <literal>policy.rejectedCIDRs[]</literal> is set, the following rules apply:</simpara>
<itemizedlist>
<listitem>
<simpara>If <literal>allowedCIDRs[]</literal> and <literal>rejectedCIDRs[]</literal> are both set, then <literal>rejectedCIDRs[]</literal> has precedence over <literal>allowedCIDRs[]</literal>.</simpara>
</listitem>
<listitem>
<simpara>If <literal>allowedCIDRs[]</literal> is set, creating a <literal>Service</literal> object with <literal>spec.ExternalIPs[]</literal> will succeed only if the specified IP addresses are allowed.</simpara>
</listitem>
<listitem>
<simpara>If <literal>rejectedCIDRs[]</literal> is set, creating a <literal>Service</literal> object with <literal>spec.ExternalIPs[]</literal> will succeed only if the specified IP addresses are not rejected.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="example-policy-objects_configuring-externalip">
<title>Example policy objects</title>
<simpara>The examples that follow demonstrate several different policy configurations.</simpara>
<itemizedlist>
<listitem>
<simpara>In the following example, the policy prevents OpenShift Container Platform from creating any service with an external IP address specified:</simpara>
<formalpara>
<title>Example policy to reject any value specified for <literal>Service</literal> object <literal>spec.externalIPs[]</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    policy: {}
  ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>In the following example, both the <literal>allowedCIDRs</literal> and <literal>rejectedCIDRs</literal> fields are set.</simpara>
<formalpara>
<title>Example policy that includes both allowed and rejected CIDR blocks</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    policy:
      allowedCIDRs:
      - 172.16.66.10/23
      rejectedCIDRs:
      - 172.16.66.10/24
  ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>In the following example, <literal>policy</literal> is set to <literal>null</literal>.
If set to <literal>null</literal>, when inspecting the configuration object by entering <literal>oc get networks.config.openshift.io -o yaml</literal>, the <literal>policy</literal> field will not appear in the output.</simpara>
<formalpara>
<title>Example policy to allow any value specified for <literal>Service</literal> object <literal>spec.externalIPs[]</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    policy: null
  ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nw-externalip-object_configuring-externalip">
<title>ExternalIP address block configuration</title>
<simpara>The configuration for ExternalIP address blocks is defined by a Network custom resource (CR) named <literal>cluster</literal>. The Network CR is part of the <literal>config.openshift.io</literal> API group.</simpara>
<important>
<simpara>During cluster installation, the Cluster Version Operator (CVO) automatically creates a Network CR named <literal>cluster</literal>.
Creating any other CR objects of this type is not supported.</simpara>
</important>
<simpara>The following YAML describes the ExternalIP configuration:</simpara>
<formalpara>
<title>Network.config.openshift.io CR named <literal>cluster</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  externalIP:
    autoAssignCIDRs: [] <co xml:id="CO224-1"/>
    policy: <co xml:id="CO224-2"/>
      ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO224-1">
<para>Defines the IP address block in CIDR format that is available for automatic assignment of external IP addresses to a service.
Only a single IP address range is allowed.</para>
</callout>
<callout arearefs="CO224-2">
<para>Defines restrictions on manual assignment of an IP address to a service.
If no restrictions are defined, specifying the <literal>spec.externalIP</literal> field in a <literal>Service</literal> object is not allowed.
By default, no restrictions are defined.</para>
</callout>
</calloutlist>
<simpara>The following YAML describes the fields for the <literal>policy</literal> stanza:</simpara>
<formalpara>
<title>Network.config.openshift.io <literal>policy</literal> stanza</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">policy:
  allowedCIDRs: [] <co xml:id="CO225-1"/>
  rejectedCIDRs: [] <co xml:id="CO225-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO225-1">
<para>A list of allowed IP address ranges in CIDR format.</para>
</callout>
<callout arearefs="CO225-2">
<para>A list of rejected IP address ranges in CIDR format.</para>
</callout>
</calloutlist>
<bridgehead xml:id="_example-external-ip-configurations" renderas="sect4">Example external IP configurations</bridgehead>
<simpara>Several possible configurations for external IP address pools are displayed in the following examples:</simpara>
<itemizedlist>
<listitem>
<simpara>The following YAML describes a configuration that enables automatically assigned external IP addresses:</simpara>
<formalpara>
<title>Example configuration with <literal>spec.externalIP.autoAssignCIDRs</literal> set</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  ...
  externalIP:
    autoAssignCIDRs:
    - 192.168.132.254/29</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>The following YAML configures policy rules for the allowed and rejected CIDR ranges:</simpara>
<formalpara>
<title>Example configuration with <literal>spec.externalIP.policy</literal> set</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  ...
  externalIP:
    policy:
      allowedCIDRs:
      - 192.168.132.0/29
      - 192.168.132.8/29
      rejectedCIDRs:
      - 192.168.132.7/32</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-externalip-configuring_configuring-externalip">
<title>Configure external IP address blocks for your cluster</title>
<simpara>As a cluster administrator, you can configure the following ExternalIP settings:</simpara>
<itemizedlist>
<listitem>
<simpara>An ExternalIP address block used by OpenShift Container Platform to automatically populate the <literal>spec.clusterIP</literal> field for a <literal>Service</literal> object.</simpara>
</listitem>
<listitem>
<simpara>A policy object to restrict what IP addresses may be manually assigned to the <literal>spec.clusterIP</literal> array of a <literal>Service</literal> object.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To display the current external IP configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe networks.config cluster</programlisting>
</listitem>
<listitem>
<simpara>To edit the configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.config cluster</programlisting>
</listitem>
<listitem>
<simpara>Modify the ExternalIP configuration, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  ...
  externalIP: <co xml:id="CO226-1"/>
  ...</programlisting>
<calloutlist>
<callout arearefs="CO226-1">
<para>Specify the configuration for the <literal>externalIP</literal> stanza.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To confirm the updated ExternalIP configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networks.config cluster -o go-template='{{.spec.externalIP}}{{"\n"}}'</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-externalip-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-ingress-cluster-traffic-service-external-ip">Configuring ingress cluster traffic for a service external IP</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-ingress-cluster-traffic-ingress-controller">
<title>Configuring ingress cluster traffic using an Ingress Controller</title>

<simpara>OpenShift Container Platform provides methods for communicating from outside the cluster with
services running in the cluster. This method uses an Ingress Controller.</simpara>
<section xml:id="nw-using-ingress-and-routes_configuring-ingress-cluster-traffic-ingress-controller">
<title>Using Ingress Controllers and routes</title>
<simpara>The Ingress Operator manages Ingress Controllers and wildcard DNS.</simpara>
<simpara>Using an Ingress Controller is the most common way to allow external access to
an OpenShift Container Platform cluster.</simpara>
<simpara>An Ingress Controller is configured to accept external requests and proxy them
based on the configured routes. This is limited to HTTP, HTTPS using SNI, and
TLS using SNI, which is sufficient for web applications and services that work
over TLS with SNI.</simpara>
<simpara>Work with your administrator to configure an Ingress Controller
to accept external requests and proxy them based on the
configured routes.</simpara>
<simpara>The administrator can create a wildcard DNS entry and then set up an Ingress
Controller. Then, you can work with the edge Ingress Controller without
having to contact the administrators.</simpara>
<simpara>By default, every Ingress Controller in the cluster can admit any route created in any project in the cluster.</simpara>
<simpara>The Ingress Controller:</simpara>
<itemizedlist>
<listitem>
<simpara>Has two replicas by default, which means it should be running on two worker nodes.</simpara>
</listitem>
<listitem>
<simpara>Can be scaled up to have more replicas on more nodes.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The procedures in this section require prerequisites performed by the cluster
administrator.</simpara>
</note>
</section>
<section xml:id="_prerequisites-3">
<title>Prerequisites</title>
<simpara>Before starting the following procedures, the administrator must:</simpara>
<itemizedlist>
<listitem>
<simpara>Set up the external port to the cluster networking environment so that requests
can reach the cluster.</simpara>
</listitem>
<listitem>
<simpara>Make sure there is at least one user with cluster admin role. To add this role
to a user, run the following command:</simpara>
<screen>$ oc adm policy add-cluster-role-to-user cluster-admin username</screen>
</listitem>
<listitem>
<simpara>Have an OpenShift Container Platform cluster with at least one master and at least one node
and a system outside the cluster that has network access to the cluster. This
procedure assumes that the external system is on the same subnet as the cluster.
The additional networking required for external systems on a different subnet is
out-of-scope for this topic.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-creating-project-and-service_configuring-ingress-cluster-traffic-ingress-controller">
<title>Creating a project and service</title>
<simpara>If the project and service that you want to expose do not exist, first create
the project, then the service.</simpara>
<simpara>If the project and service already exist, skip to the procedure on exposing the
service to create a route.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal>oc</literal> CLI and log in as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new project for your service by running the <literal>oc new-project</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project myproject</programlisting>
</listitem>
<listitem>
<simpara>Use the <literal>oc new-app</literal> command to create your service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-app nodejs:12~https://github.com/sclorg/nodejs-ex.git</programlisting>
</listitem>
<listitem>
<simpara>To verify that the service was created, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc -n myproject</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nodejs-ex   ClusterIP   172.30.197.157   &lt;none&gt;        8080/TCP   70s</programlisting>
</para>
</formalpara>
<simpara>By default, the new service does not have an external IP address.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-exposing-service_configuring-ingress-cluster-traffic-ingress-controller">
<title>Exposing the service by creating a route</title>
<simpara>You can expose the service as a route by using the <literal>oc expose</literal> command.</simpara>
<formalpara>
<title>Procedure</title>
<para>To expose the service:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Log in to OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>Log in to the project where the service you want to expose is located:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project myproject</programlisting>
</listitem>
<listitem>
<simpara>Run the <literal>oc expose service</literal> command to expose the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose service nodejs-ex</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">route.route.openshift.io/nodejs-ex exposed</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To verify that the service is exposed, you can use a tool, such as cURL, to make sure the service is accessible from outside the cluster.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Use the <literal>oc get route</literal> command to find the route&#8217;s host name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        HOST/PORT                        PATH   SERVICES    PORT       TERMINATION   WILDCARD
nodejs-ex   nodejs-ex-myproject.example.com         nodejs-ex   8080-tcp                 None</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use cURL to check that the host responds to a GET request:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --head nodejs-ex-myproject.example.com</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 200 OK
...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-sharding-route-labels_configuring-ingress-cluster-traffic-ingress-controller">
<title>Configuring Ingress Controller sharding by using route labels</title>
<simpara>Ingress Controller sharding by using route labels means that the Ingress
Controller serves any route in any namespace that is selected by the route
selector.</simpara>
<figure>
<title>Ingress sharding using route labels</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-sharding-route-labels.png"/>
</imageobject>
<textobject><phrase>A diagram showing multiple Ingress Controllers with different route selectors serving any route containing a label that matches a given route selector regardless of the namespace a route belongs to</phrase></textobject>
</mediaobject>
</figure>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among
a set of Ingress Controllers and when isolating traffic to a specific Ingress
Controller. For example, company A goes to one Ingress Controller and company B
to another.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat router-internal.yaml
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: sharded
  namespace: openshift-ingress-operator
spec:
  domain: &lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO227-1"/>
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/worker: ""
  routeSelector:
    matchLabels:
      type: sharded</programlisting>
<calloutlist>
<callout arearefs="CO227-1">
<para>Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the Ingress Controller <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc apply -f router-internal.yaml</programlisting>
<simpara>The Ingress Controller selects routes in any namespace that have the label
<literal>type: sharded</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create a new route using the domain configured in the <literal>router-internal.yaml</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-sharding-namespace-labels_configuring-ingress-cluster-traffic-ingress-controller">
<title>Configuring Ingress Controller sharding by using namespace labels</title>
<simpara>Ingress Controller sharding by using namespace labels means that the Ingress
Controller serves any route in any namespace that is selected by the namespace
selector.</simpara>
<figure>
<title>Ingress sharding using namespace labels</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-sharding-namespace-labels.png"/>
</imageobject>
<textobject><phrase>A diagram showing multiple Ingress Controllers with different namespace selectors serving routes that belong to the namespace containing a label that matches a given namespace selector</phrase></textobject>
</mediaobject>
</figure>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among
a set of Ingress Controllers and when isolating traffic to a specific Ingress
Controller. For example, company A goes to one Ingress Controller and company B
to another.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat router-internal.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: sharded
  namespace: openshift-ingress-operator
spec:
  domain: &lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO228-1"/>
  nodePlacement:
    nodeSelector:
      matchLabels:
        node-role.kubernetes.io/worker: ""
  namespaceSelector:
    matchLabels:
      type: sharded</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO228-1">
<para>Specify a domain to be used by the Ingress Controller. This domain must be different from the default Ingress Controller domain.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the Ingress Controller <literal>router-internal.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc apply -f router-internal.yaml</programlisting>
<simpara>The Ingress Controller selects routes in any namespace that is selected by the
namespace selector that have the label <literal>type: sharded</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create a new route using the domain configured in the <literal>router-internal.yaml</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose svc &lt;service-name&gt; --hostname &lt;route-name&gt;.apps-sharded.basedomain.example.net</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-ingress-sharding-route-configuration_configuring-ingress-cluster-traffic-ingress-controller">
<title>Creating a route for Ingress Controller sharding</title>
<simpara>A route allows you to host your application at a URL. In this case, the hostname is not set and the route uses a subdomain instead. When you specify a subdomain, you automatically use the domain of the Ingress Controller that exposes the route. For situations where a route is exposed by multiple Ingress Controllers, the route is hosted at multiple URLs.</simpara>
<simpara>The following procedure describes how to create a route for Ingress Controller sharding, using the <literal>hello-openshift</literal> application as an example.</simpara>
<simpara>Ingress Controller sharding is useful when balancing incoming traffic load among a set of Ingress Controllers and when isolating traffic to a specific Ingress Controller. For example, company A goes to one Ingress Controller and company B to another.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a project administrator.</simpara>
</listitem>
<listitem>
<simpara>You have a web application that exposes a port and an HTTP or TLS endpoint listening for traffic on the port.</simpara>
</listitem>
<listitem>
<simpara>You have configured the Ingress Controller for sharding.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a project called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a pod in the project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f https://raw.githubusercontent.com/openshift/origin/master/examples/hello-openshift/hello-pod.json</programlisting>
</listitem>
<listitem>
<simpara>Create a service called <literal>hello-openshift</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose pod/hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Create a route definition called <literal>hello-openshift-route.yaml</literal>:</simpara>
<formalpara>
<title>YAML definition of the created route for sharding:</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded <co xml:id="CO229-1"/>
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift <co xml:id="CO229-2"/>
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO229-1">
<para>Both the label key and its corresponding label value must match the ones specified in the Ingress Controller. In this example, the Ingress Controller has the label key and value <literal>type: sharded</literal>.</para>
</callout>
<callout arearefs="CO229-2">
<para>The route will be exposed using the value of the <literal>subdomain</literal> field. When you specify the <literal>subdomain</literal> field, you must leave the hostname unset. If you specify both the <literal>host</literal> and <literal>subdomain</literal> fields, then the route will use the value of the <literal>host</literal> field, and ignore the <literal>subdomain</literal> field.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Use <literal>hello-openshift-route.yaml</literal> to create a route to the <literal>hello-openshift</literal> application by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-openshift create -f hello-openshift-route.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Get the status of the route with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-openshift get routes/hello-openshift-edge -o yaml</programlisting>
<simpara>The resulting <literal>Route</literal> resource should look similar to the following:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: route.openshift.io/v1
kind: Route
metadata:
  labels:
    type: sharded
  name: hello-openshift-edge
  namespace: hello-openshift
spec:
  subdomain: hello-openshift
  tls:
    termination: edge
  to:
    kind: Service
    name: hello-openshift
status:
  ingress:
  - host: hello-openshift.&lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO230-1"/>
    routerCanonicalHostname: router-sharded.&lt;apps-sharded.basedomain.example.net&gt; <co xml:id="CO230-2"/>
    routerName: sharded <co xml:id="CO230-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO230-1">
<para>The hostname the Ingress Controller, or router, uses to expose the route. The value of the <literal>host</literal> field is automatically determined by the Ingress Controller, and uses its domain. In this example, the domain of the Ingress Controller is <literal>&lt;apps-sharded.basedomain.example.net&gt;</literal>.</para>
</callout>
<callout arearefs="CO230-2">
<para>The hostname of the Ingress Controller.</para>
</callout>
<callout arearefs="CO230-3">
<para>The name of the Ingress Controller. In this example, the Ingress Controller has the name <literal>sharded</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="_additional-resources-5" role="_additional-resources">
<title>Additional resources</title>
<simpara>The Ingress Operator manages wildcard DNS. For more information, see the following:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-ingress">Ingress Operator in OpenShift Container Platform</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-bare-metal">Installing a cluster on bare metal</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-vsphere">Installing a cluster on vSphere</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="about-network-policy">About network policy</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-ingress-cluster-traffic-load-balancer">
<title>Configuring ingress cluster traffic using a load balancer</title>

<simpara>OpenShift Container Platform provides methods for communicating from
outside the cluster with services running in the cluster. This method uses a
load balancer.</simpara>
<section xml:id="nw-using-load-balancer-getting-traffic_configuring-ingress-cluster-traffic-load-balancer">
<title>Using a load balancer to get traffic into the cluster</title>
<simpara>If you do not need a specific external IP address, you can configure a load
balancer service to allow external access to an OpenShift Container Platform cluster.</simpara>
<simpara>A load balancer service allocates a unique IP. The load balancer has a single
edge router IP, which can be a virtual IP (VIP), but is still a single machine
for initial load balancing.</simpara>
<note>
<simpara>If a pool is configured, it is done at the infrastructure level, not by a cluster
administrator.</simpara>
</note>
<note>
<simpara>The procedures in this section require prerequisites performed by the cluster
administrator.</simpara>
</note>
</section>
<section xml:id="_prerequisites-4">
<title>Prerequisites</title>
<simpara>Before starting the following procedures, the administrator must:</simpara>
<itemizedlist>
<listitem>
<simpara>Set up the external port to the cluster networking environment so that requests
can reach the cluster.</simpara>
</listitem>
<listitem>
<simpara>Make sure there is at least one user with cluster admin role. To add this role
to a user, run the following command:</simpara>
<screen>$ oc adm policy add-cluster-role-to-user cluster-admin username</screen>
</listitem>
<listitem>
<simpara>Have an OpenShift Container Platform cluster with at least one master and at least one node
and a system outside the cluster that has network access to the cluster. This
procedure assumes that the external system is on the same subnet as the cluster.
The additional networking required for external systems on a different subnet is
out-of-scope for this topic.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-creating-project-and-service_configuring-ingress-cluster-traffic-load-balancer">
<title>Creating a project and service</title>
<simpara>If the project and service that you want to expose do not exist, first create
the project, then the service.</simpara>
<simpara>If the project and service already exist, skip to the procedure on exposing the
service to create a route.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal>oc</literal> CLI and log in as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new project for your service by running the <literal>oc new-project</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project myproject</programlisting>
</listitem>
<listitem>
<simpara>Use the <literal>oc new-app</literal> command to create your service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-app nodejs:12~https://github.com/sclorg/nodejs-ex.git</programlisting>
</listitem>
<listitem>
<simpara>To verify that the service was created, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc -n myproject</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nodejs-ex   ClusterIP   172.30.197.157   &lt;none&gt;        8080/TCP   70s</programlisting>
</para>
</formalpara>
<simpara>By default, the new service does not have an external IP address.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-exposing-service_configuring-ingress-cluster-traffic-load-balancer">
<title>Exposing the service by creating a route</title>
<simpara>You can expose the service as a route by using the <literal>oc expose</literal> command.</simpara>
<formalpara>
<title>Procedure</title>
<para>To expose the service:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Log in to OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>Log in to the project where the service you want to expose is located:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project myproject</programlisting>
</listitem>
<listitem>
<simpara>Run the <literal>oc expose service</literal> command to expose the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose service nodejs-ex</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">route.route.openshift.io/nodejs-ex exposed</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To verify that the service is exposed, you can use a tool, such as cURL, to make sure the service is accessible from outside the cluster.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Use the <literal>oc get route</literal> command to find the route&#8217;s host name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        HOST/PORT                        PATH   SERVICES    PORT       TERMINATION   WILDCARD
nodejs-ex   nodejs-ex-myproject.example.com         nodejs-ex   8080-tcp                 None</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use cURL to check that the host responds to a GET request:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --head nodejs-ex-myproject.example.com</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 200 OK
...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-create-load-balancer-service_configuring-ingress-cluster-traffic-load-balancer">
<title>Creating a load balancer service</title>
<simpara>Use the following procedure to create a load balancer service.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Make sure that the project and service you want to expose exist.</simpara>
</listitem>
<listitem>
<simpara>Your cloud provider supports load balancers.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To create a load balancer service:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Log in to  OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>Load the project where the service you want to expose is located.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project project1</programlisting>
</listitem>
<listitem>
<simpara>Open a text file on the control plane node and paste the following text, editing the
file as needed:</simpara>
<formalpara>
<title>Sample load balancer configuration file</title>
<para>
<screen>apiVersion: v1
kind: Service
metadata:
  name: egress-2 <co xml:id="CO231-1"/>
spec:
  ports:
  - name: db
    port: 3306 <co xml:id="CO231-2"/>
  loadBalancerIP:
  loadBalancerSourceRanges: <co xml:id="CO231-3"/>
  - 10.0.0.0/8
  - 192.168.0.0/16
  type: LoadBalancer <co xml:id="CO231-4"/>
  selector:
    name: mysql <co xml:id="CO231-5"/></screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO231-1">
<para>Enter a descriptive name for the load balancer service.</para>
</callout>
<callout arearefs="CO231-2">
<para>Enter the same port that the service you want to expose is listening on.</para>
</callout>
<callout arearefs="CO231-3">
<para>Enter a list of specific IP addresses to restrict traffic through the load balancer. This field is ignored if the cloud-provider does not support the feature.</para>
</callout>
<callout arearefs="CO231-4">
<para>Enter <literal>Loadbalancer</literal> as the type.</para>
</callout>
<callout arearefs="CO231-5">
<para>Enter the name of the service.</para>
</callout>
</calloutlist>
<note>
<simpara>To restrict the traffic through the load balancer to specific IP addresses, it is recommended to use the Ingress Controller field <literal>spec.endpointPublishingStrategy.loadBalancer.allowedSourceRanges</literal>. Do not set the <literal>loadBalancerSourceRanges</literal> field.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save and exit the file.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to create the service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f mysql-lb.yaml</programlisting>
</listitem>
<listitem>
<simpara>Execute the following command to view the new service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME       TYPE           CLUSTER-IP      EXTERNAL-IP                             PORT(S)          AGE
egress-2   LoadBalancer   172.30.22.226   ad42f5d8b303045-487804948.example.com   3306:30357/TCP   15m</programlisting>
</para>
</formalpara>
<simpara>The service has an external IP address automatically assigned if there is a cloud
provider enabled.</simpara>
</listitem>
<listitem>
<simpara>On the master, use a tool, such as cURL, to make sure you can reach the service
using the public IP address:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl &lt;public-ip&gt;:&lt;port&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl 172.29.121.74:3306</programlisting>
<simpara>The examples in this section use a MySQL service, which requires a client application.
If you get a string of characters with the <literal>Got packets out of order</literal> message,
you are connecting with the service:</simpara>
<simpara>If you have a MySQL client, log in with the standard CLI command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mysql -h 172.30.131.89 -u admin -p</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Enter password:
Welcome to the MariaDB monitor.  Commands end with ; or \g.

MySQL [(none)]&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-ingress-cluster-traffic-aws">
<title>Configuring ingress cluster traffic on AWS</title>

<simpara>OpenShift Container Platform provides methods for communicating from outside the cluster with services running in the cluster. This method uses load balancers on AWS, specifically a Network Load Balancer (NLB) or a Classic Load Balancer (CLB). Both types of load balancers can forward the client&#8217;s IP address to the node, but a CLB requires proxy protocol support, which OpenShift Container Platform automatically enables.</simpara>
<simpara>There are two ways to configure an Ingress Controller to use an NLB:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>By force replacing the Ingress Controller that is currently using a CLB. This deletes the <literal>IngressController</literal> object and an outage will occur while the new DNS records propagate and the NLB is being provisioned.</simpara>
</listitem>
<listitem>
<simpara>By editing an existing Ingress Controller that uses a CLB to use an NLB. This changes the load balancer without having to delete and recreate the <literal>IngressController</literal> object.</simpara>
</listitem>
</orderedlist>
<simpara>Both methods can be used to switch from an NLB to a CLB.</simpara>
<simpara>You can configure these load balancers on a new or existing AWS cluster.</simpara>
<section xml:id="nw-configuring-elb-timeouts-aws-classic_configuring-ingress-cluster-traffic-aws">
<title>Configuring Classic Load Balancer timeouts on AWS</title>
<simpara>OpenShift Container Platform provides a method for setting a custom timeout period for a specific route or Ingress Controller. Additionally, an AWS Classic Load Balancer (CLB) has its own timeout period with a default time of 60 seconds.</simpara>
<simpara>If the timeout period of the CLB is shorter than the route timeout or Ingress Controller timeout, the load balancer can prematurely terminate the connection. You can prevent this problem by increasing both the timeout period of the route and CLB.</simpara>
<section xml:id="nw-configuring-route-timeouts_configuring-ingress-cluster-traffic-aws">
<title>Configuring route timeouts</title>
<simpara>You can configure the default timeouts for an existing route when you
have services in need of a low timeout, which is required for Service Level
Availability (SLA) purposes, or a high timeout, for cases with a slow
back end.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You need a deployed Ingress Controller on a running cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Using the <literal>oc annotate</literal> command, add the timeout to the route:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route &lt;route_name&gt; \
    --overwrite haproxy.router.openshift.io/timeout=&lt;timeout&gt;&lt;time_unit&gt; <co xml:id="CO232-1"/></programlisting>
<calloutlist>
<callout arearefs="CO232-1">
<para>Supported time units are microseconds (us), milliseconds (ms), seconds (s),
minutes (m), hours (h), or days (d).</para>
</callout>
</calloutlist>
<simpara>The following example sets  a timeout of two seconds on a route named <literal>myroute</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate route myroute --overwrite haproxy.router.openshift.io/timeout=2s</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-configuring-clb-timeouts_configuring-ingress-cluster-traffic-aws">
<title>Configuring Classic Load Balancer timeouts</title>
<simpara>You can configure the default timeouts for a Classic Load Balancer (CLB) to extend idle connections.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a deployed Ingress Controller on a running cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set an AWS connection idle timeout of five minutes for the default <literal>ingresscontroller</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"type":"LoadBalancerService", "loadBalancer": \
    {"scope":"External", "providerParameters":{"type":"AWS", "aws": \
    {"type":"Classic", "classicLoadBalancer": \
    {"connectionIdleTimeout":"5m"}}}}}}}'</programlisting>
</listitem>
<listitem>
<simpara>Optional: Restore the default value of the timeout by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"loadBalancer":{"providerParameters":{"aws":{"classicLoadBalancer": \
    {"connectionIdleTimeout":null}}}}}}}'</programlisting>
</listitem>
</orderedlist>
<note>
<simpara>You must specify the <literal>scope</literal> field when you change the connection timeout value unless the current scope is already set. When you set the <literal>scope</literal> field, you do not need to do so again if you restore the default timeout value.</simpara>
</note>
</section>
</section>
<section xml:id="nw-configuring-ingress-cluster-traffic-aws-network-load-balancer_configuring-ingress-cluster-traffic-aws">
<title>Configuring ingress cluster traffic on AWS using a Network Load Balancer</title>
<simpara>OpenShift Container Platform provides methods for communicating from outside the cluster with services that run in the cluster. One such method uses a Network Load Balancer (NLB). You can configure an NLB on a new or existing AWS cluster.</simpara>
<section xml:id="nw-aws-switching-clb-with-nlb_configuring-ingress-cluster-traffic-aws">
<title>Switching the Ingress Controller from using a Classic Load Balancer to a Network Load Balancer</title>
<simpara>You can switch the Ingress Controller that is using a Classic Load Balancer (CLB) to one that uses a Network Load Balancer (NLB) on AWS.</simpara>
<simpara>Switching between these load balancers will not delete the <literal>IngressController</literal> object.</simpara>
<warning>
<simpara>This procedure might cause the following issues:</simpara>
<itemizedlist>
<listitem>
<simpara>An outage that can last several minutes due to new DNS records propagation, new load balancers provisioning, and other factors. IP addresses and canonical names of the Ingress Controller load balancer might change after applying this procedure.</simpara>
</listitem>
<listitem>
<simpara>Leaked load balancer resources due to a change in the annotation of the service.</simpara>
</listitem>
</itemizedlist>
</warning>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the existing Ingress Controller that you want to switch to using an NLB. This example assumes that your default Ingress Controller has an <literal>External</literal> scope and no other customizations:</simpara>
<formalpara>
<title>Example <literal>ingresscontroller.yaml</literal> file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
    type: LoadBalancerService</programlisting>
</para>
</formalpara>
<note>
<simpara>If you do not specify a value for the <literal>spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.type</literal> field, the Ingress Controller uses the <literal>spec.loadBalancer.platform.aws.type</literal> value from the cluster <literal>Ingress</literal> configuration that was set during installation.</simpara>
</note>
<tip>
<simpara>If your Ingress Controller has other customizations that you want to update, such as changing the domain, consider force replacing the Ingress Controller definition file instead.</simpara>
</tip>
</listitem>
<listitem>
<simpara>Apply the changes to the Ingress Controller YAML file by running the command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ingresscontroller.yaml</programlisting>
<simpara>Expect several minutes of outages while the Ingress Controller updates.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-aws-switching-nlb-with-clb_configuring-ingress-cluster-traffic-aws">
<title>Switching the Ingress Controller from using a Network Load Balancer to a Classic Load Balancer</title>
<simpara>You can switch the Ingress Controller that is using a Network Load Balancer (NLB) to one that uses a Classic Load Balancer (CLB) on AWS.</simpara>
<simpara>Switching between these load balancers will not delete the <literal>IngressController</literal> object.</simpara>
<warning>
<simpara>This procedure might cause an outage that can last several minutes due to new DNS records propagation, new load balancers provisioning, and other factors. IP addresses and canonical names of the Ingress Controller load balancer might change after applying this procedure.</simpara>
</warning>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the existing Ingress Controller that you want to switch to using a CLB. This example assumes that your default Ingress Controller has an <literal>External</literal> scope and no other customizations:</simpara>
<formalpara>
<title>Example <literal>ingresscontroller.yaml</literal> file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: Classic
    type: LoadBalancerService</programlisting>
</para>
</formalpara>
<note>
<simpara>If you do not specify a value for the <literal>spec.endpointPublishingStrategy.loadBalancer.providerParameters.aws.type</literal> field, the Ingress Controller uses the <literal>spec.loadBalancer.platform.aws.type</literal> value from the cluster <literal>Ingress</literal> configuration that was set during installation.</simpara>
</note>
<tip>
<simpara>If your Ingress Controller has other customizations that you want to update, such as changing the domain, consider force replacing the Ingress Controller definition file instead.</simpara>
</tip>
</listitem>
<listitem>
<simpara>Apply the changes to the Ingress Controller YAML file by running the command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ingresscontroller.yaml</programlisting>
<simpara>Expect several minutes of outages while the Ingress Controller updates.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-aws-replacing-clb-with-nlb_configuring-ingress-cluster-traffic-aws">
<title>Replacing Ingress Controller Classic Load Balancer with Network Load Balancer</title>
<simpara>You can replace an Ingress Controller that is using a Classic Load Balancer (CLB) with one that uses a Network Load Balancer (NLB) on AWS.</simpara>
<warning>
<simpara>This procedure might cause the following issues:</simpara>
<itemizedlist>
<listitem>
<simpara>An outage that can last several minutes due to new DNS records propagation, new load balancers provisioning, and other factors. IP addresses and canonical names of the Ingress Controller load balancer might change after applying this procedure.</simpara>
</listitem>
<listitem>
<simpara>Leaked load balancer resources due to a change in the annotation of the service.</simpara>
</listitem>
</itemizedlist>
</warning>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file with a new default Ingress Controller. The following example assumes that your default Ingress Controller has an <literal>External</literal> scope and no other customizations:</simpara>
<formalpara>
<title>Example <literal>ingresscontroller.yml</literal> file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
    type: LoadBalancerService</programlisting>
</para>
</formalpara>
<simpara>If your default Ingress Controller has other customizations, ensure that you modify the file accordingly.</simpara>
<tip>
<simpara>If your Ingress Controller has no other customizations and you are only updating the load balancer type, consider following the procedure detailed in "Switching the Ingress Controller from using a Classic Load Balancer to a Network Load Balancer".</simpara>
</tip>
</listitem>
<listitem>
<simpara>Force replace the Ingress Controller YAML file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace --force --wait -f ingresscontroller.yml</programlisting>
<simpara>Wait until the Ingress Controller is replaced. Expect several of minutes of outages.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-aws-nlb-existing-cluster_configuring-ingress-cluster-traffic-aws">
<title>Configuring an Ingress Controller Network Load Balancer on an existing AWS cluster</title>
<simpara>You can create an Ingress Controller backed by an AWS Network Load Balancer (NLB) on an existing cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have an installed AWS cluster.</simpara>
</listitem>
<listitem>
<simpara><literal>PlatformStatus</literal> of the infrastructure resource must be AWS.</simpara>
<itemizedlist>
<listitem>
<simpara>To verify that the <literal>PlatformStatus</literal> is AWS, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get infrastructure/cluster -o jsonpath='{.status.platformStatus.type}'
AWS</programlisting>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>Create an Ingress Controller backed by an AWS NLB on an existing cluster.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create the Ingress Controller manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ cat ingresscontroller-aws-nlb.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  name: $my_ingress_controller<co xml:id="CO233-1"/>
  namespace: openshift-ingress-operator
spec:
  domain: $my_unique_ingress_domain<co xml:id="CO233-2"/>
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: External<co xml:id="CO233-3"/>
      providerParameters:
        type: AWS
        aws:
          type: NLB</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO233-1">
<para>Replace <literal>$my_ingress_controller</literal> with a unique name for the Ingress Controller.</para>
</callout>
<callout arearefs="CO233-2">
<para>Replace <literal>$my_unique_ingress_domain</literal> with a domain name that is unique among all Ingress Controllers in the cluster. This variable must be a subdomain of the DNS name <literal>&lt;clustername&gt;.&lt;domain&gt;</literal>.</para>
</callout>
<callout arearefs="CO233-3">
<para>You can replace <literal>External</literal> with <literal>Internal</literal> to use an internal NLB.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the resource in the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ingresscontroller-aws-nlb.yaml</programlisting>
</listitem>
</orderedlist>
<important>
<simpara>Before you can configure an Ingress Controller NLB on a new AWS cluster, you must complete the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installation-initializing_installing-aws-network-customizations">Creating the installation configuration file</link> procedure.</simpara>
</important>
</section>
<section xml:id="nw-aws-nlb-new-cluster_configuring-ingress-cluster-traffic-aws">
<title>Configuring an Ingress Controller Network Load Balancer on a new AWS cluster</title>
<simpara>You can create an Ingress Controller backed by an AWS Network Load Balancer (NLB) on a new cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Create the <literal>install-config.yaml</literal> file and complete any modifications to it.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>Create an Ingress Controller backed by an AWS NLB on a new cluster.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Change to the directory that contains the installation program and create the manifests:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./openshift-install create manifests --dir &lt;installation_directory&gt; <co xml:id="CO234-1"/></programlisting>
<calloutlist>
<callout arearefs="CO234-1">
<para>For <literal>&lt;installation_directory&gt;</literal>, specify the name of the directory that
contains the <literal>install-config.yaml</literal> file for your cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a file that is named <literal>cluster-ingress-default-ingresscontroller.yaml</literal> in the <literal>&lt;installation_directory&gt;/manifests/</literal> directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ touch &lt;installation_directory&gt;/manifests/cluster-ingress-default-ingresscontroller.yaml <co xml:id="CO235-1"/></programlisting>
<calloutlist>
<callout arearefs="CO235-1">
<para>For <literal>&lt;installation_directory&gt;</literal>, specify the directory name that contains the
<literal>manifests/</literal> directory for your cluster.</para>
</callout>
</calloutlist>
<simpara>After creating the file, several network configuration files are in the
<literal>manifests/</literal> directory, as shown:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ls &lt;installation_directory&gt;/manifests/cluster-ingress-default-ingresscontroller.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">cluster-ingress-default-ingresscontroller.yaml</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Open the <literal>cluster-ingress-default-ingresscontroller.yaml</literal> file in an editor and enter a custom resource (CR) that describes the Operator configuration you want:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: null
  name: default
  namespace: openshift-ingress-operator
spec:
  endpointPublishingStrategy:
    loadBalancer:
      scope: External
      providerParameters:
        type: AWS
        aws:
          type: NLB
    type: LoadBalancerService</programlisting>
</listitem>
<listitem>
<simpara>Save the <literal>cluster-ingress-default-ingresscontroller.yaml</literal> file and quit the text editor.</simpara>
</listitem>
<listitem>
<simpara>Optional: Back up the <literal>manifests/cluster-ingress-default-ingresscontroller.yaml</literal> file. The installation program deletes the <literal>manifests/</literal> directory when creating the cluster.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_configuring-ingress-cluster-traffic-aws" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-network-customizations">Installing a cluster on AWS with network customizations</link>.</simpara>
</listitem>
<listitem>
<simpara>For more information on support for NLBs, see <link xlink:href="https://kubernetes.io/docs/concepts/services-networking/service/#aws-nlb-support">Network Load Balancer support on AWS</link>.</simpara>
</listitem>
<listitem>
<simpara>For more information on proxy protocol support for CLBs, see <link xlink:href="https://docs.aws.amazon.com/elasticloadbalancing/latest/classic/enable-proxy-protocol.html">Configure proxy protocol support for your Classic Load Balancer</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-ingress-cluster-traffic-service-external-ip">
<title>Configuring ingress cluster traffic for a service external IP</title>

<simpara>You can attach an external IP address to a service so that it is available to traffic outside the cluster.
This is generally useful only for a cluster installed on bare metal hardware.
The external network infrastructure must be configured correctly to route traffic to the service.</simpara>
<section xml:id="configuring-ingress-cluster-traffic-service-external-ip-prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your cluster is configured with ExternalIPs enabled. For more information, read <link linkend="configuring-externalip">Configuring ExternalIPs for services</link>.</simpara>
<note>
<simpara>Do not use the same ExternalIP for the egress IP.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-service-externalip-create_configuring-ingress-cluster-traffic-service-external-ip">
<title>Attaching an ExternalIP to a service</title>
<simpara>You can attach an ExternalIP to a service. If your cluster is configured to allocate an ExternalIP automatically, you might not need to manually attach an ExternalIP to the service.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To confirm what IP address ranges are configured for use with ExternalIP, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networks.config cluster -o jsonpath='{.spec.externalIP}{"\n"}'</programlisting>
<simpara>If <literal>autoAssignCIDRs</literal> is set, OpenShift Container Platform automatically assigns an ExternalIP to a new <literal>Service</literal> object if the <literal>spec.externalIPs</literal> field is not specified.</simpara>
</listitem>
<listitem>
<simpara>Attach an ExternalIP to the service.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>If you are creating a new service, specify the <literal>spec.externalIPs</literal> field and provide an array of one or more valid IP addresses. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: svc-with-externalip
spec:
  ...
  externalIPs:
  - 192.174.120.10</programlisting>
</listitem>
<listitem>
<simpara>If you are attaching an ExternalIP to an existing service, enter the following command. Replace <literal>&lt;name&gt;</literal> with the service name. Replace <literal>&lt;ip_address&gt;</literal> with a valid ExternalIP address. You can provide multiple IP addresses separated by commas.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch svc &lt;name&gt; -p \
  '{
    "spec": {
      "externalIPs": [ "&lt;ip_address&gt;" ]
    }
  }'</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch svc mysql-55-rhel7 -p '{"spec":{"externalIPs":["192.174.120.10"]}}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"mysql-55-rhel7" patched</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To confirm that an ExternalIP address is attached to the service, enter the following command. If you specified an ExternalIP for a new service, you must create the service first.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME               CLUSTER-IP      EXTERNAL-IP     PORT(S)    AGE
mysql-55-rhel7     172.30.131.89   192.174.120.10  3306/TCP   13m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-ingress-cluster-traffic-service-external-ip-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-externalip">Configuring ExternalIPs for services</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-ingress-cluster-traffic-nodeport">
<title>Configuring ingress cluster traffic using a NodePort</title>

<simpara>OpenShift Container Platform provides methods for communicating from
outside the cluster with services running in the cluster. This method uses a
<literal>NodePort</literal>.</simpara>
<section xml:id="nw-using-nodeport_configuring-ingress-cluster-traffic-nodeport">
<title>Using a NodePort to get traffic into the cluster</title>
<simpara>Use a <literal>NodePort</literal>-type <literal>Service</literal> resource to expose a service on a specific port
on all nodes in the cluster. The port is specified in the <literal>Service</literal> resource&#8217;s
<literal>.spec.ports[*].nodePort</literal> field.</simpara>
<important>
<simpara>Using a node port requires additional port resources.</simpara>
</important>
<simpara>A <literal>NodePort</literal> exposes the service on a static port on the node&#8217;s IP address.
<literal>NodePort</literal>s are in the <literal>30000</literal> to <literal>32767</literal> range by default, which means a
<literal>NodePort</literal> is unlikely to match a service&#8217;s intended port. For example, port
<literal>8080</literal> may be exposed as port <literal>31020</literal> on the node.</simpara>
<simpara>The administrator must ensure the external IP addresses are routed to the nodes.</simpara>
<simpara><literal>NodePort</literal>s and external IPs are independent and both can be used
concurrently.</simpara>
<note>
<simpara>The procedures in this section require prerequisites performed by the cluster
administrator.</simpara>
</note>
</section>
<section xml:id="_prerequisites-5">
<title>Prerequisites</title>
<simpara>Before starting the following procedures, the administrator must:</simpara>
<itemizedlist>
<listitem>
<simpara>Set up the external port to the cluster networking environment so that requests
can reach the cluster.</simpara>
</listitem>
<listitem>
<simpara>Make sure there is at least one user with cluster admin role. To add this role
to a user, run the following command:</simpara>
<screen>$ oc adm policy add-cluster-role-to-user cluster-admin &lt;user_name&gt;</screen>
</listitem>
<listitem>
<simpara>Have an OpenShift Container Platform cluster with at least one master and at least one node
and a system outside the cluster that has network access to the cluster. This
procedure assumes that the external system is on the same subnet as the cluster.
The additional networking required for external systems on a different subnet is
out-of-scope for this topic.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-creating-project-and-service_configuring-ingress-cluster-traffic-nodeport">
<title>Creating a project and service</title>
<simpara>If the project and service that you want to expose do not exist, first create
the project, then the service.</simpara>
<simpara>If the project and service already exist, skip to the procedure on exposing the
service to create a route.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal>oc</literal> CLI and log in as a cluster administrator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new project for your service by running the <literal>oc new-project</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project myproject</programlisting>
</listitem>
<listitem>
<simpara>Use the <literal>oc new-app</literal> command to create your service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-app nodejs:12~https://github.com/sclorg/nodejs-ex.git</programlisting>
</listitem>
<listitem>
<simpara>To verify that the service was created, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc -n myproject</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
nodejs-ex   ClusterIP   172.30.197.157   &lt;none&gt;        8080/TCP   70s</programlisting>
</para>
</formalpara>
<simpara>By default, the new service does not have an external IP address.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-exposing-service_configuring-ingress-cluster-traffic-nodeport">
<title>Exposing the service by creating a route</title>
<simpara>You can expose the service as a route by using the <literal>oc expose</literal> command.</simpara>
<formalpara>
<title>Procedure</title>
<para>To expose the service:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Log in to OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>Log in to the project where the service you want to expose is located:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project myproject</programlisting>
</listitem>
<listitem>
<simpara>To expose a node port for the application, modify the custom resource definition (CRD) of a service by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit svc &lt;service_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  ports:
  - name: 8443-tcp
    nodePort: 30327 <co xml:id="CO236-1"/>
    port: 8443
    protocol: TCP
    targetPort: 8443
  sessionAffinity: None
  type: NodePort <co xml:id="CO236-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO236-1">
<para>Optional: Specify the node port range for the application. By default, OpenShift Container Platform selects an available port in the <literal>30000-32767</literal> range.</para>
</callout>
<callout arearefs="CO236-2">
<para>Define the service type.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: To confirm the service is available with a node port exposed, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc -n myproject</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)          AGE
nodejs-ex           ClusterIP   172.30.217.127   &lt;none&gt;        3306/TCP         9m44s
nodejs-ex-ingress   NodePort    172.30.107.72    &lt;none&gt;        3306:31345/TCP   39s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: To remove the service created automatically by the <literal>oc new-app</literal> command, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete svc nodejs-ex</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To check that the service node port is updated with a port in the <literal>30000-32767</literal> range, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc</programlisting>
<simpara>In the following example output, the updated port is <literal>30327</literal>:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME    TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
httpd   NodePort   172.xx.xx.xx    &lt;none&gt;        8443:30327/TCP   109s</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-ingress-cluster-traffic-nodeport-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="configuring-node-port-service-range">Configuring the node port service range</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-ingress-cluster-traffic-lb-allowed-source-ranges">
<title>Configuring ingress cluster traffic using load balancer allowed source ranges</title>

<simpara>You can specify a list of IP address ranges for the <literal>IngressController</literal>. This restricts access to the load balancer service when the <literal>endpointPublishingStrategy</literal> is <literal>LoadBalancerService</literal>.</simpara>
<section xml:id="nw-configuring-lb-allowed-source-ranges_configuring-ingress-cluster-traffic-lb-allowed-source-ranges">
<title>Configuring load balancer allowed source ranges</title>
<simpara>You can enable and configure the <literal>spec.endpointPublishingStrategy.loadBalancer.allowedSourceRanges</literal> field. By configuring load balancer allowed source ranges, you can limit the access to the load balancer for the Ingress Controller to a specified list of IP address ranges. The Ingress Operator reconciles the load balancer Service and sets the <literal>spec.loadBalancerSourceRanges</literal> field based on <literal>AllowedSourceRanges</literal>.</simpara>
<note>
<simpara>If you have already set the <literal>spec.loadBalancerSourceRanges</literal> field or the load balancer service anotation <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> in a previous version of OpenShift Container Platform, Ingress Controller starts reporting <literal>Progressing=True</literal> after an upgrade. To fix this, set <literal>AllowedSourceRanges</literal> that overwrites the <literal>spec.loadBalancerSourceRanges</literal> field and clears the <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> annotation. Ingress Controller starts reporting <literal>Progressing=False</literal> again.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have a deployed Ingress Controller on a running cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the allowed source ranges API for the Ingress Controller by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"loadBalancer":{"allowedSourceRanges":["0.0.0.0/0"]}}}}' <co xml:id="CO237-1"/></programlisting>
<calloutlist>
<callout arearefs="CO237-1">
<para>The example value <literal>0.0.0.0/0</literal> specifies the allowed source range.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-configuring-lb-allowed-source-ranges-migration_configuring-ingress-cluster-traffic-lb-allowed-source-ranges">
<title>Migrating to load balancer allowed source ranges</title>
<simpara>If you have already set the annotation <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal>, you can migrate to load balancer allowed source ranges. When you set the <literal>AllowedSourceRanges</literal>, the Ingress Controller sets the <literal>spec.loadBalancerSourceRanges</literal> field based on the <literal>AllowedSourceRanges</literal> value and unsets the <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> annotation.</simpara>
<note>
<simpara>If you have already set the <literal>spec.loadBalancerSourceRanges</literal> field or the load balancer service anotation <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> in a previous version of OpenShift Container Platform, the Ingress Controller starts reporting <literal>Progressing=True</literal> after an upgrade. To fix this, set <literal>AllowedSourceRanges</literal> that overwrites the <literal>spec.loadBalancerSourceRanges</literal> field and clears the <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> annotation. The Ingress Controller starts reporting <literal>Progressing=False</literal> again.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have set the <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> annotation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Ensure that the <literal>service.beta.kubernetes.io/load-balancer-source-ranges</literal> is set:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc router-default -n openshift-ingress -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  annotations:
    service.beta.kubernetes.io/load-balancer-source-ranges: 192.168.0.1/32</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Ensure that the <literal>spec.loadBalancerSourceRanges</literal> field is unset:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc router-default -n openshift-ingress -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">...
spec:
  loadBalancerSourceRanges:
  - 0.0.0.0/0
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Update your cluster to OpenShift Container Platform 4.14.</simpara>
</listitem>
<listitem>
<simpara>Set the allowed source ranges API for the <literal>ingresscontroller</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontroller/default \
    --type=merge --patch='{"spec":{"endpointPublishingStrategy": \
    {"loadBalancer":{"allowedSourceRanges":["0.0.0.0/0"]}}}}' <co xml:id="CO238-1"/></programlisting>
<calloutlist>
<callout arearefs="CO238-1">
<para>The example value <literal>0.0.0.0/0</literal> specifies the allowed source range.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="_additional-resources-6" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/updating_clusters/#understanding-openshift-updates">Introduction to OpenShift updates</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_kubernetes-nmstate">
<title>Kubernetes NMState</title>
<section xml:id="k8s-nmstate-about-the-k8s-nmstate-operator">
<title>About the Kubernetes NMState Operator</title>

<simpara>The Kubernetes NMState Operator provides a Kubernetes API for performing state-driven network configuration across the OpenShift Container Platform cluster&#8217;s nodes with NMState. The Kubernetes NMState Operator provides users with functionality to configure various network interface types, DNS, and routing on cluster nodes. Additionally, the daemons on the cluster nodes periodically report on the state of each node&#8217;s network interfaces to the API server.</simpara>
<important>
<simpara>Red Hat supports the Kubernetes NMState Operator in production environments on bare-metal, IBM Power&#174;, IBM Z&#174;, IBM&#174; LinuxONE, VMware vSphere, and OpenStack installations.</simpara>
</important>
<simpara>Before you can use NMState with OpenShift Container Platform, you must install the Kubernetes NMState Operator.</simpara>
<note>
<simpara>The Kubernetes NMState Operator updates the network configuration of a secondary NIC. It cannot update the network configuration of the primary NIC or the <literal>br-ex</literal> bridge.</simpara>
</note>
<simpara>OpenShift Container Platform uses <link xlink:href="https://nmstate.github.io/"><literal>nmstate</literal></link> to report on and configure the state of the node network. This makes it possible to modify the network policy configuration, such as by creating a Linux bridge on all nodes, by applying a single configuration manifest to the cluster.</simpara>
<simpara>Node networking is monitored and updated by the following objects:</simpara>
<variablelist>
<varlistentry>
<term><literal>NodeNetworkState</literal></term>
<listitem>
<simpara>Reports the state of the network on that node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>NodeNetworkConfigurationPolicy</literal></term>
<listitem>
<simpara>Describes the requested network configuration on nodes. You update the node network configuration, including adding and removing interfaces, by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest to the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>NodeNetworkConfigurationEnactment</literal></term>
<listitem>
<simpara>Reports the network policies enacted upon each node.</simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="installing-the-kubernetes-nmstate-operator-cli">
<title>Installing the Kubernetes NMState Operator</title>
<simpara>You can install the Kubernetes NMState Operator by using the web console or the CLI.</simpara>
<section xml:id="installing-the-kubernetes-nmstate-operator-web-console_k8s-nmstate-operator">
<title>Installing the Kubernetes NMState Operator by using the web console</title>
<simpara>You can install the Kubernetes NMState Operator by using the web console. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Select <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the search field below <emphasis role="strong">All Items</emphasis>, enter <literal>nmstate</literal> and click <emphasis role="strong">Enter</emphasis> to search for the Kubernetes NMState Operator.</simpara>
</listitem>
<listitem>
<simpara>Click on the Kubernetes NMState Operator search result.</simpara>
</listitem>
<listitem>
<simpara>Click on <emphasis role="strong">Install</emphasis> to open the <emphasis role="strong">Install Operator</emphasis> window.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis> to install the Operator.</simpara>
</listitem>
<listitem>
<simpara>After the Operator finishes installing, click <emphasis role="strong">View Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create Instance</emphasis> to open the dialog box for creating an instance of <literal>kubernetes-nmstate</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Name</emphasis> field of the dialog box, ensure the name of the instance is <literal>nmstate.</literal></simpara>
<note>
<simpara>The name restriction is a known issue. The instance is a singleton for the entire cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Accept the default settings and click <emphasis role="strong">Create</emphasis> to create the instance.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Summary</title>
<para>Once complete, the Operator has deployed the NMState State Controller as a daemon set across all of the cluster nodes.</para>
</formalpara>
</section>
<section xml:id="installing-the-kubernetes-nmstate-operator-CLI_k8s-nmstate-operator">
<title>Installing the Kubernetes NMState Operator using the CLI</title>
<simpara>You can install the Kubernetes NMState Operator by using the OpenShift CLI (<literal>oc)</literal>. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>nmstate</literal> Operator namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  labels:
    kubernetes.io/metadata.name: openshift-nmstate
    name: openshift-nmstate
  name: openshift-nmstate
spec:
  finalizers:
  - kubernetes
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>OperatorGroup</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  annotations:
    olm.providedAPIs: NMState.v1.nmstate.io
  name: openshift-nmstate
  namespace: openshift-nmstate
spec:
  targetNamespaces:
  - openshift-nmstate
EOF</programlisting>
</listitem>
<listitem>
<simpara>Subscribe to the <literal>nmstate</literal> Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  labels:
    operators.coreos.com/kubernetes-nmstate-operator.openshift-nmstate: ""
  name: kubernetes-nmstate-operator
  namespace: openshift-nmstate
spec:
  channel: stable
  installPlanApproval: Automatic
  name: kubernetes-nmstate-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create instance of the <literal>nmstate</literal> operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: nmstate.io/v1
kind: NMState
metadata:
  name: nmstate
EOF</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Confirm that the deployment for the <literal>nmstate</literal> operator is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc get clusterserviceversion -n openshift-nmstate \
 -o custom-columns=Name:.metadata.name,Phase:.status.phase</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name                                             Phase
kubernetes-nmstate-operator.4.14.0-202210210157   Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="k8s-nmstate-updating-node-network-config">
<title>Observing and updating the node network state and configuration</title>

<section xml:id="virt-viewing-network-state-of-node_k8s_nmstate-updating-node-network-config">
<title>Viewing the network state of a node by using the CLI</title>
<simpara>Node network state is the network configuration for all nodes in the cluster. A <literal>NodeNetworkState</literal> object exists on every node in the cluster. This object is periodically updated and captures the state of the network for that node.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List all the <literal>NodeNetworkState</literal> objects in the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nns</programlisting>
</listitem>
<listitem>
<simpara>Inspect a <literal>NodeNetworkState</literal> object to view the network on that node. The output in this example has been redacted for clarity:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nns node01 -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkState
metadata:
  name: node01 <co xml:id="CO239-1"/>
status:
  currentState: <co xml:id="CO239-2"/>
    dns-resolver:
# ...
    interfaces:
# ...
    route-rules:
# ...
    routes:
# ...
  lastSuccessfulUpdateTime: "2020-01-31T12:14:00Z" <co xml:id="CO239-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO239-1">
<para>The name of the <literal>NodeNetworkState</literal> object is taken from the node.</para>
</callout>
<callout arearefs="CO239-2">
<para>The <literal>currentState</literal> contains the complete network configuration for the node, including DNS, interfaces, and routes.</para>
</callout>
<callout arearefs="CO239-3">
<para>Timestamp of the last successful update. This is updated periodically as long as the node is reachable and can be used to evalute the freshness of the report.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-viewing-network-state-of-node-console_k8s_nmstate-updating-node-network-config">
<title>Viewing the network state of a node from the web console</title>
<simpara>As an administrator, you can use the OpenShift Container Platform web console to observe <literal>NodeNetworkState</literal> resources and network interfaces, and access network details.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Networking</emphasis> → <emphasis role="strong">NodeNetworkState</emphasis>.</simpara>
<simpara>In the <emphasis role="strong">NodeNetworkState</emphasis> page, you can view the list of <literal>NodeNetworkState</literal> resources and the corresponding interfaces that are created on the nodes. You can use <emphasis role="strong">Filter</emphasis> based on <emphasis role="strong">Interface state</emphasis>, <emphasis role="strong">Interface type</emphasis>, and <emphasis role="strong">IP</emphasis>, or the search bar based on criteria <emphasis role="strong">Name</emphasis> or <emphasis role="strong">Label</emphasis>, to narrow down the displayed <literal>NodeNetworkState</literal> resources.</simpara>
</listitem>
<listitem>
<simpara>To access the detailed information about <literal>NodeNetworkState</literal> resource, click the <literal>NodeNetworkState</literal> resource name listed in the <emphasis role="strong">Name</emphasis> column .</simpara>
</listitem>
<listitem>
<simpara>to expand and view the <emphasis role="strong">Network Details</emphasis> section for the <literal>NodeNetworkState</literal> resource, click the <emphasis role="strong">&gt;</emphasis> icon . Alternatively, you can click on each interface type under the <emphasis role="strong">Network interface</emphasis> column to view the network details.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-node-network-config-console_k8s_nmstate-updating-node-network-config">
<title>Managing policy from the web console</title>
<simpara>You can update the node network configuration, such as adding or removing interfaces from nodes, by applying <literal>NodeNetworkConfigurationPolicy</literal> manifests to the cluster.
Manage the policy from the web console by accessing the list of created policies in the <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis> page under the <emphasis role="strong">Networking</emphasis> menu. This page enables you to create, update, monitor, and delete the policies.</simpara>
<section xml:id="virt-monitor-node-network-config-console_k8s_nmstate-updating-node-network-config">
<title>Monitoring the policy status</title>
<simpara>You can monitor the policy status from the <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis> page. This page displays all the policies created in the cluster in a tabular format, with the following columns:</simpara>
<variablelist>
<varlistentry>
<term>Name</term>
<listitem>
<simpara>The name of the policy created.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Matched nodes</term>
<listitem>
<simpara>The count of nodes where the policies are applied. This could be either a subset of nodes based on the node selector or all the nodes on the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Node network state</term>
<listitem>
<simpara>The enactment state of the matched nodes. You can click on the enactment state and view detailed information on the status.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>To find the desired policy, you can filter the list either based on enactment state by using the <emphasis role="strong">Filter</emphasis> option, or by using the search option.</simpara>
</section>
<section xml:id="virt-create-node-network-config-console_k8s_nmstate-updating-node-network-config">
<title>Creating a policy</title>
<simpara>You can create a policy by using either a form or YAML in the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Networking</emphasis> → <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis> page, click <emphasis role="strong">Create</emphasis>, and select <emphasis role="strong">From Form</emphasis> option.</simpara>
<simpara>In case there are no existing policies, you can alternatively click <emphasis role="strong">Create NodeNetworkConfigurationPolicy</emphasis> to createa policy using form.</simpara>
<note>
<simpara>To create policy using YAML, click <emphasis role="strong">Create</emphasis>, and select <emphasis role="strong">With YAML</emphasis> option. The following steps are applicable to create a policy only by using form.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: Check the <emphasis role="strong">Apply this NodeNetworkConfigurationPolicy only to specific subsets of nodes using the node selector</emphasis> checkbox to specify the nodes where the policy must be applied.</simpara>
</listitem>
<listitem>
<simpara>Enter the policy name in the <emphasis role="strong">Policy name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: Enter the description of the policy in the <emphasis role="strong">Description</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: In the <emphasis role="strong">Policy Interface(s)</emphasis> section, a bridge interface is added by default with preset values in editable fields. Edit the values by executing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Enter the name of the interface in <emphasis role="strong">Interface name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Select the network state from <emphasis role="strong">Network state</emphasis> dropdown. The default selected value is <emphasis role="strong">Up</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the type of interface from <emphasis role="strong">Type</emphasis> dropdown. The available values are <emphasis role="strong">Bridge</emphasis>, <emphasis role="strong">Bonding</emphasis>, and <emphasis role="strong">Ethernet</emphasis>. The default selected value is <emphasis role="strong">Bridge</emphasis>.</simpara>
<note>
<simpara>Addition of a VLAN interface by using the form is not supported. To add a VLAN interface, you must use YAML to create the policy. Once added, you cannot edit the policy by using form.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: In the IP configuration section, check <emphasis role="strong">IPv4</emphasis> checkbox to assign an IPv4 address to the interface, and configure the IP address assignment details:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Click <emphasis role="strong">IP address</emphasis> to configure the interface with a static IP address, or <emphasis role="strong">DHCP</emphasis> to auto-assign an IP address.</simpara>
</listitem>
<listitem>
<simpara>If you have selected <emphasis role="strong">IP address</emphasis> option, enter the IPv4 address in <emphasis role="strong">IPV4 address</emphasis> field, and enter the prefix length in <emphasis role="strong">Prefix length</emphasis> field.</simpara>
<simpara>If you have selected <emphasis role="strong">DHCP</emphasis> option, uncheck the options that you want to disable. The available options are <emphasis role="strong">Auto-DNS</emphasis>, <emphasis role="strong">Auto-routes</emphasis>, and <emphasis role="strong">Auto-gateway</emphasis>. All the options are selected by default.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: Enter the port number in <emphasis role="strong">Port</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: Check the checkbox <emphasis role="strong">Enable STP</emphasis> to enable STP.</simpara>
</listitem>
<listitem>
<simpara>Optional: To add an interface to the policy, click <emphasis role="strong">Add another interface to the policy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Optional: To remove an interface from the policy, click <inlinemediaobject>
<imageobject>
<imagedata fileref="images/fa-minus-circle.svg"/>
</imageobject>
<textobject><phrase>minus</phrase></textobject>
</inlinemediaobject> icon next to the interface.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>Alternatively, you can click <emphasis role="strong">Edit YAML</emphasis> on the top of the page to continue editing the form using YAML.</simpara>
</note>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis> to complete policy creation.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="_updating-the-policy">
<title>Updating the policy</title>
<section xml:id="virt-update-node-network-config-form_k8s_nmstate-updating-node-network-config">
<title>Updating the policy by using form</title>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Networking</emphasis> → <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis> page, click the <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> icon placed next to the policy you want to edit, and click <emphasis role="strong">Edit</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Edit the fields that you want to update.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>Addition of a VLAN interface using the form is not supported. To add a VLAN interface, you must use YAML to create the policy. Once added, you cannot edit the policy using form.</simpara>
</note>
</section>
<section xml:id="virt-update-node-network-config-yaml_k8s_nmstate-updating-node-network-config">
<title>Updating the policy by using YAML</title>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Networking</emphasis> → <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis> page, click the policy name under the <emphasis role="strong">Name</emphasis> column for the policy you want to edit.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">YAML</emphasis> tab, and edit the YAML.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-delete-node-network-config_k8s_nmstate-updating-node-network-config">
<title>Deleting the policy</title>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Networking</emphasis> → <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">NodeNetworkConfigurationPolicy</emphasis> page, click the <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> icon placed next to the policy you want to delete, and click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the pop-up window, enter the policy name to confirm deletion, and click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-manage-nncp-cli">
<title>Managing policy by using the CLI</title>
<section xml:id="virt-creating-interface-on-nodes_k8s_nmstate-updating-node-network-config">
<title>Creating an interface on nodes</title>
<simpara>Create an interface on nodes in the cluster by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest to the cluster. The manifest details the requested configuration for the interface.</simpara>
<simpara>By default, the manifest applies to all nodes in the cluster. To add the interface to specific nodes, add the <literal>spec: nodeSelector</literal> parameter and the appropriate <literal>&lt;key&gt;:&lt;value&gt;</literal> for your node selector.</simpara>
<simpara>You can configure multiple nmstate-enabled nodes concurrently. The configuration applies to 50% of the nodes in parallel. This strategy prevents the entire cluster from being unavailable if the network connection fails. To apply the policy configuration in parallel to a specific portion of the cluster, use the <literal>maxUnavailable</literal> field.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>NodeNetworkConfigurationPolicy</literal> manifest. The following example configures a Linux bridge on all worker nodes and configures the DNS resolver:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <co xml:id="CO240-1"/>
spec:
  nodeSelector: <co xml:id="CO240-2"/>
    node-role.kubernetes.io/worker: "" <co xml:id="CO240-3"/>
  maxUnavailable: 3 <co xml:id="CO240-4"/>
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with eth1 as a port <co xml:id="CO240-5"/>
        type: linux-bridge
        state: up
        ipv4:
          dhcp: true
          enabled: true
          auto-dns: false
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: eth1
    dns-resolver: <co xml:id="CO240-6"/>
      config:
        search:
        - example.com
        - example.org
        server:
        - 8.8.8.8</programlisting>
<calloutlist>
<callout arearefs="CO240-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO240-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO240-3">
<para>This example uses the <literal>node-role.kubernetes.io/worker: ""</literal> node selector to select all worker nodes in the cluster.</para>
</callout>
<callout arearefs="CO240-4">
<para>Optional: Specifies the maximum number of nmstate-enabled nodes that the policy configuration can be applied to concurrently. This parameter can be set to either a percentage value (string), for example, <literal>"10%"</literal>, or an absolute value (number), such as <literal>3</literal>.</para>
</callout>
<callout arearefs="CO240-5">
<para>Optional: Human-readable description for the interface.</para>
</callout>
<callout arearefs="CO240-6">
<para>Optional: Specifies the search and server settings for the DNS server.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the node network policy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f br1-eth1-policy.yaml <co xml:id="CO241-1"/></programlisting>
<calloutlist>
<callout arearefs="CO241-1">
<para>File name of the node network configuration policy manifest.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<bridgehead xml:id="_additional-resources-7" role="_additional-resources" renderas="sect3">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.xml#virt-example-nmstate-multiple-interfaces_k8s_nmstate-updating-node-network-config">Example for creating multiple interfaces in the same policy</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.xml#virt-example-nmstate-IP-management_k8s_nmstate-updating-node-network-config">Examples of different IP management methods in policies</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-confirming-policy-updates-on-nodes_k8s_nmstate-updating-node-network-config">
<title>Confirming node network policy updates on nodes</title>
<simpara>A <literal>NodeNetworkConfigurationPolicy</literal> manifest describes your requested network configuration for nodes in the cluster.
The node network policy includes your requested network configuration and the status of execution of the policy on the cluster as a whole.</simpara>
<simpara>When you apply a node network policy, a <literal>NodeNetworkConfigurationEnactment</literal> object is created for every node in the cluster. The node network configuration enactment is a read-only object that represents the status of execution of the policy on that node.
If the policy fails to be applied on the node, the enactment for that node includes a traceback for troubleshooting.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To confirm that a policy has been applied to the cluster, list the policies and their status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nncp</programlisting>
</listitem>
<listitem>
<simpara>Optional: If a policy is taking longer than expected to successfully configure, you can inspect the requested state and status conditions of a particular policy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nncp &lt;policy&gt; -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: If a policy is taking longer than expected to successfully configure on all nodes, you can list the status of the enactments on the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nnce</programlisting>
</listitem>
<listitem>
<simpara>Optional: To view the configuration of a particular enactment, including any error reporting for a failed configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nnce &lt;node&gt;.&lt;policy&gt; -o yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-removing-interface-from-nodes_k8s_nmstate-updating-node-network-config">
<title>Removing an interface from nodes</title>
<simpara>You can remove an interface from one or more nodes in the cluster by editing the <literal>NodeNetworkConfigurationPolicy</literal> object and setting the <literal>state</literal> of the interface to <literal>absent</literal>.</simpara>
<simpara>Removing an interface from a node does not automatically restore the node network configuration to a previous state. If you want to restore the previous state, you will need to define that node network configuration in the policy.</simpara>
<simpara>If you remove a bridge or bonding interface, any node NICs in the cluster that were previously attached or subordinate to that bridge or bonding interface are placed in a <literal>down</literal> state and become unreachable. To avoid losing connectivity, configure the node NIC in the same policy so that it has a status of <literal>up</literal> and either DHCP or a static IP address.</simpara>
<note>
<simpara>Deleting the node network policy that added an interface does not change the configuration of the policy on the node.
Although a <literal>NodeNetworkConfigurationPolicy</literal> is an object in the cluster, it only represents the requested configuration.<?asciidoc-br?>
Similarly, removing an interface does not delete the policy.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Update the <literal>NodeNetworkConfigurationPolicy</literal> manifest used to create the interface. The following example removes a Linux bridge and configures the <literal>eth1</literal> NIC with DHCP to avoid losing connectivity:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: &lt;br1-eth1-policy&gt; <co xml:id="CO242-1"/>
spec:
  nodeSelector: <co xml:id="CO242-2"/>
    node-role.kubernetes.io/worker: "" <co xml:id="CO242-3"/>
  desiredState:
    interfaces:
    - name: br1
      type: linux-bridge
      state: absent <co xml:id="CO242-4"/>
    - name: eth1 <co xml:id="CO242-5"/>
      type: ethernet <co xml:id="CO242-6"/>
      state: up <co xml:id="CO242-7"/>
      ipv4:
        dhcp: true <co xml:id="CO242-8"/>
        enabled: true <co xml:id="CO242-9"/></programlisting>
<calloutlist>
<callout arearefs="CO242-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO242-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO242-3">
<para>This example uses the <literal>node-role.kubernetes.io/worker: ""</literal> node selector to select all worker nodes in the cluster.</para>
</callout>
<callout arearefs="CO242-4">
<para>Changing the state to <literal>absent</literal> removes the interface.</para>
</callout>
<callout arearefs="CO242-5">
<para>The name of the interface that is to be unattached from the bridge interface.</para>
</callout>
<callout arearefs="CO242-6">
<para>The type of interface. This example creates an Ethernet networking interface.</para>
</callout>
<callout arearefs="CO242-7">
<para>The requested state for the interface.</para>
</callout>
<callout arearefs="CO242-8">
<para>Optional: If you do not use <literal>dhcp</literal>, you can either set a static IP or leave the interface without an IP address.</para>
</callout>
<callout arearefs="CO242-9">
<para>Enables <literal>ipv4</literal> in this example.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Update the policy on the node and remove the interface:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;br1-eth1-policy.yaml&gt; <co xml:id="CO243-1"/></programlisting>
<calloutlist>
<callout arearefs="CO243-1">
<para>File name of the policy manifest.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-nmstate-example-policy-configurations">
<title>Example policy configurations for different interfaces</title>
<section xml:id="virt-example-bridge-nncp_k8s_nmstate-updating-node-network-config">
<title>Example: Linux bridge interface node network configuration policy</title>
<simpara>Create a Linux bridge interface on nodes in the cluster by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest
to the cluster.</simpara>
<simpara>The following YAML file is an example of a manifest for a Linux bridge interface.
It includes samples values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <co xml:id="CO244-1"/>
spec:
  nodeSelector: <co xml:id="CO244-2"/>
    kubernetes.io/hostname: &lt;node01&gt; <co xml:id="CO244-3"/>
  desiredState:
    interfaces:
      - name: br1 <co xml:id="CO244-4"/>
        description: Linux bridge with eth1 as a port <co xml:id="CO244-5"/>
        type: linux-bridge <co xml:id="CO244-6"/>
        state: up <co xml:id="CO244-7"/>
        ipv4:
          dhcp: true <co xml:id="CO244-8"/>
          enabled: true <co xml:id="CO244-9"/>
        bridge:
          options:
            stp:
              enabled: false <co xml:id="CO244-10"/>
          port:
            - name: eth1 <co xml:id="CO244-11"/></programlisting>
<calloutlist>
<callout arearefs="CO244-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO244-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO244-3">
<para>This example uses a <literal>hostname</literal> node selector.</para>
</callout>
<callout arearefs="CO244-4">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO244-5">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO244-6">
<para>The type of interface. This example creates a bridge.</para>
</callout>
<callout arearefs="CO244-7">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO244-8">
<para>Optional: If you do not use <literal>dhcp</literal>, you can either set a static IP or leave the interface without an IP address.</para>
</callout>
<callout arearefs="CO244-9">
<para>Enables <literal>ipv4</literal> in this example.</para>
</callout>
<callout arearefs="CO244-10">
<para>Disables <literal>stp</literal> in this example.</para>
</callout>
<callout arearefs="CO244-11">
<para>The node NIC to which the bridge attaches.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-vlan-nncp_k8s_nmstate-updating-node-network-config">
<title>Example: VLAN interface node network configuration policy</title>
<simpara>Create a VLAN interface on nodes in the cluster by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest
to the cluster.</simpara>
<simpara>The following YAML file is an example of a manifest for a VLAN interface.
It includes samples values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: vlan-eth1-policy <co xml:id="CO245-1"/>
spec:
  nodeSelector: <co xml:id="CO245-2"/>
    kubernetes.io/hostname: &lt;node01&gt; <co xml:id="CO245-3"/>
  desiredState:
    interfaces:
    - name: eth1.102 <co xml:id="CO245-4"/>
      description: VLAN using eth1 <co xml:id="CO245-5"/>
      type: vlan <co xml:id="CO245-6"/>
      state: up <co xml:id="CO245-7"/>
      vlan:
        base-iface: eth1 <co xml:id="CO245-8"/>
        id: 102 <co xml:id="CO245-9"/></programlisting>
<calloutlist>
<callout arearefs="CO245-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO245-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO245-3">
<para>This example uses a <literal>hostname</literal> node selector.</para>
</callout>
<callout arearefs="CO245-4">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO245-5">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO245-6">
<para>The type of interface. This example creates a VLAN.</para>
</callout>
<callout arearefs="CO245-7">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO245-8">
<para>The node NIC to which the VLAN is attached.</para>
</callout>
<callout arearefs="CO245-9">
<para>The VLAN tag.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-bond-nncp_k8s_nmstate-updating-node-network-config">
<title>Example: Bond interface node network configuration policy</title>
<simpara>Create a bond interface on nodes in the cluster by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest
to the cluster.</simpara>
<note>
<simpara>OpenShift Container Platform only supports the following bond modes:</simpara>
<itemizedlist>
<listitem>
<simpara>mode=1 active-backup<?asciidoc-br?></simpara>
</listitem>
<listitem>
<simpara>mode=2 balance-xor<?asciidoc-br?></simpara>
</listitem>
<listitem>
<simpara>mode=4 802.3ad<?asciidoc-br?></simpara>
</listitem>
<listitem>
<simpara>mode=5 balance-tlb<?asciidoc-br?></simpara>
</listitem>
<listitem>
<simpara>mode=6 balance-alb</simpara>
</listitem>
</itemizedlist>
</note>
<simpara>The following YAML file is an example of a manifest for a bond interface.
It includes samples values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: bond0-eth1-eth2-policy <co xml:id="CO246-1"/>
spec:
  nodeSelector: <co xml:id="CO246-2"/>
    kubernetes.io/hostname: &lt;node01&gt; <co xml:id="CO246-3"/>
  desiredState:
    interfaces:
    - name: bond0 <co xml:id="CO246-4"/>
      description: Bond with ports eth1 and eth2 <co xml:id="CO246-5"/>
      type: bond <co xml:id="CO246-6"/>
      state: up <co xml:id="CO246-7"/>
      ipv4:
        dhcp: true <co xml:id="CO246-8"/>
        enabled: true <co xml:id="CO246-9"/>
      link-aggregation:
        mode: active-backup <co xml:id="CO246-10"/>
        options:
          miimon: '140' <co xml:id="CO246-11"/>
        port: <co xml:id="CO246-12"/>
        - eth1
        - eth2
      mtu: 1450 <co xml:id="CO246-13"/></programlisting>
<calloutlist>
<callout arearefs="CO246-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO246-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO246-3">
<para>This example uses a <literal>hostname</literal> node selector.</para>
</callout>
<callout arearefs="CO246-4">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO246-5">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO246-6">
<para>The type of interface. This example creates a bond.</para>
</callout>
<callout arearefs="CO246-7">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO246-8">
<para>Optional: If you do not use <literal>dhcp</literal>, you can either set a static IP or leave the interface without an IP address.</para>
</callout>
<callout arearefs="CO246-9">
<para>Enables <literal>ipv4</literal> in this example.</para>
</callout>
<callout arearefs="CO246-10">
<para>The driver mode for the bond. This example uses an active backup mode.</para>
</callout>
<callout arearefs="CO246-11">
<para>Optional: This example uses miimon to inspect the bond link every 140ms.</para>
</callout>
<callout arearefs="CO246-12">
<para>The subordinate node NICs in the bond.</para>
</callout>
<callout arearefs="CO246-13">
<para>Optional: The maximum transmission unit (MTU) for the bond. If not specified, this value is set to <literal>1500</literal> by default.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-ethernet-nncp_k8s_nmstate-updating-node-network-config">
<title>Example: Ethernet interface node network configuration policy</title>
<simpara>Configure an Ethernet interface on nodes in the cluster by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest to the cluster.</simpara>
<simpara>The following YAML file is an example of a manifest for an Ethernet interface.
It includes sample values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: eth1-policy <co xml:id="CO247-1"/>
spec:
  nodeSelector: <co xml:id="CO247-2"/>
    kubernetes.io/hostname: &lt;node01&gt; <co xml:id="CO247-3"/>
  desiredState:
    interfaces:
    - name: eth1 <co xml:id="CO247-4"/>
      description: Configuring eth1 on node01 <co xml:id="CO247-5"/>
      type: ethernet <co xml:id="CO247-6"/>
      state: up <co xml:id="CO247-7"/>
      ipv4:
        dhcp: true <co xml:id="CO247-8"/>
        enabled: true <co xml:id="CO247-9"/></programlisting>
<calloutlist>
<callout arearefs="CO247-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO247-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO247-3">
<para>This example uses a <literal>hostname</literal> node selector.</para>
</callout>
<callout arearefs="CO247-4">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO247-5">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO247-6">
<para>The type of interface. This example creates an Ethernet networking interface.</para>
</callout>
<callout arearefs="CO247-7">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO247-8">
<para>Optional: If you do not use <literal>dhcp</literal>, you can either set a static IP or leave the interface without an IP address.</para>
</callout>
<callout arearefs="CO247-9">
<para>Enables <literal>ipv4</literal> in this example.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-nmstate-multiple-interfaces_k8s_nmstate-updating-node-network-config">
<title>Example: Multiple interfaces in the same node network configuration policy</title>
<simpara>You can create multiple interfaces in the same node network configuration policy. These interfaces can reference each other, allowing you to build and deploy a network configuration by using a single policy manifest.</simpara>
<simpara>The following example YAML file creates a bond that is named <literal>bond10</literal> across two NICs and VLAN that is named <literal>bond10.103</literal> that connects to the bond.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: bond-vlan <co xml:id="CO248-1"/>
spec:
  nodeSelector: <co xml:id="CO248-2"/>
    kubernetes.io/hostname: &lt;node01&gt; <co xml:id="CO248-3"/>
  desiredState:
    interfaces:
    - name: bond10 <co xml:id="CO248-4"/>
      description: Bonding eth2 and eth3 <co xml:id="CO248-5"/>
      type: bond <co xml:id="CO248-6"/>
      state: up <co xml:id="CO248-7"/>
      link-aggregation:
        mode: balance-rr <co xml:id="CO248-8"/>
        options:
          miimon: '140' <co xml:id="CO248-9"/>
        port: <co xml:id="CO248-10"/>
        - eth2
        - eth3
    - name: bond10.103 <co xml:id="CO248-11"/>
      description: vlan using bond10 <co xml:id="CO248-12"/>
      type: vlan <co xml:id="CO248-13"/>
      state: up <co xml:id="CO248-14"/>
      vlan:
         base-iface: bond10 <co xml:id="CO248-15"/>
         id: 103 <co xml:id="CO248-16"/>
      ipv4:
        dhcp: true <co xml:id="CO248-17"/>
        enabled: true <co xml:id="CO248-18"/></programlisting>
<calloutlist>
<callout arearefs="CO248-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO248-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster.</para>
</callout>
<callout arearefs="CO248-3">
<para>This example uses <literal>hostname</literal> node selector.</para>
</callout>
<callout arearefs="CO248-4 CO248-11">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO248-5 CO248-12">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO248-6 CO248-13">
<para>The type of interface.</para>
</callout>
<callout arearefs="CO248-7 CO248-14">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO248-8">
<para>The driver mode for the bond.</para>
</callout>
<callout arearefs="CO248-9">
<para>Optional: This example uses miimon to inspect the bond link every 140ms.</para>
</callout>
<callout arearefs="CO248-10">
<para>The subordinate node NICs in the bond.</para>
</callout>
<callout arearefs="CO248-15">
<para>The node NIC to which the VLAN is attached.</para>
</callout>
<callout arearefs="CO248-16">
<para>The VLAN tag.</para>
</callout>
<callout arearefs="CO248-17">
<para>Optional: If you do not use dhcp, you can either set a static IP or leave the interface without an IP address.</para>
</callout>
<callout arearefs="CO248-18">
<para>Enables ipv4 in this example.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-host-vrf_k8s_nmstate-updating-node-network-config">
<title>Example: Network interface with a VRF instance node network configuration policy</title>
<simpara>Associate a Virtual Routing and Forwarding (VRF) instance with a network interface by applying a <literal>NodeNetworkConfigurationPolicy</literal> custom resource (CR).</simpara>
<important>
<simpara>Associating a VRF instance with a network interface is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>By associating a VRF instance with a network interface, you can support traffic isolation, independent routing decisions, and the logical separation of network resources.</simpara>
<simpara>In a bare-metal environment, you can announce load balancer services through interfaces belonging to a VRF instance by using MetalLB. For more information, see the <emphasis>Additional resources</emphasis> section.</simpara>
<simpara>The following YAML file is an example of associating a VRF instance to a network interface.
It includes samples values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: vrfpolicy <co xml:id="CO249-1"/>
spec:
  nodeSelector:
    vrf: "true" <co xml:id="CO249-2"/>
  maxUnavailable: 3
  desiredState:
    interfaces:
      - name: ens4vrf <co xml:id="CO249-3"/>
        type: vrf <co xml:id="CO249-4"/>
        state: up
        vrf:
          port:
            - ens4 <co xml:id="CO249-5"/>
          route-table-id: 2 <co xml:id="CO249-6"/></programlisting>
<calloutlist>
<callout arearefs="CO249-1">
<para>The name of the policy.</para>
</callout>
<callout arearefs="CO249-2">
<para>This example applies the policy to all nodes with the label <literal>vrf:true</literal>.</para>
</callout>
<callout arearefs="CO249-3">
<para>The name of the interface.</para>
</callout>
<callout arearefs="CO249-4">
<para>The type of interface. This example creates a VRF instance.</para>
</callout>
<callout arearefs="CO249-5">
<para>The node interface to which the VRF attaches.</para>
</callout>
<callout arearefs="CO249-6">
<para>The name of the route table ID for the VRF.</para>
</callout>
</calloutlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding">About virtual routing and forwarding</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-metallb-bgp-peer-vrf_configure-metallb-bgp-peers">Exposing a service through a network VRF</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="capturing-nic-static-ip_k8s-nmstate-updating-node-network-config">
<title>Capturing the static IP of a NIC attached to a bridge</title>
<important>
<simpara>Capturing the static IP of a NIC is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="virt-example-inherit-static-ip-from-nic_k8s_nmstate-updating-node-network-config">
<title>Example: Linux bridge interface node network configuration policy to inherit static IP address from the NIC attached to the bridge</title>
<simpara>Create a Linux bridge interface on nodes in the cluster and transfer the static IP configuration of the NIC to the bridge by applying a single <literal>NodeNetworkConfigurationPolicy</literal> manifest to the cluster.</simpara>
<simpara>The following YAML file is an example of a manifest for a Linux bridge interface. It includes sample values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-copy-ipv4-policy <co xml:id="CO250-1"/>
spec:
  nodeSelector: <co xml:id="CO250-2"/>
    node-role.kubernetes.io/worker: ""
  capture:
    eth1-nic: interfaces.name=="eth1" <co xml:id="CO250-3"/>
    eth1-routes: routes.running.next-hop-interface=="eth1"
    br1-routes: capture.eth1-routes | routes.running.next-hop-interface := "br1"
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with eth1 as a port
        type: linux-bridge <co xml:id="CO250-4"/>
        state: up
        ipv4: "{{ capture.eth1-nic.interfaces.0.ipv4 }}" <co xml:id="CO250-5"/>
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: eth1 <co xml:id="CO250-6"/>
     routes:
        config: "{{ capture.br1-routes.routes.running }}"</programlisting>
<calloutlist>
<callout arearefs="CO250-1">
<para>The name of the policy.</para>
</callout>
<callout arearefs="CO250-2">
<para>Optional: If you do not include the <literal>nodeSelector</literal> parameter, the policy applies to all nodes in the cluster. This example uses the <literal>node-role.kubernetes.io/worker: ""</literal> node selector to select all worker nodes in the cluster.</para>
</callout>
<callout arearefs="CO250-3">
<para>The reference to the node NIC to which the bridge attaches.</para>
</callout>
<callout arearefs="CO250-4">
<para>The type of interface. This example creates a bridge.</para>
</callout>
<callout arearefs="CO250-5">
<para>The IP address of the bridge interface. This value matches the IP address of the NIC which is referenced by the <literal>spec.capture.eth1-nic</literal> entry.</para>
</callout>
<callout arearefs="CO250-6">
<para>The node NIC to which the bridge attaches.</para>
</callout>
</calloutlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://nmstate.io/nmpolicy/user-guide/102-policy-syntax.html">The NMPolicy project - Policy syntax</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-example-nmstate-IP-management_k8s_nmstate-updating-node-network-config">
<title>Examples: IP management</title>
<simpara>The following example configuration snippets demonstrate different methods of IP management.</simpara>
<simpara>These examples use the <literal>ethernet</literal> interface type to simplify the example while showing the related context in the policy configuration. These IP management examples can be used with the other interface types.</simpara>
<section xml:id="virt-example-nmstate-IP-management-static_k8s_nmstate-updating-node-network-config">
<title>Static</title>
<simpara>The following snippet statically configures an IP address on the Ethernet interface:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
    interfaces:
    - name: eth1
      description: static IP on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: false
        address:
        - ip: 192.168.122.250 <co xml:id="CO251-1"/>
          prefix-length: 24
        enabled: true
# ...</programlisting>
<calloutlist>
<callout arearefs="CO251-1">
<para>Replace this value with the static IP address for the interface.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-nmstate-IP-management-no-ip_k8s_nmstate-updating-node-network-config">
<title>No IP address</title>
<simpara>The following snippet ensures that the interface has no IP address:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
    interfaces:
    - name: eth1
      description: No IP on eth1
      type: ethernet
      state: up
      ipv4:
        enabled: false
# ...</programlisting>
</section>
<section xml:id="virt-example-nmstate-IP-management-dhcp_k8s_nmstate-updating-node-network-config">
<title>Dynamic host configuration</title>
<simpara>The following snippet configures an Ethernet interface that uses a dynamic IP address, gateway address, and DNS:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
    interfaces:
    - name: eth1
      description: DHCP on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: true
        enabled: true
# ...</programlisting>
<simpara>The following snippet configures an Ethernet interface that uses a dynamic IP address but does not use a dynamic gateway address or DNS:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
    interfaces:
    - name: eth1
      description: DHCP without gateway or DNS on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: true
        auto-gateway: false
        auto-dns: false
        enabled: true
# ...</programlisting>
</section>
<section xml:id="virt-example-nmstate-IP-management-dns_k8s_nmstate-updating-node-network-config">
<title>DNS</title>
<simpara>Setting the DNS configuration is analagous to modifying the <literal>/etc/resolv.conf</literal> file. The following snippet sets the DNS configuration on the host.</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
    interfaces: <co xml:id="CO252-1"/>
       ...
       ipv4:
         ...
         auto-dns: false
         ...
    dns-resolver:
      config:
        search:
        - example.com
        - example.org
        server:
        - 8.8.8.8
# ...</programlisting>
<calloutlist>
<callout arearefs="CO252-1">
<para>You must configure an interface with <literal>auto-dns: false</literal> or you must use static IP configuration on an interface in order for Kubernetes NMState to store custom DNS settings.</para>
</callout>
</calloutlist>
<important>
<simpara>You cannot use <literal>br-ex</literal>, an OVNKubernetes-managed Open vSwitch bridge, as the interface when configuring DNS resolvers.</simpara>
</important>
</section>
<section xml:id="virt-example-nmstate-IP-management-static-routing_k8s_nmstate-updating-node-network-config">
<title>Static routing</title>
<simpara>The following snippet configures a static route and a static IP on interface <literal>eth1</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
    interfaces:
    - name: eth1
      description: Static routing on eth1
      type: ethernet
      state: up
      ipv4:
        dhcp: false
        address:
        - ip: 192.0.2.251 <co xml:id="CO253-1"/>
          prefix-length: 24
        enabled: true
    routes:
      config:
      - destination: 198.51.100.0/24
        metric: 150
        next-hop-address: 192.0.2.1 <co xml:id="CO253-2"/>
        next-hop-interface: eth1
        table-id: 254
# ...</programlisting>
<calloutlist>
<callout arearefs="CO253-1">
<para>The static IP address for the Ethernet interface.</para>
</callout>
<callout arearefs="CO253-2">
<para>Next hop address for the node traffic. This must be in the same subnet as the IP address set for the Ethernet interface.</para>
</callout>
</calloutlist>
</section>
</section>
</section>
<section xml:id="k8s-nmstate-troubleshooting-node-network">
<title>Troubleshooting node network configuration</title>

<simpara>If the node network configuration encounters an issue, the policy is automatically rolled back and the enactments report failure.
This includes issues such as:</simpara>
<itemizedlist>
<listitem>
<simpara>The configuration fails to be applied on the host.</simpara>
</listitem>
<listitem>
<simpara>The host loses connection to the default gateway.</simpara>
</listitem>
<listitem>
<simpara>The host loses connection to the API server.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-troubleshooting-incorrect-policy-config_k8s-nmstate-troubleshooting-node-network">
<title>Troubleshooting an incorrect node network configuration policy configuration</title>
<simpara>You can apply changes to the node network configuration across your entire cluster by applying a node network configuration policy.
If you apply an incorrect configuration, you can use the following example to troubleshoot and correct the failed node network policy.</simpara>
<simpara>In this example, a Linux bridge policy is applied to an example cluster that has three control plane nodes and three compute nodes.
The policy fails to be applied because it references an incorrect interface.
To find the error, investigate the available NMState resources. You can then update the policy with the correct configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy and apply it to your cluster. The following example creates a simple bridge on the <literal>ens01</literal> interface:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: ens01-bridge-testfail
spec:
  desiredState:
    interfaces:
      - name: br1
        description: Linux bridge with the wrong port
        type: linux-bridge
        state: up
        ipv4:
          dhcp: true
          enabled: true
        bridge:
          options:
            stp:
              enabled: false
          port:
            - name: ens01</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ens01-bridge-testfail.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">nodenetworkconfigurationpolicy.nmstate.io/ens01-bridge-testfail created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the status of the policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nncp</programlisting>
<simpara>The output shows that the policy failed:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    STATUS
ens01-bridge-testfail   FailedToConfigure</programlisting>
</para>
</formalpara>
<simpara>However, the policy status alone does not indicate if it failed on all nodes or a subset of nodes.</simpara>
</listitem>
<listitem>
<simpara>List the node network configuration enactments to see if the policy was successful on any of the nodes. If the policy failed for only a subset of nodes, it suggests that the problem is with a specific node configuration. If the policy failed on all nodes, it suggests that the problem is with the policy.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nnce</programlisting>
<simpara>The output shows that the policy failed on all nodes:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         STATUS
control-plane-1.ens01-bridge-testfail        FailedToConfigure
control-plane-2.ens01-bridge-testfail        FailedToConfigure
control-plane-3.ens01-bridge-testfail        FailedToConfigure
compute-1.ens01-bridge-testfail              FailedToConfigure
compute-2.ens01-bridge-testfail              FailedToConfigure
compute-3.ens01-bridge-testfail              FailedToConfigure</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View one of the failed enactments and look at the traceback. The following command uses the output tool <literal>jsonpath</literal> to filter the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nnce compute-1.ens01-bridge-testfail -o jsonpath='{.status.conditions[?(@.type=="Failing")].message}'</programlisting>
<simpara>This command returns a large traceback that has been edited for brevity:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">error reconciling NodeNetworkConfigurationPolicy at desired state apply: , failed to execute nmstatectl set --no-commit --timeout 480: 'exit status 1' ''
...
libnmstate.error.NmstateVerificationError:
desired
=======
---
name: br1
type: linux-bridge
state: up
bridge:
  options:
    group-forward-mask: 0
    mac-ageing-time: 300
    multicast-snooping: true
    stp:
      enabled: false
      forward-delay: 15
      hello-time: 2
      max-age: 20
      priority: 32768
  port:
  - name: ens01
description: Linux bridge with the wrong port
ipv4:
  address: []
  auto-dns: true
  auto-gateway: true
  auto-routes: true
  dhcp: true
  enabled: true
ipv6:
  enabled: false
mac-address: 01-23-45-67-89-AB
mtu: 1500

current
=======
---
name: br1
type: linux-bridge
state: up
bridge:
  options:
    group-forward-mask: 0
    mac-ageing-time: 300
    multicast-snooping: true
    stp:
      enabled: false
      forward-delay: 15
      hello-time: 2
      max-age: 20
      priority: 32768
  port: []
description: Linux bridge with the wrong port
ipv4:
  address: []
  auto-dns: true
  auto-gateway: true
  auto-routes: true
  dhcp: true
  enabled: true
ipv6:
  enabled: false
mac-address: 01-23-45-67-89-AB
mtu: 1500

difference
==========
--- desired
+++ current
@@ -13,8 +13,7 @@
       hello-time: 2
       max-age: 20
       priority: 32768
-  port:
-  - name: ens01
+  port: []
 description: Linux bridge with the wrong port
 ipv4:
   address: []
  line 651, in _assert_interfaces_equal\n    current_state.interfaces[ifname],\nlibnmstate.error.NmstateVerificationError:</programlisting>
</para>
</formalpara>
<simpara>The <literal>NmstateVerificationError</literal> lists the <literal>desired</literal> policy configuration, the <literal>current</literal> configuration of the policy on the node, and the <literal>difference</literal> highlighting the parameters that do not match. In this example, the <literal>port</literal> is included in the <literal>difference</literal>, which suggests that the problem is the port configuration in the policy.</simpara>
</listitem>
<listitem>
<simpara>To ensure that the policy is configured properly, view the network configuration for one or all of the nodes by requesting the <literal>NodeNetworkState</literal> object. The following command returns the network configuration for the <literal>control-plane-1</literal> node:</simpara>
<screen>$ oc get nns control-plane-1 -o yaml</screen>
<simpara>The output shows that the interface name on the nodes is <literal>ens1</literal> but the failed policy incorrectly uses <literal>ens01</literal>:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">   - ipv4:
# ...
      name: ens1
      state: up
      type: ethernet</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Correct the error by editing the existing policy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit nncp ens01-bridge-testfail</programlisting>
<programlisting language="yaml" linenumbering="unnumbered"># ...
          port:
            - name: ens1</programlisting>
<simpara>Save the policy to apply the correction.</simpara>
</listitem>
<listitem>
<simpara>Check the status of the policy to ensure it updated successfully:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nncp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    STATUS
ens01-bridge-testfail   SuccessfullyConfigured</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<simpara>The updated policy is successfully configured on all nodes in the cluster.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="enable-cluster-wide-proxy">
<title>Configuring the cluster-wide proxy</title>

<simpara>Production environments can deny direct access to the internet and instead have an HTTP or HTTPS proxy available. You can configure OpenShift Container Platform to use a proxy by <link linkend="nw-proxy-configure-object_config-cluster-wide-proxy">modifying the Proxy object for existing clusters</link> or by configuring the proxy settings in the <literal>install-config.yaml</literal> file for new clusters.</simpara>
<section xml:id="_prerequisites-6">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Review the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#configuring-firewall">sites that your cluster requires access to</link> and determine whether any of them must bypass the proxy. By default, all cluster system egress traffic is proxied, including calls to the cloud provider API for the cloud that hosts your cluster. System-wide proxy affects system components only, not user workloads. Add sites to the Proxy object&#8217;s <literal>spec.noProxy</literal> field to bypass the proxy if necessary.</simpara>
<note>
<simpara>The Proxy object <literal>status.noProxy</literal> field is populated with the values of the <literal>networking.machineNetwork[].cidr</literal>, <literal>networking.clusterNetwork[].cidr</literal>, and <literal>networking.serviceNetwork[]</literal> fields from your installation configuration with most installation types.</simpara>
<simpara>For installations on Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and Red Hat OpenStack Platform (RHOSP), the <literal>Proxy</literal> object <literal>status.noProxy</literal> field is also populated with the instance metadata endpoint (<literal>169.254.169.254</literal>).</simpara>
</note>
<important>
<simpara>If your installation type does not include setting the <literal>networking.machineNetwork[].cidr</literal> field, you must include the machine IP addresses manually in the <literal>.status.noProxy</literal> field to make sure that the traffic between nodes can bypass the proxy.</simpara>
</important>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-proxy-configure-object_config-cluster-wide-proxy">
<title>Enabling the cluster-wide proxy</title>
<simpara>The <literal>Proxy</literal> object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a <literal>Proxy</literal> object is still generated but it will have a nil <literal>spec</literal>. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:</programlisting>
<simpara>A cluster administrator can configure the proxy for OpenShift Container Platform by modifying this <literal>cluster</literal> <literal>Proxy</literal> object.</simpara>
<note>
<simpara>Only the <literal>Proxy</literal> object named <literal>cluster</literal> is supported, and no additional proxies can be created.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator permissions</simpara>
</listitem>
<listitem>
<simpara>OpenShift Container Platform <literal>oc</literal> CLI tool installed</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a config map that contains any additional CA certificates required for proxying HTTPS connections.</simpara>
<note>
<simpara>You can skip this step if the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust bundle.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file called <literal>user-ca-bundle.yaml</literal> with the following contents, and provide the values of your PEM-encoded certificates:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
data:
  ca-bundle.crt: | <co xml:id="CO254-1"/>
    &lt;MY_PEM_ENCODED_CERTS&gt; <co xml:id="CO254-2"/>
kind: ConfigMap
metadata:
  name: user-ca-bundle <co xml:id="CO254-3"/>
  namespace: openshift-config <co xml:id="CO254-4"/></programlisting>
<calloutlist>
<callout arearefs="CO254-1">
<para>This data key must be named <literal>ca-bundle.crt</literal>.</para>
</callout>
<callout arearefs="CO254-2">
<para>One or more PEM-encoded X.509 certificates used to sign the proxy&#8217;s
identity certificate.</para>
</callout>
<callout arearefs="CO254-3">
<para>The config map name that will be referenced from the <literal>Proxy</literal> object.</para>
</callout>
<callout arearefs="CO254-4">
<para>The config map must be in the <literal>openshift-config</literal> namespace.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map from this file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f user-ca-bundle.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Use the <literal>oc edit</literal> command to modify the <literal>Proxy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit proxy/cluster</programlisting>
</listitem>
<listitem>
<simpara>Configure the necessary fields for the proxy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO255-1"/>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO255-2"/>
  noProxy: example.com <co xml:id="CO255-3"/>
  readinessEndpoints:
  - http://www.google.com <co xml:id="CO255-4"/>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <co xml:id="CO255-5"/></programlisting>
<calloutlist>
<callout arearefs="CO255-1">
<para>A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <literal>http</literal>.</para>
</callout>
<callout arearefs="CO255-2">
<para>A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either <literal>http</literal> or <literal>https</literal>. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use <literal>https</literal> but they only support <literal>http</literal>. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for <literal>https</literal> connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.</para>
</callout>
<callout arearefs="CO255-3">
<para>A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.</para>
<simpara>Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the <literal>networking.machineNetwork[].cidr</literal> field from the installation configuration, you must add them to this list to prevent connection issues.</simpara>
<simpara>This field is ignored if neither the <literal>httpProxy</literal> or <literal>httpsProxy</literal> fields are set.</simpara>
</callout>
<callout arearefs="CO255-4">
<para>One or more URLs external to the cluster to use to perform a readiness check before writing the <literal>httpProxy</literal> and <literal>httpsProxy</literal> values to status.</para>
</callout>
<callout arearefs="CO255-5">
<para>A reference to the config map in the <literal>openshift-config</literal> namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust bundle.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-proxy-remove_config-cluster-wide-proxy">
<title>Removing the cluster-wide proxy</title>
<simpara>The <literal>cluster</literal> Proxy object cannot be deleted. To remove the proxy from a cluster, remove all <literal>spec</literal> fields from the Proxy object.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator permissions</simpara>
</listitem>
<listitem>
<simpara>OpenShift Container Platform <literal>oc</literal> CLI tool installed</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Use the <literal>oc edit</literal> command to modify the proxy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit proxy/cluster</programlisting>
</listitem>
<listitem>
<simpara>Remove all <literal>spec</literal> fields from the Proxy object. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec: {}</programlisting>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
<bridgehead xml:id="_additional-resources-8" role="_additional-resources" renderas="sect2">Additional resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#ca-bundle-understanding_updating-ca-bundle">Replacing the CA Bundle certificate</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#customization">Proxy certificate customization</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring-a-custom-pki">
<title>Configuring a custom PKI</title>

<simpara>Some platform components, such as the web console, use Routes for communication and
must trust other components' certificates to interact with them. If
you are using a custom public key infrastructure (PKI), you must configure it so
its privately signed CA certificates are recognized across the cluster.</simpara>
<simpara>You can leverage the Proxy API to add cluster-wide trusted CA certificates. You
must do this either during installation or at runtime.</simpara>
<itemizedlist>
<listitem>
<simpara>During <emphasis>installation</emphasis>, <link xlink:href="../networking/configuring-a-custom-pki.xml#installation-configure-proxy_configuring-a-custom-pki">configure the cluster-wide proxy</link>. You must define your
privately signed CA certificates in the <literal>install-config.yaml</literal> file&#8217;s
<literal>additionalTrustBundle</literal> setting.</simpara>
<simpara>The installation program generates a ConfigMap that is named <literal>user-ca-bundle</literal>
that contains the additional CA certificates you defined. The Cluster Network
Operator then creates a <literal>trusted-ca-bundle</literal> ConfigMap that merges these CA
certificates with the Red Hat Enterprise Linux CoreOS (RHCOS) trust bundle; this ConfigMap is
referenced in the Proxy object&#8217;s <literal>trustedCA</literal> field.</simpara>
</listitem>
<listitem>
<simpara>At <emphasis>runtime</emphasis>, <link xlink:href="../networking/configuring-a-custom-pki.xml#nw-proxy-configure-object_configuring-a-custom-pki">modify the default Proxy object to include your privately signed CA certificates</link> (part of cluster&#8217;s proxy enablement workflow). This involves
creating a ConfigMap that contains the privately signed CA certificates that
should be trusted by the cluster, and then modifying the proxy resource with the
<literal>trustedCA</literal> referencing the privately signed certificates' ConfigMap.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The installer configuration&#8217;s <literal>additionalTrustBundle</literal> field and the proxy
resource&#8217;s <literal>trustedCA</literal> field are used to manage the cluster-wide trust bundle;
<literal>additionalTrustBundle</literal> is used at install time and the proxy&#8217;s <literal>trustedCA</literal> is
used at runtime.</simpara>
<simpara>The <literal>trustedCA</literal> field is a reference to a <literal>ConfigMap</literal> containing the custom
certificate and key pair used by the cluster component.</simpara>
</note>
<section xml:id="installation-configure-proxy_configuring-a-custom-pki">
<title>Configuring the cluster-wide proxy during installation</title>
<simpara>Production environments can deny direct access to the internet and instead have
an HTTP or HTTPS proxy available. You can configure a new OpenShift Container Platform
cluster to use a proxy by configuring the proxy settings in the
<literal>install-config.yaml</literal> file.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have an existing <literal>install-config.yaml</literal> file.</simpara>
</listitem>
<listitem>
<simpara>You reviewed the sites that your cluster requires access to and determined whether any of them need to bypass the proxy. By default, all cluster egress traffic is proxied, including calls to hosting cloud provider APIs. You added sites to the <literal>Proxy</literal> object&#8217;s <literal>spec.noProxy</literal> field to bypass the proxy if necessary.</simpara>
<note>
<simpara>The <literal>Proxy</literal> object <literal>status.noProxy</literal> field is populated with the values of the <literal>networking.machineNetwork[].cidr</literal>, <literal>networking.clusterNetwork[].cidr</literal>, and <literal>networking.serviceNetwork[]</literal> fields from your installation configuration.</simpara>
<simpara>For installations on Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, and Red Hat OpenStack Platform (RHOSP), the <literal>Proxy</literal> object <literal>status.noProxy</literal> field is also populated with the instance metadata endpoint (<literal>169.254.169.254</literal>).</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit your <literal>install-config.yaml</literal> file and add the proxy settings. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
baseDomain: my.domain.com
proxy:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO256-1"/>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO256-2"/>
  noProxy: ec2.&lt;aws_region&gt;.amazonaws.com,elasticloadbalancing.&lt;aws_region&gt;.amazonaws.com,s3.&lt;aws_region&gt;.amazonaws.com <co xml:id="CO256-3"/>
additionalTrustBundle: | <co xml:id="CO256-4"/>
    -----BEGIN CERTIFICATE-----
    &lt;MY_TRUSTED_CA_CERT&gt;
    -----END CERTIFICATE-----
additionalTrustBundlePolicy: &lt;policy_to_add_additionalTrustBundle&gt; <co xml:id="CO256-5"/></programlisting>
<calloutlist>
<callout arearefs="CO256-1">
<para>A proxy URL to use for creating HTTP connections outside the cluster. The
URL scheme must be <literal>http</literal>.</para>
</callout>
<callout arearefs="CO256-2">
<para>A proxy URL to use for creating HTTPS connections outside the cluster.</para>
</callout>
<callout arearefs="CO256-3">
<para>A comma-separated list of destination domain names, IP addresses, or other network CIDRs to exclude from proxying. Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass the proxy for all destinations.
If you have added the Amazon <literal>EC2</literal>,<literal>Elastic Load Balancing</literal>, and <literal>S3</literal> VPC endpoints to your VPC, you must add these endpoints to the <literal>noProxy</literal> field.</para>
</callout>
<callout arearefs="CO256-4">
<para>If provided, the installation program generates a config map that is named <literal>user-ca-bundle</literal> in
the <literal>openshift-config</literal> namespace to hold the additional CA
certificates. If you provide <literal>additionalTrustBundle</literal> and at least one proxy setting, the <literal>Proxy</literal> object is configured to reference the <literal>user-ca-bundle</literal> config map in the <literal>trustedCA</literal> field. The Cluster Network
Operator then creates a <literal>trusted-ca-bundle</literal> config map that merges the contents specified for the <literal>trustedCA</literal> parameter
with the RHCOS trust bundle. The <literal>additionalTrustBundle</literal> field is required unless
the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust
bundle.</para>
</callout>
<callout arearefs="CO256-5">
<para>Optional: The policy to determine the configuration of the <literal>Proxy</literal> object to reference the <literal>user-ca-bundle</literal> config map in the <literal>trustedCA</literal> field. The allowed values are <literal>Proxyonly</literal> and <literal>Always</literal>. Use <literal>Proxyonly</literal> to reference the <literal>user-ca-bundle</literal> config map only when <literal>http/https</literal> proxy is configured. Use <literal>Always</literal> to always reference the <literal>user-ca-bundle</literal> config map. The default value is <literal>Proxyonly</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>The installation program does not support the proxy <literal>readinessEndpoints</literal> field.</simpara>
</note>
<note>
<simpara>If the installer times out, restart and then complete the deployment by using the <literal>wait-for</literal> command of the installer. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ./openshift-install wait-for install-complete --log-level debug</programlisting>
</note>
</listitem>
<listitem>
<simpara>Save the file and reference it when installing OpenShift Container Platform.</simpara>
</listitem>
</orderedlist>
<simpara>The installation program creates a cluster-wide proxy that is named <literal>cluster</literal> that uses the proxy
settings in the provided <literal>install-config.yaml</literal> file. If no proxy settings are
provided, a <literal>cluster</literal> <literal>Proxy</literal> object is still created, but it will have a nil
<literal>spec</literal>.</simpara>
<note>
<simpara>Only the <literal>Proxy</literal> object named <literal>cluster</literal> is supported, and no additional
proxies can be created.</simpara>
</note>
</section>
<section xml:id="nw-proxy-configure-object_configuring-a-custom-pki">
<title>Enabling the cluster-wide proxy</title>
<simpara>The <literal>Proxy</literal> object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a <literal>Proxy</literal> object is still generated but it will have a nil <literal>spec</literal>. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:</programlisting>
<simpara>A cluster administrator can configure the proxy for OpenShift Container Platform by modifying this <literal>cluster</literal> <literal>Proxy</literal> object.</simpara>
<note>
<simpara>Only the <literal>Proxy</literal> object named <literal>cluster</literal> is supported, and no additional proxies can be created.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator permissions</simpara>
</listitem>
<listitem>
<simpara>OpenShift Container Platform <literal>oc</literal> CLI tool installed</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a config map that contains any additional CA certificates required for proxying HTTPS connections.</simpara>
<note>
<simpara>You can skip this step if the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust bundle.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file called <literal>user-ca-bundle.yaml</literal> with the following contents, and provide the values of your PEM-encoded certificates:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
data:
  ca-bundle.crt: | <co xml:id="CO257-1"/>
    &lt;MY_PEM_ENCODED_CERTS&gt; <co xml:id="CO257-2"/>
kind: ConfigMap
metadata:
  name: user-ca-bundle <co xml:id="CO257-3"/>
  namespace: openshift-config <co xml:id="CO257-4"/></programlisting>
<calloutlist>
<callout arearefs="CO257-1">
<para>This data key must be named <literal>ca-bundle.crt</literal>.</para>
</callout>
<callout arearefs="CO257-2">
<para>One or more PEM-encoded X.509 certificates used to sign the proxy&#8217;s
identity certificate.</para>
</callout>
<callout arearefs="CO257-3">
<para>The config map name that will be referenced from the <literal>Proxy</literal> object.</para>
</callout>
<callout arearefs="CO257-4">
<para>The config map must be in the <literal>openshift-config</literal> namespace.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map from this file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f user-ca-bundle.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Use the <literal>oc edit</literal> command to modify the <literal>Proxy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit proxy/cluster</programlisting>
</listitem>
<listitem>
<simpara>Configure the necessary fields for the proxy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO258-1"/>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO258-2"/>
  noProxy: example.com <co xml:id="CO258-3"/>
  readinessEndpoints:
  - http://www.google.com <co xml:id="CO258-4"/>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <co xml:id="CO258-5"/></programlisting>
<calloutlist>
<callout arearefs="CO258-1">
<para>A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <literal>http</literal>.</para>
</callout>
<callout arearefs="CO258-2">
<para>A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either <literal>http</literal> or <literal>https</literal>. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use <literal>https</literal> but they only support <literal>http</literal>. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for <literal>https</literal> connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.</para>
</callout>
<callout arearefs="CO258-3">
<para>A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.</para>
<simpara>Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the <literal>networking.machineNetwork[].cidr</literal> field from the installation configuration, you must add them to this list to prevent connection issues.</simpara>
<simpara>This field is ignored if neither the <literal>httpProxy</literal> or <literal>httpsProxy</literal> fields are set.</simpara>
</callout>
<callout arearefs="CO258-4">
<para>One or more URLs external to the cluster to use to perform a readiness check before writing the <literal>httpProxy</literal> and <literal>httpsProxy</literal> values to status.</para>
</callout>
<callout arearefs="CO258-5">
<para>A reference to the config map in the <literal>openshift-config</literal> namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust bundle.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="certificate-injection-using-operators_configuring-a-custom-pki">
<title>Certificate injection using Operators</title>
<simpara>Once your custom CA certificate is added to the cluster via ConfigMap, the
Cluster Network Operator merges the user-provided and system CA certificates
into a single bundle and injects the merged bundle into the Operator requesting
the trust bundle injection.</simpara>
<important>
<simpara>After adding a <literal>config.openshift.io/inject-trusted-cabundle="true"</literal> label to the config map, existing data in it is deleted. The Cluster Network Operator takes ownership of a config map and only accepts <literal>ca-bundle</literal> as data.
You must use a separate config map to store <literal>service-ca.crt</literal> by using the <literal>service.beta.openshift.io/inject-cabundle=true</literal> annotation or a similar configuration. Adding a <literal>config.openshift.io/inject-trusted-cabundle="true"</literal> label and <literal>service.beta.openshift.io/inject-cabundle=true</literal> annotation on the same config map can cause issues.</simpara>
</important>
<simpara>Operators request this injection by creating an empty ConfigMap with the
following label:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">config.openshift.io/inject-trusted-cabundle="true"</programlisting>
<simpara>An example of the empty ConfigMap:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
data: {}
kind: ConfigMap
metadata:
  labels:
    config.openshift.io/inject-trusted-cabundle: "true"
  name: ca-inject <co xml:id="CO259-1"/>
  namespace: apache</programlisting>
<calloutlist>
<callout arearefs="CO259-1">
<para>Specifies the empty ConfigMap name.</para>
</callout>
</calloutlist>
<simpara>The Operator mounts this ConfigMap into the container&#8217;s local trust store.</simpara>
<note>
<simpara>Adding a trusted CA certificate is only needed if the certificate is not
included in the Red Hat Enterprise Linux CoreOS (RHCOS) trust bundle.</simpara>
</note>
<simpara>Certificate injection is not limited to Operators. The Cluster Network Operator
injects certificates across any namespace when an empty ConfigMap is created with the
<literal>config.openshift.io/inject-trusted-cabundle=true</literal> label.</simpara>
<simpara>The ConfigMap can reside in any namespace, but the ConfigMap must be mounted as
a volume to each container within a pod that requires a custom CA. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-example-custom-ca-deployment
  namespace: my-example-custom-ca-ns
spec:
  ...
    spec:
      ...
      containers:
        - name: my-container-that-needs-custom-ca
          volumeMounts:
          - name: trusted-ca
            mountPath: /etc/pki/ca-trust/extracted/pem
            readOnly: true
      volumes:
      - name: trusted-ca
        configMap:
          name: trusted-ca
          items:
            - key: ca-bundle.crt <co xml:id="CO260-1"/>
              path: tls-ca-bundle.pem <co xml:id="CO260-2"/></programlisting>
<calloutlist>
<callout arearefs="CO260-1">
<para><literal>ca-bundle.crt</literal> is required as the ConfigMap key.</para>
</callout>
<callout arearefs="CO260-2">
<para><literal>tls-ca-bundle.pem</literal> is required as the ConfigMap path.</para>
</callout>
</calloutlist>
</section>
</chapter>
<chapter xml:id="load-balancing-openstack">
<title>Load balancing on RHOSP</title>

<section xml:id="nw-osp-loadbalancer-limitations_load-balancing-openstack">
<title>Limitations of load balancer services</title>
<simpara>OpenShift Container Platform clusters on Red Hat OpenStack Platform (RHOSP) use Octavia to handle load balancer services. As a result of this choice, such clusters have a number of functional limitations.</simpara>
<simpara>RHOSP Octavia has two supported providers: Amphora and OVN. These providers differ in terms of available features as well as implementation details. These distinctions affect load balancer services that are created on your cluster.</simpara>
<section xml:id="nw-osp-loadbalancer-etp-local_load-balancing-openstack">
<title>Local external traffic policies</title>
<simpara>You can set the external traffic policy (ETP) parameter, <literal>.spec.externalTrafficPolicy</literal>, on a load balancer service to preserve the source IP address of incoming traffic when it reaches service endpoint pods. However, if your cluster uses the Amphora Octavia provider, the source IP of the traffic is replaced with the IP address of the Amphora VM. This behavior does not occur if your cluster uses the OVN Octavia provider.</simpara>
<simpara>Having the <literal>ETP</literal> option set to <literal>Local</literal> requires that health monitors be created for the load balancer. Without health monitors, traffic can be routed to a node that doesn&#8217;t have a functional endpoint, which causes the connection to drop. To force Cloud Provider OpenStack to create health monitors, you must set the value of the <literal>create-monitor</literal> option in the cloud provider configuration to <literal>true</literal>.</simpara>
<simpara>In RHOSP 16.2, the OVN Octavia provider does not support health monitors. Therefore, setting the ETP to local is unsupported.</simpara>
<simpara>In RHOSP 16.2, the Amphora Octavia provider does not support HTTP monitors on UDP pools. As a result, UDP load balancer services have <literal>UDP-CONNECT</literal> monitors created instead. Due to implementation details, this configuration only functions properly with the OVN-Kubernetes CNI plugin. When the OpenShift SDN CNI plugin is used, the UDP services alive nodes are detected unreliably. This issue also affects the OVN Octavia provider in any RHOSP version because the driver does not support HTTP health monitors.</simpara>
</section>
</section>
<section xml:id="installation-osp-api-octavia_load-balancing-openstack">
<title>Scaling clusters for application traffic by using Octavia</title>
<simpara>OpenShift Container Platform clusters that run on Red Hat OpenStack Platform (RHOSP) can use the Octavia load balancing service to distribute traffic across multiple virtual machines (VMs) or floating IP addresses. This feature mitigates the bottleneck that single machines or addresses create.</simpara>
<simpara>You must create your own Octavia load balancer to use it for application network scaling.</simpara>
<section xml:id="installation-osp-api-scaling_load-balancing-openstack">
<title>Scaling clusters by using Octavia</title>
<simpara>If you want to use multiple API load balancers, create an Octavia load balancer and then configure your cluster to use it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Octavia is available on your Red Hat OpenStack Platform (RHOSP) deployment.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From a command line, create an Octavia load balancer that uses the Amphora driver:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack loadbalancer create --name API_OCP_CLUSTER --vip-subnet-id &lt;id_of_worker_vms_subnet&gt;</programlisting>
<simpara>You can use a name of your choice instead of <literal>API_OCP_CLUSTER</literal>.</simpara>
</listitem>
<listitem>
<simpara>After the load balancer becomes active, create listeners:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack loadbalancer listener create --name API_OCP_CLUSTER_6443 --protocol HTTPS--protocol-port 6443 API_OCP_CLUSTER</programlisting>
<note>
<simpara>To view the status of the load balancer, enter <literal>openstack loadbalancer list</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create a pool that uses the round robin algorithm and has session persistence enabled:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack loadbalancer pool create --name API_OCP_CLUSTER_pool_6443 --lb-algorithm ROUND_ROBIN --session-persistence type=&lt;source_IP_address&gt; --listener API_OCP_CLUSTER_6443 --protocol HTTPS</programlisting>
</listitem>
<listitem>
<simpara>To ensure that control plane machines are available, create a health monitor:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack loadbalancer healthmonitor create --delay 5 --max-retries 4 --timeout 10 --type TCP API_OCP_CLUSTER_pool_6443</programlisting>
</listitem>
<listitem>
<simpara>Add the control plane machines as members of the load balancer pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for SERVER in $(MASTER-0-IP MASTER-1-IP MASTER-2-IP)
do
  openstack loadbalancer member create --address $SERVER  --protocol-port 6443 API_OCP_CLUSTER_pool_6443
done</programlisting>
</listitem>
<listitem>
<simpara>Optional: To reuse the cluster API floating IP address, unset it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack floating ip unset $API_FIP</programlisting>
</listitem>
<listitem>
<simpara>Add either the unset <literal>API_FIP</literal> or a new address to the created load balancer VIP:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack floating ip set  --port $(openstack loadbalancer show -c &lt;vip_port_id&gt; -f value API_OCP_CLUSTER) $API_FIP</programlisting>
</listitem>
</orderedlist>
<simpara>Your cluster now uses Octavia for load balancing.</simpara>
</section>
</section>
<section xml:id="nw-osp-configuring-external-load-balancer_load-balancing-openstack">
<title>Configuring an external load balancer</title>
<simpara>You can configure an OpenShift Container Platform cluster
on Red Hat OpenStack Platform (RHOSP)
to use an external load balancer in place of the default load balancer.</simpara>
<important>
<simpara>Configuring an external load balancer depends on your vendor&#8217;s load balancer.</simpara>
<simpara>The information and examples in this section are for guideline purposes only. Consult the vendor documentation for more specific information about the vendor&#8217;s load balancer.</simpara>
</important>
<simpara>Red Hat supports the following services for an external load balancer:</simpara>
<itemizedlist>
<listitem>
<simpara>Ingress Controller</simpara>
</listitem>
<listitem>
<simpara>OpenShift API</simpara>
</listitem>
<listitem>
<simpara>OpenShift MachineConfig API</simpara>
</listitem>
</itemizedlist>
<simpara>You can choose whether you want to configure one or all of these services for an external load balancer. Configuring only the Ingress Controller service is a common configuration option. To better understand each service, view the following diagrams:</simpara>
<figure>
<title>Example network workflow that shows an Ingress Controller operating in an OpenShift Container Platform environment</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/external-load-balancer-default.png"/>
</imageobject>
<textobject><phrase>An image that shows an example network workflow of an Ingress Controller operating in an OpenShift Container Platform environment.</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Example network workflow that shows an OpenShift API operating in an OpenShift Container Platform environment</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/external-load-balancer-openshift-api.png"/>
</imageobject>
<textobject><phrase>An image that shows an example network workflow of an OpenShift API operating in an OpenShift Container Platform environment.</phrase></textobject>
</mediaobject>
</figure>
<figure>
<title>Example network workflow that shows an OpenShift MachineConfig API operating in an OpenShift Container Platform environment</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/external-load-balancer-machine-config-api.png"/>
</imageobject>
<textobject><phrase>An image that shows an example network workflow of an OpenShift MachineConfig API operating in an OpenShift Container Platform environment.</phrase></textobject>
</mediaobject>
</figure>
<simpara>The following configuration options are supported for external load balancers:</simpara>
<itemizedlist>
<listitem>
<simpara>Use a node selector to map the Ingress Controller to a specific set of nodes. You must assign a static IP address to each node in this set, or configure each node to receive the same IP address from the Dynamic Host Configuration Protocol (DHCP). Infrastructure nodes commonly receive this type of configuration.</simpara>
</listitem>
<listitem>
<simpara>Target all IP addresses on a subnet. This configuration can reduce maintenance overhead, because you can create and destroy nodes within those networks without reconfiguring the load balancer targets. If you deploy your ingress pods by using a machine set on a smaller network, such as a <literal>/27</literal> or <literal>/28</literal>, you can simplify your load balancer targets.</simpara>
<tip>
<simpara>You can list all IP addresses that exist in a network by checking the machine config pool&#8217;s resources.</simpara>
</tip>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Considerations</title>
<listitem>
<simpara>For a front-end IP address, you can use the same IP address for the front-end IP address, the Ingress Controller&#8217;s load balancer, and API load balancer. Check the vendor&#8217;s documentation for this capability.</simpara>
</listitem>
<listitem>
<simpara>For a back-end IP address, ensure that an IP address for an OpenShift Container Platform control plane node does not change during the lifetime of the external load balancer. You can achieve this by completing one of the following actions:</simpara>
<itemizedlist>
<listitem>
<simpara>Assign a static IP address to each control plane node.</simpara>
</listitem>
<listitem>
<simpara>Configure each node to receive the same IP address from the DHCP every time the node requests a DHCP lease. Depending on the vendor, the DHCP lease might be in the form of an IP reservation or a static DHCP assignment.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Manually define each node that runs the Ingress Controller in the external load balancer for the Ingress Controller back-end service. For example, if the Ingress Controller moves to an undefined node, a connection outage can occur.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>OpenShift API prerequisites</title>
<listitem>
<simpara>You defined a front-end IP address.</simpara>
</listitem>
<listitem>
<simpara>TCP ports 6443 and 22623 are exposed on the front-end IP address of your load balancer. Check the following items:</simpara>
<itemizedlist>
<listitem>
<simpara>Port 6443 provides access to the OpenShift API service.</simpara>
</listitem>
<listitem>
<simpara>Port 22623 can provide ignition startup configurations to nodes.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The front-end IP address and port 6443 are reachable by all users of your system with a location external to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>The front-end IP address and port 22623 are reachable only by OpenShift Container Platform nodes.</simpara>
</listitem>
<listitem>
<simpara>The load balancer backend can communicate with OpenShift Container Platform control plane nodes on port 6443 and 22623.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Ingress Controller prerequisites</title>
<listitem>
<simpara>You defined a front-end IP address.</simpara>
</listitem>
<listitem>
<simpara>TCP ports 443 and 80 are exposed on the front-end IP address of your load balancer.</simpara>
</listitem>
<listitem>
<simpara>The front-end IP address, port 80 and port 443 are be reachable by all users of your system with a location external to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>The front-end IP address, port 80 and port 443 are reachable to all nodes that operate in your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>The load balancer backend can communicate with OpenShift Container Platform nodes that run the Ingress Controller on ports 80, 443, and 1936.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Prerequisite for health check URL specifications</title>
<para>You can configure most load balancers by setting health check URLs that determine if a service is available or unavailable. OpenShift Container Platform provides these health checks for the OpenShift API, Machine Configuration API, and Ingress Controller backend services.</para>
</formalpara>
<simpara>The following examples demonstrate health check specifications for the previously listed backend services:</simpara>
<formalpara>
<title>Example of a Kubernetes API health check specification</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Path: HTTPS:6443/readyz
Healthy threshold: 2
Unhealthy threshold: 2
Timeout: 10
Interval: 10</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example of a Machine Config API health check specification</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Path: HTTPS:22623/healthz
Healthy threshold: 2
Unhealthy threshold: 2
Timeout: 10
Interval: 10</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example of an Ingress Controller health check specification</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Path: HTTP:1936/healthz/ready
Healthy threshold: 2
Unhealthy threshold: 2
Timeout: 5
Interval: 10</programlisting>
</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure the HAProxy Ingress Controller, so that you can enable access to the cluster from your load balancer on ports 6443, 443, and 80:</simpara>
<formalpara>
<title>Example HAProxy configuration</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">#...
listen my-cluster-api-6443
    bind 192.168.1.100:6443
    mode tcp
    balance roundrobin
  option httpchk
  http-check connect
  http-check send meth GET uri /readyz
  http-check expect status 200
    server my-cluster-master-2 192.168.1.101:6443 check inter 10s rise 2 fall 2
    server my-cluster-master-0 192.168.1.102:6443 check inter 10s rise 2 fall 2
    server my-cluster-master-1 192.168.1.103:6443 check inter 10s rise 2 fall 2

listen my-cluster-machine-config-api-22623
    bind 192.168.1.1000.0.0.0:22623
    mode tcp
    balance roundrobin
  option httpchk
  http-check connect
  http-check send meth GET uri /healthz
  http-check expect status 200
    server my-cluster-master-2 192.0168.21.2101:22623 check inter 10s rise 2 fall 2
    server my-cluster-master-0 192.168.1.1020.2.3:22623 check inter 10s rise 2 fall 2
    server my-cluster-master-1 192.168.1.1030.2.1:22623 check inter 10s rise 2 fall 2

listen my-cluster-apps-443
        bind 192.168.1.100:443
        mode tcp
        balance roundrobin
    option httpchk
    http-check connect
    http-check send meth GET uri /healthz/ready
    http-check expect status 200
        server my-cluster-worker-0 192.168.1.111:443 check port 1936 inter 10s rise 2 fall 2
        server my-cluster-worker-1 192.168.1.112:443 check port 1936 inter 10s rise 2 fall 2
        server my-cluster-worker-2 192.168.1.113:443 check port 1936 inter 10s rise 2 fall 2

listen my-cluster-apps-80
        bind 192.168.1.100:80
        mode tcp
        balance roundrobin
    option httpchk
    http-check connect
    http-check send meth GET uri /healthz/ready
    http-check expect status 200
        server my-cluster-worker-0 192.168.1.111:80 check port 1936 inter 10s rise 2 fall 2
        server my-cluster-worker-1 192.168.1.112:80 check port 1936 inter 10s rise 2 fall 2
        server my-cluster-worker-2 192.168.1.113:80 check port 1936 inter 10s rise 2 fall 2
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>curl</literal> CLI command to verify that the external load balancer and its resources are operational:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Verify that the cluster machine configuration API is accessible to the Kubernetes API server resource, by running the following command and observing the response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl https://&lt;loadbalancer_ip_address&gt;:6443/version --insecure</programlisting>
<simpara>If the configuration is correct, you receive a JSON object in response:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "major": "1",
  "minor": "11+",
  "gitVersion": "v1.11.0+ad103ed",
  "gitCommit": "ad103ed",
  "gitTreeState": "clean",
  "buildDate": "2019-01-09T06:44:10Z",
  "goVersion": "go1.10.3",
  "compiler": "gc",
  "platform": "linux/amd64"
}</programlisting>
</listitem>
<listitem>
<simpara>Verify that the cluster machine configuration API is accessible to the Machine config server resource, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -v https://&lt;loadbalancer_ip_address&gt;:22623/healthz --insecure</programlisting>
<simpara>If the configuration is correct, the output from the command shows the following response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 200 OK
Content-Length: 0</programlisting>
</listitem>
<listitem>
<simpara>Verify that the controller is accessible to the Ingress Controller resource on port 80, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -I -L -H "Host: console-openshift-console.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;" http://&lt;load_balancer_front_end_IP_address&gt;</programlisting>
<simpara>If the configuration is correct, the output from the command shows the following response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 302 Found
content-length: 0
location: https://console-openshift-console.apps.ocp4.private.opequon.net/
cache-control: no-cache</programlisting>
</listitem>
<listitem>
<simpara>Verify that the controller is accessible to the Ingress Controller resource on port 443, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -I -L --insecure --resolve console-openshift-console.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;:443:&lt;Load Balancer Front End IP Address&gt; https://console-openshift-console.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;</programlisting>
<simpara>If the configuration is correct, the output from the command shows the following response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 200 OK
referrer-policy: strict-origin-when-cross-origin
set-cookie: csrf-token=UlYWOyQ62LWjw2h003xtYSKlh1a0Py2hhctw0WmV2YEdhJjFyQwWcGBsja261dGLgaYO0nxzVErhiXt6QepA7g==; Path=/; Secure; SameSite=Lax
x-content-type-options: nosniff
x-dns-prefetch-control: off
x-frame-options: DENY
x-xss-protection: 1; mode=block
date: Wed, 04 Oct 2023 16:29:38 GMT
content-type: text/html; charset=utf-8
set-cookie: 1e2670d92730b515ce3a1bb65da45062=1bf5e9573c9a2760c964ed1659cc1673; path=/; HttpOnly; Secure; SameSite=None
cache-control: private</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Configure the DNS records for your cluster to target the front-end IP addresses of the external load balancer. You must update records to your DNS server for the cluster API and applications over the load balancer.</simpara>
<formalpara>
<title>Examples of modified DNS records</title>
<para>
<programlisting language="dns" linenumbering="unnumbered">&lt;load_balancer_ip_address&gt;  A  api.&lt;cluster_name&gt;.&lt;base_domain&gt;
A record pointing to Load Balancer Front End</programlisting>
</para>
</formalpara>
<programlisting language="dns" linenumbering="unnumbered">&lt;load_balancer_ip_address&gt;   A apps.&lt;cluster_name&gt;.&lt;base_domain&gt;
A record pointing to Load Balancer Front End</programlisting>
<important>
<simpara>DNS propagation might take some time for each DNS record to become available. Ensure that each DNS record propagates before validating each record.</simpara>
</important>
</listitem>
<listitem>
<simpara>Use the <literal>curl</literal> CLI command to verify that the external load balancer and DNS record configuration are operational:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Verify that you can access the cluster API, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl https://api.&lt;cluster_name&gt;.&lt;base_domain&gt;:6443/version --insecure</programlisting>
<simpara>If the configuration is correct, you receive a JSON object in response:</simpara>
<programlisting language="json" linenumbering="unnumbered">{
  "major": "1",
  "minor": "11+",
  "gitVersion": "v1.11.0+ad103ed",
  "gitCommit": "ad103ed",
  "gitTreeState": "clean",
  "buildDate": "2019-01-09T06:44:10Z",
  "goVersion": "go1.10.3",
  "compiler": "gc",
  "platform": "linux/amd64"
  }</programlisting>
</listitem>
<listitem>
<simpara>Verify that you can access the cluster machine configuration, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -v https://api.&lt;cluster_name&gt;.&lt;base_domain&gt;:22623/healthz --insecure</programlisting>
<simpara>If the configuration is correct, the output from the command shows the following response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 200 OK
Content-Length: 0</programlisting>
</listitem>
<listitem>
<simpara>Verify that you can access each cluster application on port, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl http://console-openshift-console.apps.&lt;cluster_name&gt;.&lt;base_domain&gt; -I -L --insecure</programlisting>
<simpara>If the configuration is correct, the output from the command shows the following response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 302 Found
content-length: 0
location: https://console-openshift-console.apps.&lt;cluster-name&gt;.&lt;base domain&gt;/
cache-control: no-cacheHTTP/1.1 200 OK
referrer-policy: strict-origin-when-cross-origin
set-cookie: csrf-token=39HoZgztDnzjJkq/JuLJMeoKNXlfiVv2YgZc09c3TBOBU4NI6kDXaJH1LdicNhN1UsQWzon4Dor9GWGfopaTEQ==; Path=/; Secure
x-content-type-options: nosniff
x-dns-prefetch-control: off
x-frame-options: DENY
x-xss-protection: 1; mode=block
date: Tue, 17 Nov 2020 08:42:10 GMT
content-type: text/html; charset=utf-8
set-cookie: 1e2670d92730b515ce3a1bb65da45062=9b714eb87e93cf34853e87a92d6894be; path=/; HttpOnly; Secure; SameSite=None
cache-control: private</programlisting>
</listitem>
<listitem>
<simpara>Verify that you can access each cluster application on port 443, by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl https://console-openshift-console.apps.&lt;cluster_name&gt;.&lt;base_domain&gt; -I -L --insecure</programlisting>
<simpara>If the configuration is correct, the output from the command shows the following response:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">HTTP/1.1 200 OK
referrer-policy: strict-origin-when-cross-origin
set-cookie: csrf-token=UlYWOyQ62LWjw2h003xtYSKlh1a0Py2hhctw0WmV2YEdhJjFyQwWcGBsja261dGLgaYO0nxzVErhiXt6QepA7g==; Path=/; Secure; SameSite=Lax
x-content-type-options: nosniff
x-dns-prefetch-control: off
x-frame-options: DENY
x-xss-protection: 1; mode=block
date: Wed, 04 Oct 2023 16:29:38 GMT
content-type: text/html; charset=utf-8
set-cookie: 1e2670d92730b515ce3a1bb65da45062=1bf5e9573c9a2760c964ed1659cc1673; path=/; HttpOnly; Secure; SameSite=None
cache-control: private</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="_load-balancing-with-metallb">
<title>Load balancing with MetalLB</title>
<section xml:id="about-metallb">
<title>About MetalLB and the MetalLB Operator</title>

<simpara>As a cluster administrator, you can add the MetalLB Operator to your cluster so that when a service of type <literal>LoadBalancer</literal> is added to the cluster, MetalLB can add an external IP address for the service.
The external IP address is added to the host network for your cluster.</simpara>
<section xml:id="nw-metallb-when-metallb_about-metallb-and-metallb-operator">
<title>When to use MetalLB</title>
<simpara>Using MetalLB is valuable when you have a bare-metal cluster, or an infrastructure that is like bare metal, and you want fault-tolerant access to an application through an external IP address.</simpara>
<simpara>You must configure your networking infrastructure to ensure that network traffic for the external IP address is routed from clients to the host network for the cluster.</simpara>
<simpara>After deploying MetalLB with the MetalLB Operator, when you add a service of type <literal>LoadBalancer</literal>, MetalLB provides a platform-native load balancer.</simpara>
<simpara>MetalLB operating in layer2 mode provides support for failover by utilizing a mechanism similar to IP failover. However, instead of relying on the virtual router redundancy protocol (VRRP) and keepalived, MetalLB leverages a gossip-based protocol to identify instances of node failure. When a failover is detected, another node assumes the role of the leader node, and a gratuitous ARP message is dispatched to broadcast this change.</simpara>
<simpara>MetalLB operating in layer3 or border gateway protocol (BGP) mode delegates failure detection to the network. The BGP router or routers that the OpenShift Container Platform nodes have established a connection with will identify any node failure and terminate the routes to that node.</simpara>
<simpara>Using MetalLB instead of IP failover is preferable for ensuring high availability of pods and services.</simpara>
</section>
<section xml:id="nw-metallb-operator-custom-resources_about-metallb-and-metallb-operator">
<title>MetalLB Operator custom resources</title>
<simpara>The MetalLB Operator monitors its own namespace for the following custom resources:</simpara>
<variablelist>
<varlistentry>
<term><literal>MetalLB</literal></term>
<listitem>
<simpara>When you add a <literal>MetalLB</literal> custom resource to the cluster, the MetalLB Operator deploys MetalLB on the cluster.
The Operator only supports a single instance of the custom resource.
If the instance is deleted, the Operator removes MetalLB from the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>IPAddressPool</literal></term>
<listitem>
<simpara>MetalLB requires one or more pools of IP addresses that it can assign to a service when you add a service of type <literal>LoadBalancer</literal>.
An <literal>IPAddressPool</literal> includes a list of IP addresses.
The list can be a single IP address that is set using a range, such as 1.1.1.1-1.1.1.1, a range specified in CIDR notation, a range specified as a starting and ending address separated by a hyphen, or a combination of the three.
An <literal>IPAddressPool</literal> requires a name.
The documentation uses names like <literal>doc-example</literal>, <literal>doc-example-reserved</literal>, and <literal>doc-example-ipv6</literal>.
The MetalLB <literal>controller</literal> assigns IP addresses from a pool of addresses in an <literal>IPAddressPool</literal>.
<literal>L2Advertisement</literal> and <literal>BGPAdvertisement</literal> custom resources enable the advertisement of a given IP from a given pool.
You can assign IP addresses from an <literal>IPAddressPool</literal> to services and namespaces by using the <literal>spec.serviceAllocation</literal> specification in the <literal>IPAddressPool</literal> custom resource.</simpara>
<note>
<simpara>A single <literal>IPAddressPool</literal> can be referenced by a L2 advertisement and a BGP advertisement.</simpara>
</note>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>BGPPeer</literal></term>
<listitem>
<simpara>The BGP peer custom resource identifies the BGP router for MetalLB to communicate with, the AS number of the router, the AS number for MetalLB, and customizations for route advertisement.
MetalLB advertises the routes for service load-balancer IP addresses to one or more BGP peers.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>BFDProfile</literal></term>
<listitem>
<simpara>The BFD profile custom resource configures Bidirectional Forwarding Detection (BFD) for a BGP peer.
BFD provides faster path failure detection than BGP alone provides.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>L2Advertisement</literal></term>
<listitem>
<simpara>The L2Advertisement custom resource advertises an IP coming from an <literal>IPAddressPool</literal> using the L2 protocol.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>BGPAdvertisement</literal></term>
<listitem>
<simpara>The BGPAdvertisement custom resource advertises an IP coming from an <literal>IPAddressPool</literal> using the BGP protocol.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>After you add the <literal>MetalLB</literal> custom resource to the cluster and the Operator deploys MetalLB, the <literal>controller</literal> and <literal>speaker</literal> MetalLB software components begin running.</simpara>
<simpara>MetalLB validates all relevant custom resources.</simpara>
</section>
<section xml:id="nw-metallb-software-components_about-metallb-and-metallb-operator">
<title>MetalLB software components</title>
<simpara>When you install the MetalLB Operator, the <literal>metallb-operator-controller-manager</literal> deployment starts a pod. The pod is the implementation of the Operator. The pod monitors for changes to all the relevant resources.</simpara>
<simpara>When the Operator starts an instance of MetalLB, it starts a <literal>controller</literal> deployment and a <literal>speaker</literal> daemon set.</simpara>
<note>
<simpara>You can configure deployment specifications in the MetalLB custom resource to manage how <literal>controller</literal> and <literal>speaker</literal> pods deploy and run in your cluster. For more information about these deployment specifications, see the <emphasis>Additional Resources</emphasis> section.</simpara>
</note>
<variablelist>
<varlistentry>
<term><literal>controller</literal></term>
<listitem>
<simpara>The Operator starts the deployment and a single pod. When you add a service of type <literal>LoadBalancer</literal>, Kubernetes uses the <literal>controller</literal> to allocate an IP address from an address pool.
In case of a service failure, verify you have the following entry in your <literal>controller</literal> pod logs:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"event":"ipAllocated","ip":"172.22.0.201","msg":"IP address assigned by controller</programlisting>
</para>
</formalpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>speaker</literal></term>
<listitem>
<simpara>The Operator starts a daemon set for <literal>speaker</literal> pods. By default, a pod is started on each node in your cluster. You can limit the pods to specific nodes by specifying a node selector in the <literal>MetalLB</literal> custom resource when you start MetalLB. If the <literal>controller</literal> allocated the IP address to the service and service is still unavailable, read the <literal>speaker</literal> pod logs. If the <literal>speaker</literal> pod is unavailable, run the <literal>oc describe pod -n</literal> command.</simpara>
<simpara>For layer 2 mode, after the <literal>controller</literal> allocates an IP address for the service, the <literal>speaker</literal> pods use an algorithm to determine which <literal>speaker</literal> pod on which node will announce the load balancer IP address.
The algorithm involves hashing the node name and the load balancer IP address. For more information, see "MetalLB and external traffic policy".
The <literal>speaker</literal> uses Address Resolution Protocol (ARP) to announce IPv4 addresses and Neighbor Discovery Protocol (NDP) to announce IPv6 addresses.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For Border Gateway Protocol (BGP) mode, after the <literal>controller</literal> allocates an IP address for the service, each <literal>speaker</literal> pod advertises the load balancer IP address with its BGP peers. You can configure which nodes start BGP sessions with BGP peers.</simpara>
<simpara>Requests for the load balancer IP address are routed to the node with the <literal>speaker</literal> that announces the IP address. After the node receives the packets, the service proxy routes the packets to an endpoint for the service. The endpoint can be on the same node in the optimal case, or it can be on another node. The service proxy chooses an endpoint each time a connection is established.</simpara>
</section>
<section xml:id="nw-metallb-extern-traffic-pol_about-metallb-and-metallb-operator">
<title>MetalLB and external traffic policy</title>
<simpara>With layer 2 mode, one node in your cluster receives all the traffic for the service IP address.
With BGP mode, a router on the host network opens a connection to one of the nodes in the cluster for a new client connection.
How your cluster handles the traffic after it enters the node is affected by the external traffic policy.</simpara>
<variablelist>
<varlistentry>
<term><literal>cluster</literal></term>
<listitem>
<simpara>This is the default value for <literal>spec.externalTrafficPolicy</literal>.</simpara>
<simpara>With the <literal>cluster</literal> traffic policy, after the node receives the traffic, the service proxy distributes the traffic to all the pods in your service.
This policy provides uniform traffic distribution across the pods, but it obscures the client IP address and it can appear to the application in your pods that the traffic originates from the node rather than the client.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>local</literal></term>
<listitem>
<simpara>With the <literal>local</literal> traffic policy, after the node receives the traffic, the service proxy only sends traffic to the pods on the same node.
For example, if the <literal>speaker</literal> pod on node A announces the external service IP, then all traffic is sent to node A.
After the traffic enters node A, the service proxy only sends traffic to pods for the service that are also on node A.
Pods for the service that are on additional nodes do not receive any traffic from node A.
Pods for the service on additional nodes act as replicas in case failover is needed.</simpara>
<simpara>This policy does not affect the client IP address.
Application pods can determine the client IP address from the incoming connections.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>The following information is important when configuring the external traffic policy in BGP mode.</simpara>
<simpara>Although MetalLB advertises the load balancer IP address from all the eligible nodes, the number of nodes loadbalancing the service can be limited by the capacity of the router to establish equal-cost multipath (ECMP) routes. If the number of nodes advertising the IP is greater than the ECMP group limit of the router, the router will use less nodes than the ones advertising the IP.</simpara>
<simpara>For example, if the external traffic policy is set to <literal>local</literal> and the router has an ECMP group limit set to 16 and the pods implementing a LoadBalancer service are deployed on 30 nodes, this would result in pods deployed on 14 nodes not receiving any traffic. In this situation, it would be preferable to set the external traffic policy for the service to <literal>cluster</literal>.</simpara>
</note>
</section>
<section xml:id="nw-metallb-layer2_about-metallb-and-metallb-operator">
<title>MetalLB concepts for layer 2 mode</title>
<simpara>In layer 2 mode, the <literal>speaker</literal> pod on one node announces the external IP address for a service to the host network.
From a network perspective, the node appears to have multiple IP addresses assigned to a network interface.</simpara>
<note>
<simpara>In layer 2 mode, MetalLB relies on ARP and NDP. These protocols implement local address resolution within a specific subnet. In this context, the client must be able to reach the VIP assigned by MetalLB that exists on the same subnet as the nodes announcing the service in order for MetalLB to work.</simpara>
</note>
<simpara>The <literal>speaker</literal> pod responds to ARP requests for IPv4 services and NDP requests for IPv6.</simpara>
<simpara>In layer 2 mode, all traffic for a service IP address is routed through one node.
After traffic enters the node, the service proxy for the CNI network provider distributes the traffic to all the pods for the service.</simpara>
<simpara>Because all traffic for a service enters through a single node in layer 2 mode, in a strict sense, MetalLB does not implement a load balancer for layer 2.
Rather, MetalLB implements a failover mechanism for layer 2 so that when a <literal>speaker</literal> pod becomes unavailable, a <literal>speaker</literal> pod on a different node can announce the service IP address.</simpara>
<simpara>When a node becomes unavailable, failover is automatic.
The <literal>speaker</literal> pods on the other nodes detect that a node is unavailable and a new <literal>speaker</literal> pod and node take ownership of the service IP address from the failed node.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/nw-metallb-layer2.png"/>
</imageobject>
<textobject><phrase>Conceptual diagram for MetalLB and layer 2 mode</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>The preceding graphic shows the following concepts related to MetalLB:</simpara>
<itemizedlist>
<listitem>
<simpara>An application is available through a service that has a cluster IP on the <literal>172.130.0.0/16</literal> subnet.
That IP address is accessible from inside the cluster.
The service also has an external IP address that MetalLB assigned to the service, <literal>192.168.100.200</literal>.</simpara>
</listitem>
<listitem>
<simpara>Nodes 1 and 3 have a pod for the application.</simpara>
</listitem>
<listitem>
<simpara>The <literal>speaker</literal> daemon set runs a pod on each node.
The MetalLB Operator starts these pods.</simpara>
</listitem>
<listitem>
<simpara>Each <literal>speaker</literal> pod is a host-networked pod.
The IP address for the pod is identical to the IP address for the node on the host network.</simpara>
</listitem>
<listitem>
<simpara>The <literal>speaker</literal> pod on node 1 uses ARP to announce the external IP address for the service, <literal>192.168.100.200</literal>.
The <literal>speaker</literal> pod that announces the external IP address must be on the same node as an endpoint for the service and the endpoint must be in the <literal>Ready</literal> condition.</simpara>
</listitem>
<listitem>
<simpara>Client traffic is routed to the host network and connects to the <literal>192.168.100.200</literal> IP address.
After traffic enters the node, the service proxy sends the traffic to the application pod on the same node or another node according to the external traffic policy that you set for the service.</simpara>
<itemizedlist>
<listitem>
<simpara>If the external traffic policy for the service is set to <literal>cluster</literal>, the node that advertises the <literal>192.168.100.200</literal> load balancer IP address is selected from the nodes where a <literal>speaker</literal> pod is running. Only that node can receive traffic for the service.</simpara>
</listitem>
<listitem>
<simpara>If the external traffic policy for the service is set to <literal>local</literal>, the node that advertises the <literal>192.168.100.200</literal> load balancer IP address is selected from the nodes where a <literal>speaker</literal> pod is running and at least an endpoint of the service. Only that node can receive traffic for the service. In the preceding graphic, either node 1 or 3 would advertise <literal>192.168.100.200</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If node 1 becomes unavailable, the external IP address fails over to another node.
On another node that has an instance of the application pod and service endpoint, the <literal>speaker</literal> pod begins to announce the external IP address, <literal>192.168.100.200</literal> and the new node receives the client traffic.
In the diagram, the only candidate is node 3.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-bgp_about-metallb-and-metallb-operator">
<title>MetalLB concepts for BGP mode</title>
<simpara>In BGP mode, by default each <literal>speaker</literal> pod advertises the load balancer IP address for a service to each BGP peer. It is also possible to advertise the IPs coming from a given pool to a specific set of peers by adding an optional list of BGP peers.
BGP peers are commonly network routers that are configured to use the BGP protocol.
When a router receives traffic for the load balancer IP address, the router picks one of the nodes with a <literal>speaker</literal> pod that advertised the IP address.
The router sends the traffic to that node.
After traffic enters the node, the service proxy for the CNI network plugin distributes the traffic to all the pods for the service.</simpara>
<simpara>The directly-connected router on the same layer 2 network segment as the cluster nodes can be configured as a BGP peer.
If the directly-connected router is not configured as a BGP peer, you need to configure your network so that packets for load balancer IP addresses are routed between the BGP peers and the cluster nodes that run the <literal>speaker</literal> pods.</simpara>
<simpara>Each time a router receives new traffic for the load balancer IP address, it creates a new connection to a node.
Each router manufacturer has an implementation-specific algorithm for choosing which node to initiate the connection with.
However, the algorithms commonly are designed to distribute traffic across the available nodes for the purpose of balancing the network load.</simpara>
<simpara>If a node becomes unavailable, the router initiates a new connection with another node that has a <literal>speaker</literal> pod that advertises the load balancer IP address.</simpara>
<figure>
<title>MetalLB topology diagram for BGP mode</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/209_OpenShift_BGP_0122.png"/>
</imageobject>
<textobject><phrase>Speaker pods on host network 10.0.1.0/24 use BGP to advertise the load balancer IP address, 203.0.113.200, to a router.</phrase></textobject>
</mediaobject>
</figure>
<simpara>The preceding graphic shows the following concepts related to MetalLB:</simpara>
<itemizedlist>
<listitem>
<simpara>An application is available through a service that has an IPv4 cluster IP on the <literal>172.130.0.0/16</literal> subnet.
That IP address is accessible from inside the cluster.
The service also has an external IP address that MetalLB assigned to the service, <literal>203.0.113.200</literal>.</simpara>
</listitem>
<listitem>
<simpara>Nodes 2 and 3 have a pod for the application.</simpara>
</listitem>
<listitem>
<simpara>The <literal>speaker</literal> daemon set runs a pod on each node.
The MetalLB Operator starts these pods.
You can configure MetalLB to specify which nodes run the <literal>speaker</literal> pods.</simpara>
</listitem>
<listitem>
<simpara>Each <literal>speaker</literal> pod is a host-networked pod.
The IP address for the pod is identical to the IP address for the node on the host network.</simpara>
</listitem>
<listitem>
<simpara>Each <literal>speaker</literal> pod starts a BGP session with all BGP peers and advertises the load balancer IP addresses or aggregated routes to the BGP peers.
The <literal>speaker</literal> pods advertise that they are part of Autonomous System 65010.
The diagram shows a router, R1, as a BGP peer within the same Autonomous System.
However, you can configure MetalLB to start BGP sessions with peers that belong to other Autonomous Systems.</simpara>
</listitem>
<listitem>
<simpara>All the nodes with a <literal>speaker</literal> pod that advertises the load balancer IP address can receive traffic for the service.</simpara>
<itemizedlist>
<listitem>
<simpara>If the external traffic policy for the service is set to <literal>cluster</literal>, all the nodes where a speaker pod is running advertise the <literal>203.0.113.200</literal> load balancer IP address and all the nodes with a <literal>speaker</literal> pod can receive traffic for the service. The host prefix is advertised to the router peer only if the external traffic policy is set to cluster.</simpara>
</listitem>
<listitem>
<simpara>If the external traffic policy for the service is set to <literal>local</literal>, then all the nodes where a <literal>speaker</literal> pod is running and at least an endpoint of the service is running can advertise the <literal>203.0.113.200</literal> load balancer IP address. Only those nodes can receive traffic for the service. In the preceding graphic, nodes 2 and 3 would advertise <literal>203.0.113.200</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You can configure MetalLB to control which <literal>speaker</literal> pods start BGP sessions with specific BGP peers by specifying a node selector when you add a BGP peer custom resource.</simpara>
</listitem>
<listitem>
<simpara>Any routers, such as R1, that are configured to use BGP can be set as BGP peers.</simpara>
</listitem>
<listitem>
<simpara>Client traffic is routed to one of the nodes on the host network.
After traffic enters the node, the service proxy sends the traffic to the application pod on the same node or another node according to the external traffic policy that you set for the service.</simpara>
</listitem>
<listitem>
<simpara>If a node becomes unavailable, the router detects the failure and initiates a new connection with another node.
You can configure MetalLB to use a Bidirectional Forwarding Detection (BFD) profile for BGP peers.
BFD provides faster link failure detection so that routers can initiate new connections earlier than without BFD.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="limitations-and-restrictions_about-metallb-and-metallb-operator">
<title>Limitations and restrictions</title>
<section xml:id="nw-metallb-infra-considerations_about-metallb-and-metallb-operator">
<title>Infrastructure considerations for MetalLB</title>
<simpara>MetalLB is primarily useful for on-premise, bare metal installations because these installations do not include a native load-balancer capability.
In addition to bare metal installations, installations of OpenShift Container Platform on some infrastructures might not include a native load-balancer capability.
For example, the following infrastructures can benefit from adding the MetalLB Operator:</simpara>
<itemizedlist>
<listitem>
<simpara>Bare metal</simpara>
</listitem>
<listitem>
<simpara>VMware vSphere</simpara>
</listitem>
<listitem>
<simpara>IBM Z&#174; and IBM&#174; LinuxONE</simpara>
</listitem>
<listitem>
<simpara>IBM Z&#174; and IBM&#174; LinuxONE for Red Hat Enterprise Linux (RHEL) KVM</simpara>
</listitem>
<listitem>
<simpara>IBM Power&#174;</simpara>
</listitem>
</itemizedlist>
<simpara>MetalLB Operator and MetalLB are supported with the OpenShift SDN and OVN-Kubernetes network providers.</simpara>
</section>
<section xml:id="nw-metallb-layer2-limitations_about-metallb-and-metallb-operator">
<title>Limitations for layer 2 mode</title>
<section xml:id="nw-metallb-layer2-limitations-bottleneck_about-metallb-and-metallb-operator">
<title>Single-node bottleneck</title>
<simpara>MetalLB routes all traffic for a service through a single node, the node can become a bottleneck and limit performance.</simpara>
<simpara>Layer 2 mode limits the ingress bandwidth for your service to the bandwidth of a single node.
This is a fundamental limitation of using ARP and NDP to direct traffic.</simpara>
</section>
<section xml:id="nw-metallb-layer2-limitations-failover_about-metallb-and-metallb-operator">
<title>Slow failover performance</title>
<simpara>Failover between nodes depends on cooperation from the clients.
When a failover occurs, MetalLB sends gratuitous ARP packets to notify clients that the MAC address associated with the service IP has changed.</simpara>
<simpara>Most client operating systems handle gratuitous ARP packets correctly and update their neighbor caches promptly.
When clients update their caches quickly, failover completes within a few seconds.
Clients typically fail over to a new node within 10 seconds.
However, some client operating systems either do not handle gratuitous ARP packets at all or have outdated implementations that delay the cache update.</simpara>
<simpara>Recent versions of common operating systems such as Windows, macOS, and Linux implement layer 2 failover correctly.
Issues with slow failover are not expected except for older and less common client operating systems.</simpara>
<simpara>To minimize the impact from a planned failover on outdated clients, keep the old node running for a few minutes after flipping leadership.
The old node can continue to forward traffic for outdated clients until their caches refresh.</simpara>
<simpara>During an unplanned failover, the service IPs are unreachable until the outdated clients refresh their cache entries.</simpara>
</section>
<section xml:id="additional_network_and_metallb_limitation_about-metallb-and-metallb-operator">
<title>Additional Network and MetalLB cannot use same network</title>
<simpara>Using the same VLAN for both MetalLB and an additional network interface set up on a source pod might result in a connection failure. This occurs when both the MetalLB IP and the source pod reside on the same node.</simpara>
<simpara>To avoid connection failures, place the MetalLB IP in a different subnet from the one where the source pod resides. This configuration ensures that traffic from the source pod will take the default gateway. Consequently, the traffic can effectively reach its destination by using the OVN overlay network, ensuring that the connection functions as intended.</simpara>
</section>
</section>
<section xml:id="nw-metallb-bgp-limitations_about-metallb-and-metallb-operator">
<title>Limitations for BGP mode</title>
<section xml:id="nw-metallb-bgp-limitations-break-connections_about-metallb-and-metallb-operator">
<title>Node failure can break all active connections</title>
<simpara>MetalLB shares a limitation that is common to BGP-based load balancing.
When a BGP session terminates, such as when a node fails or when a <literal>speaker</literal> pod restarts, the session termination might result in resetting all active connections.
End users can experience a <literal>Connection reset by peer</literal> message.</simpara>
<simpara>The consequence of a terminated BGP session is implementation-specific for each router manufacturer.
However, you can anticipate that a change in the number of <literal>speaker</literal> pods affects the number of BGP sessions and that active connections with BGP peers will break.</simpara>
<simpara>To avoid or reduce the likelihood of a service interruption, you can specify a node selector when you add a BGP peer.
By limiting the number of nodes that start BGP sessions, a fault on a node that does not have a BGP session has no affect on connections to the service.</simpara>
</section>
<section xml:id="nw-metallb-bgp-limitations-single-asn_about-metallb-and-metallb-operator">
<title>Support for a single ASN and a single router ID only</title>
<simpara>When you add a BGP peer custom resource, you specify the <literal>spec.myASN</literal> field to identify the Autonomous System Number (ASN) that MetalLB belongs to.
OpenShift Container Platform uses an implementation of BGP with MetalLB that requires MetalLB to belong to a single ASN.
If you attempt to add a BGP peer and specify a different value for <literal>spec.myASN</literal> than an existing BGP peer custom resource, you receive an error.</simpara>
<simpara>Similarly, when you add a BGP peer custom resource, the <literal>spec.routerID</literal> field is optional.
If you specify a value for this field, you must specify the same value for all other BGP peer custom resources that you add.</simpara>
<simpara>The limitation to support a single ASN and single router ID is a difference with the community-supported implementation of MetalLB.</simpara>
</section>
</section>
</section>
<section xml:id="additional-resources_about-metallb-and-metallb-operator" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="overview-traffic-comparision_overview-traffic">Comparison: Fault tolerant access to external IP addresses</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-ipfailover-remove_configuring-ipfailover">Removing IP failover</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-metallb-operator-deployment-specifications-for-metallb_metallb-operator-install">Deployment specifications for MetalLB</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-operator-install">
<title>Installing the MetalLB Operator</title>

<simpara>As a cluster administrator, you can add the MetallB Operator so that the Operator can manage the lifecycle for an instance of MetalLB on your cluster.</simpara>
<simpara>MetalLB and IP failover are incompatible. If you configured IP failover for your cluster, perform the steps to <link linkend="nw-ipfailover-remove_configuring-ipfailover">remove IP failover</link> before you install the Operator.</simpara>
<section xml:id="installing-the-metallb-operator-using-web-console_metallb-operator-install">
<title>Installing the MetalLB Operator from the OperatorHub using the web console</title>
<simpara>As a cluster administrator, you can install the MetalLB Operator by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Type a keyword into the <emphasis role="strong">Filter by keyword</emphasis> box or scroll to find the Operator you want. For example, type <literal>metallb</literal> to find the MetalLB Operator.</simpara>
<simpara>You can also filter options by <emphasis role="strong">Infrastructure Features</emphasis>. For example, select <emphasis role="strong">Disconnected</emphasis> if you want to see Operators that work in disconnected environments, also known as restricted network environments.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, accept the defaults and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>To confirm that the installation is successful:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Check that the Operator is installed in the <literal>openshift-operators</literal> namespace and that its status is <literal>Succeeded</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>If the Operator is not installed successfully, check the status of the Operator and review the logs:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page and inspect the <literal>Status</literal> column for any errors or failures.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and check the logs in any pods in the <literal>openshift-operators</literal> project that are reporting issues.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-installing-operator-cli_metallb-operator-install">
<title>Installing from OperatorHub using the CLI</title>
<simpara>Instead of using the OpenShift Container Platform web console, you can install an Operator from OperatorHub using the CLI. You can use the OpenShift CLI (<literal>oc</literal>) to install the MetalLB Operator.</simpara>
<simpara>It is recommended that when using the CLI you install the Operator in the <literal>metallb-system</literal> namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster installed on bare-metal hardware.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a namespace for the MetalLB Operator by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: v1
kind: Namespace
metadata:
  name: metallb-system
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create an Operator group custom resource (CR) in the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: metallb-operator
  namespace: metallb-system
EOF</programlisting>
</listitem>
<listitem>
<simpara>Confirm the Operator group is installed in the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get operatorgroup -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME               AGE
metallb-operator   14m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>Subscription</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Define the <literal>Subscription</literal> CR and save the YAML file, for example, <literal>metallb-sub.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: metallb-operator-sub
  namespace: metallb-system
spec:
  channel: stable
  name: metallb-operator
  source: redhat-operators <co xml:id="CO261-1"/>
  sourceNamespace: openshift-marketplace</programlisting>
<calloutlist>
<callout arearefs="CO261-1">
<para>You must specify the <literal>redhat-operators</literal> value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To create the <literal>Subscription</literal> CR, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f metallb-sub.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: To ensure BGP and BFD metrics appear in Prometheus, you can label the namespace as in the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label ns metallb-system "openshift.io/cluster-monitoring=true"</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>The verification steps assume the MetalLB Operator is installed in the <literal>metallb-system</literal> namespace.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Confirm the install plan is in the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get installplan -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME            CSV                                   APPROVAL    APPROVED
install-wzg94   metallb-operator.4.14.0-nnnnnnnnnnnn   Automatic   true</programlisting>
</para>
</formalpara>
<note>
<simpara>Installation of the Operator might take a few seconds.</simpara>
</note>
</listitem>
<listitem>
<simpara>To verify that the Operator is installed, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterserviceversion -n metallb-system \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name                                  Phase
metallb-operator.4.14.0-nnnnnnnnnnnn   Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-operator-initial-config_metallb-operator-install">
<title>Starting MetalLB on your cluster</title>
<simpara>After you install the Operator, you need to configure a single instance of a MetalLB custom resource. After you configure the custom resource, the Operator starts MetalLB on your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Install the MetalLB Operator.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>This procedure assumes the MetalLB Operator is installed in the <literal>metallb-system</literal> namespace. If you installed using the web console substitute <literal>openshift-operators</literal> for the namespace.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a single instance of a MetalLB custom resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
EOF</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>Confirm that the deployment for the MetalLB controller and the daemon set for the MetalLB speaker are running.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that the deployment for the controller is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -n metallb-system controller</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME         READY   UP-TO-DATE   AVAILABLE   AGE
controller   1/1     1            1           11m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the daemon set for the speaker is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get daemonset -n metallb-system speaker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
speaker   6         6         6       6            6           kubernetes.io/os=linux   18m</programlisting>
</para>
</formalpara>
<simpara>The example output indicates 6 speaker pods. The number of speaker pods in your cluster might differ from the example output. Make sure the output indicates one pod for each node in your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-operator-deployment-specifications-for-metallb_metallb-operator-install">
<title>Deployment specifications for MetalLB</title>
<simpara>When you start an instance of MetalLB using the <literal>MetalLB</literal> custom resource, you can configure deployment specifications in the <literal>MetalLB</literal> custom resource to manage how the <literal>controller</literal> or <literal>speaker</literal> pods deploy and run in your cluster. Use these deployment specifications to manage the following tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Select nodes for MetalLB pod deployment.</simpara>
</listitem>
<listitem>
<simpara>Manage scheduling by using pod priority and pod affinity.</simpara>
</listitem>
<listitem>
<simpara>Assign CPU limits for MetalLB pods.</simpara>
</listitem>
<listitem>
<simpara>Assign a container RuntimeClass for MetalLB pods.</simpara>
</listitem>
<listitem>
<simpara>Assign metadata for MetalLB pods.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-metallb-operator-limit-speaker-to-nodes_metallb-operator-install">
<title>Limit speaker pods to specific nodes</title>
<simpara>By default, when you start MetalLB with the MetalLB Operator, the Operator starts an instance of a <literal>speaker</literal> pod on each node in the cluster.
Only the nodes with a <literal>speaker</literal> pod can advertise a load balancer IP address.
You can configure the <literal>MetalLB</literal> custom resource with a node selector to specify which nodes run the <literal>speaker</literal> pods.</simpara>
<simpara>The most common reason to limit the <literal>speaker</literal> pods to specific nodes is to ensure that only nodes with network interfaces on specific networks advertise load balancer IP addresses.
Only the nodes with a running <literal>speaker</literal> pod are advertised as destinations of the load balancer IP address.</simpara>
<simpara>If you limit the <literal>speaker</literal> pods to specific nodes and specify <literal>local</literal> for the external traffic policy of a service, then you must ensure that the application pods for the service are deployed to the same nodes.</simpara>
<formalpara>
<title>Example configuration to limit speaker pods to worker nodes</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  nodeSelector:  <co xml:id="CO262-1"/>
    node-role.kubernetes.io/worker: ""
  speakerTolerations:   <co xml:id="CO262-2"/>
  - key: "Example"
    operator: "Exists"
    effect: "NoExecute"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO262-1">
<para>The example configuration specifies to assign the speaker pods to worker nodes, but you can specify labels that you assigned to nodes or any valid node selector.</para>
</callout>
<callout arearefs="CO262-2">
<para>In this example configuration, the pod that this toleration is attached to tolerates any taint that matches the <literal>key</literal> value and <literal>effect</literal> value using the <literal>operator</literal>.</para>
</callout>
</calloutlist>
<simpara>After you apply a manifest with the <literal>spec.nodeSelector</literal> field, you can check the number of pods that the Operator deployed with the <literal>oc get daemonset -n metallb-system speaker</literal> command.
Similarly, you can display the nodes that match your labels with a command like <literal>oc get nodes -l node-role.kubernetes.io/worker=</literal>.</simpara>
<simpara>You can optionally allow the node to control which speaker pods should, or should not, be scheduled on them by using affinity rules. You can also limit these pods by applying a list of tolerations. For more information about affinity rules, taints, and tolerations, see the additional resources.</simpara>
</section>
<section xml:id="nw-metallb-operator-setting-runtimeclass_metallb-operator-install">
<title>Configuring a container runtime class in a MetalLB deployment</title>
<simpara>You can optionally assign a container runtime class to <literal>controller</literal> and <literal>speaker</literal> pods by configuring the MetalLB custom resource. For example, for Windows workloads, you can assign a Windows runtime class to the pod, which uses this runtime class for all containers in the pod.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the MetalLB Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>RuntimeClass</literal> custom resource, such as <literal>myRuntimeClass.yaml</literal>, to define your runtime class:</simpara>
<programlisting role="white-space-pre" language="yaml" linenumbering="unnumbered">apiVersion: node.k8s.io/v1
kind: RuntimeClass
metadata:
  name: myclass
handler: myconfiguration</programlisting>
</listitem>
<listitem>
<simpara>Apply the <literal>RuntimeClass</literal> custom resource configuration:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc apply -f myRuntimeClass.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>MetalLB</literal> custom resource, such as <literal>MetalLBRuntime.yaml</literal>, to specify the <literal>runtimeClassName</literal> value:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    runtimeClassName: myclass
    annotations: <co xml:id="CO263-1"/>
      controller: demo
  speakerConfig:
    runtimeClassName: myclass
    annotations: <co xml:id="CO263-2"/>
      speaker: demo</programlisting>
<calloutlist>
<callout arearefs="CO263-1 CO263-2">
<para>This example uses <literal>annotations</literal> to add metadata such as build release information or GitHub pull request information. You can populate annotations with characters that are not permitted in labels. However, you cannot use annotations to identify or select objects.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>MetalLB</literal> custom resource configuration:</simpara>
<programlisting role="white-space-pre" language="bash" linenumbering="unnumbered">$ oc apply -f MetalLBRuntime.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To view the container runtime for a pod, run the following command:</simpara>
<programlisting role="white-space-pre" language="bash" linenumbering="unnumbered">$ oc get pod -o custom-columns=NAME:metadata.name,STATUS:.status.phase,RUNTIME_CLASS:.spec.runtimeClassName</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-operator-setting-pod-priority-affinity_metallb-operator-install">
<title>Configuring pod priority and pod affinity in a MetalLB deployment</title>
<simpara>You can optionally assign pod priority and pod affinity rules to <literal>controller</literal> and <literal>speaker</literal> pods by configuring the <literal>MetalLB</literal> custom resource. The pod priority indicates the relative importance of a pod on a node and schedules the pod based on this priority. Set a high priority on your <literal>controller</literal> or <literal>speaker</literal> pod to ensure scheduling priority over other pods on the node.</simpara>
<simpara>Pod affinity manages relationships among pods. Assign pod affinity to the <literal>controller</literal> or <literal>speaker</literal> pods to control on what node the scheduler places the pod in the context of pod relationships. For example, you can use pod affinity rules to ensure that certain pods are located on the same node or nodes, which can help improve network communication and reduce latency between those components.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the MetalLB Operator.</simpara>
</listitem>
<listitem>
<simpara>You have started the MetalLB Operator on your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>PriorityClass</literal> custom resource, such as <literal>myPriorityClass.yaml</literal>, to configure the priority level. This example defines a <literal>PriorityClass</literal> named <literal>high-priority</literal> with a value of <literal>1000000</literal>. Pods that are assigned this priority class are considered higher priority during scheduling compared to pods with lower priority classes:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: scheduling.k8s.io/v1
kind: PriorityClass
metadata:
  name: high-priority
value: 1000000</programlisting>
</listitem>
<listitem>
<simpara>Apply the <literal>PriorityClass</literal> custom resource configuration:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc apply -f myPriorityClass.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>MetalLB</literal> custom resource, such as <literal>MetalLBPodConfig.yaml</literal>, to specify the <literal>priorityClassName</literal> and <literal>podAffinity</literal> values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    priorityClassName: high-priority <co xml:id="CO264-1"/>
    affinity:
      podAffinity: <co xml:id="CO264-2"/>
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
             app: metallb
          topologyKey: kubernetes.io/hostname
  speakerConfig:
    priorityClassName: high-priority
    affinity:
      podAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
             app: metallb
          topologyKey: kubernetes.io/hostname</programlisting>
<calloutlist>
<callout arearefs="CO264-1">
<para>Specifies the priority class for the MetalLB controller pods. In this case, it is set to <literal>high-priority</literal>.</para>
</callout>
<callout arearefs="CO264-2">
<para>Specifies that you are configuring pod affinity rules. These rules dictate how pods are scheduled in relation to other pods or nodes. This configuration instructs the scheduler to schedule pods that have the label <literal>app: metallb</literal> onto nodes that share the same hostname. This helps to co-locate MetalLB-related pods on the same nodes, potentially optimizing network communication, latency, and resource usage between these pods.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>MetalLB</literal> custom resource configuration:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc apply -f MetalLBPodConfig.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To view the priority class that you assigned to pods in the <literal>metallb-system</literal> namespace, run the following command:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc get pods -n metallb-system -o custom-columns=NAME:.metadata.name,PRIORITY:.spec.priorityClassName</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                 PRIORITY
controller-584f5c8cd8-5zbvg                          high-priority
metallb-operator-controller-manager-9c8d9985-szkqg   &lt;none&gt;
metallb-operator-webhook-server-c895594d4-shjgx      &lt;none&gt;
speaker-dddf7                                        high-priority</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To verify that the scheduler placed pods according to pod affinity rules, view the metadata for the pod&#8217;s node or nodes by running the following command:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc get pod -o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name -n metallb-system</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-operator-setting-pod-CPU-limits_metallb-operator-install">
<title>Configuring pod CPU limits in a MetalLB deployment</title>
<simpara>You can optionally assign pod CPU limits to <literal>controller</literal> and <literal>speaker</literal> pods by configuring the <literal>MetalLB</literal> custom resource. Defining CPU limits for the <literal>controller</literal> or <literal>speaker</literal> pods helps you to manage compute resources on the node. This ensures all pods on the node have the necessary compute resources to manage workloads and cluster housekeeping.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the MetalLB Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>MetalLB</literal> custom resource file, such as <literal>CPULimits.yaml</literal>, to specify the <literal>cpu</literal> value for the <literal>controller</literal> and <literal>speaker</literal> pods:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  controllerConfig:
    resources:
      limits:
        cpu: "200m"
  speakerConfig:
    resources:
      limits:
        cpu: "300m"</programlisting>
</listitem>
<listitem>
<simpara>Apply the <literal>MetalLB</literal> custom resource configuration:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc apply -f CPULimits.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To view compute resources for a pod, run the following command, replacing <literal>&lt;pod_name&gt;</literal> with your target pod:</simpara>
<programlisting language="bash" linenumbering="unnumbered">$ oc describe pod &lt;pod_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_metallb-operator-install" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-taints-tolerations-about">Understanding taints and tolerations</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-pods-priority-about_nodes-pods-priority">Understanding pod priority</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-pod-affinity-about_nodes-scheduler-pod-affinity">Understanding pod affinity</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="next-steps_metallb-operator-install">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="metallb-configure-address-pools">Configuring MetalLB address pools</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-upgrading-operator">
<title>Upgrading the MetalLB</title>

<simpara>If you are currently running version 4.10 or an earlier version of the MetalLB Operator, please note that automatic updates to any version later than 4.10 do not work. Upgrading to a newer version from any version of the MetalLB Operator that is 4.11 or later is successful. For example, upgrading from version 4.12 to version 4.13 will occur smoothly.</simpara>
<simpara>A summary of the upgrade procedure for the MetalLB Operator from 4.10 and earlier is as follows:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Delete the installed MetalLB Operator version for example 4.10. Ensure that the namespace and the <literal>metallb</literal> custom resource are not removed.</simpara>
</listitem>
<listitem>
<simpara>Using the CLI, install the MetalLB Operator 4.14 in the same namespace where the previous version of the MetalLB Operator was installed.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>This procedure does not apply to automatic z-stream updates of the MetalLB Operator, which follow the standard straightforward method.</simpara>
</note>
<simpara>For detailed steps to upgrade the MetalLB Operator from 4.10 and earlier, see the guidance that follows. As a cluster administrator, start the upgrade process by deleting the MetalLB Operator by using the OpenShift CLI (<literal>oc</literal>) or the web console.</simpara>
<section xml:id="olm-deleting-metallb-operator-from-a-cluster-using-web-console_metallb-upgrading-operator">
<title>Deleting the MetalLB Operator from a cluster using the web console</title>
<simpara>Cluster administrators can delete installed Operators from a selected namespace by using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to an OpenShift Container Platform cluster web console using an account with
<literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Search for the MetalLB Operator. Then, click on it.</simpara>
</listitem>
<listitem>
<simpara>On the right side of the <emphasis role="strong">Operator Details</emphasis> page, select <emphasis role="strong">Uninstall Operator</emphasis> from the <emphasis role="strong">Actions</emphasis> drop-down menu.</simpara>
<simpara>An <emphasis role="strong">Uninstall Operator?</emphasis> dialog box is displayed.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Uninstall</emphasis> to remove the Operator, Operator deployments, and pods. Following this action, the Operator stops running and no longer receives updates.</simpara>
<note>
<simpara>This action does not remove resources managed by the Operator, including custom resource definitions (CRDs) and custom resources (CRs). Dashboards and navigation items enabled by the web console and off-cluster resources that continue to run might need manual clean up. To remove these after uninstalling the Operator, you might need to manually delete the Operator CRDs.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="olm-deleting-metallb-operator-from-a-cluster-using-cli_metallb-upgrading-operator">
<title>Deleting MetalLB Operator from a cluster using the CLI</title>
<simpara>Cluster administrators can delete installed Operators from a selected namespace by using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to an OpenShift Container Platform cluster using an account with
<literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara><literal>oc</literal> command installed on workstation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the current version of the subscribed MetalLB Operator in the <literal>currentCSV</literal> field:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get subscription metallb-operator -n metallb-system -o yaml | grep currentCSV</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">  currentCSV: metallb-operator.4.10.0-202207051316</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the subscription:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete subscription metallb-operator -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">subscription.operators.coreos.com "metallb-operator" deleted</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the CSV for the Operator in the target namespace using the <literal>currentCSV</literal> value from the previous step:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete clusterserviceversion metallb-operator.4.10.0-202207051316 -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">clusterserviceversion.operators.coreos.com "metallb-operator.4.10.0-202207051316" deleted</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="olm-updating-metallb-operatorgroup_metallb-upgrading-operator">
<title>Editing the MetalLB Operator Operator group</title>
<simpara>When upgrading from any MetalLB Operator version up to and including 4.10 to 4.11 and later, remove <literal>spec.targetNamespaces</literal> from the Operator group custom resource (CR). You must remove the spec regardless of whether you used the web console or the CLI to delete the MetalLB Operator.</simpara>
<note>
<simpara>The MetalLB Operator version 4.11 or later only supports the <literal>AllNamespaces</literal> install mode, whereas 4.10 or earlier versions support <literal>OwnNamespace</literal> or <literal>SingleNamespace</literal> modes.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List the Operator groups in the <literal>metallb-system</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get operatorgroup -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                   AGE
metallb-system-7jc66   85m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>spec.targetNamespaces</literal> is present in the Operator group CR associated with the <literal>metallb-system</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get operatorgroup metallb-system-7jc66 -n metallb-system -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  annotations:
    olm.providedAPIs: ""
  creationTimestamp: "2023-10-25T09:42:49Z"
  generateName: metallb-system-
  generation: 1
  name: metallb-system-7jc66
  namespace: metallb-system
  resourceVersion: "25027"
  uid: f5f644a0-eef8-4e31-a306-e2bbcfaffab3
spec:
  targetNamespaces:
  - metallb-system
  upgradeStrategy: Default
status:
  lastUpdated: "2023-10-25T09:42:49Z"
  namespaces:
  - metallb-system</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Edit the Operator group and remove the <literal>targetNamespaces</literal> and <literal>metallb-system</literal> present under the <literal>spec</literal> section by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">operatorgroup.operators.coreos.com/metallb-system-7jc66 edited</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>spec.targetNamespaces</literal> is removed from the Operator group custom resource associated with the <literal>metallb-system</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get operatorgroup metallb-system-7jc66 -n metallb-system -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  annotations:
    olm.providedAPIs: ""
  creationTimestamp: "2023-10-25T09:42:49Z"
  generateName: metallb-system-
  generation: 2
  name: metallb-system-7jc66
  namespace: metallb-system
  resourceVersion: "61658"
  uid: f5f644a0-eef8-4e31-a306-e2bbcfaffab3
spec:
  upgradeStrategy: Default
status:
  lastUpdated: "2023-10-25T14:31:30Z"
  namespaces:
  - ""</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="upgrading-metallb-operator_metallb-upgrading-operator">
<title>Upgrading the MetalLB Operator</title>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify that the <literal>metallb-system</literal> namespace still exists:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get namespaces | grep metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">metallb-system                                     Active   31m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>metallb</literal> custom resource still exists:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get metallb -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      AGE
metallb   33m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Follow the guidance in "Installing from OperatorHub using the CLI" to install the latest 4.14 version of the MetalLB Operator.</simpara>
<note>
<simpara>When installing the latest 4.14 version of the MetalLB Operator, you must install the Operator to the same namespace it was previously installed to.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify the upgraded version of the Operator is now the 4.14 version.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n metallb-system</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                   DISPLAY            VERSION               REPLACES   PHASE
metallb-operator.4.14.0-202207051316   MetalLB Operator   4.14.0-202207051316              Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-deleting-operators-from-a-cluster">Deleting Operators from a cluster</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="metallb-operator-install">Installing the MetalLB Operator</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-configure-address-pools">
<title>Configuring MetalLB address pools</title>

<simpara>As a cluster administrator, you can add, modify, and delete address pools.
The MetalLB Operator uses the address pool custom resources to set the IP addresses that MetalLB can assign to services. The namespace used in the examples assume the namespace is <literal>metallb-system</literal>.</simpara>
<section xml:id="nw-metallb-ipaddresspool-cr_configure-metallb-address-pools">
<title>About the IPAddressPool custom resource</title>
<note>
<simpara>The address pool custom resource definition (CRD) and API documented in "Load balancing with MetalLB" in OpenShift Container Platform 4.10 can still be used in 4.14. However, the enhanced functionality associated with advertising an IP address from an <literal>IPAddressPool</literal> with layer 2 protocols, or the BGP protocol, is not supported when using the <literal>AddressPool</literal> CRD.</simpara>
</note>
<simpara>The fields for the <literal>IPAddressPool</literal> custom resource are described in the following tables.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MetalLB IPAddressPool pool custom resource</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the name for the address pool.
When you add a service, you can specify this pool name in the <literal>metallb.universe.tf/address-pool</literal> annotation to select an IP address from a specific pool.
The names <literal>doc-example</literal>, <literal>silver</literal>, and <literal>gold</literal> are used throughout the documentation.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the namespace for the address pool.
Specify the same namespace that the MetalLB Operator uses.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.label</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the key value pair assigned to the <literal>IPAddressPool</literal>. This can be referenced by the <literal>ipAddressPoolSelectors</literal> in the <literal>BGPAdvertisement</literal> and <literal>L2Advertisement</literal> CRD to associate the <literal>IPAddressPool</literal> with the advertisement</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.addresses</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a list of IP addresses for MetalLB Operator to assign to services.
You can specify multiple ranges in a single pool; they will all share the same settings.
Specify each range in CIDR notation or as starting and ending IP addresses separated with a hyphen.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.autoAssign</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies whether MetalLB automatically assigns IP addresses from this pool.
Specify <literal>false</literal> if you want explicitly request an IP address from this pool with the <literal>metallb.universe.tf/address-pool</literal> annotation.
The default value is <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.avoidBuggyIPs</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: This ensures when enabled that IP addresses ending .0 and .255 are not allocated from the pool. The default value is <literal>false</literal>. Some older consumer network equipment mistakenly block IP addresses ending in .0 and .255.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>You can assign IP addresses from an <literal>IPAddressPool</literal> to services and namespaces by configuring the <literal>spec.serviceAllocation</literal> specification.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MetalLB IPAddressPool custom resource spec.serviceAllocation subfields</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>priority</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>int</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Defines the priority between IP address pools when more than one IP address pool matches a service or namespace. A lower number indicates a higher priority.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>namespaces</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array (string)</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies a list of namespaces that you can assign to IP addresses in an IP address pool.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>namespaceSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array (LabelSelector)</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies namespace labels that you can assign to IP addresses from an IP address pool by using label selectors in a list format.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>serviceSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>array (LabelSelector)</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies service labels that you can assign to IP addresses from an address pool by using label selectors in a list format.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-metallb-configure-address-pool_configure-metallb-address-pools">
<title>Configuring an address pool</title>
<simpara>As a cluster administrator, you can add address pools to your cluster to control the IP addresses that MetalLB can assign to load-balancer services.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example
  labels: <co xml:id="CO265-1"/>
    zone: east
spec:
  addresses:
  - 203.0.113.1-203.0.113.10
  - 203.0.113.65-203.0.113.75</programlisting>
<calloutlist>
<callout arearefs="CO265-1">
<para>This label assigned to the <literal>IPAddressPool</literal> can be referenced by the <literal>ipAddressPoolSelectors</literal> in the <literal>BGPAdvertisement</literal> CRD to associate the <literal>IPAddressPool</literal> with the advertisement.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>View the address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe -n metallb-system IPAddressPool doc-example</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         doc-example
Namespace:    metallb-system
Labels:       zone=east
Annotations:  &lt;none&gt;
API Version:  metallb.io/v1beta1
Kind:         IPAddressPool
Metadata:
  ...
Spec:
  Addresses:
    203.0.113.1-203.0.113.10
    203.0.113.65-203.0.113.75
  Auto Assign:  true
Events:         &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<simpara>Confirm that the address pool name, such as <literal>doc-example</literal>, and the IP address ranges appear in the output.</simpara>
</section>
<section xml:id="nw-metallb-example-addresspool_configure-metallb-address-pools">
<title>Example address pool configurations</title>
<section xml:id="_example-ipv4-and-cidr-ranges">
<title>Example: IPv4 and CIDR ranges</title>
<simpara>You can specify a range of IP addresses in CIDR notation.
You can combine CIDR notation with the notation that uses a hyphen to separate lower and upper bounds.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-cidr
  namespace: metallb-system
spec:
  addresses:
  - 192.168.100.0/24
  - 192.168.200.0/24
  - 192.168.255.1-192.168.255.5</programlisting>
</section>
<section xml:id="_example-reserve-ip-addresses">
<title>Example: Reserve IP addresses</title>
<simpara>You can set the <literal>autoAssign</literal> field to <literal>false</literal> to prevent MetalLB from automatically assigning the IP addresses from the pool.
When you add a service, you can request a specific IP address from the pool or you can specify the pool name in an annotation to request any IP address from the pool.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-reserved
  namespace: metallb-system
spec:
  addresses:
  - 10.0.100.0/28
  autoAssign: false</programlisting>
</section>
<section xml:id="_example-ipv4-and-ipv6-addresses">
<title>Example: IPv4 and IPv6 addresses</title>
<simpara>You can add address pools that use IPv4 and IPv6.
You can specify multiple ranges in the <literal>addresses</literal> list, just like several IPv4 examples.</simpara>
<simpara>Whether the service is assigned a single IPv4 address, a single IPv6 address, or both is determined by how you add the service.
The <literal>spec.ipFamilies</literal> and <literal>spec.ipFamilyPolicy</literal> fields control how IP addresses are assigned to the service.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-combined
  namespace: metallb-system
spec:
  addresses:
  - 10.0.100.0/28
  - 2002:2:2::1-2002:2:2::100</programlisting>
</section>
<section xml:id="_example-assign-ip-address-pools-to-services-or-namespaces">
<title>Example: Assign IP address pools to services or namespaces</title>
<simpara>You can assign IP addresses from an <literal>IPAddressPool</literal> to services and namespaces that you specify.</simpara>
<simpara>If you assign a service or namespace to more than one IP address pool, MetalLB uses an available IP address from the higher-priority IP address pool. If no IP addresses are available from the assigned IP address pools with a high priority, MetalLB uses available IP addresses from an IP address pool with lower priority or no priority.</simpara>
<note>
<simpara>You can use the <literal>matchLabels</literal> label selector, the <literal>matchExpressions</literal> label selector, or both, for the <literal>namespaceSelectors</literal> and <literal>serviceSelectors</literal> specifications. This example demonstrates one label selector for each specification.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: doc-example-service-allocation
  namespace: metallb-system
spec:
  addresses:
    - 192.168.20.0/24
  serviceAllocation:
    priority: 50 <co xml:id="CO266-1"/>
    namespaces: <co xml:id="CO266-2"/>
      - namespace-a
      - namespace-b
    namespaceSelectors: <co xml:id="CO266-3"/>
      - matchLabels:
          zone: east
    serviceSelectors: <co xml:id="CO266-4"/>
      - matchExpressions:
        - key: security
          operator: In
          values:
          - S1</programlisting>
<calloutlist>
<callout arearefs="CO266-1">
<para>Assign a priority to the address pool. A lower number indicates a higher priority.</para>
</callout>
<callout arearefs="CO266-2">
<para>Assign one or more namespaces to the IP address pool in a list format.</para>
</callout>
<callout arearefs="CO266-3">
<para>Assign one or more namespace labels to the IP address pool by using label selectors in a list format.</para>
</callout>
<callout arearefs="CO266-4">
<para>Assign one or more service labels to the IP address pool by using label selectors in a list format.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="additional-resources_metallb-configure-address-pools" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-metallb-configure-with-L2-advertisement-label_about-advertising-ip-address-pool">Configuring MetalLB with an L2 advertisement and label</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="next-steps_configure-metallb-address-pools">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara>For BGP mode, see <link linkend="metallb-configure-bgp-peers">Configuring MetalLB BGP peers</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="metallb-configure-services">Configuring services to use MetalLB</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="about-advertise-for-ipaddress-pools">
<title>About advertising for the IP address pools</title>

<simpara>You can configure MetalLB so that the IP address is advertised with layer 2 protocols, the BGP protocol, or both.
With layer 2, MetalLB provides a fault-tolerant external IP address. With BGP, MetalLB provides fault-tolerance for the external IP address and load balancing.</simpara>
<simpara>MetalLB supports advertising using L2 and BGP for the same set of IP addresses.</simpara>
<simpara>MetalLB provides the flexibility to assign address pools to specific BGP peers effectively to a subset of nodes on the network. This allows for more complex configurations, for example facilitating the isolation of nodes or the segmentation of the network.</simpara>
<section xml:id="nw-metallb-bgpadvertisement-cr_about-advertising-ip-address-pool">
<title>About the BGPAdvertisement custom resource</title>
<simpara>The fields for the <literal>BGPAdvertisements</literal> object are defined in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>BGPAdvertisements configuration</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the name for the BGP advertisement.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the namespace for the BGP advertisement.
Specify the same namespace that the MetalLB Operator uses.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.aggregationLength</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the number of bits to include in a 32-bit CIDR mask.
To aggregate the routes that the speaker advertises to BGP peers, the mask is applied to the routes for several service IP addresses and the speaker advertises the aggregated route.
For example, with an aggregation length of <literal>24</literal>, the speaker can aggregate several <literal>10.0.1.x/32</literal> service IP addresses and advertise a single <literal>10.0.1.0/24</literal> route.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.aggregationLengthV6</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the number of bits to include in a 128-bit CIDR mask.
For example, with an aggregation length of <literal>124</literal>, the speaker can aggregate several <literal>fc00:f853:0ccd:e799::x/128</literal> service IP addresses and advertise a single <literal>fc00:f853:0ccd:e799::0/124</literal> route.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.communities</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies one or more BGP communities.
Each community is specified as two 16-bit values separated by the colon character.
Well-known communities must be specified as 16-bit values:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>NO_EXPORT</literal>: <literal>65535:65281</literal></simpara>
</listitem>
<listitem>
<simpara><literal>NO_ADVERTISE</literal>: <literal>65535:65282</literal></simpara>
</listitem>
<listitem>
<simpara><literal>NO_EXPORT_SUBCONFED</literal>: <literal>65535:65283</literal></simpara>
<note>
<simpara>You can also use community objects that are created along with the strings.</simpara>
</note>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.localPref</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the local preference for this advertisement.
This BGP attribute applies to BGP sessions within the Autonomous System.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.ipAddressPools</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: The list of <literal>IPAddressPools</literal> to advertise with this advertisement, selected by name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.ipAddressPoolSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: A selector for the <literal>IPAddressPools</literal> that gets advertised with this advertisement. This is for associating the <literal>IPAddressPool</literal> to the advertisement based on the label assigned to the <literal>IPAddressPool</literal> instead of the name itself. If no <literal>IPAddressPool</literal> is selected by this or by the list, the advertisement is applied to all the <literal>IPAddressPools</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.nodeSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: <literal>NodeSelectors</literal> allows to limit the nodes to announce as next hops for the load balancer IP. When empty, all the nodes are announced as next hops.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.peers</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Peers limits the BGP peer to advertise the IPs of the selected pools to. When empty, the load balancer IP is announced to all the BGP peers configured.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-metallb-configure-BGP-advertisement-basic-use-case_about-advertising-ip-address-pool">
<title>Configuring MetalLB with a BGP advertisement and a basic use case</title>
<simpara>Configure MetalLB as follows so that the peer BGP routers receive one <literal>203.0.113.200/32</literal> route and one <literal>fc00:f853:ccd:e799::1/128</literal> route for each load-balancer IP address that MetalLB assigns to a service.
Because the <literal>localPref</literal> and <literal>communities</literal> fields are not specified, the routes are advertised with <literal>localPref</literal> set to zero and no BGP communities.</simpara>
<section xml:id="nw-metallb-advertise-a-basic-address-pool-configuration-bgp_about-advertising-ip-address-pool">
<title>Example: Advertise a basic address pool configuration with BGP</title>
<simpara>Configure MetalLB as follows so that the <literal>IPAddressPool</literal> is advertised with the BGP protocol.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-bgp-basic
spec:
  addresses:
    - 203.0.113.200/30
    - fc00:f853:ccd:e799::/124</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a BGP advertisement.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgpadvertisement.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-basic
  namespace: metallb-system
spec:
  ipAddressPools:
  - doc-example-bgp-basic</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgpadvertisement.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-metallb-configure-BGP-advertisement-advanced-use-case_about-advertising-ip-address-pool">
<title>Configuring MetalLB with a BGP advertisement and an advanced use case</title>
<simpara>Configure MetalLB as follows so that MetalLB assigns IP addresses to load-balancer services in the ranges between <literal>203.0.113.200</literal> and <literal>203.0.113.203</literal> and between <literal>fc00:f853:ccd:e799::0</literal> and <literal>fc00:f853:ccd:e799::f</literal>.</simpara>
<simpara>To explain the two BGP advertisements, consider an instance when MetalLB assigns the IP address of <literal>203.0.113.200</literal> to a service.
With that IP address as an example, the speaker advertises two routes to BGP peers:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>203.0.113.200/32</literal>, with <literal>localPref</literal> set to <literal>100</literal> and the community set to the numeric value of the <literal>NO_ADVERTISE</literal> community.
This specification indicates to the peer routers that they can use this route but they should not propagate information about this route to BGP peers.</simpara>
</listitem>
<listitem>
<simpara><literal>203.0.113.200/30</literal>, aggregates the load-balancer IP addresses assigned by MetalLB into a single route.
MetalLB advertises the aggregated route to BGP peers with the community attribute set to <literal>8000:800</literal>.
BGP peers propagate the <literal>203.0.113.200/30</literal> route to other BGP peers.
When traffic is routed to a node with a speaker, the <literal>203.0.113.200/32</literal> route is used to forward the traffic into the cluster and to a pod that is associated with the service.</simpara>
</listitem>
</itemizedlist>
<simpara>As you add more services and MetalLB assigns more load-balancer IP addresses from the pool, peer routers receive one local route, <literal>203.0.113.20x/32</literal>, for each service, as well as the <literal>203.0.113.200/30</literal> aggregate route.
Each service that you add generates the <literal>/30</literal> route, but MetalLB deduplicates the routes to one BGP advertisement before communicating with peer routers.</simpara>
<section xml:id="nw-metallb-advertise-an-advanced-address-pool-configuration-bgp_about-advertising-ip-address-pool">
<title>Example: Advertise an advanced address pool configuration with BGP</title>
<simpara>Configure MetalLB as follows so that the <literal>IPAddressPool</literal> is advertised with the BGP protocol.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-bgp-adv
  labels:
    zone: east
spec:
  addresses:
    - 203.0.113.200/30
    - fc00:f853:ccd:e799::/124
  autoAssign: false</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a BGP advertisement.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgpadvertisement1.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-adv-1
  namespace: metallb-system
spec:
  ipAddressPools:
    - doc-example-bgp-adv
  communities:
    - 65535:65282
  aggregationLength: 32
  localPref: 100</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgpadvertisement1.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a file, such as <literal>bgpadvertisement2.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-adv-2
  namespace: metallb-system
spec:
  ipAddressPools:
    - doc-example-bgp-adv
  communities:
    - 8000:800
  aggregationLength: 30
  aggregationLengthV6: 124</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgpadvertisement2.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nw-metallb-advertise-ip-pools-to-node-subset_about-advertising-ip-address-pool">
<title>Advertising an IP address pool from a subset of nodes</title>
<simpara>To advertise an IP address from an IP addresses pool, from a specific set of nodes only, use the <literal>.spec.nodeSelector</literal> specification in the BGPAdvertisement custom resource. This specification associates a pool of IP addresses with a set of nodes in the cluster. This is useful when you have nodes on different subnets in a cluster and you want to advertise an IP addresses from an address pool from a specific subnet, for example a public-facing subnet only.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool by using a custom resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: pool1
spec:
  addresses:
    - 4.4.4.100-4.4.4.200
    - 2001:100:4::200-2001:100:4::400</programlisting>
</listitem>
<listitem>
<simpara>Control which nodes in the cluster the IP address from <literal>pool1</literal> advertises from by defining the <literal>.spec.nodeSelector</literal> value in the BGPAdvertisement custom resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: example
spec:
  ipAddressPools:
  - pool1
  nodeSelector:
  - matchLabels:
      kubernetes.io/hostname: NodeA
  - matchLabels:
      kubernetes.io/hostname: NodeB</programlisting>
</listitem>
</orderedlist>
<simpara>In this example, the IP address from <literal>pool1</literal> advertises from <literal>NodeA</literal> and <literal>NodeB</literal> only.</simpara>
</section>
<section xml:id="nw-metallb-l2padvertisement-cr_about-advertising-ip-address-pool">
<title>About the L2Advertisement custom resource</title>
<simpara>The fields for the <literal>l2Advertisements</literal> object are defined in the following table:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>L2 advertisements configuration</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the name for the L2 advertisement.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the namespace for the L2 advertisement.
Specify the same namespace that the MetalLB Operator uses.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.ipAddressPools</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: The list of <literal>IPAddressPools</literal> to advertise with this advertisement, selected by name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.ipAddressPoolSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: A selector for the <literal>IPAddressPools</literal> that gets advertised with this advertisement. This is for associating the <literal>IPAddressPool</literal> to the advertisement based on the label assigned to the <literal>IPAddressPool</literal> instead of the name itself. If no <literal>IPAddressPool</literal> is selected by this or by the list, the advertisement is applied to all the <literal>IPAddressPools</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.nodeSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: <literal>NodeSelectors</literal> limits the nodes to announce as next hops for the load balancer IP. When empty, all the nodes are announced as next hops.</simpara>
<important>
<simpara>Limiting the nodes to announce as next hops is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.interfaces</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: The list of <literal>interfaces</literal> that are used to announce the load balancer IP.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-metallb-configure-with-L2-advertisement_about-advertising-ip-address-pool">
<title>Configuring MetalLB with an L2 advertisement</title>
<simpara>Configure MetalLB as follows so that the <literal>IPAddressPool</literal> is advertised with the L2 protocol.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-l2
spec:
  addresses:
    - 4.4.4.0/24
  autoAssign: false</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a L2 advertisement.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>l2advertisement.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
   - doc-example-l2</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f l2advertisement.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-configure-with-L2-advertisement-label_about-advertising-ip-address-pool">
<title>Configuring MetalLB with a L2 advertisement and label</title>
<simpara>The <literal>ipAddressPoolSelectors</literal> field in the <literal>BGPAdvertisement</literal> and <literal>L2Advertisement</literal> custom resource definitions is used to associate the <literal>IPAddressPool</literal> to the advertisement based on the label assigned to the <literal>IPAddressPool</literal> instead of the name itself.</simpara>
<simpara>This example shows how to configure MetalLB so that the <literal>IPAddressPool</literal> is advertised with the L2 protocol by configuring the <literal>ipAddressPoolSelectors</literal> field.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-l2-label
  labels:
    zone: east
spec:
  addresses:
    - 172.31.249.87/32</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a L2 advertisement advertising the IP using <literal>ipAddressPoolSelectors</literal>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>l2advertisement.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement-label
  namespace: metallb-system
spec:
  ipAddressPoolSelectors:
    - matchExpressions:
        - key: zone
          operator: In
          values:
            - east</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f l2advertisement.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-configure-with-L2-advertisement-interface_about-advertising-ip-address-pool">
<title>Configuring MetalLB with an L2 advertisement for selected interfaces</title>
<simpara>By default, the IP addresses from IP address pool that has been assigned to the service, is advertised from all the network interfaces. The <literal>interfaces</literal> field in the <literal>L2Advertisement</literal> custom resource definition is used to restrict those network interfaces that advertise the IP address pool.</simpara>
<simpara>This example shows how to configure MetalLB so that the IP address pool is advertised only from the network interfaces listed in the <literal>interfaces</literal> field of all nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, and enter the configuration details like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-l2
spec:
  addresses:
    - 4.4.4.0/24
  autoAssign: false</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool like the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a L2 advertisement advertising the IP with <literal>interfaces</literal> selector.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a YAML file, such as <literal>l2advertisement.yaml</literal>, and enter the configuration details like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: L2Advertisement
metadata:
  name: l2advertisement
  namespace: metallb-system
spec:
  ipAddressPools:
   - doc-example-l2
   interfaces:
   - interfaceA
   - interfaceB</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the advertisement like the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f l2advertisement.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<important>
<simpara>The interface selector does not affect how MetalLB chooses the node to announce a given IP by using L2. The chosen node does not announce the service if the node does not have the selected interface.</simpara>
</important>
</section>
<section xml:id="additional-resources_about-advertiseipaddress" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="metallb-configure-community-alias">Configuring a community alias</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-configure-bgp-peers">
<title>Configuring MetalLB BGP peers</title>

<simpara>As a cluster administrator, you can add, modify, and delete Border Gateway Protocol (BGP) peers.
The MetalLB Operator uses the BGP peer custom resources to identify which peers that MetalLB <literal>speaker</literal> pods contact to start BGP sessions.
The peers receive the route advertisements for the load-balancer IP addresses that MetalLB assigns to services.</simpara>
<section xml:id="nw-metallb-bgppeer-cr_configure-metallb-bgp-peers">
<title>About the BGP peer custom resource</title>
<simpara>The fields for the BGP peer custom resource are described in the following table.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MetalLB BGP peer custom resource</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the name for the BGP peer custom resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the namespace for the BGP peer custom resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.myASN</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the Autonomous System number for the local end of the BGP session.
Specify the same value in all BGP peer custom resources that you add.
The range is <literal>0</literal> to <literal>4294967295</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.peerASN</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the Autonomous System number for the remote end of the BGP session.
The range is <literal>0</literal> to <literal>4294967295</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.peerAddress</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the IP address of the peer to contact for establishing the BGP session.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.sourceAddress</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the IP address to use when establishing the BGP session.
The value must be an IPv4 address.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.peerPort</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the network port of the peer to contact for establishing the BGP session.
The range is <literal>0</literal> to <literal>16384</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.holdTime</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the duration for the hold time to propose to the BGP peer.
The minimum value is 3 seconds (<literal>3s</literal>).
The common units are seconds and minutes, such as <literal>3s</literal>, <literal>1m</literal>, and <literal>5m30s</literal>.
To detect path failures more quickly, also configure BFD.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.keepaliveTime</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the maximum interval between sending keep-alive messages to the BGP peer.
If you specify this field, you must also specify a value for the <literal>holdTime</literal> field.
The specified value must be less than the value for the <literal>holdTime</literal> field.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.routerID</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the router ID to advertise to the BGP peer.
If you specify this field, you must specify the same value in every BGP peer custom resource that you add.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.password</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the MD5 password to send to the peer for routers that enforce TCP MD5 authenticated BGP sessions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.passwordSecret</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies name of the authentication secret for the BGP Peer. The secret must live in the <literal>metallb</literal> namespace and be of type basic-auth.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.bfdProfile</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies the name of a BFD profile.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.nodeSelectors</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>object[]</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies a selector, using match expressions and match labels, to control which nodes can connect to the BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.ebgpMultiHop</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Specifies that the BGP peer is multiple network hops away.
If the BGP peer is not directly connected to the same network, the speaker cannot establish a BGP session unless this field is set to <literal>true</literal>.
This field applies to <emphasis>external BGP</emphasis>.
External BGP is the term that is used to describe when a BGP peer belongs to a different Autonomous System.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>The <literal>passwordSecret</literal> field is mutually exclusive with the <literal>password</literal> field, and contains a reference to a secret containing the password to use. Setting both fields results in a failure of the parsing.</simpara>
</note>
</section>
<section xml:id="nw-metallb-configure-bgppeer_configure-metallb-bgp-peers">
<title>Configuring a BGP peer</title>
<simpara>As a cluster administrator, you can add a BGP peer custom resource to exchange routing information with network routers and advertise the IP addresses for services.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Configure MetalLB with a BGP advertisement.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>bgppeer.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: doc-example-peer
spec:
  peerAddress: 10.0.0.1
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP peer:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgppeer.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-example-assign-specific-address-pools-specific-bgp-peers_configure-metallb-bgp-peers">
<title>Configure a specific set of BGP peers for a given address pool</title>
<simpara>This procedure illustrates how to:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure a set of address pools (<literal>pool1</literal> and <literal>pool2</literal>).</simpara>
</listitem>
<listitem>
<simpara>Configure a set of BGP peers (<literal>peer1</literal> and <literal>peer2</literal>).</simpara>
</listitem>
<listitem>
<simpara>Configure BGP advertisement to assign <literal>pool1</literal> to <literal>peer1</literal> and <literal>pool2</literal> to <literal>peer2</literal>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create address pool <literal>pool1</literal>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool1.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: pool1
spec:
  addresses:
    - 4.4.4.100-4.4.4.200
    - 2001:100:4::200-2001:100:4::400</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool <literal>pool1</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool1.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create address pool <literal>pool2</literal>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool2.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: pool2
spec:
  addresses:
    - 5.5.5.100-5.5.5.200
    - 2001:100:5::200-2001:100:5::400</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool <literal>pool2</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool2.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create BGP <literal>peer1</literal>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgppeer1.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: peer1
spec:
  peerAddress: 10.0.0.1
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP peer:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgppeer1.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create BGP <literal>peer2</literal>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgppeer2.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: peer2
spec:
  peerAddress: 10.0.0.2
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP peer2:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgppeer2.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create BGP advertisement 1.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgpadvertisement1.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-1
  namespace: metallb-system
spec:
  ipAddressPools:
    - pool1
  peers:
    - peer1
  communities:
    - 65535:65282
  aggregationLength: 32
  aggregationLengthV6: 128
  localPref: 100</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgpadvertisement1.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create BGP advertisement 2.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgpadvertisement2.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgpadvertisement-2
  namespace: metallb-system
spec:
  ipAddressPools:
    - pool2
  peers:
    - peer2
  communities:
    - 65535:65282
  aggregationLength: 32
  aggregationLengthV6: 128
  localPref: 100</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgpadvertisement2.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-bgp-peer-vrf_configure-metallb-bgp-peers">
<title>Exposing a service through a network VRF</title>
<simpara>You can expose a service through a virtual routing and forwarding (VRF) instance by associating a VRF on a network interface with a BGP peer.</simpara>
<important>
<simpara>Exposing a service through a VRF on a BGP peer is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>By using a VRF on a network interface to expose a service through a BGP peer, you can segregate traffic to the service, configure independent routing decisions, and enable multi-tenancy support on a network interface.</simpara>
<note>
<simpara>By establishing a BGP session through an interface belonging to a network VRF, MetalLB can advertise services through that interface and enable external traffic to reach the service through this interface. However, the network VRF routing table is different from the default VRF routing table used by OVN-Kubernetes. Therefore, the traffic cannot reach the OVN-Kubernetes network infrastructure.</simpara>
<simpara>To enable the traffic directed to the service to reach the OVN-Kubernetes network infrastructure, you must configure routing rules to define the next hops for network traffic. See the <literal>NodeNetworkConfigurationPolicy</literal> resource in "Managing symmetric routing with MetalLB" in the <emphasis>Additional resources</emphasis> section for more information.</simpara>
</note>
<simpara>These are the high-level steps to expose a service through a network VRF with a BGP peer:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Define a BGP peer and add a network VRF instance.</simpara>
</listitem>
<listitem>
<simpara>Specify an IP address pool for MetalLB.</simpara>
</listitem>
<listitem>
<simpara>Configure a BGP route advertisement for MetalLB to advertise a route using the specified IP address pool and the BGP peer associated with the VRF instance.</simpara>
</listitem>
<listitem>
<simpara>Deploy a service to test the configuration.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You defined a <literal>NodeNetworkConfigurationPolicy</literal> to associate a Virtual Routing and Forwarding (VRF) instance with a network interface. For more information about completing this prerequisite, see the <emphasis>Additional resources</emphasis> section.</simpara>
</listitem>
<listitem>
<simpara>You installed MetalLB on your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>BGPPeer</literal> custom resources (CR):</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>frrviavrf.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: frrviavrf
  namespace: metallb-system
spec:
  myASN: 100
  peerASN: 200
  peerAddress: 192.168.130.1
  vrf: ens4vrf <co xml:id="CO267-1"/></programlisting>
<calloutlist>
<callout arearefs="CO267-1">
<para>Specifies the network VRF instance to associate with the BGP peer. MetalLB can advertise services and make routing decisions based on the routing information in the VRF.</para>
</callout>
</calloutlist>
<note>
<simpara>You must configure this network VRF instance in a <literal>NodeNetworkConfigurationPolicy</literal> CR. See the <emphasis>Additional resources</emphasis> for more information.</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP peer by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f frrviavrf.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an <literal>IPAddressPool</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>first-pool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.169.10.0/32</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f first-pool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>BGPAdvertisement</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>first-adv.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: first-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - first-pool
  peers:
    - frrviavrf <co xml:id="CO268-1"/></programlisting>
<calloutlist>
<callout arearefs="CO268-1">
<para>In this example, MetalLB advertises a range of IP addresses from the <literal>first-pool</literal> IP address pool to the <literal>frrviavrf</literal> BGP peer.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP advertisement by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f first-adv.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>Namespace</literal>, <literal>Deployment</literal>, and <literal>Service</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>deploy-service.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: test
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: server
  namespace: test
spec:
  selector:
    matchLabels:
      app: server
  template:
    metadata:
      labels:
        app: server
    spec:
      containers:
      - name: server
        image: registry.redhat.io/ubi9/ubi
        ports:
        - name: http
          containerPort: 30100
        command: ["/bin/sh", "-c"]
        args: ["sleep INF"]
---
apiVersion: v1
kind: Service
metadata:
  name: server1
  namespace: test
spec:
  ports:
  - name: http
    port: 30100
    protocol: TCP
    targetPort: 30100
  selector:
    app: server
  type: LoadBalancer</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the namespace, deployment, and service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f deploy-service.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Identify a MetalLB speaker pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n metallb-system pods -l component=speaker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME            READY   STATUS    RESTARTS   AGE
speaker-c6c5f   6/6     Running   0          69m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the state of the BGP session is <literal>Established</literal> in the speaker pod by running the following command, replacing the variables to match your configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n metallb-system &lt;speaker_pod&gt; -c frr -- vtysh -c "show bgp vrf &lt;vrf_name&gt; neigh"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">BGP neighbor is 192.168.30.1, remote AS 200, local AS 100, external link
  BGP version 4, remote router ID 192.168.30.1, local router ID 192.168.30.71
  BGP state = Established, up for 04:20:09

...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the service is advertised correctly by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n metallb-system &lt;speaker_pod&gt; -c frr -- vtysh -c "show bgp vrf &lt;vrf_name&gt; ipv4"</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding">About virtual routing and forwarding</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-example-host-vrf_k8s_nmstate-updating-node-network-config">Example: Network interface with a VRF instance node network configuration policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-egress-traffic-loadbalancer-services">Configuring an egress service</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="metallb-configure-return-traffic">Managing symmetric routing with MetalLB</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-example-bgppeer_configure-metallb-bgp-peers">
<title>Example BGP peer configurations</title>
<section xml:id="nw-metallb-example-limit-nodes-bgppeer_configure-metallb-bgp-peers">
<title>Example: Limit which nodes connect to a BGP peer</title>
<simpara>You can specify the node selectors field to control which nodes can connect to a BGP peer.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-nodesel
  namespace: metallb-system
spec:
  peerAddress: 10.0.20.1
  peerASN: 64501
  myASN: 64500
  nodeSelectors:
  - matchExpressions:
    - key: kubernetes.io/hostname
      operator: In
      values: [compute-1.example.com, compute-2.example.com]</programlisting>
</section>
<section xml:id="nw-metallb-example-specify-bfd-profile_configure-metallb-bgp-peers">
<title>Example: Specify a BFD profile for a BGP peer</title>
<simpara>You can specify a BFD profile to associate with BGP peers.
BFD compliments BGP by providing more rapid detection of communication failures between peers than BGP alone.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-peer-bfd
  namespace: metallb-system
spec:
  peerAddress: 10.0.20.1
  peerASN: 64501
  myASN: 64500
  holdTime: "10s"
  bfdProfile: doc-example-bfd-profile-full</programlisting>
<note>
<simpara>Deleting the bidirectional forwarding detection (BFD) profile and removing the <literal>bfdProfile</literal> added to the border gateway protocol (BGP) peer resource does not disable the BFD. Instead, the BGP peer starts using the default BFD profile. To disable BFD from a BGP peer resource, delete the BGP peer configuration and recreate it without a BFD profile. For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=2050824"><emphasis role="strong">BZ#2050824</emphasis></link>.</simpara>
</note>
</section>
<section xml:id="nw-metallb-example-dual-stack_configure-metallb-bgp-peers">
<title>Example: Specify BGP peers for dual-stack networking</title>
<simpara>To support dual-stack networking, add one BGP peer custom resource for IPv4 and one BGP peer custom resource for IPv6.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-dual-stack-ipv4
  namespace: metallb-system
spec:
  peerAddress: 10.0.20.1
  peerASN: 64500
  myASN: 64500
---
apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: doc-example-dual-stack-ipv6
  namespace: metallb-system
spec:
  peerAddress: 2620:52:0:88::104
  peerASN: 64500
  myASN: 64500</programlisting>
</section>
</section>
<section xml:id="next-steps_configure-metallb-bgp-peers">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="metallb-configure-services">Configuring services to use MetalLB</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-configure-community-alias">
<title>Configuring community alias</title>

<simpara>As a cluster administrator, you can configure a community alias and use it across different advertisements.</simpara>
<section xml:id="nw-metallb-community-cr_configure-community-alias">
<title>About the community custom resource</title>
<simpara>The <literal>community</literal> custom resource is a collection of aliases for communities. Users can define named aliases to be used when advertising <literal>ipAddressPools</literal> using the <literal>BGPAdvertisement</literal>. The fields for the <literal>community</literal> custom resource are described in the following table.</simpara>
<note>
<simpara>The <literal>community</literal> CRD applies only to BGPAdvertisement.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title>MetalLB community custom resource</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the name for the <literal>community</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the namespace for the <literal>community</literal>.
Specify the same namespace that the MetalLB Operator uses.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.communities</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a list of BGP community aliases that can be used in BGPAdvertisements. A community alias consists of a pair of name (alias) and value (number:number). Link the BGPAdvertisement to a community alias by referring to the alias name in its <literal>spec.communities</literal> field.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>CommunityAlias</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the alias for the <literal>community</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>value</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The BGP <literal>community</literal> value corresponding to the given name.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-metallb-configure-BGP-advertisement-community-alias_configure-community-alias">
<title>Configuring MetalLB with a BGP advertisement and community alias</title>
<simpara>Configure MetalLB as follows so that the <literal>IPAddressPool</literal> is advertised with the BGP protocol and the community alias set to the numeric value of the NO_ADVERTISE community.</simpara>
<simpara>In the following example, the peer BGP router <literal>doc-example-peer-community</literal> receives one <literal>203.0.113.200/32</literal> route and one <literal>fc00:f853:ccd:e799::1/128</literal> route for each load-balancer IP address that MetalLB assigns to a service. A community alias is configured with the <literal>NO_ADVERTISE</literal> community.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an IP address pool.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>ipaddresspool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  namespace: metallb-system
  name: doc-example-bgp-community
spec:
  addresses:
    - 203.0.113.200/30
    - fc00:f853:ccd:e799::/124</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ipaddresspool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a community alias named <literal>community1</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: Community
metadata:
  name: community1
  namespace: metallb-system
spec:
  communities:
    - name: NO_ADVERTISE
      value: '65535:65282'</programlisting>
</listitem>
<listitem>
<simpara>Create a BGP peer named <literal>doc-example-bgp-peer</literal>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgppeer.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  namespace: metallb-system
  name: doc-example-bgp-peer
spec:
  peerAddress: 10.0.0.1
  peerASN: 64501
  myASN: 64500
  routerID: 10.10.10.10</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP peer:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgppeer.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a BGP advertisement with the community alias.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>bgpadvertisement.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: bgp-community-sample
  namespace: metallb-system
spec:
  aggregationLength: 32
  aggregationLengthV6: 128
  communities:
    - NO_ADVERTISE <co xml:id="CO269-1"/>
  ipAddressPools:
    - doc-example-bgp-community
  peers:
    - doc-example-peer</programlisting>
<calloutlist>
<callout arearefs="CO269-1">
<para>Specify the <literal>CommunityAlias.name</literal> here and not the community custom resource (CR) name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bgpadvertisement.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="metallb-configure-bfd-profiles">
<title>Configuring MetalLB BFD profiles</title>

<simpara>As a cluster administrator, you can add, modify, and delete Bidirectional Forwarding Detection (BFD) profiles.
The MetalLB Operator uses the BFD profile custom resources to identify which BGP sessions use BFD to provide faster path failure detection than BGP alone provides.</simpara>
<section xml:id="nw-metallb-bfdprofile-cr_configure-metallb-bfd-profiles">
<title>About the BFD profile custom resource</title>
<simpara>The fields for the BFD profile custom resource are described in the following table.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>BFD profile custom resource</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the name for the BFD profile custom resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metadata.namespace</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the namespace for the BFD profile custom resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.detectMultiplier</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the detection multiplier to determine packet loss.
The remote transmission interval is multiplied by this value to determine the connection loss detection timer.</simpara>
<simpara>For example, when the local system has the detect multiplier set to <literal>3</literal> and the remote system has the transmission interval set to <literal>300</literal>, the local system detects failures only after <literal>900</literal> ms without receiving packets.</simpara>
<simpara>The range is <literal>2</literal> to <literal>255</literal>.
The default value is <literal>3</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.echoMode</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the echo transmission mode.
If you are not using distributed BFD, echo transmission mode works only when the peer is also FRR.
The default value is <literal>false</literal> and echo transmission mode is disabled.</simpara>
<simpara>When echo transmission mode is enabled, consider increasing the transmission interval of control packets to reduce bandwidth usage.
For example, consider increasing the transmit interval to <literal>2000</literal> ms.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.echoInterval</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the minimum transmission interval, less jitter, that this system uses to send and receive echo packets.
The range is <literal>10</literal> to <literal>60000</literal>.
The default value is <literal>50</literal> ms.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.minimumTtl</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the minimum expected TTL for an incoming control packet.
This field applies to multi-hop sessions only.</simpara>
<simpara>The purpose of setting a minimum TTL is to make the packet validation requirements more stringent and avoid receiving control packets from other sessions.</simpara>
<simpara>The default value is <literal>254</literal> and indicates that the system expects only one hop between this system and the peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.passiveMode</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>boolean</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies whether a session is marked as active or passive.
A passive session does not attempt to start the connection.
Instead, a passive session waits for control packets from a peer before it begins to reply.</simpara>
<simpara>Marking a session as passive is useful when you have a router that acts as the central node of a star network and you want to avoid sending control packets that you do not need the system to send.</simpara>
<simpara>The default value is <literal>false</literal> and marks the session as active.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.receiveInterval</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the minimum interval that this system is capable of receiving control packets.
The range is <literal>10</literal> to <literal>60000</literal>.
The default value is <literal>300</literal> ms.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.transmitInterval</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>integer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the minimum transmission interval, less jitter, that this system uses to send control packets.
The range is <literal>10</literal> to <literal>60000</literal>.
The default value is <literal>300</literal> ms.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="nw-metallb-configure-bfdprofile_configure-metallb-bfd-profiles">
<title>Configuring a BFD profile</title>
<simpara>As a cluster administrator, you can add a BFD profile and configure a BGP peer to use the profile. BFD provides faster path failure detection than BGP alone.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>bfdprofile.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BFDProfile
metadata:
  name: doc-example-bfd-profile-full
  namespace: metallb-system
spec:
  receiveInterval: 300
  transmitInterval: 300
  detectMultiplier: 3
  echoMode: false
  passiveMode: true
  minimumTtl: 254</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the BFD profile:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f bfdprofile.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="next-steps_configure-metallb-bfd-profiles">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="metallb-configure-bgp-peers">Configure a BGP peer</link> to use the BFD profile.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-configure-services">
<title>Configuring services to use MetalLB</title>

<simpara>As a cluster administrator, when you add a service of type <literal>LoadBalancer</literal>, you can control how MetalLB assigns an IP address.</simpara>
<section xml:id="request-specific-ip-address_configure-services-metallb">
<title>Request a specific IP address</title>
<simpara>Like some other load-balancer implementations, MetalLB accepts the <literal>spec.loadBalancerIP</literal> field in the service specification.</simpara>
<simpara>If the requested IP address is within a range from any address pool, MetalLB assigns the requested IP address.
If the requested IP address is not within any range, MetalLB reports a warning.</simpara>
<formalpara>
<title>Example service YAML for a specific IP address</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: &lt;service_name&gt;
  annotations:
    metallb.universe.tf/address-pool: &lt;address_pool_name&gt;
spec:
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: LoadBalancer
  loadBalancerIP: &lt;ip_address&gt;</programlisting>
</para>
</formalpara>
<simpara>If MetalLB cannot assign the requested IP address, the <literal>EXTERNAL-IP</literal> for the service reports <literal>&lt;pending&gt;</literal> and running <literal>oc describe service &lt;service_name&gt;</literal> includes an event like the following example.</simpara>
<formalpara>
<title>Example event when MetalLB cannot assign a requested IP address</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">  ...
Events:
  Type     Reason            Age    From                Message
  ----     ------            ----   ----                -------
  Warning  AllocationFailed  3m16s  metallb-controller  Failed to allocate IP for "default/invalid-request": "4.3.2.1" is not allowed in config</programlisting>
</para>
</formalpara>
</section>
<section xml:id="request-ip-address-from-pool_configure-services-metallb">
<title>Request an IP address from a specific pool</title>
<simpara>To assign an IP address from a specific range, but you are not concerned with the specific IP address, then you can use the <literal>metallb.universe.tf/address-pool</literal> annotation to request an IP address from the specified address pool.</simpara>
<formalpara>
<title>Example service YAML for an IP address from a specific pool</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: &lt;service_name&gt;
  annotations:
    metallb.universe.tf/address-pool: &lt;address_pool_name&gt;
spec:
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: LoadBalancer</programlisting>
</para>
</formalpara>
<simpara>If the address pool that you specify for <literal>&lt;address_pool_name&gt;</literal> does not exist, MetalLB attempts to assign an IP address from any pool that permits automatic assignment.</simpara>
</section>
<section xml:id="accept-any-ip-address_configure-services-metallb">
<title>Accept any IP address</title>
<simpara>By default, address pools are configured to permit automatic assignment.
MetalLB assigns an IP address from these address pools.</simpara>
<simpara>To accept any IP address from any pool that is configured for automatic assignment, no special annotation or configuration is required.</simpara>
<formalpara>
<title>Example service YAML for accepting any IP address</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: &lt;service_name&gt;
spec:
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
  type: LoadBalancer</programlisting>
</para>
</formalpara>
</section>
<section xml:id="share-specific-ip-address_configure-services-metallb">
<title>Share a specific IP address</title>
<simpara>By default, services do not share IP addresses.
However, if you need to colocate services on a single IP address, you can enable selective IP sharing by adding the <literal>metallb.universe.tf/allow-shared-ip</literal> annotation to the services.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: service-http
  annotations:
    metallb.universe.tf/address-pool: doc-example
    metallb.universe.tf/allow-shared-ip: "web-server-svc"  <co xml:id="CO270-1"/>
spec:
  ports:
    - name: http
      port: 80  <co xml:id="CO270-2"/>
      protocol: TCP
      targetPort: 8080
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;  <co xml:id="CO270-3"/>
  type: LoadBalancer
  loadBalancerIP: 172.31.249.7  <co xml:id="CO270-4"/>
---
apiVersion: v1
kind: Service
metadata:
  name: service-https
  annotations:
    metallb.universe.tf/address-pool: doc-example
    metallb.universe.tf/allow-shared-ip: "web-server-svc"  <co xml:id="CO270-5"/>
spec:
  ports:
    - name: https
      port: 443  <co xml:id="CO270-6"/>
      protocol: TCP
      targetPort: 8080
  selector:
    &lt;label_key&gt;: &lt;label_value&gt;  <co xml:id="CO270-7"/>
  type: LoadBalancer
  loadBalancerIP: 172.31.249.7  <co xml:id="CO270-8"/></programlisting>
<calloutlist>
<callout arearefs="CO270-1 CO270-5">
<para>Specify the same value for the <literal>metallb.universe.tf/allow-shared-ip</literal> annotation. This value is referred to as the <emphasis>sharing key</emphasis>.</para>
</callout>
<callout arearefs="CO270-2 CO270-6">
<para>Specify different port numbers for the services.</para>
</callout>
<callout arearefs="CO270-3 CO270-7">
<para>Specify identical pod selectors if you must specify <literal>externalTrafficPolicy: local</literal> so the services send traffic to the same set of pods. If you use the <literal>cluster</literal> external traffic policy, then the pod selectors do not need to be identical.</para>
</callout>
<callout arearefs="CO270-4 CO270-8">
<para>Optional: If you specify the three preceding items, MetalLB might colocate the services on the same IP address. To ensure that services share an IP address, specify the IP address to share.</para>
</callout>
</calloutlist>
<simpara>By default, Kubernetes does not allow multiprotocol load balancer services.
This limitation would normally make it impossible to run a service like DNS that needs to listen on both TCP and UDP.
To work around this limitation of Kubernetes with MetalLB, create two services:</simpara>
<itemizedlist>
<listitem>
<simpara>For one service, specify TCP and for the second service, specify UDP.</simpara>
</listitem>
<listitem>
<simpara>In both services, specify the same pod selector.</simpara>
</listitem>
<listitem>
<simpara>Specify the same sharing key and <literal>spec.loadBalancerIP</literal> value to colocate the TCP and UDP services on the same IP address.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-configure-svc_configure-services-metallb">
<title>Configuring a service with MetalLB</title>
<simpara>You can configure a load-balancing service to use an external IP address from an address pool.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Install the MetalLB Operator and start MetalLB.</simpara>
</listitem>
<listitem>
<simpara>Configure at least one address pool.</simpara>
</listitem>
<listitem>
<simpara>Configure your network to route traffic from the clients to the host network for the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>&lt;service_name&gt;.yaml</literal> file. In the file, ensure that the <literal>spec.type</literal> field is set to <literal>LoadBalancer</literal>.</simpara>
<simpara>Refer to the examples for information about how to request the external IP address that MetalLB assigns to the service.</simpara>
</listitem>
<listitem>
<simpara>Create the service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;service_name&gt;.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">service/&lt;service_name&gt; created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Describe the service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe service &lt;service_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>Name:                     &lt;service_name&gt;
Namespace:                default
Labels:                   &lt;none&gt;
Annotations:              metallb.universe.tf/address-pool: doc-example  <co xml:id="CO271-1"/>
Selector:                 app=service_name
Type:                     LoadBalancer  <co xml:id="CO271-2"/>
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.105.237.254
IPs:                      10.105.237.254
LoadBalancer Ingress:     192.168.100.5  <co xml:id="CO271-3"/>
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               8080/TCP
NodePort:                 &lt;unset&gt;  30550/TCP
Endpoints:                10.244.0.50:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:  <co xml:id="CO271-4"/>
  Type    Reason        Age                From             Message
  ----    ------        ----               ----             -------
  Normal  nodeAssigned  32m (x2 over 32m)  metallb-speaker  announcing from node "&lt;node_name&gt;"</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO271-1">
<para>The annotation is present if you request an IP address from a specific pool.</para>
</callout>
<callout arearefs="CO271-2">
<para>The service type must indicate <literal>LoadBalancer</literal>.</para>
</callout>
<callout arearefs="CO271-3">
<para>The load-balancer ingress field indicates the external IP address if the service is assigned correctly.</para>
</callout>
<callout arearefs="CO271-4">
<para>The events field indicates the node name that is assigned to announce the external IP address.
If you experience an error, the events field indicates the reason for the error.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-configure-return-traffic">
<title>Managing symmetric routing with MetalLB</title>

<simpara>As a cluster administrator, you can effectively manage traffic for pods behind a MetalLB load-balancer service with multiple host interfaces by implementing features from MetalLB, NMState, and OVN-Kubernetes. By combining these features in this context, you can provide symmetric routing, traffic segregation, and support clients on different networks with overlapping CIDR addresses.</simpara>
<simpara>To achieve this functionality, learn how to implement virtual routing and forwarding (VRF) instances with MetalLB, and configure egress services.</simpara>
<important>
<simpara>Configuring symmetric traffic by using a VRF instance with MetalLB and an egress service is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="challenges-of-managing-symmetric-routing-with-metallb">
<title>Challenges of managing symmetric routing with MetalLB</title>
<simpara>When you use MetalLB with multiple host interfaces, MetalLB exposes and announces a service through all available interfaces on the host. This can present challenges relating to network isolation, asymmetric return traffic and overlapping CIDR addresses.</simpara>
<simpara>One option to ensure that return traffic reaches the correct client is to use static routes. However, with this solution, MetalLB cannot isolate the services and then announce each service through a different interface. Additionally, static routing requires manual configuration and requires maintenance if remote sites are added.</simpara>
<simpara>A further challenge of symmetric routing when implementing a MetalLB service is scenarios where external systems expect the source and destination IP address for an application to be the same. The default behavior for OpenShift Container Platform is to assign the IP address of the host network interface as the source IP address for traffic originating from pods. This is problematic with multiple host interfaces.</simpara>
<simpara>You can overcome these challenges by implementing a configuration that combines features from MetalLB, NMState, and OVN-Kubernetes.</simpara>
</section>
<section xml:id="overview-of-managing-symmetric-routing-using-vrf-based-networks-with-metallb">
<title>Overview of managing symmetric routing by using VRFs with MetalLB</title>
<simpara>You can overcome the challenges of implementing symmetric routing by using NMState to configure a VRF instance on a host, associating the VRF instance with a MetalLB <literal>BGPPeer</literal> resource, and configuring an egress service for egress traffic with OVN-Kubernetes.</simpara>
<figure>
<title>Network overview of managing symmetric routing by using VRFs with MetalLB</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/357_OpenShift_MetalLB_VRF_0823.png"/>
</imageobject>
<textobject><phrase>Network overview of managing symmetric routing by using VRFs with MetalLB</phrase></textobject>
</mediaobject>
</figure>
<simpara>The configuration process involves three stages:</simpara>
<itemizedlist>
<title>1. Define a VRF and routing rules</title>
<listitem>
<simpara>Configure a <literal>NodeNetworkConfigurationPolicy</literal> custom resource (CR) to associate a VRF instance with a network interface.</simpara>
</listitem>
<listitem>
<simpara>Use the VRF routing table to direct ingress and egress traffic.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>2. Link the VRF to a MetalLB <literal>BGPPeer</literal></title>
<listitem>
<simpara>Configure a MetalLB <literal>BGPPeer</literal> resource to use the VRF instance on a network interface.</simpara>
</listitem>
<listitem>
<simpara>By associating the <literal>BGPPeer</literal> resource with the VRF instance, the designated network interface becomes the primary interface for the BGP session, and MetalLB advertises the services through this interface.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>3. Configure an egress service</title>
<listitem>
<simpara>Configure an egress service to choose the network associated with the VRF instance for egress traffic.</simpara>
</listitem>
<listitem>
<simpara>Optional: Configure an egress service to use the IP address of the MetalLB load-balancer service as the source IP for egress traffic.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-configure-return-traffic-proc_metallb-configure-return-traffic">
<title>Configuring symmetric routing by using VRFs with MetalLB</title>
<simpara>You can configure symmetric network routing for applications behind a MetalLB service that require the same ingress and egress network paths.</simpara>
<simpara>This example associates a VRF routing table with MetalLB and an egress service to enable symmetric routing for ingress and egress traffic for pods behind a <literal>LoadBalancer</literal> service.</simpara>
<note>
<itemizedlist>
<listitem>
<simpara>If you use the <literal>sourceIPBy: "LoadBalancerIP"</literal> setting in the <literal>EgressService</literal> CR, you must specify the load-balancer node in the <literal>BGPAdvertisement</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>You can use the <literal>sourceIPBy: "Network"</literal> setting on clusters that use OVN-Kubernetes configured with the <literal>gatewayConfig.routingViaHost</literal> specification set to <literal>true</literal> only. Additionally, if you use the <literal>sourceIPBy: "Network"</literal> setting, you must schedule the application workload on nodes configured with the network VRF instance.</simpara>
</listitem>
</itemizedlist>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>NodeNetworkConfigurationPolicy</literal> CR to define the VRF instance:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>node-network-vrf.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: vrfpolicy <co xml:id="CO272-1"/>
spec:
  nodeSelector:
    vrf: "true" <co xml:id="CO272-2"/>
  maxUnavailable: 3
  desiredState:
    interfaces:
      - name: ens4vrf <co xml:id="CO272-3"/>
        type: vrf <co xml:id="CO272-4"/>
        state: up
        vrf:
          port:
            - ens4 <co xml:id="CO272-5"/>
          route-table-id: 2 <co xml:id="CO272-6"/>
    routes: <co xml:id="CO272-7"/>
      config:
        - destination: 0.0.0.0/0
          metric: 150
          next-hop-address: 192.168.130.1
          next-hop-interface: ens4
          table-id: 2
    route-rules: <co xml:id="CO272-8"/>
      config:
        - ip-to: 172.30.0.0/16
          priority: 998
          route-table: 254
        - ip-to: 10.132.0.0/14
          priority: 998
          route-table: 254</programlisting>
<calloutlist>
<callout arearefs="CO272-1">
<para>The name of the policy.</para>
</callout>
<callout arearefs="CO272-2">
<para>This example applies the policy to all nodes with the label <literal>vrf:true</literal>.</para>
</callout>
<callout arearefs="CO272-3">
<para>The name of the interface.</para>
</callout>
<callout arearefs="CO272-4">
<para>The type of interface. This example creates a VRF instance.</para>
</callout>
<callout arearefs="CO272-5">
<para>The node interface that the VRF attaches to.</para>
</callout>
<callout arearefs="CO272-6">
<para>The name of the route table ID for the VRF.</para>
</callout>
<callout arearefs="CO272-7">
<para>Defines the configuration for network routes. The <literal>next-hop-address</literal> field defines the IP address of the next hop for the route. The <literal>next-hop-interface</literal> field defines the outgoing interface for the route. In this example, the VRF routing table is <literal>2</literal>, which references the ID that you define in the <literal>EgressService</literal> CR.</para>
</callout>
<callout arearefs="CO272-8">
<para>Defines additional route rules. The <literal>ip-to</literal> fields must match the <literal>Cluster Network</literal> CIDR and <literal>Service Network</literal> CIDR. You can view the values for these CIDR address specifications by running the following command: <literal>oc describe network.config/cluster</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f node-network-vrf.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>BGPPeer</literal> custom resource (CR):</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>frr-via-vrf.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta2
kind: BGPPeer
metadata:
  name: frrviavrf
  namespace: metallb-system
spec:
  myASN: 100
  peerASN: 200
  peerAddress: 192.168.130.1
  vrf: ens4vrf <co xml:id="CO273-1"/></programlisting>
<calloutlist>
<callout arearefs="CO273-1">
<para>Specifies the VRF instance to associate with the BGP peer. MetalLB can advertise services and make routing decisions based on the routing information in the VRF.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP peer by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f frr-via-vrf.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an <literal>IPAddressPool</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>first-pool.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
  name: first-pool
  namespace: metallb-system
spec:
  addresses:
  - 192.169.10.0/32</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration for the IP address pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f first-pool.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>BGPAdvertisement</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>first-adv.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
  name: first-adv
  namespace: metallb-system
spec:
  ipAddressPools:
    - first-pool
  peers:
    - frrviavrf <co xml:id="CO274-1"/>
  nodeSelectors:
    - matchLabels:
        egress-service.k8s.ovn.org/test-server1: "" <co xml:id="CO274-2"/></programlisting>
<calloutlist>
<callout arearefs="CO274-1">
<para>In this example, MetalLB advertises a range of IP addresses from the <literal>first-pool</literal> IP address pool to the <literal>frrviavrf</literal> BGP peer.</para>
</callout>
<callout arearefs="CO274-2">
<para>In this example, the <literal>EgressService</literal> CR configures the source IP address for egress traffic to use the load-balancer service IP address. Therefore, you must specify the load-balancer node for return traffic to use the same return path for the traffic originating from the pod.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the BGP advertisement by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f first-adv.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an <literal>EgressService</literal> CR:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file, such as <literal>egress-service.yaml</literal>, with content like the following example:</simpara>
<programlisting role="white-space-pre" language="yaml" linenumbering="unnumbered">apiVersion: k8s.ovn.org/v1
kind: EgressService
metadata:
  name: server1 <co xml:id="CO275-1"/>
  namespace: test <co xml:id="CO275-2"/>
spec:
  sourceIPBy: "LoadBalancerIP" <co xml:id="CO275-3"/>
  nodeSelector:
    matchLabels:
      vrf: "true" <co xml:id="CO275-4"/>
  network: "2" <co xml:id="CO275-5"/></programlisting>
<calloutlist>
<callout arearefs="CO275-1">
<para>Specify the name for the egress service. The name of the <literal>EgressService</literal> resource must match the name of the load-balancer service that you want to modify.</para>
</callout>
<callout arearefs="CO275-2">
<para>Specify the namespace for the egress service. The namespace for the <literal>EgressService</literal> must match the namespace of the load-balancer service that you want to modify. The egress service is namespace-scoped.</para>
</callout>
<callout arearefs="CO275-3">
<para>This example assigns the <literal>LoadBalancer</literal> service ingress IP address as the source IP address for egress traffic.</para>
</callout>
<callout arearefs="CO275-4">
<para>If you specify <literal>LoadBalancer</literal> for the <literal>sourceIPBy</literal> specification, a single node handles the <literal>LoadBalancer</literal> service traffic. In this example, only a node with the label <literal>vrf: "true"</literal> can handle the service traffic. If you do not specify a node, OVN-Kubernetes selects a worker node to handle the service traffic. When a node is selected, OVN-Kubernetes labels the node in the following format: <literal>egress-service.k8s.ovn.org/&lt;svc_namespace&gt;-&lt;svc_name&gt;: ""</literal>.</para>
</callout>
<callout arearefs="CO275-5">
<para>Specify the routing table for egress traffic.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration for the egress service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f egress-service.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that you can access the application endpoint of the pods running behind the MetalLB service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl &lt;external_ip_address&gt;:&lt;port_number&gt; <co xml:id="CO276-1"/></programlisting>
<calloutlist>
<callout arearefs="CO276-1">
<para>Update the external IP address and port number to suit your application endpoint.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: If you assigned the <literal>LoadBalancer</literal> service ingress IP address as the source IP address for egress traffic, verify this configuration by using tools such as <literal>tcpdump</literal> to analyze packets received at the external client.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="cnf-about-virtual-routing-and-forwarding_about-virtual-routing-and-forwarding">About virtual routing and forwarding</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-metallb-bgp-peer-vrf_configure-metallb-bgp-peers">Exposing a service through a network VRF</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-example-host-vrf_k8s_nmstate-updating-node-network-config">Example: Network interface with a VRF instance node network configuration policy</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-egress-traffic-loadbalancer-services">Configuring an egress service</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="metallb-logging-troubleshooting-support">
<title>MetalLB logging, troubleshooting, and support</title>

<simpara>If you need to troubleshoot MetalLB configuration, see the following sections for commonly used commands.</simpara>
<section xml:id="nw-metallb-setting-metalb-logging-levels_metallb-troubleshoot-support">
<title>Setting the MetalLB logging levels</title>
<simpara>MetalLB uses FRRouting (FRR) in a container with the default setting of <literal>info</literal> generates a lot of logging. You can control the verbosity of the logs generated by setting the <literal>logLevel</literal> as illustrated in this example.</simpara>
<simpara>Gain a deeper insight into MetalLB by setting the <literal>logLevel</literal> to <literal>debug</literal> as follows:</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file, such as <literal>setdebugloglevel.yaml</literal>, with content like the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
  name: metallb
  namespace: metallb-system
spec:
  logLevel: debug
  nodeSelector:
    node-role.kubernetes.io/worker: ""</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace -f setdebugloglevel.yaml</programlisting>
<note>
<simpara>Use <literal>oc replace</literal> as the understanding is the <literal>metallb</literal> CR is already created and here you are changing the log level.</simpara>
</note>
</listitem>
<listitem>
<simpara>Display the names of the <literal>speaker</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n metallb-system pods -l component=speaker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME                    READY   STATUS    RESTARTS   AGE
speaker-2m9pm           4/4     Running   0          9m19s
speaker-7m4qw           3/4     Running   0          19s
speaker-szlmx           4/4     Running   0          9m19s</programlisting>
</para>
</formalpara>
<note>
<simpara>Speaker and controller pods are recreated to ensure the updated logging level is applied. The logging level is modified for all the components of MetalLB.</simpara>
</note>
</listitem>
<listitem>
<simpara>View the <literal>speaker</literal> logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n metallb-system speaker-7m4qw -c speaker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>{"branch":"main","caller":"main.go:92","commit":"3d052535","goversion":"gc / go1.17.1 / amd64","level":"info","msg":"MetalLB speaker starting (commit 3d052535, branch main)","ts":"2022-05-17T09:55:05Z","version":""}
{"caller":"announcer.go:110","event":"createARPResponder","interface":"ens4","level":"info","msg":"created ARP responder for interface","ts":"2022-05-17T09:55:05Z"}
{"caller":"announcer.go:119","event":"createNDPResponder","interface":"ens4","level":"info","msg":"created NDP responder for interface","ts":"2022-05-17T09:55:05Z"}
{"caller":"announcer.go:110","event":"createARPResponder","interface":"tun0","level":"info","msg":"created ARP responder for interface","ts":"2022-05-17T09:55:05Z"}
{"caller":"announcer.go:119","event":"createNDPResponder","interface":"tun0","level":"info","msg":"created NDP responder for interface","ts":"2022-05-17T09:55:05Z"}
I0517 09:55:06.515686      95 request.go:665] Waited for 1.026500832s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/operators.coreos.com/v1alpha1?timeout=32s
{"Starting Manager":"(MISSING)","caller":"k8s.go:389","level":"info","ts":"2022-05-17T09:55:08Z"}
{"caller":"speakerlist.go:310","level":"info","msg":"node event - forcing sync","node addr":"10.0.128.4","node event":"NodeJoin","node name":"ci-ln-qb8t3mb-72292-7s7rh-worker-a-vvznj","ts":"2022-05-17T09:55:08Z"}
{"caller":"service_controller.go:113","controller":"ServiceReconciler","enqueueing":"openshift-kube-controller-manager-operator/metrics","epslice":"{\"metadata\":{\"name\":\"metrics-xtsxr\",\"generateName\":\"metrics-\",\"namespace\":\"openshift-kube-controller-manager-operator\",\"uid\":\"ac6766d7-8504-492c-9d1e-4ae8897990ad\",\"resourceVersion\":\"9041\",\"generation\":4,\"creationTimestamp\":\"2022-05-17T07:16:53Z\",\"labels\":{\"app\":\"kube-controller-manager-operator\",\"endpointslice.kubernetes.io/managed-by\":\"endpointslice-controller.k8s.io\",\"kubernetes.io/service-name\":\"metrics\"},\"annotations\":{\"endpoints.kubernetes.io/last-change-trigger-time\":\"2022-05-17T07:21:34Z\"},\"ownerReferences\":[{\"apiVersion\":\"v1\",\"kind\":\"Service\",\"name\":\"metrics\",\"uid\":\"0518eed3-6152-42be-b566-0bd00a60faf8\",\"controller\":true,\"blockOwnerDeletion\":true}],\"managedFields\":[{\"manager\":\"kube-controller-manager\",\"operation\":\"Update\",\"apiVersion\":\"discovery.k8s.io/v1\",\"time\":\"2022-05-17T07:20:02Z\",\"fieldsType\":\"FieldsV1\",\"fieldsV1\":{\"f:addressType\":{},\"f:endpoints\":{},\"f:metadata\":{\"f:annotations\":{\".\":{},\"f:endpoints.kubernetes.io/last-change-trigger-time\":{}},\"f:generateName\":{},\"f:labels\":{\".\":{},\"f:app\":{},\"f:endpointslice.kubernetes.io/managed-by\":{},\"f:kubernetes.io/service-name\":{}},\"f:ownerReferences\":{\".\":{},\"k:{\\\"uid\\\":\\\"0518eed3-6152-42be-b566-0bd00a60faf8\\\"}\":{}}},\"f:ports\":{}}}]},\"addressType\":\"IPv4\",\"endpoints\":[{\"addresses\":[\"10.129.0.7\"],\"conditions\":{\"ready\":true,\"serving\":true,\"terminating\":false},\"targetRef\":{\"kind\":\"Pod\",\"namespace\":\"openshift-kube-controller-manager-operator\",\"name\":\"kube-controller-manager-operator-6b98b89ddd-8d4nf\",\"uid\":\"dd5139b8-e41c-4946-a31b-1a629314e844\",\"resourceVersion\":\"9038\"},\"nodeName\":\"ci-ln-qb8t3mb-72292-7s7rh-master-0\",\"zone\":\"us-central1-a\"}],\"ports\":[{\"name\":\"https\",\"protocol\":\"TCP\",\"port\":8443}]}","level":"debug","ts":"2022-05-17T09:55:08Z"}</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the FRR logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n metallb-system speaker-7m4qw -c frr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>Started watchfrr
2022/05/17 09:55:05 ZEBRA: client 16 says hello and bids fair to announce only bgp routes vrf=0
2022/05/17 09:55:05 ZEBRA: client 31 says hello and bids fair to announce only vnc routes vrf=0
2022/05/17 09:55:05 ZEBRA: client 38 says hello and bids fair to announce only static routes vrf=0
2022/05/17 09:55:05 ZEBRA: client 43 says hello and bids fair to announce only bfd routes vrf=0
2022/05/17 09:57:25.089 BGP: Creating Default VRF, AS 64500
2022/05/17 09:57:25.090 BGP: dup addr detect enable max_moves 5 time 180 freeze disable freeze_time 0
2022/05/17 09:57:25.090 BGP: bgp_get: Registering BGP instance (null) to zebra
2022/05/17 09:57:25.090 BGP: Registering VRF 0
2022/05/17 09:57:25.091 BGP: Rx Router Id update VRF 0 Id 10.131.0.1/32
2022/05/17 09:57:25.091 BGP: RID change : vrf VRF default(0), RTR ID 10.131.0.1
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF br0
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF ens4
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF ens4 addr 10.0.128.4/32
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF ens4 addr fe80::c9d:84da:4d86:5618/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF lo
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF ovs-system
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF tun0
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF tun0 addr 10.131.0.1/23
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF tun0 addr fe80::40f1:d1ff:feb6:5322/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF veth2da49fed
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF veth2da49fed addr fe80::24bd:d1ff:fec1:d88/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF veth2fa08c8c
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF veth2fa08c8c addr fe80::6870:ff:fe96:efc8/64
2022/05/17 09:57:25.091 BGP: Rx Intf add VRF 0 IF veth41e356b7
2022/05/17 09:57:25.091 BGP: Rx Intf address add VRF 0 IF veth41e356b7 addr fe80::48ff:37ff:fede:eb4b/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF veth1295c6e2
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF veth1295c6e2 addr fe80::b827:a2ff:feed:637/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF veth9733c6dc
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF veth9733c6dc addr fe80::3cf4:15ff:fe11:e541/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF veth336680ea
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF veth336680ea addr fe80::94b1:8bff:fe7e:488c/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vetha0a907b7
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vetha0a907b7 addr fe80::3855:a6ff:fe73:46c3/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vethf35a4398
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vethf35a4398 addr fe80::40ef:2fff:fe57:4c4d/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vethf831b7f4
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vethf831b7f4 addr fe80::f0d9:89ff:fe7c:1d32/64
2022/05/17 09:57:25.092 BGP: Rx Intf add VRF 0 IF vxlan_sys_4789
2022/05/17 09:57:25.092 BGP: Rx Intf address add VRF 0 IF vxlan_sys_4789 addr fe80::80c1:82ff:fe4b:f078/64
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] Timer (start timer expire).
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] BGP_Start (Idle-&gt;Connect), fd -1
2022/05/17 09:57:26.094 BGP: Allocated bnc 10.0.0.1/32(0)(VRF default) peer 0x7f807f7631a0
2022/05/17 09:57:26.094 BGP: sendmsg_zebra_rnh: sending cmd ZEBRA_NEXTHOP_REGISTER for 10.0.0.1/32 (vrf VRF default)
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] Waiting for NHT
2022/05/17 09:57:26.094 BGP: bgp_fsm_change_status : vrf default(0), Status: Connect established_peers 0
2022/05/17 09:57:26.094 BGP: 10.0.0.1 went from Idle to Connect
2022/05/17 09:57:26.094 BGP: 10.0.0.1 [FSM] TCP_connection_open_failed (Connect-&gt;Active), fd -1
2022/05/17 09:57:26.094 BGP: bgp_fsm_change_status : vrf default(0), Status: Active established_peers 0
2022/05/17 09:57:26.094 BGP: 10.0.0.1 went from Connect to Active
2022/05/17 09:57:26.094 ZEBRA: rnh_register msg from client bgp: hdr-&gt;length=8, type=nexthop vrf=0
2022/05/17 09:57:26.094 ZEBRA: 0: Add RNH 10.0.0.1/32 type Nexthop
2022/05/17 09:57:26.094 ZEBRA: 0:10.0.0.1/32: Evaluate RNH, type Nexthop (force)
2022/05/17 09:57:26.094 ZEBRA: 0:10.0.0.1/32: NH has become unresolved
2022/05/17 09:57:26.094 ZEBRA: 0: Client bgp registers for RNH 10.0.0.1/32 type Nexthop
2022/05/17 09:57:26.094 BGP: VRF default(0): Rcvd NH update 10.0.0.1/32(0) - metric 0/0 #nhops 0/0 flags 0x6
2022/05/17 09:57:26.094 BGP: NH update for 10.0.0.1/32(0)(VRF default) - flags 0x6 chgflags 0x0 - evaluate paths
2022/05/17 09:57:26.094 BGP: evaluate_paths: Updating peer (10.0.0.1(VRF default)) status with NHT
2022/05/17 09:57:30.081 ZEBRA: Event driven route-map update triggered
2022/05/17 09:57:30.081 ZEBRA: Event handler for route-map: 10.0.0.1-out
2022/05/17 09:57:30.081 ZEBRA: Event handler for route-map: 10.0.0.1-in
2022/05/17 09:57:31.104 ZEBRA: netlink_parse_info: netlink-listen (NS 0) type RTM_NEWNEIGH(28), len=76, seq=0, pid=0
2022/05/17 09:57:31.104 ZEBRA: 	Neighbor Entry received is not on a VLAN or a BRIDGE, ignoring
2022/05/17 09:57:31.105 ZEBRA: netlink_parse_info: netlink-listen (NS 0) type RTM_NEWNEIGH(28), len=76, seq=0, pid=0
2022/05/17 09:57:31.105 ZEBRA: 	Neighbor Entry received is not on a VLAN or a BRIDGE, ignoring</screen>
</para>
</formalpara>
</listitem>
</orderedlist>
<section xml:id="frr-log-levels_metallb-troubleshoot-support">
<title>FRRouting (FRR) log levels</title>
<simpara>The following table describes the FRR logging levels.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Log levels</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="top">Log level</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>all</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Supplies all logging information for all logging levels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>debug</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Information that is diagnostically helpful to people. Set to <literal>debug</literal> to give detailed troubleshooting information.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>info</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides information that always should be logged but under normal circumstances does not require user intervention. This is the default logging level.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>warn</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Anything that can potentially cause inconsistent <literal>MetalLB</literal> behaviour. Usually <literal>MetalLB</literal> automatically recovers from this type of error.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>error</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Any error that is fatal to the functioning of <literal>MetalLB</literal>. These errors usually require administrator intervention to fix.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>none</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Turn off all logging.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="nw-metallb-troubleshoot-bgp_metallb-troubleshoot-support">
<title>Troubleshooting BGP issues</title>
<simpara>The BGP implementation that Red Hat supports uses FRRouting (FRR) in a container in the <literal>speaker</literal> pods.
As a cluster administrator, if you need to troubleshoot BGP configuration issues, you need to run commands in the FRR container.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Display the names of the <literal>speaker</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n metallb-system pods -l component=speaker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME            READY   STATUS    RESTARTS   AGE
speaker-66bth   4/4     Running   0          56m
speaker-gvfnf   4/4     Running   0          56m
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Display the running configuration for FRR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show running-config"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>Building configuration...

Current configuration:
!
frr version 7.5.1_git
frr defaults traditional
hostname some-hostname
log file /etc/frr/frr.log informational
log timestamp precision 3
service integrated-vtysh-config
!
router bgp 64500  <co xml:id="CO277-1"/>
 bgp router-id 10.0.1.2
 no bgp ebgp-requires-policy
 no bgp default ipv4-unicast
 no bgp network import-check
 neighbor 10.0.2.3 remote-as 64500  <co xml:id="CO277-2"/>
 neighbor 10.0.2.3 bfd profile doc-example-bfd-profile-full  <co xml:id="CO277-3"/>
 neighbor 10.0.2.3 timers 5 15
 neighbor 10.0.2.4 remote-as 64500  <co xml:id="CO277-4"/>
 neighbor 10.0.2.4 bfd profile doc-example-bfd-profile-full  <co xml:id="CO277-5"/>
 neighbor 10.0.2.4 timers 5 15
 !
 address-family ipv4 unicast
  network 203.0.113.200/30   <co xml:id="CO277-6"/>
  neighbor 10.0.2.3 activate
  neighbor 10.0.2.3 route-map 10.0.2.3-in in
  neighbor 10.0.2.4 activate
  neighbor 10.0.2.4 route-map 10.0.2.4-in in
 exit-address-family
 !
 address-family ipv6 unicast
  network fc00:f853:ccd:e799::/124  <co xml:id="CO277-7"/>
  neighbor 10.0.2.3 activate
  neighbor 10.0.2.3 route-map 10.0.2.3-in in
  neighbor 10.0.2.4 activate
  neighbor 10.0.2.4 route-map 10.0.2.4-in in
 exit-address-family
!
route-map 10.0.2.3-in deny 20
!
route-map 10.0.2.4-in deny 20
!
ip nht resolve-via-default
!
ipv6 nht resolve-via-default
!
line vty
!
bfd
 profile doc-example-bfd-profile-full  <co xml:id="CO277-8"/>
  transmit-interval 35
  receive-interval 35
  passive-mode
  echo-mode
  echo-interval 35
  minimum-ttl 10
 !
!
end</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO277-1">
<para>The <literal>router bgp</literal> section indicates the ASN for MetalLB.</para>
</callout>
<callout arearefs="CO277-2 CO277-4">
<para>Confirm that a <literal>neighbor &lt;ip-address&gt; remote-as &lt;peer-ASN&gt;</literal> line exists for each BGP peer custom resource that you added.</para>
</callout>
<callout arearefs="CO277-3 CO277-5 CO277-8">
<para>If you configured BFD, confirm that the BFD profile is associated with the correct BGP peer and that the BFD profile appears in the command output.</para>
</callout>
<callout arearefs="CO277-6 CO277-7">
<para>Confirm that the <literal>network &lt;ip-address-range&gt;</literal> lines match the IP address ranges that you specified in address pool custom resources that you added.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Display the BGP summary:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show bgp summary"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>IPv4 Unicast Summary:
BGP router identifier 10.0.1.2, local AS number 64500 vrf-id 0
BGP table version 1
RIB entries 1, using 192 bytes of memory
Peers 2, using 29 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
10.0.2.3        4      64500       387       389        0    0    0 00:32:02            0        1  <co xml:id="CO278-1"/>
10.0.2.4        4      64500         0         0        0    0    0    never       Active        0  <co xml:id="CO278-2"/>

Total number of neighbors 2

IPv6 Unicast Summary:
BGP router identifier 10.0.1.2, local AS number 64500 vrf-id 0
BGP table version 1
RIB entries 1, using 192 bytes of memory
Peers 2, using 29 KiB of memory

Neighbor        V         AS   MsgRcvd   MsgSent   TblVer  InQ OutQ  Up/Down State/PfxRcd   PfxSnt
10.0.2.3        4      64500       387       389        0    0    0 00:32:02 NoNeg  <co xml:id="CO278-3"/>
10.0.2.4        4      64500         0         0        0    0    0    never       Active        0  <co xml:id="CO278-4"/>

Total number of neighbors 2</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO278-1 CO278-3">
<para>Confirm that the output includes a line for each BGP peer custom resource that you added.</para>
</callout>
<callout arearefs="CO278-2 CO278-4">
<para>Output that shows <literal>0</literal> messages received and messages sent indicates a BGP peer that does not have a BGP session.
Check network connectivity and the BGP configuration of the BGP peer.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Display the BGP peers that received an address pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show bgp ipv4 unicast 203.0.113.200/30"</programlisting>
<simpara>Replace <literal>ipv4</literal> with <literal>ipv6</literal> to display the BGP peers that received an IPv6 address pool.
Replace <literal>203.0.113.200/30</literal> with an IPv4 or IPv6 IP address range from an address pool.</simpara>
<formalpara>
<title>Example output</title>
<para>
<screen>BGP routing table entry for 203.0.113.200/30
Paths: (1 available, best #1, table default)
  Advertised to non peer-group peers:
  10.0.2.3  <co xml:id="CO279-1"/>
  Local
    0.0.0.0 from 0.0.0.0 (10.0.1.2)
      Origin IGP, metric 0, weight 32768, valid, sourced, local, best (First path received)
      Last update: Mon Jan 10 19:49:07 2022</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO279-1">
<para>Confirm that the output includes an IP address for a BGP peer.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-troubleshoot-bfd_metallb-troubleshoot-support">
<title>Troubleshooting BFD issues</title>
<simpara>The Bidirectional Forwarding Detection (BFD) implementation that Red Hat supports uses FRRouting (FRR) in a container in the <literal>speaker</literal> pods.
The BFD implementation relies on BFD peers also being configured as BGP peers with an established BGP session.
As a cluster administrator, if you need to troubleshoot BFD configuration issues, you need to run commands in the FRR container.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Display the names of the <literal>speaker</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n metallb-system pods -l component=speaker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME            READY   STATUS    RESTARTS   AGE
speaker-66bth   4/4     Running   0          26m
speaker-gvfnf   4/4     Running   0          26m
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Display the BFD peers:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n metallb-system speaker-66bth -c frr -- vtysh -c "show bfd peers brief"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>Session count: 2
SessionId  LocalAddress              PeerAddress              Status
=========  ============              ===========              ======
3909139637 10.0.1.2                  10.0.2.3                 up  <co xml:id="CO280-1"/></screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO280-1">
<para>Confirm that the <literal>PeerAddress</literal> column includes each BFD peer.
If the output does not list a BFD peer IP address that you expected the output to include, troubleshoot BGP connectivity with the peer.
If the status field indicates <literal>down</literal>, check for connectivity on the links and equipment between the node and the peer.
You can determine the node name for the speaker pod with a command like <literal>oc get pods -n metallb-system speaker-66bth -o jsonpath='{.spec.nodeName}'</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-metallb-metrics_metallb-troubleshoot-support">
<title>MetalLB metrics for BGP and BFD</title>
<simpara>OpenShift Container Platform captures the following metrics for MetalLB that relate to BGP peers and BFD profiles.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>MetalLB BFD metrics</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="top">Name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_control_packet_input</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BFD control packets received from each BFD peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_control_packet_output</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BFD control packets sent to each BFD peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_echo_packet_input</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BFD echo packets received from each BFD peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_echo_packet_output</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BFD echo packets sent to each BFD.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_session_down_events</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of times the BFD session with a peer entered the <literal>down</literal> state.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_session_up</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates the connection state with a BFD peer. <literal>1</literal> indicates the session is <literal>up</literal> and <literal>0</literal> indicates the session is <literal>down</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_session_up_events</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of times the BFD session with a peer entered the <literal>up</literal> state.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bfd_zebra_notifications</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BFD Zebra notifications for each BFD peer.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>MetalLB BGP metrics</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="top">Name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_announced_prefixes_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of load balancer IP address prefixes that are advertised to BGP peers. The terms <emphasis>prefix</emphasis> and <emphasis>aggregated route</emphasis> have the same meaning.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_session_up</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates the connection state with a BGP peer. <literal>1</literal> indicates the session is <literal>up</literal> and <literal>0</literal> indicates the session is <literal>down</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_updates_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP update messages sent to each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_opens_sent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP open messages sent to each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_opens_received</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP open messages received from each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_notifications_sent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP notification messages sent to each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_updates_total_received</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP update messages received from each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_keepalives_sent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP keepalive messages sent to each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_keepalives_received</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP keepalive messages received from each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_route_refresh_sent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of BGP route refresh messages sent to each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_total_sent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of total BGP messages sent to each BGP peer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>metallb_bgp_total_received</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counts the number of total BGP messages received from each BGP peer.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#about-querying-metrics_managing-metrics">Querying metrics</link> for information about using the monitoring dashboard.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-metallb-collecting-data_metallb-troubleshoot-support">
<title>About collecting MetalLB data</title>
<simpara>You can use the <literal>oc adm must-gather</literal> CLI command to collect information about your cluster, your MetalLB configuration, and the MetalLB Operator.
The following features and objects are associated with MetalLB and the MetalLB Operator:</simpara>
<itemizedlist>
<listitem>
<simpara>The namespace and child objects that the MetalLB Operator is deployed in</simpara>
</listitem>
<listitem>
<simpara>All MetalLB Operator custom resource definitions (CRDs)</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>oc adm must-gather</literal> CLI command collects the following information from FRRouting (FRR) that Red Hat uses to implement BGP and BFD:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/etc/frr/frr.conf</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/frr/frr.log</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/frr/daemons</literal> configuration file</simpara>
</listitem>
<listitem>
<simpara><literal>/etc/frr/vtysh.conf</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The log and configuration files in the preceding list are collected from the <literal>frr</literal> container in each <literal>speaker</literal> pod.</simpara>
<simpara>In addition to the log and configuration files, the <literal>oc adm must-gather</literal> CLI command collects the output from the following <literal>vtysh</literal> commands:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>show running-config</literal></simpara>
</listitem>
<listitem>
<simpara><literal>show bgp ipv4</literal></simpara>
</listitem>
<listitem>
<simpara><literal>show bgp ipv6</literal></simpara>
</listitem>
<listitem>
<simpara><literal>show bgp neighbor</literal></simpara>
</listitem>
<listitem>
<simpara><literal>show bfd peer</literal></simpara>
</listitem>
</itemizedlist>
<simpara>No additional configuration is required when you run the <literal>oc adm must-gather</literal> CLI command.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#gathering-cluster-data">Gathering data about your cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="associating-secondary-interfaces-metrics-to-network-attachments">
<title>Associating secondary interfaces metrics to network attachments</title>

<section xml:id="cnf-associating-secondary-interfaces-metrics-to-network-attachments_secondary-interfaces-metrics">
<title>Extending secondary network metrics for monitoring</title>
<simpara>Secondary devices, or interfaces, are used for different purposes. It is important to have a way to classify them to be able to aggregate the metrics for secondary devices with the same classification.</simpara>
<simpara>Exposed metrics contain the interface but do not specify where the interface originates. This is workable when there are no additional interfaces. However, if secondary interfaces are added, it can be difficult to use the metrics since it is hard to identify interfaces using only interface names.</simpara>
<simpara>When adding secondary interfaces, their names depend on the order in which they are added, and different secondary interfaces might belong to different networks and can be used for different purposes.</simpara>
<simpara>With <literal>pod_network_name_info</literal> it is possible to extend the current metrics with additional information that identifies the interface type. In this way, it is possible to aggregate the metrics and to add specific alarms to specific interface types.</simpara>
<simpara>The network type is generated using the name of the related <literal>NetworkAttachmentDefinition</literal>, that in turn is used to differentiate different classes of secondary networks. For example, different interfaces belonging to different networks or using different CNIs use different network attachment definition names.</simpara>
<section xml:id="cnf-associating-secondary-interfaces-metrics-to-network-attachments-network-metrics-daemon_secondary-interfaces-metrics">
<title>Network Metrics Daemon</title>
<simpara>The Network Metrics Daemon is a daemon component that collects and publishes network related metrics.</simpara>
<simpara>The kubelet is already publishing network related metrics you can observe. These metrics are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>container_network_receive_bytes_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_receive_errors_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_receive_packets_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_receive_packets_dropped_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_transmit_bytes_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_transmit_errors_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_transmit_packets_total</literal></simpara>
</listitem>
<listitem>
<simpara><literal>container_network_transmit_packets_dropped_total</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The labels in these metrics contain, among others:</simpara>
<itemizedlist>
<listitem>
<simpara>Pod name</simpara>
</listitem>
<listitem>
<simpara>Pod namespace</simpara>
</listitem>
<listitem>
<simpara>Interface name (such as <literal>eth0</literal>)</simpara>
</listitem>
</itemizedlist>
<simpara>These metrics work well until new interfaces are added to the pod, for example via <link xlink:href="https://github.com/intel/multus-cni">Multus</link>, as it is not clear what the interface names refer to.</simpara>
<simpara>The interface label refers to the interface name, but it is not clear what that interface is meant for. In case of many different interfaces, it would be impossible to understand what network the metrics you are monitoring refer to.</simpara>
<simpara>This is addressed by introducing the new <literal>pod_network_name_info</literal> described in the following section.</simpara>
</section>
<section xml:id="cnf-associating-secondary-interfaces-metrics-with-network-name_secondary-interfaces-metrics">
<title>Metrics with network name</title>
<simpara>This daemonset publishes a <literal>pod_network_name_info</literal> gauge metric, with a fixed value of <literal>0</literal>:</simpara>
<programlisting language="bash" linenumbering="unnumbered">pod_network_name_info{interface="net0",namespace="namespacename",network_name="nadnamespace/firstNAD",pod="podname"} 0</programlisting>
<simpara>The network name label is produced using the annotation added by Multus. It is the concatenation of the namespace the network attachment definition belongs to, plus the name of the network attachment definition.</simpara>
<simpara>The new metric alone does not provide much value, but combined with the network related <literal>container_network_*</literal> metrics, it offers better support for monitoring secondary networks.</simpara>
<simpara>Using a <literal>promql</literal> query like the following ones, it is possible to get a new metric containing the value and the network name retrieved from the <literal>k8s.v1.cni.cncf.io/network-status</literal> annotation:</simpara>
<programlisting language="bash" linenumbering="unnumbered">(container_network_receive_bytes_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_receive_errors_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_receive_packets_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_receive_packets_dropped_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_bytes_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_errors_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_packets_total) + on(namespace,pod,interface) group_left(network_name) ( pod_network_name_info )
(container_network_transmit_packets_dropped_total) + on(namespace,pod,interface) group_left(network_name)</programlisting>
</section>
</section>
</chapter>
</book>
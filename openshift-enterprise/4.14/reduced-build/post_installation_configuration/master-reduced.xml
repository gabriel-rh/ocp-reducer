<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Postinstallation configuration</title>
<date>2024-02-20</date>
</info>
<chapter xml:id="post-install-configuration-overview">
<title>Postinstallation configuration overview</title>

<simpara>After installing OpenShift Container Platform, a cluster administrator can configure and customize the following components:</simpara>
<itemizedlist>
<listitem>
<simpara>Machine</simpara>
</listitem>
<listitem>
<simpara>Bare metal</simpara>
</listitem>
<listitem>
<simpara>Cluster</simpara>
</listitem>
<listitem>
<simpara>Node</simpara>
</listitem>
<listitem>
<simpara>Network</simpara>
</listitem>
<listitem>
<simpara>Storage</simpara>
</listitem>
<listitem>
<simpara>Users</simpara>
</listitem>
<listitem>
<simpara>Alerts and notifications</simpara>
</listitem>
</itemizedlist>
<section xml:id="post-install-tasks">
<title>Configuration tasks to perform after installation</title>
<simpara>Cluster administrators can perform the following postinstallation configuration tasks:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/machine-configuration-tasks.xml#post-install-machine-configuration-tasks">Configure operating system features</link>:
Machine Config Operator (MCO) manages <literal>MachineConfig</literal> objects. By using MCO, you can perform the following tasks on an OpenShift Container Platform cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure nodes by using <literal>MachineConfig</literal> objects</simpara>
</listitem>
<listitem>
<simpara>Configure MCO-related custom resources</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/bare-metal-configuration.xml#post-install-bare-metal-configuration">Configure bare metal nodes</link>: The Bare Metal Operator (BMO) implements a Kubernetes API for managing bare metal hosts. It maintains an inventory of available bare metal hosts as instances of the BareMetalHost Custom Resource Definition (CRD). The Bare Metal Operator can:</simpara>
<itemizedlist>
<listitem>
<simpara>Inspect the host&#8217;s hardware details and report them on the corresponding BareMetalHost. This includes information about CPUs, RAM, disks, NICs, and more.</simpara>
</listitem>
<listitem>
<simpara>Inspect the host&#8217;s firmware and configure BIOS settings.</simpara>
</listitem>
<listitem>
<simpara>Provision hosts with a desired image.</simpara>
</listitem>
<listitem>
<simpara>Clean a host&#8217;s disk contents before or after provisioning.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/cluster-tasks.xml#post-install-cluster-tasks">Configure cluster features</link>:
As a cluster administrator, you can modify the configuration resources of the major features of an OpenShift Container Platform cluster. These features include:</simpara>
<itemizedlist>
<listitem>
<simpara>Image registry</simpara>
</listitem>
<listitem>
<simpara>Networking configuration</simpara>
</listitem>
<listitem>
<simpara>Image build behavior</simpara>
</listitem>
<listitem>
<simpara>Identity provider</simpara>
</listitem>
<listitem>
<simpara>The etcd configuration</simpara>
</listitem>
<listitem>
<simpara>Machine set creation to handle the workloads</simpara>
</listitem>
<listitem>
<simpara>Cloud provider credential management</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/configuring-private-cluster.xml#configuring-private-cluster">Configure cluster components to be private</link>:
By default, the installation program provisions OpenShift Container Platform by using a publicly accessible DNS and endpoints. If you want your cluster to be accessible only from within an internal network, configure the following components to be private:</simpara>
<itemizedlist>
<listitem>
<simpara>DNS</simpara>
</listitem>
<listitem>
<simpara>Ingress Controller</simpara>
</listitem>
<listitem>
<simpara>API server</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/node-tasks.xml#post-install-node-tasks">Perform node operations</link>:
By default, OpenShift Container Platform uses Red Hat Enterprise Linux CoreOS (RHCOS) compute machines.
As a cluster administrator, you can perform the following operations with the machines in your OpenShift Container Platform cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Add and remove compute machines</simpara>
</listitem>
<listitem>
<simpara>Add and remove taints and tolerations to the nodes</simpara>
</listitem>
<listitem>
<simpara>Configure the maximum number of pods per node</simpara>
</listitem>
<listitem>
<simpara>Enable Device Manager</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/network-configuration.xml#post-install-network-configuration">Configure network</link>:
After installing OpenShift Container Platform, you can configure the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Ingress cluster traffic</simpara>
</listitem>
<listitem>
<simpara>Node port service range</simpara>
</listitem>
<listitem>
<simpara>Network policy</simpara>
</listitem>
<listitem>
<simpara>Enabling the cluster-wide proxy</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/storage-configuration.xml#post-install-storage-configuration">Configure storage</link>:
By default, containers operate using ephemeral storage or transient local storage. The ephemeral storage has a lifetime limitation. TO store the data for a long time, you must configure persistent storage.
You can configure storage by using one of the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Dynamic provisioning</emphasis>: You can dynamically provision storage on demand by defining and creating storage classes that control different levels of storage, including storage access.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Static provisioning</emphasis>: You can use Kubernetes persistent volumes to make existing storage available to a cluster. Static provisioning can support various device configurations and mount options.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/preparing-for-users.xml#post-install-preparing-for-users">Configure users</link>:
OAuth access tokens allow users to authenticate themselves to the API. As a cluster administrator, you can configure OAuth to perform the following tasks:</simpara>
</listitem>
<listitem>
<simpara>Specify an identity provider</simpara>
</listitem>
<listitem>
<simpara>Use role-based access control to define and supply permissions to users</simpara>
</listitem>
<listitem>
<simpara>Install an Operator from OperatorHub</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/configuring-alert-notifications.xml#configuring-alert-notifications">Manage alerts and notifications</link>:
By default, firing alerts are displayed on the Alerting UI of the web console. You can also configure OpenShift Container Platform to send alert notifications to external systems.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring-private-cluster">
<title>Configuring a private cluster</title>

<simpara>After you install an OpenShift Container Platform version 4.14 cluster, you can set some of its core components to be private.</simpara>
<section xml:id="private-clusters-about_configuring-private-cluster">
<title>About private clusters</title>
<simpara>By default, OpenShift Container Platform is provisioned using publicly-accessible DNS and endpoints. You can set the DNS, Ingress Controller, and API server to private after you deploy your private cluster.</simpara>
<important>
<simpara>If the cluster has any public subnets, load balancer services created by administrators might be publicly accessible. To ensure cluster security, verify that these services are explicitly annotated as private.</simpara>
</important>
<bridgehead xml:id="private-clusters-about-dns_configuring-private-cluster" renderas="sect3">DNS</bridgehead>
<simpara>If you install OpenShift Container Platform on installer-provisioned infrastructure, the installation program creates records in a pre-existing public zone and, where possible, creates a private zone for the cluster&#8217;s own DNS resolution. In both the public zone and the private zone, the installation program or cluster creates DNS entries for <literal>*.apps</literal>, for the <literal>Ingress</literal> object, and <literal>api</literal>, for the API server.</simpara>
<simpara>The <literal>*.apps</literal> records in the public and private zone are identical, so when you delete the public zone, the private zone seamlessly provides all DNS resolution for the cluster.</simpara>
<bridgehead xml:id="private-clusters-about-ingress-controller_configuring-private-cluster" renderas="sect3">Ingress Controller</bridgehead>
<simpara>Because the default <literal>Ingress</literal> object is created as public, the load balancer is internet-facing and in the public subnets.</simpara>
<simpara>The Ingress Operator generates a default certificate for an Ingress Controller to serve as a placeholder until you configure a custom default certificate. Do not use Operator-generated default certificates in production clusters. The Ingress Operator does not rotate its own signing certificate or the default certificates that it generates. Operator-generated default certificates are intended as placeholders for custom default certificates that you configure.</simpara>
<bridgehead xml:id="private-clusters-about-api-server_configuring-private-cluster" renderas="sect3">API server</bridgehead>
<simpara>By default, the installation program creates appropriate network load balancers for the API server to use for both internal and external traffic.</simpara>
<simpara>On Amazon Web Services (AWS), separate public and private load balancers are created. The load balancers are identical except that an additional port is available on the internal one for use within the cluster. Although the installation program automatically creates or destroys the load balancer based on API server requirements, the cluster does not manage or maintain them. As long as you preserve the cluster&#8217;s access to the API server, you can manually modify or move the load balancers. For the public load balancer, port 6443 is open and the health check is configured for HTTPS against the <literal>/readyz</literal> path.</simpara>
<simpara>On Google Cloud Platform, a single load balancer is created to manage both internal and external API traffic, so you do not need to modify the load balancer.</simpara>
<simpara>On Microsoft Azure, both public and private load balancers are created. However, because of limitations in current implementation, you just retain both load balancers in a private cluster.</simpara>
</section>
<section xml:id="private-clusters-setting-dns-private_configuring-private-cluster">
<title>Setting DNS to private</title>
<simpara>After you deploy a cluster, you can modify its DNS to use only a private zone.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Review the <literal>DNS</literal> custom resource for your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dnses.config.openshift.io/cluster -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;: owned
  publicZone:
    id: Z2XXXXXXXXXXA4
status: {}</programlisting>
</para>
</formalpara>
<simpara>Note that the <literal>spec</literal> section contains both a private and a public zone.</simpara>
</listitem>
<listitem>
<simpara>Patch the <literal>DNS</literal> custom resource to remove the public zone:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch dnses.config.openshift.io/cluster --type=merge --patch='{"spec": {"publicZone": null}}'
dns.config.openshift.io/cluster patched</programlisting>
<simpara>Because the Ingress Controller consults the <literal>DNS</literal> definition when it creates <literal>Ingress</literal> objects, when you create or modify <literal>Ingress</literal> objects, only private records are created.</simpara>
<important>
<simpara>DNS records for the existing Ingress objects are not modified when you remove the public zone.</simpara>
</important>
</listitem>
<listitem>
<simpara>Optional: Review the <literal>DNS</literal> custom resource for your cluster and confirm that the public zone was removed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dnses.config.openshift.io/cluster -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;-wfpg4: owned
status: {}</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="private-clusters-setting-ingress-private_configuring-private-cluster">
<title>Setting the Ingress Controller to private</title>
<simpara>After you deploy a cluster, you can modify its Ingress Controller to use only a private zone.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the default Ingress Controller to use only an internal endpoint:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc replace --force --wait --filename - &lt;&lt;EOF
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  namespace: openshift-ingress-operator
  name: default
spec:
  endpointPublishingStrategy:
    type: LoadBalancerService
    loadBalancer:
      scope: Internal
EOF</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ingresscontroller.operator.openshift.io "default" deleted
ingresscontroller.operator.openshift.io/default replaced</programlisting>
</para>
</formalpara>
<simpara>The public DNS entry is removed, and the private zone entry is updated.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="private-clusters-setting-api-private_configuring-private-cluster">
<title>Restricting the API server to private</title>
<simpara>After you deploy a cluster to
Amazon Web Services (AWS) or
Microsoft Azure,
you can reconfigure the API server to use only the private zone.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Have access to the web console as a user with <literal>admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web portal or console for your cloud provider, take the following actions:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Locate and delete the appropriate load balancer component:</simpara>
<itemizedlist>
<listitem>
<simpara>For AWS, delete the external load balancer. The API DNS entry in the private zone already points to the internal load balancer, which uses an identical configuration, so you do not need to modify the internal load balancer.</simpara>
</listitem>
<listitem>
<simpara>For Azure, delete the <literal>api-internal</literal> rule for the load balancer.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Delete the <literal>api.$clustername.$yourdomain</literal> DNS entry in the public zone.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Remove the external load balancers:</simpara>
<important>
<simpara>You can run the following steps only for an installer-provisioned infrastructure (IPI) cluster. For a user-provisioned infrastructure (UPI) cluster, you must manually remove or disable the external load balancers.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>If your cluster uses a control plane machine set, delete the following lines in the control plane machine set custom resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">providerSpec:
  value:
    loadBalancers:
    - name: lk4pj-ext <co xml:id="CO1-1"/>
      type: network <co xml:id="CO1-2"/>
    - name: lk4pj-int
      type: network</programlisting>
<calloutlist>
<callout arearefs="CO1-1 CO1-2">
<para>Delete this line.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If your cluster does not use a control plane machine set, you must delete the external load balancers from each control plane machine.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>From your terminal, list the cluster machines by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                            STATE     TYPE        REGION      ZONE         AGE
lk4pj-master-0                  running   m4.xlarge   us-east-1   us-east-1a   17m
lk4pj-master-1                  running   m4.xlarge   us-east-1   us-east-1b   17m
lk4pj-master-2                  running   m4.xlarge   us-east-1   us-east-1a   17m
lk4pj-worker-us-east-1a-5fzfj   running   m4.xlarge   us-east-1   us-east-1a   15m
lk4pj-worker-us-east-1a-vbghs   running   m4.xlarge   us-east-1   us-east-1a   15m
lk4pj-worker-us-east-1b-zgpzg   running   m4.xlarge   us-east-1   us-east-1b   15m</programlisting>
</para>
</formalpara>
<simpara>The control plane machines contain <literal>master</literal> in the name.</simpara>
</listitem>
<listitem>
<simpara>Remove the external load balancer from each control plane machine:</simpara>
<orderedlist numeration="upperalpha">
<listitem>
<simpara>Edit a control plane machine object to by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machines -n openshift-machine-api &lt;control_plane_name&gt; <co xml:id="CO2-1"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>Specify the name of the control plane machine object to modify.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Remove the lines that describe the external load balancer, which are marked in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">providerSpec:
  value:
    loadBalancers:
    - name: lk4pj-ext <co xml:id="CO3-1"/>
      type: network <co xml:id="CO3-2"/>
    - name: lk4pj-int
      type: network</programlisting>
<calloutlist>
<callout arearefs="CO3-1 CO3-2">
<para>Delete this line.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save your changes and exit the object specification.</simpara>
</listitem>
<listitem>
<simpara>Repeat this process for each of the control plane machines.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<section xml:id="nw-ingresscontroller-change-internal_configuring-private-cluster">
<title>Configuring the Ingress Controller endpoint publishing scope to Internal</title>
<simpara>When a cluster administrator installs a new cluster without specifying that the cluster is private, the default Ingress Controller is created with a <literal>scope</literal> set to <literal>External</literal>. Cluster administrators can change an <literal>External</literal> scoped Ingress Controller to <literal>Internal</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To change an <literal>External</literal> scoped Ingress Controller to <literal>Internal</literal>, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"endpointPublishingStrategy":{"type":"LoadBalancerService","loadBalancer":{"scope":"Internal"}}}}'</programlisting>
</listitem>
<listitem>
<simpara>To check the status of the Ingress Controller, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator get ingresscontrollers/default -o yaml</programlisting>
<itemizedlist>
<listitem>
<simpara>The <literal>Progressing</literal> status condition indicates whether you must take further action. For example, the status condition can indicate that you need to delete the service by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress delete services/router-default</programlisting>
<simpara>If you delete the service, the Ingress Operator recreates it as <literal>Internal</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="registry-configuring-private-storage-endpoint-azure_configuring-private-cluster">
<title>Configuring a private storage endpoint on Azure</title>
<simpara>You can leverage the Image Registry Operator to use private endpoints on Azure, which enables seamless configuration of private storage accounts when OpenShift Container Platform is deployed on private Azure clusters. This allows you to deploy the image registry without exposing public-facing storage endpoints.</simpara>
<simpara>You can configure the Image Registry Operator to use private storage endpoints on Azure in one of two ways:</simpara>
<itemizedlist>
<listitem>
<simpara>By configuring the Image Registry Operator to discover the VNet and subnet names</simpara>
</listitem>
<listitem>
<simpara>With user-provided Azure Virtual Network (VNet) and subnet names</simpara>
</listitem>
</itemizedlist>
<section xml:id="limitations-configuring-private-storage-endpoint-azure">
<title>Limitations for configuring a private storage endpoint on Azure</title>
<simpara>The following limitations apply when configuring a private storage endpoint on Azure:</simpara>
<itemizedlist>
<listitem>
<simpara>When configuring the Image Registry Operator to use a private storage endpoint, public network access to the storage account is disabled. Consequently, pulling images from the registry outside of OpenShift Container Platform only works by setting <literal>disableRedirect: true</literal> in the registry Operator configuration. With redirect enabled, the registry redirects the client to pull images directly from the storage account, which will no longer work due to disabled public network access. For more information, see "Disabling redirect when using a private storage endpoint on Azure".</simpara>
</listitem>
<listitem>
<simpara>This operation cannot be undone by the Image Registry Operator.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-private-storage-endpoint-azure-vnet-subnet-iro-discovery_configuring-private-cluster">
<title>Configuring a private storage endpoint on Azure by enabling the Image Registry Operator to discover VNet and subnet names</title>
<simpara>The following procedure shows you how to set up a private storage endpoint on Azure by configuring the Image Registry Operator to discover VNet and subnet names.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured the image registry to run on Azure.</simpara>
</listitem>
<listitem>
<simpara>Your network has been set up using the Installer Provisioned Infrastructure installation method.</simpara>
<simpara>For users with a custom network setup, see "Configuring a private storage endpoint on Azure with user-provided VNet and subnet names".</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the Image Registry Operator <literal>config</literal> object and set <literal>networkAccess.type</literal> to <literal>Internal</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit configs.imageregistry/cluster</programlisting>
<programlisting language="terminal" linenumbering="unnumbered"># ...
spec:
  # ...
   storage:
      azure:
        # ...
        networkAccess:
          type: Internal
# ...</programlisting>
</listitem>
<listitem>
<simpara>Optional: Enter the following command to confirm that the Operator has completed provisioning. This might take a few minutes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configs.imageregistry/cluster -o=jsonpath="{.spec.storage.azure.privateEndpointName}" -w</programlisting>
</listitem>
<listitem>
<simpara>Optional: If the registry is exposed by a route, and you are configuring your storage account to be private, you must disable redirect if you want pulls external to the cluster to continue to work. Enter the following command to disable redirect on the Image Operator configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch configs.imageregistry cluster --type=merge -p '{"spec":{"disableRedirect": true}}'</programlisting>
<note>
<simpara>When redirect is enabled,  pulling images from outside of the cluster will not work.</simpara>
</note>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Fetch the registry service name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc registry info --internal=true</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">image-registry.openshift-image-registry.svc:5000</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter debug mode by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Run the suggested <literal>chroot</literal> command. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Enter the following command to log in to your container registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login --tls-verify=false -u unused -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Login Succeeded!</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter the following command to verify that you can pull an image from the registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman pull --tls-verify=false image-registry.openshift-image-registry.svc:5000/openshift/tools</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Trying to pull image-registry.openshift-image-registry.svc:5000/openshift/tools/openshift/tools...
Getting image source signatures
Copying blob 6b245f040973 done
Copying config 22667f5368 done
Writing manifest to image destination
Storing signatures
22667f53682a2920948d19c7133ab1c9c3f745805c14125859d20cede07f11f9</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-private-storage-endpoint-azure-user-provided-vnet-subnet_configuring-private-cluster">
<title>Configuring a private storage endpoint on Azure with user-provided VNet and subnet names</title>
<simpara>Use the following procedure to configure a storage account that has public network access disabled and is exposed behind a private storage endpoint on Azure.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured the image registry to run on Azure.</simpara>
</listitem>
<listitem>
<simpara>You must know the VNet and subnet names used for your Azure environment.</simpara>
</listitem>
<listitem>
<simpara>If your network was configured in a separate resource group in Azure, you must also know its name.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the Image Registry Operator <literal>config</literal> object and configure the private endpoint using your VNet and subnet names:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit configs.imageregistry/cluster</programlisting>
<programlisting language="terminal" linenumbering="unnumbered"># ...
spec:
  # ...
   storage:
      azure:
        # ...
        networkAccess:
          type: Internal
          internal:
            subnetName: &lt;subnet_name&gt;
            vnetName: &lt;vnet_name&gt;
            networkResourceGroupName: &lt;network_resource_group_name&gt;
# ...</programlisting>
</listitem>
<listitem>
<simpara>Optional: Enter the following command to confirm that the Operator has completed provisioning. This might take a few minutes.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configs.imageregistry/cluster -o=jsonpath="{.spec.storage.azure.privateEndpointName}" -w</programlisting>
<note>
<simpara>When redirect is enabled, pulling images from outside of the cluster will not work.</simpara>
</note>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Fetch the registry service name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc registry info --internal=true</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">image-registry.openshift-image-registry.svc:5000</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter debug mode by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Run the suggested <literal>chroot</literal> command. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Enter the following command to log in to your container registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login --tls-verify=false -u unused -p $(oc whoami -t) image-registry.openshift-image-registry.svc:5000</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Login Succeeded!</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter the following command to verify that you can pull an image from the registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman pull --tls-verify=false image-registry.openshift-image-registry.svc:5000/openshift/tools</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Trying to pull image-registry.openshift-image-registry.svc:5000/openshift/tools/openshift/tools...
Getting image source signatures
Copying blob 6b245f040973 done
Copying config 22667f5368 done
Writing manifest to image destination
Storing signatures
22667f53682a2920948d19c7133ab1c9c3f745805c14125859d20cede07f11f9</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="disabling-redirect-private-storage-endpoint-azure_configuring-private-cluster">
<title>Optional: Disabling redirect when using a private storage endpoint on Azure</title>
<simpara>By default, redirect is enabled when using the image registry. Redirect allows off-loading of traffic from the registry pods into the object storage, which makes pull faster. When redirect is enabled and the storage account is private, users from outside of the cluster are unable to pull images from the registry.</simpara>
<simpara>In some cases, users might want to disable redirect so that users from outside of the cluster can pull images from the registry.</simpara>
<simpara>Use the following procedure to disable redirect.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured the image registry to run on Azure.</simpara>
</listitem>
<listitem>
<simpara>You have configured a route.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enter the following command to disable redirect on the image
registry configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch configs.imageregistry cluster --type=merge -p '{"spec":{"disableRedirect": true}}'</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Fetch the registry service name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc registry info</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">default-route-openshift-image-registry.&lt;cluster_dns&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter the following command to log in to your container registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman login --tls-verify=false -u unused -p $(oc whoami -t) default-route-openshift-image-registry.&lt;cluster_dns&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Login Succeeded!</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter the following command to verify that you can pull an image from the registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman pull --tls-verify=false default-route-openshift-image-registry.&lt;cluster_dns&gt;
/openshift/tools</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Trying to pull default-route-openshift-image-registry.&lt;cluster_dns&gt;/openshift/tools...
Getting image source signatures
Copying blob 6b245f040973 done
Copying config 22667f5368 done
Writing manifest to image destination
Storing signatures
22667f53682a2920948d19c7133ab1c9c3f745805c14125859d20cede07f11f9</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="post-install-bare-metal-configuration">
<title>Bare metal configuration</title>

<simpara>When deploying OpenShift Container Platform on bare metal hosts, there are times when you need to make changes to the host either before or after provisioning. This can include inspecting the host&#8217;s hardware, firmware, and firmware details. It can also include formatting disks or changing modifiable firmware settings.</simpara>
<section xml:id="bmo-about-the-bare-metal-operator_post-install-bare-metal-configuration">
<title>About the Bare Metal Operator</title>
<simpara>Use the Bare Metal Operator (BMO) to provision, manage, and inspect bare-metal hosts in your cluster.</simpara>
<simpara>The BMO uses three resources to complete these tasks:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>BareMetalHost</literal></simpara>
</listitem>
<listitem>
<simpara><literal>HostFirmwareSettings</literal></simpara>
</listitem>
<listitem>
<simpara><literal>FirmwareSchema</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The BMO maintains an inventory of the physical hosts in the cluster by mapping each bare-metal host to an instance of the <literal>BareMetalHost</literal> custom resource definition. Each <literal>BareMetalHost</literal> resource features hardware, software, and firmware details. The BMO continually inspects the bare-metal hosts in the cluster to ensure each <literal>BareMetalHost</literal> resource accurately details the components of the corresponding host.</simpara>
<simpara>The BMO also uses the <literal>HostFirmwareSettings</literal> resource and the <literal>FirmwareSchema</literal> resource to detail firmware specifications for the bare-metal host.</simpara>
<simpara>The BMO interfaces with bare-metal hosts in the cluster by using the Ironic API service. The Ironic service uses the Baseboard Management Controller (BMC) on the host to interface with the machine.</simpara>
<simpara>Some common tasks you can complete by using the BMO include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Provision bare-metal hosts to the cluster with a specific image</simpara>
</listitem>
<listitem>
<simpara>Format a host&#8217;s disk contents before provisioning or after deprovisioning</simpara>
</listitem>
<listitem>
<simpara>Turn on or off a host</simpara>
</listitem>
<listitem>
<simpara>Change firmware settings</simpara>
</listitem>
<listitem>
<simpara>View the host&#8217;s hardware details</simpara>
</listitem>
</itemizedlist>
<section xml:id="bmo-bare-metal-operator-architecture_post-install-bare-metal-configuration">
<title>Bare Metal Operator architecture</title>
<simpara>The Bare Metal Operator (BMO) uses three resources to provision, manage, and inspect bare-metal hosts in your cluster. The following diagram illustrates the architecture of these resources:</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/302_OpenShift_Bare_Metal_Operator_0223.png"/>
</imageobject>
<textobject><phrase>BMO architecture overview</phrase></textobject>
</mediaobject>
</informalfigure>
<formalpara>
<title>BareMetalHost</title>
<para>The <literal>BareMetalHost</literal> resource defines a physical host and its properties. When you provision a bare-metal host to the cluster, you must define a <literal>BareMetalHost</literal> resource for that host. For ongoing management of the host, you can inspect the information in the <literal>BareMetalHost</literal> or update this information.</para>
</formalpara>
<simpara>The <literal>BareMetalHost</literal> resource features provisioning information such as the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Deployment specifications such as the operating system boot image or the custom RAM disk</simpara>
</listitem>
<listitem>
<simpara>Provisioning state</simpara>
</listitem>
<listitem>
<simpara>Baseboard Management Controller (BMC) address</simpara>
</listitem>
<listitem>
<simpara>Desired power state</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>BareMetalHost</literal> resource features hardware information such as the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Number of CPUs</simpara>
</listitem>
<listitem>
<simpara>MAC address of a NIC</simpara>
</listitem>
<listitem>
<simpara>Size of the host&#8217;s storage device</simpara>
</listitem>
<listitem>
<simpara>Current power state</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>HostFirmwareSettings</title>
<para>You can use the <literal>HostFirmwareSettings</literal> resource to retrieve and manage the firmware settings for a host. When a host moves to the <literal>Available</literal> state, the Ironic service reads the host&#8217;s firmware settings and creates the <literal>HostFirmwareSettings</literal> resource. There is a one-to-one mapping between the <literal>BareMetalHost</literal> resource and the <literal>HostFirmwareSettings</literal> resource.</para>
</formalpara>
<simpara>You can use the <literal>HostFirmwareSettings</literal> resource to inspect the firmware specifications for a host or to update a host&#8217;s firmware specifications.</simpara>
<note>
<simpara>You must adhere to the schema specific to the vendor firmware when you edit the <literal>spec</literal> field of the <literal>HostFirmwareSettings</literal> resource. This schema is defined in the read-only <literal>FirmwareSchema</literal> resource.</simpara>
</note>
<formalpara>
<title>FirmwareSchema</title>
<para>Firmware settings vary among hardware vendors and host models. A <literal>FirmwareSchema</literal> resource is a read-only resource that contains the types and limits for each firmware setting on each host model. The data comes directly from the BMC by using the Ironic service. The <literal>FirmwareSchema</literal> resource enables you to identify valid values you can specify in the <literal>spec</literal> field of the <literal>HostFirmwareSettings</literal> resource.</para>
</formalpara>
<simpara>A <literal>FirmwareSchema</literal> resource can apply to many <literal>BareMetalHost</literal> resources if the schema is the same.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://metal3.io/">Metal³ API service for provisioning bare-metal hosts</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://ironicbaremetal.org/">Ironic API service for managing bare-metal infrastructure</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="about-the-baremetalhost-resource_post-install-bare-metal-configuration">
<title>About the BareMetalHost resource</title>
<simpara>Metal<superscript>3</superscript> introduces the concept of the <literal>BareMetalHost</literal> resource, which defines a physical host and its properties. The <literal>BareMetalHost</literal> resource contains two sections:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The <literal>BareMetalHost</literal> spec</simpara>
</listitem>
<listitem>
<simpara>The <literal>BareMetalHost</literal> status</simpara>
</listitem>
</orderedlist>
<section xml:id="_the_baremetalhost_spec">
<title>The BareMetalHost spec</title>
<simpara>The <literal>spec</literal> section of the <literal>BareMetalHost</literal> resource defines the desired state of the host.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>BareMetalHost spec</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Parameters</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>automatedCleaningMode</literal></simpara></entry>
<entry align="left" valign="top"><simpara>An interface to enable or disable automated cleaning during provisioning and de-provisioning. When set to <literal>disabled</literal>, it skips automated cleaning. When set to <literal>metadata</literal>, automated cleaning is enabled. The default setting is <literal>metadata</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><screen>bmc:
  address:
  credentialsName:
  disableCertificateVerification:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>bmc</literal> configuration setting contains the connection information for the baseboard management controller (BMC) on the host. The fields are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>address</literal>: The URL for communicating with the host&#8217;s BMC controller.</simpara>
</listitem>
<listitem>
<simpara><literal>credentialsName</literal>: A reference to a secret containing the username and password for the BMC.</simpara>
</listitem>
<listitem>
<simpara><literal>disableCertificateVerification</literal>: A boolean to skip certificate validation when set to <literal>true</literal>.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>bootMACAddress</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The MAC address of the NIC used for provisioning the host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>bootMode</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The boot mode of the host. It defaults to <literal>UEFI</literal>, but it can also be set to <literal>legacy</literal> for BIOS boot, or <literal>UEFISecureBoot</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>consumerRef</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A reference to another resource that is using the host. It could be empty if another resource is not currently using the host. For example, a <literal>Machine</literal> resource might use the host when the <literal>machine-api</literal> is using the host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>description</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A human-provided string to help identify the host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>externallyProvisioned</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A boolean indicating whether the host provisioning and deprovisioning are managed externally. When set:</simpara>
<itemizedlist>
<listitem>
<simpara>Power status can still be managed using the online field.</simpara>
</listitem>
<listitem>
<simpara>Hardware inventory will be monitored, but no provisioning or deprovisioning operations are performed on the host.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>firmware</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Contains information about the BIOS configuration of bare metal hosts. Currently, <literal>firmware</literal> is only supported by iRMC, iDRAC, iLO4 and iLO5 BMCs. The sub fields are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>simultaneousMultithreadingEnabled</literal>: Allows a single physical processor core to appear as several logical processors. Valid settings are <literal>true</literal> or <literal>false</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>sriovEnabled</literal>: SR-IOV support enables a hypervisor to create virtual instances of a PCI-express device, potentially increasing performance. Valid settings are <literal>true</literal> or <literal>false</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>virtualizationEnabled</literal>: Supports the virtualization of platform hardware. Valid settings are <literal>true</literal> or <literal>false</literal>.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>image:
  url:
  checksum:
  checksumType:
  format:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>image</literal> configuration setting holds the details for the image to be deployed on the host. Ironic requires the image fields. However, when the <literal>externallyProvisioned</literal> configuration setting is set to <literal>true</literal> and the external management doesn&#8217;t require power control, the fields can be empty. The fields are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>url</literal>: The URL of an image to deploy to the host.</simpara>
</listitem>
<listitem>
<simpara><literal>checksum</literal>: The actual checksum or a URL to a file containing the checksum for the image at <literal>image.url</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>checksumType</literal>: You can specify checksum algorithms. Currently <literal>image.checksumType</literal> only supports <literal>md5</literal>, <literal>sha256</literal>, and <literal>sha512</literal>. The default checksum type is <literal>md5</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>format</literal>: This is the disk format of the image. It can be one of <literal>raw</literal>, <literal>qcow2</literal>, <literal>vdi</literal>, <literal>vmdk</literal>, <literal>live-iso</literal> or be left unset. Setting it to <literal>raw</literal> enables raw image streaming in the Ironic agent for that image. Setting it to <literal>live-iso</literal> enables iso images to live boot without deploying to disk, and it ignores the <literal>checksum</literal> fields.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>networkData</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A reference to the secret containing the network configuration data and its namespace, so that it can be attached to the host before the host boots to set up the network.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>online</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A boolean indicating whether the host should be powered on (<literal>true</literal>) or off (<literal>false</literal>). Changing this value will trigger a change in the power state of the physical host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><screen>raid:
  hardwareRAIDVolumes:
  softwareRAIDVolumes:</screen></entry>
<entry align="left" valign="top"><simpara>(Optional) Contains the information about the RAID configuration for bare metal hosts. If not specified, it retains the current configuration.</simpara>
<note>
<simpara>OpenShift Container Platform 4.14 supports hardware RAID for BMCs, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Fujitsu iRMC with support for RAID levels 0, 1, 5, 6, and 10</simpara>
</listitem>
<listitem>
<simpara>Dell iDRAC using the Redfish API with firmware version 6.10.30.20 or later and RAID levels 0, 1, and 5</simpara>
</listitem>
</itemizedlist>
<simpara>OpenShift Container Platform 4.14 does not support software RAID.</simpara>
</note>
<simpara>See the following configuration settings:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>hardwareRAIDVolumes</literal>: Contains the list of logical drives for hardware RAID, and defines the desired volume configuration in the hardware RAID. If you don&#8217;t specify <literal>rootDeviceHints</literal>, the first volume is the root volume. The sub-fields are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>level</literal>: The RAID level for the logical drive. The following levels are supported: <literal>0</literal>,<literal>1</literal>,<literal>2</literal>,<literal>5</literal>,<literal>6</literal>,<literal>1+0</literal>,<literal>5+0</literal>,<literal>6+0</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>name</literal>: The name of the volume as a string. It should be unique within the server. If not specified, the volume name will be auto-generated.</simpara>
</listitem>
<listitem>
<simpara><literal>numberOfPhysicalDisks</literal>: The number of physical drives as an integer to use for the logical drove. Defaults to the minimum number of disk drives required for the particular RAID level.</simpara>
</listitem>
<listitem>
<simpara><literal>physicalDisks</literal>: The list of names of physical disk drives as a string. This is an optional field. If specified, the controller field must be specified too.</simpara>
</listitem>
<listitem>
<simpara><literal>controller</literal>: (Optional) The name of the RAID controller as a string to use in the hardware RAID volume.</simpara>
</listitem>
<listitem>
<simpara><literal>rotational</literal>: If set to <literal>true</literal>, it will only select rotational disk drives. If set to <literal>false</literal>, it will only select solid-state and NVMe drives. If not set, it selects any drive types, which is the default behavior.</simpara>
</listitem>
<listitem>
<simpara><literal>sizeGibibytes</literal>: The size of the logical drive as an integer to create in GiB. If unspecified or set to <literal>0</literal>, it will use the maximum capacity of physical drive for the logical drive.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><literal>softwareRAIDVolumes</literal>: OpenShift Container Platform 4.14 does not support software RAID. The following information is for reference only. This configuration contains the list of logical disks for software RAID. If you don&#8217;t specify <literal>rootDeviceHints</literal>, the first volume is the root volume. If you set <literal>HardwareRAIDVolumes</literal>, this item will be invalid. Software RAIDs will always be deleted. The number of created software RAID devices must be <literal>1</literal> or <literal>2</literal>. If there is only one software RAID device, it must be <literal>RAID-1</literal>. If there are two RAID devices, the first device must be <literal>RAID-1</literal>, while the RAID level for the second device can be <literal>0</literal>, <literal>1</literal>, or <literal>1+0</literal>. The first RAID device will be the deployment device. Therefore, enforcing <literal>RAID-1</literal> reduces the risk of a non-booting node in case of a device failure. The <literal>softwareRAIDVolume</literal> field defines the desired configuration of the volume in the software RAID. The sub-fields are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>level</literal>: The RAID level for the logical drive. The following levels are supported: <literal>0</literal>,<literal>1</literal>,<literal>1+0</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>physicalDisks</literal>: A list of device hints. The number of items should be greater than or equal to <literal>2</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>sizeGibibytes</literal>: The size of the logical disk drive as an integer to be created in GiB. If unspecified or set to <literal>0</literal>, it will use the maximum capacity of physical drive for logical drive.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>You can set the <literal>hardwareRAIDVolume</literal> as an empty slice to clear the hardware RAID configuration. For example:</simpara>
<screen>spec:
   raid:
     hardwareRAIDVolume: []</screen>
<simpara>If you receive an error message indicating that the driver does not support RAID, set the <literal>raid</literal>, <literal>hardwareRAIDVolumes</literal> or <literal>softwareRAIDVolumes</literal> to nil. You might need to ensure the host has a RAID controller.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><screen>rootDeviceHints:
  deviceName:
  hctl:
  model:
  vendor:
  serialNumber:
  minSizeGigabytes:
  wwn:
  wwnWithExtension:
  wwnVendorExtension:
  rotational:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>rootDeviceHints</literal> parameter enables provisioning of the RHCOS image to a particular device. It examines the devices in the order it discovers them, and compares the discovered values with the hint values. It uses the first discovered device that matches the hint value. The configuration can combine multiple hints, but a device must match all hints to get selected. The fields are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>deviceName</literal>: A string containing a Linux device name like <literal>/dev/vda</literal>. The hint must match the actual value exactly.</simpara>
</listitem>
<listitem>
<simpara><literal>hctl</literal>: A string containing a SCSI bus address like <literal>0:0:0:0</literal>. The hint must match the actual value exactly.</simpara>
</listitem>
<listitem>
<simpara><literal>model</literal>: A string containing a vendor-specific device identifier. The hint can be a substring of the actual value.</simpara>
</listitem>
<listitem>
<simpara><literal>vendor</literal>: A string containing the name of the vendor or manufacturer of the device. The hint can be a sub-string of the actual value.</simpara>
</listitem>
<listitem>
<simpara><literal>serialNumber</literal>: A string containing the device serial number. The hint must match the actual value exactly.</simpara>
</listitem>
<listitem>
<simpara><literal>minSizeGigabytes</literal>: An integer representing the minimum size of the device in gigabytes.</simpara>
</listitem>
<listitem>
<simpara><literal>wwn</literal>: A string containing the unique storage identifier. The hint must match the actual value exactly.</simpara>
</listitem>
<listitem>
<simpara><literal>wwnWithExtension</literal>: A string containing the unique storage identifier with the vendor extension appended. The hint must match the actual value exactly.</simpara>
</listitem>
<listitem>
<simpara><literal>wwnVendorExtension</literal>: A string containing the unique vendor storage identifier. The hint must match the actual value exactly.</simpara>
</listitem>
<listitem>
<simpara><literal>rotational</literal>: A boolean indicating whether the device should be a rotating disk (true) or not (false).</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="_the_baremetalhost_status">
<title>The BareMetalHost status</title>
<simpara>The <literal>BareMetalHost</literal> status represents the host&#8217;s current state, and includes tested credentials, current hardware details, and other information.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>BareMetalHost status</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Parameters</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>goodCredentials</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A reference to the secret and its namespace holding the last set of baseboard management controller (BMC) credentials the system was able to validate as working.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>errorMessage</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Details of the last error reported by the provisioning backend, if any.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>errorType</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates the class of problem that has caused the host to enter an error state. The error types are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>provisioned registration error</literal>: Occurs when the controller is unable to re-register an already provisioned host.</simpara>
</listitem>
<listitem>
<simpara><literal>registration error</literal>: Occurs when the controller is unable to connect to the host&#8217;s baseboard management controller.</simpara>
</listitem>
<listitem>
<simpara><literal>inspection error</literal>: Occurs when an attempt to obtain hardware details from the host fails.</simpara>
</listitem>
<listitem>
<simpara><literal>preparation error</literal>: Occurs when cleaning fails.</simpara>
</listitem>
<listitem>
<simpara><literal>provisioning error</literal>: Occurs when the controller fails to provision or deprovision the host.</simpara>
</listitem>
<listitem>
<simpara><literal>power management error</literal>: Occurs when the controller is unable to modify the power state of the host.</simpara>
</listitem>
<listitem>
<simpara><literal>detach error</literal>: Occurs when the controller is unable to detatch the host from the provisioner.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>hardware:
  cpu
    arch:
    model:
    clockMegahertz:
    flags:
    count:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>hardware.cpu</literal> field details of the CPU(s) in the system. The fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>arch</literal>: The architecture of the CPU.</simpara>
</listitem>
<listitem>
<simpara><literal>model</literal>: The CPU model as a string.</simpara>
</listitem>
<listitem>
<simpara><literal>clockMegahertz</literal>: The speed in MHz of the CPU.</simpara>
</listitem>
<listitem>
<simpara><literal>flags</literal>: The list of CPU flags. For example, <literal>'mmx','sse','sse2','vmx'</literal> etc.</simpara>
</listitem>
<listitem>
<simpara><literal>count</literal>: The number of CPUs available in the system.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>hardware:
  firmware:</screen></entry>
<entry align="left" valign="top"><simpara>Contains BIOS firmware information. For example, the hardware vendor and version.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><screen>hardware:
  nics:
  - ip:
    name:
    mac:
    speedGbps:
    vlans:
    vlanId:
    pxe:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>hardware.nics</literal> field contains a list of network interfaces for the host. The fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>ip</literal>: The IP address of the NIC, if one was assigned when the discovery agent ran.</simpara>
</listitem>
<listitem>
<simpara><literal>name</literal>: A string identifying the network device. For example, <literal>nic-1</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>mac</literal>: The MAC address of the NIC.</simpara>
</listitem>
<listitem>
<simpara><literal>speedGbps</literal>: The speed of the device in Gbps.</simpara>
</listitem>
<listitem>
<simpara><literal>vlans</literal>: A list holding all the VLANs available for this NIC.</simpara>
</listitem>
<listitem>
<simpara><literal>vlanId</literal>: The untagged VLAN ID.</simpara>
</listitem>
<listitem>
<simpara><literal>pxe</literal>: Whether the NIC is able to boot using PXE.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>hardware:
  ramMebibytes:</screen></entry>
<entry align="left" valign="top"><simpara>The host&#8217;s amount of memory in Mebibytes (MiB).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><screen>hardware:
  storage:
  - name:
    rotational:
    sizeBytes:
    serialNumber:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>hardware.storage</literal> field contains a list of storage devices available to the host. The fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>name</literal>: A string identifying the storage device. For example, <literal>disk 1 (boot)</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>rotational</literal>: Indicates whether the disk is rotational, and returns either <literal>true</literal> or <literal>false</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>sizeBytes</literal>: The size of the storage device.</simpara>
</listitem>
<listitem>
<simpara><literal>serialNumber</literal>: The device&#8217;s serial number.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>hardware:
  systemVendor:
    manufacturer:
    productName:
    serialNumber:</screen></entry>
<entry align="left" valign="top"><simpara>Contains information about the host&#8217;s <literal>manufacturer</literal>, the <literal>productName</literal>, and the <literal>serialNumber</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>lastUpdated</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The timestamp of the last time the status of the host was updated.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>operationalStatus</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The status of the server. The status is one of the following:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>OK</literal>: Indicates all the details for the host are known, correctly configured, working, and manageable.</simpara>
</listitem>
<listitem>
<simpara><literal>discovered</literal>: Implies some of the host&#8217;s details are either not working correctly or missing. For example, the BMC address is known but the login credentials are not.</simpara>
</listitem>
<listitem>
<simpara><literal>error</literal>: Indicates the system found some sort of irrecoverable error. Refer to the <literal>errorMessage</literal> field in the status section for more details.</simpara>
</listitem>
<listitem>
<simpara><literal>delayed</literal>: Indicates that provisioning is delayed to limit simultaneous provisioning of multiple hosts.</simpara>
</listitem>
<listitem>
<simpara><literal>detached</literal>: Indicates the host is marked <literal>unmanaged</literal>.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>poweredOn</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Boolean indicating whether the host is powered on.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><screen>provisioning:
  state:
  id:
  image:
  raid:
  firmware:
  rootDeviceHints:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>provisioning</literal> field contains values related to deploying an image to the host. The sub-fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>state</literal>: The current state of any ongoing provisioning operation. The states include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>&lt;empty string&gt;</literal>: There is no provisioning happening at the moment.</simpara>
</listitem>
<listitem>
<simpara><literal>unmanaged</literal>: There is insufficient information available to register the host.</simpara>
</listitem>
<listitem>
<simpara><literal>registering</literal>: The agent is checking the host&#8217;s BMC details.</simpara>
</listitem>
<listitem>
<simpara><literal>match profile</literal>: The agent is comparing the discovered hardware details on the host against known profiles.</simpara>
</listitem>
<listitem>
<simpara><literal>available</literal>: The host is available for provisioning. This state was previously known as <literal>ready</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>preparing</literal>: The existing configuration will be removed, and the new configuration will be set on the host.</simpara>
</listitem>
<listitem>
<simpara><literal>provisioning</literal>: The provisioner is writing an image to the host&#8217;s storage.</simpara>
</listitem>
<listitem>
<simpara><literal>provisioned</literal>: The provisioner wrote an image to the host&#8217;s storage.</simpara>
</listitem>
<listitem>
<simpara><literal>externally provisioned</literal>: Metal<superscript>3</superscript> does not manage the image on the host.</simpara>
</listitem>
<listitem>
<simpara><literal>deprovisioning</literal>: The provisioner is wiping the image from the host&#8217;s storage.</simpara>
</listitem>
<listitem>
<simpara><literal>inspecting</literal>: The agent is collecting hardware details for the host.</simpara>
</listitem>
<listitem>
<simpara><literal>deleting</literal>: The agent is deleting the from the cluster.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><literal>id</literal>: The unique identifier for the service in the underlying provisioning tool.</simpara>
</listitem>
<listitem>
<simpara><literal>image</literal>: The image most recently provisioned to the host.</simpara>
</listitem>
<listitem>
<simpara><literal>raid</literal>: The list of hardware or software RAID volumes recently set.</simpara>
</listitem>
<listitem>
<simpara><literal>firmware</literal>: The BIOS configuration for the bare metal server.</simpara>
</listitem>
<listitem>
<simpara><literal>rootDeviceHints</literal>: The root device selection instructions used for the most recent provisioning operation.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>triedCredentials</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A reference to the secret and its namespace holding the last set of BMC credentials that were sent to the provisioning backend.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="getting-the-baremetalhost-resource_post-install-bare-metal-configuration">
<title>Getting the BareMetalHost resource</title>
<simpara>The <literal>BareMetalHost</literal> resource contains the properties of a physical host. You must get the <literal>BareMetalHost</literal> resource for a physical host to review its properties.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the list of <literal>BareMetalHost</literal> resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get bmh -n openshift-machine-api -o yaml</programlisting>
<note>
<simpara>You can use <literal>baremetalhost</literal> as the long form of <literal>bmh</literal> with <literal>oc get</literal> command.</simpara>
</note>
</listitem>
<listitem>
<simpara>Get the list of hosts:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get bmh -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Get the <literal>BareMetalHost</literal> resource for a specific host:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get bmh &lt;host_name&gt; -n openshift-machine-api -o yaml</programlisting>
<simpara>Where <literal>&lt;host_name&gt;</literal> is the name of the host.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  creationTimestamp: "2022-06-16T10:48:33Z"
  finalizers:
  - baremetalhost.metal3.io
  generation: 2
  name: openshift-worker-0
  namespace: openshift-machine-api
  resourceVersion: "30099"
  uid: 1513ae9b-e092-409d-be1b-ad08edeb1271
spec:
  automatedCleaningMode: metadata
  bmc:
    address: redfish://10.46.61.19:443/redfish/v1/Systems/1
    credentialsName: openshift-worker-0-bmc-secret
    disableCertificateVerification: true
  bootMACAddress: 48:df:37:c7:f7:b0
  bootMode: UEFI
  consumerRef:
    apiVersion: machine.openshift.io/v1beta1
    kind: Machine
    name: ocp-edge-958fk-worker-0-nrfcg
    namespace: openshift-machine-api
  customDeploy:
    method: install_coreos
  hardwareProfile: unknown
  online: true
  rootDeviceHints:
    deviceName: /dev/disk/by-id/scsi-&lt;serial_number&gt;
  userData:
    name: worker-user-data-managed
    namespace: openshift-machine-api
status:
  errorCount: 0
  errorMessage: ""
  goodCredentials:
    credentials:
      name: openshift-worker-0-bmc-secret
      namespace: openshift-machine-api
    credentialsVersion: "16120"
  hardware:
    cpu:
      arch: x86_64
      clockMegahertz: 2300
      count: 64
      flags:
      - 3dnowprefetch
      - abm
      - acpi
      - adx
      - aes
      model: Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz
    firmware:
      bios:
        date: 10/26/2020
        vendor: HPE
        version: U30
    hostname: openshift-worker-0
    nics:
    - mac: 48:df:37:c7:f7:b3
      model: 0x8086 0x1572
      name: ens1f3
    ramMebibytes: 262144
    storage:
    - hctl: "0:0:0:0"
      model: VK000960GWTTB
      name: /dev/disk/by-id/scsi-&lt;serial_number&gt;
      sizeBytes: 960197124096
      type: SSD
      vendor: ATA
    systemVendor:
      manufacturer: HPE
      productName: ProLiant DL380 Gen10 (868703-B21)
      serialNumber: CZ200606M3
  hardwareProfile: unknown
  lastUpdated: "2022-06-16T11:41:42Z"
  operationalStatus: OK
  poweredOn: true
  provisioning:
    ID: 217baa14-cfcf-4196-b764-744e184a3413
    bootMode: UEFI
    customDeploy:
      method: install_coreos
    image:
      url: ""
    raid:
      hardwareRAIDVolumes: null
      softwareRAIDVolumes: []
    rootDeviceHints:
      deviceName: /dev/disk/by-id/scsi-&lt;serial_number&gt;
    state: provisioned
  triedCredentials:
    credentials:
      name: openshift-worker-0-bmc-secret
      namespace: openshift-machine-api
    credentialsVersion: "16120"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="about-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration">
<title>About the HostFirmwareSettings resource</title>
<simpara>You can use the <literal>HostFirmwareSettings</literal> resource to retrieve and manage the BIOS settings for a host. When a host moves to the <literal>Available</literal> state, Ironic reads the host&#8217;s BIOS settings and creates the <literal>HostFirmwareSettings</literal> resource. The resource contains the complete BIOS configuration returned from the baseboard management controller (BMC). Whereas, the <literal>firmware</literal> field in the <literal>BareMetalHost</literal> resource returns three vendor-independent fields, the <literal>HostFirmwareSettings</literal> resource typically comprises many BIOS settings of vendor-specific fields per host.</simpara>
<simpara>The <literal>HostFirmwareSettings</literal> resource contains two sections:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The <literal>HostFirmwareSettings</literal> spec.</simpara>
</listitem>
<listitem>
<simpara>The <literal>HostFirmwareSettings</literal> status.</simpara>
</listitem>
</orderedlist>
<section xml:id="_the_hostfirmwaresettings_spec">
<title>The <literal>HostFirmwareSettings</literal> spec</title>
<simpara>The <literal>spec</literal> section of the <literal>HostFirmwareSettings</literal> resource defines the desired state of the host&#8217;s BIOS, and it is empty by default. Ironic uses the settings in the <literal>spec.settings</literal> section to update the baseboard management controller (BMC) when the host is in the <literal>Preparing</literal> state. Use the <literal>FirmwareSchema</literal> resource to ensure that you do not send invalid name/value pairs to hosts. See "About the FirmwareSchema resource" for additional details.</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">spec:
  settings:
    ProcTurboMode: Disabled<co xml:id="CO4-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO4-1">
<para>In the foregoing example, the <literal>spec.settings</literal> section contains a name/value pair that will set the <literal>ProcTurboMode</literal> BIOS setting to <literal>Disabled</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>Integer parameters listed in the <literal>status</literal> section appear as strings. For example, <literal>"1"</literal>. When setting integers in the <literal>spec.settings</literal> section, the values should be set as integers without quotes. For example, <literal>1</literal>.</simpara>
</note>
</section>
<section xml:id="_the_hostfirmwaresettings_status">
<title>The <literal>HostFirmwareSettings</literal> status</title>
<simpara>The <literal>status</literal> represents the current state of the host&#8217;s BIOS.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>HostFirmwareSettings</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Parameters</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><screen>status:
  conditions:
  - lastTransitionTime:
    message:
    observedGeneration:
    reason:
    status:
    type:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>conditions</literal> field contains a list of state changes. The sub-fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>lastTransitionTime</literal>: The last time the state changed.</simpara>
</listitem>
<listitem>
<simpara><literal>message</literal>: A description of the state change.</simpara>
</listitem>
<listitem>
<simpara><literal>observedGeneration</literal>: The current generation of the <literal>status</literal>. If <literal>metadata.generation</literal> and this field are not the same, the <literal>status.conditions</literal> might be out of date.</simpara>
</listitem>
<listitem>
<simpara><literal>reason</literal>: The reason for the state change.</simpara>
</listitem>
<listitem>
<simpara><literal>status</literal>: The status of the state change. The status can be <literal>True</literal>, <literal>False</literal> or <literal>Unknown</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>type</literal>: The type of state change. The types are <literal>Valid</literal> and <literal>ChangeDetected</literal>.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>status:
  schema:
    name:
    namespace:
    lastUpdated:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>FirmwareSchema</literal> for the firmware settings. The fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>name</literal>: The name or unique identifier referencing the schema.</simpara>
</listitem>
<listitem>
<simpara><literal>namespace</literal>: The namespace where the schema is stored.</simpara>
</listitem>
<listitem>
<simpara><literal>lastUpdated</literal>: The last time the resource was updated.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><screen>status:
  settings:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>settings</literal> field contains a list of name/value pairs of a host&#8217;s current BIOS settings.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="getting-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration">
<title>Getting the HostFirmwareSettings resource</title>
<simpara>The <literal>HostFirmwareSettings</literal> resource contains the vendor-specific BIOS properties of a physical host. You must get the <literal>HostFirmwareSettings</literal> resource for a physical host to review its BIOS properties.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the detailed list of <literal>HostFirmwareSettings</literal> resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hfs -n openshift-machine-api -o yaml</programlisting>
<note>
<simpara>You can use <literal>hostfirmwaresettings</literal> as the long form of <literal>hfs</literal> with the <literal>oc get</literal> command.</simpara>
</note>
</listitem>
<listitem>
<simpara>Get the list of <literal>HostFirmwareSettings</literal> resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hfs -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Get the <literal>HostFirmwareSettings</literal> resource for a particular host</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hfs &lt;host_name&gt; -n openshift-machine-api -o yaml</programlisting>
<simpara>Where <literal>&lt;host_name&gt;</literal> is the name of the host.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="editing-the-hostfirmwaresettings-resource_post-install-bare-metal-configuration">
<title>Editing the HostFirmwareSettings resource</title>
<simpara>You can edit the <literal>HostFirmwareSettings</literal> of provisioned hosts.</simpara>
<important>
<simpara>You can only edit hosts when they are in the <literal>provisioned</literal> state, excluding read-only values. You cannot edit hosts in the <literal>externally provisioned</literal> state.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the list of <literal>HostFirmwareSettings</literal> resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hfs -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Edit a host&#8217;s <literal>HostFirmwareSettings</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hfs &lt;host_name&gt; -n openshift-machine-api</programlisting>
<simpara>Where <literal>&lt;host_name&gt;</literal> is the name of a provisioned host. The <literal>HostFirmwareSettings</literal> resource will open in the default editor for your terminal.</simpara>
</listitem>
<listitem>
<simpara>Add name/value pairs to the <literal>spec.settings</literal> section:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">spec:
  settings:
    name: value <co xml:id="CO5-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO5-1">
<para>Use the <literal>FirmwareSchema</literal> resource to identify the available settings for the host. You cannot set values that are read-only.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the changes and exit the editor.</simpara>
</listitem>
<listitem>
<simpara>Get the host&#8217;s machine name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ oc get bmh &lt;host_name&gt; -n openshift-machine name</programlisting>
<simpara>Where <literal>&lt;host_name&gt;</literal> is the name of the host. The machine name appears under the <literal>CONSUMER</literal> field.</simpara>
</listitem>
<listitem>
<simpara>Annotate the machine to delete it from the machineset:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate machine &lt;machine_name&gt; machine.openshift.io/delete-machine=true -n openshift-machine-api</programlisting>
<simpara>Where <literal>&lt;machine_name&gt;</literal> is the name of the machine to delete.</simpara>
</listitem>
<listitem>
<simpara>Get a list of nodes and count the number of worker nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>Get the machineset:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machinesets -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Scale the machineset:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale machineset &lt;machineset_name&gt; -n openshift-machine-api --replicas=&lt;n-1&gt;</programlisting>
<simpara>Where <literal>&lt;machineset_name&gt;</literal> is the name of the machineset and <literal>&lt;n-1&gt;</literal> is the decremented number of worker nodes.</simpara>
</listitem>
<listitem>
<simpara>When the host enters the <literal>Available</literal> state, scale up the machineset to make the <literal>HostFirmwareSettings</literal> resource changes take effect:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale machineset &lt;machineset_name&gt; -n openshift-machine-api --replicas=&lt;n&gt;</programlisting>
<simpara>Where <literal>&lt;machineset_name&gt;</literal> is the name of the machineset and <literal>&lt;n&gt;</literal> is the number of worker nodes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="verifying-the-hostfirmware-settings-resource-is-valid_post-install-bare-metal-configuration">
<title>Verifying the HostFirmware Settings resource is valid</title>
<simpara>When the user edits the <literal>spec.settings</literal> section to make a change to the <literal>HostFirmwareSetting</literal>(HFS) resource, the Bare Metal Operator (BMO) validates the change against the <literal>FimwareSchema</literal> resource, which is a read-only resource. If the setting is invalid, the BMO will set the <literal>Type</literal> value of the <literal>status.Condition</literal> setting to <literal>False</literal> and also generate an event and store it in the HFS resource. Use the following procedure to verify that the resource is valid.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get a list of <literal>HostFirmwareSetting</literal> resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hfs -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>HostFirmwareSettings</literal> resource for a particular host is valid:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe hfs &lt;host_name&gt; -n openshift-machine-api</programlisting>
<simpara>Where <literal>&lt;host_name&gt;</literal> is the name of the host.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Events:
  Type    Reason            Age    From                                    Message
  ----    ------            ----   ----                                    -------
  Normal  ValidationFailed  2m49s  metal3-hostfirmwaresettings-controller  Invalid BIOS setting: Setting ProcTurboMode is invalid, unknown enumeration value - Foo</programlisting>
</para>
</formalpara>
<important>
<simpara>If the response returns <literal>ValidationFailed</literal>, there is an error in the resource configuration and you must update the values to conform to the <literal>FirmwareSchema</literal> resource.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="about-the-firmwareschema-resource_post-install-bare-metal-configuration">
<title>About the FirmwareSchema resource</title>
<simpara>BIOS settings vary among hardware vendors and host models. A <literal>FirmwareSchema</literal> resource is a read-only resource that contains the types and limits for each BIOS setting on each host model. The data comes directly from the BMC through Ironic. The <literal>FirmwareSchema</literal> enables you to identify valid values you can specify in the <literal>spec</literal> field of the <literal>HostFirmwareSettings</literal> resource. The <literal>FirmwareSchema</literal> resource has a unique identifier derived from its settings and limits. Identical host models use the same <literal>FirmwareSchema</literal> identifier. It is likely that multiple instances of <literal>HostFirmwareSettings</literal> use the same <literal>FirmwareSchema</literal>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>FirmwareSchema specification</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Parameters</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><screen>&lt;BIOS_setting_name&gt;
  attribute_type:
  allowable_values:
  lower_bound:
  upper_bound:
  min_length:
  max_length:
  read_only:
  unique:</screen></entry>
<entry align="left" valign="top"><simpara>The <literal>spec</literal> is a simple map consisting of the BIOS setting name and the limits of the setting. The fields include:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>attribute_type</literal>: The type of setting. The supported types are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Enumeration</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Integer</literal></simpara>
</listitem>
<listitem>
<simpara><literal>String</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Boolean</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><literal>allowable_values</literal>: A list of allowable values when the <literal>attribute_type</literal> is <literal>Enumeration</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>lower_bound</literal>: The lowest allowed value when <literal>attribute_type</literal> is <literal>Integer</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>upper_bound</literal>: The highest allowed value when <literal>attribute_type</literal> is <literal>Integer</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>min_length</literal>: The shortest string length that the value can have when <literal>attribute_type</literal> is <literal>String</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>max_length</literal>: The longest string length that the value can have when <literal>attribute_type</literal> is <literal>String</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>read_only</literal>: The setting is read only and cannot be modified.</simpara>
</listitem>
<listitem>
<simpara><literal>unique</literal>: The setting is specific to this host.</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="getting-the-firmwareschema-resource_post-install-bare-metal-configuration">
<title>Getting the FirmwareSchema resource</title>
<simpara>Each host model from each vendor has different BIOS settings. When editing the <literal>HostFirmwareSettings</literal> resource&#8217;s <literal>spec</literal> section, the name/value pairs you set must conform to that host&#8217;s firmware schema. To ensure you are setting valid name/value pairs, get the <literal>FirmwareSchema</literal> for the host and review it.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To get a list of <literal>FirmwareSchema</literal> resource instances, execute the following:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get firmwareschema -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>To get a particular <literal>FirmwareSchema</literal> instance, execute:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get firmwareschema &lt;instance_name&gt; -n openshift-machine-api -o yaml</programlisting>
<simpara>Where <literal>&lt;instance_name&gt;</literal> is the name of the schema instance stated in the <literal>HostFirmwareSettings</literal> resource (see Table 3).</simpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="_configuring_multi_architecture_compute_machines_on_an_openshift_cluster">
<title>Configuring multi-architecture compute machines on an OpenShift cluster</title>
<section xml:id="post-install-multi-architecture-configuration">
<title>About clusters with multi-architecture compute machines</title>

<simpara>An OpenShift Container Platform cluster with multi-architecture compute machines is a cluster that supports compute machines with different architectures. Clusters with multi-architecture compute machines are available only on Amazon Web Services (AWS) or Microsoft Azure installer-provisioned infrastructures and bare metal, IBM Power&#174;, and IBM Z&#174; user-provisioned infrastructures with x86_64 control plane machines.</simpara>
<note>
<simpara>When there are nodes with multiple architectures in your cluster, the architecture of your image must be consistent with the architecture of the node. You need to ensure that the pod is assigned to the node with the appropriate architecture and that it matches the image architecture. For more information on assigning pods to nodes, see <link xlink:href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/">Assigning pods to nodes</link>.</simpara>
</note>
<important>
<simpara>The Cluster Samples Operator is not supported on clusters with multi-architecture compute machines. Your cluster can be created without this capability. For more information, see <link xlink:href="../../post_installation_configuration/enabling-cluster-capabilities.xml#enabling-cluster-capabilities">Enabling cluster capabilities</link></simpara>
</important>
<simpara>For information on migrating your single-architecture cluster to a cluster that supports multi-architecture compute machines, see <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
<section xml:id="_configuring_your_cluster_with_multi_architecture_compute_machines">
<title>Configuring your cluster with multi-architecture compute machines</title>
<simpara>To create a cluster with multi-architecture compute machines for various platforms, you can use the documentation in the following sections:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-azure.xml#creating-multi-arch-compute-nodes-azure">Creating a cluster with multi-architecture compute machines on Azure</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-aws.xml#creating-multi-arch-compute-nodes-aws">Creating a cluster with multi-architecture compute machines on AWS</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-gcp.xml#creating-multi-arch-compute-nodes-gcp">Creating a cluster with multi-architecture compute machines on GCP</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-bare-metal.xml#creating-multi-arch-compute-nodes-bare-metal">Creating a cluster with multi-architecture compute machines on bare metal</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-ibm-z.xml#creating-multi-arch-compute-nodes-ibm-z">Creating a cluster with multi-architecture compute machines on IBM Z&#174; and IBM&#174; LinuxONE with z/VM</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-ibm-z-kvm.xml#creating-multi-arch-compute-nodes-ibm-z-kvm">Creating a cluster with multi-architecture compute machines on IBM Z&#174; and IBM&#174; LinuxONE with RHEL KVM</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../post_installation_configuration/configuring-multi-arch-compute-machines/creating-multi-arch-compute-nodes-ibm-power.xml#creating-multi-arch-compute-nodes-ibm-power">Creating a cluster with multi-architecture compute machines on IBM Power&#174;</link></simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Autoscaling from zero is currently not supported on Google Cloud Platform (GCP).</simpara>
</important>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-azure">
<title>Creating a cluster with multi-architecture compute machine on Azure</title>

<simpara>To deploy an Azure cluster with multi-architecture compute machines, you must first create a single-architecture Azure installer-provisioned cluster that uses the multi-architecture installer binary. For more information on Azure installations, see <link xlink:href="../../installing/installing_azure/installing-azure-customizations.xml#installing-azure-customizations">Installing a cluster on Azure with customizations</link>. You can then add an ARM64 compute machine set to your cluster to create a cluster with multi-architecture compute machines.</simpara>
<simpara>The following procedures explain how to generate an ARM64 boot image and create an Azure compute machine set that uses the ARM64 boot image. This adds ARM64 compute nodes to your cluster and deploys the amount of ARM64 virtual machines (VM) that you need.</simpara>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-azure">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="multi-architecture-creating-arm64-bootimage_creating-multi-arch-compute-nodes-azure">
<title>Creating an ARM64 boot image using the Azure image gallery</title>
<simpara>The following procedure describes how to manually generate an ARM64 boot image.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the Azure CLI (<literal>az</literal>).</simpara>
</listitem>
<listitem>
<simpara>You created a single-architecture Azure installer-provisioned cluster with the multi-architecture installer binary.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to your Azure account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az login</programlisting>
</listitem>
<listitem>
<simpara>Create a storage account and upload the <literal>arm64</literal> virtual hard disk (VHD) to your storage account. The OpenShift Container Platform installation program creates a resource group, however, the boot image can also be uploaded to a custom named resource group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage account create -n ${STORAGE_ACCOUNT_NAME} -g ${RESOURCE_GROUP} -l westus --sku Standard_LRS <co xml:id="CO6-1"/></programlisting>
<calloutlist>
<callout arearefs="CO6-1">
<para>The <literal>westus</literal> object is an example region.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a storage container using the storage account you generated:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage container create -n ${CONTAINER_NAME} --account-name ${STORAGE_ACCOUNT_NAME}</programlisting>
</listitem>
<listitem>
<simpara>You must use the OpenShift Container Platform installation program JSON file to extract the URL and <literal>aarch64</literal> VHD name:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Extract the <literal>URL</literal> field and set it to <literal>RHCOS_VHD_ORIGIN_URL</literal> as the file name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.aarch64."rhel-coreos-extensions"."azure-disk".url')</programlisting>
</listitem>
<listitem>
<simpara>Extract the <literal>aarch64</literal> VHD name and set it to <literal>BLOB_NAME</literal> as the file name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BLOB_NAME=rhcos-$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.aarch64."rhel-coreos-extensions"."azure-disk".release')-azure.aarch64.vhd</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Generate a shared access signature (SAS) token. Use this token to upload the RHCOS VHD to your storage container with the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ end=`date -u -d "30 minutes" '+%Y-%m-%dT%H:%MZ'`</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ sas=`az storage container generate-sas -n ${CONTAINER_NAME} --account-name ${STORAGE_ACCOUNT_NAME} --https-only --permissions dlrw --expiry $end -o tsv`</programlisting>
</listitem>
<listitem>
<simpara>Copy the RHCOS VHD into the storage container:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage blob copy start --account-name ${STORAGE_ACCOUNT_NAME} --sas-token "$sas" \
 --source-uri "${RHCOS_VHD_ORIGIN_URL}" \
 --destination-blob "${BLOB_NAME}" --destination-container ${CONTAINER_NAME}</programlisting>
<simpara>You can check the status of the copying process with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage blob show -c ${CONTAINER_NAME} -n ${BLOB_NAME} --account-name ${STORAGE_ACCOUNT_NAME} | jq .properties.copy</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{
 "completionTime": null,
 "destinationSnapshot": null,
 "id": "1fd97630-03ca-489a-8c4e-cfe839c9627d",
 "incrementalCopy": null,
 "progress": "17179869696/17179869696",
 "source": "https://rhcos.blob.core.windows.net/imagebucket/rhcos-411.86.202207130959-0-azure.aarch64.vhd",
 "status": "success", <co xml:id="CO7-1"/>
 "statusDescription": null
}</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO7-1">
<para>If the status parameter displays the <literal>success</literal> object, the copying process is complete.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an image gallery using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az sig create --resource-group ${RESOURCE_GROUP} --gallery-name ${GALLERY_NAME}</programlisting>
<simpara>Use the image gallery to create an image definition. In the following example command, <literal>rhcos-arm64</literal> is the name of the image definition.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az sig image-definition create --resource-group ${RESOURCE_GROUP} --gallery-name ${GALLERY_NAME} --gallery-image-definition rhcos-arm64 --publisher RedHat --offer arm --sku arm64 --os-type linux --architecture Arm64 --hyper-v-generation V2</programlisting>
</listitem>
<listitem>
<simpara>To get the URL of the VHD and set it to <literal>RHCOS_VHD_URL</literal> as the file name, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ RHCOS_VHD_URL=$(az storage blob url --account-name ${STORAGE_ACCOUNT_NAME} -c ${CONTAINER_NAME} -n "${BLOB_NAME}" -o tsv)</programlisting>
</listitem>
<listitem>
<simpara>Use the <literal>RHCOS_VHD_URL</literal> file, your storage account, resource group, and image gallery to create an image version. In the following example, <literal>1.0.0</literal> is the image version.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az sig image-version create --resource-group ${RESOURCE_GROUP} --gallery-name ${GALLERY_NAME} --gallery-image-definition rhcos-arm64 --gallery-image-version 1.0.0 --os-vhd-storage-account ${STORAGE_ACCOUNT_NAME} --os-vhd-uri ${RHCOS_VHD_URL}</programlisting>
</listitem>
<listitem>
<simpara>Your <literal>arm64</literal> boot image is now generated. You can access the ID of your image with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az sig image-version show -r $GALLERY_NAME -g $RESOURCE_GROUP -i rhcos-arm64 -e 1.0.0</programlisting>
<simpara>The following example image ID is used in the <literal>recourseID</literal> parameter of the compute machine set:</simpara>
<formalpara>
<title>Example <literal>resourceID</literal></title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">/resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Compute/galleries/${GALLERY_NAME}/images/rhcos-arm64/versions/1.0.0</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="multi-architecture-modify-machine-set_creating-multi-arch-compute-nodes-azure">
<title>Adding a multi-architecture compute machine set to your cluster</title>
<simpara>To add ARM64 compute nodes to your cluster, you must create an Azure compute machine set that uses the ARM64 boot image. To create your own custom compute machine set on Azure, see "Creating a compute machine set on Azure".</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a compute machine set and modify the <literal>resourceID</literal> and <literal>vmSize</literal> parameters with the following command. This compute machine set will control the <literal>arm64</literal> worker nodes in your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f arm64-machine-set-0.yaml</programlisting>
<formalpara>
<title>Sample YAML compute machine set with <literal>arm64</literal> boot image</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
    machine.openshift.io/cluster-api-machine-role: worker
    machine.openshift.io/cluster-api-machine-type: worker
  name: &lt;infrastructure_id&gt;-arm64-machine-set-0
  namespace: openshift-machine-api
spec:
  replicas: 2
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-arm64-machine-set-0
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-arm64-machine-set-0
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          acceleratedNetworking: true
          apiVersion: machine.openshift.io/v1beta1
          credentialsSecret:
            name: azure-cloud-credentials
            namespace: openshift-machine-api
          image:
            offer: ""
            publisher: ""
            resourceID: /resourceGroups/${RESOURCE_GROUP}/providers/Microsoft.Compute/galleries/${GALLERY_NAME}/images/rhcos-arm64/versions/1.0.0 <co xml:id="CO8-1"/>
            sku: ""
            version: ""
          kind: AzureMachineProviderSpec
          location: &lt;region&gt;
          managedIdentity: &lt;infrastructure_id&gt;-identity
          networkResourceGroup: &lt;infrastructure_id&gt;-rg
          osDisk:
            diskSettings: {}
            diskSizeGB: 128
            managedDisk:
              storageAccountType: Premium_LRS
            osType: Linux
          publicIP: false
          publicLoadBalancer: &lt;infrastructure_id&gt;
          resourceGroup: &lt;infrastructure_id&gt;-rg
          subnet: &lt;infrastructure_id&gt;-worker-subnet
          userDataSecret:
            name: worker-user-data
          vmSize: Standard_D4ps_v5 <co xml:id="CO8-2"/>
          vnet: &lt;infrastructure_id&gt;-vnet
          zone: "&lt;zone&gt;"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO8-1">
<para>Set the <literal>resourceID</literal> parameter to the <literal>arm64</literal> boot image.</para>
</callout>
<callout arearefs="CO8-2">
<para>Set the <literal>vmSize</literal> parameter to the instance type used in your installation. Some example instance types are <literal>Standard_D4ps_v5</literal> or <literal>D8ps</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the new ARM64 machines are running by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                DESIRED  CURRENT  READY  AVAILABLE  AGE
&lt;infrastructure_id&gt;-arm64-machine-set-0                   2        2      2          2  10m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>You can check that the nodes are ready and scheduable with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../machine_management/creating_machinesets/creating-machineset-azure.xml#creating-machineset-azure">Creating a compute machine set on Azure</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-aws">
<title>Creating a cluster with multi-architecture compute machines on AWS</title>

<simpara>To create an AWS cluster with multi-architecture compute machines, you must first create a single-architecture AWS installer-provisioned cluster with the multi-architecture installer binary. For more information on AWS installations, refer to <link xlink:href="../../installing/installing_aws/installing-aws-customizations.xml">Installing a cluster on AWS with customizations</link>. You can then add a ARM64 compute machine set to your AWS cluster.</simpara>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-aws">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="multi-architecture-modify-machine-set-aws_creating-multi-arch-compute-nodes-aws">
<title>Adding an ARM64 compute machine set to your cluster</title>
<simpara>To configure a cluster with multi-architecture compute machines, you must create a AWS ARM64 compute machine set. This adds ARM64 compute nodes to your cluster so that your cluster has multi-architecture compute machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You used the installation program to create an AMD64 single-architecture AWS cluster with the multi-architecture installer binary.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create and modify a compute machine set, this will control the ARM64 compute nodes in your cluster.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f aws-arm64-machine-set-0.yaml</programlisting>
<formalpara>
<title>Sample YAML compute machine set to deploy an ARM64 compute node</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO9-1"/>
  name: &lt;infrastructure_id&gt;-aws-arm64-machine-set-0 <co xml:id="CO9-2"/>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO9-3"/>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <co xml:id="CO9-4"/>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <co xml:id="CO9-5"/>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <co xml:id="CO9-6"/>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt; <co xml:id="CO9-7"/>
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          ami:
            id: ami-02a574449d4f4d280 <co xml:id="CO9-8"/>
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: &lt;infrastructure_id&gt;-worker-profile <co xml:id="CO9-9"/>
          instanceType: m6g.xlarge <co xml:id="CO9-10"/>
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: us-east-1a <co xml:id="CO9-11"/>
            region: &lt;region&gt; <co xml:id="CO9-12"/>
          securityGroups:
            - filters:
                - name: tag:Name
                  values:
                    - &lt;infrastructure_id&gt;-worker-sg <co xml:id="CO9-13"/>
          subnet:
            filters:
              - name: tag:Name
                values:
                  - &lt;infrastructure_id&gt;-private-&lt;zone&gt;
          tags:
            - name: kubernetes.io/cluster/&lt;infrastructure_id&gt; <co xml:id="CO9-14"/>
              value: owned
            - name: &lt;custom_tag_name&gt;
              value: &lt;custom_tag_value&gt;
          userDataSecret:
            name: worker-user-data</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO9-1 CO9-2 CO9-3 CO9-9 CO9-13 CO9-14">
<para>Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:</para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -o jsonpath=‘{.status.infrastructureName}{“\n”}’ infrastructure cluster</programlisting>
</callout>
<callout arearefs="CO9-4 CO9-7">
<para>Specify the infrastructure ID, role node label, and zone.</para>
</callout>
<callout arearefs="CO9-5 CO9-6">
<para>Specify the role node label to add.</para>
</callout>
<callout arearefs="CO9-8">
<para>Specify an ARM64 supported Red Hat Enterprise Linux CoreOS (RHCOS) Amazon Machine Image (AMI) for your AWS zone for your OpenShift Container Platform nodes.</para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap/coreos-bootimages \
	  -n openshift-machine-config-operator \
	  -o jsonpath='{.data.stream}' | jq \
	  -r '.architectures.&lt;arch&gt;.images.aws.regions."&lt;region&gt;".image'</programlisting>
</callout>
<callout arearefs="CO9-10">
<para>Specify an ARM64 supported machine type. For more information, refer to "Tested instance types for AWS 64-bit ARM"</para>
</callout>
<callout arearefs="CO9-11">
<para>Specify the zone, for example <literal>us-east-1a</literal>. Ensure that the zone you select offers 64-bit ARM machines.</para>
</callout>
<callout arearefs="CO9-12">
<para>Specify the region, for example, <literal>us-east-1</literal>. Ensure that the zone you select offers 64-bit ARM machines.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>View the list of compute machine sets by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset -n openshift-machine-api</programlisting>
<simpara>You can then see your created ARM64 machine set.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                DESIRED  CURRENT  READY  AVAILABLE  AGE
&lt;infrastructure_id&gt;-aws-arm64-machine-set-0                   2        2      2          2  10m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>You can check that the nodes are ready and scheduable with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../installing/installing_aws/installing-aws-customizations.xml#installation-aws-arm-tested-machine-types_installing-aws-customizations">Tested instance types for AWS 64-bit ARM</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-gcp">
<title>Creating a cluster with multi-architecture compute machines on GCP</title>

<simpara>To create a Google Cloud Platform (GCP) cluster with multi-architecture compute machines, you must first create a single-architecture GCP installer-provisioned cluster with the multi-architecture installer binary. For more information on AWS installations, refer to <link xlink:href="../../installing/installing_gcp/installing-gcp-customizations.xml">Installing a cluster on GCP with customizations</link>. You can then add ARM64 compute machines sets to your GCP cluster.</simpara>
<note>
<simpara>Secure booting is currently not supported on ARM64 machines for GCP</simpara>
</note>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-gcp">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="multi-architecture-modify-machine-set-gcp_creating-multi-arch-compute-nodes-gcp">
<title>Adding an ARM64 compute machine set to your GCP cluster</title>
<simpara>To configure a cluster with multi-architecture compute machines, you must create a GCP ARM64 compute machine set. This adds ARM64 compute nodes to your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You used the installation program to create an AMD64 single-architecture AWS cluster with the multi-architecture installer binary.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create and modify a compute machine set, this controls the ARM64 compute nodes in your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f gcp-arm64-machine-set-0.yaml</programlisting>
<formalpara>
<title>Sample GCP YAML compute machine set to deploy an ARM64 compute node</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO10-1"/>
  name: &lt;infrastructure_id&gt;-w-a
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-w-a
  template:
    metadata:
      creationTimestamp: null
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <co xml:id="CO10-2"/>
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-w-a
    spec:
      metadata:
        labels:
          node-role.kubernetes.io/&lt;role&gt;: ""
      providerSpec:
        value:
          apiVersion: gcpprovider.openshift.io/v1beta1
          canIPForward: false
          credentialsSecret:
            name: gcp-cloud-credentials
          deletionProtection: false
          disks:
          - autoDelete: true
            boot: true
            image: &lt;path_to_image&gt; <co xml:id="CO10-3"/>
            labels: null
            sizeGb: 128
            type: pd-ssd
          gcpMetadata: <co xml:id="CO10-4"/>
          - key: &lt;custom_metadata_key&gt;
            value: &lt;custom_metadata_value&gt;
          kind: GCPMachineProviderSpec
          machineType: n1-standard-4 <co xml:id="CO10-5"/>
          metadata:
            creationTimestamp: null
          networkInterfaces:
          - network: &lt;infrastructure_id&gt;-network
            subnetwork: &lt;infrastructure_id&gt;-worker-subnet
          projectID: &lt;project_name&gt; <co xml:id="CO10-6"/>
          region: us-central1 <co xml:id="CO10-7"/>
          serviceAccounts:
          - email: &lt;infrastructure_id&gt;-w@&lt;project_name&gt;.iam.gserviceaccount.com
            scopes:
            - https://www.googleapis.com/auth/cloud-platform
          tags:
            - &lt;infrastructure_id&gt;-worker
          userDataSecret:
            name: worker-user-data
          zone: us-central1-a</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO10-1">
<para>Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. You can obtain the infrastructure ID by running the following command:</para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</programlisting>
</callout>
<callout arearefs="CO10-2">
<para>Specify the role node label to add.</para>
</callout>
<callout arearefs="CO10-3">
<para>Specify the path to the image that is used in current compute machine sets. You need the project and image name for your path to image.</para>
<simpara>To access the project and image name, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap/coreos-bootimages \
  -n openshift-machine-config-operator \
  -o jsonpath='{.data.stream}' | jq \
  -r '.architectures.aarch64.images.gcp'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">  "gcp": {
    "release": "415.92.202309142014-0",
    "project": "rhcos-cloud",
    "name": "rhcos-415-92-202309142014-0-gcp-aarch64"
  }</programlisting>
</para>
</formalpara>
<simpara>Use the <literal>project</literal> and <literal>name</literal> parameters from the output to create the path to image field in your machine set. The path to the image should follow the following format:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ projects/&lt;project&gt;/global/images/&lt;image_name&gt;</programlisting>
</callout>
<callout arearefs="CO10-4">
<para>Optional: Specify custom metadata in the form of a <literal>key:value</literal> pair. For example use cases, see the GCP documentation for <link xlink:href="https://cloud.google.com/compute/docs/metadata/setting-custom-metadata">setting custom metadata</link>.</para>
</callout>
<callout arearefs="CO10-5">
<para>Specify an ARM64 supported machine type. For more information, refer to <emphasis>Tested instance types for GCP on 64-bit ARM infrastructures</emphasis> in "Additional resources".</para>
</callout>
<callout arearefs="CO10-6">
<para>Specify the name of the GCP project that you use for your cluster.</para>
</callout>
<callout arearefs="CO10-7">
<para>Specify the region, for example, <literal>us-central1</literal>. Ensure that the zone you select offers 64-bit ARM machines.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>View the list of compute machine sets by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset -n openshift-machine-api</programlisting>
<simpara>You can then see your created ARM64 machine set.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                DESIRED  CURRENT  READY  AVAILABLE  AGE
&lt;infrastructure_id&gt;-gcp-arm64-machine-set-0                   2        2      2          2  10m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>You can check that the nodes are ready and scheduable with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../installing/installing_gcp/installing-gcp-customizations.xml#installation-gcp-tested-machine-types-arm_installing-gcp-customizations">Tested instance types for GCP on 64-bit ARM infrastructures</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-bare-metal">
<title>Creating a cluster with multi-architecture compute machine on bare metal</title>

<simpara>To create a cluster with multi-architecture compute machines on bare metal, you must have an existing single-architecture bare metal cluster. For more information on bare metal installations, see <link xlink:href="../../installing/installing_bare_metal/installing-bare-metal.xml#installing-bare-metal">Installing a user provisioned cluster on bare metal</link>. You can then add 64-bit ARM compute machines to your OpenShift Container Platform cluster on bare metal.</simpara>
<simpara>Before you can add 64-bit ARM nodes to your bare metal cluster, you must upgrade your cluster to one that uses the multi-architecture payload. For more information on migrating to the multi-architecture payload, see <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
<simpara>The following procedures explain how to create a RHCOS compute machine using an ISO image or network PXE booting. This will allow you to add ARM64 nodes to your bare metal cluster and deploy a cluster with multi-architecture compute machines.</simpara>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-bare-metal">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-iso_creating-multi-arch-compute-nodes-bare-metal">
<title>Creating RHCOS machines using an ISO image</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using an ISO image to create the machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>)  installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Extract the Ignition config file from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Upload the <literal>worker.ign</literal> Ignition config file you exported from your cluster to your HTTP server. Note the URLs of these files.</simpara>
</listitem>
<listitem>
<simpara>You can validate that the ignition files are available on the URLs. The following example gets the Ignition config files for the compute node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</programlisting>
</listitem>
<listitem>
<simpara>You can access the ISO image for booting your new machine by running to following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.&lt;architecture&gt;.artifacts.metal.formats.iso.disk.location')</programlisting>
</listitem>
<listitem>
<simpara>Use the ISO file to install RHCOS on more compute machines. Use the same method that you used when you created machines before you installed the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Burn the ISO image to a disk and boot it directly.</simpara>
</listitem>
<listitem>
<simpara>Use ISO redirection with a LOM interface.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Boot the RHCOS ISO image without specifying any options, or interrupting the live boot sequence. Wait for the installer to boot into a shell prompt in the RHCOS live environment.</simpara>
<note>
<simpara>You can interrupt the RHCOS installation boot process to add kernel arguments. However, for this ISO procedure you must use the <literal>coreos-installer</literal> command as outlined in the following steps, instead of adding kernel arguments.</simpara>
</note>
</listitem>
<listitem>
<simpara>Run the <literal>coreos-installer</literal> command and specify the options that meet your installation requirements. At a minimum, you must specify the URL that points to the Ignition config file for the node type, and the device that you are installing to:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo coreos-installer install --ignition-url=http://&lt;HTTP_server&gt;/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=sha512-&lt;digest&gt; <co xml:id="CO11-1"/><co xml:id="CO11-2"/></programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>You must run the <literal>coreos-installer</literal> command by using <literal>sudo</literal>, because the <literal>core</literal> user does not have the required root privileges to perform the installation.</para>
</callout>
<callout arearefs="CO11-2">
<para>The <literal>--ignition-hash</literal> option is required when the Ignition config file is obtained through an HTTP URL to validate the authenticity of the Ignition config file on the cluster node. <literal>&lt;digest&gt;</literal> is the Ignition config file SHA512 digest obtained in a preceding step.</para>
</callout>
</calloutlist>
<note>
<simpara>If you want to provide your Ignition config files through an HTTPS server that uses TLS, you can add the internal certificate authority (CA) to the system trust store before running <literal>coreos-installer</literal>.</simpara>
</note>
<simpara>The following example initializes a bootstrap node installation to the <literal>/dev/sda</literal> device. The Ignition config file for the bootstrap node is obtained from an HTTP web server with the IP address 192.168.1.2:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo coreos-installer install --ignition-url=http://192.168.1.2:80/installation_directory/bootstrap.ign /dev/sda --ignition-hash=sha512-a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</programlisting>
</listitem>
<listitem>
<simpara>Monitor the progress of the RHCOS installation on the console of the machine.</simpara>
<important>
<simpara>Ensure that the installation is successful on each node before commencing with the OpenShift Container Platform installation. Observing the installation process can also help to determine the cause of RHCOS installation issues that might arise.</simpara>
</important>
</listitem>
<listitem>
<simpara>Continue to create more compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-pxe_creating-multi-arch-compute-nodes-bare-metal">
<title>Creating RHCOS machines by PXE or iPXE booting</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using PXE or iPXE booting.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.</simpara>
</listitem>
<listitem>
<simpara>Obtain the URLs of the RHCOS ISO image, compressed metal BIOS, <literal>kernel</literal>, and <literal>initramfs</literal> files that you uploaded to your HTTP server during cluster installation.</simpara>
</listitem>
<listitem>
<simpara>You have access to the PXE booting infrastructure that you used to create the machines for your OpenShift Container Platform cluster during installation. The machines must boot from their local disks after RHCOS is installed on them.</simpara>
</listitem>
<listitem>
<simpara>If you use UEFI, you have access to the <literal>grub.conf</literal> file that you modified during OpenShift Container Platform installation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that your PXE or iPXE installation for the RHCOS images is correct.</simpara>
<itemizedlist>
<listitem>
<simpara>For PXE:</simpara>
<screen>DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
    KERNEL http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; <co xml:id="CO12-1"/>
    APPEND initrd=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img <co xml:id="CO12-2"/></screen>
<calloutlist>
<callout arearefs="CO12-1">
<para>Specify the location of the live <literal>kernel</literal> file that you uploaded to your HTTP server.</para>
</callout>
<callout arearefs="CO12-2">
<para>Specify locations of the RHCOS files that you uploaded to your HTTP server. The <literal>initrd</literal> parameter value is the location of the live <literal>initramfs</literal> file, the <literal>coreos.inst.ignition_url</literal> parameter value is the location of the worker Ignition config file, and the <literal>coreos.live.rootfs_url</literal> parameter value is the location of the live <literal>rootfs</literal> file. The <literal>coreos.inst.ignition_url</literal> and <literal>coreos.live.rootfs_url</literal> parameters only support HTTP and HTTPS.</para>
</callout>
</calloutlist>
<note>
<simpara>This configuration does not enable serial console access on machines with a graphical console. To configure a different console, add one or more <literal>console=</literal> arguments to the <literal>APPEND</literal> line. For example, add <literal>console=tty0 console=ttyS0</literal> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <link xlink:href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>For iPXE (<literal>x86_64</literal> + <literal>aarch64</literal>):</simpara>
<screen>kernel http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; initrd=main coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <co xml:id="CO13-1"/> <co xml:id="CO13-2"/>
initrd --name main http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <co xml:id="CO13-3"/>
boot</screen>
<calloutlist>
<callout arearefs="CO13-1">
<para>Specify the locations of the RHCOS files that you uploaded to your
HTTP server. The <literal>kernel</literal> parameter value is the location of the <literal>kernel</literal> file,
the <literal>initrd=main</literal> argument is needed for booting on UEFI systems,
the <literal>coreos.live.rootfs_url</literal> parameter value is the location of the <literal>rootfs</literal> file,
and the <literal>coreos.inst.ignition_url</literal> parameter value is the
location of the worker Ignition config file.</para>
</callout>
<callout arearefs="CO13-2">
<para>If you use multiple NICs, specify a single interface in the <literal>ip</literal> option.
For example, to use DHCP on a NIC that is named <literal>eno1</literal>, set <literal>ip=eno1:dhcp</literal>.</para>
</callout>
<callout arearefs="CO13-3">
<para>Specify the location of the <literal>initramfs</literal> file that you uploaded to your HTTP server.</para>
</callout>
</calloutlist>
<note>
<simpara>This configuration does not enable serial console access on machines with a graphical console To configure a different console, add one or more <literal>console=</literal> arguments to the <literal>kernel</literal> line. For example, add <literal>console=tty0 console=ttyS0</literal> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <link xlink:href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</link> and "Enabling the serial console for PXE and ISO installation" in the "Advanced RHCOS installation configuration" section.</simpara>
</note>
<note>
<simpara>To network boot the CoreOS <literal>kernel</literal> on <literal>aarch64</literal> architecture, you need to use a version of iPXE build with the <literal>IMAGE_GZIP</literal> option enabled. See <link xlink:href="https://ipxe.org/buildcfg/image_gzip"><literal>IMAGE_GZIP</literal> option in iPXE</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>For PXE (with UEFI and GRUB as second stage) on <literal>aarch64</literal>:</simpara>
<screen>menuentry 'Install CoreOS' {
    linux rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt;  coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <co xml:id="CO14-1"/> <co xml:id="CO14-2"/>
    initrd rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <co xml:id="CO14-3"/>
}</screen>
<calloutlist>
<callout arearefs="CO14-1">
<para>Specify the locations of the RHCOS files that you uploaded to your
HTTP/TFTP server. The <literal>kernel</literal> parameter value is the location of the <literal>kernel</literal> file on your TFTP server.
The <literal>coreos.live.rootfs_url</literal> parameter value is the location of the <literal>rootfs</literal> file, and the <literal>coreos.inst.ignition_url</literal> parameter value is the location of the worker Ignition config file on your HTTP Server.</para>
</callout>
<callout arearefs="CO14-2">
<para>If you use multiple NICs, specify a single interface in the <literal>ip</literal> option.
For example, to use DHCP on a NIC that is named <literal>eno1</literal>, set <literal>ip=eno1:dhcp</literal>.</para>
</callout>
<callout arearefs="CO14-3">
<para>Specify the location of the <literal>initramfs</literal> file that you uploaded to your TFTP server.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Use the PXE or iPXE infrastructure to create the required compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-approve-csrs_creating-multi-arch-compute-nodes-bare-metal">
<title>Approving the certificate signing requests for your machines</title>
<simpara>When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You added machines to your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that the cluster recognizes the machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.28.5
master-1  Ready     master  63m  v1.28.5
master-2  Ready     master  64m  v1.28.5</programlisting>
</para>
</formalpara>
<simpara>The output lists all of the machines that you created.</simpara>
<note>
<simpara>The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the pending CSRs and ensure that you see the client requests with the <literal>Pending</literal> or <literal>Approved</literal> status for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</programlisting>
</para>
</formalpara>
<simpara>In this example, two machines are joining the cluster. You might see more approved CSRs in the list.</simpara>
</listitem>
<listitem>
<simpara>If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<note>
<simpara>Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <literal>machine-approver</literal> if the Kubelet requests a new certificate with identical parameters.</simpara>
</note>
<note>
<simpara>For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <literal>oc exec</literal>, <literal>oc rsh</literal>, and <literal>oc logs</literal> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <literal>node-bootstrapper</literal> service account in the <literal>system:node</literal> or <literal>system:admin</literal> groups, and confirm the identity of the node.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO15-1"/></programlisting>
<calloutlist>
<callout arearefs="CO15-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</programlisting>
<note>
<simpara>Some Operators might not become available until some CSRs are approved.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If the remaining CSRs are not approved, and are in the <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO16-1"/></programlisting>
<calloutlist>
<callout arearefs="CO16-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After all client and server CSRs have been approved, the machines have the <literal>Ready</literal> status. Verify this by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.28.5
master-1  Ready     master  73m  v1.28.5
master-2  Ready     master  74m  v1.28.5
worker-0  Ready     worker  11m  v1.28.5
worker-1  Ready     worker  11m  v1.28.5</programlisting>
</para>
</formalpara>
<note>
<simpara>It can take a few minutes after approval of the server CSRs for the machines to transition to the <literal>Ready</literal> status.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional information</title>
<listitem>
<simpara>For more information on CSRs, see <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-ibm-z">
<title>Creating a cluster with multi-architecture compute machines on IBM Z and IBM LinuxONE with z/VM</title>

<simpara>To create a cluster with multi-architecture compute machines on IBM Z&#174; and IBM&#174; LinuxONE (<literal>s390x</literal>) with z/VM, you must have an existing single-architecture <literal>x86_64</literal> cluster. You can then add <literal>s390x</literal> compute machines to your OpenShift Container Platform cluster.</simpara>
<simpara>Before you can add <literal>s390x</literal> nodes to your cluster, you must upgrade your cluster to one that uses the multi-architecture payload. For more information on migrating to the multi-architecture payload, see <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
<simpara>The following procedures explain how to create a RHCOS compute machine using a z/VM instance. This will allow you to add <literal>s390x</literal> nodes to your cluster and deploy a cluster with multi-architecture compute machines.</simpara>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-ibm-z">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-ibm-z_creating-multi-arch-compute-nodes-ibm-z">
<title>Creating RHCOS machines on IBM Z with z/VM</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines running on IBM Z&#174; with z/VM and attach them to your existing cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have a domain name server (DNS) that can perform hostname and reverse lookup for the nodes.</simpara>
</listitem>
<listitem>
<simpara>You have an HTTP or HTTPS server running on your provisioning machine that is accessible to the machines you create.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Disable UDP aggregation.</simpara>
<simpara>Currently, UDP aggregation is not supported on IBM Z&#174; and is not automatically deactivated on multi-architecture compute clusters with an <literal>x86_64</literal> control plane and additional <literal>s390x</literal> compute machines. To ensure that the addtional compute nodes are added to the cluster correctly, you must manually disable UDP aggregation.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a YAML file <literal>udp-aggregation-config.yaml</literal> with the following content:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
data:
  disable-udp-aggregation: "true"
metadata:
  name: udp-aggregation-config
  namespace: openshift-network-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the ConfigMap resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f udp-aggregation-config.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Extract the Ignition config file from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Upload the <literal>worker.ign</literal> Ignition config file you exported from your cluster to your HTTP server. Note the URL of this file.</simpara>
</listitem>
<listitem>
<simpara>You can validate that the Ignition file is available on the URL. The following example gets the Ignition config file for the compute node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Download the RHEL live <literal>kernel</literal>, <literal>initramfs</literal>, and <literal>rootfs</literal> files by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -LO $(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' \
| jq -r '.architectures.s390x.artifacts.metal.formats.pxe.kernel.location')</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -LO $(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' \
| jq -r '.architectures.s390x.artifacts.metal.formats.pxe.initramfs.location')</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -LO $(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' \
| jq -r '.architectures.s390x.artifacts.metal.formats.pxe.rootfs.location')</programlisting>
</listitem>
<listitem>
<simpara>Move the downloaded RHEL live <literal>kernel</literal>, <literal>initramfs</literal>, and <literal>rootfs</literal> files to an HTTP or HTTPS server that is accessible from the z/VM guest you want to add.</simpara>
</listitem>
<listitem>
<simpara>Create a parameter file for the z/VM guest. The following parameters are specific for the virtual machine:</simpara>
<itemizedlist>
<listitem>
<simpara>Optional: To specify a static IP address, add an <literal>ip=</literal> parameter with the following entries, with each separated by a colon:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>The IP address for the machine.</simpara>
</listitem>
<listitem>
<simpara>An empty string.</simpara>
</listitem>
<listitem>
<simpara>The gateway.</simpara>
</listitem>
<listitem>
<simpara>The netmask.</simpara>
</listitem>
<listitem>
<simpara>The machine host and domain name in the form <literal>hostname.domainname</literal>. Omit this value to let RHCOS decide.</simpara>
</listitem>
<listitem>
<simpara>The network interface name. Omit this value to let RHCOS decide.</simpara>
</listitem>
<listitem>
<simpara>The value <literal>none</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For <literal>coreos.inst.ignition_url=</literal>, specify the URL to the <literal>worker.ign</literal> file. Only HTTP and HTTPS protocols are supported.</simpara>
</listitem>
<listitem>
<simpara>For <literal>coreos.live.rootfs_url=</literal>, specify the matching rootfs artifact for the <literal>kernel</literal> and <literal>initramfs</literal> you are booting. Only HTTP and HTTPS protocols are supported.</simpara>
</listitem>
<listitem>
<simpara>For installations on DASD-type disks, complete the following tasks:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>For <literal>coreos.inst.install_dev=</literal>, specify <literal>/dev/dasda</literal>.</simpara>
</listitem>
<listitem>
<simpara>Use <literal>rd.dasd=</literal> to specify the DASD where RHCOS is to be installed.</simpara>
</listitem>
<listitem>
<simpara>Leave all other parameters unchanged.</simpara>
<simpara>The following is an example parameter file, <literal>additional-worker-dasd.parm</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">rd.neednet=1 \
console=ttysclp0 \
coreos.inst.install_dev=/dev/dasda \
coreos.live.rootfs_url=http://cl1.provide.example.com:8080/assets/rhcos-live-rootfs.s390x.img \
coreos.inst.ignition_url=http://cl1.provide.example.com:8080/ignition/worker.ign \
ip=172.18.78.2::172.18.78.1:255.255.255.0:::none nameserver=172.18.78.1 \
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 \
zfcp.allow_lun_scan=0 \
rd.dasd=0.0.3490</programlisting>
<simpara>Write all options in the parameter file as a single line and make sure that you have no newline characters.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For installations on FCP-type disks, complete the following tasks:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Use <literal>rd.zfcp=&lt;adapter&gt;,&lt;wwpn&gt;,&lt;lun&gt;</literal> to specify the FCP disk where RHCOS is to be installed. For multipathing, repeat this step for each additional path.</simpara>
<note>
<simpara>When you install with multiple paths, you must enable multipathing directly after the installation, not at a later point in time, as this can cause problems.</simpara>
</note>
</listitem>
<listitem>
<simpara>Set the install device as: <literal>coreos.inst.install_dev=/dev/sda</literal>.</simpara>
<note>
<simpara>If additional LUNs are configured with NPIV, FCP requires <literal>zfcp.allow_lun_scan=0</literal>. If you must enable <literal>zfcp.allow_lun_scan=1</literal> because you use a CSI driver, for example, you must configure your NPIV so that each node cannot access the boot partition of another node.</simpara>
</note>
</listitem>
<listitem>
<simpara>Leave all other parameters unchanged.</simpara>
<important>
<simpara>Additional postinstallation steps are required to fully enable multipathing. For more information, see “Enabling multipathing with kernel arguments on RHCOS" in <emphasis>Postinstallation machine configuration tasks</emphasis>.</simpara>
</important>
<simpara>The following is an example parameter file, <literal>additional-worker-fcp.parm</literal> for a worker node with multipathing:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">rd.neednet=1 \
console=ttysclp0 \
coreos.inst.install_dev=/dev/sda \
coreos.live.rootfs_url=http://cl1.provide.example.com:8080/assets/rhcos-live-rootfs.s390x.img \
coreos.inst.ignition_url=http://cl1.provide.example.com:8080/ignition/worker.ign \
ip=172.18.78.2::172.18.78.1:255.255.255.0:::none nameserver=172.18.78.1 \
rd.znet=qeth,0.0.bdf0,0.0.bdf1,0.0.bdf2,layer2=1,portno=0 \
zfcp.allow_lun_scan=0 \
rd.zfcp=0.0.1987,0x50050763070bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.19C7,0x50050763070bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.1987,0x50050763071bc5e3,0x4008400B00000000 \
rd.zfcp=0.0.19C7,0x50050763071bc5e3,0x4008400B00000000</programlisting>
<simpara>Write all options in the parameter file as a single line and make sure that you have no newline characters.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Transfer the <literal>initramfs</literal>, <literal>kernel</literal>, parameter files, and RHCOS images to z/VM, for example, by using FTP. For details about how to transfer the files with FTP and boot from the virtual reader, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/installation_guide/sect-installing-zvm-s390">Installing under Z/VM</link>.</simpara>
</listitem>
<listitem>
<simpara>Punch the files to the virtual reader of the z/VM guest virtual machine.</simpara>
<simpara>See <link xlink:href="https://www.ibm.com/docs/en/zvm/latest?topic=commands-punch">PUNCH</link> in IBM&#174; Documentation.</simpara>
<tip>
<simpara>You can use the CP PUNCH command or, if you use Linux, the <emphasis role="strong">vmur</emphasis> command to transfer files between two z/VM guest virtual machines.</simpara>
</tip>
</listitem>
<listitem>
<simpara>Log in to CMS on the bootstrap machine.</simpara>
</listitem>
<listitem>
<simpara>IPL the bootstrap machine from the reader by running the following command:</simpara>
<screen>$ ipl c</screen>
<simpara>See <link xlink:href="https://www.ibm.com/docs/en/zvm/latest?topic=commands-ipl">IPL</link> in IBM&#174; Documentation.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-approve-csrs_creating-multi-arch-compute-nodes-ibm-z">
<title>Approving the certificate signing requests for your machines</title>
<simpara>When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You added machines to your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that the cluster recognizes the machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.28.5
master-1  Ready     master  63m  v1.28.5
master-2  Ready     master  64m  v1.28.5</programlisting>
</para>
</formalpara>
<simpara>The output lists all of the machines that you created.</simpara>
<note>
<simpara>The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the pending CSRs and ensure that you see the client requests with the <literal>Pending</literal> or <literal>Approved</literal> status for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</programlisting>
</para>
</formalpara>
<simpara>In this example, two machines are joining the cluster. You might see more approved CSRs in the list.</simpara>
</listitem>
<listitem>
<simpara>If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<note>
<simpara>Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <literal>machine-approver</literal> if the Kubelet requests a new certificate with identical parameters.</simpara>
</note>
<note>
<simpara>For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <literal>oc exec</literal>, <literal>oc rsh</literal>, and <literal>oc logs</literal> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <literal>node-bootstrapper</literal> service account in the <literal>system:node</literal> or <literal>system:admin</literal> groups, and confirm the identity of the node.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO17-1"/></programlisting>
<calloutlist>
<callout arearefs="CO17-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</programlisting>
<note>
<simpara>Some Operators might not become available until some CSRs are approved.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If the remaining CSRs are not approved, and are in the <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO18-1"/></programlisting>
<calloutlist>
<callout arearefs="CO18-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After all client and server CSRs have been approved, the machines have the <literal>Ready</literal> status. Verify this by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.28.5
master-1  Ready     master  73m  v1.28.5
master-2  Ready     master  74m  v1.28.5
worker-0  Ready     worker  11m  v1.28.5
worker-1  Ready     worker  11m  v1.28.5</programlisting>
</para>
</formalpara>
<note>
<simpara>It can take a few minutes after approval of the server CSRs for the machines to transition to the <literal>Ready</literal> status.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional information</title>
<listitem>
<simpara>For more information on CSRs, see <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-ibm-z-kvm">
<title>Creating a cluster with multi-architecture compute machines on IBM Z and IBM LinuxONE with RHEL KVM</title>

<simpara>To create a cluster with multi-architecture compute machines on IBM Z&#174; and IBM&#174; LinuxONE (<literal>s390x</literal>) with RHEL KVM, you must have an existing single-architecture <literal>x86_64</literal> cluster. You can then add <literal>s390x</literal> compute machines to your OpenShift Container Platform cluster.</simpara>
<simpara>Before you can add <literal>s390x</literal> nodes to your cluster, you must upgrade your cluster to one that uses the multi-architecture payload. For more information on migrating to the multi-architecture payload, see <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
<simpara>The following procedures explain how to create a RHCOS compute machine using a RHEL KVM instance. This will allow you to add <literal>s390x</literal> nodes to your cluster and deploy a cluster with multi-architecture compute machines.</simpara>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-ibm-z-kvm">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-ibm-z-kvm_creating-multi-arch-compute-nodes-ibm-z-kvm">
<title>Creating RHCOS machines using <literal>virt-install</literal></title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your cluster by using <literal>virt-install</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have at least one LPAR running on RHEL 8.7 or later with KVM, referred to as RHEL KVM host in this procedure.</simpara>
</listitem>
<listitem>
<simpara>The KVM/QEMU hypervisor is installed on the RHEL KVM host.</simpara>
</listitem>
<listitem>
<simpara>You have a domain name server (DNS) that can perform hostname and reverse lookup for the nodes.</simpara>
</listitem>
<listitem>
<simpara>An HTTP or HTTPS server is set up.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Disable UDP aggregation.</simpara>
<simpara>Currently, UDP aggregation is not supported on IBM Z&#174; and is not automatically deactivated on multi-architecture compute clusters with an <literal>x86_64</literal> control plane and additional <literal>s390x</literal> compute machines. To ensure that the addtional compute nodes are added to the cluster correctly, you must manually disable UDP aggregation.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a YAML file <literal>udp-aggregation-config.yaml</literal> with the following content:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
data:
  disable-udp-aggregation: "true"
metadata:
  name: udp-aggregation-config
  namespace: openshift-network-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the ConfigMap resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f udp-aggregation-config.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Extract the Ignition config file from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Upload the <literal>worker.ign</literal> Ignition config file you exported from your cluster to your HTTP server. Note the URL of this file.</simpara>
</listitem>
<listitem>
<simpara>You can validate that the Ignition file is available on the URL. The following example gets the Ignition config file for the compute node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Download the RHEL live <literal>kernel</literal>, <literal>initramfs</literal>, and <literal>rootfs</literal> files by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ curl -LO $(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' \
| jq -r '.architectures.s390x.artifacts.metal.formats.pxe.kernel.location')</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -LO $(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' \
| jq -r '.architectures.s390x.artifacts.metal.formats.pxe.initramfs.location')</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -LO $(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' \
| jq -r '.architectures.s390x.artifacts.metal.formats.pxe.rootfs.location')</programlisting>
</listitem>
<listitem>
<simpara>Move the downloaded RHEL live <literal>kernel</literal>, <literal>initramfs</literal> and <literal>rootfs</literal> files to an HTTP or HTTPS server before you launch <literal>virt-install</literal>.</simpara>
</listitem>
<listitem>
<simpara>Create the new KVM guest nodes using the RHEL <literal>kernel</literal>, <literal>initramfs</literal>, and Ignition files; the new disk image; and adjusted parm line arguments.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virt-install \
   --connect qemu:///system \
   --name &lt;vm_name&gt; \
   --autostart \
   --os-variant rhel9.2 \ <co xml:id="CO19-1"/>
   --cpu host \
   --vcpus &lt;vcpus&gt; \
   --memory &lt;memory_mb&gt; \
   --disk &lt;vm_name&gt;.qcow2,size=&lt;image_size&gt; \
   --network network=&lt;virt_network_parm&gt; \
   --location &lt;media_location&gt;,kernel=&lt;rhcos_kernel&gt;,initrd=&lt;rhcos_initrd&gt; \ <co xml:id="CO19-2"/>
   --extra-args "rd.neednet=1" \
   --extra-args "coreos.inst.install_dev=/dev/vda" \
   --extra-args "coreos.inst.ignition_url=&lt;worker_ign&gt;" \ <co xml:id="CO19-3"/>
   --extra-args "coreos.live.rootfs_url=&lt;rhcos_rootfs&gt;" \ <co xml:id="CO19-4"/>
   --extra-args "ip=&lt;ip&gt;::&lt;default_gateway&gt;:&lt;subnet_mask_length&gt;:&lt;hostname&gt;::none:&lt;MTU&gt;" \ <co xml:id="CO19-5"/>
   --extra-args "nameserver=&lt;dns&gt;" \
   --extra-args "console=ttysclp0" \
   --noautoconsole \
   --wait</programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>For <literal>os-variant</literal>, specify the RHEL version for the RHCOS compute machine. <literal>rhel9.2</literal> is the recommended version. To query the supported RHEL version of your operating system, run the following command:</para>
<programlisting language="terminal" linenumbering="unnumbered">$ osinfo-query os -f short-id</programlisting>
<note>
<simpara>The <literal>os-variant</literal> is case sensitive.</simpara>
</note>
</callout>
<callout arearefs="CO19-2">
<para>For <literal>--location</literal>, specify the location of the kernel/initrd on the HTTP or HTTPS server.</para>
</callout>
<callout arearefs="CO19-3">
<para>For <literal>coreos.inst.ignition_url=</literal>, specify the <literal>worker.ign</literal> Ignition file for the machine role. Only HTTP and HTTPS protocols are supported.</para>
</callout>
<callout arearefs="CO19-4">
<para>For <literal>coreos.live.rootfs_url=</literal>, specify the matching rootfs artifact for the <literal>kernel</literal> and <literal>initramfs</literal> you are booting. Only HTTP and HTTPS protocols are supported.</para>
</callout>
<callout arearefs="CO19-5">
<para>Optional: For <literal>hostname</literal>, specify the fully qualified hostname of the client machine.</para>
</callout>
</calloutlist>
<note>
<simpara>If you are using HAProxy as a load balancer, update your HAProxy rules for <literal>ingress-router-443</literal> and <literal>ingress-router-80</literal> in the <literal>/etc/haproxy/haproxy.cfg</literal> configuration file.</simpara>
</note>
</listitem>
<listitem>
<simpara>Continue to create more compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-approve-csrs_creating-multi-arch-compute-nodes-ibm-z-kvm">
<title>Approving the certificate signing requests for your machines</title>
<simpara>When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You added machines to your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that the cluster recognizes the machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.28.5
master-1  Ready     master  63m  v1.28.5
master-2  Ready     master  64m  v1.28.5</programlisting>
</para>
</formalpara>
<simpara>The output lists all of the machines that you created.</simpara>
<note>
<simpara>The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the pending CSRs and ensure that you see the client requests with the <literal>Pending</literal> or <literal>Approved</literal> status for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</programlisting>
</para>
</formalpara>
<simpara>In this example, two machines are joining the cluster. You might see more approved CSRs in the list.</simpara>
</listitem>
<listitem>
<simpara>If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<note>
<simpara>Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <literal>machine-approver</literal> if the Kubelet requests a new certificate with identical parameters.</simpara>
</note>
<note>
<simpara>For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <literal>oc exec</literal>, <literal>oc rsh</literal>, and <literal>oc logs</literal> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <literal>node-bootstrapper</literal> service account in the <literal>system:node</literal> or <literal>system:admin</literal> groups, and confirm the identity of the node.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO20-1"/></programlisting>
<calloutlist>
<callout arearefs="CO20-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</programlisting>
<note>
<simpara>Some Operators might not become available until some CSRs are approved.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If the remaining CSRs are not approved, and are in the <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO21-1"/></programlisting>
<calloutlist>
<callout arearefs="CO21-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After all client and server CSRs have been approved, the machines have the <literal>Ready</literal> status. Verify this by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.28.5
master-1  Ready     master  73m  v1.28.5
master-2  Ready     master  74m  v1.28.5
worker-0  Ready     worker  11m  v1.28.5
worker-1  Ready     worker  11m  v1.28.5</programlisting>
</para>
</formalpara>
<note>
<simpara>It can take a few minutes after approval of the server CSRs for the machines to transition to the <literal>Ready</literal> status.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional information</title>
<listitem>
<simpara>For more information on CSRs, see <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="creating-multi-arch-compute-nodes-ibm-power">
<title>Creating a cluster with multi-architecture compute machines on IBM Power</title>

<simpara>To create a cluster with multi-architecture compute machines on IBM Power&#174; (<literal>ppc64le</literal>), you must have an existing single-architecture (<literal>x86_64</literal>) cluster. You can then add <literal>ppc64le</literal> compute machines to your OpenShift Container Platform cluster.</simpara>
<important>
<simpara>Before you can add <literal>ppc64le</literal> nodes to your cluster, you must upgrade your cluster to one that uses the multi-architecture payload. For more information on migrating to the multi-architecture payload, see <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
<simpara>The following procedures explain how to create a RHCOS compute machine using an ISO image or network PXE booting. This will allow you to add <literal>ppc64le</literal> nodes to your cluster and deploy a cluster with multi-architecture compute machines.</simpara>
<section xml:id="multi-architecture-verifying-cluster-compatibility_creating-multi-arch-compute-nodes-ibm-power">
<title>Verifying cluster compatibility</title>
<simpara>Before you can start adding compute nodes of different architectures to your cluster, you must verify that your cluster is multi-architecture compatible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>)</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>When using multiple architectures, hosts for OpenShift Container Platform nodes must share the same storage layer. If they do not have the same storage layer, use a storage provider such as <literal>nfs-provisioner</literal>.</simpara>
</note>
<note>
<simpara>You should limit the number of network hops between the compute and control plane as much as possible.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>You can check that your cluster uses the architecture payload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release info -o jsonpath="{ .metadata.metadata}"</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>If you see the following output, then your cluster is using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "release.openshift.io/architecture": "multi",
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<simpara>You can then begin adding multi-arch compute nodes to your cluster.</simpara>
</listitem>
<listitem>
<simpara>If you see the following output, then your cluster is not using the multi-architecture payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{
 "url": "https://access.redhat.com/errata/&lt;errata_version&gt;"
}</programlisting>
<important>
<simpara>To migrate your cluster so the cluster supports multi-architecture compute machines, follow the procedure in <link xlink:href="../../updating/updating_a_cluster/migrating-to-multi-payload.xml#migrating-to-multi-payload">Migrating to a cluster with multi-architecture compute machines</link>.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-iso_creating-multi-arch-compute-nodes-ibm-power">
<title>Creating RHCOS machines using an ISO image</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your cluster by using an ISO image to create the machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>)  installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Extract the Ignition config file from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Upload the <literal>worker.ign</literal> Ignition config file you exported from your cluster to your HTTP server. Note the URLs of these files.</simpara>
</listitem>
<listitem>
<simpara>You can validate that the ignition files are available on the URLs. The following example gets the Ignition config files for the compute node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</programlisting>
</listitem>
<listitem>
<simpara>You can access the ISO image for booting your new machine by running to following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.&lt;architecture&gt;.artifacts.metal.formats.iso.disk.location')</programlisting>
</listitem>
<listitem>
<simpara>Use the ISO file to install RHCOS on more compute machines. Use the same method that you used when you created machines before you installed the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Burn the ISO image to a disk and boot it directly.</simpara>
</listitem>
<listitem>
<simpara>Use ISO redirection with a LOM interface.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Boot the RHCOS ISO image without specifying any options, or interrupting the live boot sequence. Wait for the installer to boot into a shell prompt in the RHCOS live environment.</simpara>
<note>
<simpara>You can interrupt the RHCOS installation boot process to add kernel arguments. However, for this ISO procedure you must use the <literal>coreos-installer</literal> command as outlined in the following steps, instead of adding kernel arguments.</simpara>
</note>
</listitem>
<listitem>
<simpara>Run the <literal>coreos-installer</literal> command and specify the options that meet your installation requirements. At a minimum, you must specify the URL that points to the Ignition config file for the node type, and the device that you are installing to:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo coreos-installer install --ignition-url=http://&lt;HTTP_server&gt;/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=sha512-&lt;digest&gt; <co xml:id="CO22-1"/><co xml:id="CO22-2"/></programlisting>
<calloutlist>
<callout arearefs="CO22-1">
<para>You must run the <literal>coreos-installer</literal> command by using <literal>sudo</literal>, because the <literal>core</literal> user does not have the required root privileges to perform the installation.</para>
</callout>
<callout arearefs="CO22-2">
<para>The <literal>--ignition-hash</literal> option is required when the Ignition config file is obtained through an HTTP URL to validate the authenticity of the Ignition config file on the cluster node. <literal>&lt;digest&gt;</literal> is the Ignition config file SHA512 digest obtained in a preceding step.</para>
</callout>
</calloutlist>
<note>
<simpara>If you want to provide your Ignition config files through an HTTPS server that uses TLS, you can add the internal certificate authority (CA) to the system trust store before running <literal>coreos-installer</literal>.</simpara>
</note>
<simpara>The following example initializes a bootstrap node installation to the <literal>/dev/sda</literal> device. The Ignition config file for the bootstrap node is obtained from an HTTP web server with the IP address 192.168.1.2:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo coreos-installer install --ignition-url=http://192.168.1.2:80/installation_directory/bootstrap.ign /dev/sda --ignition-hash=sha512-a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</programlisting>
</listitem>
<listitem>
<simpara>Monitor the progress of the RHCOS installation on the console of the machine.</simpara>
<important>
<simpara>Ensure that the installation is successful on each node before commencing with the OpenShift Container Platform installation. Observing the installation process can also help to determine the cause of RHCOS installation issues that might arise.</simpara>
</important>
</listitem>
<listitem>
<simpara>Continue to create more compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-pxe_creating-multi-arch-compute-nodes-ibm-power">
<title>Creating RHCOS machines by PXE or iPXE booting</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using PXE or iPXE booting.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.</simpara>
</listitem>
<listitem>
<simpara>Obtain the URLs of the RHCOS ISO image, compressed metal BIOS, <literal>kernel</literal>, and <literal>initramfs</literal> files that you uploaded to your HTTP server during cluster installation.</simpara>
</listitem>
<listitem>
<simpara>You have access to the PXE booting infrastructure that you used to create the machines for your OpenShift Container Platform cluster during installation. The machines must boot from their local disks after RHCOS is installed on them.</simpara>
</listitem>
<listitem>
<simpara>If you use UEFI, you have access to the <literal>grub.conf</literal> file that you modified during OpenShift Container Platform installation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that your PXE or iPXE installation for the RHCOS images is correct.</simpara>
<itemizedlist>
<listitem>
<simpara>For PXE:</simpara>
<screen>DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
    KERNEL http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; <co xml:id="CO23-1"/>
    APPEND initrd=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img <co xml:id="CO23-2"/></screen>
<calloutlist>
<callout arearefs="CO23-1">
<para>Specify the location of the live <literal>kernel</literal> file that you uploaded to your HTTP server.</para>
</callout>
<callout arearefs="CO23-2">
<para>Specify locations of the RHCOS files that you uploaded to your HTTP server. The <literal>initrd</literal> parameter value is the location of the live <literal>initramfs</literal> file, the <literal>coreos.inst.ignition_url</literal> parameter value is the location of the worker Ignition config file, and the <literal>coreos.live.rootfs_url</literal> parameter value is the location of the live <literal>rootfs</literal> file. The <literal>coreos.inst.ignition_url</literal> and <literal>coreos.live.rootfs_url</literal> parameters only support HTTP and HTTPS.</para>
</callout>
</calloutlist>
<note>
<simpara>This configuration does not enable serial console access on machines with a graphical console. To configure a different console, add one or more <literal>console=</literal> arguments to the <literal>APPEND</literal> line. For example, add <literal>console=tty0 console=ttyS0</literal> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <link xlink:href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>For iPXE (<literal>x86_64</literal> + <literal>ppc64le</literal>):</simpara>
<screen>kernel http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; initrd=main coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <co xml:id="CO24-1"/> <co xml:id="CO24-2"/>
initrd --name main http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <co xml:id="CO24-3"/>
boot</screen>
<calloutlist>
<callout arearefs="CO24-1">
<para>Specify the locations of the RHCOS files that you uploaded to your
HTTP server. The <literal>kernel</literal> parameter value is the location of the <literal>kernel</literal> file,
the <literal>initrd=main</literal> argument is needed for booting on UEFI systems,
the <literal>coreos.live.rootfs_url</literal> parameter value is the location of the <literal>rootfs</literal> file,
and the <literal>coreos.inst.ignition_url</literal> parameter value is the
location of the worker Ignition config file.</para>
</callout>
<callout arearefs="CO24-2">
<para>If you use multiple NICs, specify a single interface in the <literal>ip</literal> option.
For example, to use DHCP on a NIC that is named <literal>eno1</literal>, set <literal>ip=eno1:dhcp</literal>.</para>
</callout>
<callout arearefs="CO24-3">
<para>Specify the location of the <literal>initramfs</literal> file that you uploaded to your HTTP server.</para>
</callout>
</calloutlist>
<note>
<simpara>This configuration does not enable serial console access on machines with a graphical console To configure a different console, add one or more <literal>console=</literal> arguments to the <literal>kernel</literal> line. For example, add <literal>console=tty0 console=ttyS0</literal> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <link xlink:href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</link> and "Enabling the serial console for PXE and ISO installation" in the "Advanced RHCOS installation configuration" section.</simpara>
</note>
<note>
<simpara>To network boot the CoreOS <literal>kernel</literal> on <literal>ppc64le</literal> architecture, you need to use a version of iPXE build with the <literal>IMAGE_GZIP</literal> option enabled. See <link xlink:href="https://ipxe.org/buildcfg/image_gzip"><literal>IMAGE_GZIP</literal> option in iPXE</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>For PXE (with UEFI and GRUB as second stage) on <literal>ppc64le</literal>:</simpara>
<screen>menuentry 'Install CoreOS' {
    linux rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt;  coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <co xml:id="CO25-1"/> <co xml:id="CO25-2"/>
    initrd rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <co xml:id="CO25-3"/>
}</screen>
<calloutlist>
<callout arearefs="CO25-1">
<para>Specify the locations of the RHCOS files that you uploaded to your
HTTP/TFTP server. The <literal>kernel</literal> parameter value is the location of the <literal>kernel</literal> file on your TFTP server.
The <literal>coreos.live.rootfs_url</literal> parameter value is the location of the <literal>rootfs</literal> file, and the <literal>coreos.inst.ignition_url</literal> parameter value is the location of the worker Ignition config file on your HTTP Server.</para>
</callout>
<callout arearefs="CO25-2">
<para>If you use multiple NICs, specify a single interface in the <literal>ip</literal> option.
For example, to use DHCP on a NIC that is named <literal>eno1</literal>, set <literal>ip=eno1:dhcp</literal>.</para>
</callout>
<callout arearefs="CO25-3">
<para>Specify the location of the <literal>initramfs</literal> file that you uploaded to your TFTP server.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Use the PXE or iPXE infrastructure to create the required compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-approve-csrs_creating-multi-arch-compute-nodes-ibm-power">
<title>Approving the certificate signing requests for your machines</title>
<simpara>When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You added machines to your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that the cluster recognizes the machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.28.5
master-1  Ready     master  63m  v1.28.5
master-2  Ready     master  64m  v1.28.5</programlisting>
</para>
</formalpara>
<simpara>The output lists all of the machines that you created.</simpara>
<note>
<simpara>The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the pending CSRs and ensure that you see the client requests with the <literal>Pending</literal> or <literal>Approved</literal> status for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</programlisting>
</para>
</formalpara>
<simpara>In this example, two machines are joining the cluster. You might see more approved CSRs in the list.</simpara>
</listitem>
<listitem>
<simpara>If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<note>
<simpara>Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <literal>machine-approver</literal> if the Kubelet requests a new certificate with identical parameters.</simpara>
</note>
<note>
<simpara>For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <literal>oc exec</literal>, <literal>oc rsh</literal>, and <literal>oc logs</literal> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <literal>node-bootstrapper</literal> service account in the <literal>system:node</literal> or <literal>system:admin</literal> groups, and confirm the identity of the node.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO26-1"/></programlisting>
<calloutlist>
<callout arearefs="CO26-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</programlisting>
<note>
<simpara>Some Operators might not become available until some CSRs are approved.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If the remaining CSRs are not approved, and are in the <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO27-1"/></programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After all client and server CSRs have been approved, the machines have the <literal>Ready</literal> status. Verify this by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME               STATUS   ROLES                  AGE   VERSION           INTERNAL-IP      EXTERNAL-IP   OS-IMAGE                                                       KERNEL-VERSION                  CONTAINER-RUNTIME
worker-0-ppc64le   Ready    worker                 42d   v1.28.2+e3ba6d9   192.168.200.21   &lt;none&gt;        Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.ppc64le   cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9
worker-1-ppc64le   Ready    worker                 42d   v1.28.2+e3ba6d9   192.168.200.20   &lt;none&gt;        Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.ppc64le   cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9
master-0-x86       Ready    control-plane,master   75d   v1.28.2+e3ba6d9   10.248.0.38      10.248.0.38   Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.x86_64    cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9
master-1-x86       Ready    control-plane,master   75d   v1.28.2+e3ba6d9   10.248.0.39      10.248.0.39   Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.x86_64    cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9
master-2-x86       Ready    control-plane,master   75d   v1.28.2+e3ba6d9   10.248.0.40      10.248.0.40   Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.x86_64    cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9
worker-0-x86       Ready    worker                 75d   v1.28.2+e3ba6d9   10.248.0.43      10.248.0.43   Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.x86_64    cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9
worker-1-x86       Ready    worker                 75d   v1.28.2+e3ba6d9   10.248.0.44      10.248.0.44   Red Hat Enterprise Linux CoreOS 415.92.202309261919-0 (Plow)   5.14.0-284.34.1.el9_2.x86_64    cri-o://1.28.1-3.rhaos4.15.gitb36169e.el9</programlisting>
</para>
</formalpara>
<note>
<simpara>It can take a few minutes after approval of the server CSRs for the machines to transition to the <literal>Ready</literal> status.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional information</title>
<listitem>
<simpara>For more information on CSRs, see <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="multi-architecture-compute-managing">
<title>Managing your cluster with multi-architecture compute machines</title>

<section xml:id="_scheduling_workloads_on_clusters_with_multi_architecture_compute_machines">
<title>Scheduling workloads on clusters with multi-architecture compute machines</title>
<simpara>Deploying a workload on a cluster with compute nodes of different architectures requires attention and monitoring of your cluster. There might be further actions you need to take in order to successfully place pods in the nodes of your cluster.</simpara>
<simpara>For more detailed information on node affinity, scheduling, taints and tolerlations, see the following documentatinon:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../nodes/scheduling/nodes-scheduler-taints-tolerations.xml#nodes-scheduler-taints-tolerations">Controlling pod placement using node taints</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../nodes/scheduling/nodes-scheduler-node-affinity.xml#nodes-scheduler-node-affinity">Controlling pod placement on nodes using node affinity</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../nodes/scheduling/nodes-scheduler-about.xml#nodes-scheduler-about">Controlling pod placement using the scheduler</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="multi-architecture-scheduling-examples_multi-architecture-compute-managing">
<title>Sample multi-architecture node workload deployments</title>
<simpara>Before you schedule workloads on a cluster with compute nodes of different architectures, consider the following use cases:</simpara>
<variablelist>
<varlistentry>
<term>Using node affinity to schedule workloads on a node</term>
<listitem>
<simpara>You can allow a workload to be scheduled on only a set of nodes with architectures supported by its images, you can set the <literal>spec.affinity.nodeAffinity</literal> field in your pod&#8217;s template specification.</simpara>
<formalpara>
<title>Example deployment with the <literal>nodeAffinity</literal> set to certain architectures</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata: # ...
spec:
   # ...
  template:
     # ...
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values: <co xml:id="CO28-1"/>
                - amd64
                - arm64</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO28-1">
<para>Specify the supported architectures. Valid values include <literal>amd64</literal>,<literal>arm64</literal>, or both values.</para>
</callout>
</calloutlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Tainting every node for a specific architecture</term>
<listitem>
<simpara>You can taint a node to avoid workloads that are not compatible with its architecture to be scheduled on that node. In the case where your cluster is using a <literal>MachineSet</literal> object, you can add parameters to the <literal>.spec.template.spec.taints</literal> field to avoid workloads being scheduled on nodes with non-supported architectures.</simpara>
<itemizedlist>
<listitem>
<simpara>Before you can taint a node, you must scale down the <literal>MachineSet</literal> object or remove available machines. You can scale down the machine set by using one of following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=0 machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>Or:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>For more information on scaling machine sets, see "Modifying a compute machine set".</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Example <literal>MachineSet</literal> with a taint set</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata: # ...
spec:
  # ...
  template:
    # ...
    spec:
      # ...
      taints:
      - effect: NoSchedule
        key: multi-arch.openshift.io/arch
        value: arm64</programlisting>
</para>
</formalpara>
<simpara>You can also set a taint on a specific node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes &lt;node-name&gt; multi-arch.openshift.io/arch=arm64:NoSchedule</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>Creating a default toleration</term>
<listitem>
<simpara>You can annotate a namespace so all of the workloads get the same default toleration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate namespace my-namespace \
  'scheduler.alpha.kubernetes.io/defaultTolerations'='[{"operator": "Exists", "effect": "NoSchedule", "key": "multi-arch.openshift.io/arch"}]'</programlisting>
</listitem>
</varlistentry>
<varlistentry>
<term>Tolerating architecture taints in workloads</term>
<listitem>
<simpara>On a node with a defined taint, workloads will not be scheduled on that node. However, you can allow them to be scheduled by setting a toleration in the pod&#8217;s specification.</simpara>
<formalpara>
<title>Example deployment with a toleration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata: # ...
spec:
  # ...
  template:
    # ...
    spec:
      tolerations:
      - key: "multi-arch.openshift.io/arch"
        value: "arm64"
        operator: "Equal"
        effect: "NoSchedule"</programlisting>
</para>
</formalpara>
<simpara>This example deployment can also be allowed on nodes with the <literal>multi-arch.openshift.io/arch=arm64</literal> taint specified.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Using node affinity with taints and tolerations</term>
<listitem>
<simpara>When a scheduler computes the set of nodes to schedule a pod, tolerations can broaden the set while node affinity restricts the set. If you set a taint to the nodes of a specific architecture, the following example toleration is required for scheduling pods.</simpara>
<formalpara>
<title>Example deployment with a node affinity and toleration set.</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata: # ...
spec:
  # ...
  template:
    # ...
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: kubernetes.io/arch
                operator: In
                values:
                - amd64
                - arm64
      tolerations:
      - key: "multi-arch.openshift.io/arch"
        value: "arm64"
        operator: "Equal"
        effect: "NoSchedule"</programlisting>
</para>
</formalpara>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../machine_management/modifying-machineset.xml#machineset-modifying_modifying-machineset">Modifying a compute machine set</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="multi-architecture-enabling-64k-pages_multi-architecture-compute-managing">
<title>Enabling 64k pages on the Red Hat Enterprise Linux CoreOS (RHCOS) kernel</title>
<simpara>You can enable the 64k memory page in the Red Hat Enterprise Linux CoreOS (RHCOS) kernel on the 64-bit ARM compute machines in your cluster. The 64k page size kernel specification can be used for large GPU or high memory workloads. This is done using the Machine Config Operator (MCO) which uses a machine config pool to update the kernel. To enable 64k page sizes, you must dedicate a machine config pool for ARM64 to enable on the kernel.</simpara>
<important>
<simpara>Using 64k pages is exclusive to 64-bit ARM architecture compute nodes or clusters installed on 64-bit ARM machines. If you configure the 64k pages kernel on a machine config pool using 64-bit x86 machines, the machine config pool and MCO will degrade.</simpara>
</important>
<itemizedlist>
<title>Prerequisites:</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You created a cluster with compute nodes of different architecture on one of the supported platforms.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure:</title>
<listitem>
<simpara>Label the nodes where you want to run the 64k page size kernel:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node_name&gt; &lt;label&gt;</programlisting>
<formalpara>
<title>Example command</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node worker-arm64-01 node-role.kubernetes.io/worker-64k-pages=</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a machine config pool that contains the worker role that uses the ARM64 architecture and the <literal>worker-64k-pages</literal> role:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-64k-pages
spec:
  machineConfigSelector:
    matchExpressions:
      - key: machineconfiguration.openshift.io/role
        operator: In
        values:
        - worker
        - worker-64k-pages
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-64k-pages: ""
      kubernetes.io/arch: arm64</programlisting>
</listitem>
<listitem>
<simpara>Create a machine config on your compute node to enable <literal>64k-pages</literal> with the <literal>64k-pages</literal> parameter.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;filename&gt;.yaml</programlisting>
<formalpara>
<title>Example MachineConfig</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "worker-64k-pages" <co xml:id="CO29-1"/>
  name: 99-worker-64kpages
spec:
  kernelType: 64k-pages <co xml:id="CO29-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO29-1">
<para>Specify the value of the <literal>machineconfiguration.openshift.io/role</literal> label in the custom machine config pool. The example MachineConfig uses the <literal>worker-64k-pages</literal> label to enable 64k pages in the <literal>worker-64k-pages</literal> pool.</para>
</callout>
<callout arearefs="CO29-2">
<para>Specify your desired kernel type. Valid values are <literal>64k-pages</literal> and <literal>default</literal></para>
</callout>
</calloutlist>
<note>
<simpara>The <literal>64k-pages</literal> type is supported on only 64-bit ARM architecture based compute nodes. The <literal>realtime</literal> type is supported on only 64-bit x86 architecture based compute nodes.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To view your new <literal>worker-64k-pages</literal> machine config pool, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                                                                UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-9d55ac9a91127c36314e1efe7d77fbf8                      True      False      False      3              3                   3                     0                      361d
worker   rendered-worker-e7b61751c4a5b7ff995d64b967c421ff                      True      False      False      7              7                   7                     0                      361d
worker-64k-pages  rendered-worker-64k-pages-e7b61751c4a5b7ff995d64b967c421ff   True      False      False      2              2                   2                     0                      35m</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="multi-architecture-import-imagestreams_multi-architecture-compute-managing">
<title>Importing manifest lists in image streams on your multi-architecture compute machines</title>
<simpara>On an OpenShift Container Platform 4.14 cluster with multi-architecture compute machines, the image streams in the cluster do not import manifest lists automatically. You must manually change the default <literal>importMode</literal> option to the <literal>PreserveOriginal</literal> option in order to import the manifest list.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift Container Platform CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>The following example command shows how to patch the <literal>ImageStream</literal> cli-artifacts so that the <literal>cli-artifacts:latest</literal> image stream tag is imported as a manifest list.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch is/cli-artifacts -n openshift -p '{"spec":{"tags":[{"name":"latest","importPolicy":{"importMode":"PreserveOriginal"}}]}}'</programlisting>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>You can check that the manifest lists imported properly by inspecting the image stream tag. The following command will list the individual architecture manifests for a particular tag.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get istag cli-artifacts:latest -n openshift -oyaml</programlisting>
<simpara>If the <literal>dockerImageManifests</literal> object is present, then the manifest list import was successful.</simpara>
<formalpara>
<title>Example output of the <literal>dockerImageManifests</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">dockerImageManifests:
  - architecture: amd64
    digest: sha256:16d4c96c52923a9968fbfa69425ec703aff711f1db822e4e9788bf5d2bee5d77
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux
  - architecture: arm64
    digest: sha256:6ec8ad0d897bcdf727531f7d0b716931728999492709d19d8b09f0d90d57f626
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux
  - architecture: ppc64le
    digest: sha256:65949e3a80349cdc42acd8c5b34cde6ebc3241eae8daaeea458498fedb359a6a
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux
  - architecture: s390x
    digest: sha256:75f4fa21224b5d5d511bea8f92dfa8e1c00231e5c81ab95e83c3013d245d1719
    manifestSize: 1252
    mediaType: application/vnd.docker.distribution.manifest.v2+json
    os: linux</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="vsphere-post-installation-encryption">
<title>Enabling encryption on a vSphere cluster</title>

<simpara>You can encrypt your virtual machines after installing OpenShift Container Platform 4.14 on vSphere by draining and shutting down your nodes one at a time. While each virtual machine is shutdown, you can enable encryption in the vCenter web interface.</simpara>
<section xml:id="encrypting-virtual-machines_vsphere-post-installation-encryption">
<title>Encrypting virtual machines</title>
<simpara>You can encrypt your virtual machines with the following process. You can drain your virtual machines, power them down and encrypt them using the vCenter interface. Finally, you can create a storage class to use the encrypted storage.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured a Standard key provider in vSphere. For more information, see <link xlink:href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.vsan.doc/GUID-AC06B3C3-901F-402E-B25F-1EE7809D1264.html">Adding a KMS to vCenter Server</link>.</simpara>
<important>
<simpara>The Native key provider in vCenter is not supported. For more information, see <link xlink:href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-54B9FBA2-FDB1-400B-A6AE-81BF3AC9DF97.html">vSphere Native Key Provider Overview</link>.</simpara>
</important>
</listitem>
<listitem>
<simpara>You have enabled host encryption mode on all of the ESXi hosts that are hosting the cluster. For more information, see <link xlink:href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-A9E1F016-51B3-472F-B8DE-803F6BDB70BC.html">Enabling host encryption mode</link>.</simpara>
</listitem>
<listitem>
<simpara>You have a vSphere account which has all cryptographic privileges enabled. For more information, see <link xlink:href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.security.doc/GUID-660CCB35-847F-46B3-81CA-10DDDB9D7AA9.html">Cryptographic Operations Privileges</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Drain and cordon one of your nodes. For detailed instructions on node management, see "Working with Nodes".</simpara>
</listitem>
<listitem>
<simpara>Shutdown the virtual machine associated with that node in the vCenter interface.</simpara>
</listitem>
<listitem>
<simpara>Right-click on the virtual machine in the vCenter interface and select <emphasis role="strong">VM Policies</emphasis> &#8594; <emphasis role="strong">Edit VM Storage Policies</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select an encrypted storage policy and select <emphasis role="strong">OK</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Start the encrypted virtual machine in the vCenter interface.</simpara>
</listitem>
<listitem>
<simpara>Repeat steps 1-5 for all nodes that you want to encrypt.</simpara>
</listitem>
<listitem>
<simpara>Configure a storage class that uses the encrypted storage policy. For more information about configuring an encrypted storage class, see "VMware vSphere CSI Driver Operator".</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_enabling-encryption-installation" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../nodes/nodes/nodes-nodes-working.xml#nodes-nodes-working-evacuating_nodes-nodes-working">Working with nodes</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../storage/container_storage_interface/persistent-storage-csi-vsphere.xml#vsphere-pv-encryption">vSphere encryption</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../installing/installing_vsphere/upi/upi-vsphere-installation-reqs.xml#installation-vsphere-encrypted-vms_upi-vsphere-installation-reqs">Requirements for encrypting virtual machines</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="installing-vsphere-post-installation-configuration">
<title>Configuring the vSphere connection settings after an installation</title>
<simpara>After installing an OpenShift Container Platform cluster on vSphere with the platform integration feature enabled, you might need to update the vSphere connection settings manually, depending on the installation method.</simpara>
<simpara>For installations using the Assisted Installer, you must update the connection settings. This is because the Assisted Installer adds default connection settings to the <emphasis role="strong">vSphere connection configuration</emphasis> wizard as placeholders during the installation.</simpara>
<simpara>For installer-provisioned or user-provisioned infrastructure installations, you should have entered valid connection settings during the installation. You can use the <emphasis role="strong">vSphere connection configuration</emphasis> wizard at any time to validate or modify the connection settings, but this is not mandatory for completing the installation.</simpara>

<section xml:id="configuring-vSphere-connection-settings_installing-vsphere-post-installation-configuration">
<title>Configuring the vSphere connection settings</title>
<simpara role="_abstract">Modify the following vSphere configuration settings as required:</simpara>
<itemizedlist>
<listitem>
<simpara>vCenter address</simpara>
</listitem>
<listitem>
<simpara>vCenter cluster</simpara>
</listitem>
<listitem>
<simpara>vCenter username</simpara>
</listitem>
<listitem>
<simpara>vCenter password</simpara>
</listitem>
<listitem>
<simpara>vCenter address</simpara>
</listitem>
<listitem>
<simpara>vSphere data center</simpara>
</listitem>
<listitem>
<simpara>vSphere datastore</simpara>
</listitem>
<listitem>
<simpara>Virtual machine folder</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The Assisted Installer has finished installing the cluster successfully.</simpara>
</listitem>
<listitem>
<simpara>The cluster is connected to <literal><link xlink:href="https://console.redhat.com">https://console.redhat.com</link></literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the Administrator perspective, navigate to <emphasis role="strong">Home &#8594; Overview</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Status</emphasis>, click <emphasis role="strong">vSphere connection</emphasis> to open the <emphasis role="strong">vSphere connection configuration</emphasis> wizard.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">vCenter</emphasis> field, enter the network address of the vSphere vCenter server. This can be either a domain name or an IP address. It appears in the vSphere web client URL; for example <literal>https://[your_vCenter_address]/ui</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">vCenter cluster</emphasis> field, enter the name of the vSphere vCenter cluster where OpenShift Container Platform is installed.</simpara>
<important>
<simpara>This step is mandatory if you installed OpenShift Container Platform 4.13 or later.</simpara>
</important>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Username</emphasis> field, enter your vSphere vCenter username.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Password</emphasis> field, enter your vSphere vCenter password.</simpara>
<warning>
<simpara>The system stores the username and password in the <literal>vsphere-creds</literal> secret in the <literal>kube-system</literal> namespace of the cluster. An incorrect vCenter username or password makes the cluster nodes unschedulable.</simpara>
</warning>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Datacenter</emphasis> field, enter the name of the vSphere data center that contains the virtual machines used to host the cluster; for example, <literal>SDDC-Datacenter</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Default data store</emphasis> field, enter the path and name of the vSphere data store that stores the persistent data volumes; for example, <literal>/SDDC-Datacenter/datastore/datastorename</literal>.</simpara>
<warning>
<simpara>Updating the vSphere data center or default data store after the configuration has been saved detaches any active vSphere <literal>PersistentVolumes</literal>.</simpara>
</warning>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Virtual Machine Folder</emphasis> field, enter the data center folder that contains the virtual machine of the cluster; for example, <literal>/SDDC-Datacenter/vm/ci-ln-hjg4vg2-c61657-t2gzr</literal>. For the OpenShift Container Platform installation to succeed, all virtual machines comprising the cluster must be located in a single data center folder.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save Configuration</emphasis>. This updates the <literal>cloud-provider-config</literal> ConfigMap resource in the <literal>openshift-config</literal> namespace, and starts the configuration process.</simpara>
</listitem>
<listitem>
<simpara>Reopen the <emphasis role="strong">vSphere connection configuration</emphasis> wizard and expand the <emphasis role="strong">Monitored operators</emphasis> panel. Check that the status of the operators is either <emphasis role="strong">Progressing</emphasis> or <emphasis role="strong">Healthy</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-vSphere-monitoring-configuration-completioninstalling-vsphere-post-installation-configuration">
<title>Verifying the configuration</title>
<simpara>The connection configuration process updates operator statuses and control plane nodes. It takes approximately an hour to complete. During the configuration process, the nodes will reboot. Previously bound <literal>PersistentVolumeClaims</literal> objects might become disconnected.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have saved the configuration settings in the <emphasis role="strong">vSphere connection configuration</emphasis> wizard.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check that the configuration process completed successfully:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift Container Platform Administrator perspective, navigate to <emphasis role="strong">Home &#8594; Overview</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Status</emphasis> click <emphasis role="strong">Operators</emphasis>. Wait for all operator statuses to change from  <emphasis role="strong">Progressing</emphasis> to <emphasis role="strong">All succeeded</emphasis>.  A <emphasis role="strong">Failed</emphasis> status indicates that the configuration failed.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Status</emphasis>, click <emphasis role="strong">Control Plane</emphasis>. Wait for the response rate of all Control Pane components to return to 100%. A <emphasis role="strong">Failed</emphasis> control plane component indicates that the configuration failed.</simpara>
</listitem>
</orderedlist>
<simpara>A failure indicates that at least one of the connection settings is incorrect. Change the settings in the <emphasis role="strong">vSphere connection configuration</emphasis> wizard and save the configuration again.</simpara>
</listitem>
<listitem>
<simpara>Check that you are able to bind <literal>PersistentVolumeClaims</literal> objects by performing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>StorageClass</literal> object using the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
 name: vsphere-sc
provisioner: kubernetes.io/vsphere-volume
parameters:
 datastore: YOURVCENTERDATASTORE
 diskformat: thin
reclaimPolicy: Delete
volumeBindingMode: Immediate</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>PersistentVolumeClaims</literal> object using the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
 name: test-pvc
 namespace: openshift-config
 annotations:
   volume.beta.kubernetes.io/storage-provisioner: kubernetes.io/vsphere-volume
 finalizers:
   - kubernetes.io/pvc-protection
spec:
 accessModes:
   - ReadWriteOnce
 resources:
   requests:
    storage: 10Gi
 storageClassName: vsphere-sc
 volumeMode: Filesystem</programlisting>
<simpara>If you are unable to create a <literal>PersistentVolumeClaims</literal> object, you can troubleshoot by navigating to <emphasis role="strong">Storage</emphasis> &#8594; <emphasis role="strong">PersistentVolumeClaims</emphasis> in the <emphasis role="strong">Administrator</emphasis> perspective of the OpenShift Container Platform web console.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>For instructions on creating storage objects, see <link xlink:href="../storage/dynamic-provisioning.xml#dynamic-provisioning">Dynamic provisioning</link>.</simpara>
</section>
</chapter>
<chapter xml:id="post-install-machine-configuration-tasks">
<title>Postinstallation machine configuration tasks</title>

<simpara>There are times when you need to make changes to the operating systems running on OpenShift Container Platform nodes. This can include changing settings for network time service, adding kernel arguments, or configuring journaling in a specific way.</simpara>
<simpara>Aside from a few specialized features, most changes to operating systems on OpenShift Container Platform nodes can be done by creating what are referred to as <literal>MachineConfig</literal> objects that are managed by the Machine Config Operator.</simpara>
<simpara>Tasks in this section describe how to use features of the Machine Config Operator to configure operating system features on OpenShift Container Platform nodes.</simpara>
<important>
<simpara>NetworkManager stores new network configurations to <literal>/etc/NetworkManager/system-connections/</literal> in a key file format.</simpara>
<simpara>Previously, NetworkManager stored new network configurations to <literal>/etc/sysconfig/network-scripts/</literal> in the ifcfg format. Starting with RHEL 9.0, RHEL stores new network configurations at <literal>/etc/NetworkManager/system-connections/</literal> in a key file format. The connections configurations stored to <literal>/etc/sysconfig/network-scripts/</literal> in the old format still work uninterrupted. Modifications in existing profiles continue updating the older files.</simpara>
</important>
<section xml:id="understanding-the-machine-config-operator">
<title>Understanding the Machine Config Operator</title>
<section xml:id="machine-config-operator_post-install-machine-configuration-tasks">
<title>Machine Config Operator</title>
<bridgehead xml:id="_purpose" renderas="sect4">Purpose</bridgehead>
<simpara>The Machine Config Operator manages and applies configuration and updates of the base operating system and container runtime, including everything between the kernel and kubelet.</simpara>
<simpara>There are four components:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>machine-config-server</literal>: Provides Ignition configuration to new machines joining the cluster.</simpara>
</listitem>
<listitem>
<simpara><literal>machine-config-controller</literal>: Coordinates the upgrade of machines to the desired configurations defined by a <literal>MachineConfig</literal> object. Options are provided to control the upgrade for sets of machines individually.</simpara>
</listitem>
<listitem>
<simpara><literal>machine-config-daemon</literal>: Applies new machine configuration during update. Validates and verifies the state of the machine to the requested machine configuration.</simpara>
</listitem>
<listitem>
<simpara><literal>machine-config</literal>: Provides a complete source of machine configuration at installation, first start up, and updates for a machine.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Currently, there is no supported way to block or restrict the machine config server endpoint. The machine config server must be exposed to the network so that newly-provisioned machines, which have no existing configuration or state, are able to fetch their configuration. In this model, the root of trust is the certificate signing requests (CSR) endpoint, which is where the kubelet sends its certificate signing request for approval to join the cluster. Because of this, machine configs should not be used to distribute sensitive information, such as secrets and certificates.</simpara>
<simpara>To ensure that the machine config server endpoints, ports 22623 and 22624, are secured in bare metal scenarios, customers must configure proper network policies.</simpara>
</important>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../networking/openshift_sdn/about-openshift-sdn.xml#about-openshift-sdn">About the OpenShift SDN network plugin</link>.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="_project" renderas="sect4">Project</bridgehead>
<simpara><link xlink:href="https://github.com/openshift/machine-config-operator">openshift-machine-config-operator</link></simpara>
</section>
<section xml:id="machine-config-overview-post-install-machine-configuration-tasks">
<title>Machine config overview</title>
<simpara>The Machine Config Operator (MCO) manages updates to systemd, CRI-O and Kubelet, the kernel, Network Manager and other system features. It also offers a <literal>MachineConfig</literal> CRD that can write configuration files onto the host (see <link xlink:href="https://github.com/openshift/machine-config-operator#machine-config-operator">machine-config-operator</link>). Understanding what MCO does and how it interacts with other components is critical to making advanced, system-level changes to an OpenShift Container Platform cluster. Here are some things you should know about MCO, machine configs, and how they are used:</simpara>
<itemizedlist>
<listitem>
<simpara>Machine configs are processed alphabetically, in lexicographically increasing order, of their name. The render controller uses the first machine config in the list as the base and appends the rest to the base machine config.</simpara>
</listitem>
<listitem>
<simpara>A machine config can make a specific change to a file or service on the operating system of each system representing a pool of OpenShift Container Platform nodes.</simpara>
</listitem>
<listitem>
<simpara>MCO applies changes to operating systems in pools of machines. All OpenShift Container Platform clusters start with worker and control plane node pools. By adding more role labels, you can configure custom pools of nodes. For example, you can set up a custom pool of worker nodes that includes particular hardware features needed by an application. However, examples in this section focus on changes to the default pool types.</simpara>
<important>
<simpara>A node can have multiple labels applied that indicate its type, such as <literal>master</literal> or <literal>worker</literal>, however it can be a member of only a <emphasis role="strong">single</emphasis> machine config pool.</simpara>
</important>
</listitem>
<listitem>
<simpara>After a machine config change, the MCO updates the affected nodes alphabetically by zone, based on the <literal>topology.kubernetes.io/zone</literal> label. If a zone has more than one node, the oldest nodes are updated first. For nodes that do not use zones, such as in bare metal deployments, the nodes are upgraded by age, with the oldest nodes updated first. The MCO updates the number of nodes as specified by the <literal>maxUnavailable</literal> field on the machine configuration pool at a time.</simpara>
</listitem>
<listitem>
<simpara>Some machine configuration must be in place before OpenShift Container Platform is installed to disk. In most cases, this can be accomplished by creating
a machine config that is injected directly into the OpenShift Container Platform installer process, instead of running as a postinstallation machine config. In other cases, you might need to do bare metal installation where you pass kernel arguments at OpenShift Container Platform installer startup, to do such things as setting per-node individual IP addresses or advanced disk partitioning.</simpara>
</listitem>
<listitem>
<simpara>MCO manages items that are set in machine configs. Manual changes you do to your systems will not be overwritten by MCO, unless MCO is explicitly told to manage a conflicting file. In other words, MCO only makes specific updates you request, it does not claim control over the whole node.</simpara>
</listitem>
<listitem>
<simpara>Manual changes to nodes are strongly discouraged. If you need to decommission a node and start a new one, those direct changes would be lost.</simpara>
</listitem>
<listitem>
<simpara>MCO is only supported for writing to files in <literal>/etc</literal> and <literal>/var</literal> directories, although there are symbolic links to some directories that can be writeable by being symbolically linked to one of those areas. The <literal>/opt</literal> and <literal>/usr/local</literal> directories are examples.</simpara>
</listitem>
<listitem>
<simpara>Ignition is the configuration format used in MachineConfigs. See the <link xlink:href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition Configuration Specification v3.2.0</link> for details.</simpara>
</listitem>
<listitem>
<simpara>Although Ignition config settings can be delivered directly at OpenShift Container Platform installation time, and are formatted in the same way that MCO delivers Ignition configs, MCO has no way of seeing what those original Ignition configs are. Therefore, you should wrap Ignition config settings into a machine config before deploying them.</simpara>
</listitem>
<listitem>
<simpara>When a file managed by MCO changes outside of MCO, the Machine Config Daemon (MCD) sets the node as <literal>degraded</literal>. It will not overwrite the
offending file, however, and should continue to operate in a <literal>degraded</literal> state.</simpara>
</listitem>
<listitem>
<simpara>A key reason for using a machine config is that it will be applied when you spin up new nodes for a pool in your OpenShift Container Platform cluster. The <literal>machine-api-operator</literal> provisions a new machine and MCO configures it.</simpara>
</listitem>
</itemizedlist>
<simpara>MCO uses <link xlink:href="https://coreos.github.io/ignition/">Ignition</link> as the configuration format. OpenShift Container Platform 4.6 moved from Ignition config specification version 2 to version 3.</simpara>
<section xml:id="_what_can_you_change_with_machine_configs">
<title>What can you change with machine configs?</title>
<simpara>The kinds of components that MCO can change include:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">config</emphasis>: Create Ignition config objects (see the <link xlink:href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition configuration specification</link>) to do things like modify files, systemd services, and other features on OpenShift Container Platform machines, including:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Configuration files</emphasis>: Create or overwrite files in the <literal>/var</literal> or <literal>/etc</literal> directory.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">systemd units</emphasis>: Create and set the status of a systemd service or add to an existing systemd service by dropping in additional settings.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">users and groups</emphasis>: Change SSH keys in the passwd section postinstallation.</simpara>
<important>
<itemizedlist>
<listitem>
<simpara>Changing SSH keys by using a machine config is supported only for the <literal>core</literal> user.</simpara>
</listitem>
<listitem>
<simpara>Adding new users by using a machine config is not supported.</simpara>
</listitem>
</itemizedlist>
</important>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">kernelArguments</emphasis>: Add arguments to the kernel command line when OpenShift Container Platform nodes boot.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">kernelType</emphasis>: Optionally identify a non-standard kernel to use instead of the standard kernel. Use <literal>realtime</literal> to use the RT kernel (for RAN). This is only supported on select platforms. Use the <literal>64k-pages</literal> parameter to enable the 64k page size kernel. This setting is exclusive to machines with 64-bit ARM architectures.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">fips</emphasis>: Enable <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/security_hardening/index#using-the-system-wide-cryptographic-policies_security-hardening">FIPS</link> mode. FIPS should be set at installation-time setting and not a postinstallation procedure.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>To enable FIPS mode for your cluster, you must run the installation program from a Red Hat Enterprise Linux (RHEL) computer configured to operate in FIPS mode. For more information about configuring FIPS mode on RHEL, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/security_hardening/assembly_installing-the-system-in-fips-mode_security-hardening">Installing the system in FIPS mode</link>. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, OpenShift Container Platform core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">extensions</emphasis>: Extend RHCOS features by adding selected pre-packaged software. For this feature, available extensions include <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/security_hardening/index#protecting-systems-against-intrusive-usb-devices_security-hardening">usbguard</link> and kernel modules.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Custom resources (for <literal>ContainerRuntime</literal> and <literal>Kubelet</literal>)</emphasis>: Outside of machine configs, MCO manages two special custom resources for modifying CRI-O container runtime settings (<literal>ContainerRuntime</literal> CR) and the Kubelet service (<literal>Kubelet</literal> CR).</simpara>
</listitem>
</itemizedlist>
<simpara>The MCO is not the only Operator that can change operating system components on OpenShift Container Platform nodes. Other Operators can modify operating system-level features as well. One example is the Node Tuning Operator, which allows you to do node-level tuning through Tuned daemon profiles.</simpara>
<simpara>Tasks for the MCO configuration that can be done postinstallation are included in the following procedures. See descriptions of RHCOS bare metal installation for system configuration tasks that must be done during or before OpenShift Container Platform installation.</simpara>
<simpara>There might be situations where the configuration on a node does not fully match what the currently-applied machine config specifies. This state is called <emphasis>configuration drift</emphasis>. The Machine Config Daemon (MCD) regularly checks the nodes for configuration drift. If the MCD detects configuration drift, the MCO marks the node <literal>degraded</literal> until an administrator corrects the node configuration. A degraded node is online and operational, but, it cannot be updated. For more information on configuration drift, see <emphasis>Understanding configuration drift detection</emphasis>.</simpara>
</section>
<section xml:id="_project_2">
<title>Project</title>
<simpara>See the <link xlink:href="https://github.com/openshift/machine-config-operator">openshift-machine-config-operator</link> GitHub site for details.</simpara>
</section>
</section>
<section xml:id="machine-config-drift-detection_post-install-machine-configuration-tasks">
<title>Understanding configuration drift detection</title>
<simpara>There might be situations when the on-disk state of a node differs from what is configured in the machine config. This is known as <emphasis>configuration drift</emphasis>. For example, a cluster admin might manually modify a file, a systemd unit file, or a file permission that was configured through a machine config. This causes configuration drift. Configuration drift can cause problems between nodes in a Machine Config Pool or when the machine configs are updated.</simpara>
<simpara>The Machine Config Operator (MCO) uses the Machine Config Daemon (MCD) to check nodes for configuration drift on a regular basis. If detected, the MCO sets the node and the machine config pool (MCP) to <literal>Degraded</literal> and reports the error. A degraded node is online and operational, but, it cannot be updated.</simpara>
<simpara>The MCD performs configuration drift detection upon each of the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>When a node boots.</simpara>
</listitem>
<listitem>
<simpara>After any of the files (Ignition files and systemd drop-in units) specified in the machine config are modified outside of the machine config.</simpara>
</listitem>
<listitem>
<simpara>Before a new machine config is applied.</simpara>
<note>
<simpara>If you apply a new machine config to the nodes, the MCD temporarily shuts down configuration drift detection. This shutdown is needed because the new machine config necessarily differs from the machine config on the nodes. After the new machine config is applied, the MCD restarts detecting configuration drift using the new machine config.</simpara>
</note>
</listitem>
</itemizedlist>
<simpara>When performing configuration drift detection, the MCD validates that the file contents and permissions fully match what the currently-applied machine config specifies. Typically, the MCD detects configuration drift in less than a second after the detection is triggered.</simpara>
<simpara>If the MCD detects configuration drift, the MCD performs the following tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Emits an error to the console logs</simpara>
</listitem>
<listitem>
<simpara>Emits a Kubernetes event</simpara>
</listitem>
<listitem>
<simpara>Stops further detection on the node</simpara>
</listitem>
<listitem>
<simpara>Sets the node and MCP to <literal>degraded</literal></simpara>
</listitem>
</itemizedlist>
<simpara>You can check if you have a degraded node by listing the MCPs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp worker</programlisting>
<simpara>If you have a degraded MCP, the <literal>DEGRADEDMACHINECOUNT</literal> field is non-zero, similar to the following output:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-404caf3180818d8ac1f50c32f14b57c3   False     True       True       2              1                   1                     1                      5h51m</programlisting>
</para>
</formalpara>
<simpara>You can determine if the problem is caused by configuration drift by examining the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe mcp worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"> ...
    Last Transition Time:  2021-12-20T18:54:00Z
    Message:               Node ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4 is reporting: "content mismatch for file \"/etc/mco-test-file\"" <co xml:id="CO30-1"/>
    Reason:                1 nodes are reporting degraded status on sync
    Status:                True
    Type:                  NodeDegraded <co xml:id="CO30-2"/>
 ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO30-1">
<para>This message shows that a node&#8217;s <literal>/etc/mco-test-file</literal> file, which was added by the machine config, has changed outside of the machine config.</para>
</callout>
<callout arearefs="CO30-2">
<para>The state of the node is <literal>NodeDegraded</literal>.</para>
</callout>
</calloutlist>
<simpara>Or, if you know which node is degraded, examine that node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node/ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"> ...

Annotations:        cloud.network.openshift.io/egress-ipconfig: [{"interface":"nic0","ifaddr":{"ipv4":"10.0.128.0/17"},"capacity":{"ip":10}}]
                    csi.volume.kubernetes.io/nodeid:
                      {"pd.csi.storage.gke.io":"projects/openshift-gce-devel-ci/zones/us-central1-a/instances/ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4"}
                    machine.openshift.io/machine: openshift-machine-api/ci-ln-j4h8nkb-72292-pxqxz-worker-a-fjks4
                    machineconfiguration.openshift.io/controlPlaneTopology: HighlyAvailable
                    machineconfiguration.openshift.io/currentConfig: rendered-worker-67bd55d0b02b0f659aef33680693a9f9
                    machineconfiguration.openshift.io/desiredConfig: rendered-worker-67bd55d0b02b0f659aef33680693a9f9
                    machineconfiguration.openshift.io/reason: content mismatch for file "/etc/mco-test-file" <co xml:id="CO31-1"/>
                    machineconfiguration.openshift.io/state: Degraded <co xml:id="CO31-2"/>
 ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO31-1">
<para>The error message indicating that configuration drift was detected between the node and the listed machine config. Here the error message indicates that the contents of the <literal>/etc/mco-test-file</literal>, which was added by the machine config, has changed outside of the machine config.</para>
</callout>
<callout arearefs="CO31-2">
<para>The state of the node is <literal>Degraded</literal>.</para>
</callout>
</calloutlist>
<simpara>You can correct configuration drift and return the node to the <literal>Ready</literal> state by performing one of the following remediations:</simpara>
<itemizedlist>
<listitem>
<simpara>Ensure that the contents and file permissions of the files on the node match what is configured in the machine config. You can manually rewrite the file
contents or change the file permissions.</simpara>
</listitem>
<listitem>
<simpara>Generate a <link xlink:href="https://access.redhat.com/solutions/5414371">force file</link> on the degraded node. The force file causes the MCD to bypass the usual configuration drift detection and reapplies the current machine config.</simpara>
<note>
<simpara>Generating a force file on a node causes that node to reboot.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="checking-mco-status_post-install-machine-configuration-tasks">
<title>Checking machine config pool status</title>
<simpara>To see the status of the Machine Config Operator (MCO), its sub-components, and the resources it manages, use the following <literal>oc</literal> commands:</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To see the number of MCO-managed nodes available on your cluster for each machine config pool (MCP), run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpool</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      CONFIG                    UPDATED  UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT  AGE
master    rendered-master-06c9c4…   True     False      False     3             3                  3                   0                     4h42m
worker    rendered-worker-f4b64…    False    True       False     3             2                  2                   0                     4h42m</programlisting>
</para>
</formalpara>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>UPDATED</term>
<listitem>
<simpara>The <literal>True</literal> status indicates that the MCO has applied the current machine config to the nodes in that MCP. The current machine config is specified in the <literal>STATUS</literal> field in the <literal>oc get mcp</literal> output. The <literal>False</literal> status indicates a node in the MCP is updating.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>UPDATING</term>
<listitem>
<simpara>The <literal>True</literal> status indicates that the MCO is applying the desired machine config, as specified in the <literal>MachineConfigPool</literal> custom resource, to at least one of the nodes in that MCP. The desired machine config is the new, edited machine config. Nodes that are updating might not be available for scheduling. The <literal>False</literal> status indicates that all nodes in the MCP are updated.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>DEGRADED</term>
<listitem>
<simpara>A <literal>True</literal> status indicates the MCO is blocked from applying the current or desired machine config to at least one of the nodes in that MCP, or the configuration is failing. Nodes that are degraded might not be available for scheduling. A <literal>False</literal> status indicates that all nodes in the MCP are ready.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>MACHINECOUNT</term>
<listitem>
<simpara>Indicates the total number of machines in that MCP.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>READYMACHINECOUNT</term>
<listitem>
<simpara>Indicates the total number of machines in that MCP that are ready for scheduling.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>UPDATEDMACHINECOUNT</term>
<listitem>
<simpara>Indicates the total number of machines in that MCP that have the current machine config.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>DEGRADEDMACHINECOUNT</term>
<listitem>
<simpara>Indicates the total number of machines in that MCP that are marked as degraded or unreconcilable.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>In the previous output, there are three control plane (master) nodes and three worker nodes. The control plane MCP and the associated nodes are updated to the current machine config. The nodes in the worker MCP are being updated to the desired machine config. Two of the nodes in the worker MCP are updated and one is still updating, as indicated by the <literal>UPDATEDMACHINECOUNT</literal> being <literal>2</literal>. There are no issues, as indicated by the <literal>DEGRADEDMACHINECOUNT</literal> being <literal>0</literal> and <literal>DEGRADED</literal> being <literal>False</literal>.</simpara>
<simpara>While the nodes in the MCP are updating, the machine config listed under <literal>CONFIG</literal> is the current machine config, which the MCP is being updated from. When the update is complete, the listed machine config is the desired machine config, which the MCP was updated to.</simpara>
<note>
<simpara>If a node is being cordoned, that node is not included in the <literal>READYMACHINECOUNT</literal>, but is included in the <literal>MACHINECOUNT</literal>. Also, the MCP status is set to <literal>UPDATING</literal>. Because the node has the current machine config, it is counted in the <literal>UPDATEDMACHINECOUNT</literal> total:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      CONFIG                    UPDATED  UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT  AGE
master    rendered-master-06c9c4…   True     False      False     3             3                  3                   0                     4h42m
worker    rendered-worker-c1b41a…   False    True       False     3             2                  3                   0                     4h42m</programlisting>
</para>
</formalpara>
</note>
</listitem>
<listitem>
<simpara>To check the status of the nodes in an MCP by examining the <literal>MachineConfigPool</literal> custom resource, run the following command:
:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe mcp worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
  Degraded Machine Count:     0
  Machine Count:              3
  Observed Generation:        2
  Ready Machine Count:        3
  Unavailable Machine Count:  0
  Updated Machine Count:      3
Events:                       &lt;none&gt;</programlisting>
</para>
</formalpara>
<note>
<simpara>If a node is being cordoned, the node is not included in the <literal>Ready Machine Count</literal>. It is included in the <literal>Unavailable Machine Count</literal>:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
  Degraded Machine Count:     0
  Machine Count:              3
  Observed Generation:        2
  Ready Machine Count:        2
  Unavailable Machine Count:  1
  Updated Machine Count:      3</programlisting>
</para>
</formalpara>
</note>
</listitem>
<listitem>
<simpara>To see each existing <literal>MachineConfig</literal> object, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigs</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             GENERATEDBYCONTROLLER          IGNITIONVERSION  AGE
00-master                        2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
00-worker                        2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
01-master-container-runtime      2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
01-master-kubelet                2c9371fbb673b97a6fe8b1c52…     3.2.0            5h18m
...
rendered-master-dde...           2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m
rendered-worker-fde...           2c9371fbb673b97a6fe8b1c52...   3.2.0            5h18m</programlisting>
</para>
</formalpara>
<simpara>Note that the <literal>MachineConfig</literal> objects listed as <literal>rendered</literal> are not meant to be changed or deleted.</simpara>
</listitem>
<listitem>
<simpara>To view the contents of a particular machine config (in this case, <literal>01-master-kubelet</literal>), run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe machineconfigs 01-master-kubelet</programlisting>
<simpara>The output from the command shows that this <literal>MachineConfig</literal> object contains both configuration files (<literal>cloud.conf</literal> and <literal>kubelet.conf</literal>) and a systemd service (Kubernetes Kubelet):</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         01-master-kubelet
...
Spec:
  Config:
    Ignition:
      Version:  3.2.0
    Storage:
      Files:
        Contents:
          Source:   data:,
        Mode:       420
        Overwrite:  true
        Path:       /etc/kubernetes/cloud.conf
        Contents:
          Source:   data:,kind%3A%20KubeletConfiguration%0AapiVersion%3A%20kubelet.config.k8s.io%2Fv1beta1%0Aauthentication%3A%0A%20%20x509%3A%0A%20%20%20%20clientCAFile%3A%20%2Fetc%2Fkubernetes%2Fkubelet-ca.crt%0A%20%20anonymous...
        Mode:       420
        Overwrite:  true
        Path:       /etc/kubernetes/kubelet.conf
    Systemd:
      Units:
        Contents:  [Unit]
Description=Kubernetes Kubelet
Wants=rpc-statd.service network-online.target crio.service
After=network-online.target crio.service

ExecStart=/usr/bin/hyperkube \
    kubelet \
      --config=/etc/kubernetes/kubelet.conf \ ...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<simpara>If something goes wrong with a machine config that you apply, you can always back out that change. For example, if you had run <literal>oc create -f ./myconfig.yaml</literal> to apply a machine config, you could remove that machine config by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -f ./myconfig.yaml</programlisting>
<simpara>If that was the only problem, the nodes in the affected pool should return to a non-degraded state. This actually causes the rendered configuration to roll back to its previously rendered state.</simpara>
<simpara>If you add your own machine configs to your cluster, you can use the commands shown in the previous example to check their status and the related status of the pool to which they are applied.</simpara>
</section>
<section xml:id="checking-mco-node-status_post-install-machine-configuration-tasks">
<title>Checking machine config node status</title>
<simpara>During updates you might want to monitor the progress of individual nodes in case issues arise and you need to troubleshoot a node.</simpara>
<simpara>To see the status of the Machine Config Operator (MCO) updates to your cluster, use the following <literal>oc</literal> commands:</simpara>
<important>
<simpara>Improved MCO state reporting is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get a summary of update statuses for all nodes in all machine config pools by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfignodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME                          UPDATED   UPDATEPREPARED   UPDATEEXECUTED   UPDATEPOSTACTIONCOMPLETED   UPDATECOMPLETED   RESUMED
ip-10-0-12-194.ec2.internal   True      False             False              False                    False              False
ip-10-0-17-102.ec2.internal   False     True              False              False                    False              False
ip-10-0-2-232.ec2.internal    False     False             True               False                    False              False
ip-10-0-59-251.ec2.internal   False     False             False              True                     False              False
ip-10-0-59-56.ec2.internal    False     False             False              False                    True               True
ip-10-0-6-214.ec2.internal    False     False             Unknown            False                    False              False</programlisting>
</para>
</formalpara>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>UPDATED</term>
<listitem>
<simpara>The <literal>True</literal> status indicates that the MCO has applied the current machine config to that particular node. The <literal>False</literal> status indicates that the node is currently updating. The <literal>Unknown</literal> status means the operation is processing.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>UPDATEPREPARED</term>
<listitem>
<simpara>The <literal>False</literal> status indicates that the MCO has not started reconciling the new machine configs to be distributed. The <literal>True</literal> status indicates that the MCO has completed this phase of the update. The <literal>Unknown</literal> status means the operation is processing.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>UPDATEEXECUTED</term>
<listitem>
<simpara>The <literal>False</literal> status indicates that the MCO has not started cordoning and draining the node. It also indicates that the disk state and operating system have not started updating. The <literal>True</literal> status indicates that the MCO has completed this phase of the update. The <literal>Unknown</literal> status means the operation is processing.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>UPDATEPOSTACTIONCOMPLETED</term>
<listitem>
<simpara>The <literal>False</literal> status indicates that the MCO has not started rebooting the node or closing the daemon. The <literal>True</literal> status indicates that the MCO has completed reboot and updating the node status. The <literal>Unknown</literal> status indicates either that an error has occurred during the update process at this phase, or that the MCO is currently applying the update.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>UPDATECOMPLETED</term>
<listitem>
<simpara>The <literal>False</literal> status indicates that the MCO has not started uncordoning the node and updating the node state and metrics. The <literal>True</literal> status indicates that the MCO has finished updating the node state and available metrics.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>RESUMED</term>
<listitem>
<simpara>The <literal>False</literal> status indicates that the MCO has not started the config drift monitor. The <literal>True</literal> status indicates that the node has resumed operation. The <literal>Unknown</literal> status means the operation is processing.</simpara>
<note>
<simpara>Within the primary phases previously described, you can have secondary phases which you can use to see the update progression in more detail. You can get more information that includes secondary phases of updates by using the <literal>-o wide</literal> option of the preceding command. This provides the additional <literal>UPDATECOMPATIBLE</literal>, <literal>UPDATEFILESANDOS</literal>, <literal>DRAINEDNODE</literal>, <literal>CORDONEDNODE</literal>, <literal>REBOOTNODE</literal>, <literal>RELOADEDCRIO</literal> and <literal>UNCORDONED</literal> columns. These secondary phases do not always occur and depend on the type of update you want to apply.</simpara>
</note>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Check the update status of nodes in a specific machine config pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfignodes $(oc get machineconfignodes -o json | jq -r '.items[]|select(.spec.pool.name=="&lt;pool_name&gt;")|.metadata.name') <co xml:id="CO32-1"/></programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>The name of the pool is the <literal>MachineConfigPool</literal> object name.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME                          UPDATED   UPDATEPREPARED   UPDATEEXECUTED   UPDATEPOSTACTIONCOMPLETE   UPDATECOMPLETE   RESUMED
ip-10-0-48-226.ec2.internal   True      False            False            False                      False            False
ip-10-0-5-241.ec2.internal    True      False            False            False                      False            False
ip-10-0-74-108.ec2.internal   True      False            False            False                      False            False</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the update status of an individual node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe machineconfignode/&lt;node_name&gt; <co xml:id="CO33-1"/></programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>The name of the node is the <literal>MachineConfigNode</literal> object name.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name:         &lt;node_name&gt;
Namespace:
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
API Version:  machineconfiguration.openshift.io/v1alpha1
Kind:         MachineConfigNode
Metadata:
  Creation Timestamp:  2023-10-17T13:08:58Z
  Generation:          1
  Resource Version:    49443
  UID:                 4bd758ab-2187-413c-ac42-882e61761b1d
Spec:
  Node Ref:
    Name:         &lt;node_name&gt;
  Pool:
    Name:         master
  ConfigVersion:
    Desired: rendered-worker-823ff8dc2b33bf444709ed7cd2b9855b <co xml:id="CO34-1"/>
Status:
  Conditions:
    Last Transition Time:  2023-10-17T13:09:02Z
    Message:               Node has completed update to config rendered-master-cf99e619747ab19165f11e3546c71f1e
    Reason:                NodeUpgradeComplete
    Status:                True
    Type:                  Updated
    Last Transition Time:  2023-10-17T13:09:02Z
    Message:               This node has not yet entered the UpdatePreparing phase
    Reason:                NotYetOccured
    Status:                False
  Config Version:
    Current:            rendered-worker-823ff8dc2b33bf444709ed7cd2b9855b
    Desired:            rendered-worker-823ff8dc2b33bf444709ed7cd2b9855b <co xml:id="CO34-2"/>
  Health:               Healthy
  Most Recent Error:
  Observed Generation:  3</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO34-1">
<para>The desired configuration specified in the <literal>spec.configversion.desired</literal> field updates immediately when a new configuration is detected on the node.</para>
</callout>
<callout arearefs="CO34-2">
<para>The desired configuration specified in the <literal>status.configversion.desired</literal> field updates only when the new configuration is validated by the Machine Config Daemon (MCD). The MCD performs validation by checking the current phase of the update. If the update successfully passes the <literal>UPDATEPREPARED</literal> phase, then the status adds the new configuration.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="checking-mco-status-certs_post-install-machine-configuration-tasks">
<title>Viewing and interacting with certificates</title>
<simpara>The following certificates are handled in the cluster by the Machine Config Controller (MCC) and can be found in the <literal>ControllerConfig</literal> resource:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>/etc/kubernetes/kubelet-ca.crt</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/kubernetes/static-pod-resources/configmaps/cloud-config/ca-bundle.pem</literal></simpara>
</listitem>
<listitem>
<simpara><literal>/etc/pki/ca-trust/source/anchors/openshift-config-user-ca-bundle.crt</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The MCC also handles the image registry certificates and its associated user bundle certificate.</simpara>
<simpara>You can get information about the listed certificates, including the underyling bundle the certificate comes from, and the signing and subject data.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Get detailed certificate information by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get controllerconfig/machine-config-controller -o yaml | yq -y '.status.controllerCertificates'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">"controllerCertificates": [
                   {
                       "bundleFile": "KubeAPIServerServingCAData",
                       "signer": "&lt;signer_data1&gt;",
                       "subject": "CN=openshift-kube-apiserver-operator_node-system-admin-signer@168909215"
                   },
                   {
                       "bundleFile": "RootCAData",
                       "signer": "&lt;signer_data2&gt;",
                       "subject": "CN=root-ca,OU=openshift"
                   }
                ]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get a simpler version of the information found in the ControllerConfig by checking the machine config pool status using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp master -o yaml | yq -y '.status.certExpirys'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">status:
  certExpirys:
  - bundle: KubeAPIServerServingCAData
    subject: CN=admin-kubeconfig-signer,OU=openshift
  - bundle: KubeAPIServerServingCAData
    subject: CN=kube-csr-signer_@1689585558
  - bundle: KubeAPIServerServingCAData
    subject: CN=kubelet-signer,OU=openshift
  - bundle: KubeAPIServerServingCAData
    subject: CN=kube-apiserver-to-kubelet-signer,OU=openshift
  - bundle: KubeAPIServerServingCAData
    subject: CN=kube-control-plane-signer,OU=openshift</programlisting>
</para>
</formalpara>
<simpara>This method is meant for OpenShift Container Platform applications that already consume machine config pool information.</simpara>
</listitem>
<listitem>
<simpara>Check which image registry certificates are on the nodes by looking at the contents of the <literal>/etc/docker/cert.d</literal> directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># ls /etc/docker/certs.d</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">image-registry.openshift-image-registry.svc.cluster.local:5000 image-registry.openshift-image-registry.svc:5000</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="using-machineconfigs-to-change-machines">
<title>Using MachineConfig objects to configure nodes</title>
<simpara>You can use the tasks in this section to create <literal>MachineConfig</literal> objects that modify files, systemd unit files, and other operating system features running on OpenShift Container Platform nodes. For more ideas on working with machine configs, see content related to <link xlink:href="https://access.redhat.com/solutions/3868301">updating</link> SSH authorized keys, <link xlink:href="https://access.redhat.com/verify-images-ocp4">verifying image signatures</link>, <link xlink:href="https://access.redhat.com/solutions/4727321">enabling SCTP</link>, and <link xlink:href="https://access.redhat.com/solutions/5170251">configuring iSCSI initiatornames</link> for OpenShift Container Platform.</simpara>
<simpara>OpenShift Container Platform supports <link xlink:href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition specification version 3.2</link>. All new machine configs you create going forward should be based on Ignition specification version 3.2. If you are upgrading your OpenShift Container Platform cluster, any existing Ignition specification version 2.x machine configs will be translated automatically to specification version 3.2.</simpara>
<simpara>There might be situations where the configuration on a node does not fully match what the currently-applied machine config specifies. This state is called <emphasis>configuration drift</emphasis>. The Machine Config Daemon (MCD) regularly checks the nodes for configuration drift. If the MCD detects configuration drift, the MCO marks the node <literal>degraded</literal> until an administrator corrects the node configuration. A degraded node is online and operational, but, it cannot be updated. For more information on configuration drift, see <link xlink:href="../post_installation_configuration/machine-configuration-tasks.xml#machine-config-drift-detection_post-install-machine-configuration-tasks">Understanding configuration drift detection</link>.</simpara>
<tip>
<simpara>Use the following "Configuring chrony time service" procedure as a model for how to go about adding other configuration files to OpenShift Container Platform nodes.</simpara>
</tip>
<section xml:id="installation-special-config-chrony_post-install-machine-configuration-tasks">
<title>Configuring chrony time service</title>
<simpara>You
can
set the time server and related settings used by the chrony time service (<literal>chronyd</literal>)
by modifying the contents of the <literal>chrony.conf</literal> file and passing those contents
to your nodes as a machine config.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a Butane config including the contents of the <literal>chrony.conf</literal> file. For example, to configure chrony on worker nodes, create a <literal>99-worker-chrony.bu</literal> file.</simpara>
<note>
<simpara>See "Creating machine configs with Butane" for information about Butane.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">variant: openshift
version: 4.14.0
metadata:
  name: 99-worker-chrony <co xml:id="CO35-1"/>
  labels:
    machineconfiguration.openshift.io/role: worker <co xml:id="CO35-2"/>
storage:
  files:
  - path: /etc/chrony.conf
    mode: 0644 <co xml:id="CO35-3"/>
    overwrite: true
    contents:
      inline: |
        pool 0.rhel.pool.ntp.org iburst <co xml:id="CO35-4"/>
        driftfile /var/lib/chrony/drift
        makestep 1.0 3
        rtcsync
        logdir /var/log/chrony</programlisting>
<calloutlist>
<callout arearefs="CO35-1 CO35-2">
<para>On control plane nodes, substitute <literal>master</literal> for <literal>worker</literal> in both of these locations.</para>
</callout>
<callout arearefs="CO35-3">
<para>Specify an octal value mode for the <literal>mode</literal> field in the machine config file. After creating the file and applying the changes, the <literal>mode</literal> is converted to a decimal value. You can check the YAML file with the command <literal>oc get mc &lt;mc-name&gt; -o yaml</literal>.</para>
</callout>
<callout arearefs="CO35-4">
<para>Specify any valid, reachable time source, such as the one provided by your DHCP server.
Alternately, you can specify any of the following NTP servers: <literal>1.rhel.pool.ntp.org</literal>, <literal>2.rhel.pool.ntp.org</literal>, or <literal>3.rhel.pool.ntp.org</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Use Butane to generate a <literal>MachineConfig</literal> object file, <literal>99-worker-chrony.yaml</literal>, containing the configuration to be delivered to the nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ butane 99-worker-chrony.bu -o 99-worker-chrony.yaml</programlisting>
</listitem>
<listitem>
<simpara>Apply the configurations in one of two ways:</simpara>
<itemizedlist>
<listitem>
<simpara>If the cluster is not running yet, after you generate manifest files, add the <literal>MachineConfig</literal> object file to the <literal>&lt;installation_directory&gt;/openshift</literal> directory, and then continue to create the cluster.</simpara>
</listitem>
<listitem>
<simpara>If the cluster is already running, apply the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f ./99-worker-chrony.yaml</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/install_config/installing-customizing.xml#installation-special-config-butane_installing-customizing">Creating machine configs with Butane</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cnf-disable-chronyd_post-install-machine-configuration-tasks">
<title>Disabling the chrony time service</title>
<simpara>You can disable the chrony time service (<literal>chronyd</literal>) for nodes with a specific role by using a <literal>MachineConfig</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>MachineConfig</literal> CR that disables <literal>chronyd</literal> for the specified node role.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Save the following YAML in the <literal>disable-chronyd.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: &lt;node_role&gt; <co xml:id="CO36-1"/>
  name: disable-chronyd
spec:
  config:
    ignition:
      version: 3.2.0
    systemd:
      units:
        - contents: |
            [Unit]
            Description=NTP client/server
            Documentation=man:chronyd(8) man:chrony.conf(5)
            After=ntpdate.service sntp.service ntpd.service
            Conflicts=ntpd.service systemd-timesyncd.service
            ConditionCapability=CAP_SYS_TIME
            [Service]
            Type=forking
            PIDFile=/run/chrony/chronyd.pid
            EnvironmentFile=-/etc/sysconfig/chronyd
            ExecStart=/usr/sbin/chronyd $OPTIONS
            ExecStartPost=/usr/libexec/chrony-helper update-daemon
            PrivateTmp=yes
            ProtectHome=yes
            ProtectSystem=full
            [Install]
            WantedBy=multi-user.target
          enabled: false
          name: "chronyd.service"</programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>Node role where you want to disable <literal>chronyd</literal>, for example, <literal>master</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>MachineConfig</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f disable-chronyd.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-nodes-kernel-arguments_post-install-machine-configuration-tasks">
<title>Adding kernel arguments to nodes</title>
<simpara>In some special cases, you might want to add kernel arguments to a set of nodes in your cluster. This should only be done with caution and clear understanding of the implications of the arguments you set.</simpara>
<warning>
<simpara>Improper use of kernel arguments can result in your systems becoming unbootable.</simpara>
</warning>
<simpara>Examples of kernel arguments you could set include:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">nosmt</emphasis>: Disables symmetric multithreading (SMT) in the kernel. Multithreading allows multiple logical threads for each CPU. You could consider <literal>nosmt</literal> in multi-tenant environments to reduce risks from potential cross-thread attacks. By disabling SMT, you essentially choose security over performance.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">systemd.unified_cgroup_hierarchy</emphasis>: Enables <link xlink:href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</link> (cgroup v2). cgroup v2 is the next version of the kernel <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01">control group</link> and offers multiple improvements.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">enforcing=0</emphasis>: Configures Security Enhanced Linux (SELinux) to run in permissive mode. In permissive mode, the system acts as if SELinux is enforcing the loaded security policy, including labeling objects and emitting access denial entries in the logs, but it does not actually deny any operations. While not supported for production systems, permissive mode can be helpful for debugging.</simpara>
<warning>
<simpara>Disabling SELinux on RHCOS in production is not supported.
Once SELinux has been disabled on a node, it must be re-provisioned before re-inclusion in a production cluster.</simpara>
</warning>
</listitem>
</itemizedlist>
<simpara>See <link xlink:href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.txt">Kernel.org kernel parameters</link> for a list and descriptions of kernel arguments.</simpara>
<simpara>In the following procedure, you create a <literal>MachineConfig</literal> object that identifies:</simpara>
<itemizedlist>
<listitem>
<simpara>A set of machines to which you want to add the kernel argument. In this case, machines with a worker role.</simpara>
</listitem>
<listitem>
<simpara>Kernel arguments that are appended to the end of the existing kernel arguments.</simpara>
</listitem>
<listitem>
<simpara>A label that indicates where in the list of machine configs the change is applied.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Have administrative privilege to a working OpenShift Container Platform cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List existing <literal>MachineConfig</literal> objects for your OpenShift Container Platform cluster to determine how to
label your machine config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get MachineConfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>MachineConfig</literal> object file that identifies the kernel argument (for example, <literal>05-worker-kernelarg-selinuxpermissive.yaml</literal>)</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker<co xml:id="CO37-1"/>
  name: 05-worker-kernelarg-selinuxpermissive<co xml:id="CO37-2"/>
spec:
  kernelArguments:
    - enforcing=0<co xml:id="CO37-3"/></programlisting>
<calloutlist>
<callout arearefs="CO37-1">
<para>Applies the new kernel argument only to worker nodes.</para>
</callout>
<callout arearefs="CO37-2">
<para>Named to identify where it fits among the machine configs (05) and what it does (adds
a kernel argument to configure SELinux permissive mode).</para>
</callout>
<callout arearefs="CO37-3">
<para>Identifies the exact kernel argument as <literal>enforcing=0</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the new machine config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f 05-worker-kernelarg-selinuxpermissive.yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the machine configs to see that the new one was added:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get MachineConfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
05-worker-kernelarg-selinuxpermissive                                                         3.2.0             105s
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.28.5
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.28.5
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.28.5
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.28.5
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.28.5
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.28.5</programlisting>
</para>
</formalpara>
<simpara>You can see that scheduling on each worker node is disabled as the change is being applied.</simpara>
</listitem>
<listitem>
<simpara>Check that the kernel argument worked by going to one of the worker nodes and listing
the kernel command line arguments (in <literal>/proc/cmdline</literal> on the host):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-141-105.ec2.internal</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Starting pod/ip-10-0-141-105ec2internal-debug ...
To use host binaries, run `chroot /host`

sh-4.2# cat /host/proc/cmdline
BOOT_IMAGE=/ostree/rhcos-... console=tty0 console=ttyS0,115200n8
rootflags=defaults,prjquota rw root=UUID=fd0... ostree=/ostree/boot.0/rhcos/16...
coreos.oem.id=qemu coreos.oem.id=ec2 ignition.platform.id=ec2 enforcing=0

sh-4.2# exit</programlisting>
</para>
</formalpara>
<simpara>You should see the <literal>enforcing=0</literal> argument added to the other kernel arguments.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="rhcos-enabling-multipath-day-2_post-install-machine-configuration-tasks">
<title>Enabling multipathing with kernel arguments on RHCOS</title>
<simpara>Red Hat Enterprise Linux CoreOS (RHCOS) supports multipathing on the primary disk, allowing stronger resilience to hardware failure to achieve higher host availability. Postinstallation support is available by activating multipathing via the machine config.</simpara>
<important>
<simpara>Enabling multipathing during installation is supported and recommended for nodes provisioned in OpenShift Container Platform 4.8 or higher. In setups where any I/O to non-optimized paths results in I/O system errors, you must enable multipathing at installation time. For more information about enabling multipathing during installation time, see "Enabling multipathing with kernel arguments on RHCOS" in the <emphasis>Installing on bare metal</emphasis> documentation.</simpara>
</important>
<important>
<simpara>On IBM Z&#174; and IBM&#174; LinuxONE, you can enable multipathing only if you configured your cluster for it during installation. For more information, see "Installing RHCOS and starting the OpenShift Container Platform bootstrap process" in <emphasis>Installing a cluster with z/VM on IBM Z&#174; and IBM&#174; LinuxONE</emphasis>.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have a running OpenShift Container Platform cluster that uses version 4.7 or later.</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster as a user with administrative privileges.</simpara>
</listitem>
<listitem>
<simpara>You have confirmed that the disk is enabled for multipathing. Multipathing is only supported on hosts that are connected to a SAN via an HBA adapter.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To enable multipathing postinstallation on control plane nodes:</simpara>
<itemizedlist>
<listitem>
<simpara>Create a machine config file, such as <literal>99-master-kargs-mpath.yaml</literal>, that instructs the cluster to add the <literal>master</literal> label and that identifies the multipath kernel argument, for example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "master"
  name: 99-master-kargs-mpath
spec:
  kernelArguments:
    - 'rd.multipath=default'
    - 'root=/dev/disk/by-label/dm-mpath-root'</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To enable multipathing postinstallation on worker nodes:</simpara>
<itemizedlist>
<listitem>
<simpara>Create a machine config file, such as <literal>99-worker-kargs-mpath.yaml</literal>, that instructs the cluster to add the <literal>worker</literal> label and that identifies the multipath kernel argument, for example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "worker"
  name: 99-worker-kargs-mpath
spec:
  kernelArguments:
    - 'rd.multipath=default'
    - 'root=/dev/disk/by-label/dm-mpath-root'</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the new machine config by using either the master or worker YAML file you previously created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ./99-worker-kargs-mpath.yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the machine configs to see that the new one was added:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get MachineConfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-kargs-mpath                              52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             105s
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS                     ROLES    AGE   VERSION
ip-10-0-136-161.ec2.internal   Ready                      worker   28m   v1.28.5
ip-10-0-136-243.ec2.internal   Ready                      master   34m   v1.28.5
ip-10-0-141-105.ec2.internal   Ready,SchedulingDisabled   worker   28m   v1.28.5
ip-10-0-142-249.ec2.internal   Ready                      master   34m   v1.28.5
ip-10-0-153-11.ec2.internal    Ready                      worker   28m   v1.28.5
ip-10-0-153-150.ec2.internal   Ready                      master   34m   v1.28.5</programlisting>
</para>
</formalpara>
<simpara>You can see that scheduling on each worker node is disabled as the change is being applied.</simpara>
</listitem>
<listitem>
<simpara>Check that the kernel argument worked by going to one of the worker nodes and listing
the kernel command line arguments (in <literal>/proc/cmdline</literal> on the host):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-141-105.ec2.internal</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Starting pod/ip-10-0-141-105ec2internal-debug ...
To use host binaries, run `chroot /host`

sh-4.2# cat /host/proc/cmdline
...
rd.multipath=default root=/dev/disk/by-label/dm-mpath-root
...

sh-4.2# exit</programlisting>
</para>
</formalpara>
<simpara>You should see the added kernel arguments.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="../installing/installing_bare_metal/installing-bare-metal.xml#rhcos-enabling-multipath_installing-bare-metal">Enabling multipathing with kernel arguments on RHCOS</link> for more information about enabling multipathing during installation time.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nodes-nodes-rtkernel-arguments_post-install-machine-configuration-tasks">
<title>Adding a real-time kernel to nodes</title>
<simpara>Some OpenShift Container Platform workloads require a high degree of determinism.While Linux is not a real-time operating system, the Linux real-time
kernel includes a preemptive scheduler that provides the operating system with real-time characteristics.</simpara>
<simpara>If your OpenShift Container Platform workloads require these real-time characteristics, you can switch your machines to the Linux real-time kernel. For OpenShift Container Platform, 4.14 you can make this switch using a <literal>MachineConfig</literal> object. Although making the change is as simple as changing a machine config <literal>kernelType</literal> setting to <literal>realtime</literal>, there are a few other considerations before making the change:</simpara>
<itemizedlist>
<listitem>
<simpara>Currently, real-time kernel is supported only on worker nodes, and only for radio access network (RAN) use.</simpara>
</listitem>
<listitem>
<simpara>The following procedure is fully supported with bare metal installations that use systems that are certified for Red Hat Enterprise Linux for Real Time 8.</simpara>
</listitem>
<listitem>
<simpara>Real-time support in OpenShift Container Platform is limited to specific subscriptions.</simpara>
</listitem>
<listitem>
<simpara>The following procedure is also supported for use with Google Cloud Platform.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Have a running OpenShift Container Platform cluster (version 4.4 or later).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster as a user with administrative privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a machine config for the real-time kernel: Create a YAML file (for example, <literal>99-worker-realtime.yaml</literal>) that contains a <literal>MachineConfig</literal>
object for the <literal>realtime</literal> kernel type. This example tells the cluster to use a real-time kernel for all worker nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; 99-worker-realtime.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: "worker"
  name: 99-worker-realtime
spec:
  kernelType: realtime
EOF</programlisting>
</listitem>
<listitem>
<simpara>Add the machine config to the cluster. Type the following to add the machine config to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f 99-worker-realtime.yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the real-time kernel: Once each impacted node reboots, log in to the cluster and run the following commands to make sure that the real-time kernel has replaced the regular kernel for the set of nodes you configured:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        STATUS  ROLES    AGE   VERSION
ip-10-0-143-147.us-east-2.compute.internal  Ready   worker   103m  v1.28.5
ip-10-0-146-92.us-east-2.compute.internal   Ready   worker   101m  v1.28.5
ip-10-0-169-2.us-east-2.compute.internal    Ready   worker   102m  v1.28.5</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-143-147.us-east-2.compute.internal</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Starting pod/ip-10-0-143-147us-east-2computeinternal-debug ...
To use host binaries, run `chroot /host`

sh-4.4# uname -a
Linux &lt;worker_node&gt; 4.18.0-147.3.1.rt24.96.el8_1.x86_64 #1 SMP PREEMPT RT
        Wed Nov 27 18:29:55 UTC 2019 x86_64 x86_64 x86_64 GNU/Linux</programlisting>
</para>
</formalpara>
<simpara>The kernel name contains <literal>rt</literal> and text “PREEMPT RT” indicates that this is a real-time kernel.</simpara>
</listitem>
<listitem>
<simpara>To go back to the regular kernel, delete the <literal>MachineConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -f 99-worker-realtime.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="machineconfig-modify-journald_post-install-machine-configuration-tasks">
<title>Configuring journald settings</title>
<simpara>If you need to configure settings for the <literal>journald</literal> service on OpenShift Container Platform nodes, you can do that by modifying the appropriate configuration file and passing the file to the appropriate pool of nodes as a machine config.</simpara>
<simpara>This procedure describes how to modify <literal>journald</literal> rate limiting settings in the <literal>/etc/systemd/journald.conf</literal> file and apply them to worker nodes. See the <literal>journald.conf</literal> man page for information on how to use that file.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Have a running OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster as a user with administrative privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a Butane config file, <literal>40-worker-custom-journald.bu</literal>, that includes an <literal>/etc/systemd/journald.conf</literal> file with the required settings.</simpara>
<note>
<simpara>See "Creating machine configs with Butane" for information about Butane.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">variant: openshift
version: 4.14.0
metadata:
  name: 40-worker-custom-journald
  labels:
    machineconfiguration.openshift.io/role: worker
storage:
  files:
  - path: /etc/systemd/journald.conf
    mode: 0644
    overwrite: true
    contents:
      inline: |
        # Disable rate limiting
        RateLimitInterval=1s
        RateLimitBurst=10000
        Storage=volatile
        Compress=no
        MaxRetentionSec=30s</programlisting>
</listitem>
<listitem>
<simpara>Use Butane to generate a <literal>MachineConfig</literal> object file, <literal>40-worker-custom-journald.yaml</literal>, containing the configuration to be delivered to the worker nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ butane 40-worker-custom-journald.bu -o 40-worker-custom-journald.yaml</programlisting>
</listitem>
<listitem>
<simpara>Apply the machine config to the pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f 40-worker-custom-journald.yaml</programlisting>
</listitem>
<listitem>
<simpara>Check that the new machine config is applied and that the nodes are not in a degraded state. It might take a few minutes. The worker pool will show the updates in progress, as each node successfully has the new machine config applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpool
NAME   CONFIG             UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE
master rendered-master-35 True    False    False    3            3                 3                   0                    34m
worker rendered-worker-d8 False   True     False    3            1                 1                   0                    34m</programlisting>
</listitem>
<listitem>
<simpara>To check that the change was applied, you can log in to a worker node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get node | grep worker
ip-10-0-0-1.us-east-2.compute.internal   Ready    worker   39m   v0.0.0-master+$Format:%h$
$ oc debug node/ip-10-0-0-1.us-east-2.compute.internal
Starting pod/ip-10-0-141-142us-east-2computeinternal-debug ...
...
sh-4.2# chroot /host
sh-4.4# cat /etc/systemd/journald.conf
# Disable rate limiting
RateLimitInterval=1s
RateLimitBurst=10000
Storage=volatile
Compress=no
MaxRetentionSec=30s
sh-4.4# exit</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/install_config/installing-customizing.xml#installation-special-config-butane_installing-customizing">Creating machine configs with Butane</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="rhcos-add-extensions_post-install-machine-configuration-tasks">
<title>Adding extensions to RHCOS</title>
<simpara>RHCOS is a minimal container-oriented RHEL operating system, designed to provide a common set of capabilities to OpenShift Container Platform clusters across all platforms. While adding software packages to RHCOS systems is generally discouraged, the MCO provides an <literal>extensions</literal> feature you can use to add a minimal set of features to RHCOS nodes.</simpara>
<simpara>Currently, the following extensions are available:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">usbguard</emphasis>: Adding the <literal>usbguard</literal> extension protects RHCOS systems from attacks from intrusive USB devices. See <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/security_hardening/index#usbguard_protecting-systems-against-intrusive-usb-devices">USBGuard</link> for details.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">kerberos</emphasis>: Adding the <literal>kerberos</literal> extension provides a mechanism that allows both users and machines to identify themselves to the network to receive defined, limited access to the areas and services that an administrator has configured. See <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/system-level_authentication_guide/using_kerberos">Using Kerberos</link> for details, including how to set up a Kerberos client and mount a Kerberized NFS share.</simpara>
</listitem>
</itemizedlist>
<simpara>The following procedure describes how to use a machine config to add one or more extensions to your RHCOS nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Have a running OpenShift Container Platform cluster (version 4.6 or later).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster as a user with administrative privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a machine config for extensions: Create a YAML file (for example, <literal>80-extensions.yaml</literal>) that contains a <literal>MachineConfig</literal> <literal>extensions</literal> object. This example tells the cluster to add the <literal>usbguard</literal> extension.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; 80-extensions.yaml
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 80-worker-extensions
spec:
  config:
    ignition:
      version: 3.2.0
  extensions:
    - usbguard
EOF</programlisting>
</listitem>
<listitem>
<simpara>Add the machine config to the cluster. Type the following to add the machine config to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f 80-extensions.yaml</programlisting>
<simpara>This sets all worker nodes to have rpm packages for <literal>usbguard</literal> installed.</simpara>
</listitem>
<listitem>
<simpara>Check that the extensions were applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig 80-worker-extensions</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                 GENERATEDBYCONTROLLER IGNITIONVERSION AGE
80-worker-extensions                       3.2.0           57s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the new machine config is now applied and that the nodes are not in a degraded state. It may take a few minutes. The worker pool will show the updates in progress, as each machine successfully has the new machine config applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpool</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME   CONFIG             UPDATED UPDATING DEGRADED MACHINECOUNT READYMACHINECOUNT UPDATEDMACHINECOUNT DEGRADEDMACHINECOUNT AGE
master rendered-master-35 True    False    False    3            3                 3                   0                    34m
worker rendered-worker-d8 False   True     False    3            1                 1                   0                    34m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the extensions. To check that the extension was applied, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get node | grep worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        STATUS  ROLES    AGE   VERSION
ip-10-0-169-2.us-east-2.compute.internal    Ready   worker   102m  v1.28.5</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-169-2.us-east-2.compute.internal</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
To use host binaries, run `chroot /host`
sh-4.4# chroot /host
sh-4.4# rpm -q usbguard
usbguard-0.7.4-4.el8.x86_64.rpm</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="rhcos-load-firmware-blobs_post-install-machine-configuration-tasks">
<title>Loading custom firmware blobs in the machine config manifest</title>
<simpara>Because the default location for firmware blobs in <literal>/usr/lib</literal> is read-only, you can locate a custom firmware blob by updating the search path. This enables you to load local firmware blobs in the machine config manifest when the blobs are not managed by RHCOS.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a Butane config file, <literal>98-worker-firmware-blob.bu</literal>, that updates the search path so that it is root-owned and writable to local storage. The following example places the custom blob file from your local workstation onto nodes under <literal>/var/lib/firmware</literal>.</simpara>
<note>
<simpara>See "Creating machine configs with Butane" for information about Butane.</simpara>
</note>
<formalpara>
<title>Butane config file for custom firmware blob</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">variant: openshift
version: 4.14.0
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 98-worker-firmware-blob
storage:
  files:
  - path: /var/lib/firmware/&lt;package_name&gt; <co xml:id="CO38-1"/>
    contents:
      local: &lt;package_name&gt; <co xml:id="CO38-2"/>
    mode: 0644 <co xml:id="CO38-3"/>
openshift:
  kernel_arguments:
    - 'firmware_class.path=/var/lib/firmware' <co xml:id="CO38-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO38-1">
<para>Sets the path on the node where the firmware package is copied to.</para>
</callout>
<callout arearefs="CO38-2">
<para>Specifies a file with contents that are read from a local file directory on the system running Butane. The path of the local file is relative to a <literal>files-dir</literal> directory, which must be specified by using the <literal>--files-dir</literal> option with Butane in the following step.</para>
</callout>
<callout arearefs="CO38-3">
<para>Sets the permissions for the file on the RHCOS node. It is recommended to set <literal>0644</literal> permissions.</para>
</callout>
<callout arearefs="CO38-4">
<para>The <literal>firmware_class.path</literal> parameter customizes the kernel search path of where to look for the custom firmware blob that was copied from your local workstation onto the root file system of the node. This example uses <literal>/var/lib/firmware</literal> as the customized path.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Run Butane to generate a <literal>MachineConfig</literal> object file that uses a copy of the firmware blob on your local workstation named <literal>98-worker-firmware-blob.yaml</literal>. The firmware blob contains the configuration to be delivered to the nodes. The following example uses the <literal>--files-dir</literal> option to specify the directory on your workstation where the local file or files are located:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ butane 98-worker-firmware-blob.bu -o 98-worker-firmware-blob.yaml --files-dir &lt;directory_including_package_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Apply the configurations to the nodes in one of two ways:</simpara>
<itemizedlist>
<listitem>
<simpara>If the cluster is not running yet, after you generate manifest files, add the <literal>MachineConfig</literal> object file to the <literal>&lt;installation_directory&gt;/openshift</literal> directory, and then continue to create the cluster.</simpara>
</listitem>
<listitem>
<simpara>If the cluster is already running, apply the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f 98-worker-firmware-blob.yaml</programlisting>
<simpara>A <literal>MachineConfig</literal> object YAML file is created for you to finish configuring your machines.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the Butane config in case you need to update the <literal>MachineConfig</literal> object in the future.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/install_config/installing-customizing.xml#installation-special-config-butane_installing-customizing">Creating machine configs with Butane</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="core-user-password_post-install-machine-configuration-tasks">
<title>Changing the core user password for node access</title>
<simpara>By default, Red Hat Enterprise Linux CoreOS (RHCOS) creates a user named <literal>core</literal> on the nodes in your cluster. You can use the <literal>core</literal> user to access the node through a cloud provider serial console or a bare metal baseboard controller manager (BMC). This can be helpful, for example, if a node is down and you cannot access that node by using SSH or the <literal>oc debug node</literal> command. However, by default, there is no password for this user, so you cannot log in without creating one.</simpara>
<simpara>You can create a password for the <literal>core</literal> user by using a machine config. The Machine Config Operator (MCO) assigns the password and injects the password into the <literal>/etc/shadow</literal> file, allowing you to log in with the <literal>core</literal> user. The MCO does not examine the password hash. As such, the MCO cannot report if there is a problem with the password.</simpara>
<note>
<itemizedlist>
<listitem>
<simpara>The password works only through a cloud provider serial console or a BMC. It does not work with SSH.</simpara>
</listitem>
<listitem>
<simpara>If you have a machine config that includes an <literal>/etc/shadow</literal> file or a systemd unit that sets a password, it takes precedence over the password hash.</simpara>
</listitem>
</itemizedlist>
</note>
<simpara>You can change the password, if needed, by editing the machine config you used to create the password. Also, you can remove the password by deleting the machine config. Deleting the machine config does not remove the user account.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Create a hashed password by using a tool that is supported by your operating system.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a machine config file that contains the <literal>core</literal> username and the hashed password:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: set-core-user-password
spec:
  config:
    ignition:
      version: 3.2.0
    passwd:
      users:
      - name: core <co xml:id="CO39-1"/>
        passwordHash: &lt;password&gt; <co xml:id="CO39-2"/></programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>This must be <literal>core</literal>.</para>
</callout>
<callout arearefs="CO39-2">
<para>The hashed password to use with the <literal>core</literal> account.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the machine config by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;.yaml</programlisting>
<simpara>The nodes do not reboot and should become available in a few moments. You can use the <literal>oc get mcp</literal> to watch for the machine config pools to be updated, as shown in the following example:</simpara>
<screen>NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-d686a3ffc8fdec47280afec446fce8dd   True      False      False      3              3                   3                     0                      64m
worker   rendered-worker-4605605a5b1f9de1d061e9d350f251e5   False     True       False      3              0                   0                     0                      64m</screen>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>After the nodes return to the <literal>UPDATED=True</literal> state, start a debug session for a node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Check the contents of the <literal>/etc/shadow</literal> file:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
core:$6$2sE/010goDuRSxxv$o18K52wor.wIwZp:19418:0:99999:7:::
...</programlisting>
</para>
</formalpara>
<simpara>The hashed password is assigned to the <literal>core</literal> user.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-machines-with-custom-resources">
<title>Configuring MCO-related custom resources</title>
<simpara>Besides managing <literal>MachineConfig</literal> objects, the MCO manages two custom resources (CRs): <literal>KubeletConfig</literal> and <literal>ContainerRuntimeConfig</literal>. Those CRs let you change node-level settings impacting how the Kubelet and CRI-O container runtime services behave.</simpara>
<section xml:id="create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-machine-configuration-tasks">
<title>Creating a KubeletConfig CRD to edit kubelet parameters</title>
<simpara>The kubelet configuration is currently serialized as an Ignition configuration, so it can be directly edited. However, there is also a new <literal>kubelet-config-controller</literal> added to the Machine Config Controller (MCC). This lets you use a <literal>KubeletConfig</literal> custom resource (CR) to edit the kubelet parameters.</simpara>
<note>
<simpara>As the fields in the <literal>kubeletConfig</literal> object are passed directly to the kubelet from upstream Kubernetes, the kubelet validates those values directly. Invalid values in the <literal>kubeletConfig</literal> object might cause cluster nodes to become unavailable. For valid values, see the <link xlink:href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">Kubernetes documentation</link>.</simpara>
</note>
<simpara>Consider the following guidance:</simpara>
<itemizedlist>
<listitem>
<simpara>Create one <literal>KubeletConfig</literal> CR for each machine config pool with all the config changes you want for that pool. If you are applying the same content to all of the pools, you need only one <literal>KubeletConfig</literal> CR for all of the pools.</simpara>
</listitem>
<listitem>
<simpara>Edit an existing <literal>KubeletConfig</literal> CR to modify existing settings or add new settings, instead of creating a CR for each change. It is recommended that you create a CR only to modify a different machine config pool, or for changes that are intended to be temporary, so that you can revert the changes.</simpara>
</listitem>
<listitem>
<simpara>As needed, create multiple <literal>KubeletConfig</literal> CRs with a limit of 10 per cluster. For the first <literal>KubeletConfig</literal> CR, the Machine Config Operator (MCO) creates a machine config appended with <literal>kubelet</literal>. With each subsequent CR, the controller creates another <literal>kubelet</literal> machine config with a numeric suffix. For example, if you have a <literal>kubelet</literal> machine config with a <literal>-2</literal> suffix, the next <literal>kubelet</literal> machine config is appended with <literal>-3</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want to delete the machine configs, delete them in reverse order to avoid exceeding the limit. For example, you delete the <literal>kubelet-3</literal> machine config before deleting the <literal>kubelet-2</literal> machine config.</simpara>
<note>
<simpara>If you have a machine config with a <literal>kubelet-9</literal> suffix, and you create another <literal>KubeletConfig</literal> CR, a new machine config is not created, even if there are fewer than 10 <literal>kubelet</literal> machine configs.</simpara>
</note>
<formalpara>
<title>Example <literal>KubeletConfig</literal> CR</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfig</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                AGE
set-max-pods        15m</programlisting>
<formalpara>
<title>Example showing a <literal>KubeletConfig</literal> machine config</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mc | grep kubelet</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">...
99-worker-generated-kubelet-1                  b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             26m
...</programlisting>
<simpara>The following procedure is an example to show how to configure the maximum number of pods per node on the worker nodes.</simpara>
<orderedlist numeration="arabic">
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the label associated with the static <literal>MachineConfigPool</literal> CR for the type of node you want to configure.
Perform one of the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>View the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe machineconfigpool &lt;name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe machineconfigpool worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: set-max-pods <co xml:id="CO40-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO40-1">
<para>If a label has been added it appears under <literal>labels</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If the label is not present, add a key/value pair:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the available machine configuration objects that you can select:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig</programlisting>
<simpara>By default, the two kubelet-related configs are <literal>01-master-kubelet</literal> and <literal>01-worker-kubelet</literal>.</simpara>
</listitem>
<listitem>
<simpara>Check the current value for the maximum pods per node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node ci-ln-5grqprb-f76d1-ncnqq-worker-a-mdv94</programlisting>
<simpara>Look for <literal>value: pods: &lt;value&gt;</literal> in the <literal>Allocatable</literal> stanza:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Allocatable:
 attachable-volumes-aws-ebs:  25
 cpu:                         3500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      15341844Ki
 pods:                        250</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Set the maximum pods per node on the worker nodes by creating a custom resource file that contains the kubelet configuration:</simpara>
<important>
<simpara>Kubelet configurations that target a specific machine config pool also affect any dependent pools. For example, creating a kubelet configuration for the pool containing worker nodes will also apply to any subset pools, including the pool containing infrastructure nodes. To avoid this, you must create a new machine config pool with a selection expression that only includes worker nodes, and have your kubelet configuration target this new pool.</simpara>
</important>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods <co xml:id="CO41-1"/>
  kubeletConfig:
    maxPods: 500 <co xml:id="CO41-2"/></programlisting>
<calloutlist>
<callout arearefs="CO41-1">
<para>Enter the label from the machine config pool.</para>
</callout>
<callout arearefs="CO41-2">
<para>Add the kubelet configuration. In this example, use <literal>maxPods</literal> to set the maximum pods per node.</para>
</callout>
</calloutlist>
<note>
<simpara>The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values, <literal>50</literal> for <literal>kubeAPIQPS</literal> and <literal>100</literal> for <literal>kubeAPIBurst</literal>, are sufficient if there are limited pods running on each node. It is recommended to update the kubelet QPS and burst rates if there are enough CPU and memory resources on the node.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
  kubeletConfig:
    maxPods: &lt;pod_count&gt;
    kubeAPIBurst: &lt;burst_rate&gt;
    kubeAPIQPS: &lt;QPS&gt;</programlisting>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Update the machine config pool for workers with the label:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>KubeletConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f change-maxPods-cr.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>KubeletConfig</literal> object is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                AGE
set-max-pods        15m</programlisting>
</para>
</formalpara>
<simpara>Depending on the number of worker nodes in the cluster, wait for the worker nodes to be rebooted one by one. For a cluster with 3 worker nodes, this could take about 10 to 15 minutes.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the changes are applied to the node:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check on a worker node that the <literal>maxPods</literal> value changed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Locate the <literal>Allocatable</literal> stanza:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> ...
Allocatable:
  attachable-volumes-gce-pd:  127
  cpu:                        3500m
  ephemeral-storage:          123201474766
  hugepages-1Gi:              0
  hugepages-2Mi:              0
  memory:                     14225400Ki
  pods:                       500 <co xml:id="CO42-1"/>
 ...</programlisting>
<calloutlist>
<callout arearefs="CO42-1">
<para>In this example, the <literal>pods</literal> parameter should report the value you set in the <literal>KubeletConfig</literal> object.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify the change in the <literal>KubeletConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfigs set-max-pods -o yaml</programlisting>
<simpara>This should show a status of <literal>True</literal> and <literal>type:Success</literal>, as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  kubeletConfig:
    maxPods: 500
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
status:
  conditions:
  - lastTransitionTime: "2021-06-30T17:04:07Z"
    message: Success
    status: "True"
    type: Success</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="create-a-containerruntimeconfig_post-install-machine-configuration-tasks">
<title>Creating a ContainerRuntimeConfig CR to edit CRI-O parameters</title>
<simpara>You can change some of the settings associated with the OpenShift Container Platform CRI-O runtime for the nodes associated with a specific machine config pool (MCP). Using a <literal>ContainerRuntimeConfig</literal> custom resource (CR), you set the configuration values and add a label to match the MCP. The MCO then rebuilds the <literal>crio.conf</literal> and <literal>storage.conf</literal> configuration files on the associated nodes with the updated values.</simpara>
<note>
<simpara>To revert the changes implemented by using a <literal>ContainerRuntimeConfig</literal> CR, you must delete the CR. Removing the label from the machine config pool does not revert the changes.</simpara>
</note>
<simpara>You can modify the following settings by using a <literal>ContainerRuntimeConfig</literal> CR:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">PIDs limit</emphasis>: Setting the PIDs limit in the <literal>ContainerRuntimeConfig</literal> is expected to be deprecated. If PIDs limits are required, it is recommended to use the <literal>podPidsLimit</literal> field in the <literal>KubeletConfig</literal> CR instead. The default value of the <literal>podPidsLimit</literal> field is <literal>4096</literal>.</simpara>
<note>
<simpara>The CRI-O flag is applied on the cgroup of the container, while the Kubelet flag is set on the cgroup of the pod. Please adjust the PIDs limit accordingly.</simpara>
</note>
</listitem>
<listitem>
<simpara><emphasis role="strong">Log level</emphasis>: The <literal>logLevel</literal> parameter sets the CRI-O <literal>log_level</literal> parameter, which is the level of verbosity for log messages. The default is <literal>info</literal> (<literal>log_level = info</literal>). Other options include <literal>fatal</literal>, <literal>panic</literal>, <literal>error</literal>, <literal>warn</literal>, <literal>debug</literal>, and <literal>trace</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Overlay size</emphasis>: The <literal>overlaySize</literal> parameter sets the CRI-O Overlay storage driver <literal>size</literal> parameter, which is the maximum size of a container image.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Maximum log size</emphasis>: Setting the maximum log size in the <literal>ContainerRuntimeConfig</literal> is expected to be deprecated. If a maximum log size is required, it is recommended to use the <literal>containerLogMaxSize</literal> field in the <literal>KubeletConfig</literal> CR instead.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Container runtime</emphasis>: The <literal>defaultRuntime</literal> parameter sets the container runtime to either <literal>runc</literal> or <literal>crun</literal>. The default is <literal>runc</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>You should have one <literal>ContainerRuntimeConfig</literal> CR for each machine config pool with all the config changes you want for that pool. If you are applying the same content to all the pools, you only need one <literal>ContainerRuntimeConfig</literal> CR for all the pools.</simpara>
<simpara>You should edit an existing <literal>ContainerRuntimeConfig</literal> CR to modify existing settings or add new settings instead of creating a new CR for each change. It is recommended to create a new <literal>ContainerRuntimeConfig</literal> CR only to modify a different machine config pool, or for changes that are intended to be temporary so that you can revert the changes.</simpara>
<simpara>You can create multiple <literal>ContainerRuntimeConfig</literal> CRs, as needed, with a limit of 10 per cluster. For the first <literal>ContainerRuntimeConfig</literal> CR, the MCO creates a machine config appended with <literal>containerruntime</literal>. With each subsequent CR, the controller creates a new <literal>containerruntime</literal> machine config with a numeric suffix. For example, if you have a <literal>containerruntime</literal> machine config with a <literal>-2</literal> suffix, the next <literal>containerruntime</literal> machine config is appended with <literal>-3</literal>.</simpara>
<simpara>If you want to delete the machine configs, you should delete them in reverse order to avoid exceeding the limit. For example, you should delete the <literal>containerruntime-3</literal> machine config before deleting the <literal>containerruntime-2</literal> machine config.</simpara>
<note>
<simpara>If you have a machine config with a <literal>containerruntime-9</literal> suffix, and you create another <literal>ContainerRuntimeConfig</literal> CR, a new machine config is not created, even if there are fewer than 10 <literal>containerruntime</literal> machine configs.</simpara>
</note>
<formalpara>
<title>Example showing multiple <literal>ContainerRuntimeConfig</literal> CRs</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ctrcfg</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME         AGE
ctr-overlay  15m
ctr-level    5m45s</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example showing multiple <literal>containerruntime</literal> machine configs</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mc | grep container</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
01-master-container-runtime                        b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             57m
...
01-worker-container-runtime                        b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             57m
...
99-worker-generated-containerruntime               b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             26m
99-worker-generated-containerruntime-1             b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             17m
99-worker-generated-containerruntime-2             b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             7m26s
...</programlisting>
</para>
</formalpara>
<simpara>The following example sets the <literal>log_level</literal> field to <literal>debug</literal> and sets the overlay size to 8 GB:</simpara>
<formalpara>
<title>Example <literal>ContainerRuntimeConfig</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: overlay-size
spec:
 machineConfigPoolSelector:
   matchLabels:
     pools.operator.machineconfiguration.openshift.io/worker: '' <co xml:id="CO43-1"/>
 containerRuntimeConfig:
   logLevel: debug <co xml:id="CO43-2"/>
   overlaySize: 8G <co xml:id="CO43-3"/>
   defaultRuntime: "crun" <co xml:id="CO43-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO43-1">
<para>Specifies the machine config pool label.</para>
</callout>
<callout arearefs="CO43-2">
<para>Optional: Specifies the level of verbosity for log messages.</para>
</callout>
<callout arearefs="CO43-3">
<para>Optional: Specifies the maximum size of a container image.</para>
</callout>
<callout arearefs="CO43-4">
<para>Optional: Specifies the container runtime to deploy to new containers. The default value is <literal>runc</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Procedure</title>
<para>To change CRI-O settings using the <literal>ContainerRuntimeConfig</literal> CR:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a YAML file for the <literal>ContainerRuntimeConfig</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: overlay-size
spec:
 machineConfigPoolSelector:
   matchLabels:
     pools.operator.machineconfiguration.openshift.io/worker: '' <co xml:id="CO44-1"/>
 containerRuntimeConfig: <co xml:id="CO44-2"/>
   logLevel: debug
   overlaySize: 8G</programlisting>
<calloutlist>
<callout arearefs="CO44-1">
<para>Specify a label for the machine config pool that you want you want to modify.</para>
</callout>
<callout arearefs="CO44-2">
<para>Set the parameters as needed.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>ContainerRuntimeConfig</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the CR is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ContainerRuntimeConfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME           AGE
overlay-size   3m19s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that a new <literal>containerruntime</literal> machine config is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigs | grep containerrun</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">99-worker-generated-containerruntime   2c9371fbb673b97a6fe8b1c52691999ed3a1bfc2  3.2.0  31s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the machine config pool until all are shown as ready:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME    CONFIG               UPDATED  UPDATING  DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT  DEGRADEDMACHINECOUNT  AGE
worker  rendered-worker-169  False    True      False     3             1                  1                    0                     9h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the settings were applied in CRI-O:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Open an <literal>oc debug</literal> session to a node in the machine config pool and run <literal>chroot /host</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Verify the changes in the <literal>crio.conf</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# crio config | grep 'log_level'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">log_level = "debug"</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the changes in the `storage.conf`file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# head -n 7 /etc/containers/storage.conf</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>[storage]
  driver = "overlay"
  runroot = "/var/run/containers/storage"
  graphroot = "/var/lib/containers/storage"
  [storage.options]
    additionalimagestores = []
    size = "8G"</screen>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="set-the-default-max-container-root-partition-size-for-overlay-with-crio_post-install-machine-configuration-tasks">
<title>Setting the default maximum container root partition size for Overlay with CRI-O</title>
<simpara>The root partition of each container shows all of the available disk space of the underlying host. Follow this guidance to set a maximum partition size for the root disk of all containers.</simpara>
<simpara>To configure the maximum Overlay size, as well as other CRI-O options like the log level, you can create the following <literal>ContainerRuntimeConfig</literal> custom resource definition (CRD):</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: ContainerRuntimeConfig
metadata:
 name: overlay-size
spec:
 machineConfigPoolSelector:
   matchLabels:
     custom-crio: overlay-size
 containerRuntimeConfig:
   logLevel: debug
   overlaySize: 8G</programlisting>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the configuration object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f overlaysize.yml</programlisting>
</listitem>
<listitem>
<simpara>To apply the new CRI-O configuration to your worker nodes, edit the worker machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool worker</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>custom-crio</literal> label based on the <literal>matchLabels</literal> name you set in the <literal>ContainerRuntimeConfig</literal> CRD:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2020-07-09T15:46:34Z"
  generation: 3
  labels:
    custom-crio: overlay-size
    machineconfiguration.openshift.io/mco-built-in: ""</programlisting>
</listitem>
<listitem>
<simpara>Save the changes, then view the machine configs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigs</programlisting>
<simpara>New <literal>99-worker-generated-containerruntime</literal> and <literal>rendered-worker-xyz</literal> objects are created:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">99-worker-generated-containerruntime  4173030d89fbf4a7a0976d1665491a4d9a6e54f1   3.2.0             7m42s
rendered-worker-xyz                   4173030d89fbf4a7a0976d1665491a4d9a6e54f1   3.2.0             7m36s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>After those objects are created, monitor the machine config pool for the changes to be applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp worker</programlisting>
<simpara>The worker nodes show <literal>UPDATING</literal> as <literal>True</literal>, as well as the number of machines, the number updated, and other details:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME   CONFIG              UPDATED   UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker rendered-worker-xyz False True False     3             2                   2                    0                      20h</programlisting>
</para>
</formalpara>
<simpara>When complete, the worker nodes transition back to <literal>UPDATING</literal> as <literal>False</literal>, and the <literal>UPDATEDMACHINECOUNT</literal> number matches the <literal>MACHINECOUNT</literal>:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME   CONFIG              UPDATED   UPDATING   DEGRADED  MACHINECOUNT  READYMACHINECOUNT  UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
worker   rendered-worker-xyz   True      False      False      3         3            3             0           20h</programlisting>
</para>
</formalpara>
<simpara>Looking at a worker machine, you see that the new 8 GB max size configuration is applied to all of the workers:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">head -n 7 /etc/containers/storage.conf
[storage]
  driver = "overlay"
  runroot = "/var/run/containers/storage"
  graphroot = "/var/lib/containers/storage"
  [storage.options]
    additionalimagestores = []
    size = "8G"</programlisting>
</para>
</formalpara>
<simpara>Looking inside a container, you see that the root partition is now 8 GB:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">~ $ df -h
Filesystem                Size      Used Available Use% Mounted on
overlay                   8.0G      8.0K      8.0G   0% /</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="post-install-cluster-tasks">
<title>Postinstallation cluster tasks</title>

<simpara>After installing OpenShift Container Platform, you can further expand and customize your cluster to your requirements.</simpara>
<section xml:id="available_cluster_customizations">
<title>Available cluster customizations</title>
<simpara>You complete most of the cluster configuration and customization after you deploy your OpenShift Container Platform cluster. A number of <emphasis>configuration resources</emphasis> are available.</simpara>
<note>
<simpara>If you install your cluster on IBM Z&#174;, not all features and functions are available.</simpara>
</note>
<simpara>You modify the configuration resources to configure the major features of the
cluster, such as the image registry, networking configuration, image build
behavior, and the identity provider.</simpara>
<simpara>For current documentation of the settings that you control by using these resources, use
the <literal>oc explain</literal> command, for example <literal>oc explain builds --api-version=config.openshift.io/v1</literal></simpara>
<section xml:id="configuration-resources_post-install-cluster-tasks">
<title>Cluster configuration resources</title>
<simpara>All cluster configuration resources are globally scoped (not namespaced) and named <literal>cluster</literal>.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Resource name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>apiserver.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides API server configuration such as <link xlink:href="../security/certificates/api-server.xml#api-server-certificates">certificates and certificate authorities</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>authentication.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Controls the <link xlink:href="../authentication/understanding-identity-provider.xml#understanding-identity-provider">identity provider</link> and authentication configuration for the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>build.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Controls default and enforced <link xlink:href="../cicd/builds/build-configuration.xml#build-configuration">configuration</link> for all builds on the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>console.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures the behavior of the web console interface, including the <link xlink:href="../web_console/configuring-web-console.xml#configuring-web-console">logout behavior</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>featuregate.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables <link xlink:href="../nodes/clusters/nodes-cluster-enabling-features.xml#nodes-cluster-enabling">FeatureGates</link>
so that you can use Tech Preview features.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>image.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures how specific <link xlink:href="../openshift_images/image-configuration.xml#image-configuration">image registries</link> should be treated (allowed, disallowed, insecure, CA details).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ingress.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configuration details related to <link xlink:href="../networking/ingress-operator.xml#nw-installation-ingress-config-asset_configuring-ingress">routing</link> such as the default domain for routes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>oauth.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures identity providers and other behavior related to <link xlink:href="../authentication/configuring-internal-oauth.xml#configuring-internal-oauth">internal OAuth server</link> flows.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>project.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures <link xlink:href="../applications/projects/configuring-project-creation.xml#configuring-project-creation">how projects are created</link> including the project template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>proxy.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Defines proxies to be used by components needing external network access.  Note: not all components currently consume this value.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>scheduler.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures <link xlink:href="../nodes/scheduling/nodes-scheduler-profiles.xml#nodes-scheduler-profiles">scheduler</link> behavior such as profiles and default node selectors.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="operator-configuration-resources_post-install-cluster-tasks">
<title>Operator configuration resources</title>
<simpara>These configuration resources are cluster-scoped instances, named <literal>cluster</literal>, which control the behavior of a specific component as
owned by a particular Operator.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Resource name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>consoles.operator.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Controls console appearance such as branding customizations</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.imageregistry.operator.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures <link xlink:href="../registry/configuring-registry-operator.xml#registry-operator-configuration-resource-overview_configuring-registry-operator">OpenShift image registry settings</link> such as public routing, log levels, proxy settings, resource constraints, replica counts, and storage type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>config.samples.operator.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures the
<link xlink:href="../openshift_images/configuring-samples-operator.xml#configuring-samples-operator">Samples Operator</link>
to control which example image streams and templates are installed on the cluster.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="additional-configuration-resources_post-install-cluster-tasks">
<title>Additional configuration resources</title>
<simpara>These configuration resources represent a single instance of a particular component. In some cases, you can request multiple
instances by creating multiple instances of the resource. In other cases, the Operator can use only a specific
resource instance name in a specific namespace. Reference the component-specific
documentation for details on how and when you can create additional resource instances.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="14.2857*"/>
<colspec colname="col_2" colwidth="14.2857*"/>
<colspec colname="col_3" colwidth="14.2857*"/>
<colspec colname="col_4" colwidth="57.1429*"/>
<thead>
<row>
<entry align="left" valign="top">Resource name</entry>
<entry align="left" valign="top">Instance name</entry>
<entry align="left" valign="top">Namespace</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>alertmanager.monitoring.coreos.com</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>main</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>openshift-monitoring</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Controls the <link xlink:href="../monitoring/managing-alerts.xml#managing-alerts">Alertmanager</link> deployment parameters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ingresscontroller.operator.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>default</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>openshift-ingress-operator</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configures <link xlink:href="../networking/ingress-operator.xml#configuring-ingress">Ingress Operator</link> behavior such as domain, number of replicas, certificates, and controller placement.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="informational-resources_post-install-cluster-tasks">
<title>Informational Resources</title>
<simpara>You use these resources to retrieve information about the cluster. Some configurations might require you to edit these resources directly.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="16.6666*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="66.6668*"/>
<thead>
<row>
<entry align="left" valign="top">Resource name</entry>
<entry align="left" valign="top">Instance name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>clusterversion.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>version</literal></simpara></entry>
<entry align="left" valign="top"><simpara>In OpenShift Container Platform 4.14, you must not customize the <literal>ClusterVersion</literal>
resource for production clusters. Instead, follow the process to
<link xlink:href="../updating/updating_a_cluster/updating-cluster-web-console.xml#updating-cluster-web-console">update a cluster</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>dns.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>cluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>You cannot modify the DNS settings for your cluster. You can
<link xlink:href="../networking/dns-operator.xml#dns-operator">view the DNS Operator status</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>infrastructure.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>cluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Configuration details allowing the cluster to interact with its cloud provider.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>network.config.openshift.io</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>cluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>You cannot modify your cluster networking after installation. To customize your network, follow the process to
<link xlink:href="../installing/installing_aws/installing-aws-network-customizations.xml#installing-aws-network-customizations">customize networking during installation</link>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="images-update-global-pull-secret_post-install-cluster-tasks">
<title>Updating the global cluster pull secret</title>
<simpara>You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret.</simpara>
<simpara>The procedure is required when users use a separate registry to store images than the registry used during installation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: To append a new pull secret to the existing pull secret, complete the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Enter the following command to download the pull secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret/pull-secret -n openshift-config --template='{{index .data ".dockerconfigjson" | base64decode}}' &gt;&lt;pull_secret_location&gt; <co xml:id="CO45-1"/></programlisting>
<calloutlist>
<callout arearefs="CO45-1">
<para>Provide the path to the pull secret file.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Enter the following command to add the new pull secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc registry login --registry="&lt;registry&gt;" \ <co xml:id="CO46-1"/>
--auth-basic="&lt;username&gt;:&lt;password&gt;" \ <co xml:id="CO46-2"/>
--to=&lt;pull_secret_location&gt; <co xml:id="CO46-3"/></programlisting>
<calloutlist>
<callout arearefs="CO46-1">
<para>Provide the new registry. You can include multiple repositories within the same registry, for example: <literal>--registry="&lt;registry/my-namespace/my-repository&gt;"</literal>.</para>
</callout>
<callout arearefs="CO46-2">
<para>Provide the credentials of the new registry.</para>
</callout>
<callout arearefs="CO46-3">
<para>Provide the path to the pull secret file.</para>
</callout>
</calloutlist>
<simpara>Alternatively, you can perform a manual update to the pull secret file.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Enter the following command to update the global pull secret for your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location&gt; <co xml:id="CO47-1"/></programlisting>
<calloutlist>
<callout arearefs="CO47-1">
<para>Provide the path to the new pull secret file.</para>
</callout>
</calloutlist>
<simpara>This update is rolled out to all nodes, which can take some time depending on the size of your cluster.</simpara>
<note>
<simpara>As of OpenShift Container Platform 4.7.4, changes to the global pull secret no longer trigger a node drain or reboot.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="adding-worker-nodes_post-install-cluster-tasks">
<title>Adding worker nodes</title>
<simpara>After you deploy your OpenShift Container Platform cluster, you can add worker nodes to scale cluster resources. There are different ways you can add worker nodes depending on the installation method and the environment of your cluster.</simpara>
<section xml:id="_adding_worker_nodes_to_installer_provisioned_infrastructure_clusters">
<title>Adding worker nodes to installer-provisioned infrastructure clusters</title>
<simpara>For installer-provisioned infrastructure clusters, you can manually or automatically scale the <literal>MachineSet</literal> object to match the number of available bare-metal hosts.</simpara>
<simpara>To add a bare-metal host, you must configure all network prerequisites, configure an associated <literal>baremetalhost</literal> object, then provision the worker node to the cluster. You can add a bare-metal host manually or by using the web console.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../scalability_and_performance/managing-bare-metal-hosts.xml#adding-bare-metal-host-to-cluster-using-web-console_managing-bare-metal-hosts">Adding worker nodes using the web console</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../scalability_and_performance/managing-bare-metal-hosts.xml#adding-bare-metal-host-to-cluster-using-yaml_managing-bare-metal-hosts">Adding worker nodes using YAML in the web console</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.xml#preparing-the-bare-metal-node_ipi-install-expanding">Manually adding a worker node to an installer-provisioned infrastructure cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_adding_worker_nodes_to_user_provisioned_infrastructure_clusters">
<title>Adding worker nodes to user-provisioned infrastructure clusters</title>
<simpara>For user-provisioned infrastructure clusters, you can add worker nodes by using a RHEL or RHCOS ISO image and connecting it to your cluster using cluster Ignition config files. For RHEL worker nodes, the following example uses Ansible playbooks to add worker nodes to the cluster. For RHCOS worker nodes, the following example uses an ISO image and network booting to add worker nodes to the cluster.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/node-tasks.xml#post-install-config-adding-fcos-compute">Adding RHCOS worker nodes to a user-provisioned infrastructure cluster</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/node-tasks.xml#post-install-config-adding-rhel-compute">Adding RHEL worker nodes to a user-provisioned infrastructure cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_adding_worker_nodes_to_clusters_managed_by_the_assisted_installer">
<title>Adding worker nodes to clusters managed by the Assisted Installer</title>
<simpara>For clusters managed by the Assisted Installer, you can add worker nodes by using the Red Hat OpenShift Cluster Manager console, the Assisted Installer REST API or you can manually add worker nodes using an ISO image and cluster Ignition config files.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../nodes/nodes/nodes-sno-worker-nodes.xml#sno-adding-worker-nodes-to-sno-clusters_add-workers">Adding worker nodes using the OpenShift Cluster Manager</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../nodes/nodes/nodes-sno-worker-nodes.xml#adding-worker-nodes-using-the-assisted-installer-api">Adding worker nodes using the Assisted Installer REST API</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../nodes/nodes/nodes-sno-worker-nodes.xml#sno-adding-worker-nodes-to-single-node-clusters-manually_add-workers">Manually adding worker nodes to a SNO cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_adding_worker_nodes_to_clusters_managed_by_the_multicluster_engine_for_kubernetes">
<title>Adding worker nodes to clusters managed by the multicluster engine for Kubernetes</title>
<simpara>For clusters managed by the multicluster engine for Kubernetes, you can add worker nodes by using the dedicated multicluster engine console.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.9/html/clusters/cluster_mce_overview#on-prem-creating-your-cluster-with-the-console">Creating your cluster with the console</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-install-adjust-worker-nodes">
<title>Adjust worker nodes</title>
<simpara>If you incorrectly sized the worker nodes during deployment, adjust them by creating one or more new compute machine sets, scale them up, then scale the original compute machine set down before removing them.</simpara>
<section xml:id="differences-between-machinesets-and-machineconfigpool_post-install-cluster-tasks">
<title>Understanding the difference between compute machine sets and the machine config pool</title>
<simpara><literal>MachineSet</literal> objects describe OpenShift Container Platform nodes with respect to the cloud or machine provider.</simpara>
<simpara>The <literal>MachineConfigPool</literal> object allows <literal>MachineConfigController</literal> components to define and provide the status of machines in the context of upgrades.</simpara>
<simpara>The <literal>MachineConfigPool</literal> object allows users to configure how upgrades are rolled out to the OpenShift Container Platform nodes in the machine config pool.</simpara>
<simpara>The <literal>NodeSelector</literal> object can be replaced with a reference to the <literal>MachineSet</literal> object.</simpara>
</section>
<section xml:id="machineset-manually-scaling_post-install-cluster-tasks">
<title>Scaling a compute machine set manually</title>
<simpara>To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.</simpara>
<simpara>This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install an OpenShift Container Platform cluster and the <literal>oc</literal> command line.</simpara>
</listitem>
<listitem>
<simpara>Log in to  <literal>oc</literal> as a user with <literal>cluster-admin</literal> permission.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the compute machine sets that are in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machinesets -n openshift-machine-api</programlisting>
<simpara>The compute machine sets are listed in the form of <literal>&lt;clusterid&gt;-worker-&lt;aws-region-az&gt;</literal>.</simpara>
</listitem>
<listitem>
<simpara>View the compute machines that are in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Set the annotation on the compute machine that you want to delete by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate machine/&lt;machine_name&gt; -n openshift-machine-api machine.openshift.io/delete-machine="true"</programlisting>
</listitem>
<listitem>
<simpara>Scale the compute machine set by running one of the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>Or:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to scale the compute machine set:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2</programlisting>
</tip>
<simpara>You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.</simpara>
<important>
<simpara>By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.</simpara>
<simpara>You can skip draining the node by annotating <literal>machine.openshift.io/exclude-node-draining</literal> in a specific machine.</simpara>
</important>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify the deletion of the intended machine by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="machineset-delete-policy_post-install-cluster-tasks">
<title>The compute machine set deletion policy</title>
<simpara><literal>Random</literal>, <literal>Newest</literal>, and <literal>Oldest</literal> are the three supported deletion options. The default is <literal>Random</literal>, meaning that random machines are chosen and deleted when scaling compute machine sets down. The deletion policy can be set according to the use case by modifying the particular compute machine set:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  deletePolicy: &lt;delete_policy&gt;
  replicas: &lt;desired_replica_count&gt;</programlisting>
<simpara>Specific machines can also be prioritized for deletion by adding the annotation <literal>machine.openshift.io/delete-machine=true</literal> to the machine of interest, regardless of the deletion policy.</simpara>
<important>
<simpara>By default, the OpenShift Container Platform router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the worker compute machine set to <literal>0</literal> unless you first relocate the router pods.</simpara>
</important>
<note>
<simpara>Custom compute machine sets can be used for use cases requiring that services run on specific nodes and that those services are ignored by the controller when the worker compute machine sets are scaling down. This prevents service disruption.</simpara>
</note>
</section>
<section xml:id="nodes-scheduler-node-selectors-cluster_post-install-cluster-tasks">
<title>Creating default cluster-wide node selectors</title>
<simpara>You can use default cluster-wide node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.</simpara>
<simpara>With cluster-wide node selectors, when you create a pod in that cluster, OpenShift Container Platform adds the default node selectors to the pod and schedules
the pod on nodes with matching labels.</simpara>
<simpara>You configure cluster-wide node selectors by editing the Scheduler Operator custom resource (CR). You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.</simpara>
<note>
<simpara>You can add additional key/value pairs to a pod. But you cannot add a different value for a default key.</simpara>
</note>
<formalpara>
<title>Procedure</title>
<para>To add a default cluster-wide node selector:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the Scheduler Operator CR to add the default cluster-wide node selectors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit scheduler cluster</programlisting>
<formalpara>
<title>Example Scheduler Operator CR with a node selector</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...
spec:
  defaultNodeSelector: type=user-node,region=east <co xml:id="CO48-1"/>
  mastersSchedulable: false</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO48-1">
<para>Add a node selector with the appropriate <literal>&lt;key&gt;:&lt;value&gt;</literal> pairs.</para>
</callout>
</calloutlist>
<simpara>After making this change, wait for the pods in the <literal>openshift-kube-apiserver</literal> project to redeploy. This can take several minutes. The default cluster-wide node selector does not take effect until the pods redeploy.</simpara>
</listitem>
<listitem>
<simpara>Add labels to a node by using a compute machine set or editing the node directly:</simpara>
<itemizedlist>
<listitem>
<simpara>Use a compute machine set to add labels to nodes managed by the compute machine set when a node is created:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to add labels to a <literal>MachineSet</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineSet &lt;name&gt; --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"&lt;key&gt;"="&lt;value&gt;","&lt;key&gt;"="&lt;value&gt;"}}]'  -n openshift-machine-api <co xml:id="CO49-1"/></programlisting>
<calloutlist>
<callout arearefs="CO49-1">
<para>Add a <literal>&lt;key&gt;/&lt;value&gt;</literal> pair for each label.</para>
</callout>
</calloutlist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add labels to a compute machine set:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"</programlisting>
</tip>
</listitem>
<listitem>
<simpara>Verify that the labels are added to the <literal>MachineSet</literal> object by using the <literal>oc edit</literal> command:</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api</programlisting>
<formalpara>
<title>Example <literal>MachineSet</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
  ...
spec:
  ...
  template:
    metadata:
  ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
  ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Redeploy the nodes associated with that compute machine set by scaling down to <literal>0</literal> and scaling up the nodes:</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=0 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=1 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>When the nodes are ready and available, verify that the label is added to the nodes by using the <literal>oc get</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l type=user-node</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-c-vmqzp   Ready    worker   61s   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Add labels directly to a node:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>Node</literal> object for the node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label nodes &lt;name&gt; &lt;key&gt;=&lt;value&gt;</programlisting>
<simpara>For example, to label a node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label nodes ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49 type=user-node region=east</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add labels to a node:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    type: "user-node"
    region: "east"</programlisting>
</tip>
</listitem>
<listitem>
<simpara>Verify that the labels are added to the node using the <literal>oc get</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l &lt;key&gt;=&lt;value&gt;,&lt;key&gt;=&lt;value&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l type=user-node,region=east</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49   Ready    worker   17m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-worker-latency-profiles">
<title>Improving cluster stability in high latency environments using worker latency profiles</title>
<simpara>If the cluster administrator has performed latency tests for platform verification, they can discover the need to adjust the operation of the cluster to ensure stability in cases of high latency. The cluster administrator need change only one parameter, recorded in a file, which controls four parameters affecting how supervisory processes read status and interpret the health of the cluster. Changing only the one parameter provides cluster tuning in an easy, supportable manner.</simpara>
<simpara>The <literal>Kubelet</literal> process provides the starting point for monitoring cluster health. The <literal>Kubelet</literal> sets status values for all nodes in the OpenShift Container Platform cluster. The Kubernetes Controller Manager (<literal>kube controller</literal>) reads the status values every 10 seconds, by default.
If the <literal>kube controller</literal> cannot read a node status value, it loses contact with that node after a configured period. The default behavior is:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The node controller on the control plane updates the node health to <literal>Unhealthy</literal> and marks the node <literal>Ready</literal> condition`Unknown`.</simpara>
</listitem>
<listitem>
<simpara>In response, the scheduler stops scheduling pods to that node.</simpara>
</listitem>
<listitem>
<simpara>The Node Lifecycle Controller adds a <literal>node.kubernetes.io/unreachable</literal> taint with a <literal>NoExecute</literal> effect to the node and schedules any pods on the node for eviction after five minutes, by default.</simpara>
</listitem>
</orderedlist>
<simpara>This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager might not receive an update from a healthy node due to network latency. The <literal>Kubelet</literal> evicts pods from the node even though the node is healthy.</simpara>
<simpara>To avoid this problem, you can use <emphasis>worker latency profiles</emphasis> to adjust the frequency that the <literal>Kubelet</literal> and the Kubernetes Controller Manager wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly if network latency between the control plane and the worker nodes is not optimal.</simpara>
<simpara>These worker latency profiles contain three sets of parameters that are pre-defined with carefully tuned values to control the reaction of the cluster to increased latency. No need to experimentally find the best values manually.</simpara>
<simpara>You can configure worker latency profiles when installing a cluster or at any time you notice increased latency in your cluster network.</simpara>
<section xml:id="nodes-cluster-worker-latency-profiles-about_post-install-cluster-tasks">
<title>Understanding worker latency profiles</title>
<simpara>Worker latency profiles are four different categories of carefully-tuned parameters. The four parameters which implement these values are <literal>node-status-update-frequency</literal>, <literal>node-monitor-grace-period</literal>, <literal>default-not-ready-toleration-seconds</literal> and <literal>default-unreachable-toleration-seconds</literal>. These parameters can use values which allow you control the reaction of the cluster to latency issues without needing to determine the best values using manual methods.</simpara>
<important>
<simpara>Setting these parameters manually is not supported. Incorrect parameter settings adversely affect cluster stability.</simpara>
</important>
<simpara>All worker latency profiles configure the following parameters:</simpara>
<variablelist>
<varlistentry>
<term>node-status-update-frequency</term>
<listitem>
<simpara>Specifies how often the kubelet posts node status to the API server.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>node-monitor-grace-period</term>
<listitem>
<simpara>Specifies the amount of time in seconds that the Kubernetes Controller Manager waits for an update from a kubelet before marking the node unhealthy and adding the <literal>node.kubernetes.io/not-ready</literal> or <literal>node.kubernetes.io/unreachable</literal> taint to the node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>default-not-ready-toleration-seconds</term>
<listitem>
<simpara>Specifies the amount of time in seconds after marking a node unhealthy that the Kube API Server Operator waits before evicting pods from that node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>default-unreachable-toleration-seconds</term>
<listitem>
<simpara>Specifies the amount of time in seconds after marking a node unreachable that the Kube API Server Operator waits before evicting pods from that node.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The following Operators monitor the changes to the worker latency profiles and respond accordingly:</simpara>
<itemizedlist>
<listitem>
<simpara>The Machine Config Operator (MCO) updates the <literal>node-status-update-frequency</literal> parameter on the worker nodes.</simpara>
</listitem>
<listitem>
<simpara>The Kubernetes Controller Manager updates the <literal>node-monitor-grace-period</literal> parameter on the control plane nodes.</simpara>
</listitem>
<listitem>
<simpara>The Kubernetes API Server Operator updates the <literal>default-not-ready-toleration-seconds</literal> and <literal>default-unreachable-toleration-seconds</literal> parameters on the control plane nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>Although the default configuration works in most cases, OpenShift Container Platform offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:</simpara>
<variablelist>
<varlistentry>
<term>Default worker latency profile</term>
<listitem>
<simpara>With the <literal>Default</literal> profile, each <literal>Kubelet</literal> updates it&#8217;s status every 10 seconds (<literal>node-status-update-frequency</literal>). The <literal>Kube Controller Manager</literal> checks the statuses of <literal>Kubelet</literal> every 5 seconds (<literal>node-monitor-grace-period</literal>).</simpara>
<simpara>The Kubernetes Controller Manager waits 40 seconds for a status update from <literal>Kubelet</literal> before considering the <literal>Kubelet</literal> unhealthy. If no status is made available to the Kubernetes Controller Manager, it then marks the node with the <literal>node.kubernetes.io/not-ready</literal> or <literal>node.kubernetes.io/unreachable</literal> taint and evicts the pods on that node.</simpara>
<simpara>If a pod on that node has the <literal>NoExecute</literal> taint, the pod is run according to <literal>tolerationSeconds</literal>. If the pod has no taint, it will be evicted in 300 seconds (<literal>default-not-ready-toleration-seconds</literal> and <literal>default-unreachable-toleration-seconds</literal> settings of the <literal>Kube API Server</literal>).</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="33.3333*"/>
<colspec colname="col_4" colwidth="16.6668*"/>
<thead>
<row>
<entry align="left" valign="top">Profile</entry>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="3"><simpara>Default</simpara></entry>
<entry align="left" valign="top"><simpara>kubelet</simpara></entry>
<entry align="left" valign="top"><simpara><literal>node-status-update-frequency</literal></simpara></entry>
<entry align="left" valign="top"><simpara>10s</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubelet Controller Manager</simpara></entry>
<entry align="left" valign="top"><simpara><literal>node-monitor-grace-period</literal></simpara></entry>
<entry align="left" valign="top"><simpara>40s</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes API Server Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>default-not-ready-toleration-seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>300s</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes API Server Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>default-unreachable-toleration-seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>300s</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</varlistentry>
<varlistentry>
<term>Medium worker latency profile</term>
<listitem>
<simpara>Use the <literal>MediumUpdateAverageReaction</literal> profile if the network latency is slightly higher than usual.</simpara>
<simpara>The <literal>MediumUpdateAverageReaction</literal> profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <literal>tolerationSeconds</literal> parameter, the eviction waits for the period specified by that parameter.</simpara>
<simpara>The Kubernetes Controller Manager waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="33.3333*"/>
<colspec colname="col_4" colwidth="16.6668*"/>
<thead>
<row>
<entry align="left" valign="top">Profile</entry>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="3"><simpara>MediumUpdateAverageReaction</simpara></entry>
<entry align="left" valign="top"><simpara>kubelet</simpara></entry>
<entry align="left" valign="top"><simpara><literal>node-status-update-frequency</literal></simpara></entry>
<entry align="left" valign="top"><simpara>20s</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubelet Controller Manager</simpara></entry>
<entry align="left" valign="top"><simpara><literal>node-monitor-grace-period</literal></simpara></entry>
<entry align="left" valign="top"><simpara>2m</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes API Server Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>default-not-ready-toleration-seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes API Server Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>default-unreachable-toleration-seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</varlistentry>
<varlistentry>
<term>Low worker latency profile</term>
<listitem>
<simpara>Use the <literal>LowUpdateSlowReaction</literal> profile if the network latency is extremely high.</simpara>
<simpara>The <literal>LowUpdateSlowReaction</literal> profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the <literal>tolerationSeconds</literal> parameter, the eviction waits for the period specified by that parameter.</simpara>
<simpara>The Kubernetes Controller Manager waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="33.3333*"/>
<colspec colname="col_4" colwidth="16.6668*"/>
<thead>
<row>
<entry align="left" valign="top">Profile</entry>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="3"><simpara>LowUpdateSlowReaction</simpara></entry>
<entry align="left" valign="top"><simpara>kubelet</simpara></entry>
<entry align="left" valign="top"><simpara><literal>node-status-update-frequency</literal></simpara></entry>
<entry align="left" valign="top"><simpara>1m</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubelet Controller Manager</simpara></entry>
<entry align="left" valign="top"><simpara><literal>node-monitor-grace-period</literal></simpara></entry>
<entry align="left" valign="top"><simpara>5m</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes API Server Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>default-not-ready-toleration-seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Kubernetes API Server Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>default-unreachable-toleration-seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>60s</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="nodes-cluster-worker-latency-profiles-using_post-install-cluster-tasks">
<title>Using and changing worker latency profiles</title>
<simpara>To change a worker latency profile to deal with network latency, edit the <literal>node.config</literal> object to add the name of the profile. You can change the profile at any time as latency increases or decreases.</simpara>
<simpara>You must move one worker latency profile at a time. For example, you cannot move directly from the <literal>Default</literal> profile to the <literal>LowUpdateSlowReaction</literal> worker latency profile. You must move from the <literal>Default</literal> worker latency profile to the <literal>MediumUpdateAverageReaction</literal> profile first, then to <literal>LowUpdateSlowReaction</literal>. Similarly, when returning to the <literal>Default</literal> profile, you must move from the low profile to the medium profile first, then to <literal>Default</literal>.</simpara>
<note>
<simpara>You can also configure worker latency profiles upon installing an OpenShift Container Platform cluster.</simpara>
</note>
<formalpara>
<title>Procedure</title>
<para>To move from the default worker latency profile:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Move to the medium worker latency profile:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>node.config</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit nodes.config/cluster</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>spec.workerLatencyProfile: MediumUpdateAverageReaction</literal>:</simpara>
<formalpara>
<title>Example <literal>node.config</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: MediumUpdateAverageReaction <co xml:id="CO50-1"/>

# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO50-1">
<para>Specifies the medium worker latency policy.</para>
</callout>
</calloutlist>
<simpara>Scheduling on each worker node is disabled as the change is being applied.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: Move to the low worker latency profile:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>node.config</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit nodes.config/cluster</programlisting>
</listitem>
<listitem>
<simpara>Change the <literal>spec.workerLatencyProfile</literal> value to <literal>LowUpdateSlowReaction</literal>:</simpara>
<formalpara>
<title>Example <literal>node.config</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: LowUpdateSlowReaction <co xml:id="CO51-1"/>

# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO51-1">
<para>Specifies use of the low worker latency policy.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>Scheduling on each worker node is disabled as the change is being applied.</simpara>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>When all nodes return to the <literal>Ready</literal> condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get KubeControllerManager -o yaml | grep -i workerlatency -A 5 -B 5</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># ...
    - lastTransitionTime: "2022-07-11T19:47:10Z"
      reason: ProfileUpdated
      status: "False"
      type: WorkerLatencyProfileProgressing
    - lastTransitionTime: "2022-07-11T19:47:10Z" <co xml:id="CO52-1"/>
      message: all static pod revision(s) have updated latency profile
      reason: ProfileUpdated
      status: "True"
      type: WorkerLatencyProfileComplete
    - lastTransitionTime: "2022-07-11T19:20:11Z"
      reason: AsExpected
      status: "False"
      type: WorkerLatencyProfileDegraded
    - lastTransitionTime: "2022-07-11T19:20:36Z"
      status: "False"
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO52-1">
<para>Specifies that the profile is applied and active.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<simpara>To change the medium profile to default or change the default to medium, edit the <literal>node.config</literal> object and set the <literal>spec.workerLatencyProfile</literal> parameter to the appropriate value.</simpara>
</section>
</section>
<section xml:id="post-install-cpms-setup">
<title>Managing control plane machines</title>
<simpara><link xlink:href="../machine_management/control_plane_machine_management/cpmso-about.xml#cpmso-about">Control plane machine sets</link> provide management capabilities for control plane machines that are similar to what compute machine sets provide for compute machines. The availability and initial status of control plane machine sets on your cluster depend on your cloud provider and the version of OpenShift Container Platform that you installed. For more information, see <link xlink:href="../machine_management/control_plane_machine_management/cpmso-getting-started.xml#cpmso-getting-started">Getting started with control plane machine sets</link>.</simpara>
</section>
<section xml:id="post-install-creating-infrastructure-machinesets-production">
<title>Creating infrastructure machine sets for production environments</title>
<simpara>You can create a compute machine set to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.</simpara>
<simpara>In a production deployment, it is recommended that you deploy at least three compute machine sets to hold infrastructure components. Both OpenShift Logging and Red Hat OpenShift Service Mesh deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. A configuration like this requires three different compute machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.</simpara>
<simpara>For information on infrastructure nodes and which components can run on infrastructure nodes, see <link xlink:href="../machine_management/creating-infrastructure-machinesets.xml#creating-infrastructure-machinesets">Creating infrastructure machine sets</link>.</simpara>
<simpara>To create an infrastructure node, you can <link xlink:href="../post_installation_configuration/cluster-tasks.xml#machineset-creating_post-install-cluster-tasks">use a machine set</link>, <link xlink:href="../post_installation_configuration/cluster-tasks.xml#creating-an-infra-node_post-install-cluster-tasks">assign a label to the nodes</link>, or <link xlink:href="../post_installation_configuration/cluster-tasks.xml#creating-infra-machines_post-install-cluster-tasks">use a machine config pool</link>.</simpara>
<simpara>For sample machine sets that you can use with these procedures, see <link xlink:href="../machine_management/creating-infrastructure-machinesets.xml#creating-infrastructure-machinesets-clouds">Creating machine sets for different clouds</link>.</simpara>
<simpara>Applying a specific node selector to all infrastructure components causes OpenShift Container Platform to <link xlink:href="../post_installation_configuration/cluster-tasks.xml#moving-resources-to-infrastructure-machinesets">schedule those workloads on nodes with that label</link>.</simpara>
<section xml:id="machineset-creating_post-install-cluster-tasks">
<title>Creating a compute machine set</title>
<simpara>In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Deploy an OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to <literal>oc</literal> as a user with <literal>cluster-admin</literal> permission.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <literal>&lt;file_name&gt;.yaml</literal>.</simpara>
<simpara>Ensure that you set the <literal>&lt;clusterID&gt;</literal> and <literal>&lt;role&gt;</literal> parameter values.</simpara>
</listitem>
<listitem>
<simpara>Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the compute machine sets in your cluster, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machinesets -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To view values of a specific compute machine set custom resource (CR), run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO53-1"/>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <co xml:id="CO53-2"/>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <co xml:id="CO53-3"/>
        ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO53-1">
<para>The cluster infrastructure ID.</para>
</callout>
<callout arearefs="CO53-2">
<para>A default node label.</para>
<note>
<simpara>For clusters that have user-provisioned infrastructure, a compute machine set can only create <literal>worker</literal> and <literal>infra</literal> type machines.</simpara>
</note>
</callout>
<callout arearefs="CO53-3">
<para>The values in the <literal>&lt;providerSpec&gt;</literal> section of the compute machine set CR are platform-specific. For more information about <literal>&lt;providerSpec&gt;</literal> parameters in the CR, see the sample compute machine set CR configuration for your provider.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>MachineSet</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>View the list of compute machine sets by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</programlisting>
</para>
</formalpara>
<simpara>When the new compute machine set is available, the <literal>DESIRED</literal> and <literal>CURRENT</literal> values match. If the compute machine set is not available, wait a few minutes and run the command again.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-an-infra-node_post-install-cluster-tasks">
<title>Creating an infrastructure node</title>
<important>
<simpara>See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.</simpara>
</important>
<simpara>Requirements of the cluster dictate that infrastructure, also called <literal>infra</literal> nodes, be provisioned. The installer only provides provisions for control plane and worker nodes. Worker nodes can be designated as infrastructure nodes or application, also called <literal>app</literal>, nodes through labeling.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add a label to the worker node that you want to act as application node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/app=""</programlisting>
</listitem>
<listitem>
<simpara>Add a label to the worker nodes that you want to act as infrastructure nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node-name&gt; node-role.kubernetes.io/infra=""</programlisting>
</listitem>
<listitem>
<simpara>Check to see if applicable nodes now have the <literal>infra</literal> role and <literal>app</literal> roles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>Create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces. This creates an intersection with any existing node selectors on a pod, which additionally constrains the pod&#8217;s selector.</simpara>
<important>
<simpara>If the default node selector key conflicts with the key of a pod&#8217;s label, then the default node selector is not applied.</simpara>
<simpara>However, do not set a default node selector that might cause a pod to become unschedulable. For example, setting the default node selector to a specific node role, such as <literal>node-role.kubernetes.io/infra=""</literal>, when a pod&#8217;s label is set to a different node role, such as <literal>node-role.kubernetes.io/master=""</literal>, can cause the pod to become unschedulable. For this reason, use caution when setting the default node selector to specific node roles.</simpara>
<simpara>You can alternatively use a project node selector to avoid cluster-wide node selector key conflicts.</simpara>
</important>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>Scheduler</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit scheduler cluster</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>defaultNodeSelector</literal> field with the appropriate node selector:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec:
  defaultNodeSelector: topology.kubernetes.io/region=us-east-1 <co xml:id="CO54-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO54-1">
<para>This example node selector deploys pods on nodes in the <literal>us-east-1</literal> region by default.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>You can now move infrastructure resources to the newly labeled <literal>infra</literal> nodes.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For information on how to configure project node selectors to avoid cluster-wide node selector key conflicts, see <link xlink:href="../nodes/scheduling/nodes-scheduler-node-selectors.xml#project-node-selectors_nodes-scheduler-node-selectors">Project node selectors</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-infra-machines_post-install-cluster-tasks">
<title>Creating a machine config pool for infrastructure machines</title>
<simpara>If you need infrastructure machines to have dedicated configurations, you must create an infra pool.</simpara>
<important>
<simpara>Creating a custom machine configuration pool overrides default worker pool configurations if they refer to the same file or unit.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add a label to the node you want to assign as the infra node with a specific label:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node_name&gt; &lt;label&gt;</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node ci-ln-n8mqwr2-f76d1-xscn2-worker-c-6fmtx node-role.kubernetes.io/infra=</programlisting>
</listitem>
<listitem>
<simpara>Create a machine config pool that contains both the worker role and your custom role as machine config selector:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat infra.mcp.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} <co xml:id="CO55-1"/>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: "" <co xml:id="CO55-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO55-1">
<para>Add the worker role and your custom role.</para>
</callout>
<callout arearefs="CO55-2">
<para>Add the label you added to the node as a <literal>nodeSelector</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>Custom machine config pools inherit machine configs from the worker pool. Custom pools use any machine config targeted for the worker pool, but add the ability to also deploy changes that are targeted at only the custom pool. Because a custom pool inherits resources from the worker pool, any change to the worker pool also affects the custom pool.</simpara>
</note>
</listitem>
<listitem>
<simpara>After you have the YAML file, you can create the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f infra.mcp.yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the machine configs to ensure that the infrastructure configuration rendered successfully:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
00-worker                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-1ae2a1e0-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-ssh                                                                                          3.2.0             31d
99-worker-1ae64748-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-worker-ssh                                                                                          3.2.0             31d
rendered-infra-4e48906dca84ee702959c71a53ee80e7             365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             23m
rendered-master-072d4b2da7f88162636902b074e9e28e            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-master-3e88ec72aed3886dec061df60d16d1af            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-master-419bee7de96134963a15fdf9dd473b25            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-master-53f5c91c7661708adce18739cc0f40fb            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d
rendered-master-a6a357ec18e5bce7f5ac426fc7c5ffcd            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-master-dc7f874ec77fc4b969674204332da037            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-1a75960c52ad18ff5dfa6674eb7e533d            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-2640531be11ba43c61d72e82dc634ce6            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-4e48906dca84ee702959c71a53ee80e7            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-worker-4f110718fe88e5f349987854a1147755            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-worker-afc758e194d6188677eb837842d3b379            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-worker-daa08cc1e8f5fcdeba24de60cd955cc3            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d</programlisting>
</para>
</formalpara>
<simpara>You should see a new machine config, with the <literal>rendered-infra-*</literal> prefix.</simpara>
</listitem>
<listitem>
<simpara>Optional: To deploy changes to a custom pool, create a machine config that uses the custom pool name as the label, such as <literal>infra</literal>. Note that this is not required and only shown for instructional purposes. In this manner, you can apply any custom configurations specific to only your infra nodes.</simpara>
<note>
<simpara>After you create the new machine config pool, the MCO generates a new rendered config for that pool, and associated nodes of that pool reboot to apply the new configuration.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a machine config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat infra.mc.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 51-infra
  labels:
    machineconfiguration.openshift.io/role: infra <co xml:id="CO56-1"/>
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - path: /etc/infratest
        mode: 0644
        contents:
          source: data:,infra</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO56-1">
<para>Add the label you added to the node as a <literal>nodeSelector</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the machine config to the infra-labeled nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f infra.mc.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Confirm that your new machine config pool is available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-infra-60e35c2e99f42d976e084fa94da4d0fc    True      False      False      1              1                   1                     0                      4m20s
master   rendered-master-9360fdb895d4c131c7c4bebbae099c90   True      False      False      3              3                   3                     0                      91m
worker   rendered-worker-60e35c2e99f42d976e084fa94da4d0fc   True      False      False      2              2                   2                     0                      91m</programlisting>
</para>
</formalpara>
<simpara>In this example, a worker node was changed to an infra node.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="../architecture/control-plane.xml#architecture-machine-config-pools_control-plane">Node configuration management with machine config pools</link> for more information on grouping infra machines in a custom pool.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assigning-machine-set-resources-to-infra-nodes">
<title>Assigning machine set resources to infrastructure nodes</title>
<simpara>After creating an infrastructure machine set, the <literal>worker</literal> and <literal>infra</literal> roles are applied to new infra nodes. Nodes with the <literal>infra</literal> role are not counted toward the total number of subscriptions that are required to run the environment, even when the <literal>worker</literal> role is also applied.</simpara>
<simpara>However, when an infra node is assigned the worker role, there is a chance that user workloads can get assigned inadvertently to the infra node. To avoid this, you can apply a taint to the infra node and tolerations for the pods that you want to control.</simpara>
<section xml:id="binding-infra-node-workloads-using-taints-tolerations_post-install-cluster-tasks">
<title>Binding infrastructure node workloads using taints and tolerations</title>
<simpara>If you have an infra node that has the <literal>infra</literal> and <literal>worker</literal> roles assigned, you must configure the node so that user workloads are not assigned to it.</simpara>
<important>
<simpara>It is recommended that you preserve the dual <literal>infra,worker</literal> label that is created for infra nodes and use taints and tolerations to manage nodes that user workloads are scheduled on. If you remove the <literal>worker</literal> label from the node, you must create a custom pool to manage it. A node with a label other than <literal>master</literal> or <literal>worker</literal> is not recognized by the MCO without a custom pool. Maintaining the <literal>worker</literal> label allows the node to be managed by the default worker machine config pool, if no custom pools that select the custom label exists. The <literal>infra</literal> label communicates to the cluster that it does not count toward the total number of subscriptions.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Configure additional <literal>MachineSet</literal> objects in your OpenShift Container Platform cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add a taint to the infra node to prevent scheduling user workloads on it:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Determine if the node has the taint:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe nodes &lt;node_name&gt;</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">oc describe node ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Name:               ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Roles:              worker
 ...
Taints:             node-role.kubernetes.io/infra:NoSchedule
 ...</programlisting>
</para>
</formalpara>
<simpara>This example shows that the node has a taint. You can proceed with adding a toleration to your pod in the next step.</simpara>
</listitem>
<listitem>
<simpara>If you have not configured a taint to prevent scheduling user workloads on it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes &lt;node_name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes node1 node-role.kubernetes.io/infra=reserved:NoExecute</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add the taint:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Node
apiVersion: v1
metadata:
  name: &lt;node_name&gt;
  labels:
    ...
spec:
  taints:
    - key: node-role.kubernetes.io/infra
      effect: NoExecute
      value: reserved
  ...</programlisting>
</tip>
<simpara>This example places a taint on <literal>node1</literal> that has key <literal>node-role.kubernetes.io/infra</literal> and taint effect <literal>NoSchedule</literal>. Nodes with the <literal>NoSchedule</literal> effect schedule only pods that tolerate the taint, but allow existing pods to remain scheduled on the node.</simpara>
<note>
<simpara>If a descheduler is used, pods violating node taints could be evicted from the cluster.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Add tolerations for the pod configurations you want to schedule on the infra node, like router, registry, and monitoring workloads. Add the following code to the <literal>Pod</literal> object specification:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">tolerations:
  - effect: NoExecute <co xml:id="CO57-1"/>
    key: node-role.kubernetes.io/infra <co xml:id="CO57-2"/>
    operator: Exists <co xml:id="CO57-3"/>
    value: reserved <co xml:id="CO57-4"/></programlisting>
<calloutlist>
<callout arearefs="CO57-1">
<para>Specify the effect that you added to the node.</para>
</callout>
<callout arearefs="CO57-2">
<para>Specify the key that you added to the node.</para>
</callout>
<callout arearefs="CO57-3">
<para>Specify the <literal>Exists</literal> Operator to require a taint with the key <literal>node-role.kubernetes.io/infra</literal> to be present on the node.</para>
</callout>
<callout arearefs="CO57-4">
<para>Specify the value of the key-value pair taint that you added to the node.</para>
</callout>
</calloutlist>
<simpara>This toleration matches the taint created by the <literal>oc adm taint</literal> command. A pod with this toleration can be scheduled onto the infra node.</simpara>
<note>
<simpara>Moving pods for an Operator installed via OLM to an infra node is not always possible. The capability to move Operator pods depends on the configuration of each Operator.</simpara>
</note>
</listitem>
<listitem>
<simpara>Schedule the pod to the infra node using a scheduler. See the documentation for <emphasis>Controlling pod placement onto nodes</emphasis> for details.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="../nodes/scheduling/nodes-scheduler-about.xml#nodes-scheduler-about">Controlling pod placement using the scheduler</link> for general information on scheduling a pod to a node.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="moving-resources-to-infrastructure-machinesets">
<title>Moving resources to infrastructure machine sets</title>
<simpara>Some of the infrastructure resources are deployed in your cluster by default. You can move them to the infrastructure machine sets that you created.</simpara>
<section xml:id="infrastructure-moving-router_post-install-cluster-tasks">
<title>Moving the router</title>
<simpara>You can deploy the router pod to a different compute machine set. By default, the pod is deployed to a worker node.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Configure additional compute machine sets in your OpenShift Container Platform cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the <literal>IngressController</literal> custom resource for the router Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml</programlisting>
<simpara>The command output resembles the following text:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-18T12:35:39Z
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 1
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "11341"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 79509e05-61d6-11e9-bc55-02ce4781844a
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-18T12:36:15Z
    status: "True"
    type: Available
  domain: apps.&lt;cluster&gt;.example.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>ingresscontroller</literal> resource and change the <literal>nodeSelector</literal> to use the <literal>infra</literal> label:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit ingresscontroller default -n openshift-ingress-operator</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">  spec:
    nodePlacement:
      nodeSelector: <co xml:id="CO58-1"/>
        matchLabels:
          node-role.kubernetes.io/infra: ""
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved</programlisting>
<calloutlist>
<callout arearefs="CO58-1">
<para>Add a <literal>nodeSelector</literal> parameter with the appropriate value to the component you want to move. You can use a <literal>nodeSelector</literal> in the format shown or use <literal>&lt;key&gt;: &lt;value&gt;</literal> pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Confirm that the router pod is running on the <literal>infra</literal> node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>View the list of router pods and note the node name of the running pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -n openshift-ingress -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                           NOMINATED NODE   READINESS GATES
router-default-86798b4b5d-bdlvd   1/1      Running       0          28s       10.130.2.4   ip-10-0-217-226.ec2.internal   &lt;none&gt;           &lt;none&gt;
router-default-955d875f4-255g8    0/1      Terminating   0          19h       10.129.2.4   ip-10-0-148-172.ec2.internal   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
<simpara>In this example, the running pod is on the <literal>ip-10-0-217-226.ec2.internal</literal> node.</simpara>
</listitem>
<listitem>
<simpara>View the node status of the running pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get node &lt;node_name&gt; <co xml:id="CO59-1"/></programlisting>
<calloutlist>
<callout arearefs="CO59-1">
<para>Specify the <literal>&lt;node_name&gt;</literal> that you obtained from the pod list.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                          STATUS  ROLES         AGE   VERSION
ip-10-0-217-226.ec2.internal  Ready   infra,worker  17h   v1.28.5</programlisting>
</para>
</formalpara>
<simpara>Because the role list includes <literal>infra</literal>, the pod is running on the correct node.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="infrastructure-moving-registry_post-install-cluster-tasks">
<title>Moving the default registry</title>
<simpara>You configure the registry Operator to deploy its pods to different nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Configure additional compute machine sets in your OpenShift Container Platform cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the <literal>config/instance</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: 2019-02-05T13:52:05Z
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 1
  name: cluster
  resourceVersion: "56174"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 36fd3724-294d-11e9-a524-12ffeee2931b
spec:
  httpSecret: d9a012ccd117b1e6616ceccb2c3bb66a5fed1b5e481623
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 1
  requests:
    read: {}
    write: {}
  storage:
    s3:
      bucket: image-registry-us-east-1-c92e88cad85b48ec8b312344dff03c82-392c
      region: us-east-1
status:
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Edit the <literal>config/instance</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit configs.imageregistry.operator.openshift.io/cluster</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          namespaces:
          - openshift-image-registry
          topologyKey: kubernetes.io/hostname
        weight: 100
  logLevel: Normal
  managementState: Managed
  nodeSelector: <co xml:id="CO60-1"/>
    node-role.kubernetes.io/infra: ""
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/infra
    value: reserved
  - effect: NoExecute
    key: node-role.kubernetes.io/infra
    value: reserved</programlisting>
<calloutlist>
<callout arearefs="CO60-1">
<para>Add a <literal>nodeSelector</literal> parameter with the appropriate value to the component you want to move. You can use a <literal>nodeSelector</literal> in the format shown or use <literal>&lt;key&gt;: &lt;value&gt;</literal> pairs, based on the value specified for the node.  If you added a taint to the infrasructure node, also add a matching toleration.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify the registry pod has been moved to the infrastructure node.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to identify the node where the registry pod is located:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -o wide -n openshift-image-registry</programlisting>
</listitem>
<listitem>
<simpara>Confirm the node has the label you specified:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
<simpara>Review the command output and confirm that <literal>node-role.kubernetes.io/infra</literal> is in the <literal>LABELS</literal> list.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="infrastructure-moving-monitoring_post-install-cluster-tasks">
<title>Moving the monitoring solution</title>
<simpara>The monitoring stack includes multiple components, including Prometheus, Thanos Querier, and Alertmanager.
The Cluster Monitoring Operator manages this stack. To redeploy the monitoring stack to infrastructure nodes, you can create and apply a custom config map.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> config map and change the <literal>nodeSelector</literal> to use the <literal>infra</literal> label:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit configmap cluster-monitoring-config -n openshift-monitoring</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector: <co xml:id="CO61-1"/>
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute</programlisting>
<calloutlist>
<callout arearefs="CO61-1">
<para>Add a <literal>nodeSelector</literal> parameter with the appropriate value to the component you want to move. You can use a <literal>nodeSelector</literal> in the format shown or use <literal>&lt;key&gt;: &lt;value&gt;</literal> pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Watch the monitoring pods move to the new machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch 'oc get pod -n openshift-monitoring -o wide'</programlisting>
</listitem>
<listitem>
<simpara>If a component has not moved to the <literal>infra</literal> node, delete the pod with this component:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod -n openshift-monitoring &lt;pod&gt;</programlisting>
<simpara>The component from the deleted pod is re-created on the <literal>infra</literal> node.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="infrastructure-moving-logging_post-install-cluster-tasks">
<title>Moving logging resources</title>
<simpara>You can configure the Red Hat OpenShift Logging Operator to deploy the pods for logging components, such as Elasticsearch and Kibana, to different nodes. You cannot move the Red Hat OpenShift Logging Operator pod from its installed location.</simpara>
<simpara>For example, you can move the Elasticsearch pods to a separate node because of high CPU, memory, and disk requirements.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the Red Hat OpenShift Logging Operator and the OpenShift Elasticsearch Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ClusterLogging</literal> custom resource (CR) in the <literal>openshift-logging</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit ClusterLogging instance</programlisting>
<formalpara>
<title>Example <literal>ClusterLogging</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
# ...
spec:
  logStore:
    elasticsearch:
      nodeCount: 3
      nodeSelector: <co xml:id="CO62-1"/>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      redundancyPolicy: SingleRedundancy
      resources:
        limits:
          cpu: 500m
          memory: 16Gi
        requests:
          cpu: 500m
          memory: 16Gi
      storage: {}
    type: elasticsearch
  managementState: Managed
  visualization:
    kibana:
      nodeSelector: <co xml:id="CO62-2"/>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO62-1 CO62-2">
<para>Add a <literal>nodeSelector</literal> parameter with the appropriate value to the component you want to move. You can use a <literal>nodeSelector</literal> in the format shown or use <literal>&lt;key&gt;: &lt;value&gt;</literal> pairs, based on the value specified for the node.  If you added a taint to the infrasructure node, also add a matching toleration.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>To verify that a component has moved, you can use the <literal>oc get pod -o wide</literal> command.</para>
</formalpara>
<simpara>For example:</simpara>
<itemizedlist>
<listitem>
<simpara>You want to move the Kibana pod from the <literal>ip-10-0-147-79.us-east-2.compute.internal</literal> node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod kibana-5b8bdf44f9-ccpq9 -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-5b8bdf44f9-ccpq9   2/2     Running   0          27s   10.129.2.18   ip-10-0-147-79.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>You want to move the Kibana pod to the <literal>ip-10-0-139-48.us-east-2.compute.internal</literal> node, a dedicated infrastructure node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-133-216.us-east-2.compute.internal   Ready    master         60m   v1.28.5
ip-10-0-139-146.us-east-2.compute.internal   Ready    master         60m   v1.28.5
ip-10-0-139-192.us-east-2.compute.internal   Ready    worker         51m   v1.28.5
ip-10-0-139-241.us-east-2.compute.internal   Ready    worker         51m   v1.28.5
ip-10-0-147-79.us-east-2.compute.internal    Ready    worker         51m   v1.28.5
ip-10-0-152-241.us-east-2.compute.internal   Ready    master         60m   v1.28.5
ip-10-0-139-48.us-east-2.compute.internal    Ready    infra          51m   v1.28.5</programlisting>
</para>
</formalpara>
<simpara>Note that the node has a <literal>node-role.kubernetes.io/infra: ''</literal> label:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get node ip-10-0-139-48.us-east-2.compute.internal -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: Node
apiVersion: v1
metadata:
  name: ip-10-0-139-48.us-east-2.compute.internal
  selfLink: /api/v1/nodes/ip-10-0-139-48.us-east-2.compute.internal
  uid: 62038aa9-661f-41d7-ba93-b5f1b6ef8751
  resourceVersion: '39083'
  creationTimestamp: '2020-04-13T19:07:55Z'
  labels:
    node-role.kubernetes.io/infra: ''
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To move the Kibana pod, edit the <literal>ClusterLogging</literal> CR to add a node selector:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: logging.openshift.io/v1
kind: ClusterLogging
# ...
spec:
# ...
  visualization:
    kibana:
      nodeSelector: <co xml:id="CO63-1"/>
        node-role.kubernetes.io/infra: ''
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana</programlisting>
<calloutlist>
<callout arearefs="CO63-1">
<para>Add a node selector to match the label in the node specification.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>After you save the CR, the current Kibana pod is terminated and new pod is deployed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                            READY   STATUS        RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running       0          29m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running       0          28m
collector-42dzz                                 1/1     Running       0          28m
collector-d74rq                                 1/1     Running       0          28m
collector-m5vr9                                 1/1     Running       0          28m
collector-nkxl7                                 1/1     Running       0          28m
collector-pdvqb                                 1/1     Running       0          28m
collector-tflh6                                 1/1     Running       0          28m
kibana-5b8bdf44f9-ccpq9                         2/2     Terminating   0          4m11s
kibana-7d85dcffc8-bfpfp                         2/2     Running       0          33s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>The new pod is on the <literal>ip-10-0-139-48.us-east-2.compute.internal</literal> node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod kibana-7d85dcffc8-bfpfp -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                      READY   STATUS        RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-7d85dcffc8-bfpfp   2/2     Running       0          43s   10.131.0.22   ip-10-0-139-48.us-east-2.compute.internal   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>After a few moments, the original Kibana pod is removed.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                            READY   STATUS    RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running   0          30m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running   0          29m
collector-42dzz                                 1/1     Running   0          29m
collector-d74rq                                 1/1     Running   0          29m
collector-m5vr9                                 1/1     Running   0          29m
collector-nkxl7                                 1/1     Running   0          29m
collector-pdvqb                                 1/1     Running   0          29m
collector-tflh6                                 1/1     Running   0          29m
kibana-7d85dcffc8-bfpfp                         2/2     Running   0          62s</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cluster-autoscaler-about_post-install-cluster-tasks">
<title>About the cluster autoscaler</title>
<simpara>The cluster autoscaler adjusts the size of an OpenShift Container Platform cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler has a cluster scope, and is not associated with a particular namespace.</simpara>
<simpara>The cluster autoscaler increases the size of the cluster when there are pods that fail to schedule on any of the current worker nodes due to insufficient resources or when another node is necessary to meet deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.</simpara>
<simpara>The cluster autoscaler computes the total
memory, CPU, and GPU
on all nodes the cluster, even though it does not manage the control plane nodes. These values are not single-machine oriented. They are an aggregation of all the resources in the entire cluster. For example, if you set the maximum memory resource limit, the cluster autoscaler includes all the nodes in the cluster when calculating the current memory usage. That calculation is then used to determine if the cluster autoscaler has the capacity to add more worker resources.</simpara>
<important>
<simpara>Ensure that the <literal>maxNodesTotal</literal> value in the <literal>ClusterAutoscaler</literal> resource definition that you create is large enough to account for the total possible number of machines in your cluster. This value must encompass the number of control plane machines and the possible number of compute machines that you might scale to.</simpara>
</important>
<simpara>Every 10 seconds, the cluster autoscaler checks which nodes are unnecessary in the cluster and removes them. The cluster autoscaler considers a node for removal if the following conditions apply:</simpara>
<itemizedlist>
<listitem>
<simpara>The node utilization is less than the <emphasis>node utilization level</emphasis> threshold for the cluster. The node utilization level is the sum of the requested resources divided by the allocated resources for the node. If you do not specify a value in the <literal>ClusterAutoscaler</literal> custom resource, the cluster autoscaler uses a default value of <literal>0.5</literal>, which corresponds to 50% utilization.</simpara>
</listitem>
<listitem>
<simpara>The cluster autoscaler can move all pods running on the node to the other nodes. The Kubernetes scheduler is responsible for scheduling pods on the nodes.</simpara>
</listitem>
<listitem>
<simpara>The cluster autoscaler does not have scale down disabled annotation.</simpara>
</listitem>
</itemizedlist>
<simpara>If the following types of pods are present on a node, the cluster autoscaler will not remove the node:</simpara>
<itemizedlist>
<listitem>
<simpara>Pods with restrictive pod disruption budgets (PDBs).</simpara>
</listitem>
<listitem>
<simpara>Kube-system pods that do not run on the node by default.</simpara>
</listitem>
<listitem>
<simpara>Kube-system pods that do not have a PDB or have a PDB that is too restrictive.</simpara>
</listitem>
<listitem>
<simpara>Pods that are not backed by a controller object such as a deployment, replica set, or stateful set.</simpara>
</listitem>
<listitem>
<simpara>Pods with local storage.</simpara>
</listitem>
<listitem>
<simpara>Pods that cannot be moved elsewhere because of a lack of resources, incompatible node selectors or affinity, matching anti-affinity, and so on.</simpara>
</listitem>
<listitem>
<simpara>Unless they also have a <literal>"cluster-autoscaler.kubernetes.io/safe-to-evict": "true"</literal> annotation, pods that have a <literal>"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"</literal> annotation.</simpara>
</listitem>
</itemizedlist>
<simpara>For example, you set the maximum CPU limit to 64 cores and configure the cluster autoscaler to only create machines that have 8 cores each. If your cluster starts with 30 cores, the cluster autoscaler can add up to 4 more nodes with 32 cores, for a total of 62.</simpara>
<simpara>If you configure the cluster autoscaler, additional usage restrictions apply:</simpara>
<itemizedlist>
<listitem>
<simpara>Do not modify the nodes that are in autoscaled node groups directly. All nodes within the same node group have the same capacity and labels and run the same system pods.</simpara>
</listitem>
<listitem>
<simpara>Specify requests for your pods.</simpara>
</listitem>
<listitem>
<simpara>If you have to prevent pods from being deleted too quickly, configure appropriate PDBs.</simpara>
</listitem>
<listitem>
<simpara>Confirm that your cloud provider quota is large enough to support the maximum node pools that you configure.</simpara>
</listitem>
<listitem>
<simpara>Do not run additional node group autoscalers, especially the ones offered by your cloud provider.</simpara>
</listitem>
</itemizedlist>
<simpara>The horizontal pod autoscaler (HPA) and the cluster autoscaler modify cluster resources in different ways. The HPA changes the deployment&#8217;s or replica set&#8217;s number of replicas based on the current CPU load. If the load increases, the HPA creates new replicas, regardless of the amount of resources available to the cluster. If there are not enough resources, the cluster autoscaler adds resources so that the HPA-created pods can run. If the load decreases, the HPA stops some replicas. If this action causes some nodes to be underutilized or completely empty, the cluster autoscaler deletes the unnecessary nodes.</simpara>
<simpara>The cluster autoscaler takes pod priorities into account. The Pod Priority and Preemption feature enables scheduling pods based on priorities if the cluster does not have enough resources, but the cluster autoscaler ensures that the cluster has resources to run all pods. To honor the intention of both features, the cluster autoscaler includes a priority cutoff function. You can use this cutoff to schedule "best-effort" pods, which do not cause the cluster autoscaler to increase resources but instead run only when spare resources are available.</simpara>
<simpara>Pods with priority lower than the cutoff value do not cause the cluster to scale up or prevent the cluster from scaling down. No new nodes are added to run the pods, and nodes running these pods might be deleted to free resources.</simpara>
<simpara>Cluster autoscaling is supported for the platforms that have machine API available on it.</simpara>
<section xml:id="cluster-autoscaler-cr_post-install-cluster-tasks">
<title>Cluster autoscaler resource definition</title>
<simpara>This <literal>ClusterAutoscaler</literal> resource definition shows the parameters and sample values for the cluster autoscaler.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 <co xml:id="CO64-1"/>
  resourceLimits:
    maxNodesTotal: 24 <co xml:id="CO64-2"/>
    cores:
      min: 8 <co xml:id="CO64-3"/>
      max: 128 <co xml:id="CO64-4"/>
    memory:
      min: 4 <co xml:id="CO64-5"/>
      max: 256 <co xml:id="CO64-6"/>
    gpus:
      - type: nvidia.com/gpu <co xml:id="CO64-7"/>
        min: 0 <co xml:id="CO64-8"/>
        max: 16 <co xml:id="CO64-9"/>
      - type: amd.com/gpu
        min: 0
        max: 4
  logVerbosity: 4 <co xml:id="CO64-10"/>
  scaleDown: <co xml:id="CO64-11"/>
    enabled: true <co xml:id="CO64-12"/>
    delayAfterAdd: 10m <co xml:id="CO64-13"/>
    delayAfterDelete: 5m <co xml:id="CO64-14"/>
    delayAfterFailure: 30s <co xml:id="CO64-15"/>
    unneededTime: 5m <co xml:id="CO64-16"/>
    utilizationThreshold: "0.4" <co xml:id="CO64-17"/></programlisting>
<calloutlist>
<callout arearefs="CO64-1">
<para>Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The <literal>podPriorityThreshold</literal> value is compared to the value of the <literal>PriorityClass</literal> that you assign to each pod.</para>
</callout>
<callout arearefs="CO64-2">
<para>Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your <literal>MachineAutoscaler</literal> resources.</para>
</callout>
<callout arearefs="CO64-3">
<para>Specify the minimum number of cores to deploy in the cluster.</para>
</callout>
<callout arearefs="CO64-4">
<para>Specify the maximum number of cores to deploy in the cluster.</para>
</callout>
<callout arearefs="CO64-5">
<para>Specify the minimum amount of memory, in GiB, in the cluster.</para>
</callout>
<callout arearefs="CO64-6">
<para>Specify the maximum amount of memory, in GiB, in the cluster.</para>
</callout>
<callout arearefs="CO64-7">
<para>Optional: Specify the type of GPU node to deploy. Only <literal>nvidia.com/gpu</literal> and <literal>amd.com/gpu</literal> are valid types.</para>
</callout>
<callout arearefs="CO64-8">
<para>Specify the minimum number of GPUs to deploy in the cluster.</para>
</callout>
<callout arearefs="CO64-9">
<para>Specify the maximum number of GPUs to deploy in the cluster.</para>
</callout>
<callout arearefs="CO64-10">
<para>Specify the logging verbosity level between <literal>0</literal> and <literal>10</literal>. The following log level thresholds are provided for guidance:</para>
<itemizedlist>
<listitem>
<simpara><literal>1</literal>: (Default) Basic information about changes.</simpara>
</listitem>
<listitem>
<simpara><literal>4</literal>: Debug-level verbosity for troubleshooting typical issues.</simpara>
</listitem>
<listitem>
<simpara><literal>9</literal>: Extensive, protocol-level debugging information.</simpara>
</listitem>
</itemizedlist>
<simpara>If you do not specify a value, the default value of <literal>1</literal> is used.</simpara>
</callout>
<callout arearefs="CO64-11">
<para>In this section, you can specify the period to wait for each action by using any valid <link xlink:href="https://golang.org/pkg/time/#ParseDuration">ParseDuration</link> interval, including <literal>ns</literal>, <literal>us</literal>, <literal>ms</literal>, <literal>s</literal>, <literal>m</literal>, and <literal>h</literal>.</para>
</callout>
<callout arearefs="CO64-12">
<para>Specify whether the cluster autoscaler can remove unnecessary nodes.</para>
</callout>
<callout arearefs="CO64-13">
<para>Optional: Specify the period to wait before deleting a node after a node has recently been <emphasis>added</emphasis>. If you do not specify a value, the default value of <literal>10m</literal> is used.</para>
</callout>
<callout arearefs="CO64-14">
<para>Optional: Specify the period to wait before deleting a node after a node has recently been <emphasis>deleted</emphasis>. If you do not specify a value, the default value of <literal>0s</literal> is used.</para>
</callout>
<callout arearefs="CO64-15">
<para>Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of <literal>3m</literal> is used.</para>
</callout>
<callout arearefs="CO64-16">
<para>Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of <literal>10m</literal> is used.</para>
</callout>
<callout arearefs="CO64-17">
<para>Optional:  Specify the <emphasis>node utilization level</emphasis>. Nodes below this utilization level are eligible for deletion. If you do not specify a value, the default value of <literal>10m</literal> is used.. The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than <literal>"0"</literal> but less than <literal>"1"</literal>. If you do not specify a value, the cluster autoscaler uses a default value of <literal>"0.5"</literal>, which corresponds to 50% utilization. This value must be expressed as a string.</para>
</callout>
</calloutlist>
<note>
<simpara>When performing a scaling operation, the cluster autoscaler remains within the ranges set in the <literal>ClusterAutoscaler</literal> resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.</simpara>
<simpara>The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.</simpara>
</note>
</section>
<section xml:id="ClusterAutoscaler-deploying_post-install-cluster-tasks">
<title>Deploying a cluster autoscaler</title>
<simpara>To deploy a cluster autoscaler, you create an instance of the <literal>ClusterAutoscaler</literal> resource.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for a <literal>ClusterAutoscaler</literal> resource that contains the custom resource definition.</simpara>
</listitem>
<listitem>
<simpara>Create the custom resource in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;filename&gt;.yaml <co xml:id="CO65-1"/></programlisting>
<calloutlist>
<callout arearefs="CO65-1">
<para><literal>&lt;filename&gt;</literal> is the name of the custom resource file.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="machine-autoscaler-about_post-install-cluster-tasks">
<title>About the machine autoscaler</title>
<simpara>The machine autoscaler adjusts the number of Machines in the compute machine sets that you deploy in an OpenShift Container Platform cluster. You can scale both the default <literal>worker</literal> compute machine set and any other compute machine sets that you create. The machine autoscaler makes more Machines when the cluster runs out of resources to support more deployments. Any changes to the values in <literal>MachineAutoscaler</literal> resources, such as the minimum or maximum number of instances, are immediately applied to the compute machine set they target.</simpara>
<important>
<simpara>You must deploy a machine autoscaler for the cluster autoscaler to scale your machines. The cluster autoscaler uses the annotations on compute machine sets that the machine autoscaler sets to determine the resources that it can scale. If you define a cluster autoscaler without also defining machine autoscalers, the cluster autoscaler will never scale your cluster.</simpara>
</important>
<section xml:id="machine-autoscaler-cr_post-install-cluster-tasks">
<title>Machine autoscaler resource definition</title>
<simpara>This <literal>MachineAutoscaler</literal> resource definition shows the parameters and sample values for the machine autoscaler.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "autoscaling.openshift.io/v1beta1"
kind: "MachineAutoscaler"
metadata:
  name: "worker-us-east-1a" <co xml:id="CO66-1"/>
  namespace: "openshift-machine-api"
spec:
  minReplicas: 1 <co xml:id="CO66-2"/>
  maxReplicas: 12 <co xml:id="CO66-3"/>
  scaleTargetRef: <co xml:id="CO66-4"/>
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet <co xml:id="CO66-5"/>
    name: worker-us-east-1a <co xml:id="CO66-6"/></programlisting>
<calloutlist>
<callout arearefs="CO66-1">
<para>Specify the machine autoscaler name. To make it easier to identify which compute machine set this machine autoscaler scales, specify or include the name of the compute machine set to scale. The compute machine set name takes the following form: <literal>&lt;clusterid&gt;-&lt;machineset&gt;-&lt;region&gt;</literal>.</para>
</callout>
<callout arearefs="CO66-2">
<para>Specify the minimum number machines of the specified type that must remain in the specified zone after the cluster autoscaler initiates cluster scaling. If running in AWS, GCP, Azure, RHOSP, or vSphere, this value can be set to <literal>0</literal>. For other providers, do not set this value to <literal>0</literal>.</para>
<simpara>You can save on costs by setting this value to <literal>0</literal> for use cases such as running expensive or limited-usage hardware that is used for specialized workloads, or by scaling a compute machine set with extra large machines. The cluster autoscaler scales the compute machine set down to zero if the machines are not in use.</simpara>
<important>
<simpara>Do not set the <literal>spec.minReplicas</literal> value to <literal>0</literal> for the three compute machine sets that are created during the OpenShift Container Platform installation process for an installer provisioned infrastructure.</simpara>
</important>
</callout>
<callout arearefs="CO66-3">
<para>Specify the maximum number machines of the specified type that the cluster autoscaler can deploy in the specified zone after it initiates cluster scaling. Ensure that the <literal>maxNodesTotal</literal> value in the <literal>ClusterAutoscaler</literal> resource definition is large enough to allow the machine autoscaler to deploy this number of machines.</para>
</callout>
<callout arearefs="CO66-4">
<para>In this section, provide values that describe the existing compute machine set to scale.</para>
</callout>
<callout arearefs="CO66-5">
<para>The <literal>kind</literal> parameter value is always <literal>MachineSet</literal>.</para>
</callout>
<callout arearefs="CO66-6">
<para>The <literal>name</literal> value must match the name of an existing compute machine set, as shown in the <literal>metadata.name</literal> parameter value.</para>
</callout>
</calloutlist>
</section>
<section xml:id="MachineAutoscaler-deploying_post-install-cluster-tasks">
<title>Deploying a machine autoscaler</title>
<simpara>To deploy a machine autoscaler, you create an instance of the <literal>MachineAutoscaler</literal> resource.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for a <literal>MachineAutoscaler</literal> resource that contains the custom resource definition.</simpara>
</listitem>
<listitem>
<simpara>Create the custom resource in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;filename&gt;.yaml <co xml:id="CO67-1"/></programlisting>
<calloutlist>
<callout arearefs="CO67-1">
<para><literal>&lt;filename&gt;</literal> is the name of the custom resource file.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nodes-clusters-cgroups-2_post-install-cluster-tasks">
<title>Configuring Linux cgroup</title>
<simpara>As of OpenShift Container Platform 4.14, OpenShift Container Platform uses <link xlink:href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html">Linux control group version 2</link> (cgroup v2) in your cluster. If you are using cgroup v1 on OpenShift Container Platform 4.13 or earlier, migrating to OpenShift Container Platform 4.14 or later will not automatically update your cgroup configuration to version 2. A fresh installation of OpenShift Container Platform 4.14 or later will use cgroup v2 by default. However, you can enable <link xlink:href="https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html">Linux control group version 1</link> (cgroup v1) upon installation.</simpara>
<simpara>cgroup v2 is the current version of the Linux cgroup API. cgroup v2 offers several improvements over cgroup v1, including a unified hierarchy, safer sub-tree delegation, new features such as <link xlink:href="https://www.kernel.org/doc/html/latest/accounting/psi.html">Pressure Stall Information</link>, and enhanced resource management and isolation. However, cgroup v2 has different CPU, memory, and I/O management characteristics than cgroup v1. Therefore, some workloads might experience slight differences in memory or CPU usage on clusters that run cgroup v2.</simpara>
<simpara>You can change between cgroup v1 and cgroup v2, as needed. Enabling cgroup v1 in OpenShift Container Platform disables all cgroup v2 controllers and hierarchies in your cluster.</simpara>
<note>
<simpara>Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have a running OpenShift Container Platform cluster that uses version 4.12 or later.</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster as a user with administrative privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable cgroup v1 on nodes:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>node.config</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit nodes.config/cluster</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>spec.cgroupMode: "v1"</literal>:</simpara>
<formalpara>
<title>Example <literal>node.config</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v2
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v2
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  cgroupMode: "v1" <co xml:id="CO68-1"/>
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO68-1">
<para>Enables cgroup v1.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Check the machine configs to see that the new machine configs were added:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mc</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
97-master-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23d4317815a5f854bd3553d689cfe2e9   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s <co xml:id="CO69-1"/>
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-dcc7f1b92892d34db74d6832bcc9ccd4   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO69-1">
<para>New machine configs are created, as expected.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check that the new <literal>kernelArguments</literal> were added to the new machine configs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe mc &lt;name&gt;</programlisting>
<formalpara>
<title>Example output for cgroup v1</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v2
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-selinuxpermissive
spec:
  kernelArguments:
    systemd.unified_cgroup_hierarchy=0 <co xml:id="CO70-1"/>
    systemd.legacy_systemd_cgroup_controller=1 <co xml:id="CO70-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO70-1">
<para>Disables cgroup v2.</para>
</callout>
<callout arearefs="CO70-2">
<para>Enables cgroup v1 in systemd.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       STATUS                     ROLES    AGE   VERSION
ci-ln-fm1qnwt-72292-99kt6-master-0         Ready,SchedulingDisabled   master   58m   v1.28.5
ci-ln-fm1qnwt-72292-99kt6-master-1         Ready                      master   58m   v1.28.5
ci-ln-fm1qnwt-72292-99kt6-master-2         Ready                      master   58m   v1.28.5
ci-ln-fm1qnwt-72292-99kt6-worker-a-h5gt4   Ready,SchedulingDisabled   worker   48m   v1.28.5
ci-ln-fm1qnwt-72292-99kt6-worker-b-7vtmd   Ready                      worker   48m   v1.28.5
ci-ln-fm1qnwt-72292-99kt6-worker-c-rhzkv   Ready                      worker   48m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>After a node returns to the <literal>Ready</literal> state, start a debug session for that node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Check that the <literal>sys/fs/cgroup/cgroup2fs</literal> file is present on your nodes. This file is created by cgroup v1:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ stat -c %T -f /sys/fs/cgroup</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">cgroup2fs</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../nodes/clusters/nodes-cluster-cgroups-2.xml#nodes-cluster-cgroups-2">Configuring the Linux cgroup version on your nodes</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="post-install-tp-tasks">
<title>Enabling Technology Preview features using FeatureGates</title>
<simpara>You can turn on a subset of the current Technology Preview features on for all nodes in the cluster by editing the <literal>FeatureGate</literal> custom resource (CR).</simpara>
<section xml:id="nodes-cluster-enabling-features-about_post-install-cluster-tasks">
<title>Understanding feature gates</title>
<simpara>You can use the <literal>FeatureGate</literal> custom resource (CR) to enable specific feature sets in your cluster. A feature set is a collection of OpenShift Container Platform features that are not enabled by default.</simpara>
<simpara>You can activate the following feature set by using the <literal>FeatureGate</literal> CR:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>TechPreviewNoUpgrade</literal>. This feature set is a subset of the current Technology Preview features. This feature set allows you to enable these Technology Preview features on test clusters, where you can fully test them, while leaving the features disabled on production clusters.</simpara>
<warning>
<simpara>Enabling the <literal>TechPreviewNoUpgrade</literal> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.</simpara>
</warning>
<simpara>The following Technology Preview features are enabled by this feature set:</simpara>
<itemizedlist>
<listitem>
<simpara>External cloud providers. Enables support for external cloud providers for clusters on vSphere, AWS, Azure, and GCP. Support for OpenStack is GA. This is an internal feature that most users do not need to interact with. (<literal>ExternalCloudProvider</literal>)</simpara>
</listitem>
<listitem>
<simpara>Shared Resources CSI Driver in OpenShift Builds. Enables the Container Storage Interface (CSI). (<literal>CSIDriverSharedResource</literal>)</simpara>
</listitem>
<listitem>
<simpara>Swap memory on nodes. Enables swap memory use for OpenShift Container Platform workloads on a per-node basis. (<literal>NodeSwap</literal>)</simpara>
</listitem>
<listitem>
<simpara>OpenStack Machine API Provider. This gate has no effect and is planned to be removed from this feature set in a future release. (<literal>MachineAPIProviderOpenStack</literal>)</simpara>
</listitem>
<listitem>
<simpara>Insights Operator. Enables the Insights Operator, which gathers OpenShift Container Platform configuration data and sends it to Red Hat. (<literal>InsightsConfigAPI</literal>)</simpara>
</listitem>
<listitem>
<simpara>Retroactive Default Storage Class. Enables OpenShift Container Platform to retroactively assign the default storage class to PVCs if there was no default storage class when the PVC was created.(<literal>RetroactiveDefaultStorageClass</literal>)</simpara>
</listitem>
<listitem>
<simpara>Dynamic Resource Allocation API. Enables a new API for requesting and sharing resources between pods and containers. This is an internal feature that most users do not need to interact with. (<literal>DynamicResourceAllocation</literal>)</simpara>
</listitem>
<listitem>
<simpara>Pod security admission enforcement. Enables the restricted enforcement mode for pod security admission. Instead of only logging a warning, pods are rejected if they violate pod security standards. (<literal>OpenShiftPodSecurityAdmission</literal>)</simpara>
</listitem>
<listitem>
<simpara>StatefulSet pod availability upgrading limits. Enables users to define the maximum number of statefulset pods unavailable during updates which reduces application downtime. (<literal>MaxUnavailableStatefulSet</literal>)</simpara>
</listitem>
<listitem>
<simpara>Admin Network Policy and Baseline Admin Network Policy. Enables <literal>AdminNetworkPolicy</literal> and <literal>BaselineAdminNetworkPolicy</literal> resources, which are part of the Network Policy V2 API, in clusters running the OVN-Kubernetes CNI plugin. Cluster administrators can apply cluster-scoped policies and safeguards for an entire cluster before namespaces are created. Network administrators can secure clusters by enforcing network traffic controls that cannot be overridden by users. Network administrators can enforce optional baseline network traffic controls that can be overridden by users in the cluster, if necessary. Currently, these APIs support only expressing policies for intra-cluster traffic. (<literal>AdminNetworkPolicy</literal>)</simpara>
</listitem>
<listitem>
<simpara><literal>MatchConditions</literal> is a list of conditions that must be met for a request to be sent to this webhook. Match conditions filter requests that have already been matched by the rules, namespaceSelector, and objectSelector. An empty list of <literal>matchConditions</literal> matches all requests. (<literal>admissionWebhookMatchConditions</literal>)</simpara>
</listitem>
<listitem>
<simpara>Gateway API. To enable the OpenShift Container Platform Gateway API, set the value of the <literal>enabled</literal> field to <literal>true</literal> in the <literal>techPreview.gatewayAPI</literal> specification of the <literal>ServiceMeshControlPlane</literal> resource.(<literal>gateGatewayAPI</literal>)</simpara>
</listitem>
<listitem>
<simpara><literal>gcpLabelsTags</literal></simpara>
</listitem>
<listitem>
<simpara><literal>vSphereStaticIPs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>routeExternalCertificate</literal></simpara>
</listitem>
<listitem>
<simpara><literal>automatedEtcdBackup</literal></simpara>
</listitem>
<listitem>
<simpara><literal>gcpClusterHostedDNS</literal></simpara>
</listitem>
<listitem>
<simpara><literal>vSphereControlPlaneMachineset</literal></simpara>
</listitem>
<listitem>
<simpara><literal>dnsNameResolver</literal></simpara>
</listitem>
<listitem>
<simpara><literal>machineConfigNodes</literal></simpara>
</listitem>
<listitem>
<simpara><literal>metricsServer</literal></simpara>
</listitem>
<listitem>
<simpara><literal>installAlternateInfrastructureAWS</literal></simpara>
</listitem>
<listitem>
<simpara><literal>sdnLiveMigration</literal></simpara>
</listitem>
<listitem>
<simpara><literal>mixedCPUsAllocation</literal></simpara>
</listitem>
<listitem>
<simpara><literal>managedBootImages</literal></simpara>
</listitem>
<listitem>
<simpara><literal>onClusterBuild</literal></simpara>
</listitem>
<listitem>
<simpara><literal>signatureStores</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="nodes-cluster-enabling-features-console_post-install-cluster-tasks">
<title>Enabling feature sets using the web console</title>
<simpara>You can use the OpenShift Container Platform web console to enable feature sets for all of the nodes in a cluster by editing the <literal>FeatureGate</literal> custom resource (CR).</simpara>
<formalpara>
<title>Procedure</title>
<para>To enable feature sets:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>In the OpenShift Container Platform web console, switch to the <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Custom Resource Definitions</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Custom Resource Definitions</emphasis> page, click <emphasis role="strong">FeatureGate</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Custom Resource Definition Details</emphasis> page, click the <emphasis role="strong">Instances</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">cluster</emphasis> feature gate, then click the <emphasis role="strong">YAML</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Edit the <emphasis role="strong">cluster</emphasis> instance to add specific feature sets:</simpara>
<warning>
<simpara>Enabling the <literal>TechPreviewNoUpgrade</literal> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.</simpara>
</warning>
<formalpara>
<title>Sample Feature Gate custom resource</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <co xml:id="CO71-1"/>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <co xml:id="CO71-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO71-1">
<para>The name of the <literal>FeatureGate</literal> CR must be <literal>cluster</literal>.</para>
</callout>
<callout arearefs="CO71-2">
<para>Add the feature set that you want to enable:</para>
<itemizedlist>
<listitem>
<simpara><literal>TechPreviewNoUpgrade</literal> enables specific Technology Preview features.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
<simpara>After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>You can verify that the feature gates are enabled by looking at the <literal>kubelet.conf</literal> file on a node after the nodes return to the ready state.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>From the <emphasis role="strong">Administrator</emphasis> perspective in the web console, navigate to <emphasis role="strong">Compute</emphasis> &#8594; <emphasis role="strong">Nodes</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a node.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Node details</emphasis> page, click <emphasis role="strong">Terminal</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the terminal window, change your root directory to <literal>/host</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>View the <literal>kubelet.conf</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# cat /etc/kubernetes/kubelet.conf</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</programlisting>
</para>
</formalpara>
<simpara>The features that are listed as <literal>true</literal> are enabled on your cluster.</simpara>
<note>
<simpara>The features listed vary depending upon the OpenShift Container Platform version.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-cluster-enabling-features-cli_post-install-cluster-tasks">
<title>Enabling feature sets using the CLI</title>
<simpara>You can use the OpenShift CLI (<literal>oc</literal>) to enable feature sets for all of the nodes in a cluster by editing the <literal>FeatureGate</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To enable feature sets:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>FeatureGate</literal> CR named <literal>cluster</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit featuregate cluster</programlisting>
<warning>
<simpara>Enabling the <literal>TechPreviewNoUpgrade</literal> feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.</simpara>
</warning>
<formalpara>
<title>Sample FeatureGate custom resource</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <co xml:id="CO72-1"/>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <co xml:id="CO72-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO72-1">
<para>The name of the <literal>FeatureGate</literal> CR must be <literal>cluster</literal>.</para>
</callout>
<callout arearefs="CO72-2">
<para>Add the feature set that you want to enable:</para>
<itemizedlist>
<listitem>
<simpara><literal>TechPreviewNoUpgrade</literal> enables specific Technology Preview features.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
<simpara>After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>You can verify that the feature gates are enabled by looking at the <literal>kubelet.conf</literal> file on a node after the nodes return to the ready state.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>From the <emphasis role="strong">Administrator</emphasis> perspective in the web console, navigate to <emphasis role="strong">Compute</emphasis> &#8594; <emphasis role="strong">Nodes</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a node.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Node details</emphasis> page, click <emphasis role="strong">Terminal</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the terminal window, change your root directory to <literal>/host</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>View the <literal>kubelet.conf</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# cat /etc/kubernetes/kubelet.conf</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...</programlisting>
</para>
</formalpara>
<simpara>The features that are listed as <literal>true</literal> are enabled on your cluster.</simpara>
<note>
<simpara>The features listed vary depending upon the OpenShift Container Platform version.</simpara>
</note>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-etcd-tasks">
<title>etcd tasks</title>
<simpara>Back up etcd, enable or disable etcd encryption, or defragment etcd data.</simpara>
<section xml:id="about-etcd_post-install-cluster-tasks">
<title>About etcd encryption</title>
<simpara>By default, etcd data is not encrypted in OpenShift Container Platform. You can enable etcd encryption for your cluster to provide an additional layer of data security. For example, it can help protect the loss of sensitive data if an etcd backup is exposed to the incorrect parties.</simpara>
<simpara>When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:</simpara>
<itemizedlist>
<listitem>
<simpara>Secrets</simpara>
</listitem>
<listitem>
<simpara>Config maps</simpara>
</listitem>
<listitem>
<simpara>Routes</simpara>
</listitem>
<listitem>
<simpara>OAuth access tokens</simpara>
</listitem>
<listitem>
<simpara>OAuth authorize tokens</simpara>
</listitem>
</itemizedlist>
<simpara>When you enable etcd encryption, encryption keys are created. You must have these keys to restore from an etcd backup.</simpara>
<note>
<simpara>Etcd encryption only encrypts values, not keys. Resource types, namespaces, and object names are unencrypted.</simpara>
<simpara>If etcd encryption is enabled during a backup, the <literal><emphasis>static_kuberesources_&lt;datetimestamp&gt;.tar.gz</emphasis></literal> file contains the encryption keys for the etcd snapshot. For security reasons, store this file separately from the etcd snapshot. However, this file is required to restore a previous state of etcd from the respective etcd snapshot.</simpara>
</note>
</section>
<section xml:id="etcd-encryption-types_post-install-cluster-tasks">
<title>Supported encryption types</title>
<simpara>The following encryption types are supported for encrypting etcd data in OpenShift Container Platform:</simpara>
<variablelist>
<varlistentry>
<term>AES-CBC</term>
<listitem>
<simpara>Uses AES-CBC with PKCS#7 padding and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>AES-GCM</term>
<listitem>
<simpara>Uses AES-GCM with a random nonce and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="enabling-etcd-encryption_post-install-cluster-tasks">
<title>Enabling etcd encryption</title>
<simpara>You can enable etcd encryption to encrypt sensitive resources in your cluster.</simpara>
<warning>
<simpara>Do not back up etcd resources until the initial encryption process is completed. If the encryption process is not completed, the backup might be only partially encrypted.</simpara>
<simpara>After you enable etcd encryption, several changes can occur:</simpara>
<itemizedlist>
<listitem>
<simpara>The etcd encryption might affect the memory consumption of a few resources.</simpara>
</listitem>
<listitem>
<simpara>You might notice a transient affect on backup performance because the leader must serve the backup.</simpara>
</listitem>
<listitem>
<simpara>A disk I/O can affect the node that receives the backup state.</simpara>
</listitem>
</itemizedlist>
</warning>
<simpara>You can encrypt the etcd database in either AES-GCM or AES-CBC encryption.</simpara>
<note>
<simpara>To migrate your etcd database from one encryption type to the other, you can modify the API server&#8217;s <literal>spec.encryption.type</literal> field. Migration of the etcd data to the new encryption type occurs automatically.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the <literal>APIServer</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit apiserver</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>spec.encryption.type</literal> field to <literal>aesgcm</literal> or <literal>aescbc</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  encryption:
    type: aesgcm <co xml:id="CO73-1"/></programlisting>
<calloutlist>
<callout arearefs="CO73-1">
<para>Set to <literal>aesgcm</literal> for AES-GCM encryption or <literal>aescbc</literal> for AES-CBC encryption.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<simpara>The encryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of the etcd database.</simpara>
</listitem>
<listitem>
<simpara>Verify that etcd encryption was successful.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Review the <literal>Encrypted</literal> status condition for the OpenShift API server to verify that its resources were successfully encrypted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>The output shows <literal>EncryptionCompleted</literal> upon successful encryption:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">EncryptionCompleted
All resources encrypted: routes.route.openshift.io</programlisting>
<simpara>If the output shows <literal>EncryptionInProgress</literal>, encryption is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Review the <literal>Encrypted</literal> status condition for the Kubernetes API server to verify that its resources were successfully encrypted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>The output shows <literal>EncryptionCompleted</literal> upon successful encryption:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">EncryptionCompleted
All resources encrypted: secrets, configmaps</programlisting>
<simpara>If the output shows <literal>EncryptionInProgress</literal>, encryption is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Review the <literal>Encrypted</literal> status condition for the OpenShift OAuth API server to verify that its resources were successfully encrypted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>The output shows <literal>EncryptionCompleted</literal> upon successful encryption:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">EncryptionCompleted
All resources encrypted: oauthaccesstokens.oauth.openshift.io, oauthauthorizetokens.oauth.openshift.io</programlisting>
<simpara>If the output shows <literal>EncryptionInProgress</literal>, encryption is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="disabling-etcd-encryption_post-install-cluster-tasks">
<title>Disabling etcd encryption</title>
<simpara>You can disable encryption of etcd data in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Modify the <literal>APIServer</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit apiserver</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>encryption</literal> field type to <literal>identity</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  encryption:
    type: identity <co xml:id="CO74-1"/></programlisting>
<calloutlist>
<callout arearefs="CO74-1">
<para>The <literal>identity</literal> type is the default value and means that no encryption is performed.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<simpara>The decryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of your cluster.</simpara>
</listitem>
<listitem>
<simpara>Verify that etcd decryption was successful.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Review the <literal>Encrypted</literal> status condition for the OpenShift API server to verify that its resources were successfully decrypted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>The output shows <literal>DecryptionCompleted</literal> upon successful decryption:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">DecryptionCompleted
Encryption mode set to identity and everything is decrypted</programlisting>
<simpara>If the output shows <literal>DecryptionInProgress</literal>, decryption is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Review the <literal>Encrypted</literal> status condition for the Kubernetes API server to verify that its resources were successfully decrypted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>The output shows <literal>DecryptionCompleted</literal> upon successful decryption:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">DecryptionCompleted
Encryption mode set to identity and everything is decrypted</programlisting>
<simpara>If the output shows <literal>DecryptionInProgress</literal>, decryption is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Review the <literal>Encrypted</literal> status condition for the OpenShift OAuth API server to verify that its resources were successfully decrypted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>The output shows <literal>DecryptionCompleted</literal> upon successful decryption:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">DecryptionCompleted
Encryption mode set to identity and everything is decrypted</programlisting>
<simpara>If the output shows <literal>DecryptionInProgress</literal>, decryption is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="backing-up-etcd-data_post-install-cluster-tasks">
<title>Backing up etcd data</title>
<simpara>Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd.</simpara>
<important>
<simpara>Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have checked whether the cluster-wide proxy is enabled.</simpara>
<tip>
<simpara>You can check whether the proxy is enabled by reviewing the output of <literal>oc get proxy cluster -o yaml</literal>. The proxy is enabled if the <literal>httpProxy</literal>, <literal>httpsProxy</literal>, and <literal>noProxy</literal> fields have values set.</simpara>
</tip>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start a debug session as root for a control plane node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug --as-root node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Change your root directory to <literal>/host</literal> in the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>If the cluster-wide proxy is enabled, be sure that you have exported the <literal>NO_PROXY</literal>, <literal>HTTP_PROXY</literal>, and <literal>HTTPS_PROXY</literal> environment variables.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal>cluster-backup.sh</literal> script in the debug shell and pass in the location to save the backup to.</simpara>
<tip>
<simpara>The <literal>cluster-backup.sh</literal> script is maintained as a component of the etcd Cluster Operator and is a wrapper around the <literal>etcdctl snapshot save</literal> command.</simpara>
</tip>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup</programlisting>
<formalpara>
<title>Example script output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6
found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7
found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6
found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3
ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1
etcdctl version: 3.4.14
API version: 3.4
{"level":"info","ts":1624647639.0188997,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part"}
{"level":"info","ts":"2021-06-25T19:00:39.030Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1624647639.0301006,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://10.0.0.5:2379"}
{"level":"info","ts":"2021-06-25T19:00:40.215Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}
{"level":"info","ts":1624647640.6032252,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://10.0.0.5:2379","size":"114 MB","took":1.584090459}
{"level":"info","ts":1624647640.6047094,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db"}
Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db
{"hash":3866667823,"revision":31407,"totalKey":12828,"totalSize":114446336}
snapshot db and kube resources are successfully saved to /home/core/assets/backup</programlisting>
</para>
</formalpara>
<simpara>In this example, two files are created in the <literal>/home/core/assets/backup/</literal> directory on the control plane host:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>snapshot_&lt;datetimestamp&gt;.db</literal>: This file is the etcd snapshot. The <literal>cluster-backup.sh</literal> script confirms its validity.</simpara>
</listitem>
<listitem>
<simpara><literal>static_kuberesources_&lt;datetimestamp&gt;.tar.gz</literal>: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot.</simpara>
<note>
<simpara>If etcd encryption is enabled, it is recommended to store this second file separately from the etcd snapshot for security reasons. However, this file is required to restore from the etcd snapshot.</simpara>
<simpara>Keep in mind that etcd encryption only encrypts values, not keys. This means that resource types, namespaces, and object names are unencrypted.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="etcd-defrag_post-install-cluster-tasks">
<title>Defragmenting etcd data</title>
<simpara>For large and dense clusters, etcd can suffer from poor performance if the keyspace grows too large and exceeds the space quota. Periodically maintain and defragment etcd to free up space in the data store. Monitor Prometheus for etcd metrics and defragment it when required; otherwise, etcd can raise a cluster-wide alarm that puts the cluster into a maintenance mode that accepts only key reads and deletes.</simpara>
<simpara>Monitor these key metrics:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>etcd_server_quota_backend_bytes</literal>, which is the current quota limit</simpara>
</listitem>
<listitem>
<simpara><literal>etcd_mvcc_db_total_size_in_use_in_bytes</literal>, which indicates the actual database usage after a history compaction</simpara>
</listitem>
<listitem>
<simpara><literal>etcd_mvcc_db_total_size_in_bytes</literal>, which shows the database size, including free space waiting for defragmentation</simpara>
</listitem>
</itemizedlist>
<simpara>Defragment etcd data to reclaim disk space after events that cause disk fragmentation, such as etcd history compaction.</simpara>
<simpara>History compaction is performed automatically every five minutes and leaves gaps in the back-end database. This fragmented space is available for use by etcd, but is not available to the host file system. You must defragment etcd to make this space available to the host file system.</simpara>
<simpara>Defragmentation occurs automatically, but you can also trigger it manually.</simpara>
<note>
<simpara>Automatic defragmentation is good for most cases, because the etcd operator uses cluster information to determine the most efficient operation for the user.</simpara>
</note>
<section xml:id="automatic-defrag-etcd-data_post-install-cluster-tasks">
<title>Automatic defragmentation</title>
<simpara>The etcd Operator automatically defragments disks. No manual intervention is needed.</simpara>
<simpara>Verify that the defragmentation process is successful by viewing one of these logs:</simpara>
<itemizedlist>
<listitem>
<simpara>etcd logs</simpara>
</listitem>
<listitem>
<simpara>cluster-etcd-operator pod</simpara>
</listitem>
<listitem>
<simpara>operator status error log</simpara>
</listitem>
</itemizedlist>
<warning>
<simpara>Automatic defragmentation can cause leader election failure in various OpenShift core components, such as the Kubernetes controller manager, which triggers a restart of the failing component. The restart is harmless and either triggers failover to the next running instance or the component resumes work again after the restart.</simpara>
</warning>
<formalpara>
<title>Example log output for successful defragmentation</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd member has been defragmented: <emphasis>&lt;member_name&gt;</emphasis>, memberID: <emphasis>&lt;member_id&gt;</emphasis></programlisting>
</para>
</formalpara>
<formalpara>
<title>Example log output for unsuccessful defragmentation</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">failed defrag on member: <emphasis>&lt;member_name&gt;</emphasis>, memberID: <emphasis>&lt;member_id&gt;</emphasis>: <emphasis>&lt;error_message&gt;</emphasis></programlisting>
</para>
</formalpara>
</section>
<section xml:id="manual-defrag-etcd-data_post-install-cluster-tasks">
<title>Manual defragmentation</title>
<simpara>A Prometheus alert indicates when you need to use manual defragmentation. The alert is displayed in two cases:</simpara>
<itemizedlist>
<listitem>
<simpara>When etcd uses more than 50% of its available space for more than 10 minutes</simpara>
</listitem>
<listitem>
<simpara>When etcd is actively using less than 50% of its total database size for more than 10 minutes</simpara>
</listitem>
</itemizedlist>
<simpara>You can also determine whether defragmentation is needed by checking the etcd database size in MB that will be freed by defragmentation with the PromQL expression: <literal>(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024</literal></simpara>
<warning>
<simpara>Defragmenting etcd is a blocking action. The etcd member will not respond until defragmentation is complete. For this reason, wait at least one minute between defragmentation actions on each of the pods to allow the cluster to recover.</simpara>
</warning>
<simpara>Follow this procedure to defragment etcd data on each etcd member.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Determine which etcd member is the leader, because the leader should be defragmented last.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of etcd pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-159-225.example.redhat.com                3/3     Running     0          175m   10.0.159.225   ip-10-0-159-225.example.redhat.com   &lt;none&gt;           &lt;none&gt;
etcd-ip-10-0-191-37.example.redhat.com                 3/3     Running     0          173m   10.0.191.37    ip-10-0-191-37.example.redhat.com    &lt;none&gt;           &lt;none&gt;
etcd-ip-10-0-199-170.example.redhat.com                3/3     Running     0          176m   10.0.199.170   ip-10-0-199-170.example.redhat.com   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Choose a pod and run the following command to determine which etcd member is the leader:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com etcdctl endpoint status --cluster -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-ip-10-0-159-225.example.redhat.com -n openshift-etcd' to see all of the containers in this pod.
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</programlisting>
</para>
</formalpara>
<simpara>Based on the <literal>IS LEADER</literal> column of this output, the <literal>https://10.0.199.170:2379</literal> endpoint is the leader. Matching this endpoint with the output of the previous step, the pod name of the leader is <literal>etcd-ip-10-0-199-170.example.redhat.com</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Defragment an etcd member.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Connect to the running etcd container, passing in the name of a pod that is <emphasis>not</emphasis> the leader:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com</programlisting>
</listitem>
<listitem>
<simpara>Unset the <literal>ETCDCTL_ENDPOINTS</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# unset ETCDCTL_ENDPOINTS</programlisting>
</listitem>
<listitem>
<simpara>Defragment the etcd member:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# etcdctl --command-timeout=30s --endpoints=https://localhost:2379 defrag</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Finished defragmenting etcd member[https://localhost:2379]</programlisting>
</para>
</formalpara>
<simpara>If a timeout error occurs, increase the value for <literal>--command-timeout</literal> until the command succeeds.</simpara>
</listitem>
<listitem>
<simpara>Verify that the database size was reduced:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# etcdctl endpoint status -w table --cluster</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |   41 MB |     false |      false |         7 |      91624 |              91624 |        | <co xml:id="CO75-1"/>
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+</programlisting>
</para>
</formalpara>
<simpara>This example shows that the database size for this etcd member is now 41 MB as opposed to the starting size of 104 MB.</simpara>
</listitem>
<listitem>
<simpara>Repeat these steps to connect to each of the other etcd members and defragment them. Always defragment the leader last.</simpara>
<simpara>Wait at least one minute between defragmentation actions to allow the etcd pod to recover. Until the etcd pod recovers, the etcd member will not respond.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>If any <literal>NOSPACE</literal> alarms were triggered due to the space quota being exceeded, clear them.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check if there are any <literal>NOSPACE</literal> alarms:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# etcdctl alarm list</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">memberID:12345678912345678912 alarm:NOSPACE</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Clear the alarms:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# etcdctl alarm disarm</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="dr-scenario-2-restoring-cluster-state_post-install-cluster-tasks">
<title>Restoring to a previous cluster state</title>
<simpara>You can use a saved <literal>etcd</literal> backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.</simpara>
<note>
<simpara>If your cluster uses a control plane machine set, see "Troubleshooting the control plane machine set" for a more simple <literal>etcd</literal> recovery procedure.</simpara>
</note>
<important>
<simpara>When you restore your cluster, you must use an <literal>etcd</literal> backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.7.2 cluster must use an <literal>etcd</literal> backup that was taken from 4.7.2.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role through a certificate-based <literal>kubeconfig</literal> file, like the one that was used during installation.</simpara>
</listitem>
<listitem>
<simpara>A healthy control plane host to use as the recovery host.</simpara>
</listitem>
<listitem>
<simpara>SSH access to control plane hosts.</simpara>
</listitem>
<listitem>
<simpara>A backup directory containing both the <literal>etcd</literal> snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: <literal>snapshot_&lt;datetimestamp&gt;.db</literal> and <literal>static_kuberesources_&lt;datetimestamp&gt;.tar.gz</literal>.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and recreate other non-recovery, control plane machines, one by one.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on.</simpara>
</listitem>
<listitem>
<simpara>Establish SSH connectivity to each of the control plane nodes, including the recovery host.</simpara>
<simpara><literal>kube-apiserver</literal> becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal.</simpara>
<important>
<simpara>If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.</simpara>
</important>
</listitem>
<listitem>
<simpara>Copy the <literal>etcd</literal> backup directory to the recovery control plane host.</simpara>
<simpara>This procedure assumes that you copied the <literal>backup</literal> directory containing the <literal>etcd</literal> snapshot and the resources for the static pods to the <literal>/home/core/</literal> directory of your recovery control plane host.</simpara>
</listitem>
<listitem>
<simpara>Stop the static pods on any other control plane nodes.</simpara>
<note>
<simpara>You do not need to stop the static pods on the recovery host.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Access a control plane host that is not the recovery host.</simpara>
</listitem>
<listitem>
<simpara>Move the existing etcd pod file out of the kubelet manifest directory by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/etcd-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>etcd</literal> pods are stopped by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the existing <literal>kube-apiserver</literal> file out of the kubelet manifest directory by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kube-apiserver</literal> containers are stopped by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep kube-apiserver | egrep -v "operator|guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the existing <literal>kube-controller-manager</literal> file out of the kubelet manifest directory by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/kube-controller-manager-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kube-controller-manager</literal> containers are stopped by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep kube-controller-manager | egrep -v "operator|guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the existing <literal>kube-scheduler</literal> file out of the kubelet manifest directory by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/kube-scheduler-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kube-scheduler</literal> containers are stopped by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep kube-scheduler | egrep -v "operator|guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>etcd</literal> data directory to a different location with the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /var/lib/etcd/ /tmp</programlisting>
</listitem>
<listitem>
<simpara>If the <literal>/etc/kubernetes/manifests/keepalived.yaml</literal> file exists, follow these steps:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Move the <literal>/etc/kubernetes/manifests/keepalived.yaml</literal> file out of the kubelet manifest directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/keepalived.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that any containers managed by the <literal>keepalived</literal> daemon are stopped:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps --name keepalived</programlisting>
<simpara>The output of this command should be empty. If it is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Check if the control plane has any Virtual IPs (VIPs) assigned to it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip -o address | egrep '&lt;api_vip&gt;|&lt;ingress_vip&gt;'</programlisting>
</listitem>
<listitem>
<simpara>For each reported VIP, run the following command to remove it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo ip address del &lt;reported_vip&gt; dev &lt;reported_vip_device&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Repeat this step on each of the other control plane hosts that is not the recovery host.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Access the recovery control plane host.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>keepalived</literal> daemon is in use, verify that the recovery control plane node owns the VIP:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip -o address | grep &lt;api_vip&gt;</programlisting>
<simpara>The address of the VIP is highlighted in the output if it exists. This command returns an empty string if the VIP is not set or configured incorrectly.</simpara>
</listitem>
<listitem>
<simpara>If the cluster-wide proxy is enabled, be sure that you have exported the <literal>NO_PROXY</literal>, <literal>HTTP_PROXY</literal>, and <literal>HTTPS_PROXY</literal> environment variables.</simpara>
<tip>
<simpara>You can check whether the proxy is enabled by reviewing the output of <literal>oc get proxy cluster -o yaml</literal>. The proxy is enabled if the <literal>httpProxy</literal>, <literal>httpsProxy</literal>, and <literal>noProxy</literal> fields have values set.</simpara>
</tip>
</listitem>
<listitem>
<simpara>Run the restore script on the recovery control plane host and pass in the path to the <literal>etcd</literal> backup directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo -E /usr/local/bin/cluster-restore.sh /home/core/backup</programlisting>
<formalpara>
<title>Example script output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...stopping kube-scheduler-pod.yaml
...stopping kube-controller-manager-pod.yaml
...stopping etcd-pod.yaml
...stopping kube-apiserver-pod.yaml
Waiting for container etcd to stop
.complete
Waiting for container etcdctl to stop
.............................complete
Waiting for container etcd-metrics to stop
complete
Waiting for container kube-controller-manager to stop
complete
Waiting for container kube-apiserver to stop
..........................................................................................complete
Waiting for container kube-scheduler to stop
complete
Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup
starting restore-etcd static pod
starting kube-apiserver-pod.yaml
static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml
starting kube-controller-manager-pod.yaml
static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml
starting kube-scheduler-pod.yaml
static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml</programlisting>
</para>
</formalpara>
<simpara>The cluster-restore.sh script must show that <literal>etcd</literal>, <literal>kube-apiserver</literal>, <literal>kube-controller-manager</literal>, and <literal>kube-scheduler</literal> pods are stopped and then started at the end of the restore process.</simpara>
<note>
<simpara>The restore process can cause nodes to enter the <literal>NotReady</literal> state if the node certificates were updated after the last <literal>etcd</literal> backup.</simpara>
</note>
</listitem>
<listitem>
<simpara>Check the nodes to ensure they are in the <literal>Ready</literal> state.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -w</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                STATUS  ROLES          AGE     VERSION
host-172-25-75-28   Ready   master         3d20h   v1.28.5
host-172-25-75-38   Ready   infra,worker   3d20h   v1.28.5
host-172-25-75-40   Ready   master         3d20h   v1.28.5
host-172-25-75-65   Ready   master         3d20h   v1.28.5
host-172-25-75-74   Ready   infra,worker   3d20h   v1.28.5
host-172-25-75-79   Ready   worker         3d20h   v1.28.5
host-172-25-75-86   Ready   worker         3d20h   v1.28.5
host-172-25-75-98   Ready   infra,worker   3d20h   v1.28.5</programlisting>
</para>
</formalpara>
<simpara>It can take several minutes for all nodes to report their state.</simpara>
</listitem>
<listitem>
<simpara>If any nodes are in the <literal>NotReady</literal> state, log in to the nodes and remove all of the PEM files from the <literal>/var/lib/kubelet/pki</literal> directory on each node. You can SSH into the nodes or use the terminal window in the web console.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  ssh -i &lt;ssh-key-path&gt; core@&lt;master-hostname&gt;</programlisting>
<formalpara>
<title>Sample <literal>pki</literal> directory</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# pwd
/var/lib/kubelet/pki
sh-4.4# ls
kubelet-client-2022-04-28-11-24-09.pem  kubelet-server-2022-04-28-11-24-15.pem
kubelet-client-current.pem              kubelet-server-current.pem</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Restart the kubelet service on all control plane hosts.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>From the recovery host, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo systemctl restart kubelet.service</programlisting>
</listitem>
<listitem>
<simpara>Repeat this step on all other control plane hosts.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Approve the pending Certificate Signing Requests (CSRs):</simpara>
<note>
<simpara>Clusters with no worker nodes, such as single-node clusters or clusters consisting of three schedulable control plane nodes, will not have any pending CSRs to approve. You can skip all the commands listed in this step.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of current CSRs by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME        AGE    SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-2s94x   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <co xml:id="CO75-2"/>
csr-4bd6t   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <co xml:id="CO75-3"/>
csr-4hl85   13m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <co xml:id="CO75-4"/>
csr-zhhhp   3m8s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <co xml:id="CO75-5"/>
...</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO75-1 CO75-2 CO75-3">
<para>A pending kubelet service</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>CSR (for user-provisioned installations).
&lt;2&gt; A pending <literal>node-bootstrapper</literal> CSR.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Review the details of a CSR to verify that it is valid by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe csr &lt;csr_name&gt; <co xml:id="CO76-1"/></programlisting>
<calloutlist>
<callout arearefs="CO76-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Approve each valid <literal>node-bootstrapper</literal> CSR by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>For user-provisioned installations, approve each valid kubelet service CSR by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that the single member control plane has started successfully.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>From the recovery host, verify that the <literal>etcd</literal> container is running by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">3ad41b7908e32       36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009                                                         About a minute ago   Running             etcd                                          0                   7c05f8af362f0</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>From the recovery host, verify that the <literal>etcd</literal> pod is running by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                             READY   STATUS      RESTARTS   AGE
etcd-ip-10-0-143-125.ec2.internal                1/1     Running     1          2m47s</programlisting>
</para>
</formalpara>
<simpara>If the status is <literal>Pending</literal>, or the output lists more than one running <literal>etcd</literal> pod, wait a few minutes and check again.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>If you are using the <literal>OVNKubernetes</literal> network plugin, you must restart <literal>ovnkube-controlplane</literal> pods.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete all of the <literal>ovnkube-controlplane</literal> pods by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-control-plane</programlisting>
</listitem>
<listitem>
<simpara>Verify that all of the <literal>ovnkube-controlplane</literal> pods were redeployed by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane</programlisting>
<orderedlist numeration="arabic">
<listitem>
<simpara>If you are using the OVN-Kubernetes network plugin, restart the Open Virtual Network (OVN) Kubernetes pods on all the nodes one by one. Use the following steps to restart OVN-Kubernetes pods on each node:</simpara>
<important>
<orderedlist numeration="arabic">
<title>Restart OVN-Kubernetes pods in the following order:</title>
<listitem>
<simpara>The recovery control plane host</simpara>
</listitem>
<listitem>
<simpara>The other control plane hosts (if available)</simpara>
</listitem>
<listitem>
<simpara>The other nodes</simpara>
</listitem>
</orderedlist>
</important>
<note>
<simpara>Validating and mutating admission webhooks can reject pods. If you add any additional webhooks with the <literal>failurePolicy</literal> set to <literal>Fail</literal>, then they can reject pods and the restoration process can fail. You can avoid this by saving and deleting webhooks while restoring the cluster state. After the cluster state is restored successfully, you can enable the webhooks again.</simpara>
<simpara>Alternatively, you can temporarily set the <literal>failurePolicy</literal> to <literal>Ignore</literal> while restoring the cluster state. After the cluster state is restored successfully, you can set the <literal>failurePolicy</literal> to <literal>Fail</literal>.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Remove the northbound database (nbdb) and southbound database (sbdb). Access the recovery host and the remaining control plane nodes by using Secure Shell (SSH) and run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo rm -f /var/lib/ovn-ic/etc/*.db</programlisting>
</listitem>
<listitem>
<simpara>Restart the OpenVSwitch services. Access the node by using Secure Shell (SSH) and run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo systemctl restart ovs-vswitchd ovsdb-server</programlisting>
</listitem>
<listitem>
<simpara>Delete the <literal>ovnkube-node</literal> pod on the node by running the following command, replacing <literal>&lt;node&gt;</literal> with the name of the node that you are restarting:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-node --field-selector=spec.nodeName==&lt;node&gt;</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>ovnkube-node</literal> pod is running again with:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-node --field-selector=spec.nodeName==&lt;node&gt;</programlisting>
<note>
<simpara>It might take several minutes for the pods to restart.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Delete and re-create other non-recovery, control plane machines, one by one. After the machines are re-created, a new revision is forced and <literal>etcd</literal> automatically scales up.</simpara>
<itemizedlist>
<listitem>
<simpara>If you use a user-provisioned bare metal installation, you can re-create a control plane machine by using the same method that you used to originally create it. For more information, see "Installing a user-provisioned cluster on bare metal".</simpara>
<warning>
<simpara>Do not delete and re-create the machine for the recovery host.</simpara>
</warning>
</listitem>
<listitem>
<simpara>If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps:</simpara>
<warning>
<simpara>Do not delete and re-create the machine for the recovery host.</simpara>
<simpara>For bare metal installations on installer-provisioned infrastructure, control plane machines are not re-created. For more information, see "Replacing a bare-metal control plane node".</simpara>
</warning>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Obtain the machine for one of the lost control plane hosts.</simpara>
<simpara>In a terminal that has access to the cluster as a cluster-admin user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<simpara>Example output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-0                  Running   m4.xlarge   us-east-1   us-east-1a   3h37m   ip-10-0-131-183.ec2.internal   aws:///us-east-1a/i-0ec2782f8287dfb7e   stopped <co xml:id="CO77-1"/>
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
<calloutlist>
<callout arearefs="CO77-1">
<para>This is the control plane machine for the lost control plane host, <literal>ip-10-0-131-183.ec2.internal</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the machine configuration to a file on your file system by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine clustername-8qw5l-master-0 \ <co xml:id="CO78-1"/>
    -n openshift-machine-api \
    -o yaml \
    &gt; new-master-machine.yaml</programlisting>
<calloutlist>
<callout arearefs="CO78-1">
<para>Specify the name of the control plane machine for the lost control plane host.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>new-master-machine.yaml</literal> file that was created in the previous step to assign a new name and remove unnecessary fields.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Remove the entire <literal>status</literal> section by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">status:
  addresses:
  - address: 10.0.131.183
    type: InternalIP
  - address: ip-10-0-131-183.ec2.internal
    type: InternalDNS
  - address: ip-10-0-131-183.ec2.internal
    type: Hostname
  lastUpdated: "2020-04-20T17:44:29Z"
  nodeRef:
    kind: Node
    name: ip-10-0-131-183.ec2.internal
    uid: acca4411-af0d-4387-b73e-52b2484295ad
  phase: Running
  providerStatus:
    apiVersion: awsproviderconfig.openshift.io/v1beta1
    conditions:
    - lastProbeTime: "2020-04-20T16:53:50Z"
      lastTransitionTime: "2020-04-20T16:53:50Z"
      message: machine successfully created
      reason: MachineCreationSucceeded
      status: "True"
      type: MachineCreation
    instanceId: i-0fdb85790d76d0c3f
    instanceState: stopped
    kind: AWSMachineProviderStatus</programlisting>
</listitem>
<listitem>
<simpara>Change the <literal>metadata.name</literal> field to a new name by running:</simpara>
<simpara>It is recommended to keep the same base name as the old machine and change the ending number to the next available number. In this example, <literal>clustername-8qw5l-master-0</literal> is changed to <literal>clustername-8qw5l-master-3</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
  name: clustername-8qw5l-master-3
  ...</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>spec.providerID</literal> field by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">providerID: aws:///us-east-1a/i-0fdb85790d76d0c3f</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>metadata.annotations</literal> and <literal>metadata.generation</literal> fields by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">annotations:
  machine.openshift.io/instance-state: running
...
generation: 2</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>metadata.resourceVersion</literal> and <literal>metadata.uid</literal> fields by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">resourceVersion: "13291"
uid: a282eb70-40a2-4e89-8009-d05dd420d31a</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete the machine of the lost control plane host by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete machine -n openshift-machine-api clustername-8qw5l-master-0 <co xml:id="CO79-1"/></programlisting>
<calloutlist>
<callout arearefs="CO79-1">
<para>Specify the name of the control plane machine for the lost control plane host.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the machine was deleted by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<simpara>Example output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal   aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
</listitem>
<listitem>
<simpara>Create a machine by using the <literal>new-master-machine.yaml</literal> file by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f new-master-machine.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the new machine has been created by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<simpara>Example output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE          TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running        m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running        m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-master-3                  Provisioning   m4.xlarge   us-east-1   us-east-1a   85s     ip-10-0-173-171.ec2.internal    aws:///us-east-1a/i-015b0888fe17bc2c8  running <co xml:id="CO80-1"/>
clustername-8qw5l-worker-us-east-1a-wbtgd   Running        m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running        m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running        m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
<calloutlist>
<callout arearefs="CO80-1">
<para>The new machine, <literal>clustername-8qw5l-master-3</literal> is being created and is ready after the phase changes from <literal>Provisioning</literal> to <literal>Running</literal>.</para>
</callout>
</calloutlist>
<simpara>It might take a few minutes for the new machine to be created. The <literal>etcd</literal> cluster Operator will automatically sync when the machine or node returns to a healthy state.</simpara>
</listitem>
<listitem>
<simpara>Repeat these steps for each lost control plane host that is not the recovery host.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Turn off the quorum guard by entering:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'</programlisting>
<simpara>This command ensures that you can successfully re-create secrets and roll out the static pods.</simpara>
</listitem>
<listitem>
<simpara>In a separate terminal window within the recovery host, export the recovery <literal>kubeconfig</literal> file by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfig</programlisting>
</listitem>
<listitem>
<simpara>Force <literal>etcd</literal> redeployment.</simpara>
<simpara>In the same terminal window where you exported the recovery <literal>kubeconfig</literal> file, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <co xml:id="CO81-1"/></programlisting>
<calloutlist>
<callout arearefs="CO81-1">
<para>The <literal>forceRedeploymentReason</literal> value must be unique, which is why a timestamp is appended.</para>
</callout>
</calloutlist>
<simpara>When the <literal>etcd</literal> cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up.</simpara>
</listitem>
<listitem>
<simpara>Turn the quorum guard back on by entering:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'</programlisting>
</listitem>
<listitem>
<simpara>You can verify that the <literal>unsupportedConfigOverrides</literal> section is removed from the object by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd/cluster -oyaml</programlisting>
</listitem>
<listitem>
<simpara>Verify all nodes are updated to the latest revision.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition for <literal>etcd</literal> to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO82-1"/></programlisting>
<calloutlist>
<callout arearefs="CO82-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>After <literal>etcd</literal> is redeployed, force new rollouts for the control plane. <literal>kube-apiserver</literal> will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run:</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Force a new rollout for <literal>kube-apiserver</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubeapiserver cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</programlisting>
<simpara>Verify all nodes are updated to the latest revision.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO83-1"/></programlisting>
<calloutlist>
<callout arearefs="CO83-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Force a new rollout for the Kubernetes controller manager by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubecontrollermanager cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</programlisting>
<simpara>Verify all nodes are updated to the latest revision by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO84-1"/></programlisting>
<calloutlist>
<callout arearefs="CO84-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Force a new rollout for the <literal>kube-scheduler</literal> by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubescheduler cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</programlisting>
<simpara>Verify all nodes are updated to the latest revision by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO85-1"/></programlisting>
<calloutlist>
<callout arearefs="CO85-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that all control plane hosts have started and joined the cluster.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-143-125.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-154-194.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-173-171.ec2.internal                2/2     Running     0          9h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>To ensure that all workloads return to normal operation following a recovery procedure, restart each pod that stores <literal>kube-apiserver</literal> information. This includes OpenShift Container Platform components such as routers, Operators, and third-party components.</simpara>
<note>
<simpara>On completion of the previous procedural steps, you might need to wait a few minutes for all services to return to their restored state. For example, authentication by using <literal>oc login</literal> might not immediately work until the OAuth server pods are restarted.</simpara>
<simpara>Consider using the <literal>system:admin</literal> <literal>kubeconfig</literal> file for immediate authentication. This method basis its authentication on SSL/TLS client certificates as against OAuth tokens. You can authenticate with this file by issuing the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KUBECONFIG=&lt;installation_directory&gt;/auth/kubeconfig</programlisting>
<simpara>Issue the following command to display your authenticated user name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc whoami</programlisting>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/installing_bare_metal/installing-bare-metal.xml#installing-bare-metal">Installing a user-provisioned cluster on bare metal</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../installing/index.xml#replacing-a-bare-metal-control-plane-node_ipi-install-expanding">Replacing a bare-metal control plane node</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="dr-scenario-cluster-state-issues_post-install-cluster-tasks">
<title>Issues and workarounds for restoring a persistent storage state</title>
<simpara>If your OpenShift Container Platform cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a <literal>StatefulSet</literal> object. When you restore from an etcd backup, the status of the workloads in OpenShift Container Platform is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.</simpara>
<important>
<simpara>The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an OpenShift Container Platform cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.</simpara>
</important>
<simpara>The following are some example scenarios that produce an out-of-date status:</simpara>
<itemizedlist>
<listitem>
<simpara>MySQL database is running in a pod backed up by a PV object. Restoring OpenShift Container Platform from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.</simpara>
</listitem>
<listitem>
<simpara>Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. OpenShift Container Platform is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.</simpara>
</listitem>
<listitem>
<simpara>Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.</simpara>
</listitem>
<listitem>
<simpara>A device is removed or renamed from OpenShift Container Platform nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from <literal>/dev/disk/by-id</literal> or <literal>/dev</literal> directories. This situation might cause the local PVs to refer to devices that no longer exist.</simpara>
<simpara>To fix this problem, an administrator must:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Manually remove the PVs with invalid devices.</simpara>
</listitem>
<listitem>
<simpara>Remove symlinks from respective nodes.</simpara>
</listitem>
<listitem>
<simpara>Delete <literal>LocalVolume</literal> or <literal>LocalVolumeSet</literal> objects (see <emphasis>Storage</emphasis> &#8594; <emphasis>Configuring persistent storage</emphasis> &#8594; <emphasis>Persistent storage using local volumes</emphasis> &#8594; <emphasis>Deleting the Local Storage Operator Resources</emphasis>).</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-install-pod-disruption-budgets">
<title>Pod disruption budgets</title>
<simpara>Understand and configure pod disruption budgets.</simpara>
<section xml:id="nodes-pods-pod-distruption-about_post-install-cluster-tasks">
<title>Understanding how to use pod disruption budgets to specify the number of pods that must be up</title>
<simpara>A <emphasis>pod disruption budget</emphasis> allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.</simpara>
<simpara><literal>PodDisruptionBudget</literal> is an API object that specifies the minimum number or
percentage of replicas that must be up at a time. Setting these in projects can
be helpful during node maintenance (such as scaling a cluster down or a cluster
upgrade) and is only honored on voluntary evictions (not on node failures).</simpara>
<simpara>A <literal>PodDisruptionBudget</literal> object&#8217;s configuration consists of the following key
parts:</simpara>
<itemizedlist>
<listitem>
<simpara>A label selector, which is a label query over a set of pods.</simpara>
</listitem>
<listitem>
<simpara>An availability level, which specifies the minimum number of pods that must be
available simultaneously, either:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>minAvailable</literal> is the number of pods must always be available, even during a disruption.</simpara>
</listitem>
<listitem>
<simpara><literal>maxUnavailable</literal> is the number of pods can be unavailable during a disruption.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara><literal>Available</literal> refers to the number of pods that has condition <literal>Ready=True</literal>.
<literal>Ready=True</literal> refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.</simpara>
<simpara>A <literal>maxUnavailable</literal> of <literal>0%</literal> or <literal>0</literal> or a <literal>minAvailable</literal> of <literal>100%</literal> or equal to the number of replicas
is permitted but can block nodes from being drained.</simpara>
</note>
<simpara>You can check for pod disruption budgets across all projects with the following:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get poddisruptionbudget --all-namespaces</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...</programlisting>
</para>
</formalpara>
<simpara>The <literal>PodDisruptionBudget</literal> is considered healthy when there are at least
<literal>minAvailable</literal> pods running in the system. Every pod above that limit can be evicted.</simpara>
<note>
<simpara>Depending on your pod priority and preemption settings,
lower-priority pods might be removed despite their pod disruption budget requirements.</simpara>
</note>
</section>
<section xml:id="nodes-pods-pod-disruption-configuring_post-install-cluster-tasks">
<title>Specifying the number of pods that must be up with pod disruption budgets</title>
<simpara>You can use a <literal>PodDisruptionBudget</literal> object to specify the minimum number or
percentage of replicas that must be up at a time.</simpara>
<formalpara>
<title>Procedure</title>
<para>To configure a pod disruption budget:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a YAML file with the an object definition similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: policy/v1 <co xml:id="CO86-1"/>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2  <co xml:id="CO86-2"/>
  selector:  <co xml:id="CO86-3"/>
    matchLabels:
      name: my-pod</programlisting>
<calloutlist>
<callout arearefs="CO86-1">
<para><literal>PodDisruptionBudget</literal> is part of the <literal>policy/v1</literal> API group.</para>
</callout>
<callout arearefs="CO86-2">
<para>The minimum number of pods that must be available simultaneously. This can
be either an integer or a string specifying a percentage, for example, <literal>20%</literal>.</para>
</callout>
<callout arearefs="CO86-3">
<para>A label query over a set of resources. The result of <literal>matchLabels</literal> and
<literal>matchExpressions</literal> are logically conjoined. Leave this parameter blank, for example <literal>selector {}</literal>, to select all pods in the project.</para>
</callout>
</calloutlist>
<simpara>Or:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: policy/v1 <co xml:id="CO87-1"/>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 25% <co xml:id="CO87-2"/>
  selector: <co xml:id="CO87-3"/>
    matchLabels:
      name: my-pod</programlisting>
<calloutlist>
<callout arearefs="CO87-1">
<para><literal>PodDisruptionBudget</literal> is part of the <literal>policy/v1</literal> API group.</para>
</callout>
<callout arearefs="CO87-2">
<para>The maximum number of pods that can be unavailable simultaneously. This can
be either an integer or a string specifying a percentage, for example, <literal>20%</literal>.</para>
</callout>
<callout arearefs="CO87-3">
<para>A label query over a set of resources. The result of <literal>matchLabels</literal> and
<literal>matchExpressions</literal> are logically conjoined. Leave this parameter blank, for example <literal>selector {}</literal>, to select all pods in the project.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Run the following command to add the object to project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;/path/to/file&gt; -n &lt;project_name&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="pod-disruption-eviction-policy_post-install-cluster-tasks">
<title>Specifying the eviction policy for unhealthy pods</title>
<simpara>When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.</simpara>
<simpara>You can choose one of the following policies:</simpara>
<variablelist>
<varlistentry>
<term>IfHealthyBudget</term>
<listitem>
<simpara>Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>AlwaysAllow</term>
<listitem>
<simpara>Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the <literal>CrashLoopBackOff</literal> state or failing to report the <literal>Ready</literal> status.</simpara>
<note>
<simpara>It is recommended to set the <literal>unhealthyPodEvictionPolicy</literal> field to <literal>AlwaysAllow</literal> in the <literal>PodDisruptionBudget</literal> object to support the eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.</simpara>
</note>
</listitem>
</varlistentry>
</variablelist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file that defines a <literal>PodDisruptionBudget</literal> object and specify the unhealthy pod eviction policy:</simpara>
<formalpara>
<title>Example <literal>pod-disruption-budget.yaml</literal> file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      name: my-pod
  unhealthyPodEvictionPolicy: AlwaysAllow <co xml:id="CO88-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO88-1">
<para>Choose either <literal>IfHealthyBudget</literal> or <literal>AlwaysAllow</literal> as the unhealthy pod eviction policy. The default is <literal>IfHealthyBudget</literal> when the <literal>unhealthyPodEvictionPolicy</literal> field is empty.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>PodDisruptionBudget</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f pod-disruption-budget.yaml</programlisting>
</listitem>
</orderedlist>
<simpara>With a PDB that has the <literal>AlwaysAllow</literal> unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../nodes/clusters/nodes-cluster-enabling-features.xml#nodes-cluster-enabling">Enabling features using feature gates</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy">Unhealthy Pod Eviction Policy</link> in the Kubernetes documentation</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-install-rotate-remove-cloud-creds">
<title>Rotating or removing cloud provider credentials</title>
<simpara>After installing OpenShift Container Platform, some organizations require the rotation or removal of the cloud provider credentials that were used during the initial installation.</simpara>
<simpara>To allow the cluster to use the new credentials, you must update the secrets that the <link xlink:href="../operators/operator-reference.xml#cloud-credential-operator_cluster-operators-ref">Cloud Credential Operator (CCO)</link> uses to manage cloud provider credentials.</simpara>
<section xml:id="ccoctl-rotate-remove-cloud-creds">
<title>Rotating cloud provider credentials with the Cloud Credential Operator utility</title>
<simpara>The Cloud Credential Operator (CCO) utility <literal>ccoctl</literal> supports updating secrets for clusters installed on IBM Cloud&#174;.</simpara>
<section xml:id="refreshing-service-ids-ibm-cloud_post-install-cluster-tasks">
<title>Rotating API keys</title>
<simpara>You can rotate API keys for your existing service IDs and update the corresponding secrets.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured the <literal>ccoctl</literal> binary.</simpara>
</listitem>
<listitem>
<simpara>You have existing service IDs in a live OpenShift Container Platform cluster installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Use the <literal>ccoctl</literal> utility to rotate your API keys for the service IDs and update the secrets:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ccoctl &lt;provider_name&gt; refresh-keys \ <co xml:id="CO89-1"/>
    --kubeconfig &lt;openshift_kubeconfig_file&gt; \ <co xml:id="CO89-2"/>
    --credentials-requests-dir &lt;path_to_credential_requests_directory&gt; \ <co xml:id="CO89-3"/>
    --name &lt;name&gt; <co xml:id="CO89-4"/></programlisting>
<calloutlist>
<callout arearefs="CO89-1">
<para>The name of the provider. For example: <literal>ibmcloud</literal> or <literal>powervs</literal>.</para>
</callout>
<callout arearefs="CO89-2">
<para>The <literal>kubeconfig</literal> file associated with the cluster. For example, <literal>&lt;installation_directory&gt;/auth/kubeconfig</literal>.</para>
</callout>
<callout arearefs="CO89-3">
<para>The directory where the credential requests are stored.</para>
</callout>
<callout arearefs="CO89-4">
<para>The name of the OpenShift Container Platform cluster.</para>
</callout>
</calloutlist>
<note>
<simpara>If your cluster uses Technology Preview features that are enabled by the <literal>TechPreviewNoUpgrade</literal> feature set, you must include the <literal>--enable-tech-preview</literal> parameter.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="manually-rotating-cloud-creds_post-install-cluster-tasks">
<title>Rotating cloud provider credentials manually</title>
<simpara>If your cloud provider credentials are changed for any reason, you must manually update the secret that the Cloud Credential Operator (CCO) uses to manage cloud provider credentials.</simpara>
<simpara>The process for rotating cloud credentials depends on the mode that the CCO is configured to use. After you rotate credentials for a cluster that is using mint mode, you must manually remove the component credentials that were created by the removed credential.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster is installed on a platform that supports rotating cloud credentials manually with the CCO mode that you are using:</simpara>
<itemizedlist>
<listitem>
<simpara>For mint mode, Amazon Web Services (AWS) and Google Cloud Platform (GCP) are supported.</simpara>
</listitem>
<listitem>
<simpara>For passthrough mode, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), Red Hat OpenStack Platform (RHOSP), and VMware vSphere are supported.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have changed the credentials that are used to interface with your cloud provider.</simpara>
</listitem>
<listitem>
<simpara>The new credentials have sufficient permissions for the mode CCO is configured to use in your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective of the web console, navigate to <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Secrets</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the table on the <emphasis role="strong">Secrets</emphasis> page, find the root secret for your cloud provider.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Platform</entry>
<entry align="left" valign="top">Secret name</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>AWS</simpara></entry>
<entry align="left" valign="top"><simpara><literal>aws-creds</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Azure</simpara></entry>
<entry align="left" valign="top"><simpara><literal>azure-credentials</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>GCP</simpara></entry>
<entry align="left" valign="top"><simpara><literal>gcp-credentials</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RHOSP</simpara></entry>
<entry align="left" valign="top"><simpara><literal>openstack-credentials</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>VMware vSphere</simpara></entry>
<entry align="left" valign="top"><simpara><literal>vsphere-creds</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> in the same row as the secret and select <emphasis role="strong">Edit Secret</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Record the contents of the <emphasis role="strong">Value</emphasis> field or fields. You can use this information to verify that the value is different after updating the credentials.</simpara>
</listitem>
<listitem>
<simpara>Update the text in the <emphasis role="strong">Value</emphasis> field or fields with the new authentication information for your cloud provider, and then click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you are updating the credentials for a vSphere cluster that does not have the vSphere CSI Driver Operator enabled, you must force a rollout of the Kubernetes controller manager to apply the updated credentials.</simpara>
<note>
<simpara>If the vSphere CSI Driver Operator is enabled, this step is not required.</simpara>
</note>
<simpara>To apply the updated vSphere credentials, log in to the OpenShift Container Platform CLI as a user with the <literal>cluster-admin</literal> role and run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubecontrollermanager cluster \
  -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date )"'"}}' \
  --type=merge</programlisting>
<simpara>While the credentials are rolling out, the status of the Kubernetes Controller Manager Operator reports <literal>Progressing=true</literal>. To view the status, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get co kube-controller-manager</programlisting>
</listitem>
<listitem>
<simpara>If the CCO for your cluster is configured to use mint mode, delete each component secret that is referenced by the individual <literal>CredentialsRequest</literal> objects.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Log in to the OpenShift Container Platform CLI as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Get the names and namespaces of all referenced component secrets:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-cloud-credential-operator get CredentialsRequest \
  -o json | jq -r '.items[] | select (.spec.providerSpec.kind=="&lt;provider_spec&gt;") | .spec.secretRef'</programlisting>
<simpara>where <literal>&lt;provider_spec&gt;</literal> is the corresponding value for your cloud provider:</simpara>
<itemizedlist>
<listitem>
<simpara>AWS: <literal>AWSProviderSpec</literal></simpara>
</listitem>
<listitem>
<simpara>GCP: <literal>GCPProviderSpec</literal></simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Partial example output for AWS</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "name": "ebs-cloud-credentials",
  "namespace": "openshift-cluster-csi-drivers"
}
{
  "name": "cloud-credential-operator-iam-ro-creds",
  "namespace": "openshift-cloud-credential-operator"
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete each of the referenced component secrets:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret &lt;secret_name&gt; \<co xml:id="CO90-1"/>
  -n &lt;secret_namespace&gt; <co xml:id="CO90-2"/></programlisting>
<calloutlist>
<callout arearefs="CO90-1">
<para>Specify the name of a secret.</para>
</callout>
<callout arearefs="CO90-2">
<para>Specify the namespace that contains the secret.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example deletion of an AWS secret</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret ebs-cloud-credentials -n openshift-cluster-csi-drivers</programlisting>
</para>
</formalpara>
<simpara>You do not need to manually delete the credentials from your provider console. Deleting the referenced component secrets will cause the CCO to delete the existing credentials from the platform and create new ones.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>To verify that the credentials have changed:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective of the web console, navigate to <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Secrets</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Verify that the contents of the <emphasis role="strong">Value</emphasis> field or fields have changed.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../storage/container_storage_interface/persistent-storage-csi-vsphere.xml">vSphere CSI Driver Operator</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="manually-removing-cloud-creds_post-install-cluster-tasks">
<title>Removing cloud provider credentials</title>
<simpara>After installing an OpenShift Container Platform cluster with the Cloud Credential Operator (CCO) in mint mode, you can remove the administrator-level credential secret from the <literal>kube-system</literal> namespace in the cluster. The administrator-level credential is required only during changes that require its elevated permissions, such as upgrades.</simpara>
<note>
<simpara>Prior to a non z-stream upgrade, you must reinstate the credential secret with the administrator-level credential. If the credential is not present, the upgrade might be blocked.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster is installed on a platform that supports removing cloud credentials from the CCO. Supported platforms are AWS and GCP.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective of the web console, navigate to <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Secrets</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the table on the <emphasis role="strong">Secrets</emphasis> page, find the root secret for your cloud provider.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Platform</entry>
<entry align="left" valign="top">Secret name</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>AWS</simpara></entry>
<entry align="left" valign="top"><simpara><literal>aws-creds</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>GCP</simpara></entry>
<entry align="left" valign="top"><simpara><literal>gcp-credentials</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Options</emphasis> menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> in the same row as the secret and select <emphasis role="strong">Delete Secret</emphasis>.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../authentication/managing_cloud_provider_credentials/about-cloud-credential-operator.xml#about-cloud-credential-operator">About the Cloud Credential Operator</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../authentication/managing_cloud_provider_credentials/cco-mode-passthrough.xml#admin-credentials-root-secret-formats_cco-mode-passthrough">Admin credentials root secret format</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-install-must-gather-disconnected">
<title>Configuring image streams for a disconnected cluster</title>
<simpara>After installing OpenShift Container Platform in a disconnected environment, configure the image streams for the Cluster Samples Operator and the <literal>must-gather</literal> image stream.</simpara>
<section xml:id="installation-images-samples-disconnected-mirroring-assist_post-install-cluster-tasks">
<title>Cluster Samples Operator assistance for mirroring</title>
<simpara>During installation, OpenShift Container Platform creates a config map named <literal>imagestreamtag-to-image</literal> in the <literal>openshift-cluster-samples-operator</literal> namespace. The <literal>imagestreamtag-to-image</literal> config map contains an entry, the populating image, for each image stream tag.</simpara>
<simpara>The format of the key for each entry in the data field in the config map is <literal>&lt;image_stream_name&gt;_&lt;image_stream_tag_name&gt;</literal>.</simpara>
<simpara>During a disconnected installation of OpenShift Container Platform, the status of the Cluster Samples Operator is set to <literal>Removed</literal>. If you choose to change it to <literal>Managed</literal>, it installs samples.</simpara>
<note>
<simpara>The use of samples in a network-restricted or discontinued environment may require access to services external to your network. Some example services include: Github, Maven Central, npm, RubyGems, PyPi and others. There might be additional steps to take that allow the cluster samples operators&#8217;s objects to reach the services they require.</simpara>
</note>
<simpara>You can use this config map as a reference for which images need to be mirrored for your image streams to import.</simpara>
<itemizedlist>
<listitem>
<simpara>While the Cluster Samples Operator is set to <literal>Removed</literal>, you can create your mirrored registry, or determine which existing mirrored registry you want to use.</simpara>
</listitem>
<listitem>
<simpara>Mirror the samples you want to the mirrored registry using the new config map as your guide.</simpara>
</listitem>
<listitem>
<simpara>Add any of the image streams you did not mirror to the <literal>skippedImagestreams</literal> list of the Cluster Samples Operator configuration object.</simpara>
</listitem>
<listitem>
<simpara>Set <literal>samplesRegistry</literal> of the Cluster Samples Operator configuration object to the mirrored registry.</simpara>
</listitem>
<listitem>
<simpara>Then set the Cluster Samples Operator to <literal>Managed</literal> to install the image streams you have mirrored.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installation-restricted-network-samples_post-install-cluster-tasks">
<title>Using Cluster Samples Operator image streams with alternate or mirrored registries</title>
<simpara>Most image streams in the <literal>openshift</literal> namespace managed by the Cluster Samples Operator
point to images located in the Red Hat registry at <link xlink:href="https://registry.redhat.io">registry.redhat.io</link>.
Mirroring
will not apply to these image streams.</simpara>
<note>
<simpara>The <literal>cli</literal>, <literal>installer</literal>, <literal>must-gather</literal>, and <literal>tests</literal> image streams, while
part of the install payload, are not managed by the Cluster Samples Operator. These are
not addressed in this procedure.</simpara>
</note>
<important>
<simpara>The Cluster Samples Operator must be set to <literal>Managed</literal> in a disconnected environment. To install the image streams, you have a mirrored registry.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Create a pull secret for your mirror registry.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Access the images of a specific image stream to mirror, for example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get is &lt;imagestream&gt; -n openshift -o json | jq .spec.tags[].from.name | grep registry.redhat.io</programlisting>
</listitem>
<listitem>
<simpara>Mirror images from <link xlink:href="https://registry.redhat.io">registry.redhat.io</link> associated with any image streams you need
in the restricted network environment into one of the defined mirrors, for example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc image mirror registry.redhat.io/rhscl/ruby-25-rhel7:latest ${MIRROR_ADDR}/rhscl/ruby-25-rhel7:latest</programlisting>
</listitem>
<listitem>
<simpara>Create the cluster&#8217;s image configuration object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..5000=$path/ca.crt -n openshift-config</programlisting>
</listitem>
<listitem>
<simpara>Add the required trusted CAs for the mirror in the cluster&#8217;s image
configuration object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge</programlisting>
</listitem>
<listitem>
<simpara>Update the <literal>samplesRegistry</literal> field in the Cluster Samples Operator configuration object
to contain the <literal>hostname</literal> portion of the mirror location defined in the mirror
configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit configs.samples.operator.openshift.io -n openshift-cluster-samples-operator</programlisting>
<note>
<simpara>This is required because the image stream import process does not use the mirror or search mechanism at this time.</simpara>
</note>
</listitem>
<listitem>
<simpara>Add any image streams that are not mirrored into the <literal>skippedImagestreams</literal> field
of the Cluster Samples Operator configuration object. Or if you do not want to support
any of the sample image streams, set the Cluster Samples Operator to <literal>Removed</literal> in the
Cluster Samples Operator configuration object.</simpara>
<note>
<simpara>The Cluster Samples Operator issues alerts if image stream imports are failing but the Cluster Samples Operator is either periodically retrying or does not appear to be retrying them.</simpara>
</note>
<simpara>Many of the templates in the <literal>openshift</literal> namespace
reference the image streams. So using <literal>Removed</literal> to purge both the image streams
and templates will eliminate the possibility of attempts to use them if they
are not functional because of any missing image streams.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-preparing-restricted-cluster-to-gather-support-data_post-install-cluster-tasks">
<title>Preparing your cluster to gather support data</title>
<simpara>Clusters using a restricted network must import the default must-gather image to gather debugging data for Red Hat support. The must-gather image is not imported by default, and clusters on a restricted network do not have access to the internet to pull the latest image from a remote repository.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If you have not added your mirror registry&#8217;s trusted CA to your cluster&#8217;s image configuration object as part of the Cluster Samples Operator configuration, perform the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the cluster&#8217;s image configuration object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..5000=$path/ca.crt -n openshift-config</programlisting>
</listitem>
<listitem>
<simpara>Add the required trusted CAs for the mirror in the cluster&#8217;s image
configuration object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Import the default must-gather image from your installation payload:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc import-image is/must-gather -n openshift</programlisting>
</listitem>
</orderedlist>
<simpara>When running the <literal>oc adm must-gather</literal> command, use the <literal>--image</literal> flag and point to the payload image, as in the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather --image=$(oc adm release info --image-for must-gather)</programlisting>
</section>
</section>
<section xml:id="images-cluster-sample-imagestream-import_post-install-cluster-tasks">
<title>Configuring periodic importing of Cluster Sample Operator image stream tags</title>
<simpara>You can ensure that you always have access to the latest versions of the Cluster Sample Operator images by periodically importing the image stream tags when new versions become available.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Fetch all the imagestreams in the <literal>openshift</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc get imagestreams -nopenshift</programlisting>
</listitem>
<listitem>
<simpara>Fetch the tags for every imagestream in the <literal>openshift</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get is &lt;image-stream-name&gt; -o jsonpath="{range .spec.tags[*]}{.name}{'\t'}{.from.name}{'\n'}{end}" -nopenshift</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get is ubi8-openjdk-17 -o jsonpath="{range .spec.tags[*]}{.name}{'\t'}{.from.name}{'\n'}{end}" -nopenshift</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">1.11	registry.access.redhat.com/ubi8/openjdk-17:1.11
1.12	registry.access.redhat.com/ubi8/openjdk-17:1.12</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Schedule periodic importing of images for each tag present in the image stream by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc tag &lt;repository/image&gt; &lt;image-stream-name:tag&gt; --scheduled -nopenshift</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc tag registry.access.redhat.com/ubi8/openjdk-17:1.11 ubi8-openjdk-17:1.11 --scheduled -nopenshift
$ oc tag registry.access.redhat.com/ubi8/openjdk-17:1.12 ubi8-openjdk-17:1.12 --scheduled -nopenshift</programlisting>
<simpara>This command causes OpenShift Container Platform to periodically update this particular image stream tag. This period is a cluster-wide setting set to 15 minutes by default.</simpara>
</listitem>
<listitem>
<simpara>Verify the scheduling status of the periodic import by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc get imagestream &lt;image-stream-name&gt; -o jsonpath="{range .spec.tags[*]}Tag: {.name}{'\t'}Scheduled: {.importPolicy.scheduled}{'\n'}{end}" -nopenshift</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc get imagestream ubi8-openjdk-17 -o jsonpath="{range .spec.tags[*]}Tag: {.name}{'\t'}Scheduled: {.importPolicy.scheduled}{'\n'}{end}" -nopenshift</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Tag: 1.11	Scheduled: true
Tag: 1.12	Scheduled: true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="post-install-node-tasks">
<title>Postinstallation node tasks</title>

<simpara>After installing OpenShift Container Platform, you can further expand and customize your
cluster to your requirements through certain node tasks.</simpara>
<section xml:id="post-install-config-adding-rhel-compute">
<title>Adding RHEL compute machines to an OpenShift Container Platform cluster</title>
<simpara>Understand and work with RHEL compute nodes.</simpara>
<section xml:id="rhel-compute-overview_post-install-node-tasks">
<title>About adding RHEL compute nodes to a cluster</title>
<simpara>In OpenShift Container Platform 4.14, you have the option of using Red Hat Enterprise Linux (RHEL) machines as compute machines in your cluster if you use a user-provisioned or installer-provisioned infrastructure installation on the <literal>x86_64</literal> architecture. You must use Red Hat Enterprise Linux CoreOS (RHCOS) machines for the control plane machines in your cluster.</simpara>
<simpara>If you choose to use RHEL compute machines in your cluster, you are responsible for all operating system life cycle management and maintenance. You must perform system updates, apply patches, and complete all other required tasks.</simpara>
<simpara>For installer-provisioned infrastructure clusters, you must manually add RHEL compute machines because automatic scaling in installer-provisioned infrastructure clusters adds Red Hat Enterprise Linux CoreOS (RHCOS) compute machines by default.</simpara>
<important>
<itemizedlist>
<listitem>
<simpara>Because removing OpenShift Container Platform from a machine in the cluster requires destroying the operating system, you must use dedicated hardware for any RHEL machines that you add to the cluster.</simpara>
</listitem>
<listitem>
<simpara>Swap memory is disabled on all RHEL machines that you add to your OpenShift Container Platform cluster. You cannot enable swap memory on these machines.</simpara>
</listitem>
</itemizedlist>
</important>
<simpara>You must add any RHEL compute machines to the cluster after you initialize the control plane.</simpara>
</section>
<section xml:id="rhel-compute-requirements_post-install-node-tasks">
<title>System requirements for RHEL compute nodes</title>
<simpara>The Red Hat Enterprise Linux (RHEL) compute machine hosts in your OpenShift Container Platform environment must meet the following minimum hardware specifications and system-level requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>You must have an active OpenShift Container Platform subscription on your Red Hat account. If you do not, contact your sales representative for more information.</simpara>
</listitem>
<listitem>
<simpara>Production environments must provide compute machines to support your expected workloads. As a cluster administrator, you must calculate the expected workload and add about 10% for overhead. For production environments, allocate enough resources so that a node host failure does not affect your maximum capacity.</simpara>
</listitem>
<listitem>
<simpara>Each system must meet the following hardware requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>Physical or virtual system, or an instance running on a public or private IaaS.</simpara>
</listitem>
<listitem>
<simpara>Base OS: <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/performing_a_standard_rhel_installation/index">RHEL 8.6 and later</link> with "Minimal" installation option.</simpara>
<important>
<simpara>Adding RHEL 7 compute machines to an OpenShift Container Platform cluster is not supported.</simpara>
<simpara>If you have RHEL 7 compute machines that were previously supported in a past OpenShift Container Platform version, you cannot upgrade them to RHEL 8. You must deploy new RHEL 8 hosts, and the old RHEL 7 hosts should be removed. See the "Deleting nodes" section for more information.</simpara>
<simpara>For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <emphasis>Deprecated and removed features</emphasis> section of the OpenShift Container Platform release notes.</simpara>
</important>
</listitem>
<listitem>
<simpara>If you deployed OpenShift Container Platform in FIPS mode, you must enable FIPS on the RHEL machine before you boot it. See <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/assembly_installing-a-rhel-8-system-with-fips-mode-enabled_security-hardening">Installing a RHEL 8 system with FIPS mode enabled</link> in the RHEL 8 documentation.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<important>
<simpara>To enable FIPS mode for your cluster, you must run the installation program from a Red Hat Enterprise Linux (RHEL) computer configured to operate in FIPS mode. For more information about configuring FIPS mode on RHEL, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/security_hardening/assembly_installing-the-system-in-fips-mode_security-hardening">Installing the system in FIPS mode</link>. When running Red Hat Enterprise Linux (RHEL) or Red Hat Enterprise Linux CoreOS (RHCOS) booted in FIPS mode, OpenShift Container Platform core components use the RHEL cryptographic libraries that have been submitted to NIST for FIPS 140-2/140-3 Validation on only the x86_64, ppc64le, and s390x architectures.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>NetworkManager 1.0 or later.</simpara>
</listitem>
<listitem>
<simpara>1 vCPU.</simpara>
</listitem>
<listitem>
<simpara>Minimum 8 GB RAM.</simpara>
</listitem>
<listitem>
<simpara>Minimum 15 GB hard disk space for the file system containing <literal>/var/</literal>.</simpara>
</listitem>
<listitem>
<simpara>Minimum 1 GB hard disk space for the file system containing <literal>/usr/local/bin/</literal>.</simpara>
</listitem>
<listitem>
<simpara>Minimum 1 GB hard disk space for the file system containing its temporary directory. The temporary system directory is determined according to the rules defined in the tempfile module in the Python standard library.</simpara>
<itemizedlist>
<listitem>
<simpara>Each system must meet any additional requirements for your system provider. For example, if you installed your cluster on VMware vSphere, your disks must be configured according to its <link xlink:href="https://vmware.github.io/vsphere-storage-for-kubernetes/documentation/index.html">storage guidelines</link> and the <literal>disk.enableUUID=true</literal> attribute must be set.</simpara>
</listitem>
<listitem>
<simpara>Each system must be able to access the cluster&#8217;s API endpoints by using DNS-resolvable hostnames. Any network security access control that is in place must allow system access to the cluster&#8217;s API service endpoints.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../nodes/nodes/nodes-nodes-working.xml#nodes-nodes-working-deleting_nodes-nodes-working">Deleting nodes</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="csr-management_post-install-node-tasks">
<title>Certificate signing requests management</title>
<simpara>Because your cluster has limited access to automatic machine management when you use infrastructure that you provision, you must provide a mechanism for approving cluster certificate signing requests (CSRs) after installation. The <literal>kube-controller-manager</literal> only approves the kubelet client CSRs. The <literal>machine-approver</literal> cannot guarantee the validity of a serving certificate that is requested by using kubelet credentials because it cannot confirm that the correct machine issued the request. You must determine and implement a method of verifying the validity of the kubelet serving certificate requests and approving them.</simpara>
</section>
</section>
<section xml:id="rhel-preparing-playbook-machine_post-install-node-tasks">
<title>Preparing the machine to run the playbook</title>
<simpara>Before you can add compute machines that use Red Hat Enterprise Linux (RHEL) as the operating system to an OpenShift Container Platform 4.14 cluster, you must prepare a RHEL 8 machine to run an Ansible playbook that adds the new node to the cluster. This machine is not part of the cluster but must be able to access it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>) on the machine that you run the playbook on.</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> permission.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Ensure that the <literal>kubeconfig</literal> file for the cluster and the installation program that you used to install the cluster are on the RHEL 8 machine. One way to accomplish this is to use the same machine that you used to install the cluster.</simpara>
</listitem>
<listitem>
<simpara>Configure the machine to access all of the RHEL hosts that you plan to use as compute machines. You can use any method that your company allows, including a bastion with an SSH proxy or a VPN.</simpara>
</listitem>
<listitem>
<simpara>Configure a user on the machine that you run the playbook on that has SSH access to all of the RHEL hosts.</simpara>
<important>
<simpara>If you use SSH key-based authentication, you must manage the key with an SSH agent.</simpara>
</important>
</listitem>
<listitem>
<simpara>If you have not already done so, register the machine with RHSM and attach a pool with an <literal>OpenShift</literal> subscription to it:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Register the machine with RHSM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</programlisting>
</listitem>
<listitem>
<simpara>Pull the latest subscription data from RHSM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager refresh</programlisting>
</listitem>
<listitem>
<simpara>List the available subscriptions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager list --available --matches '*OpenShift*'</programlisting>
</listitem>
<listitem>
<simpara>In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager attach --pool=&lt;pool_id&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Enable the repositories required by OpenShift Container Platform 4.14:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.14-for-rhel-8-x86_64-rpms"</programlisting>
</listitem>
<listitem>
<simpara>Install the required packages, including <literal>openshift-ansible</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum install openshift-ansible openshift-clients jq</programlisting>
<simpara>The <literal>openshift-ansible</literal> package provides installation program utilities and pulls in other packages that you require to add a RHEL compute node to your cluster, such as Ansible, playbooks, and related configuration files. The <literal>openshift-clients</literal> provides the <literal>oc</literal> CLI, and the <literal>jq</literal> package improves the display of JSON output on your command line.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="rhel-preparing-node_post-install-node-tasks">
<title>Preparing a RHEL compute node</title>
<simpara>Before you add a Red Hat Enterprise Linux (RHEL) machine to your OpenShift Container Platform cluster, you must register each host with Red Hat Subscription Manager (RHSM), attach an active OpenShift Container Platform subscription, and enable the required repositories. Ensure <literal>NetworkManager</literal> is enabled and configured to control all interfaces on the host.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>On each host, register with RHSM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager register --username=&lt;user_name&gt; --password=&lt;password&gt;</programlisting>
</listitem>
<listitem>
<simpara>Pull the latest subscription data from RHSM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager refresh</programlisting>
</listitem>
<listitem>
<simpara>List the available subscriptions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager list --available --matches '*OpenShift*'</programlisting>
</listitem>
<listitem>
<simpara>In the output for the previous command, find the pool ID for an OpenShift Container Platform subscription and attach it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager attach --pool=&lt;pool_id&gt;</programlisting>
</listitem>
<listitem>
<simpara>Disable all yum repositories:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Disable all the enabled RHSM repositories:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --disable="*"</programlisting>
</listitem>
<listitem>
<simpara>List the remaining yum repositories and note their names under <literal>repo id</literal>, if any:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum repolist</programlisting>
</listitem>
<listitem>
<simpara>Use <literal>yum-config-manager</literal> to disable the remaining yum repositories:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum-config-manager --disable &lt;repo_id&gt;</programlisting>
<simpara>Alternatively, disable all repositories:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum-config-manager --disable \*</programlisting>
<simpara>Note that this might take a few minutes if you have a large number of available repositories</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Enable only the repositories required by OpenShift Container Platform 4.14:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos \
    --enable="rhel-8-for-x86_64-baseos-rpms" \
    --enable="rhel-8-for-x86_64-appstream-rpms" \
    --enable="rhocp-4.14-for-rhel-8-x86_64-rpms" \
    --enable="fast-datapath-for-rhel-8-x86_64-rpms"</programlisting>
</listitem>
<listitem>
<simpara>Stop and disable firewalld on the host:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># systemctl disable --now firewalld.service</programlisting>
<note>
<simpara>You must not enable firewalld later. If you do, you cannot access OpenShift Container Platform logs on the worker.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="rhel-adding-node_post-install-node-tasks">
<title>Adding a RHEL compute machine to your cluster</title>
<simpara>You can add compute machines that use Red Hat Enterprise Linux as the operating system to an OpenShift Container Platform 4.14 cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the required packages and performed the necessary configuration on the machine that you run the playbook on.</simpara>
</listitem>
<listitem>
<simpara>You prepared the RHEL hosts for installation.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>Perform the following steps on the machine that you prepared to run the playbook:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create an Ansible inventory file that is named <literal>/&lt;path&gt;/inventory/hosts</literal> that defines your compute machine hosts and required variables:</simpara>
<screen>[all:vars]
ansible_user=root <co xml:id="CO91-1"/>
#ansible_become=True <co xml:id="CO91-2"/>

openshift_kubeconfig_path="~/.kube/config" <co xml:id="CO91-3"/>

[new_workers] <co xml:id="CO91-4"/>
mycluster-rhel8-0.example.com
mycluster-rhel8-1.example.com</screen>
<calloutlist>
<callout arearefs="CO91-1">
<para>Specify the user name that runs the Ansible tasks on the remote compute machines.</para>
</callout>
<callout arearefs="CO91-2">
<para>If you do not specify <literal>root</literal> for the <literal>ansible_user</literal>, you must set <literal>ansible_become</literal> to <literal>True</literal> and assign the user sudo permissions.</para>
</callout>
<callout arearefs="CO91-3">
<para>Specify the path and file name of the <literal>kubeconfig</literal> file for your cluster.</para>
</callout>
<callout arearefs="CO91-4">
<para>List each RHEL machine to add to your cluster. You must provide the fully-qualified domain name for each host. This name is the hostname that the cluster uses to access the machine, so set the correct public or private name to access the machine.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Navigate to the Ansible playbook directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cd /usr/share/ansible/openshift-ansible</programlisting>
</listitem>
<listitem>
<simpara>Run the playbook:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ansible-playbook -i /&lt;path&gt;/inventory/hosts playbooks/scaleup.yml <co xml:id="CO92-1"/></programlisting>
<calloutlist>
<callout arearefs="CO92-1">
<para>For <literal>&lt;path&gt;</literal>, specify the path to the Ansible inventory file that you created.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="rhel-ansible-parameters_post-install-node-tasks">
<title>Required parameters for the Ansible hosts file</title>
<simpara>You must define the following parameters in the Ansible hosts file before you add Red Hat Enterprise Linux (RHEL) compute machines to your cluster.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="40*"/>
<colspec colname="col_3" colwidth="40*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Values</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>ansible_user</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The SSH user that allows SSH-based authentication without requiring a password. If you use SSH key-based authentication, then you must manage the key with an SSH agent.</simpara></entry>
<entry align="left" valign="top"><simpara>A user name on the system. The default value is <literal>root</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ansible_become</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If the values of <literal>ansible_user</literal> is not root, you must set <literal>ansible_become</literal> to <literal>True</literal>, and the user that you specify as the <literal>ansible_user</literal>  must be configured for passwordless sudo access.</simpara></entry>
<entry align="left" valign="top"><simpara><literal>True</literal>. If the value is not <literal>True</literal>, do not specify and define this parameter.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift_kubeconfig_path</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a path and file name to a local directory that contains the <literal>kubeconfig</literal> file for your cluster.</simpara></entry>
<entry align="left" valign="top"><simpara>The path and name of the configuration file.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="rhel-removing-rhcos_post-install-node-tasks">
<title>Optional: Removing RHCOS compute machines from a cluster</title>
<simpara>After you add the Red Hat Enterprise Linux (RHEL) compute machines to your cluster, you can optionally remove the Red Hat Enterprise Linux CoreOS (RHCOS) compute machines to free up resources.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have added RHEL compute machines to your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the list of machines and record the node names of the RHCOS compute machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -o wide</programlisting>
</listitem>
<listitem>
<simpara>For each RHCOS compute machine, delete the node:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Mark the node as unschedulable by running the <literal>oc adm cordon</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm cordon &lt;node_name&gt; <co xml:id="CO93-1"/></programlisting>
<calloutlist>
<callout arearefs="CO93-1">
<para>Specify the node name of one of the RHCOS compute machines.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Drain all the pods from the node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm drain &lt;node_name&gt; --force --delete-emptydir-data --ignore-daemonsets <co xml:id="CO94-1"/></programlisting>
<calloutlist>
<callout arearefs="CO94-1">
<para>Specify the node name of the RHCOS compute machine that you isolated.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Delete the node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete nodes &lt;node_name&gt; <co xml:id="CO95-1"/></programlisting>
<calloutlist>
<callout arearefs="CO95-1">
<para>Specify the node name of the RHCOS compute machine that you drained.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Review the list of compute machines to ensure that only the RHEL nodes remain:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -o wide</programlisting>
</listitem>
<listitem>
<simpara>Remove the RHCOS machines from the load balancer for your cluster&#8217;s compute machines. You can delete the virtual machines or reimage the physical hardware for the RHCOS compute machines.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-config-adding-fcos-compute">
<title>Adding RHCOS compute machines to an OpenShift Container Platform cluster</title>
<simpara>You can add more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines to your OpenShift Container Platform cluster on bare metal.</simpara>
<simpara>Before you add more compute machines to a cluster that you installed on bare metal infrastructure, you must create RHCOS machines for it to use. You can either use an ISO image or network PXE booting to create the machines.</simpara>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You installed a cluster on bare metal.</simpara>
</listitem>
<listitem>
<simpara>You have installation media and Red Hat Enterprise Linux CoreOS (RHCOS) images that you used to create your cluster. If you do not have these files, you must obtain them by following the instructions in the <link xlink:href="../installing/installing_bare_metal/installing-bare-metal.xml#installing-bare-metal">installation procedure</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="machine-user-infra-machines-iso_post-install-node-tasks">
<title>Creating RHCOS machines using an ISO image</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using an ISO image to create the machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>)  installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Extract the Ignition config file from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc extract -n openshift-machine-api secret/worker-user-data-managed --keys=userData --to=- &gt; worker.ign</programlisting>
</listitem>
<listitem>
<simpara>Upload the <literal>worker.ign</literal> Ignition config file you exported from your cluster to your HTTP server. Note the URLs of these files.</simpara>
</listitem>
<listitem>
<simpara>You can validate that the ignition files are available on the URLs. The following example gets the Ignition config files for the compute node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -k http://&lt;HTTP_server&gt;/worker.ign</programlisting>
</listitem>
<listitem>
<simpara>You can access the ISO image for booting your new machine by running to following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">RHCOS_VHD_ORIGIN_URL=$(oc -n openshift-machine-config-operator get configmap/coreos-bootimages -o jsonpath='{.data.stream}' | jq -r '.architectures.&lt;architecture&gt;.artifacts.metal.formats.iso.disk.location')</programlisting>
</listitem>
<listitem>
<simpara>Use the ISO file to install RHCOS on more compute machines. Use the same method that you used when you created machines before you installed the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Burn the ISO image to a disk and boot it directly.</simpara>
</listitem>
<listitem>
<simpara>Use ISO redirection with a LOM interface.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Boot the RHCOS ISO image without specifying any options, or interrupting the live boot sequence. Wait for the installer to boot into a shell prompt in the RHCOS live environment.</simpara>
<note>
<simpara>You can interrupt the RHCOS installation boot process to add kernel arguments. However, for this ISO procedure you must use the <literal>coreos-installer</literal> command as outlined in the following steps, instead of adding kernel arguments.</simpara>
</note>
</listitem>
<listitem>
<simpara>Run the <literal>coreos-installer</literal> command and specify the options that meet your installation requirements. At a minimum, you must specify the URL that points to the Ignition config file for the node type, and the device that you are installing to:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo coreos-installer install --ignition-url=http://&lt;HTTP_server&gt;/&lt;node_type&gt;.ign &lt;device&gt; --ignition-hash=sha512-&lt;digest&gt; <co xml:id="CO96-1"/><co xml:id="CO96-2"/></programlisting>
<calloutlist>
<callout arearefs="CO96-1">
<para>You must run the <literal>coreos-installer</literal> command by using <literal>sudo</literal>, because the <literal>core</literal> user does not have the required root privileges to perform the installation.</para>
</callout>
<callout arearefs="CO96-2">
<para>The <literal>--ignition-hash</literal> option is required when the Ignition config file is obtained through an HTTP URL to validate the authenticity of the Ignition config file on the cluster node. <literal>&lt;digest&gt;</literal> is the Ignition config file SHA512 digest obtained in a preceding step.</para>
</callout>
</calloutlist>
<note>
<simpara>If you want to provide your Ignition config files through an HTTPS server that uses TLS, you can add the internal certificate authority (CA) to the system trust store before running <literal>coreos-installer</literal>.</simpara>
</note>
<simpara>The following example initializes a bootstrap node installation to the <literal>/dev/sda</literal> device. The Ignition config file for the bootstrap node is obtained from an HTTP web server with the IP address 192.168.1.2:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo coreos-installer install --ignition-url=http://192.168.1.2:80/installation_directory/bootstrap.ign /dev/sda --ignition-hash=sha512-a5a2d43879223273c9b60af66b44202a1d1248fc01cf156c46d4a79f552b6bad47bc8cc78ddf0116e80c59d2ea9e32ba53bc807afbca581aa059311def2c3e3b</programlisting>
</listitem>
<listitem>
<simpara>Monitor the progress of the RHCOS installation on the console of the machine.</simpara>
<important>
<simpara>Ensure that the installation is successful on each node before commencing with the OpenShift Container Platform installation. Observing the installation process can also help to determine the cause of RHCOS installation issues that might arise.</simpara>
</important>
</listitem>
<listitem>
<simpara>Continue to create more compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="machine-user-infra-machines-pxe_post-install-node-tasks">
<title>Creating RHCOS machines by PXE or iPXE booting</title>
<simpara>You can create more Red Hat Enterprise Linux CoreOS (RHCOS) compute machines for your bare metal cluster by using PXE or iPXE booting.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the URL of the Ignition config file for the compute machines for your cluster. You uploaded this file to your HTTP server during installation.</simpara>
</listitem>
<listitem>
<simpara>Obtain the URLs of the RHCOS ISO image, compressed metal BIOS, <literal>kernel</literal>, and <literal>initramfs</literal> files that you uploaded to your HTTP server during cluster installation.</simpara>
</listitem>
<listitem>
<simpara>You have access to the PXE booting infrastructure that you used to create the machines for your OpenShift Container Platform cluster during installation. The machines must boot from their local disks after RHCOS is installed on them.</simpara>
</listitem>
<listitem>
<simpara>If you use UEFI, you have access to the <literal>grub.conf</literal> file that you modified during OpenShift Container Platform installation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that your PXE or iPXE installation for the RHCOS images is correct.</simpara>
<itemizedlist>
<listitem>
<simpara>For PXE:</simpara>
<screen>DEFAULT pxeboot
TIMEOUT 20
PROMPT 0
LABEL pxeboot
    KERNEL http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; <co xml:id="CO97-1"/>
    APPEND initrd=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img <co xml:id="CO97-2"/></screen>
<calloutlist>
<callout arearefs="CO97-1">
<para>Specify the location of the live <literal>kernel</literal> file that you uploaded to your HTTP server.</para>
</callout>
<callout arearefs="CO97-2">
<para>Specify locations of the RHCOS files that you uploaded to your HTTP server. The <literal>initrd</literal> parameter value is the location of the live <literal>initramfs</literal> file, the <literal>coreos.inst.ignition_url</literal> parameter value is the location of the worker Ignition config file, and the <literal>coreos.live.rootfs_url</literal> parameter value is the location of the live <literal>rootfs</literal> file. The <literal>coreos.inst.ignition_url</literal> and <literal>coreos.live.rootfs_url</literal> parameters only support HTTP and HTTPS.</para>
</callout>
</calloutlist>
<note>
<simpara>This configuration does not enable serial console access on machines with a graphical console. To configure a different console, add one or more <literal>console=</literal> arguments to the <literal>APPEND</literal> line. For example, add <literal>console=tty0 console=ttyS0</literal> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <link xlink:href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>For iPXE (<literal>x86_64</literal> + <literal>aarch64</literal>):</simpara>
<screen>kernel http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt; initrd=main coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <co xml:id="CO98-1"/> <co xml:id="CO98-2"/>
initrd --name main http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <co xml:id="CO98-3"/>
boot</screen>
<calloutlist>
<callout arearefs="CO98-1">
<para>Specify the locations of the RHCOS files that you uploaded to your
HTTP server. The <literal>kernel</literal> parameter value is the location of the <literal>kernel</literal> file,
the <literal>initrd=main</literal> argument is needed for booting on UEFI systems,
the <literal>coreos.live.rootfs_url</literal> parameter value is the location of the <literal>rootfs</literal> file,
and the <literal>coreos.inst.ignition_url</literal> parameter value is the
location of the worker Ignition config file.</para>
</callout>
<callout arearefs="CO98-2">
<para>If you use multiple NICs, specify a single interface in the <literal>ip</literal> option.
For example, to use DHCP on a NIC that is named <literal>eno1</literal>, set <literal>ip=eno1:dhcp</literal>.</para>
</callout>
<callout arearefs="CO98-3">
<para>Specify the location of the <literal>initramfs</literal> file that you uploaded to your HTTP server.</para>
</callout>
</calloutlist>
<note>
<simpara>This configuration does not enable serial console access on machines with a graphical console To configure a different console, add one or more <literal>console=</literal> arguments to the <literal>kernel</literal> line. For example, add <literal>console=tty0 console=ttyS0</literal> to set the first PC serial port as the primary console and the graphical console as a secondary console. For more information, see <link xlink:href="https://access.redhat.com/articles/7212">How does one set up a serial terminal and/or console in Red Hat Enterprise Linux?</link> and "Enabling the serial console for PXE and ISO installation" in the "Advanced RHCOS installation configuration" section.</simpara>
</note>
<note>
<simpara>To network boot the CoreOS <literal>kernel</literal> on <literal>aarch64</literal> architecture, you need to use a version of iPXE build with the <literal>IMAGE_GZIP</literal> option enabled. See <link xlink:href="https://ipxe.org/buildcfg/image_gzip"><literal>IMAGE_GZIP</literal> option in iPXE</link>.</simpara>
</note>
</listitem>
<listitem>
<simpara>For PXE (with UEFI and GRUB as second stage) on <literal>aarch64</literal>:</simpara>
<screen>menuentry 'Install CoreOS' {
    linux rhcos-&lt;version&gt;-live-kernel-&lt;architecture&gt;  coreos.live.rootfs_url=http://&lt;HTTP_server&gt;/rhcos-&lt;version&gt;-live-rootfs.&lt;architecture&gt;.img coreos.inst.install_dev=/dev/sda coreos.inst.ignition_url=http://&lt;HTTP_server&gt;/worker.ign <co xml:id="CO99-1"/> <co xml:id="CO99-2"/>
    initrd rhcos-&lt;version&gt;-live-initramfs.&lt;architecture&gt;.img <co xml:id="CO99-3"/>
}</screen>
<calloutlist>
<callout arearefs="CO99-1">
<para>Specify the locations of the RHCOS files that you uploaded to your
HTTP/TFTP server. The <literal>kernel</literal> parameter value is the location of the <literal>kernel</literal> file on your TFTP server.
The <literal>coreos.live.rootfs_url</literal> parameter value is the location of the <literal>rootfs</literal> file, and the <literal>coreos.inst.ignition_url</literal> parameter value is the location of the worker Ignition config file on your HTTP Server.</para>
</callout>
<callout arearefs="CO99-2">
<para>If you use multiple NICs, specify a single interface in the <literal>ip</literal> option.
For example, to use DHCP on a NIC that is named <literal>eno1</literal>, set <literal>ip=eno1:dhcp</literal>.</para>
</callout>
<callout arearefs="CO99-3">
<para>Specify the location of the <literal>initramfs</literal> file that you uploaded to your TFTP server.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Use the PXE or iPXE infrastructure to create the required compute machines for your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-approve-csrs_post-install-node-tasks">
<title>Approving the certificate signing requests for your machines</title>
<simpara>When you add machines to a cluster, two pending certificate signing requests (CSRs) are generated for each machine that you added. You must confirm that these CSRs are approved or, if necessary, approve them yourself. The client requests must be approved first, followed by the server requests.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You added machines to your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Confirm that the cluster recognizes the machines:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  63m  v1.28.5
master-1  Ready     master  63m  v1.28.5
master-2  Ready     master  64m  v1.28.5</programlisting>
</para>
</formalpara>
<simpara>The output lists all of the machines that you created.</simpara>
<note>
<simpara>The preceding output might not include the compute nodes, also known as worker nodes, until some CSRs are approved.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the pending CSRs and ensure that you see the client requests with the <literal>Pending</literal> or <literal>Approved</literal> status for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-8b2br   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
csr-8vnps   15m     system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</programlisting>
</para>
</formalpara>
<simpara>In this example, two machines are joining the cluster. You might see more approved CSRs in the list.</simpara>
</listitem>
<listitem>
<simpara>If the CSRs were not approved, after all of the pending CSRs for the machines you added are in <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<note>
<simpara>Because the CSRs rotate automatically, approve your CSRs within an hour of adding the machines to the cluster. If you do not approve them within an hour, the certificates will rotate, and more than two certificates will be present for each node. You must approve all of these certificates. After the client CSR is approved, the Kubelet creates a secondary CSR for the serving certificate, which requires manual approval. Then, subsequent serving certificate renewal requests are automatically approved by the <literal>machine-approver</literal> if the Kubelet requests a new certificate with identical parameters.</simpara>
</note>
<note>
<simpara>For clusters running on platforms that are not machine API enabled, such as bare metal and other user-provisioned infrastructure, you must implement a method of automatically approving the kubelet serving certificate requests (CSRs). If a request is not approved, then the <literal>oc exec</literal>, <literal>oc rsh</literal>, and <literal>oc logs</literal> commands cannot succeed, because a serving certificate is required when the API server connects to the kubelet. Any operation that contacts the Kubelet endpoint requires this certificate approval to be in place. The method must watch for new CSRs, confirm that the CSR was submitted by the <literal>node-bootstrapper</literal> service account in the <literal>system:node</literal> or <literal>system:admin</literal> groups, and confirm the identity of the node.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO100-1"/></programlisting>
<calloutlist>
<callout arearefs="CO100-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs --no-run-if-empty oc adm certificate approve</programlisting>
<note>
<simpara>Some Operators might not become available until some CSRs are approved.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Now that your client requests are approved, you must review the server requests for each machine that you added to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        AGE     REQUESTOR                                                                   CONDITION
csr-bfd72   5m26s   system:node:ip-10-0-50-126.us-east-2.compute.internal                       Pending
csr-c57lv   5m26s   system:node:ip-10-0-95-157.us-east-2.compute.internal                       Pending
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If the remaining CSRs are not approved, and are in the <literal>Pending</literal> status, approve the CSRs for your cluster machines:</simpara>
<itemizedlist>
<listitem>
<simpara>To approve them individually, run the following command for each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt; <co xml:id="CO101-1"/></programlisting>
<calloutlist>
<callout arearefs="CO101-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To approve all pending CSRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr -o go-template='{{range .items}}{{if not .status}}{{.metadata.name}}{{"\n"}}{{end}}{{end}}' | xargs oc adm certificate approve</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After all client and server CSRs have been approved, the machines have the <literal>Ready</literal> status. Verify this by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME      STATUS    ROLES   AGE  VERSION
master-0  Ready     master  73m  v1.28.5
master-1  Ready     master  73m  v1.28.5
master-2  Ready     master  74m  v1.28.5
worker-0  Ready     worker  11m  v1.28.5
worker-1  Ready     worker  11m  v1.28.5</programlisting>
</para>
</formalpara>
<note>
<simpara>It can take a few minutes after approval of the server CSRs for the machines to transition to the <literal>Ready</literal> status.</simpara>
</note>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional information</title>
<listitem>
<simpara>For more information on CSRs, see <link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">Certificate Signing Requests</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="machine-node-custom-partition_post-install-node-tasks">
<title>Adding a new RHCOS worker node with a custom <literal>/var</literal> partition in AWS</title>
<simpara>OpenShift Container Platform supports partitioning devices during installation by using machine configs that are processed during the bootstrap. However, if you use <literal>/var</literal> partitioning, the device name must be determined at installation and cannot be changed. You cannot add different instance types as nodes if they have a different device naming schema. For example, if you configured the <literal>/var</literal> partition with the default AWS device name for <literal>m4.large</literal> instances, <literal>dev/xvdb</literal>, you cannot directly add an AWS <literal>m5.large</literal> instance, as <literal>m5.large</literal> instances use a <literal>/dev/nvme1n1</literal> device by default. The device might fail to partition due to the different naming schema.</simpara>
<simpara>The procedure in this section shows how to add a new Red Hat Enterprise Linux CoreOS (RHCOS) compute node with an instance that uses a different device name from what was configured at installation. You create a custom user data secret and configure a new compute machine set. These steps are specific to an AWS cluster. The principles apply to other cloud deployments also. However, the device naming schema is different for other deployments and should be determined on a per-case basis.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>On a command line, change to the <literal>openshift-machine-api</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Create a new secret from the <literal>worker-user-data</literal> secret:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Export the <literal>userData</literal> section of the secret to a text file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret worker-user-data --template='{{index .data.userData | base64decode}}' | jq &gt; userData.txt</programlisting>
</listitem>
<listitem>
<simpara>Edit the text file to add the <literal>storage</literal>, <literal>filesystems</literal>, and <literal>systemd</literal> stanzas for the partitions you want to use for the new node. You can specify any <link xlink:href="https://coreos.github.io/ignition/configuration-v3_2/">Ignition configuration parameters</link> as needed.</simpara>
<note>
<simpara>Do not change the values in the <literal>ignition</literal> stanza.</simpara>
</note>
<programlisting language="terminal" linenumbering="unnumbered">{
  "ignition": {
    "config": {
      "merge": [
        {
          "source": "https:...."
        }
      ]
    },
    "security": {
      "tls": {
        "certificateAuthorities": [
          {
            "source": "data:text/plain;charset=utf-8;base64,.....=="
          }
        ]
      }
    },
    "version": "3.2.0"
  },
  "storage": {
    "disks": [
      {
        "device": "/dev/nvme1n1", <co xml:id="CO102-1"/>
        "partitions": [
          {
            "label": "var",
            "sizeMiB": 50000, <co xml:id="CO102-2"/>
            "startMiB": 0 <co xml:id="CO102-3"/>
          }
        ]
      }
    ],
    "filesystems": [
      {
        "device": "/dev/disk/by-partlabel/var", <co xml:id="CO102-4"/>
        "format": "xfs", <co xml:id="CO102-5"/>
        "path": "/var" <co xml:id="CO102-6"/>
      }
    ]
  },
  "systemd": {
    "units": [ <co xml:id="CO102-7"/>
      {
        "contents": "[Unit]\nBefore=local-fs.target\n[Mount]\nWhere=/var\nWhat=/dev/disk/by-partlabel/var\nOptions=defaults,pquota\n[Install]\nWantedBy=local-fs.target\n",
        "enabled": true,
        "name": "var.mount"
      }
    ]
  }
}</programlisting>
<calloutlist>
<callout arearefs="CO102-1">
<para>Specifies an absolute path to the AWS block device.</para>
</callout>
<callout arearefs="CO102-2">
<para>Specifies the size of the data partition in Mebibytes.</para>
</callout>
<callout arearefs="CO102-3">
<para>Specifies the start of the partition in Mebibytes. When adding a data partition to the boot disk, a minimum value of 25000 MB (Mebibytes) is recommended. The root file system is automatically resized to fill all available space up to the specified offset. If no value is specified, or if the specified value is smaller than the recommended minimum, the resulting root file system will be too small, and future reinstalls of RHCOS might overwrite the beginning of the data partition.</para>
</callout>
<callout arearefs="CO102-4">
<para>Specifies an absolute path to the <literal>/var</literal> partition.</para>
</callout>
<callout arearefs="CO102-5">
<para>Specifies the filesystem format.</para>
</callout>
<callout arearefs="CO102-6">
<para>Specifies the mount-point of the filesystem while Ignition is running relative to where the root filesystem will be mounted. This is not necessarily the same as where it should be mounted in the real root, but it is encouraged to make it the same.</para>
</callout>
<callout arearefs="CO102-7">
<para>Defines a systemd mount unit that mounts the <literal>/dev/disk/by-partlabel/var</literal> device to the <literal>/var</literal> partition.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Extract the <literal>disableTemplating</literal> section from the <literal>work-user-data</literal> secret to a text file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret worker-user-data --template='{{index .data.disableTemplating | base64decode}}' | jq &gt; disableTemplating.txt</programlisting>
</listitem>
<listitem>
<simpara>Create the new user data secret file from the two text files. This user data secret passes the additional node partition information in the <literal>userData.txt</literal> file to the newly created node.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic worker-user-data-x5 --from-file=userData=userData.txt --from-file=disableTemplating=disableTemplating.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a new compute machine set for the new node:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a new compute machine set YAML file, similar to the following, which is configured for AWS. Add the required partitions and the newly-created user data secret:</simpara>
<tip>
<simpara>Use an existing compute machine set as a template and change the parameters as needed for the new node.</simpara>
</tip>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: auto-52-92tf4
  name: worker-us-east-2-nvme1n1 <co xml:id="CO103-1"/>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: auto-52-92tf4
      machine.openshift.io/cluster-api-machineset: auto-52-92tf4-worker-us-east-2b
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: auto-52-92tf4
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: auto-52-92tf4-worker-us-east-2b
    spec:
      metadata: {}
      providerSpec:
        value:
          ami:
            id: ami-0c2dbd95931a
          apiVersion: awsproviderconfig.openshift.io/v1beta1
          blockDevices:
          - DeviceName: /dev/nvme1n1 <co xml:id="CO103-2"/>
            ebs:
              encrypted: true
              iops: 0
              volumeSize: 120
              volumeType: gp2
          - DeviceName: /dev/nvme1n2 <co xml:id="CO103-3"/>
            ebs:
              encrypted: true
              iops: 0
              volumeSize: 50
              volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: auto-52-92tf4-worker-profile
          instanceType: m6i.large
          kind: AWSMachineProviderConfig
          metadata:
            creationTimestamp: null
          placement:
            availabilityZone: us-east-2b
            region: us-east-2
          securityGroups:
          - filters:
            - name: tag:Name
              values:
              - auto-52-92tf4-worker-sg
          subnet:
            id: subnet-07a90e5db1
          tags:
          - name: kubernetes.io/cluster/auto-52-92tf4
            value: owned
          userDataSecret:
            name: worker-user-data-x5 <co xml:id="CO103-4"/></programlisting>
<calloutlist>
<callout arearefs="CO103-1">
<para>Specifies a name for the new node.</para>
</callout>
<callout arearefs="CO103-2">
<para>Specifies an absolute path to the AWS block device, here an encrypted EBS volume.</para>
</callout>
<callout arearefs="CO103-3">
<para>Optional. Specifies an additional EBS volume.</para>
</callout>
<callout arearefs="CO103-4">
<para>Specifies the user data secret file.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the compute machine set:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;.yaml</programlisting>
<simpara>The machines might take a few moments to become available.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the new partition and nodes are created:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Verify that the compute machine set is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               DESIRED   CURRENT   READY   AVAILABLE   AGE
ci-ln-2675bt2-76ef8-bdgsc-worker-us-east-1a        1         1         1       1           124m
ci-ln-2675bt2-76ef8-bdgsc-worker-us-east-1b        2         2         2       2           124m
worker-us-east-2-nvme1n1                           1         1         1       1           2m35s <co xml:id="CO104-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO104-1">
<para>This is the new compute machine set.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the new node is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS   ROLES    AGE     VERSION
ip-10-0-128-78.ec2.internal    Ready    worker   117m    v1.28.5
ip-10-0-146-113.ec2.internal   Ready    master   127m    v1.28.5
ip-10-0-153-35.ec2.internal    Ready    worker   118m    v1.28.5
ip-10-0-176-58.ec2.internal    Ready    master   126m    v1.28.5
ip-10-0-217-135.ec2.internal   Ready    worker   2m57s   v1.28.5 <co xml:id="CO105-1"/>
ip-10-0-225-248.ec2.internal   Ready    master   127m    v1.28.5
ip-10-0-245-59.ec2.internal    Ready    worker   116m    v1.28.5</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO105-1">
<para>This is new new node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the custom <literal>/var</literal> partition is created on the new node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node-name&gt; -- chroot /host lsblk</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-217-135.ec2.internal -- chroot /host lsblk</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        MAJ:MIN  RM  SIZE RO TYPE MOUNTPOINT
nvme0n1     202:0    0   120G  0 disk
|-nvme0n1p1 202:1    0     1M  0 part
|-nvme0n1p2 202:2    0   127M  0 part
|-nvme0n1p3 202:3    0   384M  0 part /boot
`-nvme0n1p4 202:4    0 119.5G  0 part /sysroot
nvme1n1     202:16   0    50G  0 disk
`-nvme1n1p1 202:17   0  48.8G  0 part /var <co xml:id="CO106-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO106-1">
<para>The <literal>nvme1n1</literal> device is mounted to the <literal>/var</literal> partition.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information on how OpenShift Container Platform uses disk partitioning, see <link xlink:href="../installing/installing_bare_metal/installing-bare-metal.xml#installation-user-infra-machines-advanced_disk_installing-bare-metal">Disk partitioning</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-installation-config-deploying-machine-health-checks">
<title>Deploying machine health checks</title>
<simpara>Understand and deploy machine health checks.</simpara>
<important>
<simpara>You can use the advanced machine management and scaling capabilities only in clusters where the Machine API is operational. Clusters with user-provisioned infrastructure require additional validation and configuration to use the Machine API.</simpara>
<simpara>Clusters with the infrastructure platform type <literal>none</literal> cannot use the Machine API. This limitation applies even if the compute machines that are attached to the cluster are installed on a platform that supports the feature. This parameter cannot be changed after installation.</simpara>
<simpara>To view the platform type for your cluster, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get infrastructure cluster -o jsonpath='{.status.platform}'</programlisting>
</important>
<section xml:id="machine-health-checks-about_post-install-node-tasks">
<title>About machine health checks</title>
<note>
<simpara>You can only apply a machine health check to machines that are managed by compute machine sets or control plane machine sets.</simpara>
</note>
<simpara>To monitor machine health, create a resource to define the configuration for a controller. Set a condition to check, such as staying in the <literal>NotReady</literal> status for five minutes or displaying a permanent condition in the node-problem-detector, and a label for the set of machines to monitor.</simpara>
<simpara>The controller that observes a <literal>MachineHealthCheck</literal> resource checks for the defined condition. If a machine fails the health check, the machine is automatically deleted and one is created to take its place. When a machine is deleted, you see a <literal>machine deleted</literal> event.</simpara>
<simpara>To limit disruptive impact of the machine deletion, the controller drains and deletes only one node at a time. If there are more unhealthy machines than the <literal>maxUnhealthy</literal> threshold allows for in the targeted pool of machines, remediation stops and therefore enables manual intervention.</simpara>
<note>
<simpara>Consider the timeouts carefully, accounting for workloads and requirements.</simpara>
<itemizedlist>
<listitem>
<simpara>Long timeouts can result in long periods of downtime for the workload on the unhealthy machine.</simpara>
</listitem>
<listitem>
<simpara>Too short timeouts can result in a remediation loop. For example, the timeout for checking the <literal>NotReady</literal> status must be long enough to allow the machine to complete the startup process.</simpara>
</listitem>
</itemizedlist>
</note>
<simpara>To stop the check, remove the resource.</simpara>
<section xml:id="machine-health-checks-limitations_post-install-node-tasks">
<title>Limitations when deploying machine health checks</title>
<simpara>There are limitations to consider before deploying a machine health check:</simpara>
<itemizedlist>
<listitem>
<simpara>Only machines owned by a machine set are remediated by a machine health check.</simpara>
</listitem>
<listitem>
<simpara>If the node for a machine is removed from the cluster, a machine health check considers the machine to be unhealthy and remediates it immediately.</simpara>
</listitem>
<listitem>
<simpara>If the corresponding node for a machine does not join the cluster after the <literal>nodeStartupTimeout</literal>, the machine is remediated.</simpara>
</listitem>
<listitem>
<simpara>A machine is remediated immediately if the <literal>Machine</literal> resource phase is <literal>Failed</literal>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../machine_management/control_plane_machine_management/cpmso-about.xml#cpmso-about">About control plane machine sets</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="machine-health-checks-resource_post-install-node-tasks">
<title>Sample MachineHealthCheck resource</title>
<simpara>The <literal>MachineHealthCheck</literal> resource for all cloud-based installation types, and other than bare metal, resembles the following YAML file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineHealthCheck
metadata:
  name: example <co xml:id="CO107-1"/>
  namespace: openshift-machine-api
spec:
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-machine-role: &lt;role&gt; <co xml:id="CO107-2"/>
      machine.openshift.io/cluster-api-machine-type: &lt;role&gt; <co xml:id="CO107-3"/>
      machine.openshift.io/cluster-api-machineset: &lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt; <co xml:id="CO107-4"/>
  unhealthyConditions:
  - type:    "Ready"
    timeout: "300s" <co xml:id="CO107-5"/>
    status: "False"
  - type:    "Ready"
    timeout: "300s" <co xml:id="CO107-6"/>
    status: "Unknown"
  maxUnhealthy: "40%" <co xml:id="CO107-7"/>
  nodeStartupTimeout: "10m" <co xml:id="CO107-8"/></programlisting>
<calloutlist>
<callout arearefs="CO107-1">
<para>Specify the name of the machine health check to deploy.</para>
</callout>
<callout arearefs="CO107-2 CO107-3">
<para>Specify a label for the machine pool that you want to check.</para>
</callout>
<callout arearefs="CO107-4">
<para>Specify the machine set to track in <literal>&lt;cluster_name&gt;-&lt;label&gt;-&lt;zone&gt;</literal> format. For example, <literal>prod-node-us-east-1a</literal>.</para>
</callout>
<callout arearefs="CO107-5 CO107-6">
<para>Specify the timeout duration for a node condition. If a condition is met for the duration of the timeout, the machine will be remediated. Long timeouts can result in long periods of downtime for a workload on an unhealthy machine.</para>
</callout>
<callout arearefs="CO107-7">
<para>Specify the amount of machines allowed to be concurrently remediated in the targeted pool. This can be set as a percentage or an integer. If the number of unhealthy machines exceeds the limit set by <literal>maxUnhealthy</literal>, remediation is not performed.</para>
</callout>
<callout arearefs="CO107-8">
<para>Specify the timeout duration that a machine health check must wait for a node to join the cluster before a machine is determined to be unhealthy.</para>
</callout>
</calloutlist>
<note>
<simpara>The <literal>matchLabels</literal> are examples only; you must map your machine groups based on your specific needs.</simpara>
</note>
<section xml:id="machine-health-checks-short-circuiting_post-install-node-tasks">
<title>Short-circuiting machine health check remediation</title>
<simpara>Short-circuiting ensures that machine health checks remediate machines only when the cluster is healthy.
Short-circuiting is configured through the <literal>maxUnhealthy</literal> field in the <literal>MachineHealthCheck</literal> resource.</simpara>
<simpara>If the user defines a value for the <literal>maxUnhealthy</literal> field, before remediating any machines, the <literal>MachineHealthCheck</literal> compares the value of <literal>maxUnhealthy</literal> with the number of machines within its target pool that it has determined to be unhealthy. Remediation is not performed if the number of unhealthy machines exceeds the <literal>maxUnhealthy</literal> limit.</simpara>
<important>
<simpara>If <literal>maxUnhealthy</literal> is not set, the value defaults to <literal>100%</literal> and the machines are remediated regardless of the state of the cluster.</simpara>
</important>
<simpara>The appropriate <literal>maxUnhealthy</literal> value depends on the scale of the cluster you deploy and how many machines the <literal>MachineHealthCheck</literal> covers. For example, you can use the <literal>maxUnhealthy</literal> value to cover multiple compute machine sets across multiple availability zones so that if you lose an entire zone, your <literal>maxUnhealthy</literal> setting prevents further remediation within the cluster. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.</simpara>
<important>
<simpara>If you configure a <literal>MachineHealthCheck</literal> resource for the control plane, set the value of <literal>maxUnhealthy</literal> to <literal>1</literal>.</simpara>
<simpara>This configuration ensures that the machine health check takes no action when multiple control plane machines appear to be unhealthy. Multiple unhealthy control plane machines can indicate that the etcd cluster is degraded or that a scaling operation to replace a failed machine is in progress.</simpara>
<simpara>If the etcd cluster is degraded, manual intervention might be required. If a scaling operation is in progress, the machine health check should allow it to finish.</simpara>
</important>
<simpara>The <literal>maxUnhealthy</literal> field can be set as either an integer or percentage.
There are different remediation implementations depending on the <literal>maxUnhealthy</literal> value.</simpara>
<section xml:id="_setting_maxunhealthy_by_using_an_absolute_value">
<title>Setting maxUnhealthy by using an absolute value</title>
<simpara>If <literal>maxUnhealthy</literal> is set to <literal>2</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>Remediation will be performed if 2 or fewer nodes are unhealthy</simpara>
</listitem>
<listitem>
<simpara>Remediation will not be performed if 3 or more nodes are unhealthy</simpara>
</listitem>
</itemizedlist>
<simpara>These values are independent of how many machines are being checked by the machine health check.</simpara>
</section>
<section xml:id="_setting_maxunhealthy_by_using_percentages">
<title>Setting maxUnhealthy by using percentages</title>
<simpara>If <literal>maxUnhealthy</literal> is set to <literal>40%</literal> and there are 25 machines being checked:</simpara>
<itemizedlist>
<listitem>
<simpara>Remediation will be performed if 10 or fewer nodes are unhealthy</simpara>
</listitem>
<listitem>
<simpara>Remediation will not be performed if 11 or more nodes are unhealthy</simpara>
</listitem>
</itemizedlist>
<simpara>If <literal>maxUnhealthy</literal> is set to <literal>40%</literal> and there are 6 machines being checked:</simpara>
<itemizedlist>
<listitem>
<simpara>Remediation will be performed if 2 or fewer nodes are unhealthy</simpara>
</listitem>
<listitem>
<simpara>Remediation will not be performed if 3 or more nodes are unhealthy</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The allowed number of machines is rounded down when the percentage of <literal>maxUnhealthy</literal> machines that are checked is not a whole number.</simpara>
</note>
</section>
</section>
</section>
<section xml:id="machine-health-checks-creating_post-install-node-tasks">
<title>Creating a machine health check resource</title>
<simpara>You can create a <literal>MachineHealthCheck</literal> resource for machine sets in your cluster.</simpara>
<note>
<simpara>You can only apply a machine health check to machines that are managed by compute machine sets or control plane machine sets.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal>oc</literal> command line interface.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>healthcheck.yml</literal> file that contains the definition of your machine health check.</simpara>
</listitem>
<listitem>
<simpara>Apply the <literal>healthcheck.yml</literal> file to your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f healthcheck.yml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="machineset-manually-scaling_post-install-node-tasks">
<title>Scaling a compute machine set manually</title>
<simpara>To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.</simpara>
<simpara>This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install an OpenShift Container Platform cluster and the <literal>oc</literal> command line.</simpara>
</listitem>
<listitem>
<simpara>Log in to  <literal>oc</literal> as a user with <literal>cluster-admin</literal> permission.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the compute machine sets that are in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machinesets -n openshift-machine-api</programlisting>
<simpara>The compute machine sets are listed in the form of <literal>&lt;clusterid&gt;-worker-&lt;aws-region-az&gt;</literal>.</simpara>
</listitem>
<listitem>
<simpara>View the compute machines that are in the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Set the annotation on the compute machine that you want to delete by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate machine/&lt;machine_name&gt; -n openshift-machine-api machine.openshift.io/delete-machine="true"</programlisting>
</listitem>
<listitem>
<simpara>Scale the compute machine set by running one of the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>Or:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to scale the compute machine set:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2</programlisting>
</tip>
<simpara>You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.</simpara>
<important>
<simpara>By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.</simpara>
<simpara>You can skip draining the node by annotating <literal>machine.openshift.io/exclude-node-draining</literal> in a specific machine.</simpara>
</important>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify the deletion of the intended machine by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="differences-between-machinesets-and-machineconfigpool_post-install-node-tasks">
<title>Understanding the difference between compute machine sets and the machine config pool</title>
<simpara><literal>MachineSet</literal> objects describe OpenShift Container Platform nodes with respect to the cloud or machine provider.</simpara>
<simpara>The <literal>MachineConfigPool</literal> object allows <literal>MachineConfigController</literal> components to define and provide the status of machines in the context of upgrades.</simpara>
<simpara>The <literal>MachineConfigPool</literal> object allows users to configure how upgrades are rolled out to the OpenShift Container Platform nodes in the machine config pool.</simpara>
<simpara>The <literal>NodeSelector</literal> object can be replaced with a reference to the <literal>MachineSet</literal> object.</simpara>
</section>
</section>
<section xml:id="recommended-node-host-practices_post-install-node-tasks">
<title>Recommended node host practices</title>
<simpara>The OpenShift Container Platform node configuration file contains important options. For
example, two parameters control the maximum number of pods that can be scheduled
to a node: <literal>podsPerCore</literal> and <literal>maxPods</literal>.</simpara>
<simpara>When both options are in use, the lower of the two values limits the number of
pods on a node. Exceeding these values can result in:</simpara>
<itemizedlist>
<listitem>
<simpara>Increased CPU utilization.</simpara>
</listitem>
<listitem>
<simpara>Slow pod scheduling.</simpara>
</listitem>
<listitem>
<simpara>Potential out-of-memory scenarios, depending on the amount of memory in the node.</simpara>
</listitem>
<listitem>
<simpara>Exhausting the pool of IP addresses.</simpara>
</listitem>
<listitem>
<simpara>Resource overcommitting, leading to poor user application performance.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>In Kubernetes, a pod that is holding a single container actually uses two
containers. The second container is used to set up networking prior to the
actual container starting. Therefore, a system running 10 pods will actually
have 20 containers running.</simpara>
</important>
<note>
<simpara>Disk IOPS throttling from the cloud provider might have an impact on CRI-O and kubelet.
They might get overloaded when there are large number of I/O intensive pods running on
the nodes. It is recommended that you monitor the disk I/O on the nodes and use volumes
with sufficient throughput for the workload.</simpara>
</note>
<simpara><literal>podsPerCore</literal> sets the number of pods the node can run based on the number of
processor cores on the node. For example, if <literal>podsPerCore</literal> is set to <literal>10</literal> on a
node with 4 processor cores, the maximum number of pods allowed on the node will
be <literal>40</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kubeletConfig:
  podsPerCore: 10</programlisting>
<simpara>Setting <literal>podsPerCore</literal> to <literal>0</literal> disables this limit. The default is <literal>0</literal>.
<literal>podsPerCore</literal> cannot exceed <literal>maxPods</literal>.</simpara>
<simpara><literal>maxPods</literal> sets the number of pods the node can run to a fixed value, regardless
of the properties of the node.</simpara>
<programlisting language="yaml" linenumbering="unnumbered"> kubeletConfig:
    maxPods: 250</programlisting>
<section xml:id="create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-node-tasks">
<title>Creating a KubeletConfig CRD to edit kubelet parameters</title>
<simpara>The kubelet configuration is currently serialized as an Ignition configuration, so it can be directly edited. However, there is also a new <literal>kubelet-config-controller</literal> added to the Machine Config Controller (MCC). This lets you use a <literal>KubeletConfig</literal> custom resource (CR) to edit the kubelet parameters.</simpara>
<note>
<simpara>As the fields in the <literal>kubeletConfig</literal> object are passed directly to the kubelet from upstream Kubernetes, the kubelet validates those values directly. Invalid values in the <literal>kubeletConfig</literal> object might cause cluster nodes to become unavailable. For valid values, see the <link xlink:href="https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/">Kubernetes documentation</link>.</simpara>
</note>
<simpara>Consider the following guidance:</simpara>
<itemizedlist>
<listitem>
<simpara>Create one <literal>KubeletConfig</literal> CR for each machine config pool with all the config changes you want for that pool. If you are applying the same content to all of the pools, you need only one <literal>KubeletConfig</literal> CR for all of the pools.</simpara>
</listitem>
<listitem>
<simpara>Edit an existing <literal>KubeletConfig</literal> CR to modify existing settings or add new settings, instead of creating a CR for each change. It is recommended that you create a CR only to modify a different machine config pool, or for changes that are intended to be temporary, so that you can revert the changes.</simpara>
</listitem>
<listitem>
<simpara>As needed, create multiple <literal>KubeletConfig</literal> CRs with a limit of 10 per cluster. For the first <literal>KubeletConfig</literal> CR, the Machine Config Operator (MCO) creates a machine config appended with <literal>kubelet</literal>. With each subsequent CR, the controller creates another <literal>kubelet</literal> machine config with a numeric suffix. For example, if you have a <literal>kubelet</literal> machine config with a <literal>-2</literal> suffix, the next <literal>kubelet</literal> machine config is appended with <literal>-3</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>If you want to delete the machine configs, delete them in reverse order to avoid exceeding the limit. For example, you delete the <literal>kubelet-3</literal> machine config before deleting the <literal>kubelet-2</literal> machine config.</simpara>
<note>
<simpara>If you have a machine config with a <literal>kubelet-9</literal> suffix, and you create another <literal>KubeletConfig</literal> CR, a new machine config is not created, even if there are fewer than 10 <literal>kubelet</literal> machine configs.</simpara>
</note>
<formalpara>
<title>Example <literal>KubeletConfig</literal> CR</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfig</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                AGE
set-max-pods        15m</programlisting>
<formalpara>
<title>Example showing a <literal>KubeletConfig</literal> machine config</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mc | grep kubelet</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">...
99-worker-generated-kubelet-1                  b5c5119de007945b6fe6fb215db3b8e2ceb12511   3.2.0             26m
...</programlisting>
<simpara>The following procedure is an example to show how to configure the maximum number of pods per node on the worker nodes.</simpara>
<orderedlist numeration="arabic">
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the label associated with the static <literal>MachineConfigPool</literal> CR for the type of node you want to configure.
Perform one of the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>View the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe machineconfigpool &lt;name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe machineconfigpool worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: 2019-02-08T14:52:39Z
  generation: 1
  labels:
    custom-kubelet: set-max-pods <co xml:id="CO108-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO108-1">
<para>If a label has been added it appears under <literal>labels</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If the label is not present, add a key/value pair:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the available machine configuration objects that you can select:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig</programlisting>
<simpara>By default, the two kubelet-related configs are <literal>01-master-kubelet</literal> and <literal>01-worker-kubelet</literal>.</simpara>
</listitem>
<listitem>
<simpara>Check the current value for the maximum pods per node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node ci-ln-5grqprb-f76d1-ncnqq-worker-a-mdv94</programlisting>
<simpara>Look for <literal>value: pods: &lt;value&gt;</literal> in the <literal>Allocatable</literal> stanza:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Allocatable:
 attachable-volumes-aws-ebs:  25
 cpu:                         3500m
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      15341844Ki
 pods:                        250</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Set the maximum pods per node on the worker nodes by creating a custom resource file that contains the kubelet configuration:</simpara>
<important>
<simpara>Kubelet configurations that target a specific machine config pool also affect any dependent pools. For example, creating a kubelet configuration for the pool containing worker nodes will also apply to any subset pools, including the pool containing infrastructure nodes. To avoid this, you must create a new machine config pool with a selection expression that only includes worker nodes, and have your kubelet configuration target this new pool.</simpara>
</important>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods <co xml:id="CO109-1"/>
  kubeletConfig:
    maxPods: 500 <co xml:id="CO109-2"/></programlisting>
<calloutlist>
<callout arearefs="CO109-1">
<para>Enter the label from the machine config pool.</para>
</callout>
<callout arearefs="CO109-2">
<para>Add the kubelet configuration. In this example, use <literal>maxPods</literal> to set the maximum pods per node.</para>
</callout>
</calloutlist>
<note>
<simpara>The rate at which the kubelet talks to the API server depends on queries per second (QPS) and burst values. The default values, <literal>50</literal> for <literal>kubeAPIQPS</literal> and <literal>100</literal> for <literal>kubeAPIBurst</literal>, are sufficient if there are limited pods running on each node. It is recommended to update the kubelet QPS and burst rates if there are enough CPU and memory resources on the node.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
  kubeletConfig:
    maxPods: &lt;pod_count&gt;
    kubeAPIBurst: &lt;burst_rate&gt;
    kubeAPIQPS: &lt;QPS&gt;</programlisting>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Update the machine config pool for workers with the label:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label machineconfigpool worker custom-kubelet=set-max-pods</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>KubeletConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f change-maxPods-cr.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>KubeletConfig</literal> object is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                AGE
set-max-pods        15m</programlisting>
</para>
</formalpara>
<simpara>Depending on the number of worker nodes in the cluster, wait for the worker nodes to be rebooted one by one. For a cluster with 3 worker nodes, this could take about 10 to 15 minutes.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the changes are applied to the node:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check on a worker node that the <literal>maxPods</literal> value changed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Locate the <literal>Allocatable</literal> stanza:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> ...
Allocatable:
  attachable-volumes-gce-pd:  127
  cpu:                        3500m
  ephemeral-storage:          123201474766
  hugepages-1Gi:              0
  hugepages-2Mi:              0
  memory:                     14225400Ki
  pods:                       500 <co xml:id="CO110-1"/>
 ...</programlisting>
<calloutlist>
<callout arearefs="CO110-1">
<para>In this example, the <literal>pods</literal> parameter should report the value you set in the <literal>KubeletConfig</literal> object.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify the change in the <literal>KubeletConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeletconfigs set-max-pods -o yaml</programlisting>
<simpara>This should show a status of <literal>True</literal> and <literal>type:Success</literal>, as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  kubeletConfig:
    maxPods: 500
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: set-max-pods
status:
  conditions:
  - lastTransitionTime: "2021-06-30T17:04:07Z"
    message: Success
    status: "True"
    type: Success</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="modify-unavailable-workers_post-install-node-tasks">
<title>Modifying the number of unavailable worker nodes</title>
<simpara>By default, only one machine is allowed to be unavailable when applying the kubelet-related configuration to the available worker nodes. For a large cluster, it can take a long time for the configuration change to be reflected. At any time, you can adjust the number of machines that are updating to speed up the process.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>worker</literal> machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool worker</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>maxUnavailable</literal> field and set the value:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  maxUnavailable: &lt;node_count&gt;</programlisting>
<important>
<simpara>When setting the value, consider the number of worker nodes that can be
unavailable without affecting the applications running on the cluster.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="master-node-sizing_post-install-node-tasks">
<title>Control plane node sizing</title>
<simpara>The control plane node resource requirements depend on the number and type of nodes and objects in the cluster. The following control plane node size recommendations are based on the results of a control plane density focused testing, or <emphasis>Cluster-density</emphasis>. This test creates the following objects across a given number of namespaces:</simpara>
<itemizedlist>
<listitem>
<simpara>1 image stream</simpara>
</listitem>
<listitem>
<simpara>1 build</simpara>
</listitem>
<listitem>
<simpara>5 deployments, with 2 pod replicas in a <literal>sleep</literal> state, mounting 4 secrets, 4 config maps, and 1 downward API volume each</simpara>
</listitem>
<listitem>
<simpara>5 services, each one pointing to the TCP/8080 and TCP/8443 ports of one of the previous deployments</simpara>
</listitem>
<listitem>
<simpara>1 route pointing to the first of the previous services</simpara>
</listitem>
<listitem>
<simpara>10 secrets containing 2048 random string characters</simpara>
</listitem>
<listitem>
<simpara>10 config maps containing 2048 random string characters</simpara>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Number of worker nodes</entry>
<entry align="left" valign="top">Cluster-density (namespaces)</entry>
<entry align="left" valign="top">CPU cores</entry>
<entry align="left" valign="top">Memory (GB)</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>24</simpara></entry>
<entry align="left" valign="top"><simpara>500</simpara></entry>
<entry align="left" valign="top"><simpara>4</simpara></entry>
<entry align="left" valign="top"><simpara>16</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>120</simpara></entry>
<entry align="left" valign="top"><simpara>1000</simpara></entry>
<entry align="left" valign="top"><simpara>8</simpara></entry>
<entry align="left" valign="top"><simpara>32</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>252</simpara></entry>
<entry align="left" valign="top"><simpara>4000</simpara></entry>
<entry align="left" valign="top"><simpara>16, but 24 if using the OVN-Kubernetes network plug-in</simpara></entry>
<entry align="left" valign="top"><simpara>64, but 128 if using the OVN-Kubernetes network plug-in</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>501, but untested with the OVN-Kubernetes network plug-in</simpara></entry>
<entry align="left" valign="top"><simpara>4000</simpara></entry>
<entry align="left" valign="top"><simpara>16</simpara></entry>
<entry align="left" valign="top"><simpara>96</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>The data from the table above is based on an OpenShift Container Platform running on top of AWS, using r5.4xlarge instances as control-plane nodes and m5.2xlarge instances as worker nodes.</simpara>
<simpara>On a large and dense cluster with three control plane nodes, the CPU and memory usage will spike up when one of the nodes is stopped, rebooted, or fails. The failures can be due to unexpected issues with power, network, underlying infrastructure, or intentional cases where the cluster is restarted after shutting it down to save costs. The remaining two control plane nodes must handle the load in order to be highly available, which leads to increase in the resource usage. This is also expected during upgrades because the control plane nodes are cordoned, drained, and rebooted serially to apply the operating system updates, as well as the control plane Operators update. To avoid cascading failures, keep the overall CPU and memory resource usage on the control plane nodes to at most 60% of all available capacity to handle the resource usage spikes. Increase the CPU and memory on the control plane nodes accordingly to avoid potential downtime due to lack of resources.</simpara>
<important>
<simpara>The node sizing varies depending on the number of nodes and object counts in the cluster. It also depends on whether the objects are actively being created on the cluster. During object creation, the control plane is more active in terms of resource usage compared to when the objects are in the <literal>running</literal> phase.</simpara>
</important>
<simpara>Operator Lifecycle Manager (OLM ) runs on the control plane nodes and its memory footprint depends on the number of namespaces and user installed operators that OLM needs to manage on the cluster. Control plane nodes need to be sized accordingly to avoid OOM kills. Following data points are based on the results from cluster maximums testing.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Number of namespaces</entry>
<entry align="left" valign="top">OLM memory at idle state (GB)</entry>
<entry align="left" valign="top">OLM memory with 5 user operators installed (GB)</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>500</simpara></entry>
<entry align="left" valign="top"><simpara>0.823</simpara></entry>
<entry align="left" valign="top"><simpara>1.7</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1000</simpara></entry>
<entry align="left" valign="top"><simpara>1.2</simpara></entry>
<entry align="left" valign="top"><simpara>2.5</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1500</simpara></entry>
<entry align="left" valign="top"><simpara>1.7</simpara></entry>
<entry align="left" valign="top"><simpara>3.2</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>2000</simpara></entry>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara>4.4</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>3000</simpara></entry>
<entry align="left" valign="top"><simpara>2.7</simpara></entry>
<entry align="left" valign="top"><simpara>5.6</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>4000</simpara></entry>
<entry align="left" valign="top"><simpara>3.8</simpara></entry>
<entry align="left" valign="top"><simpara>7.6</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>5000</simpara></entry>
<entry align="left" valign="top"><simpara>4.2</simpara></entry>
<entry align="left" valign="top"><simpara>9.02</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>6000</simpara></entry>
<entry align="left" valign="top"><simpara>5.8</simpara></entry>
<entry align="left" valign="top"><simpara>11.3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>7000</simpara></entry>
<entry align="left" valign="top"><simpara>6.6</simpara></entry>
<entry align="left" valign="top"><simpara>12.9</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>8000</simpara></entry>
<entry align="left" valign="top"><simpara>6.9</simpara></entry>
<entry align="left" valign="top"><simpara>14.8</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>9000</simpara></entry>
<entry align="left" valign="top"><simpara>8</simpara></entry>
<entry align="left" valign="top"><simpara>17.7</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>10,000</simpara></entry>
<entry align="left" valign="top"><simpara>9.9</simpara></entry>
<entry align="left" valign="top"><simpara>21.6</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<important>
<simpara>You can modify the control plane node size in a running OpenShift Container Platform 4.14 cluster for the following configurations only:</simpara>
<itemizedlist>
<listitem>
<simpara>Clusters installed with a user-provisioned installation method.</simpara>
</listitem>
<listitem>
<simpara>AWS clusters installed with an installer-provisioned infrastructure installation method.</simpara>
</listitem>
<listitem>
<simpara>Clusters that use a control plane machine set to manage control plane machines.</simpara>
</listitem>
</itemizedlist>
<simpara>For all other configurations, you must estimate your total node count and use the suggested control plane node size during installation.</simpara>
</important>
<important>
<simpara>The recommendations are based on the data points captured on OpenShift Container Platform clusters with OpenShift SDN as the network plugin.</simpara>
</important>
<note>
<simpara>In OpenShift Container Platform 4.14, half of a CPU core (500 millicore) is now reserved by the system by default compared to OpenShift Container Platform 3.11 and previous versions. The sizes are determined taking that into consideration.</simpara>
</note>
</section>
<section xml:id="setting_up_cpu_manager_post-install-node-tasks">
<title>Setting up CPU Manager</title>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Optional: Label a node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc label node perf-node.example.com cpumanager=true</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>MachineConfigPool</literal> of the nodes where CPU Manager should be enabled. In this example, all workers have CPU Manager enabled:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc edit machineconfigpool worker</programlisting>
</listitem>
<listitem>
<simpara>Add a label to the worker machine config pool:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  creationTimestamp: 2020-xx-xxx
  generation: 3
  labels:
    custom-kubelet: cpumanager-enabled</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>KubeletConfig</literal>, <literal>cpumanager-kubeletconfig.yaml</literal>, custom resource (CR). Refer to the label created in the previous step to have the correct nodes updated with the new kubelet config. See the <literal>machineConfigPoolSelector</literal> section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static <co xml:id="CO111-1"/>
     cpuManagerReconcilePeriod: 5s <co xml:id="CO111-2"/></programlisting>
<calloutlist>
<callout arearefs="CO111-1">
<para>Specify a policy:</para>
<itemizedlist>
<listitem>
<simpara><literal>none</literal>. This policy explicitly enables the existing default CPU affinity scheme, providing no affinity beyond what the scheduler does automatically. This is the default policy.</simpara>
</listitem>
<listitem>
<simpara><literal>static</literal>. This policy allows containers in guaranteed pods with integer CPU requests. It also limits access to exclusive CPUs on the node. If <literal>static</literal>, you must use a lowercase <literal>s</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO111-2">
<para>Optional. Specify the CPU Manager reconcile frequency. The default is <literal>5s</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the dynamic kubelet config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc create -f cpumanager-kubeletconfig.yaml</programlisting>
<simpara>This adds the CPU Manager feature to the kubelet config and, if needed, the Machine Config Operator (MCO) reboots the node. To enable CPU Manager, a reboot is not needed.</simpara>
</listitem>
<listitem>
<simpara>Check for the merged kubelet config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc get machineconfig 99-worker-XXXXXX-XXXXX-XXXX-XXXXX-kubelet -o json | grep ownerReference -A7</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">       "ownerReferences": [
            {
                "apiVersion": "machineconfiguration.openshift.io/v1",
                "kind": "KubeletConfig",
                "name": "cpumanager-enabled",
                "uid": "7ed5616d-6b72-11e9-aae1-021e1ce18878"
            }
        ]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the worker for the updated <literal>kubelet.conf</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc debug node/perf-node.example.com
sh-4.2# cat /host/etc/kubernetes/kubelet.conf | grep cpuManager</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">cpuManagerPolicy: static        <co xml:id="CO112-1"/>
cpuManagerReconcilePeriod: 5s   <co xml:id="CO112-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO112-1">
<para><literal>cpuManagerPolicy</literal> is defined when you create the <literal>KubeletConfig</literal> CR.</para>
</callout>
<callout arearefs="CO112-2">
<para><literal>cpuManagerReconcilePeriod</literal> is defined when you create the <literal>KubeletConfig</literal> CR.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a pod that requests a core or multiple cores. Both limits and requests must have their CPU value set to a whole integer. That is the number of cores that will be dedicated to this pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat cpumanager-pod.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  generateName: cpumanager-
spec:
  containers:
  - name: cpumanager
    image: gcr.io/google_containers/pause:3.2
    resources:
      requests:
        cpu: 1
        memory: "1G"
      limits:
        cpu: 1
        memory: "1G"
  nodeSelector:
    cpumanager: "true"</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create the pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc create -f cpumanager-pod.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is scheduled to the node that you labeled:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc describe pod cpumanager</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:               cpumanager-6cqz7
Namespace:          default
Priority:           0
PriorityClassName:  &lt;none&gt;
Node:  perf-node.example.com/xxx.xx.xx.xxx
...
 Limits:
      cpu:     1
      memory:  1G
    Requests:
      cpu:        1
      memory:     1G
...
QoS Class:       Guaranteed
Node-Selectors:  cpumanager=true</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>cgroups</literal> are set up correctly. Get the process ID (PID) of the <literal>pause</literal> process:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># ├─init.scope
│ └─1 /usr/lib/systemd/systemd --switched-root --system --deserialize 17
└─kubepods.slice
  ├─kubepods-pod69c01f8e_6b74_11e9_ac0f_0a2b62178a22.slice
  │ ├─crio-b5437308f1a574c542bdf08563b865c0345c8f8c0b0a655612c.scope
  │ └─32706 /pause</programlisting>
<simpara>Pods of quality of service (QoS) tier <literal>Guaranteed</literal> are placed within the <literal>kubepods.slice</literal>. Pods of other QoS tiers end up in child <literal>cgroups</literal> of <literal>kubepods</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cd /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-pod69c01f8e_6b74_11e9_ac0f_0a2b62178a22.slice/crio-b5437308f1ad1a7db0574c542bdf08563b865c0345c86e9585f8c0b0a655612c.scope
# for i in `ls cpuset.cpus tasks` ; do echo -n "$i "; cat $i ; done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">cpuset.cpus 1
tasks 32706</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the allowed CPU list for the task:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># grep ^Cpus_allowed_list /proc/32706/status</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"> Cpus_allowed_list:    1</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that another pod (in this case, the pod in the <literal>burstable</literal> QoS tier) on the system cannot run on the core allocated for the <literal>Guaranteed</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># cat /sys/fs/cgroup/cpuset/kubepods.slice/kubepods-besteffort.slice/kubepods-besteffort-podc494a073_6b77_11e9_98c0_06bba5c387ea.slice/crio-c56982f57b75a2420947f0afc6cafe7534c5734efc34157525fa9abbf99e3849.scope/cpuset.cpus
0
# oc describe node perf-node.example.com</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
Capacity:
 attachable-volumes-aws-ebs:  39
 cpu:                         2
 ephemeral-storage:           124768236Ki
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      8162900Ki
 pods:                        250
Allocatable:
 attachable-volumes-aws-ebs:  39
 cpu:                         1500m
 ephemeral-storage:           124768236Ki
 hugepages-1Gi:               0
 hugepages-2Mi:               0
 memory:                      7548500Ki
 pods:                        250
-------                               ----                           ------------  ----------  ---------------  -------------  ---
  default                                 cpumanager-6cqz7               1 (66%)       1 (66%)     1G (12%)         1G (12%)       29m

Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource                    Requests          Limits
  --------                    --------          ------
  cpu                         1440m (96%)       1 (66%)</programlisting>
</para>
</formalpara>
<simpara>This VM has two CPU cores. The <literal>system-reserved</literal> setting reserves 500 millicores, meaning that half of one core is subtracted from the total capacity of the node to arrive at the <literal>Node Allocatable</literal> amount. You can see that <literal>Allocatable CPU</literal> is 1500 millicores. This means you can run one of the CPU Manager pods since each will take one whole core. A whole core is equivalent to 1000 millicores. If you try to schedule a second pod, the system will accept the pod, but it will never be scheduled:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    READY   STATUS    RESTARTS   AGE
cpumanager-6cqz7        1/1     Running   0          33m
cpumanager-7qc2t        0/1     Pending   0          11s</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-huge-pages">
<title>Huge pages</title>
<simpara>Understand and configure huge pages.</simpara>
<section xml:id="what-huge-pages-do_post-install-node-tasks">
<title>What huge pages do</title>
<simpara>Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi
of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs
have a built-in memory management unit that manages a list of these pages in
hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of
virtual-to-physical page mappings. If the virtual address passed in a hardware
instruction can be found in the TLB, the mapping can be determined quickly. If
not, a TLB miss occurs, and the system falls back to slower, software-based
address translation, resulting in performance issues. Since the size of the TLB
is fixed, the only way to reduce the chance of a TLB miss is to increase the
page size.</simpara>
<simpara>A huge page is a memory page that is larger than 4Ki. On x86_64 architectures,
there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other
architectures. To use huge pages, code must be written so that
applications are aware of them. Transparent Huge Pages (THP) attempt to automate
the management of huge pages without application knowledge, but they have
limitations. In particular, they are limited to 2Mi page sizes. THP can lead to
performance degradation on nodes with high memory utilization or fragmentation
due to defragmenting efforts of THP, which can lock memory pages. For this
reason, some applications may be designed to (or recommend) usage of
pre-allocated huge pages instead of THP.</simpara>
</section>
<section xml:id="how-huge-pages-are-consumed-by-apps_post-install-node-tasks">
<title>How huge pages are consumed by apps</title>
<simpara>Nodes must pre-allocate huge pages in order for the node to report its huge page
capacity. A node can only pre-allocate huge pages for a single size.</simpara>
<simpara>Huge pages can be consumed through container-level resource requirements using the
resource name <literal>hugepages-&lt;size&gt;</literal>, where size is the most compact binary
notation using integer values supported on a particular node. For example, if a
node supports 2048KiB page sizes, it exposes a schedulable resource
<literal>hugepages-2Mi</literal>. Unlike CPU or memory, huge pages do not support over-commitment.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  generateName: hugepages-volume-
spec:
  containers:
  - securityContext:
      privileged: true
    image: rhel7:latest
    command:
    - sleep
    - inf
    name: example
    volumeMounts:
    - mountPath: /dev/hugepages
      name: hugepage
    resources:
      limits:
        hugepages-2Mi: 100Mi <co xml:id="CO113-1"/>
        memory: "1Gi"
        cpu: "1"
  volumes:
  - name: hugepage
    emptyDir:
      medium: HugePages</programlisting>
<calloutlist>
<callout arearefs="CO113-1">
<para>Specify the amount of memory for <literal>hugepages</literal> as the exact amount to be
allocated. Do not specify this value as the amount of memory for <literal>hugepages</literal>
multiplied by the size of the page. For example, given a huge page size of 2MB,
if you want to use 100MB of huge-page-backed RAM for your application, then you
would allocate 50 huge pages. OpenShift Container Platform handles the math for you. As in
the above example, you can specify <literal>100MB</literal> directly.</para>
</callout>
</calloutlist>
<simpara><emphasis role="strong">Allocating huge pages of a specific size</emphasis></simpara>
<simpara>Some platforms support multiple huge page sizes. To allocate huge pages of a
specific size, precede the huge pages boot command parameters with a huge page
size selection parameter <literal>hugepagesz=&lt;size&gt;</literal>. The <literal>&lt;size&gt;</literal> value must be
specified in bytes with an optional scale suffix [<literal>kKmMgG</literal>]. The default huge
page size can be defined with the <literal>default_hugepagesz=&lt;size&gt;</literal> boot parameter.</simpara>
<simpara><emphasis role="strong">Huge page requirements</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>Huge page requests must equal the limits. This is the default if limits are
specified, but requests are not.</simpara>
</listitem>
<listitem>
<simpara>Huge pages are isolated at a pod scope. Container isolation is planned in a
future iteration.</simpara>
</listitem>
<listitem>
<simpara><literal>EmptyDir</literal> volumes backed by huge pages must not consume more huge page memory
than the pod request.</simpara>
</listitem>
<listitem>
<simpara>Applications that consume huge pages via <literal>shmget()</literal> with <literal>SHM_HUGETLB</literal> must run
with a supplemental group that matches <emphasis role="strong"><emphasis>proc/sys/vm/hugetlb_shm_group</emphasis></emphasis>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-huge-pages_post-install-node-tasks">
<title>Configuring huge pages at boot time</title>
<simpara>Nodes must pre-allocate huge pages used in an OpenShift Container Platform cluster. There are two ways of reserving huge pages: at boot time and at run time. Reserving at boot time increases the possibility of success because the memory has not yet been significantly fragmented. The Node Tuning Operator currently supports boot time allocation of huge pages on specific nodes.</simpara>
<formalpara>
<title>Procedure</title>
<para>To minimize node reboots, the order of the steps below needs to be followed:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Label all nodes that need the same huge pages setting by a label.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node_using_hugepages&gt; node-role.kubernetes.io/worker-hp=</programlisting>
</listitem>
<listitem>
<simpara>Create a file with the following content and name it <literal>hugepages-tuned-boottime.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: hugepages <co xml:id="CO114-1"/>
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile: <co xml:id="CO114-2"/>
  - data: |
      [main]
      summary=Boot time configuration for hugepages
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_hugepages=hugepagesz=2M hugepages=50 <co xml:id="CO114-3"/>
    name: openshift-node-hugepages

  recommend:
  - machineConfigLabels: <co xml:id="CO114-4"/>
      machineconfiguration.openshift.io/role: "worker-hp"
    priority: 30
    profile: openshift-node-hugepages</programlisting>
<calloutlist>
<callout arearefs="CO114-1">
<para>Set the <literal>name</literal> of the Tuned resource to <literal>hugepages</literal>.</para>
</callout>
<callout arearefs="CO114-2">
<para>Set the <literal>profile</literal> section to allocate huge pages.</para>
</callout>
<callout arearefs="CO114-3">
<para>Note the order of parameters is important as some platforms support huge pages of various sizes.</para>
</callout>
<callout arearefs="CO114-4">
<para>Enable machine config pool based matching.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the Tuned <literal>hugepages</literal> object</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f hugepages-tuned-boottime.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a file with the following content and name it <literal>hugepages-mcp.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-hp
  labels:
    worker-hp: ""
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker-hp]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-hp: ""</programlisting>
</listitem>
<listitem>
<simpara>Create the machine config pool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f hugepages-mcp.yaml</programlisting>
</listitem>
</orderedlist>
<simpara>Given enough non-fragmented memory, all the nodes in the <literal>worker-hp</literal> machine config pool should now have 50 2Mi huge pages allocated.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get node &lt;node_using_hugepages&gt; -o jsonpath="{.status.allocatable.hugepages-2Mi}"
100Mi</programlisting>
<note>
<simpara>The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.</simpara>
</note>
</section>
</section>
<section xml:id="nodes-pods-plugins-about_post-install-node-tasks">
<title>Understanding device plugins</title>
<simpara>The device plugin provides a consistent and portable solution to consume hardware
devices across clusters. The device plugin provides support for these devices
through an extension mechanism, which makes these devices available to
Containers, provides health checks of these devices, and securely shares them.</simpara>
<important>
<simpara>OpenShift Container Platform supports the device plugin API, but the device plugin
Containers are supported by individual vendors.</simpara>
</important>
<simpara>A device plugin is a gRPC service running on the nodes (external to
the <literal>kubelet</literal>) that is responsible for managing specific
hardware resources. Any device plugin must support following remote procedure
calls (RPCs):</simpara>
<programlisting language="golang" linenumbering="unnumbered">service DevicePlugin {
      // GetDevicePluginOptions returns options to be communicated with Device
      // Manager
      rpc GetDevicePluginOptions(Empty) returns (DevicePluginOptions) {}

      // ListAndWatch returns a stream of List of Devices
      // Whenever a Device state change or a Device disappears, ListAndWatch
      // returns the new list
      rpc ListAndWatch(Empty) returns (stream ListAndWatchResponse) {}

      // Allocate is called during container creation so that the Device
      // Plug-in can run device specific operations and instruct Kubelet
      // of the steps to make the Device available in the container
      rpc Allocate(AllocateRequest) returns (AllocateResponse) {}

      // PreStartcontainer is called, if indicated by Device Plug-in during
      // registration phase, before each container start. Device plug-in
      // can run device specific operations such as resetting the device
      // before making devices available to the container
      rpc PreStartcontainer(PreStartcontainerRequest) returns (PreStartcontainerResponse) {}
}</programlisting>
<bridgehead xml:id="_example_device_plugins" renderas="sect4">Example device plugins</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://github.com/GoogleCloudPlatform/Container-engine-accelerators/tree/master/cmd/nvidia_gpu">Nvidia GPU device plugin for COS-based operating system</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/NVIDIA/k8s-device-plugin">Nvidia official GPU device plugin</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/vikaschoudhary16/sfc-device-plugin">Solarflare device plugin</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/kubevirt/kubernetes-device-plugins">KubeVirt device plugins: vfio and kvm</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://github.com/ibm-s390-cloud/k8s-cex-dev-plugin">Kubernetes device plugin for IBM&#174; Crypto Express (CEX) cards</link></simpara>
</listitem>
</itemizedlist>
<note>
<simpara>For easy device plugin reference implementation, there is a stub device plugin
in the Device Manager code:
<emphasis role="strong"><emphasis>vendor/k8s.io/kubernetes/pkg/kubelet/cm/deviceplugin/device_plugin_stub.go</emphasis></emphasis>.</simpara>
</note>
<section xml:id="methods-for-deploying-a-device-plugin_post-install-node-tasks">
<title>Methods for deploying a device plugin</title>
<itemizedlist>
<listitem>
<simpara>Daemon sets are the recommended approach for device plugin deployments.</simpara>
</listitem>
<listitem>
<simpara>Upon start, the device plugin will try to create a UNIX domain socket at
<emphasis role="strong"><emphasis>/var/lib/kubelet/device-plugin/</emphasis></emphasis> on the node to serve RPCs from Device Manager.</simpara>
</listitem>
<listitem>
<simpara>Since device plugins must manage hardware resources, access to the host
file system, as well as socket creation, they must be run in a privileged
security context.</simpara>
</listitem>
<listitem>
<simpara>More specific details regarding deployment steps can be found with each device
plugin implementation.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nodes-pods-plugins-device-mgr_post-install-node-tasks">
<title>Understanding the Device Manager</title>
<simpara>Device Manager provides a mechanism for advertising specialized node hardware resources
with the help of plugins known as device plugins.</simpara>
<simpara>You can advertise specialized hardware without requiring any upstream code changes.</simpara>
<important>
<simpara>OpenShift Container Platform supports the device plugin API, but the device plugin
Containers are supported by individual vendors.</simpara>
</important>
<simpara>Device Manager advertises devices as <emphasis role="strong">Extended Resources</emphasis>. User pods can consume
devices, advertised by Device Manager, using the same <emphasis role="strong">Limit/Request</emphasis> mechanism,
which is used for requesting any other <emphasis role="strong">Extended Resource</emphasis>.</simpara>
<simpara>Upon start, the device plugin registers itself with Device Manager invoking <literal>Register</literal> on the
<emphasis role="strong"><emphasis>/var/lib/kubelet/device-plugins/kubelet.sock</emphasis></emphasis> and starts a gRPC service at
<emphasis role="strong"><emphasis>/var/lib/kubelet/device-plugins/&lt;plugin&gt;.sock</emphasis></emphasis> for serving Device Manager
requests.</simpara>
<simpara>Device Manager, while processing a new registration request, invokes
<literal>ListAndWatch</literal> remote procedure call (RPC) at the device plugin service. In
response, Device Manager gets a list of <emphasis role="strong">Device</emphasis> objects from the plugin over a
gRPC stream. Device Manager will keep watching on the stream for new updates
from the plugin. On the plugin side, the plugin will also keep the stream
open and whenever there is a change in the state of any of the devices, a new
device list is sent to the Device Manager over the same streaming connection.</simpara>
<simpara>While handling a new pod admission request, Kubelet passes requested <literal>Extended
Resources</literal> to the Device Manager for device allocation. Device Manager checks in
its database to verify if a corresponding plugin exists or not. If the plugin exists
and there are free allocatable devices as well as per local cache, <literal>Allocate</literal>
RPC is invoked at that particular device plugin.</simpara>
<simpara>Additionally, device plugins can also perform several other device-specific
operations, such as driver installation, device initialization, and device
resets. These functionalities vary from implementation to implementation.</simpara>
</section>
<section xml:id="nodes-pods-plugins-install_post-install-node-tasks">
<title>Enabling Device Manager</title>
<simpara>Enable Device Manager to implement a device plugin to advertise specialized
hardware without any upstream code changes.</simpara>
<simpara>Device Manager provides a mechanism for advertising specialized node hardware resources
with the help of plugins known as device plugins.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Obtain the label associated with the static <literal>MachineConfigPool</literal> CRD for the type of node you want to configure by entering the following command.
Perform one of the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>View the machine config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc describe machineconfig &lt;name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc describe machineconfig 00-worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         00-worker
Namespace:
Labels:       machineconfiguration.openshift.io/role=worker <co xml:id="CO115-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO115-1">
<para>Label required for the Device Manager.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a custom resource (CR) for your configuration change.</simpara>
<formalpara>
<title>Sample configuration for a Device Manager CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: devicemgr <co xml:id="CO116-1"/>
spec:
  machineConfigPoolSelector:
    matchLabels:
       machineconfiguration.openshift.io: devicemgr <co xml:id="CO116-2"/>
  kubeletConfig:
    feature-gates:
      - DevicePlugins=true <co xml:id="CO116-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO116-1">
<para>Assign a name to CR.</para>
</callout>
<callout arearefs="CO116-2">
<para>Enter the label from the Machine Config Pool.</para>
</callout>
<callout arearefs="CO116-3">
<para>Set <literal>DevicePlugins</literal> to 'true`.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the Device Manager:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f devicemgr.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kubeletconfig.machineconfiguration.openshift.io/devicemgr created</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Ensure that Device Manager was actually enabled by confirming that
<emphasis role="strong"><emphasis>/var/lib/kubelet/device-plugins/kubelet.sock</emphasis></emphasis> is created on the node. This is
the UNIX domain socket on which the Device Manager gRPC server listens for new
plugin registrations. This sock file is created when the Kubelet is started
only if Device Manager is enabled.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-taints-tolerations">
<title>Taints and tolerations</title>
<simpara>Understand and work with taints and tolerations.</simpara>
<section xml:id="nodes-scheduler-taints-tolerations-about_post-install-node-tasks">
<title>Understanding taints and tolerations</title>
<simpara>A <emphasis>taint</emphasis> allows a node to refuse a pod to be scheduled unless that pod has a matching <emphasis>toleration</emphasis>.</simpara>
<simpara>You apply taints to a node through the <literal>Node</literal> specification (<literal>NodeSpec</literal>) and apply tolerations to a pod through the <literal>Pod</literal> specification (<literal>PodSpec</literal>). When you apply a taint a node, the scheduler cannot place a pod on that node unless the pod can tolerate the taint.</simpara>
<formalpara>
<title>Example taint in a node specification</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Node
metadata:
  name: my-node
#...
spec:
  taints:
  - effect: NoExecute
    key: key1
    value: value1
#...</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example toleration in a <literal>Pod</literal> spec</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Equal"
    value: "value1"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</programlisting>
</para>
</formalpara>
<simpara>Taints and tolerations consist of a key, value, and effect.</simpara>
<table xml:id="taint-components-table_post-install-node-tasks" frame="all" rowsep="1" colsep="1">
<title>Taint and toleration components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>key</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>key</literal> is any string, up to 253 characters. The key must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>value</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>value</literal> is any string, up to 63 characters. The value must begin with a letter or number, and may contain letters, numbers, hyphens, dots, and underscores.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>effect</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The effect is one of the following:</simpara>
<informaltable frame="none" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>NoSchedule</literal> <superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>New pods that do not match the taint are not scheduled onto that node.</simpara>
</listitem>
<listitem>
<simpara>Existing pods on the node remain.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>PreferNoSchedule</literal></simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>New pods that do not match the taint might be scheduled onto that node, but the scheduler tries not to.</simpara>
</listitem>
<listitem>
<simpara>Existing pods on the node remain.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>NoExecute</literal></simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>New pods that do not match the taint cannot be scheduled onto that node.</simpara>
</listitem>
<listitem>
<simpara>Existing pods on the node that do not have a matching toleration  are removed.</simpara>
</listitem>
</itemizedlist></entry>
</row>
</tbody>
</tgroup>
</informaltable></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>operator</literal></simpara></entry>
<entry align="left" valign="top"><informaltable frame="none" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>Equal</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>key</literal>/<literal>value</literal>/<literal>effect</literal> parameters must match. This is the default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Exists</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>key</literal>/<literal>effect</literal> parameters must match. You must leave a blank <literal>value</literal> parameter, which matches any.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>If you add a <literal>NoSchedule</literal> taint to a control plane node, the node must have the <literal>node-role.kubernetes.io/master=:NoSchedule</literal> taint, which is added by default.</simpara>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Node
metadata:
  annotations:
    machine.openshift.io/machine: openshift-machine-api/ci-ln-62s7gtb-f76d1-v8jxv-master-0
    machineconfiguration.openshift.io/currentConfig: rendered-master-cdc1ab7da414629332cc4c3926e6e59c
  name: my-node
#...
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
#...</programlisting>
</listitem>
</orderedlist>
</para>
<simpara>A toleration matches a taint:</simpara>
<itemizedlist>
<listitem>
<simpara>If the <literal>operator</literal> parameter is set to <literal>Equal</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>the <literal>key</literal> parameters are the same;</simpara>
</listitem>
<listitem>
<simpara>the <literal>value</literal> parameters are the same;</simpara>
</listitem>
<listitem>
<simpara>the <literal>effect</literal> parameters are the same.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If the <literal>operator</literal> parameter is set to <literal>Exists</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>the <literal>key</literal> parameters are the same;</simpara>
</listitem>
<listitem>
<simpara>the <literal>effect</literal> parameters are the same.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>The following taints are built into OpenShift Container Platform:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>node.kubernetes.io/not-ready</literal>: The node is not ready. This corresponds to the node condition <literal>Ready=False</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>node.kubernetes.io/unreachable</literal>: The node is unreachable from the node controller. This corresponds to the node condition <literal>Ready=Unknown</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>node.kubernetes.io/memory-pressure</literal>: The node has memory pressure issues. This corresponds to the node condition <literal>MemoryPressure=True</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>node.kubernetes.io/disk-pressure</literal>: The node has disk pressure issues. This corresponds to the node condition <literal>DiskPressure=True</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>node.kubernetes.io/network-unavailable</literal>: The node network is unavailable.</simpara>
</listitem>
<listitem>
<simpara><literal>node.kubernetes.io/unschedulable</literal>: The node is unschedulable.</simpara>
</listitem>
<listitem>
<simpara><literal>node.cloudprovider.kubernetes.io/uninitialized</literal>: When the node controller is started with an external cloud provider, this taint is set on a node to mark it as unusable. After a controller from the cloud-controller-manager initializes this node, the kubelet removes this taint.</simpara>
</listitem>
<listitem>
<simpara><literal>node.kubernetes.io/pid-pressure</literal>: The node has pid pressure. This corresponds to the node condition <literal>PIDPressure=True</literal>.</simpara>
<important>
<simpara>OpenShift Container Platform does not set a default pid.available <literal>evictionHard</literal>.</simpara>
</important>
</listitem>
</itemizedlist>
</section>
<section xml:id="nodes-scheduler-taints-tolerations-adding_post-install-node-tasks">
<title>Adding taints and tolerations</title>
<simpara>You add tolerations to pods and taints to nodes to allow the node to control which pods should or should not be scheduled on them. For existing pods and nodes, you should add the toleration to the pod first, then add the taint to the node to avoid pods being removed from the node before you can add the toleration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add a toleration to a pod by editing the <literal>Pod</literal> spec to include a <literal>tolerations</literal> stanza:</simpara>
<formalpara>
<title>Sample pod configuration file with an Equal operator</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1" <co xml:id="CO117-1"/>
    value: "value1"
    operator: "Equal"
    effect: "NoExecute"
    tolerationSeconds: 3600 <co xml:id="CO117-2"/>
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO117-1">
<para>The toleration parameters, as described in the <emphasis role="strong">Taint and toleration components</emphasis> table.</para>
</callout>
<callout arearefs="CO117-2">
<para>The <literal>tolerationSeconds</literal> parameter specifies how long a pod can remain bound to a node before being evicted.</para>
</callout>
</calloutlist>
<simpara>For example:</simpara>
<formalpara>
<title>Sample pod configuration file with an Exists operator</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
   tolerations:
    - key: "key1"
      operator: "Exists" <co xml:id="CO118-1"/>
      effect: "NoExecute"
      tolerationSeconds: 3600
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO118-1">
<para>The <literal>Exists</literal> operator does not take a <literal>value</literal>.</para>
</callout>
</calloutlist>
<simpara>This example places a taint on <literal>node1</literal> that has key <literal>key1</literal>, value <literal>value1</literal>, and taint effect <literal>NoExecute</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add a taint to a node by using the following command with the parameters described in the <emphasis role="strong">Taint and toleration components</emphasis> table:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes &lt;node_name&gt; &lt;key&gt;=&lt;value&gt;:&lt;effect&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes node1 key1=value1:NoExecute</programlisting>
<simpara>This command places a taint on <literal>node1</literal> that has key <literal>key1</literal>, value <literal>value1</literal>, and effect <literal>NoExecute</literal>.</simpara>
<note>
<simpara>If you add a <literal>NoSchedule</literal> taint to a control plane node, the node must have the <literal>node-role.kubernetes.io/master=:NoSchedule</literal> taint, which is added by default.</simpara>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Node
metadata:
  annotations:
    machine.openshift.io/machine: openshift-machine-api/ci-ln-62s7gtb-f76d1-v8jxv-master-0
    machineconfiguration.openshift.io/currentConfig: rendered-master-cdc1ab7da414629332cc4c3926e6e59c
  name: my-node
#...
spec:
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
#...</programlisting>
</note>
<simpara>The tolerations on the pod match the taint on the node. A pod with either toleration can be scheduled onto <literal>node1</literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-scheduler-taints-tolerations-adding-machineset_post-install-node-tasks">
<title>Adding taints and tolerations using a compute machine set</title>
<simpara>You can add taints to nodes using a compute machine set. All nodes associated with the <literal>MachineSet</literal> object are updated with the taint. Tolerations respond to taints added by a compute machine set in the same manner as taints added directly to the nodes.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add a toleration to a pod by editing the <literal>Pod</literal> spec to include a <literal>tolerations</literal> stanza:</simpara>
<formalpara>
<title>Sample pod configuration file with <literal>Equal</literal> operator</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1" <co xml:id="CO119-1"/>
    value: "value1"
    operator: "Equal"
    effect: "NoExecute"
    tolerationSeconds: 3600 <co xml:id="CO119-2"/>
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO119-1">
<para>The toleration parameters, as described in the <emphasis role="strong">Taint and toleration components</emphasis> table.</para>
</callout>
<callout arearefs="CO119-2">
<para>The <literal>tolerationSeconds</literal> parameter specifies how long a pod is bound to a node before being evicted.</para>
</callout>
</calloutlist>
<simpara>For example:</simpara>
<formalpara>
<title>Sample pod configuration file with <literal>Exists</literal> operator</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key1"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Add the taint to the <literal>MachineSet</literal> object:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>MachineSet</literal> YAML for the nodes you want to taint or you can create a new <literal>MachineSet</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset&gt;</programlisting>
</listitem>
<listitem>
<simpara>Add the taint to the <literal>spec.template.spec</literal> section:</simpara>
<formalpara>
<title>Example taint in a compute machine set specification</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: my-machineset
#...
spec:
#...
  template:
#...
    spec:
      taints:
      - effect: NoExecute
        key: key1
        value: value1
#...</programlisting>
</para>
</formalpara>
<simpara>This example places a taint that has the key <literal>key1</literal>, value <literal>value1</literal>, and taint effect <literal>NoExecute</literal> on the nodes.</simpara>
</listitem>
<listitem>
<simpara>Scale down the compute machine set to 0:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=0 machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to scale the compute machine set:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: &lt;machineset&gt;
  namespace: openshift-machine-api
spec:
  replicas: 0</programlisting>
</tip>
<simpara>Wait for the machines to be removed.</simpara>
</listitem>
<listitem>
<simpara>Scale up the compute machine set as needed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>Or:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>Wait for the machines to start. The taint is added to the nodes associated with the <literal>MachineSet</literal> object.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-scheduler-taints-tolerations-bindings_post-install-node-tasks">
<title>Binding a user to a node using taints and tolerations</title>
<simpara>If you want to dedicate a set of nodes for exclusive use by a particular set of users, add a toleration to their pods. Then, add a corresponding taint to those nodes.  The pods with the tolerations are allowed to use the tainted nodes or any other nodes in the cluster.</simpara>
<simpara>If you want ensure the pods are scheduled to only those tainted nodes, also add a label to the same set of nodes and add a node affinity to the pods so that the pods can only be scheduled onto nodes with that label.</simpara>
<formalpara>
<title>Procedure</title>
<para>To configure a node so that users can use only that node:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add a corresponding taint to those nodes:</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes node1 dedicated=groupName:NoSchedule</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add the taint:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Node
apiVersion: v1
metadata:
  name: my-node
#...
spec:
  taints:
    - key: dedicated
      value: groupName
      effect: NoSchedule
#...</programlisting>
</tip>
</listitem>
<listitem>
<simpara>Add a toleration to the pods by writing a custom admission controller.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-scheduler-taints-tolerations-special_post-install-node-tasks">
<title>Controlling nodes with special hardware using taints and tolerations</title>
<simpara>In a cluster where a small subset of nodes have specialized hardware, you can use taints and tolerations to keep pods that do not need the specialized hardware off of those nodes, leaving the nodes for pods that do need the specialized hardware. You can also require pods that need specialized hardware to use specific nodes.</simpara>
<simpara>You can achieve this by adding a toleration to pods that need the special hardware and tainting the nodes that have the specialized hardware.</simpara>
<formalpara>
<title>Procedure</title>
<para>To ensure nodes with specialized hardware are reserved for specific pods:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add a toleration to pods that need the special hardware.</simpara>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
    - key: "disktype"
      value: "ssd"
      operator: "Equal"
      effect: "NoSchedule"
      tolerationSeconds: 3600
#...</programlisting>
</listitem>
<listitem>
<simpara>Taint the nodes that have the specialized hardware using one of the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:NoSchedule</programlisting>
<simpara>Or:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes &lt;node-name&gt; disktype=ssd:PreferNoSchedule</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add the taint:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Node
apiVersion: v1
metadata:
  name: my_node
#...
spec:
  taints:
    - key: disktype
      value: ssd
      effect: PreferNoSchedule
#...</programlisting>
</tip>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-scheduler-taints-tolerations-removing_post-install-node-tasks">
<title>Removing taints and tolerations</title>
<simpara>You can remove taints from nodes and tolerations from pods as needed. You should add the toleration to the pod first, then add the taint to the node to avoid pods being removed from the node before you can add the toleration.</simpara>
<formalpara>
<title>Procedure</title>
<para>To remove taints and tolerations:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To remove a taint from a node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes &lt;node-name&gt; &lt;key&gt;-</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm taint nodes ip-10-0-132-248.ec2.internal key1-</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">node/ip-10-0-132-248.ec2.internal untainted</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To remove a toleration from a pod, edit the <literal>Pod</literal> spec to remove the toleration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: my-pod
#...
spec:
  tolerations:
  - key: "key2"
    operator: "Exists"
    effect: "NoExecute"
    tolerationSeconds: 3600
#...</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-topology-manager">
<title>Topology Manager</title>
<simpara>Understand and work with Topology Manager.</simpara>
<section xml:id="topology_manager_policies_post-install-node-tasks">
<title>Topology Manager policies</title>
<simpara>Topology Manager aligns <literal>Pod</literal> resources of all Quality of Service (QoS) classes by collecting topology hints from Hint Providers, such as CPU Manager and Device Manager, and using the collected hints to align the <literal>Pod</literal> resources.</simpara>
<simpara>Topology Manager supports four allocation policies, which you assign in the <literal>KubeletConfig</literal> custom resource (CR) named <literal>cpumanager-enabled</literal>:</simpara>
<variablelist>
<varlistentry>
<term><literal>none</literal> policy</term>
<listitem>
<simpara>This is the default policy and does not perform any topology alignment.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>best-effort</literal> policy</term>
<listitem>
<simpara>For each container in a pod with the <literal>best-effort</literal> topology management policy, kubelet calls each Hint Provider to discover their resource
availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not preferred, Topology Manager stores this and admits the pod to the node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>restricted</literal> policy</term>
<listitem>
<simpara>For each container in a pod with the <literal>restricted</literal> topology management policy, kubelet calls each Hint Provider to discover their resource
availability. Using this information, the Topology Manager stores the preferred NUMA Node affinity for that container. If the affinity is not
preferred, Topology Manager rejects this pod from the node, resulting in a pod in a <literal>Terminated</literal> state with a pod admission failure.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>single-numa-node</literal> policy</term>
<listitem>
<simpara>For each container in a pod with the <literal>single-numa-node</literal> topology management policy, kubelet calls each Hint Provider to discover their resource availability. Using this information, the Topology Manager determines if a single NUMA Node affinity is possible. If it is, the pod is admitted to the node. If a single NUMA Node affinity is not possible, the Topology Manager rejects the pod from the node. This results in a pod in a Terminated state with a pod admission failure.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="setting_up_topology_manager_post-install-node-tasks">
<title>Setting up Topology Manager</title>
<simpara>To use Topology Manager, you must configure an allocation policy in the <literal>KubeletConfig</literal> custom resource (CR) named <literal>cpumanager-enabled</literal>. This file might exist if you have set up CPU Manager. If the file does not exist, you can create the file.</simpara>
<itemizedlist>
<title>Prequisites</title>
<listitem>
<simpara>Configure the CPU Manager policy to be <literal>static</literal>.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To activate Topololgy Manager:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure the Topology Manager allocation policy in the custom resource.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit KubeletConfig cpumanager-enabled</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: cpumanager-enabled
spec:
  machineConfigPoolSelector:
    matchLabels:
      custom-kubelet: cpumanager-enabled
  kubeletConfig:
     cpuManagerPolicy: static <co xml:id="CO120-1"/>
     cpuManagerReconcilePeriod: 5s
     topologyManagerPolicy: single-numa-node <co xml:id="CO120-2"/></programlisting>
<calloutlist>
<callout arearefs="CO120-1">
<para>This parameter must be <literal>static</literal> with a lowercase <literal>s</literal>.</para>
</callout>
<callout arearefs="CO120-2">
<para>Specify your selected Topology Manager allocation policy. Here, the policy is <literal>single-numa-node</literal>.
Acceptable values are: <literal>default</literal>, <literal>best-effort</literal>, <literal>restricted</literal>, <literal>single-numa-node</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="pod-interactions-with-topology-manager_post-install-node-tasks">
<title>Pod interactions with Topology Manager policies</title>
<simpara>The example <literal>Pod</literal> specs below help illustrate pod interactions with Topology Manager.</simpara>
<simpara>The following pod runs in the <literal>BestEffort</literal> QoS class because no resource requests or limits are specified.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx</programlisting>
<simpara>The next pod runs in the <literal>Burstable</literal> QoS class because requests are less than limits.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
      requests:
        memory: "100Mi"</programlisting>
<simpara>If the selected policy is anything other than <literal>none</literal>, Topology Manager would not consider either of these <literal>Pod</literal> specifications.</simpara>
<simpara>The last example pod below runs in the Guaranteed QoS class because requests are equal to limits.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  containers:
  - name: nginx
    image: nginx
    resources:
      limits:
        memory: "200Mi"
        cpu: "2"
        example.com/device: "1"
      requests:
        memory: "200Mi"
        cpu: "2"
        example.com/device: "1"</programlisting>
<simpara>Topology Manager would consider this pod. The Topology Manager would consult the hint providers, which are CPU Manager and Device Manager, to get topology hints for the pod.</simpara>
<simpara>Topology Manager will use this information to store the best topology for this container. In the case of this pod, CPU Manager and Device Manager will use this stored information at the resource allocation stage.</simpara>
</section>
</section>
<section xml:id="nodes-cluster-overcommit-resource-requests_post-install-node-tasks">
<title>Resource requests and overcommitment</title>
<simpara>For each compute resource, a container may specify a resource request and limit.
Scheduling decisions are made based on the request to ensure that a node has
enough capacity available to meet the requested value. If a container specifies
limits, but omits requests, the requests are defaulted to the limits. A
container is not able to exceed the specified limit on the node.</simpara>
<simpara>The enforcement of limits is dependent upon the compute resource type. If a
container makes no request or limit, the container is scheduled to a node with
no resource guarantees. In practice, the container is able to consume as much of
the specified resource as is available with the lowest local priority. In low
resource situations, containers that specify no resource requests are given the
lowest quality of service.</simpara>
<simpara>Scheduling is based on resources requested, while quota and hard limits refer
to resource limits, which can be set higher than requested resources. The
difference between request and limit determines the level of overcommit;
for instance, if a container is given a memory request of 1Gi and a memory limit
of 2Gi, it is scheduled based on the 1Gi request being available on the node,
but could use up to 2Gi; so it is 200% overcommitted.</simpara>
</section>
<section xml:id="nodes-cluster-resource-override_post-install-node-tasks">
<title>Cluster-level overcommit using the Cluster Resource Override Operator</title>
<simpara>The Cluster Resource Override Operator is an admission webhook that allows you to control the level of overcommit and manage
container density across all the nodes in your cluster. The Operator controls how nodes in specific projects can exceed defined memory and CPU limits.</simpara>
<simpara>You must install the Cluster Resource Override Operator using the OpenShift Container Platform console or CLI as shown in the following sections.
During the installation, you create a <literal>ClusterResourceOverride</literal> custom resource (CR), where you set the level of overcommit, as shown in the
following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster <co xml:id="CO121-1"/>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <co xml:id="CO121-2"/>
      cpuRequestToLimitPercent: 25 <co xml:id="CO121-3"/>
      limitCPUToMemoryPercent: 200 <co xml:id="CO121-4"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO121-1">
<para>The name must be <literal>cluster</literal>.</para>
</callout>
<callout arearefs="CO121-2">
<para>Optional. If a container memory limit has been specified or defaulted, the memory request is overridden to this percentage of the limit, between 1-100. The default is 50.</para>
</callout>
<callout arearefs="CO121-3">
<para>Optional. If a container CPU limit has been specified or defaulted, the CPU request is overridden to this percentage of the limit, between 1-100. The default is 25.</para>
</callout>
<callout arearefs="CO121-4">
<para>Optional. If a container memory limit has been specified or defaulted, the CPU limit is overridden to a percentage of the memory limit, if specified. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request (if configured). The default is 200.</para>
</callout>
</calloutlist>
<note>
<simpara>The Cluster Resource Override Operator overrides have no effect if limits have not
been set on containers. Create a <literal>LimitRange</literal> object with default limits per individual project
or configure limits in <literal>Pod</literal> specs for the overrides to apply.</simpara>
</note>
<simpara>When configured, overrides can be enabled per-project by applying the following
label to the Namespace object for each project:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:

# ...

  labels:
    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: "true"

# ...</programlisting>
<simpara>The Operator watches for the <literal>ClusterResourceOverride</literal> CR and ensures that the <literal>ClusterResourceOverride</literal> admission webhook is installed into the same namespace as the operator.</simpara>
<section xml:id="nodes-cluster-resource-override-deploy-console_post-install-node-tasks">
<title>Installing the Cluster Resource Override Operator using the web console</title>
<simpara>You can use the OpenShift Container Platform web console to install the Cluster Resource Override Operator to help control overcommit in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The Cluster Resource Override Operator has no effect if limits have not
been set on containers. You must specify default limits for a project using a <literal>LimitRange</literal> object or configure limits in <literal>Pod</literal> specs for the overrides to apply.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To install the Cluster Resource Override Operator using the OpenShift Container Platform web console:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>In the OpenShift Container Platform web console, navigate to <emphasis role="strong">Home</emphasis> &#8594; <emphasis role="strong">Projects</emphasis></simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Create Project</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Specify <literal>clusterresourceoverride-operator</literal> as the name of the project.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Choose  <emphasis role="strong">ClusterResourceOverride Operator</emphasis> from the list of available Operators and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, make sure <emphasis role="strong">A specific Namespace on the cluster</emphasis> is selected for <emphasis role="strong">Installation Mode</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Make sure <emphasis role="strong">clusterresourceoverride-operator</emphasis> is selected for <emphasis role="strong">Installed Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select an <emphasis role="strong">Update Channel</emphasis> and <emphasis role="strong">Approval Strategy</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Installed Operators</emphasis> page, click <emphasis role="strong">ClusterResourceOverride</emphasis>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>On the <emphasis role="strong">ClusterResourceOverride Operator</emphasis> details page, click <emphasis role="strong">Create ClusterResourceOverride</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Create ClusterResourceOverride</emphasis> page, click <emphasis role="strong">YAML view</emphasis> and edit the YAML template to set the overcommit values as needed:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  name: cluster <co xml:id="CO122-1"/>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <co xml:id="CO122-2"/>
      cpuRequestToLimitPercent: 25 <co xml:id="CO122-3"/>
      limitCPUToMemoryPercent: 200 <co xml:id="CO122-4"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO122-1">
<para>The name must be <literal>cluster</literal>.</para>
</callout>
<callout arearefs="CO122-2">
<para>Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.</para>
</callout>
<callout arearefs="CO122-3">
<para>Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.</para>
</callout>
<callout arearefs="CO122-4">
<para>Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Check the current state of the admission webhook by checking the status of the cluster custom resource:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>On the <emphasis role="strong">ClusterResourceOverride Operator</emphasis> page, click <emphasis role="strong">cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">ClusterResourceOverride Details</emphasis> page, click <emphasis role="strong">YAML</emphasis>. The <literal>mutatingWebhookConfigurationRef</literal> section appears when the webhook is called.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"operator.autoscaling.openshift.io/v1","kind":"ClusterResourceOverride","metadata":{"annotations":{},"name":"cluster"},"spec":{"podResourceOverride":{"spec":{"cpuRequestToLimitPercent":25,"limitCPUToMemoryPercent":200,"memoryRequestToLimitPercent":50}}}}
  creationTimestamp: "2019-12-18T22:35:02Z"
  generation: 1
  name: cluster
  resourceVersion: "127622"
  selfLink: /apis/operator.autoscaling.openshift.io/v1/clusterresourceoverrides/cluster
  uid: 978fc959-1717-4bd1-97d0-ae00ee111e8d
spec:
  podResourceOverride:
    spec:
      cpuRequestToLimitPercent: 25
      limitCPUToMemoryPercent: 200
      memoryRequestToLimitPercent: 50
status:

# ...

    mutatingWebhookConfigurationRef: <co xml:id="CO123-1"/>
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: clusterresourceoverrides.admission.autoscaling.openshift.io
      resourceVersion: "127621"
      uid: 98b3b8ae-d5ce-462b-8ab5-a729ea8f38f3

# ...</programlisting>
<calloutlist>
<callout arearefs="CO123-1">
<para>Reference to the <literal>ClusterResourceOverride</literal> admission webhook.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-cluster-resource-override-deploy-cli_post-install-node-tasks">
<title>Installing the Cluster Resource Override Operator using the CLI</title>
<simpara>You can use the OpenShift Container Platform CLI to install the Cluster Resource Override Operator to help control overcommit in your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The Cluster Resource Override Operator has no effect if limits have not been set on containers. You must specify default limits for a project using a <literal>LimitRange</literal> object or configure limits in <literal>Pod</literal> specs for the overrides to apply.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To install the Cluster Resource Override Operator using the CLI:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a namespace for the Cluster Resource Override Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>Namespace</literal> object YAML file (for example, <literal>cro-namespace.yaml</literal>) for the Cluster Resource Override Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: clusterresourceoverride-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;.yaml</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f cro-namespace.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an Operator group:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create an <literal>OperatorGroup</literal> object YAML file (for example, cro-og.yaml) for the Cluster Resource Override Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: clusterresourceoverride-operator
  namespace: clusterresourceoverride-operator
spec:
  targetNamespaces:
    - clusterresourceoverride-operator</programlisting>
</listitem>
<listitem>
<simpara>Create the Operator Group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;.yaml</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f cro-og.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a subscription:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>Subscription</literal> object YAML file (for example, cro-sub.yaml) for the Cluster Resource Override Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: clusterresourceoverride
  namespace: clusterresourceoverride-operator
spec:
  channel: "4.14"
  name: clusterresourceoverride
  source: redhat-operators
  sourceNamespace: openshift-marketplace</programlisting>
</listitem>
<listitem>
<simpara>Create the subscription:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;.yaml</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f cro-sub.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>ClusterResourceOverride</literal> custom resource (CR) object in the <literal>clusterresourceoverride-operator</literal> namespace:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Change to the <literal>clusterresourceoverride-operator</literal> namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project clusterresourceoverride-operator</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>ClusterResourceOverride</literal> object YAML file (for example, cro-cr.yaml) for the Cluster Resource Override Operator:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster <co xml:id="CO124-1"/>
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <co xml:id="CO124-2"/>
      cpuRequestToLimitPercent: 25 <co xml:id="CO124-3"/>
      limitCPUToMemoryPercent: 200 <co xml:id="CO124-4"/></programlisting>
<calloutlist>
<callout arearefs="CO124-1">
<para>The name must be <literal>cluster</literal>.</para>
</callout>
<callout arearefs="CO124-2">
<para>Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.</para>
</callout>
<callout arearefs="CO124-3">
<para>Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.</para>
</callout>
<callout arearefs="CO124-4">
<para>Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>ClusterResourceOverride</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file-name&gt;.yaml</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f cro-cr.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify the current state of the admission webhook by checking the status of the cluster custom resource.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterresourceoverride cluster -n clusterresourceoverride-operator -o yaml</programlisting>
<simpara>The <literal>mutatingWebhookConfigurationRef</literal> section appears when the webhook is called.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"operator.autoscaling.openshift.io/v1","kind":"ClusterResourceOverride","metadata":{"annotations":{},"name":"cluster"},"spec":{"podResourceOverride":{"spec":{"cpuRequestToLimitPercent":25,"limitCPUToMemoryPercent":200,"memoryRequestToLimitPercent":50}}}}
  creationTimestamp: "2019-12-18T22:35:02Z"
  generation: 1
  name: cluster
  resourceVersion: "127622"
  selfLink: /apis/operator.autoscaling.openshift.io/v1/clusterresourceoverrides/cluster
  uid: 978fc959-1717-4bd1-97d0-ae00ee111e8d
spec:
  podResourceOverride:
    spec:
      cpuRequestToLimitPercent: 25
      limitCPUToMemoryPercent: 200
      memoryRequestToLimitPercent: 50
status:

# ...

    mutatingWebhookConfigurationRef: <co xml:id="CO125-1"/>
      apiVersion: admissionregistration.k8s.io/v1
      kind: MutatingWebhookConfiguration
      name: clusterresourceoverrides.admission.autoscaling.openshift.io
      resourceVersion: "127621"
      uid: 98b3b8ae-d5ce-462b-8ab5-a729ea8f38f3

# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO125-1">
<para>Reference to the <literal>ClusterResourceOverride</literal> admission webhook.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-cluster-resource-configure_post-install-node-tasks">
<title>Configuring cluster-level overcommit</title>
<simpara>The Cluster Resource Override Operator requires a <literal>ClusterResourceOverride</literal> custom resource (CR)
and a label for each project where you want the Operator to control overcommit.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The Cluster Resource Override Operator has no effect if limits have not
been set on containers. You must specify default limits for a project using a <literal>LimitRange</literal> object or configure limits in <literal>Pod</literal> specs for the overrides to apply.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To modify cluster-level overcommit:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>ClusterResourceOverride</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.autoscaling.openshift.io/v1
kind: ClusterResourceOverride
metadata:
    name: cluster
spec:
  podResourceOverride:
    spec:
      memoryRequestToLimitPercent: 50 <co xml:id="CO126-1"/>
      cpuRequestToLimitPercent: 25 <co xml:id="CO126-2"/>
      limitCPUToMemoryPercent: 200 <co xml:id="CO126-3"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO126-1">
<para>Optional. Specify the percentage to override the container memory limit, if used, between 1-100. The default is 50.</para>
</callout>
<callout arearefs="CO126-2">
<para>Optional. Specify the percentage to override the container CPU limit, if used, between 1-100. The default is 25.</para>
</callout>
<callout arearefs="CO126-3">
<para>Optional. Specify the percentage to override the container memory limit, if used. Scaling 1Gi of RAM at 100 percent is equal to 1 CPU core. This is processed prior to overriding the CPU request, if configured. The default is 200.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Ensure the following label has been added to the Namespace object for each project where you want the Cluster Resource Override Operator to control overcommit:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:

# ...

  labels:
    clusterresourceoverrides.admission.autoscaling.openshift.io/enabled: "true" <co xml:id="CO127-1"/>

# ...</programlisting>
<calloutlist>
<callout arearefs="CO127-1">
<para>Add this label to each project.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="nodes-cluster-node-overcommit_post-install-node-tasks">
<title>Node-level overcommit</title>
<simpara>You can use various ways to control overcommit on specific nodes, such as quality of service (QOS)
guarantees, CPU limits, or reserve resources. You can also disable overcommit for specific nodes
and specific projects.</simpara>
<section xml:id="nodes-cluster-overcommit-reserving-memory_post-install-node-tasks">
<title>Understanding compute resources and containers</title>
<simpara>The node-enforced behavior for compute resources is specific to the resource
type.</simpara>
<section xml:id="understanding-container-CPU-requests_post-install-node-tasks">
<title>Understanding container CPU requests</title>
<simpara>A container is guaranteed the amount of CPU it requests and is additionally able
to consume excess CPU available on the node, up to any limit specified by the
container. If multiple containers are attempting to use excess CPU, CPU time is
distributed based on the amount of CPU requested by each container.</simpara>
<simpara>For example, if one container requested 500m of CPU time and another container
requested 250m of CPU time, then any extra CPU time available on the node is
distributed among the containers in a 2:1 ratio. If a container specified a
limit, it will be throttled not to use more CPU than the specified limit.
CPU requests are enforced using the CFS shares support in the Linux kernel. By
default, CPU limits are enforced using the CFS quota support in the Linux kernel
over a 100ms measuring interval, though this can be disabled.</simpara>
</section>
<section xml:id="understanding-memory-requests-container_post-install-node-tasks">
<title>Understanding container memory requests</title>
<simpara>A container is guaranteed the amount of memory it requests. A container can use
more memory than requested, but once it exceeds its requested amount, it could
be terminated in a low memory situation on the node.
If a container uses less memory than requested, it will not be terminated unless
system tasks or daemons need more memory than was accounted for in the node&#8217;s
resource reservation. If a container specifies a limit on memory, it is
immediately terminated if it exceeds the limit amount.</simpara>
</section>
</section>
<section xml:id="nodes-cluster-overcommit-qos-about_post-install-node-tasks">
<title>Understanding overcomitment and quality of service classes</title>
<simpara>A node is <emphasis>overcommitted</emphasis> when it has a pod scheduled that makes no request, or
when the sum of limits across all pods on that node exceeds available machine
capacity.</simpara>
<simpara>In an overcommitted environment, it is possible that the pods on the node will
attempt to use more compute resource than is available at any given point in
time. When this occurs, the node must give priority to one pod over another. The
facility used to make this decision is referred to as a Quality of Service (QoS)
Class.</simpara>
<simpara>A pod is designated as one of three QoS classes with decreasing order of priority:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Quality of Service Classes</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="14.2857*"/>
<colspec colname="col_2" colwidth="14.2857*"/>
<colspec colname="col_3" colwidth="71.4286*"/>
<thead>
<row>
<entry align="left" valign="top">Priority</entry>
<entry align="left" valign="top">Class Name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>1 (highest)</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Guaranteed</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>If limits and optionally requests are set (not equal to 0) for all resources
and they are equal, then the pod is classified as <emphasis role="strong">Guaranteed</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>2</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Burstable</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>If requests and optionally limits are set (not equal to 0) for all resources,
and they are not equal, then the pod is classified as <emphasis role="strong">Burstable</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>3 (lowest)</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">BestEffort</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>If requests and limits are not set for any of the resources, then the pod
is classified as <emphasis role="strong">BestEffort</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Memory is an incompressible resource, so in low memory situations, containers
that have the lowest priority are terminated first:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Guaranteed</emphasis> containers are considered top priority, and are guaranteed to
only be terminated if they exceed their limits, or if the system is under memory
pressure and there are no lower priority containers that can be evicted.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Burstable</emphasis> containers under system memory pressure are more likely to be
terminated once they exceed their requests and no other <emphasis role="strong">BestEffort</emphasis> containers
exist.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">BestEffort</emphasis> containers are treated with the lowest priority. Processes in
these containers are first to be terminated if the system runs out of memory.</simpara>
</listitem>
</itemizedlist>
<section xml:id="qos-about-reserve_post-install-node-tasks">
<title>Understanding how to reserve memory across quality of service tiers</title>
<simpara>You can use the <literal>qos-reserved</literal> parameter to specify a percentage of memory to be reserved
by a pod in a particular QoS level. This feature attempts to reserve requested resources to exclude pods
from lower OoS classes from using resources requested by pods in higher QoS classes.</simpara>
<simpara>OpenShift Container Platform uses the <literal>qos-reserved</literal> parameter as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>A value of <literal>qos-reserved=memory=100%</literal> will prevent the <literal>Burstable</literal> and <literal>BestEffort</literal> QoS classes from consuming memory
that was requested by a higher QoS class. This increases the risk of inducing OOM
on <literal>BestEffort</literal> and <literal>Burstable</literal> workloads in favor of increasing memory resource guarantees
for <literal>Guaranteed</literal> and <literal>Burstable</literal> workloads.</simpara>
</listitem>
<listitem>
<simpara>A value of <literal>qos-reserved=memory=50%</literal> will allow the <literal>Burstable</literal> and <literal>BestEffort</literal> QoS classes
to consume half of the memory requested by a higher QoS class.</simpara>
</listitem>
<listitem>
<simpara>A value of <literal>qos-reserved=memory=0%</literal>
will allow a <literal>Burstable</literal> and <literal>BestEffort</literal> QoS classes to consume up to the full node
allocatable amount if available, but increases the risk that a <literal>Guaranteed</literal> workload
will not have access to requested memory. This condition effectively disables this feature.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nodes-qos-about-swap_post-install-node-tasks">
<title>Understanding swap memory and QOS</title>
<simpara>You can disable swap by default on your nodes to preserve quality of
service (QOS) guarantees. Otherwise, physical resources on a node can oversubscribe,
affecting the resource guarantees the Kubernetes scheduler makes during pod
placement.</simpara>
<simpara>For example, if two guaranteed pods have reached their memory limit, each
container could start using swap memory. Eventually, if there is not enough swap
space, processes in the pods can be terminated due to the system being
oversubscribed.</simpara>
<simpara>Failing to disable swap results in nodes not recognizing that they are
experiencing <emphasis role="strong">MemoryPressure</emphasis>, resulting in pods not receiving the memory they
made in their scheduling request. As a result, additional pods are placed on the
node to further increase memory pressure, ultimately increasing your risk of
experiencing a system out of memory (OOM) event.</simpara>
<important>
<simpara>If swap is enabled, any out-of-resource handling eviction thresholds for available memory will not work as
expected. Take advantage of out-of-resource handling to allow pods to be evicted
from a node when it is under memory pressure, and rescheduled on an alternative
node that has no such pressure.</simpara>
</important>
</section>
<section xml:id="nodes-cluster-overcommit-configure-nodes_post-install-node-tasks">
<title>Understanding nodes overcommitment</title>
<simpara>In an overcommitted environment, it is important to properly configure your node to provide best system behavior.</simpara>
<simpara>When the node starts, it ensures that the kernel tunable flags for memory
management are set properly. The kernel should never fail memory allocations
unless it runs out of physical memory.</simpara>
<simpara>To ensure this behavior, OpenShift Container Platform configures the kernel to always overcommit
memory by setting the <literal>vm.overcommit_memory</literal> parameter to <literal>1</literal>, overriding the
default operating system setting.</simpara>
<simpara>OpenShift Container Platform also configures the kernel not to panic when it runs out of memory
by setting the <literal>vm.panic_on_oom</literal> parameter to <literal>0</literal>. A setting of 0 instructs the
kernel to call oom_killer in an Out of Memory (OOM) condition, which kills
processes based on priority</simpara>
<simpara>You can view the current setting by running the following commands on your nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sysctl -a |grep commit</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">#...
vm.overcommit_memory = 0
#...</programlisting>
</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sysctl -a |grep panic</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">#...
vm.panic_on_oom = 0
#...</programlisting>
</para>
</formalpara>
<note>
<simpara>The above flags should already be set on nodes, and no further action is
required.</simpara>
</note>
<simpara>You can also perform the following configurations for each node:</simpara>
<itemizedlist>
<listitem>
<simpara>Disable or enforce CPU limits using CPU CFS quotas</simpara>
</listitem>
<listitem>
<simpara>Reserve resources for system processes</simpara>
</listitem>
<listitem>
<simpara>Reserve memory across quality of service tiers</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nodes-cluster-overcommit-node-enforcing_post-install-node-tasks">
<title>Disabling or enforcing CPU limits using CPU CFS quotas</title>
<simpara>Nodes by default enforce specified CPU limits using the Completely Fair Scheduler (CFS) quota support in the Linux kernel.</simpara>
<simpara>If you disable CPU limit enforcement, it is important to understand the impact on your node:</simpara>
<itemizedlist>
<listitem>
<simpara>If a container has a CPU request, the request continues to be enforced by CFS shares in the Linux kernel.</simpara>
</listitem>
<listitem>
<simpara>If a container does not have a CPU request, but does have a CPU limit, the CPU request defaults to the specified CPU limit, and is enforced by CFS shares in the Linux kernel.</simpara>
</listitem>
<listitem>
<simpara>If a container has both a CPU request and limit, the CPU request is enforced by CFS shares in the Linux kernel, and the CPU limit has no impact on the node.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the label associated with the static <literal>MachineConfigPool</literal> CRD for the type of node you want to configure by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool &lt;name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO128-1"/>
  name: worker</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO128-1">
<para>The label appears under Labels.</para>
</callout>
</calloutlist>
<tip>
<simpara>If the label is not present, add a key/value pair such as:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label machineconfigpool worker custom-kubelet=small-pods</programlisting>
</tip>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a custom resource (CR) for your configuration change.</simpara>
<formalpara>
<title>Sample configuration for a disabling CPU limits</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: disable-cpu-units <co xml:id="CO129-1"/>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO129-2"/>
  kubeletConfig:
    cpuCfsQuota: false <co xml:id="CO129-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO129-1">
<para>Assign a name to CR.</para>
</callout>
<callout arearefs="CO129-2">
<para>Specify the label from the machine config pool.</para>
</callout>
<callout arearefs="CO129-3">
<para>Set the <literal>cpuCfsQuota</literal> parameter to <literal>false</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Run the following command to create the CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-cluster-overcommit-node-resources_post-install-node-tasks">
<title>Reserving resources for system processes</title>
<simpara>To provide more reliable scheduling and minimize node resource overcommitment,
each node can reserve a portion of its resources for use by system daemons
that are required to run on your node for your cluster to function.
In particular, it is recommended that you reserve resources for incompressible resources such as memory.</simpara>
<formalpara>
<title>Procedure</title>
<para>To explicitly reserve resources for non-pod processes, allocate node resources by specifying resources
available for scheduling.
For more details, see Allocating Resources for Nodes.</para>
</formalpara>
</section>
<section xml:id="nodes-cluster-overcommit-node-disable_post-install-node-tasks">
<title>Disabling overcommitment for a node</title>
<simpara>When enabled, overcommitment can be disabled on each node.</simpara>
<formalpara>
<title>Procedure</title>
<para>To disable overcommitment in a node run the following command on that node:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sysctl -w vm.overcommit_memory=0</programlisting>
</section>
</section>
<section xml:id="nodes-cluster-project-overcommit_post-install-node-tasks">
<title>Project-level limits</title>
<simpara>To help control overcommit, you can set per-project resource limit ranges,
specifying memory and CPU limits and defaults for a project that overcommit
cannot exceed.</simpara>
<simpara>For information on project-level resource limits, see Additional resources.</simpara>
<simpara>Alternatively, you can disable overcommitment for specific projects.</simpara>
<section xml:id="nodes-cluster-overcommit-project-disable_post-install-node-tasks">
<title>Disabling overcommitment for a project</title>
<simpara>When enabled, overcommitment can be disabled per-project. For example, you can allow infrastructure components to be configured independently of overcommitment.</simpara>
<formalpara>
<title>Procedure</title>
<para>To disable overcommitment in a project:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create or edit the namespace object file.</simpara>
</listitem>
<listitem>
<simpara>Add the following annotation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  annotations:
    quota.openshift.io/cluster-resource-override-enabled: "false" <co xml:id="CO130-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO130-1">
<para>Setting this annotation to <literal>false</literal> disables overcommit for this namespace.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-garbage-collection">
<title>Freeing node resources using garbage collection</title>
<simpara>Understand and use garbage collection.</simpara>
<section xml:id="nodes-nodes-garbage-collection-containers_post-install-node-tasks">
<title>Understanding how terminated containers are removed through garbage collection</title>
<simpara>Container garbage collection removes terminated containers by using eviction thresholds.</simpara>
<simpara>When eviction thresholds are set for garbage collection, the node tries to keep any container for any pod accessible from the API. If the pod has been deleted, the containers will be as well. Containers are preserved as long the pod is not deleted and the eviction threshold is not reached. If the node is under disk pressure, it will remove containers and their logs will no longer be accessible using <literal>oc logs</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">eviction-soft</emphasis> - A soft eviction threshold pairs an eviction threshold with a required administrator-specified grace period.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">eviction-hard</emphasis> - A hard eviction threshold has no grace period, and if observed, OpenShift Container Platform takes immediate action.</simpara>
</listitem>
</itemizedlist>
<simpara>The following table lists the eviction thresholds:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Variables for configuring container garbage collection</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Node condition</entry>
<entry align="left" valign="top">Eviction signal</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>MemoryPressure</simpara></entry>
<entry align="left" valign="top"><simpara><literal>memory.available</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The available memory on the node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>DiskPressure</simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara><literal>nodefs.available</literal></simpara>
</listitem>
<listitem>
<simpara><literal>nodefs.inodesFree</literal></simpara>
</listitem>
<listitem>
<simpara><literal>imagefs.available</literal></simpara>
</listitem>
<listitem>
<simpara><literal>imagefs.inodesFree</literal></simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara>The available disk space or inodes on the node root file system, <literal>nodefs</literal>, or image file system, <literal>imagefs</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>For <literal>evictionHard</literal> you must specify all of these parameters.  If you do not specify all parameters, only the specified parameters are applied and the garbage collection will not function properly.</simpara>
</note>
<simpara>If a node is oscillating above and below a soft eviction threshold, but not exceeding its associated grace period, the corresponding node would constantly oscillate between <literal>true</literal> and <literal>false</literal>. As a consequence, the scheduler could make poor scheduling decisions.</simpara>
<simpara>To protect against this oscillation, use the <literal>eviction-pressure-transition-period</literal> flag to control how long OpenShift Container Platform must wait before transitioning out of a pressure condition. OpenShift Container Platform will not set an eviction threshold as being met for the specified pressure condition for the period specified before toggling the condition back to false.</simpara>
</section>
<section xml:id="nodes-nodes-garbage-collection-images_post-install-node-tasks">
<title>Understanding how images are removed through garbage collection</title>
<simpara>Image garbage collection removes images that are not referenced by any running pods.</simpara>
<simpara>OpenShift Container Platform determines which images to remove from a node based on the disk usage that is reported by <emphasis role="strong">cAdvisor</emphasis>.</simpara>
<simpara>The policy for image garbage collection is based on two conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>The percent of disk usage (expressed as an integer) which triggers image
garbage collection. The default is <emphasis role="strong">85</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>The percent of disk usage (expressed as an integer) to which image garbage
collection attempts to free. Default is <emphasis role="strong">80</emphasis>.</simpara>
</listitem>
</itemizedlist>
<simpara>For image garbage collection, you can modify any of the following variables using
a custom resource.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Variables for configuring image garbage collection</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>imageMinimumGCAge</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The minimum age for an unused image before the image is removed by garbage collection. The default is <emphasis role="strong">2m</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>imageGCHighThresholdPercent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The percent of disk usage, expressed as an integer, which triggers image
garbage collection. The default is <emphasis role="strong">85</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>imageGCLowThresholdPercent</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The percent of disk usage, expressed as an integer, to which image garbage
collection attempts to free. The default is <emphasis role="strong">80</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>Two lists of images are retrieved in each garbage collector run:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>A list of images currently running in at least one pod.</simpara>
</listitem>
<listitem>
<simpara>A list of images available on a host.</simpara>
</listitem>
</orderedlist>
<simpara>As new containers are run, new images appear. All images are marked with a time
stamp. If the image is running (the first list above) or is newly detected (the
second list above), it is marked with the current time. The remaining images are
already marked from the previous spins. All images are then sorted by the time
stamp.</simpara>
<simpara>Once the collection starts, the oldest images get deleted first until the
stopping criterion is met.</simpara>
</section>
<section xml:id="nodes-nodes-garbage-collection-configuring_post-install-node-tasks">
<title>Configuring garbage collection for containers and images</title>
<simpara>As an administrator, you can configure how OpenShift Container Platform performs garbage collection by creating a <literal>kubeletConfig</literal> object for each machine config pool.</simpara>
<note>
<simpara>OpenShift Container Platform supports only one <literal>kubeletConfig</literal> object for each machine config pool.</simpara>
</note>
<simpara>You can configure any combination of the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Soft eviction for containers</simpara>
</listitem>
<listitem>
<simpara>Hard eviction for containers</simpara>
</listitem>
<listitem>
<simpara>Eviction for images</simpara>
</listitem>
</itemizedlist>
<simpara>Container garbage collection removes terminated containers. Image garbage collection removes images that are not referenced by any running pods.</simpara>
<orderedlist numeration="arabic">
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the label associated with the static <literal>MachineConfigPool</literal> CRD for the type of node you want to configure by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool &lt;name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO131-1"/>
  name: worker
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO131-1">
<para>The label appears under Labels.</para>
</callout>
</calloutlist>
<tip>
<simpara>If the label is not present, add a key/value pair such as:</simpara>
<screen>$ oc label machineconfigpool worker custom-kubelet=small-pods</screen>
</tip>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a custom resource (CR) for your configuration change.</simpara>
<important>
<simpara>If there is one file system, or if <literal>/var/lib/kubelet</literal> and <literal>/var/lib/containers/</literal> are in the same file system, the settings with the highest values trigger evictions, as those are met first. The file system triggers the eviction.</simpara>
</important>
<formalpara>
<title>Sample configuration for a container garbage collection CR:</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: worker-kubeconfig <co xml:id="CO132-1"/>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO132-2"/>
  kubeletConfig:
    evictionSoft: <co xml:id="CO132-3"/>
      memory.available: "500Mi" <co xml:id="CO132-4"/>
      nodefs.available: "10%"
      nodefs.inodesFree: "5%"
      imagefs.available: "15%"
      imagefs.inodesFree: "10%"
    evictionSoftGracePeriod:  <co xml:id="CO132-5"/>
      memory.available: "1m30s"
      nodefs.available: "1m30s"
      nodefs.inodesFree: "1m30s"
      imagefs.available: "1m30s"
      imagefs.inodesFree: "1m30s"
    evictionHard: <co xml:id="CO132-6"/>
      memory.available: "200Mi"
      nodefs.available: "5%"
      nodefs.inodesFree: "4%"
      imagefs.available: "10%"
      imagefs.inodesFree: "5%"
    evictionPressureTransitionPeriod: 0s <co xml:id="CO132-7"/>
    imageMinimumGCAge: 5m <co xml:id="CO132-8"/>
    imageGCHighThresholdPercent: 80 <co xml:id="CO132-9"/>
    imageGCLowThresholdPercent: 75 <co xml:id="CO132-10"/>
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO132-1">
<para>Name for the object.</para>
</callout>
<callout arearefs="CO132-2">
<para>Specify the label from the machine config pool.</para>
</callout>
<callout arearefs="CO132-3">
<para>For container garbage collection: Type of eviction: <literal>evictionSoft</literal> or <literal>evictionHard</literal>.</para>
</callout>
<callout arearefs="CO132-4">
<para>For container garbage collection: Eviction thresholds based on a specific eviction trigger signal.</para>
</callout>
<callout arearefs="CO132-5">
<para>For container garbage collection: Grace periods for the soft eviction. This parameter does not apply to <literal>eviction-hard</literal>.</para>
</callout>
<callout arearefs="CO132-6">
<para>For container garbage collection: Eviction thresholds based on a specific eviction trigger signal.
For <literal>evictionHard</literal> you must specify all of these parameters.  If you do not specify all parameters, only the specified parameters are applied and the garbage collection will not function properly.</para>
</callout>
<callout arearefs="CO132-7">
<para>For container garbage collection: The duration to wait before transitioning out of an eviction pressure condition.</para>
</callout>
<callout arearefs="CO132-8">
<para>For image garbage collection: The minimum age for an unused image before the image is removed by garbage collection.</para>
</callout>
<callout arearefs="CO132-9">
<para>For image garbage collection: The percent of disk usage (expressed as an integer) that triggers image garbage collection.</para>
</callout>
<callout arearefs="CO132-10">
<para>For image garbage collection: The percent of disk usage (expressed as an integer) that image garbage collection attempts to free.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Run the following command to create the CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f gc-container.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">kubeletconfig.machineconfiguration.openshift.io/gc-container created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that garbage collection is active by entering the following command. The Machine Config Pool you specified in the custom resource appears with <literal>UPDATING</literal> as 'true` until the change is fully implemented:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpool</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                                   UPDATED   UPDATING
master   rendered-master-546383f80705bd5aeaba93   True      False
worker   rendered-worker-b4c51bb33ccaae6fc4a6a5   False     True</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-using-node-tuning-operator">
<title>Using the Node Tuning Operator</title>
<simpara>Understand and use the Node Tuning Operator.</simpara>
<bridgehead xml:id="about-node-tuning-operator_post-install-node-tasks" renderas="sect4">Purpose</bridgehead>
<simpara>The Node Tuning Operator helps you manage node-level tuning by orchestrating the TuneD daemon and achieves low latency performance by using the Performance Profile controller. The majority of high-performance applications require some level of kernel tuning. The Node Tuning Operator provides a unified management interface to users of node-level sysctls and more flexibility to add custom tuning specified by user needs.</simpara>
<simpara>The Operator manages the containerized TuneD daemon for OpenShift Container Platform as a Kubernetes daemon set. It ensures the custom tuning specification is passed to all containerized TuneD daemons running in the cluster in the format that the daemons understand. The daemons run on all nodes in the cluster, one per node.</simpara>
<simpara>Node-level settings applied by the containerized TuneD daemon are rolled back on an event that triggers a profile change or when the containerized TuneD daemon is terminated gracefully by receiving and handling a termination signal.</simpara>
<simpara>The Node Tuning Operator uses the Performance Profile controller to implement automatic tuning to achieve low latency performance for OpenShift Container Platform applications.</simpara>
<simpara>The cluster administrator configures a performance profile to define node-level settings such as the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Updating the kernel to kernel-rt.</simpara>
</listitem>
<listitem>
<simpara>Choosing CPUs for housekeeping.</simpara>
</listitem>
<listitem>
<simpara>Choosing CPUs for running workloads.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.</simpara>
</note>
<simpara>The Node Tuning Operator is part of a standard OpenShift Container Platform installation in version 4.1 and later.</simpara>
<note>
<simpara>In earlier versions of OpenShift Container Platform, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In OpenShift Container Platform 4.11 and later, this functionality is part of the Node Tuning Operator.</simpara>
</note>
<section xml:id="accessing-an-example-node-tuning-operator-specification_post-install-node-tasks">
<title>Accessing an example Node Tuning Operator specification</title>
<simpara>Use this process to access an example Node Tuning Operator specification.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to access an example Node Tuning Operator specification:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc get tuned.tuned.openshift.io/default -o yaml -n openshift-cluster-node-tuning-operator</programlisting>
</listitem>
</itemizedlist>
<simpara>The default CR is meant for delivering standard node-level tuning for the OpenShift Container Platform platform and it can only be modified to set the Operator Management state. Any other custom changes to the default CR will be overwritten by the Operator. For custom tuning, create your own Tuned CRs. Newly created CRs will be combined with the default CR and custom tuning applied to OpenShift Container Platform nodes based on node or pod labels and profile priorities.</simpara>
<warning>
<simpara>While in certain situations the support for pod labels can be a convenient way of automatically delivering required tuning, this practice is discouraged and strongly advised against, especially in large-scale clusters. The default Tuned CR ships without pod label matching. If a custom profile is created with pod label matching, then the functionality will be enabled at that time. The pod label functionality will be deprecated in future versions of the Node Tuning Operator.</simpara>
</warning>
</section>
<section xml:id="custom-tuning-specification_post-install-node-tasks">
<title>Custom tuning specification</title>
<simpara>The custom resource (CR) for the Operator has two major sections. The first section, <literal>profile:</literal>, is a list of TuneD profiles and their names. The second, <literal>recommend:</literal>, defines the profile selection logic.</simpara>
<simpara>Multiple custom tuning specifications can co-exist as multiple CRs in the Operator&#8217;s namespace. The existence of new CRs or the deletion of old CRs is detected by the Operator. All existing custom tuning specifications are merged and appropriate objects for the containerized TuneD daemons are updated.</simpara>
<simpara><emphasis role="strong">Management state</emphasis></simpara>
<simpara>The Operator Management state is set by adjusting the default Tuned CR. By default, the Operator is in the Managed state and the <literal>spec.managementState</literal> field is not present in the default Tuned CR. Valid values for the Operator Management state are as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>Managed: the Operator will update its operands as configuration resources are updated</simpara>
</listitem>
<listitem>
<simpara>Unmanaged: the Operator will ignore changes to the configuration resources</simpara>
</listitem>
<listitem>
<simpara>Removed: the Operator will remove its operands and resources the Operator provisioned</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">Profile data</emphasis></simpara>
<simpara>The <literal>profile:</literal> section lists TuneD profiles and their names.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">profile:
- name: tuned_profile_1
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_1 profile

    [sysctl]
    net.ipv4.ip_forward=1
    # ... other sysctl's or other TuneD daemon plugins supported by the containerized TuneD

# ...

- name: tuned_profile_n
  data: |
    # TuneD profile specification
    [main]
    summary=Description of tuned_profile_n profile

    # tuned_profile_n profile settings</programlisting>
<simpara><emphasis role="strong">Recommended profiles</emphasis></simpara>
<simpara>The <literal>profile:</literal> selection logic is defined by the <literal>recommend:</literal> section of the CR. The <literal>recommend:</literal> section is a list of items to recommend the profiles based on a selection criteria.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">recommend:
&lt;recommend-item-1&gt;
# ...
&lt;recommend-item-n&gt;</programlisting>
<simpara>The individual items of the list:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">- machineConfigLabels: <co xml:id="CO133-1"/>
    &lt;mcLabels&gt; <co xml:id="CO133-2"/>
  match: <co xml:id="CO133-3"/>
    &lt;match&gt; <co xml:id="CO133-4"/>
  priority: &lt;priority&gt; <co xml:id="CO133-5"/>
  profile: &lt;tuned_profile_name&gt; <co xml:id="CO133-6"/>
  operand: <co xml:id="CO133-7"/>
    debug: &lt;bool&gt; <co xml:id="CO133-8"/>
    tunedConfig:
      reapply_sysctl: &lt;bool&gt; <co xml:id="CO133-9"/></programlisting>
<calloutlist>
<callout arearefs="CO133-1">
<para>Optional.</para>
</callout>
<callout arearefs="CO133-2">
<para>A dictionary of key/value <literal>MachineConfig</literal> labels. The keys must be unique.</para>
</callout>
<callout arearefs="CO133-3">
<para>If omitted, profile match is assumed unless a profile with a higher priority matches first or <literal>machineConfigLabels</literal> is set.</para>
</callout>
<callout arearefs="CO133-4">
<para>An optional list.</para>
</callout>
<callout arearefs="CO133-5">
<para>Profile ordering priority. Lower numbers mean higher priority (<literal>0</literal> is the highest priority).</para>
</callout>
<callout arearefs="CO133-6">
<para>A TuneD profile to apply on a match. For example <literal>tuned_profile_1</literal>.</para>
</callout>
<callout arearefs="CO133-7">
<para>Optional operand configuration.</para>
</callout>
<callout arearefs="CO133-8">
<para>Turn debugging on or off for the TuneD daemon. Options are <literal>true</literal> for on or <literal>false</literal> for off. The default is <literal>false</literal>.</para>
</callout>
<callout arearefs="CO133-9">
<para>Turn <literal>reapply_sysctl</literal> functionality on or off for the TuneD daemon. Options are <literal>true</literal> for on and <literal>false</literal> for off.</para>
</callout>
</calloutlist>
<simpara><literal>&lt;match&gt;</literal> is an optional list recursively defined as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">- label: &lt;label_name&gt; <co xml:id="CO134-1"/>
  value: &lt;label_value&gt; <co xml:id="CO134-2"/>
  type: &lt;label_type&gt; <co xml:id="CO134-3"/>
    &lt;match&gt; <co xml:id="CO134-4"/></programlisting>
<calloutlist>
<callout arearefs="CO134-1">
<para>Node or pod label name.</para>
</callout>
<callout arearefs="CO134-2">
<para>Optional node or pod label value. If omitted, the presence of <literal>&lt;label_name&gt;</literal> is enough to match.</para>
</callout>
<callout arearefs="CO134-3">
<para>Optional object type (<literal>node</literal> or <literal>pod</literal>). If omitted, <literal>node</literal> is assumed.</para>
</callout>
<callout arearefs="CO134-4">
<para>An optional <literal>&lt;match&gt;</literal> list.</para>
</callout>
</calloutlist>
<simpara>If <literal>&lt;match&gt;</literal> is not omitted, all nested <literal>&lt;match&gt;</literal> sections must also evaluate to <literal>true</literal>. Otherwise, <literal>false</literal> is assumed and the profile with the respective <literal>&lt;match&gt;</literal> section will not be applied or recommended. Therefore, the nesting (child <literal>&lt;match&gt;</literal> sections) works as logical AND operator. Conversely, if any item of the <literal>&lt;match&gt;</literal> list matches, the entire <literal>&lt;match&gt;</literal> list evaluates to <literal>true</literal>. Therefore, the list acts as logical OR operator.</simpara>
<simpara>If <literal>machineConfigLabels</literal> is defined, machine config pool based matching is turned on for the given <literal>recommend:</literal> list item. <literal>&lt;mcLabels&gt;</literal> specifies the labels for a machine config. The machine config is created automatically to apply host settings, such as kernel boot parameters, for the profile <literal>&lt;tuned_profile_name&gt;</literal>. This involves finding all machine config pools with machine config selector matching <literal>&lt;mcLabels&gt;</literal> and setting the profile <literal>&lt;tuned_profile_name&gt;</literal> on all nodes that are assigned the found machine config pools. To target nodes that have both master and worker roles, you must use the master role.</simpara>
<simpara>The list items <literal>match</literal> and <literal>machineConfigLabels</literal> are connected by the logical OR operator. The <literal>match</literal> item is evaluated first in a short-circuit manner. Therefore, if it evaluates to <literal>true</literal>, the <literal>machineConfigLabels</literal> item is not considered.</simpara>
<important>
<simpara>When using machine config pool based matching, it is advised to group nodes with the same hardware configuration into the same machine config pool. Not following this practice might result in TuneD operands calculating conflicting kernel parameters for two or more nodes sharing the same machine config pool.</simpara>
</important>
<formalpara>
<title>Example: Node or pod label based matching</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">- match:
  - label: tuned.openshift.io/elasticsearch
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
    type: pod
  priority: 10
  profile: openshift-control-plane-es
- match:
  - label: node-role.kubernetes.io/master
  - label: node-role.kubernetes.io/infra
  priority: 20
  profile: openshift-control-plane
- priority: 30
  profile: openshift-node</programlisting>
</para>
</formalpara>
<simpara>The CR above is translated for the containerized TuneD daemon into its <literal>recommend.conf</literal> file based on the profile priorities. The profile with the highest priority (<literal>10</literal>) is <literal>openshift-control-plane-es</literal> and, therefore, it is considered first. The containerized TuneD daemon running on a given node looks to see if there is a pod running on the same node with the <literal>tuned.openshift.io/elasticsearch</literal> label set. If not, the entire <literal>&lt;match&gt;</literal> section evaluates as <literal>false</literal>. If there is such a pod with the label, in order for the <literal>&lt;match&gt;</literal> section to evaluate to <literal>true</literal>, the node label also needs to be <literal>node-role.kubernetes.io/master</literal> or <literal>node-role.kubernetes.io/infra</literal>.</simpara>
<simpara>If the labels for the profile with priority <literal>10</literal> matched, <literal>openshift-control-plane-es</literal> profile is applied and no other profile is considered. If the node/pod label combination did not match, the second highest priority profile (<literal>openshift-control-plane</literal>) is considered. This profile is applied if the containerized TuneD pod runs on a node with labels <literal>node-role.kubernetes.io/master</literal> or <literal>node-role.kubernetes.io/infra</literal>.</simpara>
<simpara>Finally, the profile <literal>openshift-node</literal> has the lowest priority of <literal>30</literal>. It lacks the <literal>&lt;match&gt;</literal> section and, therefore, will always match. It acts as a profile catch-all to set <literal>openshift-node</literal> profile, if no other profile with higher priority matches on a given node.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/node-tuning-operator-workflow-revised.png"/>
</imageobject>
<textobject><phrase>Decision workflow</phrase></textobject>
</mediaobject>
</informalfigure>
<formalpara>
<title>Example: Machine config pool based matching</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: openshift-node-custom
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Custom OpenShift node profile with an additional kernel parameter
      include=openshift-node
      [bootloader]
      cmdline_openshift_node_custom=+skew_tick=1
    name: openshift-node-custom

  recommend:
  - machineConfigLabels:
      machineconfiguration.openshift.io/role: "worker-custom"
    priority: 20
    profile: openshift-node-custom</programlisting>
</para>
</formalpara>
<simpara>To minimize node reboots, label the target nodes with a label the machine config pool&#8217;s node selector will match, then create the Tuned CR above and finally create the custom machine config pool itself.</simpara>
<simpara><emphasis role="strong">Cloud provider-specific TuneD profiles</emphasis></simpara>
<simpara>With this functionality, all Cloud provider-specific nodes can conveniently be assigned a TuneD profile specifically tailored to a given Cloud provider on a OpenShift Container Platform cluster. This can be accomplished without adding additional node labels or grouping nodes into
machine config pools.</simpara>
<simpara>This functionality takes advantage of <literal>spec.providerID</literal> node object values in the form of <literal>&lt;cloud-provider&gt;://&lt;cloud-provider-specific-id&gt;</literal> and writes the file <literal>/var/lib/tuned/provider</literal> with the value <literal>&lt;cloud-provider&gt;</literal> in NTO operand containers.  The content of this file is then used by TuneD to load <literal>provider-&lt;cloud-provider&gt;</literal> profile if such profile exists.</simpara>
<simpara>The <literal>openshift</literal> profile that both <literal>openshift-control-plane</literal> and <literal>openshift-node</literal> profiles inherit settings from is now updated to use this functionality through the use of conditional profile loading. Neither NTO nor TuneD currently include any Cloud provider-specific profiles. However, it is possible to create a custom profile <literal>provider-&lt;cloud-provider&gt;</literal> that will be applied to all Cloud provider-specific cluster nodes.</simpara>
<formalpara>
<title>Example GCE Cloud provider profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: provider-gce
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=GCE Cloud provider-specific profile
      # Your tuning for GCE Cloud provider goes here.
    name: provider-gce</programlisting>
</para>
</formalpara>
<note>
<simpara>Due to profile inheritance, any setting specified in the <literal>provider-&lt;cloud-provider&gt;</literal> profile will be overwritten by the <literal>openshift</literal> profile and its child profiles.</simpara>
</note>
</section>
<section xml:id="custom-tuning-default-profiles-set_post-install-node-tasks">
<title>Default profiles set on a cluster</title>
<simpara>The following are the default profiles set on a cluster.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: tuned.openshift.io/v1
kind: Tuned
metadata:
  name: default
  namespace: openshift-cluster-node-tuning-operator
spec:
  profile:
  - data: |
      [main]
      summary=Optimize systems running OpenShift (provider specific parent profile)
      include=-provider-${f:exec:cat:/var/lib/tuned/provider},openshift
    name: openshift
  recommend:
  - profile: openshift-control-plane
    priority: 30
    match:
    - label: node-role.kubernetes.io/master
    - label: node-role.kubernetes.io/infra
  - profile: openshift-node
    priority: 40</programlisting>
<simpara>Starting with OpenShift Container Platform 4.9, all OpenShift TuneD profiles are shipped with
the TuneD package. You can use the <literal>oc exec</literal> command to view the contents of these profiles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec $tuned_pod -n openshift-cluster-node-tuning-operator -- find /usr/lib/tuned/openshift{,-control-plane,-node} -name tuned.conf -exec grep -H ^ {} \;</programlisting>
</section>
<section xml:id="supported-tuned-daemon-plug-ins_post-install-node-tasks">
<title>Supported TuneD daemon plugins</title>
<simpara>Excluding the <literal>[main]</literal> section, the following TuneD plugins are supported when
using custom profiles defined in the <literal>profile:</literal> section of the Tuned CR:</simpara>
<itemizedlist>
<listitem>
<simpara>audio</simpara>
</listitem>
<listitem>
<simpara>cpu</simpara>
</listitem>
<listitem>
<simpara>disk</simpara>
</listitem>
<listitem>
<simpara>eeepc_she</simpara>
</listitem>
<listitem>
<simpara>modules</simpara>
</listitem>
<listitem>
<simpara>mounts</simpara>
</listitem>
<listitem>
<simpara>net</simpara>
</listitem>
<listitem>
<simpara>scheduler</simpara>
</listitem>
<listitem>
<simpara>scsi_host</simpara>
</listitem>
<listitem>
<simpara>selinux</simpara>
</listitem>
<listitem>
<simpara>sysctl</simpara>
</listitem>
<listitem>
<simpara>sysfs</simpara>
</listitem>
<listitem>
<simpara>usb</simpara>
</listitem>
<listitem>
<simpara>video</simpara>
</listitem>
<listitem>
<simpara>vm</simpara>
</listitem>
<listitem>
<simpara>bootloader</simpara>
</listitem>
</itemizedlist>
<simpara>There is some dynamic tuning functionality provided by some of these plugins
that is not supported. The following TuneD plugins are currently not supported:</simpara>
<itemizedlist>
<listitem>
<simpara>script</simpara>
</listitem>
<listitem>
<simpara>systemd</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The TuneD bootloader plugin only supports Red Hat Enterprise Linux CoreOS (RHCOS) worker nodes.</simpara>
</note>
<itemizedlist>
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/customizing-tuned-profiles_monitoring-and-managing-system-status-and-performance#available-tuned-plug-ins_customizing-tuned-profiles">Available TuneD Plugins</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/monitoring_and_managing_system_status_and_performance/getting-started-with-tuned_monitoring-and-managing-system-status-and-performance">Getting Started with TuneD</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="nodes-nodes-managing-max-pods-proc_post-install-node-tasks">
<title>Configuring the maximum number of pods per node</title>
<simpara>Two parameters control the maximum number of pods that can be scheduled to a node: <literal>podsPerCore</literal> and <literal>maxPods</literal>. If you use both options, the lower of the two limits the number of pods on a node.</simpara>
<simpara>For example, if <literal>podsPerCore</literal> is set to <literal>10</literal> on a node with 4 processor cores, the maximum number of pods allowed on the node will be 40.</simpara>
<orderedlist numeration="arabic">
<title>Prerequisites</title>
<listitem>
<simpara>Obtain the label associated with the static <literal>MachineConfigPool</literal> CRD for the type of node you want to configure by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool &lt;name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineconfigpool worker</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  creationTimestamp: "2022-11-16T15:34:25Z"
  generation: 4
  labels:
    pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO135-1"/>
  name: worker
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO135-1">
<para>The label appears under Labels.</para>
</callout>
</calloutlist>
<tip>
<simpara>If the label is not present, add a key/value pair such as:</simpara>
<screen>$ oc label machineconfigpool worker custom-kubelet=small-pods</screen>
</tip>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a custom resource (CR) for your configuration change.</simpara>
<formalpara>
<title>Sample configuration for a <literal>max-pods</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: KubeletConfig
metadata:
  name: set-max-pods <co xml:id="CO136-1"/>
spec:
  machineConfigPoolSelector:
    matchLabels:
      pools.operator.machineconfiguration.openshift.io/worker: "" <co xml:id="CO136-2"/>
  kubeletConfig:
    podsPerCore: 10 <co xml:id="CO136-3"/>
    maxPods: 250 <co xml:id="CO136-4"/>
#...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO136-1">
<para>Assign a name to CR.</para>
</callout>
<callout arearefs="CO136-2">
<para>Specify the label from the machine config pool.</para>
</callout>
<callout arearefs="CO136-3">
<para>Specify the number of pods the node can run based on the number of processor cores on the node.</para>
</callout>
<callout arearefs="CO136-4">
<para>Specify the number of pods the node can run to a fixed value, regardless of the properties of the node.</para>
</callout>
</calloutlist>
<note>
<simpara>Setting <literal>podsPerCore</literal> to <literal>0</literal> disables this limit.</simpara>
</note>
<simpara>In the above example, the default value for <literal>podsPerCore</literal> is <literal>10</literal> and the default value for <literal>maxPods</literal> is <literal>250</literal>. This means that unless the node has 25 cores or more, by default, <literal>podsPerCore</literal> will be the limiting factor.</simpara>
</listitem>
<listitem>
<simpara>Run the following command to create the CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>List the <literal>MachineConfigPool</literal> CRDs to see if the change is applied. The <literal>UPDATING</literal> column reports <literal>True</literal> if the change is picked up by the Machine Config Controller:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpools</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     False      False
worker   worker-8cecd1236b33ee3f8a5e   False     True       False</programlisting>
</para>
</formalpara>
<simpara>Once the change is complete, the <literal>UPDATED</literal> column reports <literal>True</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfigpools</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                        UPDATED   UPDATING   DEGRADED
master   master-9cc2c72f205e103bb534   False     True       False
worker   worker-8cecd1236b33ee3f8a5e   True      False      False</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-vsphere-scale-static-ip-addresses">
<title>Machine scaling with static IP addresses</title>
<simpara>After you deployed your cluster to run nodes with static IP addresses, you can scale an instance of a machine or a machine set to use one of these static IP addresses.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/installing_vsphere/ipi/ipi-vsphere-installation-reqs.xml#installation-vsphere-installer-infra-requirements_ipi-vsphere-installation-reqs">Static IP addresses for vSphere nodes</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="nodes-vsphere-scaling-machines-static-ip_post-install-node-tasks">
<title>Scaling machines to use static IP addresses</title>
<simpara>You can scale additional machine sets to use pre-defined static IP addresses on your cluster. For this configuration, you need to create a machine resource YAML file and then define static IP addresses in this file.</simpara>
<important>
<simpara>Static IP addresses for vSphere nodes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You included <literal>featureSet:TechPreviewNoUpgrade</literal> as the initial entry in the <literal>install-config.yaml</literal> file.</simpara>
</listitem>
<listitem>
<simpara>You deployed a cluster that runs at least one node with a configured static IP address.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a machine resource YAML file and define static IP address network information in the <literal>network</literal> parameter.</simpara>
<formalpara>
<title>Example of a machine resource YAML file with static IP address information defined in the <literal>network</literal> parameter.</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  creationTimestamp: null
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
    machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
    machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
    machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  name: &lt;infrastructure_id&gt;-&lt;role&gt;
  namespace: openshift-machine-api
spec:
  lifecycleHooks: {}
  metadata: {}
  providerSpec:
    value:
      apiVersion: machine.openshift.io/v1beta1
      credentialsSecret:
        name: vsphere-cloud-credentials
      diskGiB: 120
      kind: VSphereMachineProviderSpec
      memoryMiB: 8192
      metadata:
        creationTimestamp: null
      network:
        devices:
        - gateway: 192.168.204.1 <co xml:id="CO137-1"/>
          ipAddrs:
          - 192.168.204.8/24 <co xml:id="CO137-2"/>
          nameservers: <co xml:id="CO137-3"/>
          - 192.168.204.1
          networkName: qe-segment-204
      numCPUs: 4
      numCoresPerSocket: 2
      snapshot: ""
      template: &lt;vm_template_name&gt;
      userDataSecret:
        name: worker-user-data
      workspace:
        datacenter: &lt;vcenter_datacenter_name&gt;
        datastore: &lt;vcenter_datastore_name&gt;
        folder: &lt;vcenter_vm_folder_path&gt;
        resourcepool: &lt;vsphere_resource_pool&gt;
        server: &lt;vcenter_server_ip&gt;
status: {}</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO137-1">
<para>The IP address for the default gateway for the network interface.</para>
</callout>
<callout arearefs="CO137-2">
<para>Lists IPv4, IPv6, or both IP addresses that installation program passes to the network interface. Both IP families must use the same network interface for the default network.</para>
</callout>
<callout arearefs="CO137-3">
<para>Lists a DNS nameserver. You can define up to 3 DNS nameservers. Consider defining more than one DNS nameserver to take advantage of DNS resolution if that one DNS nameserver becomes unreachable.</para>
<itemizedlist>
<listitem>
<simpara>Create a <literal>machine</literal> custom resource (CR) by entering the following command in your terminal:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="nodes-vsphere-machine-set-concept-static-ip_post-install-node-tasks">
<title>Machine set scaling of machines with configured static IP addresses</title>
<simpara>You can use a machine set to scale machines with configured static IP addresses.</simpara>
<important>
<simpara>Static IP addresses for vSphere nodes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>After you configure a machine set to request a static IP address for a machine, the machine controller creates an <literal>IPAddressClaim</literal> resource in the <literal>openshift-machine-api</literal> namespace. The external controller then creates an <literal>IPAddress</literal> resource and binds any static IP addresses to the <literal>IPAddressClaim</literal> resource.</simpara>
<important>
<simpara>Your organization might use numerous types of IP address management (IPAM) services. If you want to enable a particular IPAM service on OpenShift Container Platform, you might need to manually create the <literal>IPAddressClaim</literal> resource in a YAML definition and then bind a static IP address to this resource by entering the following command in your <literal>oc</literal> CLI:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;ipaddressclaim_filename&gt;</programlisting>
</important>
<simpara>The following demonstrates an example of an <literal>IPAddressClaim</literal> resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: IPAddressClaim
metadata:
  finalizers:
  - machine.openshift.io/ip-claim-protection
  name: cluster-dev-9n5wg-worker-0-m7529-claim-0-0
  namespace: openshift-machine-api
spec:
  poolRef:
    apiGroup: ipamcontroller.example.io
    kind: IPPool
    name: static-ci-pool
status: {}</programlisting>
<simpara>The machine controller updates the machine with a status of <literal>IPAddressClaimed</literal> to indicate that a static IP address has succesfully bound to the <literal>IPAddressClaim</literal> resource. The machine controller applies the same status to a machine with multiple <literal>IPAddressClaim</literal> resources that each contain a bound static IP address.The machine controller then creates a virtual machine and applies static IP addresses to any nodes listed in the <literal>providerSpec</literal> of a machine&#8217;s configuration.</simpara>
</section>
<section xml:id="nodes-vsphere-machine-set-scaling-static-ip_post-install-node-tasks">
<title>Using a machine set to scale machines with configured static IP addresses</title>
<simpara>You can use a machine set to scale machines with configured static IP addresses.</simpara>
<important>
<simpara>Static IP addresses for vSphere nodes is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>The example in the procedure demonstrates the use of controllers for scaling machines in a machine set.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You included <literal>featureSet:TechPreviewNoUpgrade</literal> as the initial entry in the <literal>install-config.yaml</literal> file.</simpara>
</listitem>
<listitem>
<simpara>You deployed a cluster that runs at least one node with a configured static IP address.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure a machine set by specifying IP pool information in the <literal>network.devices.addressesFromPools</literal> schema of the machine set&#8217;s YAML file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  annotations:
    machine.openshift.io/memoryMb: "8192"
    machine.openshift.io/vCPU: "4"
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
  name: &lt;infrastructure_id&gt;-&lt;role&gt;
  namespace: openshift-machine-api
spec:
  replicas: 0
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        ipam: "true"
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: worker
        machine.openshift.io/cluster-api-machine-type: worker
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      lifecycleHooks: {}
      metadata: {}
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1beta1
          credentialsSecret:
            name: vsphere-cloud-credentials
          diskGiB: 120
          kind: VSphereMachineProviderSpec
          memoryMiB: 8192
          metadata: {}
          network:
            devices:
            - addressesFromPools: <co xml:id="CO138-1"/>
              - group: ipamcontroller.example.io
                name: static-ci-pool
                resource: IPPool
              nameservers:
              - "192.168.204.1" <co xml:id="CO138-2"/>
              networkName: qe-segment-204
          numCPUs: 4
          numCoresPerSocket: 2
          snapshot: ""
          template: rvanderp4-dev-9n5wg-rhcos-generated-region-generated-zone
          userDataSecret:
            name: worker-user-data
          workspace:
            datacenter: IBMCdatacenter
            datastore: /IBMCdatacenter/datastore/vsanDatastore
            folder: /IBMCdatacenter/vm/rvanderp4-dev-9n5wg
            resourcePool: /IBMCdatacenter/host/IBMCcluster//Resources
            server: vcenter.ibmc.devcluster.openshift.com</programlisting>
<calloutlist>
<callout arearefs="CO138-1">
<para>Specifies an IP pool, which lists a static IP address or a range of static IP addresses. The IP Pool can either be a reference to a custom resource definition (CRD) or a resource supported by the <literal>IPAddressClaims</literal> resource handler. The machine controller accesses static IP addresses listed in the machine set&#8217;s configuration and then allocates each address to each machine.</para>
</callout>
<callout arearefs="CO138-2">
<para>Lists a nameserver. You must specify a nameserver for nodes that receive static IP address, because the Dynamic Host Configuration Protocol (DHCP) network configuration does not support static IP addresses.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Scale the machine set by entering the following commands in your <literal>oc</literal> CLI:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=2 machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>Or:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset&gt; -n openshift-machine-api</programlisting>
<simpara>After each machine is scaled up, the machine controller creates an <literal>IPAddresssClaim</literal> resource.</simpara>
</listitem>
<listitem>
<simpara>Optional: Check that the <literal>IPAddressClaim</literal> resource exists in the <literal>openshift-machine-api</literal> namespace by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get ipaddressclaims.ipam.cluster.x-k8s.io -n openshift-machine-api</programlisting>
<formalpara>
<title>Example <literal>oc</literal> CLI output that lists two IP pools listed in the <literal>openshift-machine-api</literal> namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         POOL NAME        POOL KIND
cluster-dev-9n5wg-worker-0-m7529-claim-0-0   static-ci-pool   IPPool
cluster-dev-9n5wg-worker-0-wdqkt-claim-0-0   static-ci-pool   IPPool</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create an <literal>IPAddress</literal> resource by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f ipaddress.yaml</programlisting>
<simpara>The following example shows an <literal>IPAddress</literal> resource with defined network configuration information and one defined static IP address:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: ipam.cluster.x-k8s.io/v1alpha1
kind: IPAddress
metadata:
  name: cluster-dev-9n5wg-worker-0-m7529-ipaddress-0-0
  namespace: openshift-machine-api
spec:
  address: 192.168.204.129
  claimRef: <co xml:id="CO139-1"/>
    name: cluster-dev-9n5wg-worker-0-m7529-claim-0-0
  gateway: 192.168.204.1
  poolRef: <co xml:id="CO139-2"/>
    apiGroup: ipamcontroller.example.io
    kind: IPPool
    name: static-ci-pool
  prefix: 23</programlisting>
<calloutlist>
<callout arearefs="CO139-1">
<para>The name of the target <literal>IPAddressClaim</literal> resource.</para>
</callout>
<callout arearefs="CO139-2">
<para>Details information about the static IP address or addresses from your nodes.</para>
</callout>
</calloutlist>
<note>
<simpara>By default, the external controller automatically scans any resources in the machine set for recognizable address pool types. When the external controller finds <literal>kind: IPPool</literal> defined in the <literal>IPAddress</literal> resource, the controller binds any static IP addresses to the <literal>IPAddressClaim</literal> resource.</simpara>
</note>
</listitem>
<listitem>
<simpara>Update the <literal>IPAddressClaim</literal> status with a reference to the <literal>IPAddress</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc --type=merge patch IPAddressClaim cluster-dev-9n5wg-worker-0-m7529-claim-0-0 -p='{"status":{"addressRef": {"name": "cluster-dev-9n5wg-worker-0-m7529-ipaddress-0-0"}}}' -n openshift-machine-api --subresource=status</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
<chapter xml:id="post-install-network-configuration">
<title>Postinstallation network configuration</title>

<simpara>After installing OpenShift Container Platform, you can further expand and customize your
network to your requirements.</simpara>
<section xml:id="nw-operator-cr_post-install-network-configuration">
<title>Cluster Network Operator configuration</title>
<simpara>The configuration for the cluster network is specified as part of the Cluster Network Operator (CNO) configuration and stored in a custom resource (CR) object that is named <literal>cluster</literal>. The CR specifies the fields for the <literal>Network</literal> API in the <literal>operator.openshift.io</literal> API group.</simpara>
<simpara>The CNO configuration inherits the following fields during cluster installation from the <literal>Network</literal> API in the <literal>Network.config.openshift.io</literal> API group:</simpara>
<variablelist>
<varlistentry>
<term><literal>clusterNetwork</literal></term>
<listitem>
<simpara>IP address pools from which pod IP addresses are allocated.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>serviceNetwork</literal></term>
<listitem>
<simpara>IP address pool for services.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>defaultNetwork.type</literal></term>
<listitem>
<simpara>Cluster network plugin, such as OpenShift SDN or OVN-Kubernetes.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>After cluster installation, you can only modify the <literal>clusterNetwork</literal> IP address range. The default network type can only be changed from OpenShift SDN to OVN-Kubernetes through migration.</simpara>
</note>
</section>
<section xml:id="nw-proxy-configure-object_post-install-network-configuration">
<title>Enabling the cluster-wide proxy</title>
<simpara>The <literal>Proxy</literal> object is used to manage the cluster-wide egress proxy. When a cluster is installed or upgraded without the proxy configured, a <literal>Proxy</literal> object is still generated but it will have a nil <literal>spec</literal>. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  trustedCA:
    name: ""
status:</programlisting>
<simpara>A cluster administrator can configure the proxy for OpenShift Container Platform by modifying this <literal>cluster</literal> <literal>Proxy</literal> object.</simpara>
<note>
<simpara>Only the <literal>Proxy</literal> object named <literal>cluster</literal> is supported, and no additional proxies can be created.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator permissions</simpara>
</listitem>
<listitem>
<simpara>OpenShift Container Platform <literal>oc</literal> CLI tool installed</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a config map that contains any additional CA certificates required for proxying HTTPS connections.</simpara>
<note>
<simpara>You can skip this step if the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust bundle.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a file called <literal>user-ca-bundle.yaml</literal> with the following contents, and provide the values of your PEM-encoded certificates:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
data:
  ca-bundle.crt: | <co xml:id="CO140-1"/>
    &lt;MY_PEM_ENCODED_CERTS&gt; <co xml:id="CO140-2"/>
kind: ConfigMap
metadata:
  name: user-ca-bundle <co xml:id="CO140-3"/>
  namespace: openshift-config <co xml:id="CO140-4"/></programlisting>
<calloutlist>
<callout arearefs="CO140-1">
<para>This data key must be named <literal>ca-bundle.crt</literal>.</para>
</callout>
<callout arearefs="CO140-2">
<para>One or more PEM-encoded X.509 certificates used to sign the proxy&#8217;s
identity certificate.</para>
</callout>
<callout arearefs="CO140-3">
<para>The config map name that will be referenced from the <literal>Proxy</literal> object.</para>
</callout>
<callout arearefs="CO140-4">
<para>The config map must be in the <literal>openshift-config</literal> namespace.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the config map from this file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f user-ca-bundle.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Use the <literal>oc edit</literal> command to modify the <literal>Proxy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit proxy/cluster</programlisting>
</listitem>
<listitem>
<simpara>Configure the necessary fields for the proxy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Proxy
metadata:
  name: cluster
spec:
  httpProxy: http://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO141-1"/>
  httpsProxy: https://&lt;username&gt;:&lt;pswd&gt;@&lt;ip&gt;:&lt;port&gt; <co xml:id="CO141-2"/>
  noProxy: example.com <co xml:id="CO141-3"/>
  readinessEndpoints:
  - http://www.google.com <co xml:id="CO141-4"/>
  - https://www.google.com
  trustedCA:
    name: user-ca-bundle <co xml:id="CO141-5"/></programlisting>
<calloutlist>
<callout arearefs="CO141-1">
<para>A proxy URL to use for creating HTTP connections outside the cluster. The URL scheme must be <literal>http</literal>.</para>
</callout>
<callout arearefs="CO141-2">
<para>A proxy URL to use for creating HTTPS connections outside the cluster. The URL scheme must be either <literal>http</literal> or <literal>https</literal>. Specify a URL for the proxy that supports the URL scheme. For example, most proxies will report an error if they are configured to use <literal>https</literal> but they only support <literal>http</literal>. This failure message may not propagate to the logs and can appear to be a network connection failure instead. If using a proxy that listens for <literal>https</literal> connections from the cluster, you may need to configure the cluster to accept the CAs and certificates that the proxy uses.</para>
</callout>
<callout arearefs="CO141-3">
<para>A comma-separated list of destination domain names, domains, IP addresses or other network CIDRs to exclude proxying.</para>
<simpara>Preface a domain with <literal>.</literal> to match subdomains only. For example, <literal>.y.com</literal> matches <literal>x.y.com</literal>, but not <literal>y.com</literal>. Use <literal>*</literal> to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the <literal>networking.machineNetwork[].cidr</literal> field from the installation configuration, you must add them to this list to prevent connection issues.</simpara>
<simpara>This field is ignored if neither the <literal>httpProxy</literal> or <literal>httpsProxy</literal> fields are set.</simpara>
</callout>
<callout arearefs="CO141-4">
<para>One or more URLs external to the cluster to use to perform a readiness check before writing the <literal>httpProxy</literal> and <literal>httpsProxy</literal> values to status.</para>
</callout>
<callout arearefs="CO141-5">
<para>A reference to the config map in the <literal>openshift-config</literal> namespace that contains additional CA certificates required for proxying HTTPS connections. Note that the config map must already exist before referencing it here. This field is required unless the proxy&#8217;s identity certificate is signed by an authority from the RHCOS trust bundle.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="private-clusters-setting-dns-private_post-install-network-configuration">
<title>Setting DNS to private</title>
<simpara>After you deploy a cluster, you can modify its DNS to use only a private zone.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Review the <literal>DNS</literal> custom resource for your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dnses.config.openshift.io/cluster -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;: owned
  publicZone:
    id: Z2XXXXXXXXXXA4
status: {}</programlisting>
</para>
</formalpara>
<simpara>Note that the <literal>spec</literal> section contains both a private and a public zone.</simpara>
</listitem>
<listitem>
<simpara>Patch the <literal>DNS</literal> custom resource to remove the public zone:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch dnses.config.openshift.io/cluster --type=merge --patch='{"spec": {"publicZone": null}}'
dns.config.openshift.io/cluster patched</programlisting>
<simpara>Because the Ingress Controller consults the <literal>DNS</literal> definition when it creates <literal>Ingress</literal> objects, when you create or modify <literal>Ingress</literal> objects, only private records are created.</simpara>
<important>
<simpara>DNS records for the existing Ingress objects are not modified when you remove the public zone.</simpara>
</important>
</listitem>
<listitem>
<simpara>Optional: Review the <literal>DNS</literal> custom resource for your cluster and confirm that the public zone was removed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dnses.config.openshift.io/cluster -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: DNS
metadata:
  creationTimestamp: "2019-10-25T18:27:09Z"
  generation: 2
  name: cluster
  resourceVersion: "37966"
  selfLink: /apis/config.openshift.io/v1/dnses/cluster
  uid: 0e714746-f755-11f9-9cb1-02ff55d8f976
spec:
  baseDomain: &lt;base_domain&gt;
  privateZone:
    tags:
      Name: &lt;infrastructure_id&gt;-int
      kubernetes.io/cluster/&lt;infrastructure_id&gt;-wfpg4: owned
status: {}</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="post-install-configuring_ingress_cluster_traffic">
<title>Configuring ingress cluster traffic</title>
<simpara>OpenShift Container Platform provides the following methods for communicating from outside the cluster with services running in the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>If you have HTTP/HTTPS, use an Ingress Controller.</simpara>
</listitem>
<listitem>
<simpara>If you have a TLS-encrypted protocol other than HTTPS, such as TLS with the SNI header, use an Ingress Controller.</simpara>
</listitem>
<listitem>
<simpara>Otherwise, use a load balancer, an external IP, or a node port.</simpara>
</listitem>
</itemizedlist>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Method</entry>
<entry align="left" valign="top">Purpose</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-ingress-controller.xml#configuring-ingress-cluster-traffic-ingress-controller">Use an Ingress Controller</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows access to HTTP/HTTPS traffic and TLS-encrypted protocols other than HTTPS, such as TLS with the SNI header.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-load-balancer.xml#configuring-ingress-cluster-traffic-load-balancer">Automatically assign an external IP by using a load balancer service</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows traffic to non-standard ports through an IP address assigned from a pool.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-service-external-ip.xml#configuring-ingress-cluster-traffic-service-external-ip">Manually assign an external IP to a service</link></simpara></entry>
<entry align="left" valign="top"><simpara>Allows traffic to non-standard ports through a specific IP address.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../networking/configuring_ingress_cluster_traffic/configuring-ingress-cluster-traffic-nodeport.xml#configuring-ingress-cluster-traffic-nodeport">Configure a <literal>NodePort</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Expose a service on all nodes in the cluster.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="post-install-configuring-node-port-service-range">
<title>Configuring the node port service range</title>
<simpara>As a cluster administrator, you can expand the available node port range. If your cluster uses of a large number of node ports, you might need to increase the number of available ports.</simpara>
<simpara>The default port range is <literal>30000-32767</literal>. You can never reduce the port range, even if you first expand it beyond the default range.</simpara>
<section xml:id="post-install-configuring-node-port-service-range-prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your cluster infrastructure must allow access to the ports that you specify within the expanded range. For example, if you expand the node port range to <literal>30000-32900</literal>, the inclusive port range of <literal>32768-32900</literal> must be allowed by your firewall or packet filtering configuration.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-nodeport-service-range-edit_post-install-network-configuration">
<title>Expanding the node port range</title>
<simpara>You can expand the node port range for the cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To expand the node port range, enter the following command. Replace <literal>&lt;port&gt;</literal> with the largest port number in the new range.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch network.config.openshift.io cluster --type=merge -p \
  '{
    "spec":
      { "serviceNodePortRange": "30000-&lt;port&gt;" }
  }'</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to update the node port range:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  serviceNodePortRange: "30000-&lt;port&gt;"</programlisting>
</tip>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">network.config.openshift.io/cluster patched</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To confirm that the configuration is active, enter the following command. It can take several minutes for the update to apply.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmaps -n openshift-kube-apiserver config \
  -o jsonpath="{.data['config\.yaml']}" | \
  grep -Eo '"service-node-port-range":["[[:digit:]]+-[[:digit:]]+"]'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"service-node-port-range":["30000-33000"]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="post-install-configuring-ipsec-ovn">
<title>Configuring IPsec encryption</title>
<simpara>With IPsec enabled, all network traffic between nodes on the OVN-Kubernetes network plugin travels through an encrypted tunnel.</simpara>
<simpara>IPsec is disabled by default.</simpara>
<section xml:id="post-install-configuring-ipsec-ovn-prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Your cluster must use the OVN-Kubernetes network plugin.</simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-ovn-ipsec-enable_post-install-network-configuration">
<title>Enabling pod-to-pod IPsec encryption</title>
<simpara>As a cluster administrator, you can enable pod-to-pod IPsec encryption after cluster installation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have reduced the size of your cluster MTU by <literal>46</literal> bytes to allow for the overhead of the IPsec ESP header.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To enable IPsec encryption, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch networks.operator.openshift.io cluster --type=merge \
-p '{"spec":{"defaultNetwork":{"ovnKubernetesConfig":{"ipsecConfig":{ }}}}}'</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-ovn-ipsec-verification_post-install-network-configuration">
<title>Verifying that IPsec is enabled</title>
<simpara>As a cluster administrator, you can verify that IPsec is enabled.</simpara>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>To find the names of the OVN-Kubernetes data plane pods, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-ovn-kubernetes -l=app=ovnkube-node</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ovnkube-node-5xqbf                       8/8     Running   0              28m
ovnkube-node-6mwcx                       8/8     Running   0              29m
ovnkube-node-ck5fr                       8/8     Running   0              31m
ovnkube-node-fr4ld                       8/8     Running   0              26m
ovnkube-node-wgs4l                       8/8     Running   0              33m
ovnkube-node-zfvcl                       8/8     Running   0              34m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that IPsec is enabled on your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes -c nbdb rsh ovnkube-node-&lt;XXXXX&gt; ovn-nbctl --no-leader-only get nb_global . ipsec</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;XXXXX&gt;</literal></term>
<listitem>
<simpara>Specifies the random sequence of letters for a pod from the previous step.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="post-install-configuring-network-policy">
<title>Configuring network policy</title>
<simpara>As a cluster administrator or project administrator, you can configure network policies for a project.</simpara>
<section xml:id="nw-networkpolicy-about_post-install-network-configuration">
<title>About network policy</title>
<simpara>In a cluster using a network plugin that supports Kubernetes network policy, network isolation is controlled entirely by <literal>NetworkPolicy</literal> objects.
In OpenShift Container Platform 4.14, OpenShift SDN supports using network policy in its default network isolation mode.</simpara>
<warning>
<simpara>Network policy does not apply to the host network namespace. Pods with host networking enabled are unaffected by network policy rules. However, pods connecting to the host-networked pods might be affected by the network policy rules.</simpara>
<simpara>Network policies cannot block traffic from localhost or from their resident nodes.</simpara>
</warning>
<simpara>By default, all pods in a project are accessible from other pods and network endpoints. To isolate one or more pods in a project, you can create <literal>NetworkPolicy</literal> objects in that project to indicate the allowed incoming connections. Project administrators can create and delete <literal>NetworkPolicy</literal> objects within their own project.</simpara>
<simpara>If a pod is matched by selectors in one or more <literal>NetworkPolicy</literal> objects, then the pod will accept only connections that are allowed by at least one of those <literal>NetworkPolicy</literal> objects. A pod that is not selected by any <literal>NetworkPolicy</literal> objects is fully accessible.</simpara>
<simpara>A network policy applies to only the TCP, UDP, ICMP, and SCTP protocols. Other protocols are not affected.</simpara>
<simpara>The following example <literal>NetworkPolicy</literal> objects demonstrate supporting different scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>Deny all traffic:</simpara>
<simpara>To make a project deny by default, add a <literal>NetworkPolicy</literal> object that matches all pods but accepts no traffic:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector: {}
  ingress: []</programlisting>
</listitem>
<listitem>
<simpara>Only allow connections from the OpenShift Container Platform Ingress Controller:</simpara>
<simpara>To make a project allow only connections from the OpenShift Container Platform Ingress Controller, add the following <literal>NetworkPolicy</literal> object.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: ingress
  podSelector: {}
  policyTypes:
  - Ingress</programlisting>
</listitem>
<listitem>
<simpara>Only accept connections from pods within a project:</simpara>
<simpara>To make pods accept connections from other pods in the same project, but reject all other connections from pods in other projects, add the following <literal>NetworkPolicy</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector: {}
  ingress:
  - from:
    - podSelector: {}</programlisting>
</listitem>
<listitem>
<simpara>Only allow HTTP and HTTPS traffic based on pod labels:</simpara>
<simpara>To enable only HTTP and HTTPS access to the pods with a specific label (<literal>role=frontend</literal> in following example), add a <literal>NetworkPolicy</literal> object similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-http-and-https
spec:
  podSelector:
    matchLabels:
      role: frontend
  ingress:
  - ports:
    - protocol: TCP
      port: 80
    - protocol: TCP
      port: 443</programlisting>
</listitem>
<listitem>
<simpara>Accept connections by using both namespace and pod selectors:</simpara>
<simpara>To match network traffic by combining namespace and pod selectors, you can use a <literal>NetworkPolicy</literal> object similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-pod-and-namespace-both
spec:
  podSelector:
    matchLabels:
      name: test-pods
  ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            project: project_name
        podSelector:
          matchLabels:
            name: test-pods</programlisting>
</listitem>
</itemizedlist>
<simpara><literal>NetworkPolicy</literal> objects are additive, which means you can combine multiple <literal>NetworkPolicy</literal> objects together to satisfy complex network requirements.</simpara>
<simpara>For example, for the <literal>NetworkPolicy</literal> objects defined in previous samples, you can define both <literal>allow-same-namespace</literal> and <literal>allow-http-and-https</literal> policies within the same project. Thus allowing the pods with the label <literal>role=frontend</literal>, to accept any connection allowed by each policy. That is, connections on any port from pods in the same namespace, and connections on ports <literal>80</literal> and <literal>443</literal> from pods in any namespace.</simpara>
<section xml:id="nw-networkpolicy-allow-from-router_post-install-network-configuration">
<title>Using the allow-from-router network policy</title>
<simpara>Use the following <literal>NetworkPolicy</literal> to allow external traffic regardless of the router configuration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-router
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""<co xml:id="CO142-1"/>
  podSelector: {}
  policyTypes:
  - Ingress</programlisting>
<calloutlist>
<callout arearefs="CO142-1">
<para><literal>policy-group.network.openshift.io/ingress:""</literal> label supports both OpenShift-SDN and OVN-Kubernetes.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-networkpolicy-allow-from-hostnetwork_post-install-network-configuration">
<title>Using the allow-from-hostnetwork network policy</title>
<simpara>Add the following <literal>allow-from-hostnetwork</literal> <literal>NetworkPolicy</literal> object to direct traffic from the host network pods:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-hostnetwork
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/host-network: ""
  podSelector: {}
  policyTypes:
  - Ingress</programlisting>
</section>
</section>
<section xml:id="nw-networkpolicy-object_post-install-network-configuration">
<title>Example NetworkPolicy object</title>
<simpara>The following annotates an example NetworkPolicy object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-27107 <co xml:id="CO143-1"/>
spec:
  podSelector: <co xml:id="CO143-2"/>
    matchLabels:
      app: mongodb
  ingress:
  - from:
    - podSelector: <co xml:id="CO143-3"/>
        matchLabels:
          app: app
    ports: <co xml:id="CO143-4"/>
    - protocol: TCP
      port: 27017</programlisting>
<calloutlist>
<callout arearefs="CO143-1">
<para>The name of the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO143-2">
<para>A selector that describes the pods to which the policy applies. The policy object can
only select pods in the project that defines the NetworkPolicy object.</para>
</callout>
<callout arearefs="CO143-3">
<para>A selector that matches the pods from which the policy object allows ingress traffic. The selector matches pods in the same namespace as the NetworkPolicy.</para>
</callout>
<callout arearefs="CO143-4">
<para>A list of one or more destination ports on which to accept traffic.</para>
</callout>
</calloutlist>
</section>
<section xml:id="nw-networkpolicy-create-cli_post-install-network-configuration">
<title>Creating a network policy using the CLI</title>
<simpara>To define granular rules describing ingress or egress network traffic allowed for namespaces in your cluster, you can create a network policy.</simpara>
<note>
<simpara>If you log in with a user with the <literal>cluster-admin</literal> role, then you can create a network policy in any namespace in the cluster.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You are working in the namespace that the network policy applies to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a policy rule:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>&lt;policy_name&gt;.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ touch &lt;policy_name&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the network policy file name.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Define a network policy in the file that you just created, such as in the following examples:</simpara>
<formalpara>
<title>Deny ingress from all pods in all namespaces</title>
<para>This is a fundamental policy, blocking all cross-pod networking other than cross-pod traffic allowed by the configuration of other Network Policies.</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: deny-by-default
spec:
  podSelector:
  ingress: []</programlisting>
<simpara>+
.Allow ingress from all pods in the same namespace</simpara>
<simpara>+</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<screen>kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}</screen>
<simpara>+
.Allow ingress traffic to one pod from a particular namespace</simpara>
<simpara>+
This policy allows traffic to pods labelled <literal>pod-a</literal> from pods running in <literal>namespace-y</literal>.</simpara>
<simpara>+</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-traffic-pod
spec:
  podSelector:
   matchLabels:
      pod: pod-a
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
           kubernetes.io/metadata.name: namespace-y</programlisting>
<simpara>+</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To create the network policy object, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;policy_name&gt;.yaml -n &lt;namespace&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;policy_name&gt;</literal></term>
<listitem>
<simpara>Specifies the network policy file name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Optional: Specifies the namespace if the object is defined in a different namespace than the current namespace.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">networkpolicy.networking.k8s.io/deny-by-default created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>If you log in to the web console with <literal>cluster-admin</literal> privileges, you have a choice of creating a network policy in any namespace in the cluster directly in YAML or from a form in the web console.</simpara>
</note>
</section>
<section xml:id="nw-networkpolicy-multitenant-isolation_post-install-network-configuration">
<title>Configuring multitenant isolation by using network policy</title>
<simpara>You can configure your project to isolate it from pods and services in other
project namespaces.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OVN-Kubernetes network plugin or the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.
This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>NetworkPolicy</literal> objects:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>A policy named <literal>allow-from-openshift-ingress</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-ingress
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          policy-group.network.openshift.io/ingress: ""
  podSelector: {}
  policyTypes:
  - Ingress
EOF</programlisting>
<note>
<simpara><literal>policy-group.network.openshift.io/ingress: ""</literal> is the preferred namespace selector label for OpenShift SDN. You can use the <literal>network.openshift.io/policy-group: ingress</literal> namespace selector label, but this is a legacy label.</simpara>
</note>
</listitem>
<listitem>
<simpara>A policy named <literal>allow-from-openshift-monitoring</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-openshift-monitoring
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
EOF</programlisting>
</listitem>
<listitem>
<simpara>A policy named <literal>allow-same-namespace</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-same-namespace
spec:
  podSelector:
  ingress:
  - from:
    - podSelector: {}
EOF</programlisting>
</listitem>
<listitem>
<simpara>A policy named <literal>allow-from-kube-apiserver-operator</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-kube-apiserver-operator
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          kubernetes.io/metadata.name: openshift-kube-apiserver-operator
      podSelector:
        matchLabels:
          app: kube-apiserver-operator
  policyTypes:
  - Ingress
EOF</programlisting>
<simpara>For more details, see <link xlink:href="https://access.redhat.com/solutions/6964520">New <literal>kube-apiserver-operator</literal> webhook controller validating health of webhook</link>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: To confirm that the network policies exist in your current project, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe networkpolicy</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">Name:         allow-from-openshift-ingress
Namespace:    example1
Created on:   2020-06-09 00:28:17 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: ingress
  Not affecting egress traffic
  Policy Types: Ingress


Name:         allow-from-openshift-monitoring
Namespace:    example1
Created on:   2020-06-09 00:29:57 -0400 EDT
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Spec:
  PodSelector:     &lt;none&gt; (Allowing the specific traffic to all pods in this namespace)
  Allowing ingress traffic:
    To Port: &lt;any&gt; (traffic allowed to all ports)
    From:
      NamespaceSelector: network.openshift.io/policy-group: monitoring
  Not affecting egress traffic
  Policy Types: Ingress</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="post-install-nw-networkpolicy-creating-default-networkpolicy-objects-for-a-new-project">
<title>Creating default network policies for a new project</title>
<simpara>As a cluster administrator, you can modify the new project template to
automatically include <literal>NetworkPolicy</literal> objects when you create a new project.</simpara>
</section>
<section xml:id="modifying-template-for-new-projects_post-install-network-configuration">
<title>Modifying the template for new projects</title>
<simpara>As a cluster administrator, you can modify the default project template so that
new projects are created using your custom requirements.</simpara>
<simpara>To create your own custom project template:</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Generate the default project template:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm create-bootstrap-project-template -o yaml &gt; template.yaml</programlisting>
</listitem>
<listitem>
<simpara>Use a text editor to modify the generated <literal>template.yaml</literal> file by adding
objects or modifying existing objects.</simpara>
</listitem>
<listitem>
<simpara>The project template must be created in the <literal>openshift-config</literal> namespace. Load
your modified template:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f template.yaml -n openshift-config</programlisting>
</listitem>
<listitem>
<simpara>Edit the project configuration resource using the web console or CLI.</simpara>
<itemizedlist>
<listitem>
<simpara>Using the web console:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Cluster Settings</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Configuration</emphasis> to view all configuration resources.</simpara>
</listitem>
<listitem>
<simpara>Find the entry for <emphasis role="strong">Project</emphasis> and click <emphasis role="strong">Edit YAML</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Using the CLI:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Edit the <literal>project.config.openshift.io/cluster</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit project.config.openshift.io/cluster</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Update the <literal>spec</literal> section to include the <literal>projectRequestTemplate</literal> and <literal>name</literal>
parameters, and set the name of your uploaded project template. The default name
is <literal>project-request</literal>.</simpara>
<formalpara>
<title>Project configuration resource with custom project template</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Project
metadata:
  ...
spec:
  projectRequestTemplate:
    name: &lt;template_name&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>After you save your changes, create a new project to verify that your changes
were successfully applied.</simpara>
</listitem>
</orderedlist>
<section xml:id="nw-networkpolicy-project-defaults_post-install-network-configuration">
<title>Adding network policies to the new project template</title>
<simpara>As a cluster administrator, you can add network policies to the default template for new projects.
OpenShift Container Platform will automatically create all the <literal>NetworkPolicy</literal> objects specified in the template in the project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster uses a default CNI network plugin that supports <literal>NetworkPolicy</literal> objects, such as the OpenShift SDN network plugin with <literal>mode: NetworkPolicy</literal> set. This mode is the default for OpenShift SDN.</simpara>
</listitem>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must log in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You must have created a custom default project template for new projects.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the default template for a new project by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit template &lt;project_template&gt; -n openshift-config</programlisting>
<simpara>Replace <literal>&lt;project_template&gt;</literal> with the name of the default template that you
configured for your cluster. The default template name is <literal>project-request</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the template, add each <literal>NetworkPolicy</literal> object as an element to the <literal>objects</literal> parameter. The <literal>objects</literal> parameter accepts a collection of one or more objects.</simpara>
<simpara>In the following example, the <literal>objects</literal> parameter collection includes several <literal>NetworkPolicy</literal> objects.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">objects:
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-same-namespace
  spec:
    podSelector: {}
    ingress:
    - from:
      - podSelector: {}
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-openshift-ingress
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            network.openshift.io/policy-group: ingress
    podSelector: {}
    policyTypes:
    - Ingress
- apiVersion: networking.k8s.io/v1
  kind: NetworkPolicy
  metadata:
    name: allow-from-kube-apiserver-operator
  spec:
    ingress:
    - from:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: openshift-kube-apiserver-operator
        podSelector:
          matchLabels:
            app: kube-apiserver-operator
    policyTypes:
    - Ingress
...</programlisting>
</listitem>
<listitem>
<simpara>Optional: Create a new project to confirm that your network policy objects are created successfully by running the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a new project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project &lt;project&gt; <co xml:id="CO144-1"/></programlisting>
<calloutlist>
<callout arearefs="CO144-1">
<para>Replace <literal>&lt;project&gt;</literal> with the name for the project you are creating.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Confirm that the network policy objects in the new project template exist in the new project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networkpolicy
NAME                           POD-SELECTOR   AGE
allow-from-openshift-ingress   &lt;none&gt;         7s
allow-from-same-namespace      &lt;none&gt;         7s</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="ossm-supported-configurations_post-install-network-configuration">
<title>Supported configurations</title>
<simpara>The following configurations are supported for the current release of Red Hat OpenShift Service Mesh.</simpara>
<section xml:id="ossm-supported-platforms_post-install-network-configuration">
<title>Supported platforms</title>
<simpara>The Red Hat OpenShift Service Mesh Operator supports multiple versions of the <literal>ServiceMeshControlPlane</literal> resource. Version 2.4 Service Mesh control planes are supported on the following platform versions:</simpara>
<itemizedlist>
<listitem>
<simpara>Red Hat OpenShift Container Platform version 4.10 or later.</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenShift Dedicated version 4.</simpara>
</listitem>
<listitem>
<simpara>Azure Red Hat OpenShift (ARO) version 4.</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenShift Service on AWS (ROSA).</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-unsupported-configurations_post-install-network-configuration">
<title>Unsupported configurations</title>
<simpara>Explicitly unsupported cases include:</simpara>
<itemizedlist>
<listitem>
<simpara>OpenShift Online is not supported for Red Hat OpenShift Service Mesh.</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenShift Service Mesh does not support the management of microservices outside the cluster where Service Mesh is running.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-supported-configurations-networks_post-install-network-configuration">
<title>Supported network configurations</title>
<simpara>Red Hat OpenShift Service Mesh supports the following network configurations.</simpara>
<itemizedlist>
<listitem>
<simpara>OpenShift-SDN</simpara>
</listitem>
<listitem>
<simpara>OVN-Kubernetes is available on all supported versions of OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>Third-Party Container Network Interface (CNI) plugins that have been certified on OpenShift Container Platform and passed Service Mesh conformance testing. See <link xlink:href="https://access.redhat.com/articles/5436171">Certified OpenShift CNI Plug-ins</link> for more information.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-supported-configurations-sm_post-install-network-configuration">
<title>Supported configurations for Service Mesh</title>
<itemizedlist>
<listitem>
<simpara>This release of Red Hat OpenShift Service Mesh is only available on OpenShift Container Platform x86_64, IBM Z&#174;, and IBM Power&#174;.</simpara>
<itemizedlist>
<listitem>
<simpara>IBM Z&#174; is only supported on OpenShift Container Platform 4.10 and later.</simpara>
</listitem>
<listitem>
<simpara>IBM Power&#174; is only supported on OpenShift Container Platform 4.10 and later.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Configurations where all Service Mesh components are contained within a single OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Configurations that do not integrate external services such as virtual machines.</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenShift Service Mesh does not support <literal>EnvoyFilter</literal> configuration except where explicitly documented.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-supported-configurations-kiali_post-install-network-configuration">
<title>Supported configurations for Kiali</title>
<itemizedlist>
<listitem>
<simpara>The Kiali console is only supported on the two most recent releases of the Google Chrome, Microsoft Edge, Mozilla Firefox, or Apple Safari browsers.</simpara>
</listitem>
<listitem>
<simpara>The <literal>openshift</literal> authentication strategy is the only supported authentication configuration when Kiali is deployed with Red Hat OpenShift Service Mesh (OSSM). The <literal>openshift</literal> strategy controls access based on the individual&#8217;s role-based access control (RBAC) roles of the OpenShift Container Platform.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-supported-configurations-jaeger_post-install-network-configuration">
<title>Supported configurations for Distributed Tracing</title>
<itemizedlist>
<listitem>
<simpara>Jaeger agent as a sidecar is the only supported configuration for Jaeger. Jaeger as a daemonset is not supported for multitenant installations or OpenShift Dedicated.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-supported-configurations-webassembly_post-install-network-configuration">
<title>Supported WebAssembly module</title>
<itemizedlist>
<listitem>
<simpara>3scale WebAssembly is the only provided WebAssembly module. You can create custom WebAssembly modules.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="ossm-installation-activities_post-install-network-configuration">
<title>Operator overview</title>
<simpara>Red Hat OpenShift Service Mesh requires the following Operators:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">OpenShift Elasticsearch</emphasis> - (Optional) Provides database storage for tracing and logging with the distributed tracing platform (Jaeger). It is based on the open source <link xlink:href="https://www.elastic.co/">Elasticsearch</link> project.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Red Hat OpenShift distributed tracing platform (Jaeger)</emphasis> - Provides distributed tracing to monitor and troubleshoot transactions in complex distributed systems. It is based on the open source <link xlink:href="https://www.jaegertracing.io/">Jaeger</link> project.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Kiali Operator provided by Red Hat</emphasis> - Provides observability for your service mesh. You can view configurations, monitor traffic, and analyze traces in a single console. It is based on the open source <link xlink:href="https://www.kiali.io/">Kiali</link> project.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Red Hat OpenShift Service Mesh</emphasis> - Allows you to connect, secure, control, and observe the microservices that comprise your applications. The Service Mesh Operator defines and monitors the <literal>ServiceMeshControlPlane</literal> resources that manage the deployment, updating, and deletion of the Service Mesh components. It is based on the open source <link xlink:href="https://istio.io/">Istio</link> project.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara><link xlink:href="../service_mesh/v2x/installing-ossm.xml#installing-ossm">Install Red Hat OpenShift Service Mesh</link> in your OpenShift Container Platform environment.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-installationrouting-optimization">
<title>Optimizing routing</title>
<simpara>The OpenShift Container Platform HAProxy router can be scaled or configured to optimize performance.</simpara>
<section xml:id="baseline-router-performance_post-install-network-configuration">
<title>Baseline Ingress Controller (router) performance</title>
<simpara>The OpenShift Container Platform Ingress Controller, or router, is the ingress point for ingress traffic for applications and services that are configured using routes and ingresses.</simpara>
<simpara>When evaluating a single HAProxy router performance in terms of HTTP requests handled per second, the performance varies depending on many factors. In particular:</simpara>
<itemizedlist>
<listitem>
<simpara>HTTP keep-alive/close mode</simpara>
</listitem>
<listitem>
<simpara>Route type</simpara>
</listitem>
<listitem>
<simpara>TLS session resumption client support</simpara>
</listitem>
<listitem>
<simpara>Number of concurrent connections per target route</simpara>
</listitem>
<listitem>
<simpara>Number of target routes</simpara>
</listitem>
<listitem>
<simpara>Back end server page size</simpara>
</listitem>
<listitem>
<simpara>Underlying infrastructure (network/SDN solution, CPU, and so on)</simpara>
</listitem>
</itemizedlist>
<simpara>While performance in your specific environment will vary, Red Hat lab tests on a public cloud instance of size 4 vCPU/16GB RAM. A single HAProxy router handling 100 routes terminated by backends serving 1kB static pages is able to handle the following number of transactions per second.</simpara>
<simpara>In HTTP keep-alive mode scenarios:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Encryption</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">LoadBalancerService</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">HostNetwork</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>none</simpara></entry>
<entry align="left" valign="top"><simpara>21515</simpara></entry>
<entry align="left" valign="top"><simpara>29622</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>edge</simpara></entry>
<entry align="left" valign="top"><simpara>16743</simpara></entry>
<entry align="left" valign="top"><simpara>22913</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>passthrough</simpara></entry>
<entry align="left" valign="top"><simpara>36786</simpara></entry>
<entry align="left" valign="top"><simpara>53295</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>re-encrypt</simpara></entry>
<entry align="left" valign="top"><simpara>21583</simpara></entry>
<entry align="left" valign="top"><simpara>25198</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>In HTTP close (no keep-alive) scenarios:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Encryption</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">LoadBalancerService</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">HostNetwork</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>none</simpara></entry>
<entry align="left" valign="top"><simpara>5719</simpara></entry>
<entry align="left" valign="top"><simpara>8273</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>edge</simpara></entry>
<entry align="left" valign="top"><simpara>2729</simpara></entry>
<entry align="left" valign="top"><simpara>4069</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>passthrough</simpara></entry>
<entry align="left" valign="top"><simpara>4121</simpara></entry>
<entry align="left" valign="top"><simpara>5344</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>re-encrypt</simpara></entry>
<entry align="left" valign="top"><simpara>2320</simpara></entry>
<entry align="left" valign="top"><simpara>2941</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>The default Ingress Controller configuration was used with the <literal>spec.tuningOptions.threadCount</literal> field set to <literal>4</literal>. Two different endpoint publishing strategies were tested: Load Balancer Service and Host Network. TLS session resumption was used for encrypted routes. With HTTP keep-alive, a single HAProxy router is capable of saturating a 1 Gbit NIC at page sizes as small as 8 kB.</simpara>
<simpara>When running on bare metal with modern processors, you can expect roughly twice the performance of the public cloud instance above. This overhead is introduced by the virtualization layer in place on public clouds and holds mostly true for private cloud-based virtualization as well. The following table is a guide to how many applications to use behind the router:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="66.6667*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Number of applications</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Application type</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>5-10</simpara></entry>
<entry align="left" valign="top"><simpara>static file/web server or caching proxy</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>100-1000</simpara></entry>
<entry align="left" valign="top"><simpara>applications generating dynamic content</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>In general, HAProxy can support routes for up to 1000 applications, depending on the technology in use. Ingress Controller performance might be limited by the
capabilities and performance of the applications behind it, such as language or static versus dynamic content.</simpara>
<simpara>Ingress, or router, sharding should be used to serve more routes towards applications and help horizontally scale the routing tier.</simpara>
</section>
<section xml:id="ingress-liveness-readiness-startup-probes_post-install-network-configuration">
<title>Configuring Ingress Controller liveness, readiness, and startup probes</title>
<simpara>Cluster administrators can configure the timeout values for the kubelet&#8217;s liveness, readiness, and startup probes for router deployments that are managed by the OpenShift Container Platform Ingress Controller (router). The liveness and readiness probes of the router use the default timeout value
of 1 second, which is too brief when networking or runtime performance is severely degraded. Probe timeouts can cause unwanted router restarts that interrupt application connections. The ability to set larger timeout values can reduce the risk of unnecessary and unwanted restarts.</simpara>
<simpara>You can update the <literal>timeoutSeconds</literal> value on the <literal>livenessProbe</literal>, <literal>readinessProbe</literal>, and <literal>startupProbe</literal> parameters of the router container.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>livenessProbe</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>livenessProbe</literal> reports to the kubelet whether a pod is dead and needs to be restarted.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>readinessProbe</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>readinessProbe</literal> reports whether a pod is healthy or unhealthy. When the readiness probe reports an unhealthy pod, then the kubelet marks the pod as not ready to accept traffic. Subsequently, the endpoints for that pod are marked as not ready, and this status propagates to the kube-proxy. On cloud platforms with a configured load balancer, the kube-proxy communicates to the cloud load-balancer not to send traffic to the node with that pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>startupProbe</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>startupProbe</literal> gives the router pod up to 2 minutes to initialize before the kubelet begins sending the router liveness and readiness probes.  This initialization time can prevent routers with many routes or endpoints from prematurely restarting.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<important>
<simpara>The timeout configuration option is an advanced tuning technique that can be used to work around issues. However, these issues should eventually be diagnosed and possibly a support case or <link xlink:href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Summary&amp;issuetype=1&amp;priority=10200&amp;versions=12385624">Jira issue</link> opened for any issues that causes probes to time out.</simpara>
</important>
<simpara>The following example demonstrates how you can directly patch the default router deployment to set a 5-second timeout for the liveness and readiness probes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress patch deploy/router-default --type=strategic --patch='{"spec":{"template":{"spec":{"containers":[{"name":"router","livenessProbe":{"timeoutSeconds":5},"readinessProbe":{"timeoutSeconds":5}}]}}}}'</programlisting>
<formalpara>
<title>Verification</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress describe deploy/router-default | grep -e Liveness: -e Readiness:
    Liveness:   http-get http://:1936/healthz delay=0s timeout=5s period=10s #success=1 #failure=3
    Readiness:  http-get http://:1936/healthz/ready delay=0s timeout=5s period=10s #success=1 #failure=3</programlisting>
</para>
</formalpara>
</section>
<section xml:id="configuring-haproxy-interval_post-install-network-configuration">
<title>Configuring HAProxy reload interval</title>
<simpara>When you update a route or an endpoint associated with a route, OpenShift Container Platform router updates the configuration for HAProxy. Then, HAProxy reloads the updated configuration for those changes to take effect. When HAProxy reloads, it generates a new process that handles new connections using the updated configuration.</simpara>
<simpara>HAProxy keeps the old process running to handle existing connections until those connections are all closed. When old processes have long-lived connections, these processes can accumulate and consume resources.</simpara>
<simpara>The default minimum HAProxy reload interval is five seconds. You can configure an Ingress Controller using its <literal>spec.tuningOptions.reloadInterval</literal> field to set a longer minimum reload interval.</simpara>
<warning>
<simpara>Setting a large value for the minimum HAProxy reload interval can cause latency in observing updates to routes and their endpoints. To lessen the risk, avoid setting a value larger than the tolerable latency for updates.</simpara>
</warning>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Change the minimum HAProxy reload interval of the default Ingress Controller to 15 seconds by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch='{"spec":{"tuningOptions":{"reloadInterval":"15s"}}}'</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-installation-osp-fips">
<title>Postinstallation RHOSP network configuration</title>
<simpara>You can configure some aspects of an OpenShift Container Platform on Red Hat OpenStack Platform (RHOSP) cluster after installation.</simpara>
<section xml:id="installation-osp-configuring-api-floating-ip_post-install-network-configuration">
<title>Configuring application access with floating IP addresses</title>
<simpara>After you install OpenShift Container Platform, configure Red Hat OpenStack Platform (RHOSP) to allow application network traffic.</simpara>
<note>
<simpara>You do not need to perform this procedure if you provided values for <literal>platform.openstack.apiFloatingIP</literal> and <literal>platform.openstack.ingressFloatingIP</literal> in the <literal>install-config.yaml</literal> file, or <literal>os_api_fip</literal> and <literal>os_ingress_fip</literal> in the <literal>inventory.yaml</literal> playbook, during installation. The floating IP addresses are already set.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>OpenShift Container Platform cluster must be installed</simpara>
</listitem>
<listitem>
<simpara>Floating IP addresses are enabled as described in the OpenShift Container Platform on RHOSP installation documentation.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>After you install the OpenShift Container Platform cluster, attach a floating IP address to the ingress port:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Show the port:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack port show &lt;cluster_name&gt;-&lt;cluster_ID&gt;-ingress-port</programlisting>
</listitem>
<listitem>
<simpara>Attach the port to the IP address:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack floating ip set --port &lt;ingress_port_ID&gt; &lt;apps_FIP&gt;</programlisting>
</listitem>
<listitem>
<simpara>Add a wildcard <literal>A</literal> record for <literal>*apps.</literal> to your DNS file:</simpara>
<programlisting language="dns" linenumbering="unnumbered">*.apps.&lt;cluster_name&gt;.&lt;base_domain&gt;  IN  A  &lt;apps_FIP&gt;</programlisting>
</listitem>
</orderedlist>
<note>
<simpara>If you do not control the DNS server but want to enable application access for non-production purposes, you can add these hostnames to <literal>/etc/hosts</literal>:</simpara>
<programlisting language="dns" linenumbering="unnumbered">&lt;apps_FIP&gt; console-openshift-console.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; integrated-oauth-server-openshift-authentication.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; oauth-openshift.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; prometheus-k8s-openshift-monitoring.apps.&lt;cluster name&gt;.&lt;base domain&gt;
&lt;apps_FIP&gt; &lt;app name&gt;.apps.&lt;cluster name&gt;.&lt;base domain&gt;</programlisting>
</note>
</section>
<section xml:id="nw-osp-enabling-ovs-offload_post-install-network-configuration">
<title>Enabling OVS hardware offloading</title>
<simpara>For clusters that run on Red Hat OpenStack Platform (RHOSP), you can enable <link xlink:href="https://www.openvswitch.org/">Open vSwitch (OVS)</link> hardware offloading.</simpara>
<simpara>OVS is a multi-layer virtual switch that enables large-scale, multi-server network virtualization.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed a cluster on RHOSP that is configured for single-root input/output virtualization (SR-IOV).</simpara>
</listitem>
<listitem>
<simpara>You installed the SR-IOV Network Operator on your cluster.</simpara>
</listitem>
<listitem>
<simpara>You created two <literal>hw-offload</literal> type virtual function (VF) interfaces on your cluster.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Application layer gateway flows are broken in OpenShift Container Platform version 4.10, 4.11, and 4.12. Also, you cannot offload the application layer gateway flow for OpenShift Container Platform version 4.13.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> policy for the two <literal>hw-offload</literal> type VF interfaces that are on your cluster:</simpara>
<formalpara>
<title>The first virtual function interface</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy <co xml:id="CO145-1"/>
metadata:
  name: "hwoffload9"
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    pfNames: <co xml:id="CO145-2"/>
    - ens6
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: "hwoffload9"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO145-1">
<para>Insert the <literal>SriovNetworkNodePolicy</literal> value here.</para>
</callout>
<callout arearefs="CO145-2">
<para>Both interfaces must include physical function (PF) names.</para>
</callout>
</calloutlist>
<formalpara>
<title>The second virtual function interface</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy <co xml:id="CO146-1"/>
metadata:
  name: "hwoffload10"
  namespace: openshift-sriov-network-operator
spec:
  deviceType: netdevice
  isRdma: true
  nicSelector:
    pfNames: <co xml:id="CO146-2"/>
    - ens5
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: 'true'
  numVfs: 1
  priority: 99
  resourceName: "hwoffload10"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO146-1">
<para>Insert the <literal>SriovNetworkNodePolicy</literal> value here.</para>
</callout>
<callout arearefs="CO146-2">
<para>Both interfaces must include physical function (PF) names.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create <literal>NetworkAttachmentDefinition</literal> resources for the two interfaces:</simpara>
<formalpara>
<title>A <literal>NetworkAttachmentDefinition</literal> resource for the first interface</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload9
  name: hwoffload9
  namespace: default
spec:
    config: '{ "cniVersion":"0.3.1", "name":"hwoffload9","type":"host-device","device":"ens6"
    }'</programlisting>
</para>
</formalpara>
<formalpara>
<title>A <literal>NetworkAttachmentDefinition</literal> resource for the second interface</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  annotations:
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload10
  name: hwoffload10
  namespace: default
spec:
    config: '{ "cniVersion":"0.3.1", "name":"hwoffload10","type":"host-device","device":"ens5"
    }'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the interfaces that you created with a pod. For example:</simpara>
<formalpara>
<title>A pod that uses the two OVS offload interfaces</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Pod
metadata:
  name: dpdk-testpmd
  namespace: default
  annotations:
    irq-load-balancing.crio.io: disable
    cpu-quota.crio.io: disable
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload9
    k8s.v1.cni.cncf.io/resourceName: openshift.io/hwoffload10
spec:
  restartPolicy: Never
  containers:
  - name: dpdk-testpmd
    image: quay.io/krister/centos8_nfv-container-dpdk-testpmd:latest</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-osp-hardware-offload-attaching-network_post-install-network-configuration">
<title>Attaching an OVS hardware offloading network</title>
<simpara>You can attach an Open vSwitch (OVS) hardware offloading network to your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster is installed and running.</simpara>
</listitem>
<listitem>
<simpara>You provisioned an OVS hardware offloading network on Red Hat OpenStack Platform (RHOSP) to use with your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a file named <literal>network.yaml</literal> from the following template:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  additionalNetworks:
  - name: hwoffload1
    namespace: cnf
    rawCNIConfig: '{ "cniVersion": "0.3.1", "name": "hwoffload1", "type": "host-device","pciBusId": "0000:00:05.0", "ipam": {}}' <co xml:id="CO147-1"/>
    type: Raw</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>pciBusId</literal></term>
<listitem>
<simpara>Specifies the device that is connected to the offloading network. If you do not have it, you can find this value by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe SriovNetworkNodeState -n openshift-sriov-network-operator</programlisting>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>From a command line, enter the following command to patch your cluster with the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f network.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-osp-pod-connections-ipv6_post-install-network-configuration">
<title>Enabling IPv6 connectivity to pods on RHOSP</title>
<simpara>To enable IPv6 connectivity between pods that have additional networks that are on different nodes, disable port security for the IPv6 port of the server. Disabling port security obviates the need to create allowed address pairs for each IPv6 address that is assigned to pods and enables traffic on the security group.</simpara>
<important>
<simpara>Only the following IPv6 additional network configurations are supported:</simpara>
<itemizedlist>
<listitem>
<simpara>SLAAC and host-device</simpara>
</listitem>
<listitem>
<simpara>SLAAC and MACVLAN</simpara>
</listitem>
<listitem>
<simpara>DHCP stateless and host-device</simpara>
</listitem>
<listitem>
<simpara>DHCP stateless and MACVLAN</simpara>
</listitem>
</itemizedlist>
</important>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>On a command line, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ openstack port set --no-security-group --disable-port-security &lt;compute_ipv6_port&gt;</programlisting>
<important>
<simpara>This command removes security groups from the port and disables port security. Traffic restrictions are removed entirely from the port.</simpara>
</important>
</listitem>
</itemizedlist>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;compute_ipv6_port&gt;</term>
<listitem>
<simpara>Specifies the IPv6 port of the compute server.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="nw-osp-pod-adding-connections-ipv6_post-install-network-configuration">
<title>Adding IPv6 connectivity to pods on RHOSP</title>
<simpara>After you enable IPv6 connectivity in pods, add connectivity to them by using a Container Network Interface (CNI) configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To edit the Cluster Network Operator (CNO), enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit networks.operator.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>Specify your CNI configuration under the <literal>spec</literal> field. For example, the following configuration uses a SLAAC address mode with MACVLAN:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">...
spec:
  additionalNetworks:
  - name: ipv6
    namespace: ipv6 <co xml:id="CO147-2"/>
    rawCNIConfig: '{ "cniVersion": "0.3.1", "name": "ipv6", "type": "macvlan", "master": "ens4"}' <co xml:id="CO147-3"/>
    type: Raw</programlisting>
<calloutlist>
<callout arearefs="CO147-1 CO147-2">
<para>Be sure to create pods in the same namespace.</para>
</callout>
<callout arearefs="CO147-3">
<para>The interface in the network attachment <literal>"master"</literal> field can differ from <literal>"ens4"</literal> when more networks are configured or when a different kernel driver is used.</para>
</callout>
</calloutlist>
<note>
<simpara>If you are using stateful address mode, include the IP Address Management (IPAM) in the CNI configuration.</simpara>
<simpara>DHCPv6 is not supported by Multus.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save your changes and quit the text editor to commit your changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>On a command line, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definitions -A</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE       NAME            AGE
ipv6            ipv6            21h</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<simpara>You can now create pods that have secondary IPv6 connections.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../networking/multiple_networks/configuring-additional-network.xml#configuring-additional-network_configuration-additional-network-attachment">Configuration for an additional network attachment</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nw-osp-pod-creating-ipv6_post-install-network-configuration">
<title>Create pods that have IPv6 connectivity on RHOSP</title>
<simpara>After you enable IPv6 connectivty for pods and add it to them, create pods that have secondary IPv6 connections.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Define pods that use your IPv6 namespace and the annotation <literal>k8s.v1.cni.cncf.io/networks: &lt;additional_network_name&gt;</literal>, where <literal>&lt;additional_network_name</literal> is the name of the additional network. For example, as part of a <literal>Deployment</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-openshift
  namespace: ipv6
spec:
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
         - labelSelector:
            matchExpressions:
            - key: app
              operator: In
              values:
              - hello-openshift
  replicas: 2
  selector:
    matchLabels:
      app: hello-openshift
  template:
    metadata:
      labels:
        app: hello-openshift
      annotations:
        k8s.v1.cni.cncf.io/networks: ipv6
    spec:
      securityContext:
        runAsNonRoot: true
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: hello-openshift
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
        image: quay.io/openshift/origin-hello-openshift
        ports:
        - containerPort: 8080</programlisting>
</listitem>
<listitem>
<simpara>Create the pod. For example, on a command line, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;ipv6_enabled_resource&gt;</programlisting>
</listitem>
</orderedlist>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;ipv6_enabled_resource&gt;</term>
<listitem>
<simpara>Specifies the file that contains your resource definition.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
</chapter>
<chapter xml:id="post-install-storage-configuration">
<title>Postinstallation storage configuration</title>

<simpara>After installing OpenShift Container Platform, you can further expand and customize your
cluster to your requirements, including storage configuration.</simpara>
<section xml:id="post-install-dynamic-provisioning">
<title>Dynamic provisioning</title>
<section xml:id="about_post-install-storage-configuration">
<title>About dynamic provisioning</title>
<simpara>The <literal>StorageClass</literal> resource object describes and classifies storage that can
be requested, as well as provides a means for passing parameters for
dynamically provisioned storage on demand. <literal>StorageClass</literal> objects can also
serve as a management mechanism for controlling different levels of
storage and access to the storage. Cluster Administrators (<literal>cluster-admin</literal>)
 or Storage Administrators (<literal>storage-admin</literal>) define and create the
<literal>StorageClass</literal> objects that users can request without needing any detailed
knowledge about the underlying storage volume sources.</simpara>
<simpara>The OpenShift Container Platform persistent volume framework enables this functionality
and allows administrators to provision a cluster with persistent storage.
The framework also gives users a way to request those resources without
having any knowledge of the underlying infrastructure.</simpara>
<simpara>Many storage types are available for use as persistent volumes in
OpenShift Container Platform. While all of them can be statically provisioned by an
administrator, some types of storage are created dynamically using the
built-in provider and plugin APIs.</simpara>
</section>
<section xml:id="available-plug-ins_post-install-storage-configuration">
<title>Available dynamic provisioning plugins</title>
<simpara>OpenShift Container Platform provides the following provisioner plugins, which have
generic implementations for dynamic provisioning that use the cluster&#8217;s
configured provider&#8217;s API to create new storage resources:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Storage type</entry>
<entry align="left" valign="top">Provisioner plugin name</entry>
<entry align="left" valign="top">Notes</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Red Hat OpenStack Platform (RHOSP) Cinder</simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubernetes.io/cinder</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RHOSP Manila Container Storage Interface (CSI)</simpara></entry>
<entry align="left" valign="top"><simpara><literal>manila.csi.openstack.org</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Once installed, the OpenStack Manila CSI Driver Operator and ManilaDriver automatically create the required storage classes for all available Manila share types needed for dynamic provisioning.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Amazon Elastic Block Store (Amazon EBS)</simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubernetes.io/aws-ebs</literal></simpara></entry>
<entry align="left" valign="top"><simpara>For dynamic provisioning when using multiple clusters in different zones,
tag each node with <literal>Key=kubernetes.io/cluster/&lt;cluster_name&gt;,Value=&lt;cluster_id&gt;</literal>
where <literal>&lt;cluster_name&gt;</literal> and <literal>&lt;cluster_id&gt;</literal> are unique per cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Azure Disk</simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubernetes.io/azure-disk</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Azure File</simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubernetes.io/azure-file</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The <literal>persistent-volume-binder</literal> service account requires permissions to create
and get secrets to store the Azure storage account and keys.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>GCE Persistent Disk (gcePD)</simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubernetes.io/gce-pd</literal></simpara></entry>
<entry align="left" valign="top"><simpara>In multi-zone configurations, it is advisable to run one OpenShift Container Platform
cluster per GCE project to avoid PVs from being created in zones where
no node in the current cluster exists.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>IBM Power&#174; Virtual Server Block</simpara></entry>
<entry align="left" valign="top"><simpara><literal>powervs.csi.ibm.com</literal></simpara></entry>
<entry align="left" valign="top"><simpara>After installation, the IBM Power&#174; Virtual Server Block CSI Driver Operator and IBM Power&#174; Virtual Server Block CSI Driver automatically create the required storage classes for dynamic provisioning.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="https://www.vmware.com/support/vsphere.html">VMware vSphere</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubernetes.io/vsphere-volume</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<important>
<simpara>Any chosen provisioner plugin also requires configuration for the relevant
cloud, host, or third-party provider as per the relevant documentation.</simpara>
</important>
</section>
</section>
<section xml:id="defining-storage-classes_post-install-storage-configuration">
<title>Defining a storage class</title>
<simpara><literal>StorageClass</literal> objects are currently a globally scoped object and must be
created by <literal>cluster-admin</literal> or <literal>storage-admin</literal> users.</simpara>
<important>
<simpara>The Cluster Storage Operator might install a default storage class depending
on the platform in use. This storage class is owned and controlled by the
Operator. It cannot be deleted or modified beyond defining annotations
and labels. If different behavior is desired, you must define a custom
storage class.</simpara>
</important>
<simpara>The following sections describe the basic definition for a
<literal>StorageClass</literal> object and specific examples for each of the supported plugin types.</simpara>
<section xml:id="basic-storage-class-definition_post-install-storage-configuration">
<title>Basic StorageClass object definition</title>
<simpara>The following resource shows the parameters and default values that you
use to configure a storage class. This example uses the AWS
ElasticBlockStore (EBS) object definition.</simpara>
<formalpara>
<title>Sample <literal>StorageClass</literal> definition</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass <co xml:id="CO148-1"/>
apiVersion: storage.k8s.io/v1 <co xml:id="CO148-2"/>
metadata:
  name: &lt;storage-class-name&gt; <co xml:id="CO148-3"/>
  annotations: <co xml:id="CO148-4"/>
    storageclass.kubernetes.io/is-default-class: 'true'
    ...
provisioner: kubernetes.io/aws-ebs <co xml:id="CO148-5"/>
parameters: <co xml:id="CO148-6"/>
  type: gp3
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO148-1">
<para>(required) The API object type.</para>
</callout>
<callout arearefs="CO148-2">
<para>(required) The current apiVersion.</para>
</callout>
<callout arearefs="CO148-3">
<para>(required) The name of the storage class.</para>
</callout>
<callout arearefs="CO148-4">
<para>(optional) Annotations for the storage class.</para>
</callout>
<callout arearefs="CO148-5">
<para>(required) The type of provisioner associated with this storage class.</para>
</callout>
<callout arearefs="CO148-6">
<para>(optional) The parameters required for the specific provisioner, this
will change from plugin to plug-iin.</para>
</callout>
</calloutlist>
</section>
<section xml:id="storage-class-annotations_post-install-storage-configuration">
<title>Storage class annotations</title>
<simpara>To set a storage class as the cluster-wide default, add
the following annotation to your storage class metadata:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">storageclass.kubernetes.io/is-default-class: "true"</programlisting>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
...</programlisting>
<simpara>This enables any persistent volume claim (PVC) that does not specify a
specific storage class to automatically be provisioned through the
default storage class. However, your cluster can have more than one storage class, but only one of them can be the default storage class.</simpara>
<note>
<simpara>The beta annotation <literal>storageclass.beta.kubernetes.io/is-default-class</literal> is
still working; however, it will be removed in a future release.</simpara>
</note>
<simpara>To set a storage class description, add the following annotation
to your storage class metadata:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kubernetes.io/description: My Storage Class Description</programlisting>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubernetes.io/description: My Storage Class Description
...</programlisting>
</section>
<section xml:id="openstack-cinder-storage-class_post-install-storage-configuration">
<title>RHOSP Cinder object definition</title>
<formalpara>
<title>cinder-storageclass.yaml</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;storage-class-name&gt; <co xml:id="CO149-1"/>
provisioner: kubernetes.io/cinder
parameters:
  type: fast  <co xml:id="CO149-2"/>
  availability: nova <co xml:id="CO149-3"/>
  fsType: ext4 <co xml:id="CO149-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO149-1">
<para>Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.</para>
</callout>
<callout arearefs="CO149-2">
<para>Volume type created in Cinder. Default is empty.</para>
</callout>
<callout arearefs="CO149-3">
<para>Availability Zone. If not specified, volumes are generally
round-robined across all active zones where the OpenShift Container Platform cluster
has a node.</para>
</callout>
<callout arearefs="CO149-4">
<para>File system that is created on dynamically provisioned volumes. This
value is copied to the <literal>fsType</literal> field of dynamically provisioned
persistent volumes and the file system is created when the volume is
mounted for the first time. The default value is <literal>ext4</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="aws-definition_post-install-storage-configuration">
<title>AWS Elastic Block Store (EBS) object definition</title>
<formalpara>
<title>aws-ebs-storageclass.yaml</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;storage-class-name&gt; <co xml:id="CO150-1"/>
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1 <co xml:id="CO150-2"/>
  iopsPerGB: "10" <co xml:id="CO150-3"/>
  encrypted: "true" <co xml:id="CO150-4"/>
  kmsKeyId: keyvalue <co xml:id="CO150-5"/>
  fsType: ext4 <co xml:id="CO150-6"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO150-1">
<para>(required) Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.</para>
</callout>
<callout arearefs="CO150-2">
<para>(required) Select from <literal>io1</literal>, <literal>gp3</literal>, <literal>sc1</literal>, <literal>st1</literal>. The default is <literal>gp3</literal>.
See the
<link xlink:href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">AWS documentation</link>
for valid Amazon Resource Name (ARN) values.</para>
</callout>
<callout arearefs="CO150-3">
<para>Optional: Only for <emphasis role="strong">io1</emphasis> volumes. I/O operations per second per GiB.
The AWS volume plugin multiplies this with the size of the requested
volume to compute IOPS of the volume. The value cap is 20,000 IOPS, which
is the maximum supported by AWS. See the
<link xlink:href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">AWS documentation</link>
for further details.</para>
</callout>
<callout arearefs="CO150-4">
<para>Optional: Denotes whether to encrypt the EBS volume. Valid values
are <literal>true</literal> or <literal>false</literal>.</para>
</callout>
<callout arearefs="CO150-5">
<para>Optional: The full ARN of the key to use when encrypting the volume.
If none is supplied, but <literal>encypted</literal> is set to <literal>true</literal>, then AWS generates a
key. See the
<link xlink:href="http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html">AWS documentation</link>
for a valid ARN value.</para>
</callout>
<callout arearefs="CO150-6">
<para>Optional: File system that is created on dynamically provisioned
volumes. This value is copied to the <literal>fsType</literal> field of dynamically
provisioned persistent volumes and the file system is created when the
volume is mounted for the first time. The default value is <literal>ext4</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="azure-disk-definition_post-install-storage-configuration">
<title>Azure Disk object definition</title>
<formalpara>
<title>azure-advanced-disk-storageclass.yaml</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;storage-class-name&gt; <co xml:id="CO151-1"/>
provisioner: kubernetes.io/azure-disk
volumeBindingMode: WaitForFirstConsumer <co xml:id="CO151-2"/>
allowVolumeExpansion: true
parameters:
  kind: Managed <co xml:id="CO151-3"/>
  storageaccounttype: Premium_LRS <co xml:id="CO151-4"/>
reclaimPolicy: Delete</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO151-1">
<para>Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.</para>
</callout>
<callout arearefs="CO151-2">
<para>Using <literal>WaitForFirstConsumer</literal> is strongly recommended. This provisions the volume while allowing enough storage to schedule the pod on a free worker node from an available zone.</para>
</callout>
<callout arearefs="CO151-3">
<para>Possible values are <literal>Shared</literal> (default), <literal>Managed</literal>, and <literal>Dedicated</literal>.</para>
<important>
<simpara>Red Hat only supports the use of <literal>kind: Managed</literal> in the storage class.</simpara>
<simpara>With <literal>Shared</literal> and <literal>Dedicated</literal>, Azure creates unmanaged disks, while OpenShift Container Platform creates a managed disk for machine OS (root) disks. But because Azure Disk does not allow the use of both managed and unmanaged disks on a node, unmanaged disks created with <literal>Shared</literal> or <literal>Dedicated</literal> cannot be attached to OpenShift Container Platform nodes.</simpara>
</important>
</callout>
<callout arearefs="CO151-4">
<para>Azure storage account SKU tier. Default is empty. Note that Premium VMs can attach both <literal>Standard_LRS</literal> and <literal>Premium_LRS</literal> disks, Standard VMs can only attach <literal>Standard_LRS</literal> disks, Managed VMs can only attach managed disks, and unmanaged VMs can only attach unmanaged disks.</para>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>If <literal>kind</literal> is set to <literal>Shared</literal>, Azure creates all unmanaged disks in a few shared storage accounts in the same resource group as the cluster.</simpara>
</listitem>
<listitem>
<simpara>If <literal>kind</literal> is set to <literal>Managed</literal>, Azure creates new managed disks.</simpara>
</listitem>
<listitem>
<simpara>If <literal>kind</literal> is set to <literal>Dedicated</literal> and a <literal>storageAccount</literal> is specified, Azure uses the specified storage account for the new unmanaged disk in the same resource group as the cluster. For this to work:</simpara>
<itemizedlist>
<listitem>
<simpara>The specified storage account must be in the same region.</simpara>
</listitem>
<listitem>
<simpara>Azure Cloud Provider must have write access to the storage account.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If <literal>kind</literal> is set to <literal>Dedicated</literal> and a <literal>storageAccount</literal> is not specified, Azure creates a new dedicated storage account for the new unmanaged disk in the same resource group as the cluster.</simpara>
</listitem>
</orderedlist>
</callout>
</calloutlist>
</section>
<section xml:id="azure-file-definition_post-install-storage-configuration">
<title>Azure File object definition</title>
<simpara>The Azure File storage class uses secrets to store the Azure storage account name
and the storage account key that are required to create an Azure Files share. These
permissions are created as part of the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Define a <literal>ClusterRole</literal> object that allows access to create and view secrets:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
#  name: system:azure-cloud-provider
  name: &lt;persistent-volume-binder-role&gt; <co xml:id="CO152-1"/>
rules:
- apiGroups: ['']
  resources: ['secrets']
  verbs:     ['get','create']</programlisting>
<calloutlist>
<callout arearefs="CO152-1">
<para>The name of the cluster role to view and create secrets.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Add the cluster role to the service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-cluster-role-to-user &lt;persistent-volume-binder-role&gt; system:serviceaccount:kube-system:persistent-volume-binder</programlisting>
</listitem>
<listitem>
<simpara>Create the Azure File <literal>StorageClass</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;azure-file&gt; <co xml:id="CO153-1"/>
provisioner: kubernetes.io/azure-file
parameters:
  location: eastus <co xml:id="CO153-2"/>
  skuName: Standard_LRS <co xml:id="CO153-3"/>
  storageAccount: &lt;storage-account&gt; <co xml:id="CO153-4"/>
reclaimPolicy: Delete
volumeBindingMode: Immediate</programlisting>
<calloutlist>
<callout arearefs="CO153-1">
<para>Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.</para>
</callout>
<callout arearefs="CO153-2">
<para>Location of the Azure storage account, such as <literal>eastus</literal>. Default is empty, meaning that a new Azure storage account will be created in the OpenShift Container Platform cluster&#8217;s location.</para>
</callout>
<callout arearefs="CO153-3">
<para>SKU tier of the Azure storage account, such as <literal>Standard_LRS</literal>. Default is empty, meaning that a new Azure storage account will be created with the <literal>Standard_LRS</literal> SKU.</para>
</callout>
<callout arearefs="CO153-4">
<para>Name of the Azure storage account. If a storage account is provided, then
<literal>skuName</literal> and <literal>location</literal> are ignored. If no storage account is provided, then
the storage class searches for any storage account that is associated with the
resource group for any accounts that match the defined <literal>skuName</literal> and <literal>location</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<section xml:id="azure-file-considerations_post-install-storage-configuration">
<title>Considerations when using Azure File</title>
<simpara>The following file system features are not supported by the default Azure File storage class:</simpara>
<itemizedlist>
<listitem>
<simpara>Symlinks</simpara>
</listitem>
<listitem>
<simpara>Hard links</simpara>
</listitem>
<listitem>
<simpara>Extended attributes</simpara>
</listitem>
<listitem>
<simpara>Sparse files</simpara>
</listitem>
<listitem>
<simpara>Named pipes</simpara>
</listitem>
</itemizedlist>
<simpara>Additionally, the owner user identifier (UID) of the Azure File mounted directory is different from the process UID of the container. The <literal>uid</literal> mount option can be specified in the <literal>StorageClass</literal> object to define
a specific user identifier to use for the mounted directory.</simpara>
<simpara>The following <literal>StorageClass</literal> object demonstrates modifying the user and group identifier, along with enabling symlinks for the mounted directory.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: azure-file
mountOptions:
  - uid=1500 <co xml:id="CO154-1"/>
  - gid=1500 <co xml:id="CO154-2"/>
  - mfsymlinks <co xml:id="CO154-3"/>
provisioner: kubernetes.io/azure-file
parameters:
  location: eastus
  skuName: Standard_LRS
reclaimPolicy: Delete
volumeBindingMode: Immediate</programlisting>
<calloutlist>
<callout arearefs="CO154-1">
<para>Specifies the user identifier to use for the mounted directory.</para>
</callout>
<callout arearefs="CO154-2">
<para>Specifies the group identifier to use for the mounted directory.</para>
</callout>
<callout arearefs="CO154-3">
<para>Enables symlinks.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="gce-persistentdisk-storage-class_post-install-storage-configuration">
<title>GCE PersistentDisk (gcePD) object definition</title>
<formalpara>
<title>gce-pd-storageclass.yaml</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: &lt;storage-class-name&gt; <co xml:id="CO155-1"/>
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard <co xml:id="CO155-2"/>
  replication-type: none
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO155-1">
<para>Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.</para>
</callout>
<callout arearefs="CO155-2">
<para>Select either <literal>pd-standard</literal> or <literal>pd-ssd</literal>. The default is <literal>pd-standard</literal>.</para>
</callout>
</calloutlist>
</section>
<section xml:id="vsphere-definition_post-install-storage-configuration">
<title>VMware vSphere object definition</title>
<formalpara>
<title>vsphere-storageclass.yaml</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: &lt;storage-class-name&gt; <co xml:id="CO156-1"/>
provisioner: csi.vsphere.vmware.com <co xml:id="CO156-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO156-1">
<para>Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.</para>
</callout>
<callout arearefs="CO156-2">
<para>For more information about using VMware vSphere CSI with OpenShift Container Platform,
see the
<link xlink:href="https://kubernetes.io/docs/concepts/storage/volumes/#vsphere-csi-migration">Kubernetes documentation</link>.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="change-default-storage-class_post-install-storage-configuration">
<title>Changing the default storage class</title>
<simpara>Use the following procedure to change the default storage class.</simpara>
<simpara>For example, if you have two defined storage classes, <literal>gp3</literal> and <literal>standard</literal>, and you want to change the default storage class from <literal>gp3</literal> to <literal>standard</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster with cluster-admin privileges.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To change the default storage class:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>List the storage classes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                 TYPE
gp3 (default)        kubernetes.io/aws-ebs <co xml:id="CO157-1"/>
standard             kubernetes.io/aws-ebs</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO157-1">
<para><literal>(default)</literal> indicates the default storage class.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Make the desired storage class the default.</simpara>
<simpara>For the desired storage class, set the <literal>storageclass.kubernetes.io/is-default-class</literal> annotation to <literal>true</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageclass standard -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'</programlisting>
<note>
<simpara>You can have multiple default storage classes for a short time. However, you should ensure that only one default storage class exists eventually.</simpara>
<simpara>With multiple default storage classes present, any persistent volume claim (PVC) requesting the default storage class (<literal>pvc.spec.storageClassName</literal>=nil) gets the most recently created default storage class, regardless of the default status of that storage class, and the administrator receives an alert in the alerts dashboard that there are multiple default storage classes, <literal>MultipleDefaultStorageClasses</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Remove the default storage class setting from the old default storage class.</simpara>
<simpara>For the old default storage class, change the value of the <literal>storageclass.kubernetes.io/is-default-class</literal> annotation to <literal>false</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageclass gp3 -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}'</programlisting>
</listitem>
<listitem>
<simpara>Verify the changes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                 TYPE
gp3                  kubernetes.io/aws-ebs
standard (default)   kubernetes.io/aws-ebs</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="post-install-optimizing-storage">
<title>Optimizing storage</title>
<simpara>Optimizing storage helps to minimize storage use across all resources. By
optimizing storage, administrators help ensure that existing storage resources
are working in an efficient manner.</simpara>
</section>
<section xml:id="available-persistent-storage-options_post-install-storage-configuration">
<title>Available persistent storage options</title>
<simpara>Understand your persistent storage options so that you can optimize your
OpenShift Container Platform environment.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Available storage options</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="12.5*"/>
<colspec colname="col_2" colwidth="50*"/>
<colspec colname="col_3" colwidth="37.5*"/>
<thead>
<row>
<entry align="left" valign="top">Storage type</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Examples</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Block</simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Presented to the operating system (OS) as a block device</simpara>
</listitem>
<listitem>
<simpara>Suitable for applications that need full control of storage and operate at a low level on files
bypassing the file system</simpara>
</listitem>
<listitem>
<simpara>Also referred to as a Storage Area Network (SAN)</simpara>
</listitem>
<listitem>
<simpara>Non-shareable, which means that only one client at a time can mount an endpoint of this type</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara>AWS EBS and VMware vSphere support dynamic persistent volume (PV) provisioning natively in OpenShift Container Platform.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>File</simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Presented to the OS as a file system export to be mounted</simpara>
</listitem>
<listitem>
<simpara>Also referred to as Network Attached Storage (NAS)</simpara>
</listitem>
<listitem>
<simpara>Concurrency, latency, file locking mechanisms, and other capabilities vary widely between protocols, implementations, vendors, and scales.</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara>RHEL NFS, NetApp NFS <superscript>[1]</superscript>, and Vendor NFS</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Object</simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara>Accessible through a REST API endpoint</simpara>
</listitem>
<listitem>
<simpara>Configurable for use in the OpenShift image registry</simpara>
</listitem>
<listitem>
<simpara>Applications must build their drivers into the application and/or container.</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara>AWS S3</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>NetApp NFS supports dynamic PV provisioning when using the Trident plugin.</simpara>
</listitem>
</orderedlist>
</para>
</section>
<section xml:id="recommended-configurable-storage-technology_post-install-storage-configuration">
<title>Recommended configurable storage technology</title>
<simpara>The following table summarizes the recommended and configurable storage technologies for the given OpenShift Container Platform cluster application.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Recommended and configurable storage technology</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Storage type</entry>
<entry align="left" valign="top">Block</entry>
<entry align="left" valign="top">File</entry>
<entry align="left" valign="top">Object</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>ROX<superscript>1</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Yes<superscript>4</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Yes<superscript>4</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RWX<superscript>2</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Registry</simpara></entry>
<entry align="left" valign="top"><simpara>Configurable</simpara></entry>
<entry align="left" valign="top"><simpara>Configurable</simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Scaled registry</simpara></entry>
<entry align="left" valign="top"><simpara>Not configurable</simpara></entry>
<entry align="left" valign="top"><simpara>Configurable</simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Metrics<superscript>3</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
<entry align="left" valign="top"><simpara>Configurable<superscript>5</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Not configurable</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Elasticsearch Logging</simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
<entry align="left" valign="top"><simpara>Configurable<superscript>6</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Not supported<superscript>6</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Loki Logging</simpara></entry>
<entry align="left" valign="top"><simpara>Not configurable</simpara></entry>
<entry align="left" valign="top"><simpara>Not configurable</simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Apps</simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
<entry align="left" valign="top"><simpara>Recommended</simpara></entry>
<entry align="left" valign="top"><simpara>Not configurable<superscript>7</superscript></simpara></entry>
</row>
</tbody>
<tfoot>
<row>
<entry align="left" valign="top" namest="col_1" nameend="col_4"><simpara><superscript>1</superscript> <literal>ReadOnlyMany</literal></simpara>
<simpara><superscript>2</superscript> <literal>ReadWriteMany</literal></simpara>
<simpara><superscript>3</superscript> Prometheus is the underlying technology used for metrics.</simpara>
<simpara><superscript>4</superscript> This does not apply to physical disk, VM physical disk, VMDK, loopback over NFS, AWS EBS, and Azure Disk.</simpara>
<simpara><superscript>5</superscript> For metrics, using file storage with the <literal>ReadWriteMany</literal> (RWX) access mode is unreliable. If you use file storage, do not configure the RWX access mode on any persistent volume claims (PVCs) that are configured for use with metrics.</simpara>
<simpara><superscript>6</superscript> For logging, review the recommended storage solution in Configuring persistent storage for the log store section. Using NFS storage as a persistent volume or through NAS, such as Gluster, can corrupt the data. Hence, NFS is not supported for Elasticsearch storage and LokiStack log store in OpenShift Container Platform Logging. You must use one persistent volume type per log store.</simpara>
<simpara><superscript>7</superscript> Object storage is not consumed through OpenShift Container Platform&#8217;s PVs or PVCs. Apps must integrate with the object storage REST API.</simpara></entry>
</row>
</tfoot>
</tgroup>
</table>
<note>
<simpara>A scaled registry is an OpenShift image registry where two or more pod replicas are running.</simpara>
</note>
<section xml:id="_specific_application_storage_recommendations">
<title>Specific application storage recommendations</title>
<important>
<simpara>Testing shows issues with using the NFS server on Red Hat Enterprise Linux (RHEL) as storage backend for core services. This includes the OpenShift Container Registry and Quay, Prometheus for monitoring storage, and Elasticsearch for logging storage. Therefore, using RHEL NFS to back PVs used by core services is not recommended.</simpara>
<simpara>Other NFS implementations on the marketplace might not have these issues. Contact the individual NFS implementation vendor for more information on any testing that was possibly completed against these OpenShift Container Platform core components.</simpara>
</important>
<section xml:id="_registry">
<title>Registry</title>
<simpara>In a non-scaled/high-availability (HA) OpenShift image registry cluster deployment:</simpara>
<itemizedlist>
<listitem>
<simpara>The storage technology does not have to support RWX access mode.</simpara>
</listitem>
<listitem>
<simpara>The storage technology must ensure read-after-write consistency.</simpara>
</listitem>
<listitem>
<simpara>The preferred storage technology is object storage followed by block storage.</simpara>
</listitem>
<listitem>
<simpara>File storage is not recommended for OpenShift image registry cluster deployment with production workloads.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_scaled_registry">
<title>Scaled registry</title>
<simpara>In a scaled/HA OpenShift image registry cluster deployment:</simpara>
<itemizedlist>
<listitem>
<simpara>The storage technology must support RWX access mode.</simpara>
</listitem>
<listitem>
<simpara>The storage technology must ensure read-after-write consistency.</simpara>
</listitem>
<listitem>
<simpara>The preferred storage technology is object storage.</simpara>
</listitem>
<listitem>
<simpara>Red Hat OpenShift Data Foundation (ODF), Amazon Simple Storage Service (Amazon S3), Google Cloud Storage (GCS), Microsoft Azure Blob Storage, and OpenStack Swift are supported.</simpara>
</listitem>
<listitem>
<simpara>Object storage should be S3 or Swift compliant.</simpara>
</listitem>
<listitem>
<simpara>For non-cloud platforms, such as vSphere and bare metal installations, the only configurable technology is file storage.</simpara>
</listitem>
<listitem>
<simpara>Block storage is not configurable.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_metrics">
<title>Metrics</title>
<simpara>In an OpenShift Container Platform hosted metrics cluster deployment:</simpara>
<itemizedlist>
<listitem>
<simpara>The preferred storage technology is block storage.</simpara>
</listitem>
<listitem>
<simpara>Object storage is not configurable.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>It is not recommended to use file storage for a hosted metrics cluster deployment with production workloads.</simpara>
</important>
</section>
<section xml:id="_logging">
<title>Logging</title>
<simpara>In an OpenShift Container Platform hosted logging cluster deployment:</simpara>
<itemizedlist>
<listitem>
<simpara>Loki Operator:</simpara>
<itemizedlist>
<listitem>
<simpara>The preferred storage technology is S3 compatible Object storage.</simpara>
</listitem>
<listitem>
<simpara>Block storage is not configurable.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>OpenShift Elasticsearch Operator:</simpara>
<itemizedlist>
<listitem>
<simpara>The preferred storage technology is block storage.</simpara>
</listitem>
<listitem>
<simpara>Object storage is not supported.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>As of logging version 5.4.3 the OpenShift Elasticsearch Operator is deprecated and is planned to be removed in a future release. Red Hat will provide bug fixes and support for this feature during the current release lifecycle, but this feature will no longer receive enhancements and will be removed. As an alternative to using the OpenShift Elasticsearch Operator to manage the default log storage, you can use the Loki Operator.</simpara>
</note>
</section>
<section xml:id="_applications">
<title>Applications</title>
<simpara>Application use cases vary from application to application, as described in the following examples:</simpara>
<itemizedlist>
<listitem>
<simpara>Storage technologies that support dynamic PV provisioning have low mount time latencies, and are not tied to nodes to support a healthy cluster.</simpara>
</listitem>
<listitem>
<simpara>Application developers are responsible for knowing and understanding the storage requirements for their application, and how it works with the provided storage to ensure that issues do not occur when an application scales or interacts with the storage layer.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_other_specific_application_storage_recommendations">
<title>Other specific application storage recommendations</title>
<important>
<simpara>It is not recommended to use RAID configurations on <literal>Write</literal> intensive workloads, such as <literal>etcd</literal>. If you are running <literal>etcd</literal> with a RAID configuration, you might be at risk of encountering performance issues with your workloads.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>Red Hat OpenStack Platform (RHOSP) Cinder: RHOSP Cinder tends to be adept in ROX access mode use cases.</simpara>
</listitem>
<listitem>
<simpara>Databases: Databases (RDBMSs, NoSQL DBs, etc.) tend to perform best with dedicated block storage.</simpara>
</listitem>
<listitem>
<simpara>The etcd database must have enough storage and adequate performance capacity to enable a large cluster. Information about monitoring and benchmarking tools to establish ample storage and a high-performance environment is described in <emphasis>Recommended etcd practices</emphasis>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.xml#recommended-etcd-practices">Recommended etcd practices</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-install-deploy-OCS">
<title>Deploy Red Hat OpenShift Data Foundation</title>
<simpara>Red Hat OpenShift Data Foundation is a provider of agnostic persistent storage for OpenShift Container Platform supporting file, block, and object storage, either in-house or in hybrid clouds. As a Red Hat storage solution, Red Hat OpenShift Data Foundation is completely integrated with OpenShift Container Platform for deployment, management, and monitoring.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">If you are looking for Red Hat OpenShift Data Foundation information about&#8230;&#8203;</entry>
<entry align="left" valign="top">See the following Red Hat OpenShift Data Foundation documentation:</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>What&#8217;s new, known issues, notable bug fixes, and Technology Previews</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/4.12_release_notes">OpenShift Data Foundation 4.12 Release Notes</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Supported workloads, layouts, hardware and software requirements, sizing and scaling recommendations</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/planning_your_deployment">Planning your OpenShift Data Foundation 4.12 deployment</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying OpenShift Data Foundation to use an external Red Hat Ceph Storage cluster</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_in_external_mode">Deploying OpenShift Data Foundation 4.12 in external mode</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying OpenShift Data Foundation to local storage on bare metal infrastructure</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure">Deploying OpenShift Data Foundation 4.12 using bare metal infrastructure</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying OpenShift Data Foundation on Red Hat OpenShift Container Platform VMware vSphere clusters</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_on_vmware_vsphere">Deploying OpenShift Data Foundation 4.12 on VMware vSphere</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying OpenShift Data Foundation using Amazon Web Services for local or cloud storage</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_amazon_web_services">Deploying OpenShift Data Foundation 4.12 using Amazon Web Services</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying and managing OpenShift Data Foundation on existing Red Hat OpenShift Container Platform Google Cloud clusters</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_and_managing_openshift_data_foundation_using_google_cloud">Deploying and managing OpenShift Data Foundation 4.12 using Google Cloud</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying and managing OpenShift Data Foundation on existing Red Hat OpenShift Container Platform Azure clusters</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_microsoft_azure/index">Deploying and managing OpenShift Data Foundation 4.12 using Microsoft Azure</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying OpenShift Data Foundation to use local storage on IBM Power&#174; infrastructure</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html-single/deploying_openshift_data_foundation_using_ibm_power/index">Deploying OpenShift Data Foundation on IBM Power&#174;</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instructions on deploying OpenShift Data Foundation to use local storage on IBM Z&#174; infrastructure</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_ibm_z_infrastructure/index">Deploying OpenShift Data Foundation on IBM Z&#174; infrastructure</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Allocating storage to core services and hosted applications in Red Hat OpenShift Data Foundation, including snapshot and clone</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_and_allocating_storage_resources">Managing and allocating resources</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Managing storage resources across a hybrid cloud or multicloud environment using the Multicloud Object Gateway (NooBaa)</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_hybrid_and_multicloud_resources">Managing hybrid and multicloud resources</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Safely replacing storage devices for Red Hat OpenShift Data Foundation</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/replacing_devices">Replacing devices</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Safely replacing a node in a Red Hat OpenShift Data Foundation cluster</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/replacing_nodes">Replacing nodes</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Scaling operations in Red Hat OpenShift Data Foundation</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/scaling_storage">Scaling storage</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Monitoring a Red Hat OpenShift Data Foundation 4.12 cluster</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/monitoring_openshift_data_foundation">Monitoring Red Hat OpenShift Data Foundation 4.12</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Resolve issues encountered during operations</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/troubleshooting_openshift_data_foundation">Troubleshooting OpenShift Data Foundation 4.12</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Migrating your OpenShift Container Platform cluster from version 3 to version 4</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/migrating_from_version_3_to_4/index">Migration</link></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="admission-plug-ins-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../logging/log_storage/logging-config-es-store.xml#logging-config-es-store">Configuring the Elasticsearch log store</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="post-install-preparing-for-users">
<title>Preparing for users</title>

<simpara>After installing OpenShift Container Platform, you can further expand and customize your
cluster to your requirements, including taking steps to prepare for users.</simpara>
<section xml:id="post-install-understanding-identity-provider">
<title>Understanding identity provider configuration</title>
<simpara>The OpenShift Container Platform control plane includes a built-in OAuth server. Developers and
administrators obtain OAuth access tokens to authenticate themselves to the API.</simpara>
<simpara>As an administrator, you can configure OAuth to specify an identity provider
after you install your cluster.</simpara>
<section xml:id="identity-provider-overview_post-install-preparing-for-users">
<title>About identity providers in OpenShift Container Platform</title>
<simpara>By default, only a <literal>kubeadmin</literal> user exists on your cluster. To specify an
identity provider, you must create a custom resource (CR) that describes
that identity provider and add it to the cluster.</simpara>
<note>
<simpara>OpenShift Container Platform user names containing <literal>/</literal>, <literal>:</literal>, and <literal>%</literal> are not supported.</simpara>
</note>
</section>
<section xml:id="post-install-supported-identity-providers">
<title>Supported identity providers</title>
<simpara>You can configure the following types of identity providers:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Identity provider</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-htpasswd-identity-provider.xml#configuring-htpasswd-identity-provider">htpasswd</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure the <literal>htpasswd</literal> identity provider to validate user names and passwords
against a flat file generated using
<link xlink:href="http://httpd.apache.org/docs/2.4/programs/htpasswd.html"><literal>htpasswd</literal></link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-keystone-identity-provider.xml#configuring-keystone-identity-provider">Keystone</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure the <literal>keystone</literal> identity provider to integrate
your OpenShift Container Platform cluster with Keystone to enable shared authentication with
an OpenStack Keystone v3 server configured to store users in an internal
database.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-ldap-identity-provider.xml#configuring-ldap-identity-provider">LDAP</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure the <literal>ldap</literal> identity provider to validate user names and passwords
against an LDAPv3 server, using simple bind authentication.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-basic-authentication-identity-provider.xml#configuring-basic-authentication-identity-provider">Basic authentication</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure a <literal>basic-authentication</literal> identity provider for users to log in to
OpenShift Container Platform with credentials validated against a remote identity provider.
Basic authentication is a generic backend integration mechanism.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-request-header-identity-provider.xml#configuring-request-header-identity-provider">Request header</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure a <literal>request-header</literal> identity provider to identify users from request
header values, such as <literal>X-Remote-User</literal>. It is typically used in combination with
an authenticating proxy, which sets the request header value.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-github-identity-provider.xml#configuring-github-identity-provider">GitHub or GitHub Enterprise</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure a <literal>github</literal> identity provider to validate user names and passwords
against GitHub or GitHub Enterprise&#8217;s OAuth authentication server.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-gitlab-identity-provider.xml#configuring-gitlab-identity-provider">GitLab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure a <literal>gitlab</literal> identity provider to use
<link xlink:href="https://gitlab.com/">GitLab.com</link> or any other GitLab instance as an identity
provider.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-google-identity-provider.xml#configuring-google-identity-provider">Google</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure a <literal>google</literal> identity provider using
<link xlink:href="https://developers.google.com/identity/protocols/OpenIDConnect">Google&#8217;s OpenID Connect integration</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link xlink:href="../authentication/identity_providers/configuring-oidc-identity-provider.xml#configuring-oidc-identity-provider">OpenID Connect</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configure an <literal>oidc</literal> identity provider to integrate with an OpenID Connect
identity provider using an
<link xlink:href="http://openid.net/specs/openid-connect-core-1_0.html#CodeFlowAuth">Authorization Code Flow</link>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>After you define an identity provider, you can
<link xlink:href="../authentication/using-rbac.xml#authorization-overview_using-rbac">use
RBAC to define and apply permissions</link>.</simpara>
</section>
<section xml:id="identity-provider-parameters_post-install-preparing-for-users">
<title>Identity provider parameters</title>
<simpara>The following parameters are common to all identity providers:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The provider name is prefixed to provider user names to form an
identity name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>mappingMethod</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Defines how new identities are mapped to users when they log in.
Enter one of the following values:</simpara>
<variablelist>
<varlistentry>
<term>claim</term>
<listitem>
<simpara>The default value. Provisions a user with the identity&#8217;s preferred
user name. Fails if a user with that user name is already mapped to another
identity.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>lookup</term>
<listitem>
<simpara>Looks up an existing identity, user identity mapping, and user,
but does not automatically provision users or identities. This allows cluster
administrators to set up identities and users manually, or using an external
process. Using this method requires you to manually provision users.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>add</term>
<listitem>
<simpara>Provisions a user with the identity&#8217;s preferred user name. If a user
with that user name already exists, the identity is mapped to the existing user,
adding to any existing identity mappings for the user. Required when multiple
identity providers are configured that identify the same set of users and map to
the same user names.</simpara>
</listitem>
</varlistentry>
</variablelist></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<note>
<simpara>When adding or changing identity providers, you can map identities from the new
provider to existing users by setting the <literal>mappingMethod</literal> parameter to
<literal>add</literal>.</simpara>
</note>
</section>
<section xml:id="identity-provider-default-CR_post-install-preparing-for-users">
<title>Sample identity provider CR</title>
<simpara>The following custom resource (CR) shows the parameters and default
values that you use to configure an identity provider. This example
uses the htpasswd identity provider.</simpara>
<formalpara>
<title>Sample identity provider CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - name: my_identity_provider <co xml:id="CO158-1"/>
    mappingMethod: claim <co xml:id="CO158-2"/>
    type: HTPasswd
    htpasswd:
      fileData:
        name: htpass-secret <co xml:id="CO158-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO158-1">
<para>This provider name is prefixed to provider user names to form an
identity name.</para>
</callout>
<callout arearefs="CO158-2">
<para>Controls how mappings are established between this provider&#8217;s
identities and <literal>User</literal> objects.</para>
</callout>
<callout arearefs="CO158-3">
<para>An existing secret containing a file generated using
<link xlink:href="http://httpd.apache.org/docs/2.4/programs/htpasswd.html"><literal>htpasswd</literal></link>.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="post-install-using-rbac-to-define-and-apply-permissions">
<title>Using RBAC to define and apply permissions</title>
<simpara>Understand and apply role-based access control.</simpara>
<section xml:id="authorization-overview_post-install-preparing-for-users">
<title>RBAC overview</title>
<simpara>Role-based access control (RBAC) objects determine whether a user is allowed to
perform a given action within a project.</simpara>
<simpara>Cluster administrators
can use the cluster roles and bindings to control who has various access levels to the OpenShift Container Platform platform itself and all projects.</simpara>
<simpara>Developers can use local roles and bindings to control who has access
to their projects. Note that authorization is a separate step from
authentication, which is more about determining the identity of who is taking the action.</simpara>
<simpara>Authorization is managed using:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Authorization object</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Rules</simpara></entry>
<entry align="left" valign="top"><simpara>Sets of permitted verbs on a set of objects. For example,
whether a user or service account can <literal>create</literal> pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Roles</simpara></entry>
<entry align="left" valign="top"><simpara>Collections of rules. You can associate, or bind, users and groups
to multiple roles.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Bindings</simpara></entry>
<entry align="left" valign="top"><simpara>Associations between users and/or groups with a role.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>There are two levels of RBAC roles and bindings that control authorization:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">RBAC level</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Cluster RBAC</simpara></entry>
<entry align="left" valign="top"><simpara>Roles and bindings that are applicable across
all projects. <emphasis>Cluster roles</emphasis> exist cluster-wide, and <emphasis>cluster role bindings</emphasis>
can reference only cluster roles.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Local RBAC</simpara></entry>
<entry align="left" valign="top"><simpara>Roles and bindings that are scoped to a given project. While
<emphasis>local roles</emphasis> exist only in a single project, local role bindings can
reference <emphasis>both</emphasis> cluster and local roles.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>A cluster role binding is a binding that exists at the cluster level.
A role binding exists at the project level. The cluster role <emphasis>view</emphasis> must be
bound to a user using a local role binding for that user to view the project.
Create local roles only if a cluster role does not provide the set
of permissions needed for a particular situation.</simpara>
<simpara>This two-level hierarchy allows reuse across multiple projects through the
cluster roles while allowing customization inside of individual projects
through local roles.</simpara>
<simpara>During evaluation, both the cluster role bindings and the local role bindings are used.
For example:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Cluster-wide "allow" rules are checked.</simpara>
</listitem>
<listitem>
<simpara>Locally-bound "allow" rules are checked.</simpara>
</listitem>
<listitem>
<simpara>Deny by default.</simpara>
</listitem>
</orderedlist>
<section xml:id="default-roles_post-install-preparing-for-users">
<title>Default cluster roles</title>
<simpara>OpenShift Container Platform includes a set of default cluster roles that you can bind to users and groups cluster-wide or locally.</simpara>
<important>
<simpara>It is not recommended to manually modify the default cluster roles. Modifications to these system roles can prevent a cluster from functioning properly.</simpara>
</important>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Default cluster role</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>admin</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A project manager. If used in a local binding, an <literal>admin</literal> has
rights to view any resource in the project and modify any resource in the
project except for quota.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>basic-user</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can get basic information about projects and users.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cluster-admin</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A super-user that can perform any action in any project. When
bound to a user with a local binding, they have full control over quota and
every action on every resource in the project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cluster-status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can get basic cluster status information.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cluster-reader</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can get or view most of the objects but
cannot modify them.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>edit</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can modify most objects in a project but does not have the
power to view or modify roles or bindings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>self-provisioner</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can create their own projects.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>view</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user who cannot make any modifications, but can see most objects in a
project. They cannot view or modify roles or bindings.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Be mindful of the difference between local and cluster bindings. For example,
if you bind the <literal>cluster-admin</literal> role to a user by using a local role binding,
it might appear that this user has the privileges of a cluster administrator.
This is not the case. Binding the <literal>cluster-admin</literal> to a user in a project
grants super administrator privileges for only that project to the user. That user has the permissions of the cluster role <literal>admin</literal>, plus a few additional permissions like the ability to edit rate limits, for that project. This binding can be confusing via the web console UI, which does not list cluster role bindings that are bound to true cluster administrators. However, it does list local role bindings that you can use to locally bind <literal>cluster-admin</literal>.</simpara>
<simpara>The relationships between cluster roles, local roles, cluster role bindings,
local role bindings, users, groups and service accounts are illustrated below.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/rbac.png"/>
</imageobject>
<textobject><phrase>OpenShift Container Platform RBAC</phrase></textobject>
</mediaobject>
</informalfigure>
<warning>
<simpara>The <literal>get pods/exec</literal>, <literal>get pods/*</literal>, and <literal>get *</literal> rules grant execution privileges when they are applied to a role. Apply the principle of least privilege and assign only the minimal RBAC rights required for users and agents. For more information, see <link xlink:href="https://access.redhat.com/solutions/6989997">RBAC rules allow execution privileges</link>.</simpara>
</warning>
</section>
<section xml:id="evaluating-authorization_post-install-preparing-for-users">
<title>Evaluating authorization</title>
<simpara>OpenShift Container Platform evaluates authorization by using:</simpara>
<variablelist>
<varlistentry>
<term>Identity</term>
<listitem>
<simpara>The user name and list of groups that the user belongs to.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Action</term>
<listitem>
<simpara>The action you perform. In most cases, this consists of:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Project</emphasis>: The project you access. A project is a Kubernetes namespace with
additional annotations that allows a community of users to organize and manage
their content in isolation from other communities.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Verb</emphasis> : The action itself:  <literal>get</literal>, <literal>list</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal>, <literal>deletecollection</literal>, or <literal>watch</literal>.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Resource name</emphasis>: The API endpoint that you access.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Bindings</term>
<listitem>
<simpara>The full list of bindings, the associations between users or groups
with a role.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>OpenShift Container Platform evaluates authorization by using the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The identity and the project-scoped action is used to find all bindings that
apply to the user or their groups.</simpara>
</listitem>
<listitem>
<simpara>Bindings are used to locate all the roles that apply.</simpara>
</listitem>
<listitem>
<simpara>Roles are used to find all the rules that apply.</simpara>
</listitem>
<listitem>
<simpara>The action is checked against each rule to find a match.</simpara>
</listitem>
<listitem>
<simpara>If no matching rule is found, the action is then denied by default.</simpara>
</listitem>
</orderedlist>
<tip>
<simpara>Remember that users and groups can be associated with, or bound to, multiple
roles at the same time.</simpara>
</tip>
<simpara>Project administrators can use the CLI to
view local roles and bindings,
including a matrix of the verbs and resources each are associated with.</simpara>
<important>
<simpara>The cluster role bound to the project administrator is limited in a project
through a local binding. It is not bound cluster-wide like the cluster roles granted to the <emphasis role="strong">cluster-admin</emphasis> or <emphasis role="strong">system:admin</emphasis>.</simpara>
<simpara>Cluster roles are roles defined at the cluster level but can be bound either at
the cluster level or at the project level.</simpara>
</important>
<section xml:id="cluster-role-aggregations_post-install-preparing-for-users">
<title>Cluster role aggregation</title>
<simpara>The default admin, edit, view, and cluster-reader cluster roles support
<link xlink:href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#aggregated-clusterroles">cluster role aggregation</link>, where the cluster rules for each role are dynamically updated as new rules are created. This feature is relevant only if you extend the Kubernetes API by creating custom resources.</simpara>
</section>
</section>
</section>
<section xml:id="rbac-projects-namespaces_post-install-preparing-for-users">
<title>Projects and namespaces</title>
<simpara>A Kubernetes <emphasis>namespace</emphasis> provides a mechanism to scope resources in a cluster.
The
<link xlink:href="https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">Kubernetes documentation</link>
has more information on namespaces.</simpara>
<simpara>Namespaces provide a unique scope for:</simpara>
<itemizedlist>
<listitem>
<simpara>Named resources to avoid basic naming collisions.</simpara>
</listitem>
<listitem>
<simpara>Delegated management authority to trusted users.</simpara>
</listitem>
<listitem>
<simpara>The ability to limit community resource consumption.</simpara>
</listitem>
</itemizedlist>
<simpara>Most objects in the system are scoped by namespace, but some are
excepted and have no namespace, including nodes and users.</simpara>
<simpara>A <emphasis>project</emphasis> is a Kubernetes namespace with additional annotations and is the central vehicle
by which access to resources for regular users is managed.
A project allows a community of users to organize and manage their content in
isolation from other communities. Users must be given access to projects by administrators,
or if allowed to create projects, automatically have access to their own projects.</simpara>
<simpara>Projects can have a separate <literal>name</literal>, <literal>displayName</literal>, and <literal>description</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>The mandatory <literal>name</literal> is a unique identifier for the project and is most visible when using the CLI tools or API. The maximum name length is 63 characters.</simpara>
</listitem>
<listitem>
<simpara>The optional <literal>displayName</literal> is how the project is displayed in the web console (defaults to <literal>name</literal>).</simpara>
</listitem>
<listitem>
<simpara>The optional <literal>description</literal> can be a more detailed description of the project and is also visible in the web console.</simpara>
</listitem>
</itemizedlist>
<simpara>Each project scopes its own set of:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="80*"/>
<thead>
<row>
<entry align="left" valign="top">Object</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>Objects</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Pods, services, replication controllers, etc.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Policies</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Rules for which users can or cannot perform actions on objects.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Constraints</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Quotas for each kind of object that can be limited.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>Service accounts</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Service accounts act automatically with designated access to objects in the project.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>Cluster administrators
can create projects and delegate administrative rights for the project to any member of the user community.
Cluster administrators
can also allow developers to create their own projects.</simpara>
<simpara>Developers and administrators can interact with projects by using the CLI or the
web console.</simpara>
</section>
<section xml:id="rbac-default-projects_post-install-preparing-for-users">
<title>Default projects</title>
<simpara>OpenShift Container Platform comes with a number of default projects, and projects
starting with <literal>openshift-</literal> are the most essential to users.
These projects host master components that run as pods and other infrastructure
components. The pods created in these namespaces that have a
<link xlink:href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/#rescheduler-guaranteed-scheduling-of-critical-add-ons">critical pod annotation</link>
are considered critical, and the have guaranteed admission by kubelet.
Pods created for master components in these namespaces are already marked as
critical.</simpara>
<important>
<simpara>Do not run workloads in or share access to default projects. Default projects are reserved for running core cluster components.</simpara>
<simpara>The following default projects are considered highly privileged: <literal>default</literal>, <literal>kube-public</literal>, <literal>kube-system</literal>, <literal>openshift</literal>, <literal>openshift-infra</literal>, <literal>openshift-node</literal>, and other system-created projects that have the <literal>openshift.io/run-level</literal> label set to <literal>0</literal> or <literal>1</literal>. Functionality that relies on admission plugins, such as pod security admission, security context constraints, cluster resource quotas, and image reference resolution, does not work in highly privileged projects.</simpara>
</important>
</section>
<section xml:id="viewing-cluster-roles_post-install-preparing-for-users">
<title>Viewing cluster roles and bindings</title>
<simpara>You can use the <literal>oc</literal> CLI to view cluster roles and bindings by using the
<literal>oc describe</literal> command.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal>oc</literal> CLI.</simpara>
</listitem>
<listitem>
<simpara>Obtain permission to view the cluster roles and bindings.</simpara>
</listitem>
</itemizedlist>
<simpara>Users with the <literal>cluster-admin</literal> default cluster role bound cluster-wide can
perform any action on any resource, including viewing cluster roles and bindings.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To view the cluster roles and their associated rule sets:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe clusterrole.rbac</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
  Resources                                                  Non-Resource URLs  Resource Names  Verbs
  ---------                                                  -----------------  --------------  -----
  .packages.apps.redhat.com                                  []                 []              [* create update patch delete get list watch]
  imagestreams                                               []                 []              [create delete deletecollection get list patch update watch create get list watch]
  imagestreams.image.openshift.io                            []                 []              [create delete deletecollection get list patch update watch create get list watch]
  secrets                                                    []                 []              [create delete deletecollection get list patch update watch get list watch create delete deletecollection patch update]
  buildconfigs/webhooks                                      []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildconfigs                                               []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildlogs                                                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs/scale                                    []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs                                          []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamimages                                          []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreammappings                                        []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamtags                                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  processedtemplates                                         []                 []              [create delete deletecollection get list patch update watch get list watch]
  routes                                                     []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateconfigs                                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateinstances                                          []                 []              [create delete deletecollection get list patch update watch get list watch]
  templates                                                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs.apps.openshift.io/scale                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  deploymentconfigs.apps.openshift.io                        []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildconfigs.build.openshift.io/webhooks                   []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildconfigs.build.openshift.io                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  buildlogs.build.openshift.io                               []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamimages.image.openshift.io                       []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreammappings.image.openshift.io                     []                 []              [create delete deletecollection get list patch update watch get list watch]
  imagestreamtags.image.openshift.io                         []                 []              [create delete deletecollection get list patch update watch get list watch]
  routes.route.openshift.io                                  []                 []              [create delete deletecollection get list patch update watch get list watch]
  processedtemplates.template.openshift.io                   []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateconfigs.template.openshift.io                      []                 []              [create delete deletecollection get list patch update watch get list watch]
  templateinstances.template.openshift.io                    []                 []              [create delete deletecollection get list patch update watch get list watch]
  templates.template.openshift.io                            []                 []              [create delete deletecollection get list patch update watch get list watch]
  serviceaccounts                                            []                 []              [create delete deletecollection get list patch update watch impersonate create delete deletecollection patch update get list watch]
  imagestreams/secrets                                       []                 []              [create delete deletecollection get list patch update watch]
  rolebindings                                               []                 []              [create delete deletecollection get list patch update watch]
  roles                                                      []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.authorization.openshift.io                    []                 []              [create delete deletecollection get list patch update watch]
  roles.authorization.openshift.io                           []                 []              [create delete deletecollection get list patch update watch]
  imagestreams.image.openshift.io/secrets                    []                 []              [create delete deletecollection get list patch update watch]
  rolebindings.rbac.authorization.k8s.io                     []                 []              [create delete deletecollection get list patch update watch]
  roles.rbac.authorization.k8s.io                            []                 []              [create delete deletecollection get list patch update watch]
  networkpolicies.extensions                                 []                 []              [create delete deletecollection patch update create delete deletecollection get list patch update watch get list watch]
  networkpolicies.networking.k8s.io                          []                 []              [create delete deletecollection patch update create delete deletecollection get list patch update watch get list watch]
  configmaps                                                 []                 []              [create delete deletecollection patch update get list watch]
  endpoints                                                  []                 []              [create delete deletecollection patch update get list watch]
  persistentvolumeclaims                                     []                 []              [create delete deletecollection patch update get list watch]
  pods                                                       []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers/scale                               []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers                                     []                 []              [create delete deletecollection patch update get list watch]
  services                                                   []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.apps                                            []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/scale                                     []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps                                           []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps/scale                                     []                 []              [create delete deletecollection patch update get list watch]
  replicasets.apps                                           []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps/scale                                    []                 []              [create delete deletecollection patch update get list watch]
  statefulsets.apps                                          []                 []              [create delete deletecollection patch update get list watch]
  horizontalpodautoscalers.autoscaling                       []                 []              [create delete deletecollection patch update get list watch]
  cronjobs.batch                                             []                 []              [create delete deletecollection patch update get list watch]
  jobs.batch                                                 []                 []              [create delete deletecollection patch update get list watch]
  daemonsets.extensions                                      []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions/scale                               []                 []              [create delete deletecollection patch update get list watch]
  deployments.extensions                                     []                 []              [create delete deletecollection patch update get list watch]
  ingresses.extensions                                       []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions/scale                               []                 []              [create delete deletecollection patch update get list watch]
  replicasets.extensions                                     []                 []              [create delete deletecollection patch update get list watch]
  replicationcontrollers.extensions/scale                    []                 []              [create delete deletecollection patch update get list watch]
  poddisruptionbudgets.policy                                []                 []              [create delete deletecollection patch update get list watch]
  deployments.apps/rollback                                  []                 []              [create delete deletecollection patch update]
  deployments.extensions/rollback                            []                 []              [create delete deletecollection patch update]
  catalogsources.operators.coreos.com                        []                 []              [create update patch delete get list watch]
  clusterserviceversions.operators.coreos.com                []                 []              [create update patch delete get list watch]
  installplans.operators.coreos.com                          []                 []              [create update patch delete get list watch]
  packagemanifests.operators.coreos.com                      []                 []              [create update patch delete get list watch]
  subscriptions.operators.coreos.com                         []                 []              [create update patch delete get list watch]
  buildconfigs/instantiate                                   []                 []              [create]
  buildconfigs/instantiatebinary                             []                 []              [create]
  builds/clone                                               []                 []              [create]
  deploymentconfigrollbacks                                  []                 []              [create]
  deploymentconfigs/instantiate                              []                 []              [create]
  deploymentconfigs/rollback                                 []                 []              [create]
  imagestreamimports                                         []                 []              [create]
  localresourceaccessreviews                                 []                 []              [create]
  localsubjectaccessreviews                                  []                 []              [create]
  podsecuritypolicyreviews                                   []                 []              [create]
  podsecuritypolicyselfsubjectreviews                        []                 []              [create]
  podsecuritypolicysubjectreviews                            []                 []              [create]
  resourceaccessreviews                                      []                 []              [create]
  routes/custom-host                                         []                 []              [create]
  subjectaccessreviews                                       []                 []              [create]
  subjectrulesreviews                                        []                 []              [create]
  deploymentconfigrollbacks.apps.openshift.io                []                 []              [create]
  deploymentconfigs.apps.openshift.io/instantiate            []                 []              [create]
  deploymentconfigs.apps.openshift.io/rollback               []                 []              [create]
  localsubjectaccessreviews.authorization.k8s.io             []                 []              [create]
  localresourceaccessreviews.authorization.openshift.io      []                 []              [create]
  localsubjectaccessreviews.authorization.openshift.io       []                 []              [create]
  resourceaccessreviews.authorization.openshift.io           []                 []              [create]
  subjectaccessreviews.authorization.openshift.io            []                 []              [create]
  subjectrulesreviews.authorization.openshift.io             []                 []              [create]
  buildconfigs.build.openshift.io/instantiate                []                 []              [create]
  buildconfigs.build.openshift.io/instantiatebinary          []                 []              [create]
  builds.build.openshift.io/clone                            []                 []              [create]
  imagestreamimports.image.openshift.io                      []                 []              [create]
  routes.route.openshift.io/custom-host                      []                 []              [create]
  podsecuritypolicyreviews.security.openshift.io             []                 []              [create]
  podsecuritypolicyselfsubjectreviews.security.openshift.io  []                 []              [create]
  podsecuritypolicysubjectreviews.security.openshift.io      []                 []              [create]
  jenkins.build.openshift.io                                 []                 []              [edit view view admin edit view]
  builds                                                     []                 []              [get create delete deletecollection get list patch update watch get list watch]
  builds.build.openshift.io                                  []                 []              [get create delete deletecollection get list patch update watch get list watch]
  projects                                                   []                 []              [get delete get delete get patch update]
  projects.project.openshift.io                              []                 []              [get delete get delete get patch update]
  namespaces                                                 []                 []              [get get list watch]
  pods/attach                                                []                 []              [get list watch create delete deletecollection patch update]
  pods/exec                                                  []                 []              [get list watch create delete deletecollection patch update]
  pods/portforward                                           []                 []              [get list watch create delete deletecollection patch update]
  pods/proxy                                                 []                 []              [get list watch create delete deletecollection patch update]
  services/proxy                                             []                 []              [get list watch create delete deletecollection patch update]
  routes/status                                              []                 []              [get list watch update]
  routes.route.openshift.io/status                           []                 []              [get list watch update]
  appliedclusterresourcequotas                               []                 []              [get list watch]
  bindings                                                   []                 []              [get list watch]
  builds/log                                                 []                 []              [get list watch]
  deploymentconfigs/log                                      []                 []              [get list watch]
  deploymentconfigs/status                                   []                 []              [get list watch]
  events                                                     []                 []              [get list watch]
  imagestreams/status                                        []                 []              [get list watch]
  limitranges                                                []                 []              [get list watch]
  namespaces/status                                          []                 []              [get list watch]
  pods/log                                                   []                 []              [get list watch]
  pods/status                                                []                 []              [get list watch]
  replicationcontrollers/status                              []                 []              [get list watch]
  resourcequotas/status                                      []                 []              [get list watch]
  resourcequotas                                             []                 []              [get list watch]
  resourcequotausages                                        []                 []              [get list watch]
  rolebindingrestrictions                                    []                 []              [get list watch]
  deploymentconfigs.apps.openshift.io/log                    []                 []              [get list watch]
  deploymentconfigs.apps.openshift.io/status                 []                 []              [get list watch]
  controllerrevisions.apps                                   []                 []              [get list watch]
  rolebindingrestrictions.authorization.openshift.io         []                 []              [get list watch]
  builds.build.openshift.io/log                              []                 []              [get list watch]
  imagestreams.image.openshift.io/status                     []                 []              [get list watch]
  appliedclusterresourcequotas.quota.openshift.io            []                 []              [get list watch]
  imagestreams/layers                                        []                 []              [get update get]
  imagestreams.image.openshift.io/layers                     []                 []              [get update get]
  builds/details                                             []                 []              [update]
  builds.build.openshift.io/details                          []                 []              [update]


Name:         basic-user
Labels:       &lt;none&gt;
Annotations:  openshift.io/description: A user that can get basic information about projects.
	              rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
	Resources                                           Non-Resource URLs  Resource Names  Verbs
	  ---------                                           -----------------  --------------  -----
	  selfsubjectrulesreviews                             []                 []              [create]
	  selfsubjectaccessreviews.authorization.k8s.io       []                 []              [create]
	  selfsubjectrulesreviews.authorization.openshift.io  []                 []              [create]
	  clusterroles.rbac.authorization.k8s.io              []                 []              [get list watch]
	  clusterroles                                        []                 []              [get list]
	  clusterroles.authorization.openshift.io             []                 []              [get list]
	  storageclasses.storage.k8s.io                       []                 []              [get list]
	  users                                               []                 [~]             [get]
	  users.user.openshift.io                             []                 [~]             [get]
	  projects                                            []                 []              [list watch]
	  projects.project.openshift.io                       []                 []              [list watch]
	  projectrequests                                     []                 []              [list]
	  projectrequests.project.openshift.io                []                 []              [list]

Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
PolicyRule:
Resources  Non-Resource URLs  Resource Names  Verbs
---------  -----------------  --------------  -----
*.*        []                 []              [*]
           [*]                []              [*]

...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To view the current set of cluster role bindings, which shows the users and
groups that are bound to various roles:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe clusterrolebinding.rbac</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         alertmanager-main
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  alertmanager-main
Subjects:
  Kind            Name               Namespace
  ----            ----               ---------
  ServiceAccount  alertmanager-main  openshift-monitoring


Name:         basic-users
Labels:       &lt;none&gt;
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  basic-user
Subjects:
  Kind   Name                  Namespace
  ----   ----                  ---------
  Group  system:authenticated


Name:         cloud-credential-operator-rolebinding
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  cloud-credential-operator-role
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  default  openshift-cloud-credential-operator


Name:         cluster-admin
Labels:       kubernetes.io/bootstrapping=rbac-defaults
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name            Namespace
  ----   ----            ---------
  Group  system:masters


Name:         cluster-admins
Labels:       &lt;none&gt;
Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
Role:
  Kind:  ClusterRole
  Name:  cluster-admin
Subjects:
  Kind   Name                   Namespace
  ----   ----                   ---------
  Group  system:cluster-admins
  User   system:admin


Name:         cluster-api-manager-rolebinding
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  cluster-api-manager-role
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  default  openshift-machine-api

...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="viewing-local-roles_post-install-preparing-for-users">
<title>Viewing local roles and bindings</title>
<simpara>You can use the <literal>oc</literal> CLI to view local roles and bindings by using the
<literal>oc describe</literal> command.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the <literal>oc</literal> CLI.</simpara>
</listitem>
<listitem>
<simpara>Obtain permission to view the local roles and bindings:</simpara>
<itemizedlist>
<listitem>
<simpara>Users with the <literal>cluster-admin</literal> default cluster role bound cluster-wide can
perform any action on any resource, including viewing local roles and bindings.</simpara>
</listitem>
<listitem>
<simpara>Users with the <literal>admin</literal> default cluster role bound locally can view and manage
roles and bindings in that project.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To view the current set of local role bindings, which show the users and groups
that are bound to various roles for the current project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe rolebinding.rbac</programlisting>
</listitem>
<listitem>
<simpara>To view the local role bindings for a different project, add the <literal>-n</literal> flag
to the command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe rolebinding.rbac -n joe-project</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         admin
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name        Namespace
  ----  ----        ---------
  User  kube:admin


Name:         system:deployers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows deploymentconfigs in this namespace to rollout pods in
                this namespace.  It is auto-managed by a controller; remove
                subjects to disa...
Role:
  Kind:  ClusterRole
  Name:  system:deployer
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  deployer  joe-project


Name:         system:image-builders
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows builds in this namespace to push images to this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-builder
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  builder  joe-project


Name:         system:image-pullers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows all pods in this namespace to pull images from this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-puller
Subjects:
  Kind   Name                                Namespace
  ----   ----                                ---------
  Group  system:serviceaccounts:joe-project</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="adding-roles_post-install-preparing-for-users">
<title>Adding roles to users</title>
<simpara>You can use  the <literal>oc adm</literal> administrator CLI to manage the roles and bindings.</simpara>
<simpara>Binding, or adding, a role to users or groups gives the user or group the access
that is granted by the role. You can add and remove roles to and from users and
groups using <literal>oc adm policy</literal> commands.</simpara>
<simpara>You can bind any of the default cluster roles to local users or groups in your
project.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add a role to a user in a specific project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-role-to-user &lt;role&gt; &lt;user&gt; -n &lt;project&gt;</programlisting>
<simpara>For example, you can add the <literal>admin</literal> role to the <literal>alice</literal> user in <literal>joe</literal> project
by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-role-to-user admin alice -n joe</programlisting>
<tip>
<simpara>You can alternatively apply the following YAML to add the role to the user:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: admin-0
  namespace: joe
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: admin
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: alice</programlisting>
</tip>
</listitem>
<listitem>
<simpara>View the local role bindings and verify the addition in the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe rolebinding.rbac -n &lt;project&gt;</programlisting>
<simpara>For example, to view the local role bindings for the <literal>joe</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe rolebinding.rbac -n joe</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         admin
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name        Namespace
  ----  ----        ---------
  User  kube:admin


Name:         admin-0
Labels:       &lt;none&gt;
Annotations:  &lt;none&gt;
Role:
  Kind:  ClusterRole
  Name:  admin
Subjects:
  Kind  Name   Namespace
  ----  ----   ---------
  User  alice <co xml:id="CO159-1"/>


Name:         system:deployers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows deploymentconfigs in this namespace to rollout pods in
                this namespace.  It is auto-managed by a controller; remove
                subjects to disa...
Role:
  Kind:  ClusterRole
  Name:  system:deployer
Subjects:
  Kind            Name      Namespace
  ----            ----      ---------
  ServiceAccount  deployer  joe


Name:         system:image-builders
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows builds in this namespace to push images to this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-builder
Subjects:
  Kind            Name     Namespace
  ----            ----     ---------
  ServiceAccount  builder  joe


Name:         system:image-pullers
Labels:       &lt;none&gt;
Annotations:  openshift.io/description:
                Allows all pods in this namespace to pull images from this
                namespace.  It is auto-managed by a controller; remove subjects
                to disable.
Role:
  Kind:  ClusterRole
  Name:  system:image-puller
Subjects:
  Kind   Name                                Namespace
  ----   ----                                ---------
  Group  system:serviceaccounts:joe</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO159-1">
<para>The <literal>alice</literal> user has been added to the <literal>admins</literal> <literal>RoleBinding</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-local-role_post-install-preparing-for-users">
<title>Creating a local role</title>
<simpara>You can create a local role for a project and then bind it to a user.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create a local role for a project, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create role &lt;name&gt; --verb=&lt;verb&gt; --resource=&lt;resource&gt; -n &lt;project&gt;</programlisting>
<simpara>In this command, specify:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>&lt;name&gt;</literal>, the local role&#8217;s name</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;verb&gt;</literal>, a comma-separated list of the verbs to apply to the role</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;resource&gt;</literal>, the resources that the role applies to</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;project&gt;</literal>, the project name</simpara>
</listitem>
</itemizedlist>
<simpara>For example, to create a local role that allows a user to view pods in the
<literal>blue</literal> project, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create role podview --verb=get --resource=pod -n blue</programlisting>
</listitem>
<listitem>
<simpara>To bind the new role to a user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-role-to-user podview user2 --role-namespace=blue -n blue</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-cluster-role_post-install-preparing-for-users">
<title>Creating a cluster role</title>
<simpara>You can create a cluster role.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create a cluster role, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create clusterrole &lt;name&gt; --verb=&lt;verb&gt; --resource=&lt;resource&gt;</programlisting>
<simpara>In this command, specify:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>&lt;name&gt;</literal>, the local role&#8217;s name</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;verb&gt;</literal>, a comma-separated list of the verbs to apply to the role</simpara>
</listitem>
<listitem>
<simpara><literal>&lt;resource&gt;</literal>, the resources that the role applies to</simpara>
</listitem>
</itemizedlist>
<simpara>For example, to create a cluster role that allows a user to view pods, run the
following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create clusterrole podviewonly --verb=get --resource=pod</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="local-role-binding-commands_post-install-preparing-for-users">
<title>Local role binding commands</title>
<simpara>When you manage a user or group&#8217;s associated roles for local role bindings using the
following operations, a project may be specified with the <literal>-n</literal> flag. If it is
not specified, then the current project is used.</simpara>
<simpara>You can use the following commands for local RBAC management.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Local role binding operations</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy who-can <emphasis>&lt;verb&gt;</emphasis> <emphasis>&lt;resource&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates which users can perform an action on a resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy add-role-to-user <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;username&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Binds a specified role to specified users in the current project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy remove-role-from-user <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;username&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Removes a given role from specified users in the current project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy remove-user <emphasis>&lt;username&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Removes specified users and all of their roles in the current project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy add-role-to-group <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;groupname&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Binds a given role to specified groups in the current project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy remove-role-from-group <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;groupname&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Removes a given role from specified groups in the current project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy remove-group <emphasis>&lt;groupname&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Removes specified groups and all of their roles in the current project.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="cluster-role-binding-commands_post-install-preparing-for-users">
<title>Cluster role binding commands</title>
<simpara>You can also manage cluster role bindings using the following
operations. The <literal>-n</literal> flag is not used for these operations because
cluster role bindings use non-namespaced resources.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster role binding operations</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy add-cluster-role-to-user <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;username&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Binds a given role to specified users for all projects in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy remove-cluster-role-from-user <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;username&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Removes a given role from specified users for all projects in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy add-cluster-role-to-group <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;groupname&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Binds a given role to specified groups for all projects in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>$ oc adm policy remove-cluster-role-from-group <emphasis>&lt;role&gt;</emphasis> <emphasis>&lt;groupname&gt;</emphasis></literal></simpara></entry>
<entry align="left" valign="top"><simpara>Removes a given role from specified groups for all projects in the cluster.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="creating-cluster-admin_post-install-preparing-for-users">
<title>Creating a cluster admin</title>
<simpara>The <literal>cluster-admin</literal> role is required to perform administrator
level tasks on the OpenShift Container Platform cluster, such as modifying
cluster resources.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have created a user to define as the cluster admin.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Define the user as a cluster admin:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm policy add-cluster-role-to-user cluster-admin &lt;user&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="understanding-kubeadmin_post-install-preparing-for-users">
<title>The kubeadmin user</title>
<simpara>OpenShift Container Platform creates a cluster administrator, <literal>kubeadmin</literal>, after the
installation process completes.</simpara>
<simpara>This user has the <literal>cluster-admin</literal> role automatically applied and is treated
as the root user for the cluster. The password is dynamically generated
and unique to your OpenShift Container Platform environment. After installation
completes the password is provided in the installation program&#8217;s output.
For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">INFO Install complete!
INFO Run 'export KUBECONFIG=&lt;your working directory&gt;/auth/kubeconfig' to manage the cluster with 'oc', the OpenShift CLI.
INFO The cluster is ready when 'oc login -u kubeadmin -p &lt;provided&gt;' succeeds (wait a few minutes).
INFO Access the OpenShift web-console here: https://console-openshift-console.apps.demo1.openshift4-beta-abcorp.com
INFO Login to the console with user: kubeadmin, password: &lt;provided&gt;</programlisting>
<section xml:id="removing-kubeadmin_post-install-preparing-for-users">
<title>Removing the kubeadmin user</title>
<simpara>After you define an identity provider and create a new <literal>cluster-admin</literal>
user, you can remove the <literal>kubeadmin</literal> to improve cluster security.</simpara>
<warning>
<simpara>If you follow this procedure before another user is a <literal>cluster-admin</literal>,
then OpenShift Container Platform must be reinstalled. It is not possible to undo
this command.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have configured at least one identity provider.</simpara>
</listitem>
<listitem>
<simpara>You must have added the <literal>cluster-admin</literal> role to a user.</simpara>
</listitem>
<listitem>
<simpara>You must be logged in as an administrator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Remove the <literal>kubeadmin</literal> secrets:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secrets kubeadmin -n kube-system</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="post-install-image-configuration-resources">
<title>Image configuration</title>
<simpara>Understand and configure image registry settings.</simpara>
<section xml:id="images-configuration-parameters_post-install-preparing-for-users">
<title>Image controller configuration parameters</title>
<simpara>The <literal>image.config.openshift.io/cluster</literal> resource holds cluster-wide information about how to handle images. The canonical, and only valid name is <literal>cluster</literal>. Its <literal>spec</literal> offers the following configuration parameters.</simpara>
<note>
<simpara>Parameters such as <literal>DisableScheduledImport</literal>, <literal>MaxImagesBulkImportedPerRepository</literal>, <literal>MaxScheduledImportsPerMinute</literal>, <literal>ScheduledImageImportMinimumIntervalSeconds</literal>, <literal>InternalRegistryHostname</literal> are not configurable.</simpara>
</note>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>allowedRegistriesForImport</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Limits the container image registries from which normal users can import images. Set this list to the registries that you trust to contain valid images, and that you want applications to be able to import from. Users with permission to create images or <literal>ImageStreamMappings</literal> from the API are not affected by this policy. Typically only cluster administrators have the appropriate permissions.</simpara>
<simpara>Every element of this list contains a location of the registry specified by the registry domain name.</simpara>
<simpara><literal>domainName</literal>: Specifies a domain name for the registry. If the registry uses a non-standard <literal>80</literal> or <literal>443</literal> port, the port should be included in the domain name as well.</simpara>
<simpara><literal>insecure</literal>: Insecure indicates whether the registry is secure or insecure. By default, if not otherwise specified, the registry is assumed to be secure.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>additionalTrustedCA</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A reference to a config map containing additional CAs that should be trusted during <literal>image stream import</literal>, <literal>pod image pull</literal>, <literal>openshift-image-registry pullthrough</literal>, and builds.</simpara>
<simpara>The namespace for this config map is <literal>openshift-config</literal>. The format of the config map is to use the registry hostname as the key, and the PEM-encoded certificate as the value, for each additional registry CA to trust.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>externalRegistryHostnames</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides the hostnames for the default external image registry. The external hostname should be set only when the image registry is exposed externally. The first value is used in <literal>publicDockerImageRepository</literal> field in image streams. The value must be in <literal>hostname[:port]</literal> format.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>registrySources</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Contains configuration that determines how the container runtime should treat individual registries when accessing images for builds and
pods. For instance, whether or not to allow insecure access. It does not contain configuration for the internal cluster registry.</simpara>
<simpara><literal>insecureRegistries</literal>: Registries which do not have a valid TLS certificate or only support HTTP connections. To specify all subdomains, add the asterisk (<literal>*</literal>) wildcard character as a prefix to the domain name. For example, <literal>*.example.com</literal>. You can specify an individual repository within a registry. For example: <literal>reg1.io/myrepo/myapp:latest</literal>.</simpara>
<simpara><literal>blockedRegistries</literal>: Registries for which image pull and push actions are denied. To specify all subdomains, add the asterisk (<literal>*</literal>) wildcard character as a prefix to the domain name. For example, <literal>*.example.com</literal>. You can specify an individual repository within a registry. For example: <literal>reg1.io/myrepo/myapp:latest</literal>. All other registries are allowed.</simpara>
<simpara><literal>allowedRegistries</literal>: Registries for which image pull and push actions are allowed. To specify all subdomains, add the asterisk (<literal>*</literal>) wildcard character as a prefix to the domain name. For example, <literal>*.example.com</literal>. You can specify an individual repository within a registry. For example: <literal>reg1.io/myrepo/myapp:latest</literal>. All other registries are blocked.</simpara>
<simpara><literal>containerRuntimeSearchRegistries</literal>: Registries for which image pull and push actions are allowed using image short names. All other registries are blocked.</simpara>
<simpara>Either <literal>blockedRegistries</literal> or <literal>allowedRegistries</literal> can be set, but not both.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<warning>
<simpara>When the <literal>allowedRegistries</literal> parameter is defined, all registries, including <literal>registry.redhat.io</literal> and <literal>quay.io</literal> registries and the default OpenShift image registry, are blocked unless explicitly listed. When using the parameter, to prevent pod failure, add all registries including the <literal>registry.redhat.io</literal> and <literal>quay.io</literal> registries and the <literal>internalRegistryHostname</literal> to the <literal>allowedRegistries</literal> list, as they are required by payload images within your environment. For disconnected clusters, mirror registries should also be added.</simpara>
</warning>
<simpara>The <literal>status</literal> field of the <literal>image.config.openshift.io/cluster</literal> resource holds observed values from the cluster.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="27.2727*"/>
<colspec colname="col_2" colwidth="72.7273*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>internalRegistryHostname</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set by the Image Registry Operator, which controls the <literal>internalRegistryHostname</literal>. It sets the hostname for the default OpenShift image registry. The value must be in <literal>hostname[:port]</literal> format. For backward compatibility, you can still use the <literal>OPENSHIFT_DEFAULT_REGISTRY</literal> environment variable, but this setting overrides the environment variable.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>externalRegistryHostnames</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Set by the Image Registry Operator, provides the external hostnames for the image registry when it is exposed externally. The first value is used in <literal>publicDockerImageRepository</literal> field in image streams. The values must be in <literal>hostname[:port]</literal> format.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="images-configuration-file_post-install-preparing-for-users">
<title>Configuring image registry settings</title>
<simpara>You can configure image registry settings by editing the <literal>image.config.openshift.io/cluster</literal> custom resource (CR).
When changes to the registry are applied to the <literal>image.config.openshift.io/cluster</literal> CR, the Machine Config Operator (MCO) performs the following sequential actions:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Cordons the node</simpara>
</listitem>
<listitem>
<simpara>Applies changes by restarting CRI-O</simpara>
</listitem>
<listitem>
<simpara>Uncordons the node</simpara>
<note>
<simpara>The MCO does not restart nodes when it detects changes.</simpara>
</note>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>image.config.openshift.io/cluster</literal> custom resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit image.config.openshift.io/cluster</programlisting>
<simpara>The following is an example <literal>image.config.openshift.io/cluster</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Image <co xml:id="CO160-1"/>
metadata:
  annotations:
    release.openshift.io/create-only: "true"
  creationTimestamp: "2019-05-17T13:44:26Z"
  generation: 1
  name: cluster
  resourceVersion: "8302"
  selfLink: /apis/config.openshift.io/v1/images/cluster
  uid: e34555da-78a9-11e9-b92b-06d6c7da38dc
spec:
  allowedRegistriesForImport: <co xml:id="CO160-2"/>
    - domainName: quay.io
      insecure: false
  additionalTrustedCA: <co xml:id="CO160-3"/>
    name: myconfigmap
  registrySources: <co xml:id="CO160-4"/>
    allowedRegistries:
    - example.com
    - quay.io
    - registry.redhat.io
    - image-registry.openshift-image-registry.svc:5000
    - reg1.io/myrepo/myapp:latest
    insecureRegistries:
    - insecure.com
status:
  internalRegistryHostname: image-registry.openshift-image-registry.svc:5000</programlisting>
<calloutlist>
<callout arearefs="CO160-1">
<para><literal>Image</literal>: Holds cluster-wide information about how to handle images. The canonical, and only valid name is <literal>cluster</literal>.</para>
</callout>
<callout arearefs="CO160-2">
<para><literal>allowedRegistriesForImport</literal>: Limits the container image registries from which normal users may import images. Set this list to the registries that you trust to contain valid images, and that you want applications to be able to import from. Users with permission to create images or <literal>ImageStreamMappings</literal> from the API are not affected by this policy. Typically only cluster administrators have the appropriate permissions.</para>
</callout>
<callout arearefs="CO160-3">
<para><literal>additionalTrustedCA</literal>: A reference to a config map containing additional certificate authorities (CA) that are trusted during image stream import, pod image pull, <literal>openshift-image-registry</literal> pullthrough, and builds. The namespace for this config map is <literal>openshift-config</literal>. The format of the config map is to use the registry hostname as the key, and the PEM certificate as the value, for each additional registry CA to trust.</para>
</callout>
<callout arearefs="CO160-4">
<para><literal>registrySources</literal>: Contains configuration that determines whether the container runtime allows or blocks individual registries when accessing images for builds and pods.  Either the <literal>allowedRegistries</literal> parameter or the <literal>blockedRegistries</literal> parameter can be set, but not both. You can also define whether or not to allow access to insecure registries or registries that allow registries that use image short names. This example uses the <literal>allowedRegistries</literal> parameter, which defines the registries that are allowed to be used. The insecure registry <literal>insecure.com</literal> is also allowed. The <literal>registrySources</literal> parameter does not contain configuration for the internal cluster registry.</para>
</callout>
</calloutlist>
<note>
<simpara>When the <literal>allowedRegistries</literal> parameter is defined, all registries, including the registry.redhat.io and quay.io registries and the default OpenShift image registry, are blocked unless explicitly listed. If you use the parameter, to prevent pod failure, you must add the <literal>registry.redhat.io</literal> and <literal>quay.io</literal> registries and the <literal>internalRegistryHostname</literal> to the <literal>allowedRegistries</literal> list, as they are required by payload images within your environment. Do not add the <literal>registry.redhat.io</literal> and <literal>quay.io</literal> registries to the <literal>blockedRegistries</literal> list.</simpara>
<simpara>When using the <literal>allowedRegistries</literal>, <literal>blockedRegistries</literal>, or <literal>insecureRegistries</literal> parameter, you can specify an individual repository within a registry. For example: <literal>reg1.io/myrepo/myapp:latest</literal>.</simpara>
<simpara>Insecure external registries should be avoided to reduce possible security risks.</simpara>
</note>
</listitem>
<listitem>
<simpara>To check that the changes are applied, list your nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-137-182.us-east-2.compute.internal   Ready,SchedulingDisabled   worker                 65m   v1.28.5
ip-10-0-139-120.us-east-2.compute.internal   Ready,SchedulingDisabled   control-plane          74m   v1.28.5
ip-10-0-176-102.us-east-2.compute.internal   Ready                      control-plane          75m   v1.28.5
ip-10-0-188-96.us-east-2.compute.internal    Ready                      worker                 65m   v1.28.5
ip-10-0-200-59.us-east-2.compute.internal    Ready                      worker                 63m   v1.28.5
ip-10-0-223-123.us-east-2.compute.internal   Ready                      control-plane          73m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<simpara>For more information on the allowed, blocked, and insecure registry parameters, see <link xlink:href="../openshift_images/image-configuration.xml#images-configuration-file_image-configuration">Configuring image registry settings</link>.</simpara>
</section>
<section xml:id="images-configuration-cas_post-install-preparing-for-users">
<title>Configuring additional trust stores for image registry access</title>
<simpara>The <literal>image.config.openshift.io/cluster</literal> custom resource can contain a reference to a config map that contains additional certificate authorities to be trusted during image registry access.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The certificate authorities (CA) must be PEM-encoded.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>You can create a config map in the <literal>openshift-config</literal> namespace and use its name in <literal>AdditionalTrustedCA</literal> in the <literal>image.config.openshift.io</literal> custom resource to provide additional CAs that should be trusted when contacting external registries.</para>
</formalpara>
<simpara>The config map key is the hostname of a registry with the port for which this CA is to be trusted, and the PEM certificate content is the value, for each additional registry CA to trust.</simpara>
<formalpara>
<title>Image registry CA config map example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: my-registry-ca
data:
  registry.example.com: |
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----
  registry-with-port.example.com..5000: | <co xml:id="CO161-1"/>
    -----BEGIN CERTIFICATE-----
    ...
    -----END CERTIFICATE-----</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO161-1">
<para>If the registry has the port, such as <literal>registry-with-port.example.com:5000</literal>, <literal>:</literal> should be replaced with <literal>..</literal>.</para>
</callout>
</calloutlist>
<simpara>You can configure additional CAs with the following procedure.</simpara>
<itemizedlist>
<listitem>
<simpara>To configure an additional CA:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap registry-config --from-file=&lt;external_registry_address&gt;=ca.crt -n openshift-config</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit image.config.openshift.io cluster</programlisting>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  additionalTrustedCA:
    name: registry-config</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="images-configuration-registry-mirror_post-install-preparing-for-users">
<title>Understanding image registry repository mirroring</title>
<simpara>Setting up container registry repository mirroring enables you to perform the following tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure your OpenShift Container Platform cluster to redirect requests to pull images from a repository on a source image registry and have it resolved by a repository on a mirrored image registry.</simpara>
</listitem>
<listitem>
<simpara>Identify multiple mirrored repositories for each target repository, to make sure that if one mirror is down, another can be used.</simpara>
</listitem>
</itemizedlist>
<simpara>Repository mirroring in OpenShift Container Platform includes the following attributes:</simpara>
<itemizedlist>
<listitem>
<simpara>Image pulls are resilient to registry downtimes.</simpara>
</listitem>
<listitem>
<simpara>Clusters in disconnected environments can pull images from critical locations, such as quay.io, and have registries behind a company firewall provide the requested images.</simpara>
</listitem>
<listitem>
<simpara>A particular order of registries is tried when an image pull request is made, with the permanent registry typically being the last one tried.</simpara>
</listitem>
<listitem>
<simpara>The mirror information you enter is added to the <literal>/etc/containers/registries.conf</literal> file on every node in the OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>When a node makes a request for an image from the source repository, it tries each mirrored repository in turn until it finds the requested content. If all mirrors fail, the cluster tries the source repository. If successful, the image is pulled to the node.</simpara>
</listitem>
</itemizedlist>
<simpara>Setting up repository mirroring can be done in the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara>At OpenShift Container Platform installation:</simpara>
<simpara>By pulling container images needed by OpenShift Container Platform and then bringing those images behind your company&#8217;s firewall, you can install OpenShift Container Platform into a datacenter that is in a disconnected environment.</simpara>
</listitem>
<listitem>
<simpara>After OpenShift Container Platform installation:</simpara>
<simpara>If you did not configure mirroring during OpenShift Container Platform installation, you can do so postinstallation by using any of the following custom resource (CR) objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>ImageDigestMirrorSet</literal> (IDMS). This object allows you to pull images from a mirrored registry by using digest specifications. The IDMS CR enables you to set a fall back policy that allows or stops continued attempts to pull from the source registry if the image pull fails.</simpara>
</listitem>
<listitem>
<simpara><literal>ImageTagMirrorSet</literal> (ITMS). This object allows you to pull images from a mirrored registry by using image tags. The ITMS CR enables you to set a fall back policy that allows or stops continued attempts to pull from the source registry if the image pull fails.</simpara>
</listitem>
<listitem>
<simpara><literal>ImageContentSourcePolicy</literal> (ICSP). This object allows you to pull images from a mirrored registry by using digest specifications. The ICSP CR always falls back to the source registry if the mirrors do not work.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Using an <literal>ImageContentSourcePolicy</literal> (ICSP) object to configure repository mirroring is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments. If you have existing YAML files that you used to create <literal>ImageContentSourcePolicy</literal> objects, you can use the <literal>oc adm migrate icsp</literal> command to convert those files to an <literal>ImageDigestMirrorSet</literal> YAML file. For more information, see "Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring" in the following section.</simpara>
</important>
</listitem>
</itemizedlist>
<simpara>Each of these custom resource objects identify the following information:</simpara>
<itemizedlist>
<listitem>
<simpara>The source of the container image repository you want to mirror.</simpara>
</listitem>
<listitem>
<simpara>A separate entry for each mirror repository you want to offer the content
requested from the source repository.</simpara>
</listitem>
</itemizedlist>
<simpara>For new clusters, you can use IDMS, ITMS, and ICSP CRs objects as desired. However, using IDMS and ITMS is recommended.</simpara>
<simpara>If you upgraded a cluster, any existing ICSP objects remain stable, and both IDMS and ICSP objects are supported. Workloads using ICSP objects continue to function as expected. However, if you want to take advantage of the fallback policies introduced in the IDMS CRs, you can migrate current workloads to IDMS objects by using the <literal>oc adm migrate icsp</literal> command as shown in the <emphasis role="strong">Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring</emphasis> section that follows. Migrating to IDMS objects does not require a cluster reboot.</simpara>
<note>
<simpara>If your cluster uses an <literal>ImageDigestMirrorSet</literal>, <literal>ImageTagMirrorSet</literal>, or <literal>ImageContentSourcePolicy</literal> object to configure repository mirroring, you can use only global pull secrets for mirrored registries. You cannot add a pull secret to a project.</simpara>
</note>
<section xml:id="images-configuration-registry-mirror-configuring_post-install-preparing-for-users">
<title>Configuring image registry repository mirroring</title>
<simpara>You can create postinstallation mirror configuration custom resources (CR) to redirect image pull requests from a source image registry to a mirrored image registry.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure mirrored repositories, by either:</simpara>
<itemizedlist>
<listitem>
<simpara>Setting up a mirrored repository with Red Hat Quay, as described in <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/manage_red_hat_quay/repo-mirroring-in-red-hat-quay">Red Hat Quay Repository Mirroring</link>. Using Red Hat Quay allows you to copy images from one repository to another and also automatically sync those repositories repeatedly over time.</simpara>
</listitem>
<listitem>
<simpara>Using a tool such as <literal>skopeo</literal> to copy images manually from the source repository to the mirrored repository.</simpara>
<simpara>For example, after installing the skopeo RPM package on a Red Hat Enterprise Linux (RHEL) 7 or RHEL 8 system, use the <literal>skopeo</literal> command as shown in this example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ skopeo copy \
docker://registry.access.redhat.com/ubi9/ubi-minimal:latest@sha256:5cf... \
docker://example.io/example/ubi-minimal</programlisting>
<simpara>In this example, you have a container image registry that is named <literal>example.io</literal> with an image repository named <literal>example</literal> to which you want to copy the <literal>ubi9/ubi-minimal</literal> image from <literal>registry.access.redhat.com</literal>. After you create the mirrored registry, you can configure your OpenShift Container Platform cluster to redirect requests made of the source repository to the mirrored repository.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Log in to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Create a postinstallation mirror configuration CR, by using one of the following examples:</simpara>
<itemizedlist>
<listitem>
<simpara>Create an <literal>ImageDigestMirrorSet</literal> or <literal>ImageTagMirrorSet</literal> CR, as needed, replacing the source and mirrors with your own registry and repository pairs and images:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1 <co xml:id="CO162-1"/>
kind: ImageDigestMirrorSet <co xml:id="CO162-2"/>
metadata:
  name: ubi9repo
spec:
  imageDigestMirrors: <co xml:id="CO162-3"/>
  - mirrors:
    - example.io/example/ubi-minimal <co xml:id="CO162-4"/>
    - example.com/example/ubi-minimal <co xml:id="CO162-5"/>
    source: registry.access.redhat.com/ubi9/ubi-minimal <co xml:id="CO162-6"/>
    mirrorSourcePolicy: AllowContactingSource <co xml:id="CO162-7"/>
  - mirrors:
    - mirror.example.com/redhat
    source: registry.redhat.io/openshift4 <co xml:id="CO162-8"/>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.com
    source: registry.redhat.io <co xml:id="CO162-9"/>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/image
    source: registry.example.com/example/myimage <co xml:id="CO162-10"/>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net
    source: registry.example.com/example <co xml:id="CO162-11"/>
    mirrorSourcePolicy: AllowContactingSource
  - mirrors:
    - mirror.example.net/registry-example-com
    source: registry.example.com <co xml:id="CO162-12"/>
    mirrorSourcePolicy: AllowContactingSource</programlisting>
<calloutlist>
<callout arearefs="CO162-1">
<para>Indicates the API to use with this CR. This must be <literal>config.openshift.io/v1</literal>.</para>
</callout>
<callout arearefs="CO162-2">
<para>Indicates the kind of object according to the pull type:</para>
<itemizedlist>
<listitem>
<simpara><literal>ImageDigestMirrorSet</literal>: Pulls a digest reference image.</simpara>
</listitem>
<listitem>
<simpara><literal>ImageTagMirrorSet</literal>: Pulls a tag reference image.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO162-3">
<para>Indicates the type of image pull method, either:</para>
<itemizedlist>
<listitem>
<simpara><literal>imageDigestMirrors</literal>: Use for an <literal>ImageDigestMirrorSet</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>imageTagMirrors</literal>: Use for an <literal>ImageTagMirrorSet</literal> CR.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO162-4">
<para>Indicates the name of the mirrored image registry and repository.</para>
</callout>
<callout arearefs="CO162-5">
<para>Optional: Indicates a secondary mirror repository for each target repository. If one mirror is down, the target repository can use another mirror.</para>
</callout>
<callout arearefs="CO162-6">
<para>Indicates the registry and repository source, which is the repository that is referred to in image pull specifications.</para>
</callout>
<callout arearefs="CO162-7">
<para>Optional: Indicates the fallback policy if the image pull fails:</para>
<itemizedlist>
<listitem>
<simpara><literal>AllowContactingSource</literal>: Allows continued attempts to pull the image from the source repository. This is the default.</simpara>
</listitem>
<listitem>
<simpara><literal>NeverContactSource</literal>: Prevents continued attempts to pull the image from the source repository.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO162-8">
<para>Optional: Indicates a namespace inside a registry, which allows you to use any image in that namespace. If you use a registry domain as a source, the object is applied to all repositories from the registry.</para>
</callout>
<callout arearefs="CO162-9">
<para>Optional: Indicates a registry, which allows you to use any image in that registry. If you specify a registry name, the object is applied to all repositories from a source registry to a mirror registry.</para>
</callout>
<callout arearefs="CO162-10">
<para>Pulls the image <literal>registry.example.com/example/myimage@sha256:&#8230;&#8203;</literal> from the mirror <literal>mirror.example.net/image@sha256:..</literal>.</para>
</callout>
<callout arearefs="CO162-11">
<para>Pulls the image <literal>registry.example.com/example/image@sha256:&#8230;&#8203;</literal> in the source registry namespace from the mirror <literal>mirror.example.net/image@sha256:&#8230;&#8203;</literal>.</para>
</callout>
<callout arearefs="CO162-12">
<para>Pulls the image <literal>registry.example.com/myimage@sha256</literal> from the mirror registry <literal>example.net/registry-example-com/myimage@sha256:&#8230;&#8203;</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an <literal>ImageContentSourcePolicy</literal> custom resource, replacing the source and mirrors with your own registry and repository pairs and images:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1alpha1
kind: ImageContentSourcePolicy
metadata:
  name: mirror-ocp
spec:
  repositoryDigestMirrors:
  - mirrors:
    - mirror.registry.com:443/ocp/release <co xml:id="CO163-1"/>
    source: quay.io/openshift-release-dev/ocp-release <co xml:id="CO163-2"/>
  - mirrors:
    - mirror.registry.com:443/ocp/release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</programlisting>
<calloutlist>
<callout arearefs="CO163-1">
<para>Specifies the name of the mirror image registry and repository.</para>
</callout>
<callout arearefs="CO163-2">
<para>Specifies the online registry and repository containing the content that is mirrored.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Create the new object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f registryrepomirror.yaml</programlisting>
<simpara>After the object is created, the Machine Config Operator (MCO) drains the nodes for <literal>ImageTagMirrorSet</literal> objects only. The MCO does not drain the nodes for <literal>ImageDigestMirrorSet</literal> and <literal>ImageContentSourcePolicy</literal> objects.</simpara>
</listitem>
<listitem>
<simpara>To check that the mirrored configuration settings are applied, do the following on one of the nodes.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>List your nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get node</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS                     ROLES    AGE  VERSION
ip-10-0-137-44.ec2.internal    Ready                      worker   7m   v1.28.5
ip-10-0-138-148.ec2.internal   Ready                      master   11m  v1.28.5
ip-10-0-139-122.ec2.internal   Ready                      master   11m  v1.28.5
ip-10-0-147-35.ec2.internal    Ready                      worker   7m   v1.28.5
ip-10-0-153-12.ec2.internal    Ready                      worker   7m   v1.28.5
ip-10-0-154-10.ec2.internal    Ready                      master   11m  v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Start the debugging process to access the node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-147-35.ec2.internal</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Starting pod/ip-10-0-147-35ec2internal-debug ...
To use host binaries, run `chroot /host`</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Change your root directory to <literal>/host</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>/etc/containers/registries.conf</literal> file to make sure the changes were made:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# cat /etc/containers/registries.conf</programlisting>
<simpara>The following output represents a <literal>registries.conf</literal> file where postinstallation mirror configuration CRs were applied. The final two entries are marked <literal>digest-only</literal> and <literal>tag-only</literal> respectively.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]
short-name-mode = ""

[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal" <co xml:id="CO164-1"/>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal" <co xml:id="CO164-2"/>
    pull-from-mirror = "digest-only" <co xml:id="CO164-3"/>

  [[registry.mirror]]
    location = "example.com/example/ubi-minimal"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com"

  [[registry.mirror]]
    location = "mirror.example.net/registry-example-com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example"

  [[registry.mirror]]
    location = "mirror.example.net"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.example.com/example/myimage"

  [[registry.mirror]]
    location = "mirror.example.net/image"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io"

  [[registry.mirror]]
    location = "mirror.example.com"
    pull-from-mirror = "digest-only"

[[registry]]
  prefix = ""
  location = "registry.redhat.io/openshift4"

  [[registry.mirror]]
    location = "mirror.example.com/redhat"
    pull-from-mirror = "digest-only"
[[registry]]
  prefix = ""
  location = "registry.access.redhat.com/ubi9/ubi-minimal"
  blocked = true <co xml:id="CO164-4"/>

  [[registry.mirror]]
    location = "example.io/example/ubi-minimal-tag"
    pull-from-mirror = "tag-only" <co xml:id="CO164-5"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO164-1">
<para>Indicates the repository that is referred to in a pull spec.</para>
</callout>
<callout arearefs="CO164-2">
<para>Indicates the mirror for that repository.</para>
</callout>
<callout arearefs="CO164-3">
<para>Indicates that the image pull from the mirror is a digest reference image.</para>
</callout>
<callout arearefs="CO164-4">
<para>Indicates that the <literal>NeverContactSource</literal> parameter is set for this repository.</para>
</callout>
<callout arearefs="CO164-5">
<para>Indicates that the image pull from the mirror is a tag reference image.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Pull an image to the node from the source and check if it is resolved by the mirror.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# podman pull --log-level=debug registry.access.redhat.com/ubi9/ubi-minimal@sha256:5cf...</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<formalpara>
<title>Troubleshooting repository mirroring</title>
<para>If the repository mirroring procedure does not work as described, use the following information about how repository mirroring works to help troubleshoot the problem.</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>The first working mirror is used to supply the pulled image.</simpara>
</listitem>
<listitem>
<simpara>The main registry is only used if no other mirror works.</simpara>
</listitem>
<listitem>
<simpara>From the system context, the <literal>Insecure</literal> flags are used as fallback.</simpara>
</listitem>
<listitem>
<simpara>The format of the <literal>/etc/containers/registries.conf</literal> file has changed recently. It is now version 2 and in TOML format.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="images-configuration-registry-mirror-convert_post-install-preparing-for-users">
<title>Converting ImageContentSourcePolicy (ICSP) files for image registry repository mirroring</title>
<simpara>Using an <literal>ImageContentSourcePolicy</literal> (ICSP) object to configure repository mirroring is a deprecated feature. This functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.</simpara>
<simpara>ICSP objects are being replaced by <literal>ImageDigestMirrorSet</literal> and <literal>ImageTagMirrorSet</literal> objects to configure repository mirroring. If you have existing YAML files that you used to create <literal>ImageContentSourcePolicy</literal> objects, you can use the <literal>oc adm migrate icsp</literal> command to convert those files to an <literal>ImageDigestMirrorSet</literal> YAML file. The command updates the API to the current version, changes the <literal>kind</literal> value to <literal>ImageDigestMirrorSet</literal>, and changes <literal>spec.repositoryDigestMirrors</literal> to <literal>spec.imageDigestMirrors</literal>. The rest of the file is not changed.</simpara>
<simpara>Because the migration does not change the <literal>registries.conf</literal> file, the cluster does not need to reboot.</simpara>
<simpara>For more information about <literal>ImageDigestMirrorSet</literal> or <literal>ImageTagMirrorSet</literal> objects, see "Configuring image registry repository mirroring" in the previous section.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Ensure that you have <literal>ImageContentSourcePolicy</literal> objects on your cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Use the following command to convert one or more <literal>ImageContentSourcePolicy</literal> YAML files to an <literal>ImageDigestMirrorSet</literal> YAML file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm migrate icsp &lt;file_name&gt;.yaml &lt;file_name&gt;.yaml &lt;file_name&gt;.yaml --dest-dir &lt;path_to_the_directory&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;file_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the source <literal>ImageContentSourcePolicy</literal> YAML. You can list multiple file names.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>--dest-dir</literal></term>
<listitem>
<simpara>Optional: Specifies a directory for the output <literal>ImageDigestMirrorSet</literal> YAML. If unset, the file is written to the current directory.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example, the following command converts the <literal>icsp.yaml</literal> and <literal>icsp-2.yaml</literal> file and saves the new YAML files to the <literal>idms-files</literal> directory.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm migrate icsp icsp.yaml icsp-2.yaml --dest-dir idms-files</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi8repo.5911620242173376087.yaml
wrote ImageDigestMirrorSet to idms-files/imagedigestmirrorset_ubi9repo.6456931852378115011.yaml</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create the CR object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;path_to_the_directory&gt;/&lt;file-name&gt;.yaml</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;path_to_the_directory&gt;</literal></term>
<listitem>
<simpara>Specifies the path to the directory, if you used the <literal>--dest-dir</literal> flag.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;file_name&gt;</literal></term>
<listitem>
<simpara>Specifies the name of the <literal>ImageDigestMirrorSet</literal> YAML.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Remove the ICSP objects after the IDMS objects are rolled out.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="post-install-mirrored-catalogs">
<title>Populating OperatorHub from mirrored Operator catalogs</title>
<simpara>If you mirrored Operator catalogs for use with disconnected clusters, you can populate OperatorHub with the Operators from your mirrored catalogs. You can use the generated manifests from the mirroring process to create the required <literal>ImageContentSourcePolicy</literal> and <literal>CatalogSource</literal> objects.</simpara>
<section xml:id="prerequisites_post-install-mirrored-catalogs">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../installing/disconnected_install/installing-mirroring-installation-images.xml#olm-mirror-catalog_installing-mirroring-installation-images">Mirroring Operator catalogs for use with disconnected clusters</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="olm-mirror-catalog-icsp_post-install-preparing-for-users">
<title>Creating the ImageContentSourcePolicy object</title>
<simpara>After mirroring Operator catalog content to your mirror registry, create the required <literal>ImageContentSourcePolicy</literal> (ICSP) object. The ICSP object configures nodes to translate between the image references stored in Operator manifests and the mirrored registry.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>On a host with access to the disconnected cluster, create the ICSP by running the following command to specify the <literal>imageContentSourcePolicy.yaml</literal> file in your manifests directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;path/to/manifests/dir&gt;/imageContentSourcePolicy.yaml</programlisting>
<simpara>where <literal>&lt;path/to/manifests/dir&gt;</literal> is the path to the manifests directory for your mirrored content.</simpara>
<simpara>You can now create a <literal>CatalogSource</literal> object to reference your mirrored index image and Operator content.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="olm-creating-catalog-from-index_post-install-preparing-for-users">
<title>Adding a catalog source to a cluster</title>
<simpara>Adding a catalog source to an OpenShift Container Platform cluster enables the discovery and installation of Operators for users.
Cluster administrators
can create a <literal>CatalogSource</literal> object that references an index image. OperatorHub uses catalog sources to populate the user interface.</simpara>
<tip>
<simpara>Alternatively, you can use the web console to manage catalog sources. From the <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Cluster Settings</emphasis> &#8594; <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis> page, click the <emphasis role="strong">Sources</emphasis> tab, where you can create, update, delete, disable, and enable individual sources.</simpara>
</tip>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You built and pushed an index image to a registry.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>CatalogSource</literal> object that references your index image.
If you used the <literal>oc adm catalog mirror</literal> command to mirror your catalog to a target registry, you can use the generated <literal>catalogSource.yaml</literal> file in your manifests directory as a starting point.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Modify the following to your specifications and save it as a <literal>catalogSource.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-operator-catalog <co xml:id="CO165-1"/>
  namespace: openshift-marketplace <co xml:id="CO165-2"/>
spec:
  sourceType: grpc
  grpcPodConfig:
    securityContextConfig: &lt;security_mode&gt; <co xml:id="CO165-3"/>
  image: &lt;registry&gt;/&lt;namespace&gt;/redhat-operator-index:v4.14 <co xml:id="CO165-4"/>
  displayName: My Operator Catalog
  publisher: &lt;publisher_name&gt; <co xml:id="CO165-5"/>
  updateStrategy:
    registryPoll: <co xml:id="CO165-6"/>
      interval: 30m</programlisting>
<calloutlist>
<callout arearefs="CO165-1">
<para>If you mirrored content to local files before uploading to a registry, remove any backslash (<literal>/</literal>) characters from the <literal>metadata.name</literal> field to avoid an "invalid resource name" error when you create the object.</para>
</callout>
<callout arearefs="CO165-2">
<para>If you want the catalog source to be available globally to users in all namespaces, specify the <literal>openshift-marketplace</literal> namespace. Otherwise, you can specify a different namespace for the catalog to be scoped and available only for that namespace.</para>
</callout>
<callout arearefs="CO165-3">
<para>Specify the value of <literal>legacy</literal> or <literal>restricted</literal>. If the field is not set, the default value is <literal>legacy</literal>. In a future OpenShift Container Platform release, it is planned that the default value will be <literal>restricted</literal>. If your catalog cannot run with <literal>restricted</literal> permissions, it is recommended that you manually set this field to <literal>legacy</literal>.</para>
</callout>
<callout arearefs="CO165-4">
<para>Specify your index image. If you specify a tag after the image name, for example <literal>:v4.14</literal>, the catalog source pod uses an image pull policy of <literal>Always</literal>, meaning the pod always pulls the image prior to starting the container. If you specify a digest, for example <literal>@sha256:&lt;id&gt;</literal>, the image pull policy is <literal>IfNotPresent</literal>, meaning the pod pulls the image only if it does not already exist on the node.</para>
</callout>
<callout arearefs="CO165-5">
<para>Specify your name or an organization name publishing the catalog.</para>
</callout>
<callout arearefs="CO165-6">
<para>Catalog sources can automatically check for new versions to keep up to date.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Use the file to create the <literal>CatalogSource</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f catalogSource.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify the following resources are created successfully.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check the pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-marketplace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                    READY   STATUS    RESTARTS  AGE
my-operator-catalog-6njx6               1/1     Running   0         28s
marketplace-operator-d9f549946-96sgr    1/1     Running   0         26h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the catalog source:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get catalogsource -n openshift-marketplace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                  DISPLAY               TYPE PUBLISHER  AGE
my-operator-catalog   My Operator Catalog   grpc            5s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the package manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get packagemanifest -n openshift-marketplace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                          CATALOG               AGE
jaeger-product                My Operator Catalog   93s</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>You can now install the Operators from the <emphasis role="strong">OperatorHub</emphasis> page on your OpenShift Container Platform web console.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../operators/admin/olm-managing-custom-catalogs.xml#olm-accessing-images-private-registries_olm-managing-custom-catalogs">Accessing images for Operators from private registries</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../operators/understanding/olm/olm-understanding-olm.xml#olm-catalogsource-image-template_olm-understanding-olm">Image template for custom catalog sources</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../openshift_images/managing_images/image-pull-policy.xml#image-pull-policy">Image pull policy</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="olm-installing-operators-from-operatorhub_post-install-preparing-for-users">
<title>About Operator installation with OperatorHub</title>
<simpara>OperatorHub is a user interface for discovering Operators; it works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster.</simpara>
<simpara>As a cluster administrator, you can install an Operator from OperatorHub by using the OpenShift Container Platform
web console or CLI. Subscribing an Operator to one or more namespaces makes the Operator available to developers on your cluster.</simpara>
<simpara>During installation, you must determine the following initial settings for the Operator:</simpara>
<variablelist>
<varlistentry>
<term>Installation Mode</term>
<listitem>
<simpara>Choose <emphasis role="strong">All namespaces on the cluster (default)</emphasis> to have the Operator installed on all namespaces or choose individual namespaces, if available, to only install the Operator on selected namespaces. This example chooses <emphasis role="strong">All namespaces&#8230;&#8203;</emphasis> to make the Operator available to all users and projects.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Update Channel</term>
<listitem>
<simpara>If an Operator is available through multiple channels, you can choose which channel you want to subscribe to. For example, to deploy from the <emphasis role="strong">stable</emphasis> channel, if available, select it from the list.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Approval Strategy</term>
<listitem>
<simpara>You can choose automatic or manual updates.</simpara>
<simpara>If you choose automatic updates for an installed Operator, when a new version of that Operator is available in the selected channel, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without human intervention.</simpara>
<simpara>If you select manual updates, when a newer version of an Operator is available, OLM creates an update request. As a
cluster administrator,
you must then manually approve that update request to have the Operator updated to the new version.</simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="olm-installing-from-operatorhub-using-web-console_post-install-preparing-for-users">
<title>Installing from OperatorHub using the web console</title>
<simpara>You can install and subscribe to an Operator from OperatorHub by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to an OpenShift Container Platform cluster using an account with
<literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate in the web console to the <emphasis role="strong">Operators → OperatorHub</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Scroll or type a keyword into the <emphasis role="strong">Filter by keyword</emphasis> box to find the Operator you want. For example, type <literal>jaeger</literal> to find the Jaeger Operator.</simpara>
<simpara>You can also filter options by <emphasis role="strong">Infrastructure Features</emphasis>. For example, select <emphasis role="strong">Disconnected</emphasis> if you want to see Operators that work in disconnected environments, also known as restricted network environments.</simpara>
</listitem>
<listitem>
<simpara>Select the Operator to display additional information.</simpara>
<note>
<simpara>Choosing a Community Operator warns that Red Hat does not certify Community Operators; you must acknowledge the warning before continuing.</simpara>
</note>
</listitem>
<listitem>
<simpara>Read the information about the Operator and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Select one of the following:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">All namespaces on the cluster (default)</emphasis> installs the Operator in the default <literal>openshift-operators</literal> namespace to watch and be made available to all namespaces in the cluster. This option is not always available.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">A specific namespace on the cluster</emphasis> allows you to choose a specific, single namespace in which to install the Operator. The Operator will only watch and be made available for use in this single namespace.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If the cluster is in AWS STS mode, enter the Amazon Resource Name (ARN) of the AWS IAM role of your service account in the <emphasis role="strong">role ARN</emphasis> field.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/oadp-install-operator-role-arn.png"/>
</imageobject>
<textobject><phrase>Entering the ARN</phrase></textobject>
</mediaobject>
</informalfigure>
<simpara>To create the role&#8217;s ARN, follow the procedure described in <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_service_on_aws/4/html/tutorials/cloud-experts-deploy-api-data-protection#prepare-aws-account_cloud-experts-deploy-api-data-protection">Preparing AWS account</link>.</simpara>
</listitem>
<listitem>
<simpara>If more than one update channel is available, select an <emphasis role="strong">Update channel</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Automatic</emphasis> or <emphasis role="strong">Manual</emphasis> approval strategy, as described earlier.</simpara>
<important>
<simpara>If the web console shows that the cluster is in "STS mode", you must set <emphasis role="strong">Update approval</emphasis> to <emphasis role="strong">Manual</emphasis>.</simpara>
<simpara>Subscriptions with automatic update approvals are not recommended because there might be permission changes to make prior to updating. Subscriptions with manual update approvals ensure that administrators have the opportunity to verify the permissions of the later version and take any necessary steps prior to update.</simpara>
</important>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis> to make the Operator available to the selected namespaces on this OpenShift Container Platform cluster.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>If you selected a <emphasis role="strong">Manual</emphasis> approval strategy, the upgrade status of the subscription remains <emphasis role="strong">Upgrading</emphasis> until you review and approve the install plan.</simpara>
<simpara>After approving on the <emphasis role="strong">Install Plan</emphasis> page, the subscription upgrade status moves to <emphasis role="strong">Up to date</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you selected an <emphasis role="strong">Automatic</emphasis> approval strategy, the upgrade status should resolve to <emphasis role="strong">Up to date</emphasis> without intervention.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>After the upgrade status of the subscription is <emphasis role="strong">Up to date</emphasis>, select <emphasis role="strong">Operators → Installed Operators</emphasis> to verify that the cluster service version (CSV) of the installed Operator eventually shows up. The <emphasis role="strong">Status</emphasis> should ultimately resolve to <emphasis role="strong">InstallSucceeded</emphasis> in the relevant namespace.</simpara>
<note>
<simpara>For the <emphasis role="strong">All namespaces&#8230;&#8203;</emphasis> installation mode, the status resolves to <emphasis role="strong">InstallSucceeded</emphasis> in the <literal>openshift-operators</literal> namespace, but the status is <emphasis role="strong">Copied</emphasis> if you check in other namespaces.</simpara>
</note>
<simpara>If it does not:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check the logs in any pods in the <literal>openshift-operators</literal> project (or other relevant namespace if <emphasis role="strong">A specific namespace&#8230;&#8203;</emphasis> installation mode was selected) on the <emphasis role="strong">Workloads → Pods</emphasis> page that are reporting issues to troubleshoot further.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="olm-installing-operator-from-operatorhub-using-cli_post-install-preparing-for-users">
<title>Installing from OperatorHub using the CLI</title>
<simpara>Instead of using the OpenShift Container Platform web console, you can install an Operator from OperatorHub by using the CLI. Use the <literal>oc</literal> command to create or update a <literal>Subscription</literal> object.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to an OpenShift Container Platform cluster using an account with
<literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the list of Operators available to the cluster from OperatorHub:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get packagemanifests -n openshift-marketplace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                               CATALOG               AGE
3scale-operator                    Red Hat Operators     91m
advanced-cluster-management        Red Hat Operators     91m
amq7-cert-manager                  Red Hat Operators     91m
...
couchbase-enterprise-certified     Certified Operators   91m
crunchy-postgres-operator          Certified Operators   91m
mongodb-enterprise                 Certified Operators   91m
...
etcd                               Community Operators   91m
jaeger                             Community Operators   91m
kubefed                            Community Operators   91m
...</programlisting>
</para>
</formalpara>
<simpara>Note the catalog for your desired Operator.</simpara>
</listitem>
<listitem>
<simpara>Inspect your desired Operator to verify its supported install modes and available channels:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe packagemanifests &lt;operator_name&gt; -n openshift-marketplace</programlisting>
</listitem>
<listitem>
<simpara>An Operator group, defined by an <literal>OperatorGroup</literal> object, selects target namespaces in which to generate required RBAC access for all Operators in the same namespace as the Operator group.</simpara>
<simpara>The namespace to which you subscribe the Operator must have an Operator group that matches the install mode of the Operator, either the <literal>AllNamespaces</literal> or <literal>SingleNamespace</literal> mode. If the Operator you intend to install uses the <literal>AllNamespaces</literal>, then the <literal>openshift-operators</literal> namespace already has an appropriate Operator group in place.</simpara>
<simpara>However, if the Operator uses the <literal>SingleNamespace</literal> mode and you do not already have an appropriate Operator group in place, you must create one.</simpara>
<note>
<simpara>The web console version of this procedure handles the creation of the <literal>OperatorGroup</literal> and <literal>Subscription</literal> objects automatically behind the scenes for you when choosing <literal>SingleNamespace</literal> mode.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create an <literal>OperatorGroup</literal> object YAML file, for example <literal>operatorgroup.yaml</literal>:</simpara>
<formalpara>
<title>Example <literal>OperatorGroup</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: &lt;operatorgroup_name&gt;
  namespace: &lt;namespace&gt;
spec:
  targetNamespaces:
  - &lt;namespace&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create the <literal>OperatorGroup</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f operatorgroup.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>Subscription</literal> object YAML file to subscribe a namespace to an Operator, for example <literal>sub.yaml</literal>:</simpara>
<formalpara>
<title>Example <literal>Subscription</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: &lt;subscription_name&gt;
  namespace: openshift-operators <co xml:id="CO166-1"/>
spec:
  channel: &lt;channel_name&gt; <co xml:id="CO166-2"/>
  name: &lt;operator_name&gt; <co xml:id="CO166-3"/>
  source: redhat-operators <co xml:id="CO166-4"/>
  sourceNamespace: openshift-marketplace <co xml:id="CO166-5"/>
  config:
    env: <co xml:id="CO166-6"/>
    - name: ARGS
      value: "-v=10"
    envFrom: <co xml:id="CO166-7"/>
    - secretRef:
        name: license-secret
    volumes: <co xml:id="CO166-8"/>
    - name: &lt;volume_name&gt;
      configMap:
        name: &lt;configmap_name&gt;
    volumeMounts: <co xml:id="CO166-9"/>
    - mountPath: &lt;directory_name&gt;
      name: &lt;volume_name&gt;
    tolerations: <co xml:id="CO166-10"/>
    - operator: "Exists"
    resources: <co xml:id="CO166-11"/>
      requests:
        memory: "64Mi"
        cpu: "250m"
      limits:
        memory: "128Mi"
        cpu: "500m"
    nodeSelector: <co xml:id="CO166-12"/>
      foo: bar</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO166-1">
<para>For default <literal>AllNamespaces</literal> install mode usage, specify the <literal>openshift-operators</literal> namespace. Alternatively, you can specify a custom global namespace, if you have created one. Otherwise, specify the relevant single namespace for <literal>SingleNamespace</literal> install mode usage.</para>
</callout>
<callout arearefs="CO166-2">
<para>Name of the channel to subscribe to.</para>
</callout>
<callout arearefs="CO166-3">
<para>Name of the Operator to subscribe to.</para>
</callout>
<callout arearefs="CO166-4">
<para>Name of the catalog source that provides the Operator.</para>
</callout>
<callout arearefs="CO166-5">
<para>Namespace of the catalog source. Use <literal>openshift-marketplace</literal> for the default OperatorHub catalog sources.</para>
</callout>
<callout arearefs="CO166-6">
<para>The <literal>env</literal> parameter defines a list of Environment Variables that must exist in all containers in the pod created by OLM.</para>
</callout>
<callout arearefs="CO166-7">
<para>The <literal>envFrom</literal> parameter defines a list of sources to populate Environment Variables in the container.</para>
</callout>
<callout arearefs="CO166-8">
<para>The <literal>volumes</literal> parameter defines a list of Volumes that must exist on the pod created by OLM.</para>
</callout>
<callout arearefs="CO166-9">
<para>The <literal>volumeMounts</literal> parameter defines a list of volume mounts that must exist in all containers in the pod created by OLM. If a <literal>volumeMount</literal> references a <literal>volume</literal> that does not exist, OLM fails to deploy the Operator.</para>
</callout>
<callout arearefs="CO166-10">
<para>The <literal>tolerations</literal> parameter defines a list of Tolerations for the pod created by OLM.</para>
</callout>
<callout arearefs="CO166-11">
<para>The <literal>resources</literal> parameter defines resource constraints for all the containers in the pod created by OLM.</para>
</callout>
<callout arearefs="CO166-12">
<para>The <literal>nodeSelector</literal> parameter defines a <literal>NodeSelector</literal> for the pod created by OLM.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If the cluster is in STS mode, include the following fields in the <literal>Subscription</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Subscription
# ...
spec:
  installPlanApproval: Manual <co xml:id="CO167-1"/>
  config:
    env:
    - name: ROLEARN
      value: "&lt;role_arn&gt;" <co xml:id="CO167-2"/></programlisting>
<calloutlist>
<callout arearefs="CO167-1">
<para>Subscriptions with automatic update approvals are not recommended because there might be permission changes to make prior to updating. Subscriptions with manual update approvals ensure that administrators have the opportunity to verify the permissions of the later version and take any necessary steps prior to update.</para>
</callout>
<callout arearefs="CO167-2">
<para>Include the role ARN details.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>Subscription</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f sub.yaml</programlisting>
<simpara>At this point, OLM is now aware of the selected Operator. A cluster service version (CSV) for the Operator should appear in the target namespace, and APIs provided by the Operator should be available for creation.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../operators/understanding/olm/olm-understanding-operatorgroups.xml#olm-operatorgroups-about_olm-understanding-operatorgroups">About OperatorGroups</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="configuring-alert-notifications">
<title>Configuring alert notifications</title>

<simpara>In OpenShift Container Platform, an alert is fired when the conditions defined in an alerting rule are true. An alert provides a notification that a set of circumstances are apparent within a cluster. Firing alerts can be viewed in the Alerting UI in the OpenShift Container Platform web console by default. After an installation, you can configure OpenShift Container Platform to send alert notifications to external systems.</simpara>
<section xml:id="sending-notifications-to-external-systems_configuring-alert-notifications">
<title>Sending notifications to external systems</title>
<simpara>In OpenShift Container Platform 4.14, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure OpenShift Container Platform to send alerts to the following receiver types:</simpara>
<itemizedlist>
<listitem>
<simpara>PagerDuty</simpara>
</listitem>
<listitem>
<simpara>Webhook</simpara>
</listitem>
<listitem>
<simpara>Email</simpara>
</listitem>
<listitem>
<simpara>Slack</simpara>
</listitem>
</itemizedlist>
<simpara>Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review.</simpara>
<formalpara>
<title>Checking that alerting is operational by using the watchdog alert</title>
<para>OpenShift Container Platform monitoring includes a watchdog alert that fires continuously. Alertmanager repeatedly sends watchdog alert notifications to configured notification providers. The provider is usually configured to notify an administrator when it stops receiving the watchdog alert. This mechanism helps you quickly identify any communication issues between Alertmanager and the notification provider.</para>
</formalpara>
<section xml:id="configuring-alert-receivers_configuring-alert-notifications">
<title>Configuring alert receivers</title>
<simpara>You can configure alert receivers to ensure that you learn about important issues with your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, navigate to <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Cluster Settings</emphasis> &#8594; <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Alertmanager</emphasis>.</simpara>
<note>
<simpara>Alternatively, you can navigate to the same page through the notification drawer. Select the bell icon at the top right of the OpenShift Container Platform web console and choose <emphasis role="strong">Configure</emphasis> in the <emphasis role="strong">AlertmanagerReceiverNotConfigured</emphasis> alert.</simpara>
</note>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Create Receiver</emphasis> in the <emphasis role="strong">Receivers</emphasis> section of the page.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Create Receiver</emphasis> form, add a <emphasis role="strong">Receiver Name</emphasis> and choose a <emphasis role="strong">Receiver Type</emphasis> from the list.</simpara>
</listitem>
<listitem>
<simpara>Edit the receiver configuration:</simpara>
<itemizedlist>
<listitem>
<simpara>For PagerDuty receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Choose an integration type and add a PagerDuty integration key.</simpara>
</listitem>
<listitem>
<simpara>Add the URL of your PagerDuty installation.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the client and incident details or the severity specification.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For webhook receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the endpoint to send HTTP POST requests to.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the default option to send resolved alerts to the receiver.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For email receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the email address to send notifications to.</simpara>
</listitem>
<listitem>
<simpara>Add SMTP configuration details, including the address to send notifications from, the smarthost and port number used for sending emails, the hostname of the SMTP server, and authentication details.</simpara>
</listitem>
<listitem>
<simpara>Choose whether TLS is required.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the default option not to send resolved alerts to the receiver or edit the body of email notifications configuration.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For Slack receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the URL of the Slack webhook.</simpara>
</listitem>
<listitem>
<simpara>Add the Slack channel or user name to send notifications to.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the default option not to send resolved alerts to the receiver or edit the icon and username configuration. You can also choose whether to find and link channel names and usernames.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>By default, firing alerts with labels that match all of the selectors will be sent to the receiver. If you want label values for firing alerts to be matched exactly before they are sent to the receiver:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add routing label names and values in the <emphasis role="strong">Routing Labels</emphasis> section of the form.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Regular Expression</emphasis> if want to use a regular expression.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Add Label</emphasis> to add further routing labels.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Create</emphasis> to create the receiver.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-alert-notifications-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../monitoring/monitoring-overview.xml#monitoring-overview">Monitoring overview</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../monitoring/managing-alerts.xml#managing-alerts">Managing alerts</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="connected-to-disconnected">
<title>Converting a connected cluster to a disconnected cluster</title>

<simpara>There might be some scenarios where you need to convert your OpenShift Container Platform cluster from a connected cluster to a disconnected cluster.</simpara>
<simpara>A disconnected cluster, also known as a restricted cluster, does not have an active connection to the internet. As such, you must mirror the contents of your registries and installation media. You can create this mirror registry on a host that can access both the internet and your closed network, or copy images to a device that you can move across network boundaries.</simpara>
<simpara>This topic describes the general process for converting an existing, connected cluster into a disconnected cluster.</simpara>
<section xml:id="installation-about-mirror-registry_connected-to-disconnected">
<title>About the mirror registry</title>
<simpara>You can mirror the images that are required for OpenShift Container Platform installation and subsequent product updates to a container mirror registry such as Red Hat Quay, JFrog Artifactory, Sonatype Nexus Repository, or Harbor. If you do not have access to a large-scale container registry, you can use the <emphasis>mirror registry for Red Hat OpenShift</emphasis>, a small-scale container registry included with OpenShift Container Platform subscriptions.</simpara>
<simpara>You can use any container registry that supports <link xlink:href="https://docs.docker.com/registry/spec/manifest-v2-2">Docker v2-2</link>, such as Red Hat Quay, the <emphasis>mirror registry for Red Hat OpenShift</emphasis>, Artifactory, Sonatype Nexus Repository, or Harbor. Regardless of your chosen registry, the procedure to mirror content from Red Hat hosted sites on the internet to an isolated image registry is the same. After you mirror the content, you configure each cluster to retrieve this content from your mirror registry.</simpara>
<important>
<simpara>The OpenShift image registry cannot be used as the target registry because it does not support pushing without a tag, which is required during the mirroring process.</simpara>
</important>
<simpara>If choosing a container registry that is not the <emphasis>mirror registry for Red Hat OpenShift</emphasis>, it must be reachable by every machine in the clusters that you provision. If the registry is unreachable, installation, updating, or normal operations such as workload relocation might fail. For that reason, you must run mirror registries in a highly available way, and the mirror registries must at least match the production availability of your OpenShift Container Platform clusters.</simpara>
<simpara>When you populate your mirror registry with OpenShift Container Platform images, you can follow two scenarios. If you have a host that can access both the internet and your mirror registry, but not your cluster nodes, you can directly mirror the content from that machine. This process is referred to as <emphasis>connected mirroring</emphasis>. If you have no such host, you must mirror the images to a file system and then bring that host or removable media into your restricted environment. This process is referred to as <emphasis>disconnected mirroring</emphasis>.</simpara>
<simpara>For mirrored registries, to view the source of pulled images, you must review the <literal>Trying to access</literal> log entry in the CRI-O logs. Other methods to view the image pull source, such as using the <literal>crictl images</literal> command on a node, show the non-mirrored image name, even though the image is pulled from the mirrored location.</simpara>
<note>
<simpara>Red Hat does not test third party registries with OpenShift Container Platform.</simpara>
</note>
</section>
<section xml:id="prerequisites_connected-to-disconnected">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>The <literal>oc</literal> client is installed.</simpara>
</listitem>
<listitem>
<simpara>A running cluster.</simpara>
</listitem>
<listitem>
<simpara>An installed mirror registry, which is a container image registry that supports <link xlink:href="https://docs.docker.com/registry/spec/manifest-v2-2/">Docker v2-2</link> in the location that will host the OpenShift Container Platform cluster, such as one of the following registries:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://www.redhat.com/en/technologies/cloud-computing/quay">Red Hat Quay</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://jfrog.com/artifactory/">JFrog Artifactory</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://www.sonatype.com/products/repository-oss?topnav=true">Sonatype Nexus Repository</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://goharbor.io/">Harbor</link></simpara>
</listitem>
</itemizedlist>
<simpara>If you have an subscription to Red Hat Quay, see the documentation on deploying Red Hat Quay <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploy_red_hat_quay_for_proof-of-concept_non-production_purposes/">for proof-of-concept purposes</link> or <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html/deploying_the_red_hat_quay_operator_on_openshift_container_platform/index">by using the Quay Operator</link>.</simpara>
</listitem>
<listitem>
<simpara>The mirror repository must be configured to share images. For example, a Red Hat Quay repository requires <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_quay/3/html-single/use_red_hat_quay/index#user-org-intro_use-quay">Organizations</link> in order to share images.</simpara>
</listitem>
<listitem>
<simpara>Access to the internet to obtain the necessary container images.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="connected-to-disconnected-prepare-mirror_connected-to-disconnected">
<title>Preparing the cluster for mirroring</title>
<simpara>Before disconnecting your cluster, you must mirror, or copy, the images to a mirror registry that is reachable by every node in your disconnected cluster. In order to mirror the images, you must prepare your cluster by:</simpara>
<itemizedlist>
<listitem>
<simpara>Adding the mirror registry certificates to the list of trusted CAs on your host.</simpara>
</listitem>
<listitem>
<simpara>Creating a <literal>.dockerconfigjson</literal> file that contains your image pull secret, which is from the <literal>cloud.openshift.com</literal> token.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configuring credentials that allow image mirroring:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the CA certificate for the mirror registry, in the simple PEM or DER file formats, to the list of trusted CAs. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cp &lt;/path/to/cert.crt&gt; /usr/share/pki/ca-trust-source/anchors/</programlisting>
<variablelist>
<varlistentry>
<term>where</term>
<term><literal>&lt;/path/to/cert.crt&gt;</literal></term>
<listitem>
<simpara>Specifies the path to the certificate on your local file system.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Update the CA trust. For example, in Linux:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ update-ca-trust</programlisting>
</listitem>
<listitem>
<simpara>Extract the <literal>.dockerconfigjson</literal> file from the global pull secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc extract secret/pull-secret -n openshift-config --confirm --to=.</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">.dockerconfigjson</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Edit the <literal>.dockerconfigjson</literal> file to add your mirror registry and authentication credentials and save it as a new file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{"auths":{"&lt;local_registry&gt;": {"auth": "&lt;credentials&gt;","email": "you@example.com"}}},"&lt;registry&gt;:&lt;port&gt;/&lt;namespace&gt;/":{"auth":"&lt;token&gt;"}}}</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;local_registry&gt;</literal></term>
<listitem>
<simpara>Specifies the registry domain name, and optionally the port, that your mirror registry uses to serve content.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>auth</literal></term>
<listitem>
<simpara>Specifies the base64-encoded user name and password for your mirror registry.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;registry&gt;:&lt;port&gt;/&lt;namespace&gt;</literal></term>
<listitem>
<simpara>Specifies the mirror registry details.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;token&gt;</literal></term>
<listitem>
<simpara>Specifies  the base64-encoded <literal>username:password</literal> for your mirror registry.</simpara>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ {"auths":{"cloud.openshift.com":{"auth":"b3BlbnNoaWZ0Y3UjhGOVZPT0lOMEFaUjdPUzRGTA==","email":"user@example.com"},
"quay.io":{"auth":"b3BlbnNoaWZ0LXJlbGVhc2UtZGOVZPT0lOMEFaUGSTd4VGVGVUjdPUzRGTA==","email":"user@example.com"},
"registry.connect.redhat.com"{"auth":"NTE3MTMwNDB8dWhjLTFEZlN3VHkxOSTd4VGVGVU1MdTpleUpoYkdjaUailA==","email":"user@example.com"},
"registry.redhat.io":{"auth":"NTE3MTMwNDB8dWhjLTFEZlN3VH3BGSTd4VGVGVU1MdTpleUpoYkdjaU9fZw==","email":"user@example.com"},
"registry.svc.ci.openshift.org":{"auth":"dXNlcjpyWjAwWVFjSEJiT2RKVW1pSmg4dW92dGp1SXRxQ3RGN1pwajJhN1ZXeTRV"},"my-registry:5000/my-namespace/":{"auth":"dXNlcm5hbWU6cGFzc3dvcmQ="}}}</programlisting>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="connected-to-disconnected-mirror-images_connected-to-disconnected">
<title>Mirroring the images</title>
<simpara>After the cluster is properly configured, you can mirror the images from your external repositories to the mirror repository.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Mirror the Operator Lifecycle Manager (OLM) images:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm catalog mirror registry.redhat.io/redhat/redhat-operator-index:v{product-version} &lt;mirror_registry&gt;:&lt;port&gt;/olm -a &lt;reg_creds&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>product-version</literal></term>
<listitem>
<simpara>Specifies the tag that corresponds to the version of OpenShift Container Platform to install, such as <literal>4.8</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>mirror_registry</literal></term>
<listitem>
<simpara>Specifies the fully qualified domain name (FQDN) for the target registry and namespace to mirror the Operator content to, where <literal>&lt;namespace&gt;</literal> is any existing namespace on the registry.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>reg_creds</literal></term>
<listitem>
<simpara>Specifies the location of your modified <literal>.dockerconfigjson</literal> file.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm catalog mirror registry.redhat.io/redhat/redhat-operator-index:v4.8 mirror.registry.com:443/olm -a ./.dockerconfigjson  --index-filter-by-os='.*'</programlisting>
</listitem>
<listitem>
<simpara>Mirror the content for any other Red Hat-provided Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm catalog mirror &lt;index_image&gt; &lt;mirror_registry&gt;:&lt;port&gt;/&lt;namespace&gt; -a &lt;reg_creds&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>index_image</literal></term>
<listitem>
<simpara>Specifies the index image for the catalog that you want to mirror.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>mirror_registry</literal></term>
<listitem>
<simpara>Specifies the FQDN for the target registry and namespace to mirror the Operator content to, where <literal>&lt;namespace&gt;</literal> is any existing namespace on the registry.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>reg_creds</literal></term>
<listitem>
<simpara>Optional: Specifies the location of your registry credentials file, if required.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm catalog mirror registry.redhat.io/redhat/community-operator-index:v4.8 mirror.registry.com:443/olm -a ./.dockerconfigjson  --index-filter-by-os='.*'</programlisting>
</listitem>
<listitem>
<simpara>Mirror the OpenShift Container Platform image repository:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release mirror -a .dockerconfigjson --from=quay.io/openshift-release-dev/ocp-release:v&lt;product-version&gt;-&lt;architecture&gt; --to=&lt;local_registry&gt;/&lt;local_repository&gt; --to-release-image=&lt;local_registry&gt;/&lt;local_repository&gt;:v&lt;product-version&gt;-&lt;architecture&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>product-version</literal></term>
<listitem>
<simpara>Specifies the tag that corresponds to the version of OpenShift Container Platform to install, such as <literal>4.8.15-x86_64</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>architecture</literal></term>
<listitem>
<simpara>Specifies the type of architecture for your server, such as <literal>x86_64</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>local_registry</literal></term>
<listitem>
<simpara>Specifies the registry domain name for your mirror repository.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>local_repository</literal></term>
<listitem>
<simpara>Specifies the name of the repository to create in your registry, such as <literal>ocp4/openshift4</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm release mirror -a .dockerconfigjson --from=quay.io/openshift-release-dev/ocp-release:4.8.15-x86_64 --to=mirror.registry.com:443/ocp/release --to-release-image=mirror.registry.com:443/ocp/release:4.8.15-x86_64</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">info: Mirroring 109 images to mirror.registry.com/ocp/release ...
mirror.registry.com:443/
  ocp/release
	manifests:
  	sha256:086224cadce475029065a0efc5244923f43fb9bb3bb47637e0aaf1f32b9cad47 -&gt; 4.8.15-x86_64-thanos
  	sha256:0a214f12737cb1cfbec473cc301aa2c289d4837224c9603e99d1e90fc00328db -&gt; 4.8.15-x86_64-kuryr-controller
  	sha256:0cf5fd36ac4b95f9de506623b902118a90ff17a07b663aad5d57c425ca44038c -&gt; 4.8.15-x86_64-pod
  	sha256:0d1c356c26d6e5945a488ab2b050b75a8b838fc948a75c0fa13a9084974680cb -&gt; 4.8.15-x86_64-kube-client-agent

…..
sha256:66e37d2532607e6c91eedf23b9600b4db904ce68e92b43c43d5b417ca6c8e63c mirror.registry.com:443/ocp/release:4.5.41-multus-admission-controller
sha256:d36efdbf8d5b2cbc4dcdbd64297107d88a31ef6b0ec4a39695915c10db4973f1 mirror.registry.com:443/ocp/release:4.5.41-cluster-kube-scheduler-operator
sha256:bd1baa5c8239b23ecdf76819ddb63cd1cd6091119fecdbf1a0db1fb3760321a2 mirror.registry.com:443/ocp/release:4.5.41-aws-machine-controllers
info: Mirroring completed in 2.02s (0B/s)

Success
Update image:  mirror.registry.com:443/ocp/release:4.5.41-x86_64
Mirror prefix: mirror.registry.com:443/ocp/release</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Mirror any other registries, as needed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc image mirror &lt;online_registry&gt;/my/image:latest &lt;mirror_registry&gt;</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Additional information</title>
<listitem>
<simpara>For more information about mirroring Operator catalogs, see <link xlink:href="../operators/admin/olm-restricted-networks.xml#olm-mirror-catalog_olm-restricted-networks">Mirroring an Operator catalog</link>.</simpara>
</listitem>
<listitem>
<simpara>For more information about the <literal>oc adm catalog mirror</literal> command, see the <link xlink:href="../cli_reference/openshift_cli/administrator-cli-commands.xml#oc-adm-catalog-mirror">OpenShift CLI administrator command reference</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="connected-to-disconnected-config-registry_connected-to-disconnected">
<title>Configuring the cluster for the mirror registry</title>
<simpara>After creating and mirroring the images to the mirror registry, you must modify your cluster so that pods can pull images from the mirror registry.</simpara>
<simpara>You must:</simpara>
<itemizedlist>
<listitem>
<simpara>Add the mirror registry credentials to the global pull secret.</simpara>
</listitem>
<listitem>
<simpara>Add the mirror registry server certificate to the cluster.</simpara>
</listitem>
<listitem>
<simpara>Create an <literal>ImageContentSourcePolicy</literal> custom resource (ICSP), which associates the mirror registry with the source registry.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Add mirror registry credential to the cluster global pull-secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=&lt;pull_secret_location&gt; <co xml:id="CO168-1"/></programlisting>
<calloutlist>
<callout arearefs="CO168-1">
<para>Provide the path to the new pull secret file.</para>
</callout>
</calloutlist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=.mirrorsecretconfigjson</programlisting>
</listitem>
<listitem>
<simpara>Add the CA-signed mirror registry server certificate to the nodes in the cluster:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a config map that includes the server certificate for the mirror registry</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap &lt;config_map_name&gt; --from-file=&lt;mirror_address_host&gt;..&lt;port&gt;=$path/ca.crt -n openshift-config</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">S oc create configmap registry-config --from-file=mirror.registry.com..443=/root/certs/ca-chain.cert.pem -n openshift-config</programlisting>
</listitem>
<listitem>
<simpara>Use the config map to update the <literal>image.config.openshift.io/cluster</literal> custom resource (CR). OpenShift Container Platform applies the changes to this CR to all nodes in the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"&lt;config_map_name&gt;"}}}' --type=merge</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an ICSP to redirect container pull requests from the online registries to the mirror registry:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the <literal>ImageContentSourcePolicy</literal> custom resource:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1alpha1
kind: ImageContentSourcePolicy
metadata:
  name: mirror-ocp
spec:
  repositoryDigestMirrors:
  - mirrors:
    - mirror.registry.com:443/ocp/release <co xml:id="CO169-1"/>
    source: quay.io/openshift-release-dev/ocp-release <co xml:id="CO169-2"/>
  - mirrors:
    - mirror.registry.com:443/ocp/release
    source: quay.io/openshift-release-dev/ocp-v4.0-art-dev</programlisting>
<calloutlist>
<callout arearefs="CO169-1">
<para>Specifies the name of the mirror image registry and repository.</para>
</callout>
<callout arearefs="CO169-2">
<para>Specifies the online registry and repository containing the content that is mirrored.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the ICSP object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f registryrepomirror.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">imagecontentsourcepolicy.operator.openshift.io/mirror-ocp created</programlisting>
</para>
</formalpara>
<simpara>OpenShift Container Platform applies the changes to this CR to all nodes in the cluster.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the credentials, CA, and ICSP for mirror registry were added:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Log into a node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>config.json</literal> file for the credentials:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# cat /var/lib/kubelet/config.json</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{"auths":{"brew.registry.redhat.io":{"xx=="},"brewregistry.stage.redhat.io":{"auth":"xxx=="},"mirror.registry.com:443":{"auth":"xx="}}} <co xml:id="CO170-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO170-1">
<para>Ensure that the mirror registry and credentials are present.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Change to the <literal>certs.d</literal> directory</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# cd /etc/docker/certs.d/</programlisting>
</listitem>
<listitem>
<simpara>List the certificates in the <literal>certs.d</literal> directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# ls</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>image-registry.openshift-image-registry.svc.cluster.local:5000
image-registry.openshift-image-registry.svc:5000
mirror.registry.com:443 <co xml:id="CO171-1"/></screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO171-1">
<para>Ensure that the mirror registry is in the list.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check that the ICSP added the mirror registry to the <literal>registries.conf</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# cat /etc/containers/registries.conf</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"]

[[registry]]
  prefix = ""
  location = "quay.io/openshift-release-dev/ocp-release"
  mirror-by-digest-only = true

  [[registry.mirror]]
    location = "mirror.registry.com:443/ocp/release"

[[registry]]
  prefix = ""
  location = "quay.io/openshift-release-dev/ocp-v4.0-art-dev"
  mirror-by-digest-only = true

  [[registry.mirror]]
    location = "mirror.registry.com:443/ocp/release"</programlisting>
</para>
</formalpara>
<simpara>The <literal>registry.mirror</literal> parameters indicate that the mirror registry is searched before the original registry.</simpara>
</listitem>
<listitem>
<simpara>Exit the node.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# exit</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="connected-to-disconnected-verify_connected-to-disconnected">
<title>Ensure applications continue to work</title>
<simpara>Before disconnecting the cluster from the network, ensure that your cluster is working as expected and all of your applications are working as expected.</simpara>
<formalpara>
<title>Procedure</title>
<para>Use the following commands to check the status of your cluster:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Ensure your pods are running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terinal" linenumbering="unnumbered">NAMESPACE                                          NAME                                                          READY   STATUS      RESTARTS   AGE
kube-system                                        apiserver-watcher-ci-ln-47ltxtb-f76d1-mrffg-master-0          1/1     Running     0          39m
kube-system                                        apiserver-watcher-ci-ln-47ltxtb-f76d1-mrffg-master-1          1/1     Running     0          39m
kube-system                                        apiserver-watcher-ci-ln-47ltxtb-f76d1-mrffg-master-2          1/1     Running     0          39m
openshift-apiserver-operator                       openshift-apiserver-operator-79c7c646fd-5rvr5                 1/1     Running     3          45m
openshift-apiserver                                apiserver-b944c4645-q694g                                     2/2     Running     0          29m
openshift-apiserver                                apiserver-b944c4645-shdxb                                     2/2     Running     0          31m
openshift-apiserver                                apiserver-b944c4645-x7rf2                                     2/2     Running     0          33m
 ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Ensure your nodes are in the READY status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-47ltxtb-f76d1-mrffg-master-0         Ready    master   42m   v1.28.5
ci-ln-47ltxtb-f76d1-mrffg-master-1         Ready    master   42m   v1.28.5
ci-ln-47ltxtb-f76d1-mrffg-master-2         Ready    master   42m   v1.28.5
ci-ln-47ltxtb-f76d1-mrffg-worker-a-gsxbz   Ready    worker   35m   v1.28.5
ci-ln-47ltxtb-f76d1-mrffg-worker-b-5qqdx   Ready    worker   35m   v1.28.5
ci-ln-47ltxtb-f76d1-mrffg-worker-c-rjkpq   Ready    worker   34m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="connected-to-disconnected-disconnect_connected-to-disconnected">
<title>Disconnect the cluster from the network</title>
<simpara>After mirroring all the required repositories and configuring your cluster to work as a disconnected cluster, you can disconnect the cluster from the network.</simpara>
<note>
<simpara>The Insights Operator is degraded when the cluster loses its Internet connection. You can avoid this problem by temporarily <link xlink:href="../support/remote_health_monitoring/opting-out-of-remote-health-reporting.xml">disabling the Insights Operator</link> until you can restore it.</simpara>
</note>
</section>
<section xml:id="connected-to-disconnected-restore-insights_connected-to-disconnected">
<title>Restoring a degraded Insights Operator</title>
<simpara>Disconnecting the cluster from the network necessarily causes the cluster to lose the Internet connection. The Insights Operator becomes degraded because it requires access to <link xlink:href="https://console.redhat.com">Red Hat Insights</link>.</simpara>
<simpara>This topic describes how to recover from a degraded Insights Operator.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit your <literal>.dockerconfigjson</literal> file to remove the <literal>cloud.openshift.com</literal> entry, for example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">"cloud.openshift.com":{"auth":"&lt;hash&gt;","email":"user@example.com"}</programlisting>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
<listitem>
<simpara>Update the cluster secret with the edited <literal>.dockerconfigjson</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=./.dockerconfigjson</programlisting>
</listitem>
<listitem>
<simpara>Verify that the Insights Operator is no longer degraded:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get co insights</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
insights   4.5.41    True        False         False      3d</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="connected-to-disconnected-restore_connected-to-disconnected">
<title>Restoring the network</title>
<simpara>If you want to reconnect a disconnected cluster and pull images from online registries, delete the cluster&#8217;s ImageContentSourcePolicy (ICSP) objects. Without the ICSP, pull requests to external registries are no longer redirected to the mirror registry.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the ICSP objects in your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get imagecontentsourcepolicy</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                 AGE
mirror-ocp           6d20h
ocp4-index-0         6d18h
qe45-index-0         6d15h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete all the ICSP objects you created when disconnecting your cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete imagecontentsourcepolicy &lt;icsp_name&gt; &lt;icsp_name&gt; &lt;icsp_name&gt;</programlisting>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete imagecontentsourcepolicy mirror-ocp ocp4-index-0 qe45-index-0</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">imagecontentsourcepolicy.operator.openshift.io "mirror-ocp" deleted
imagecontentsourcepolicy.operator.openshift.io "ocp4-index-0" deleted
imagecontentsourcepolicy.operator.openshift.io "qe45-index-0" deleted</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Wait for all the nodes to restart and return to the READY status and verify that the <literal>registries.conf</literal> file is pointing to the original registries and not the mirror registries:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Log into a node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Examine the <literal>registries.conf</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# cat /etc/containers/registries.conf</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">unqualified-search-registries = ["registry.access.redhat.com", "docker.io"] <co xml:id="CO172-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO172-1">
<para>The <literal>registry</literal> and <literal>registry.mirror</literal> entries created by the ICSPs you deleted are removed.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="enabling-cluster-capabilities">
<title>Enabling cluster capabilities</title>

<simpara>Cluster administrators can enable cluster capabilities that were disabled prior to installation.</simpara>
<note>
<simpara>Cluster administrators cannot disable a cluster capability after it is enabled.</simpara>
</note>
<section xml:id="viewing_the_cluster_capabilities_enabling-cluster-capabilities">
<title>Viewing the cluster capabilities</title>
<simpara>As a cluster administrator, you can view the capabilities by using the <literal>clusterversion</literal> resource status.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To view the status of the cluster capabilities, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterversion version -o jsonpath='{.spec.capabilities}{"\n"}{.status.capabilities}{"\n"}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{"additionalEnabledCapabilities":["openshift-samples"],"baselineCapabilitySet":"None"}
{"enabledCapabilities":["openshift-samples"],"knownCapabilities":["CSISnapshot","Console","Insights","Storage","baremetal","marketplace","openshift-samples"]}</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting_baseline_capability_set_enabling-cluster-capabilities">
<title>Enabling the cluster capabilities by setting baseline capability set</title>
<simpara>As a cluster administrator, you can enable the capabilities by setting <literal>baselineCapabilitySet</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To set the <literal>baselineCapabilitySet</literal>, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch clusterversion version --type merge -p '{"spec":{"capabilities":{"baselineCapabilitySet":"vCurrent"}}}' <co xml:id="CO173-1"/></programlisting>
<calloutlist>
<callout arearefs="CO173-1">
<para>For <literal>baselineCapabilitySet</literal> you can specify <literal>vCurrent</literal>, <literal>v4.14</literal>, or <literal>None</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<simpara>The following table describes the <literal>baselineCapabilitySet</literal> values.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster capabilities <literal>baselineCapabilitySet</literal> values description</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="40*"/>
<colspec colname="col_2" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="middle">Value</entry>
<entry align="left" valign="middle">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>vCurrent</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify this option when you want to automatically add new, default capabilities that are introduced in new releases.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v4.11</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.11. By specifying <literal>v4.11</literal>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.11 are <literal>baremetal</literal>, <literal>MachineAPI</literal>, <literal>marketplace</literal>, and <literal>openshift-samples</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v4.12</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.12. By specifying <literal>v4.12</literal>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.12 are <literal>baremetal</literal>, <literal>MachineAPI</literal>, <literal>marketplace</literal>, <literal>openshift-samples</literal>, <literal>Console</literal>, <literal>Insights</literal>, <literal>Storage</literal>, and <literal>CSISnapshot</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v4.13</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.13. By specifying <literal>v4.13</literal>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.13 are <literal>baremetal</literal>, <literal>MachineAPI</literal>, <literal>marketplace</literal>, <literal>openshift-samples</literal>, <literal>Console</literal>, <literal>Insights</literal>, <literal>Storage</literal>, <literal>CSISnapshot</literal>, and <literal>NodeTuning</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v4.14</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.14. By specifying <literal>v4.14</literal>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.14 are <literal>baremetal</literal>, <literal>MachineAPI</literal>, <literal>marketplace</literal>, <literal>openshift-samples</literal>, <literal>Console</literal>, <literal>Insights</literal>, <literal>Storage</literal>, <literal>CSISnapshot</literal>, <literal>NodeTuning</literal>, <literal>ImageRegistry</literal>, <literal>Build</literal>, and <literal>DeploymentConfig</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>v4.15</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify this option when you want to enable the default capabilities for OpenShift Container Platform 4.15. By specifying <literal>v4.15</literal>, capabilities that are introduced in newer versions of OpenShift Container Platform are not enabled. The default capabilities in OpenShift Container Platform 4.15 are <literal>baremetal</literal>, <literal>MachineAPI</literal>, <literal>marketplace</literal>, <literal>OperatorLifecycleManager</literal>, <literal>openshift-samples</literal>, <literal>Console</literal>, <literal>Insights</literal>, <literal>Storage</literal>, <literal>CSISnapshot</literal>, <literal>NodeTuning</literal>, <literal>ImageRegistry</literal>, <literal>Build</literal>, and <literal>DeploymentConfig</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>None</literal></simpara></entry>
<entry align="left" valign="middle"><simpara>Specify when the other sets are too large, and you do not need any capabilities or want to fine-tune via <literal>additionalEnabledCapabilities</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="setting_additional_enabled_capabilities_enabling-cluster-capabilities">
<title>Enabling the cluster capabilities by setting additional enabled capabilities</title>
<simpara>As a cluster administrator, you can enable the cluster capabilities by setting <literal>additionalEnabledCapabilities</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the additional enabled capabilities by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterversion version -o jsonpath='{.spec.capabilities.additionalEnabledCapabilities}{"\n"}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">["openshift-samples"]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To set the <literal>additionalEnabledCapabilities</literal>, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch clusterversion/version --type merge -p '{"spec":{"capabilities":{"additionalEnabledCapabilities":["openshift-samples", "marketplace"]}}}'</programlisting>
</listitem>
</orderedlist>
<important>
<simpara>It is not possible to disable a capability which is already enabled in a cluster. The cluster version Operator (CVO) continues to reconcile the capability which is already enabled in the cluster.</simpara>
</important>
<simpara>If you try to disable a capability, then CVO shows the divergent spec:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterversion version -o jsonpath='{.status.conditions[?(@.type=="ImplicitlyEnabledCapabilities")]}{"\n"}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{"lastTransitionTime":"2022-07-22T03:14:35Z","message":"The following capabilities could not be disabled: openshift-samples","reason":"CapabilitiesImplicitlyEnabled","status":"True","type":"ImplicitlyEnabledCapabilities"}</programlisting>
</para>
</formalpara>
<note>
<simpara>During the cluster upgrades, it is possible that a given capability could be implicitly enabled. If a resource was already running on the cluster before the upgrade, then any capabilities that is part of the resource will be enabled. For example, during a cluster upgrade, a resource that is already running on the cluster has been changed to be part of the <literal>marketplace</literal> capability by the system. Even if a cluster administrator does not explicitly enabled the <literal>marketplace</literal> capability, it is implicitly enabled by the system.</simpara>
</note>
</section>
<section xml:id="additional-resources_enabling-cluster-capabilities" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../installing/cluster-capabilities.xml#cluster-capabilities">Cluster capabilities</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="post-install-configure-additional-devices-ibmz">
<title>Configuring additional devices in an IBM Z or IBM LinuxONE environment</title>

<simpara>After installing OpenShift Container Platform, you can configure additional devices for your cluster in an IBM Z&#174; or IBM&#174; LinuxONE environment, which is installed with z/VM. The following devices can be configured:</simpara>
<itemizedlist>
<listitem>
<simpara>Fibre Channel Protocol (FCP) host</simpara>
</listitem>
<listitem>
<simpara>FCP LUN</simpara>
</listitem>
<listitem>
<simpara>DASD</simpara>
</listitem>
<listitem>
<simpara>qeth</simpara>
</listitem>
</itemizedlist>
<simpara>You can configure devices by adding udev rules using the Machine Config Operator (MCO) or you can configure devices manually.</simpara>
<note>
<simpara>The procedures described here apply only to z/VM installations. If you have installed your cluster with RHEL KVM on IBM Z&#174; or IBM&#174; LinuxONE infrastructure, no additional configuration is needed inside the KVM guest after the devices were added to the KVM guests. However, both in z/VM and RHEL KVM environments the next steps to configure the Local Storage Operator and Kubernetes NMState Operator need to be applied.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/machine-configuration-tasks.xml#post-install-machine-configuration-tasks">Postinstallation machine configuration tasks</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="configure-additional-devices-using-mco_post-install-configure-additional-devices-ibmz">
<title>Configuring additional devices using the Machine Config Operator (MCO)</title>
<simpara>Tasks in this section describe how to use features of the Machine Config Operator (MCO) to configure additional devices in an IBM Z&#174; or IBM&#174; LinuxONE environment. Configuring devices with the MCO is persistent but only allows specific configurations for compute nodes. MCO does not allow control plane nodes to have different configurations.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster as a user with administrative privileges.</simpara>
</listitem>
<listitem>
<simpara>The device must be available to the z/VM guest.</simpara>
</listitem>
<listitem>
<simpara>The device is already attached.</simpara>
</listitem>
<listitem>
<simpara>The device is not included in the <literal>cio_ignore</literal> list, which can be set in the kernel parameters.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>MachineConfig</literal> object file with the following YAML:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker0
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,worker0]}
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker0: ""</programlisting>
</listitem>
</itemizedlist>
<section xml:id="configuring-fcp-host">
<title>Configuring a Fibre Channel Protocol (FCP) host</title>
<simpara>The following is an example of how to configure an FCP host adapter with N_Port Identifier Virtualization (NPIV) by adding a udev rule.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Take the following sample udev rule <literal>441-zfcp-host-0.0.8000.rules</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.8000", DRIVER=="zfcp", GOTO="cfg_zfcp_host_0.0.8000"
ACTION=="add", SUBSYSTEM=="drivers", KERNEL=="zfcp", TEST=="[ccw/0.0.8000]", GOTO="cfg_zfcp_host_0.0.8000"
GOTO="end_zfcp_host_0.0.8000"

LABEL="cfg_zfcp_host_0.0.8000"
ATTR{[ccw/0.0.8000]online}="1"

LABEL="end_zfcp_host_0.0.8000"</programlisting>
</listitem>
<listitem>
<simpara>Convert the rule to Base64 encoded by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ base64 /path/to/file/</programlisting>
</listitem>
<listitem>
<simpara>Copy the following MCO sample profile into a YAML file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <co xml:id="CO174-1"/>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <co xml:id="CO174-2"/>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-zfcp-host-0.0.8000.rules <co xml:id="CO174-3"/></programlisting>
<calloutlist>
<callout arearefs="CO174-1">
<para>The role you have defined in the machine config file.</para>
</callout>
<callout arearefs="CO174-2">
<para>The Base64 encoded string that you have generated in the previous step.</para>
</callout>
<callout arearefs="CO174-3">
<para>The path where the udev rule is located.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-fcp-lun">
<title>Configuring an FCP LUN</title>
<simpara>The following is an example of how to configure an FCP LUN by adding a udev rule. You can add new FCP LUNs or add additional paths to LUNs that are already configured with multipathing.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Take the following sample udev rule <literal>41-zfcp-lun-0.0.8000:0x500507680d760026:0x00bc000000000000.rules</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">ACTION=="add", SUBSYSTEMS=="ccw", KERNELS=="0.0.8000", GOTO="start_zfcp_lun_0.0.8207"
GOTO="end_zfcp_lun_0.0.8000"

LABEL="start_zfcp_lun_0.0.8000"
SUBSYSTEM=="fc_remote_ports", ATTR{port_name}=="0x500507680d760026", GOTO="cfg_fc_0.0.8000_0x500507680d760026"
GOTO="end_zfcp_lun_0.0.8000"

LABEL="cfg_fc_0.0.8000_0x500507680d760026"
ATTR{[ccw/0.0.8000]0x500507680d760026/unit_add}="0x00bc000000000000"
GOTO="end_zfcp_lun_0.0.8000"

LABEL="end_zfcp_lun_0.0.8000"</programlisting>
</listitem>
<listitem>
<simpara>Convert the rule to Base64 encoded by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ base64 /path/to/file/</programlisting>
</listitem>
<listitem>
<simpara>Copy the following MCO sample profile into a YAML file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <co xml:id="CO175-1"/>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <co xml:id="CO175-2"/>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-zfcp-lun-0.0.8000:0x500507680d760026:0x00bc000000000000.rules <co xml:id="CO175-3"/></programlisting>
<calloutlist>
<callout arearefs="CO175-1">
<para>The role you have defined in the machine config file.</para>
</callout>
<callout arearefs="CO175-2">
<para>The Base64 encoded string that you have generated in the previous step.</para>
</callout>
<callout arearefs="CO175-3">
<para>The path where the udev rule is located.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-dasd">
<title>Configuring DASD</title>
<simpara>The following is an example of how to configure a DASD device by adding a udev rule.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Take the following sample udev rule <literal>41-dasd-eckd-0.0.4444.rules</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.4444", DRIVER=="dasd-eckd", GOTO="cfg_dasd_eckd_0.0.4444"
ACTION=="add", SUBSYSTEM=="drivers", KERNEL=="dasd-eckd", TEST=="[ccw/0.0.4444]", GOTO="cfg_dasd_eckd_0.0.4444"
GOTO="end_dasd_eckd_0.0.4444"

LABEL="cfg_dasd_eckd_0.0.4444"
ATTR{[ccw/0.0.4444]online}="1"

LABEL="end_dasd_eckd_0.0.4444"</programlisting>
</listitem>
<listitem>
<simpara>Convert the rule to Base64 encoded by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ base64 /path/to/file/</programlisting>
</listitem>
<listitem>
<simpara>Copy the following MCO sample profile into a YAML file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <co xml:id="CO176-1"/>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <co xml:id="CO176-2"/>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-dasd-eckd-0.0.4444.rules <co xml:id="CO176-3"/></programlisting>
<calloutlist>
<callout arearefs="CO176-1">
<para>The role you have defined in the machine config file.</para>
</callout>
<callout arearefs="CO176-2">
<para>The Base64 encoded string that you have generated in the previous step.</para>
</callout>
<callout arearefs="CO176-3">
<para>The path where the udev rule is located.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="configuring-qeth">
<title>Configuring qeth</title>
<simpara>The following is an example of how to configure a qeth device by adding a udev rule.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Take the following sample udev rule <literal>41-qeth-0.0.1000.rules</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">ACTION=="add", SUBSYSTEM=="drivers", KERNEL=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.1000", DRIVER=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.1001", DRIVER=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccw", KERNEL=="0.0.1002", DRIVER=="qeth", GOTO="group_qeth_0.0.1000"
ACTION=="add", SUBSYSTEM=="ccwgroup", KERNEL=="0.0.1000", DRIVER=="qeth", GOTO="cfg_qeth_0.0.1000"
GOTO="end_qeth_0.0.1000"

LABEL="group_qeth_0.0.1000"
TEST=="[ccwgroup/0.0.1000]", GOTO="end_qeth_0.0.1000"
TEST!="[ccw/0.0.1000]", GOTO="end_qeth_0.0.1000"
TEST!="[ccw/0.0.1001]", GOTO="end_qeth_0.0.1000"
TEST!="[ccw/0.0.1002]", GOTO="end_qeth_0.0.1000"
ATTR{[drivers/ccwgroup:qeth]group}="0.0.1000,0.0.1001,0.0.1002"
GOTO="end_qeth_0.0.1000"

LABEL="cfg_qeth_0.0.1000"
ATTR{[ccwgroup/0.0.1000]online}="1"

LABEL="end_qeth_0.0.1000"</programlisting>
</listitem>
<listitem>
<simpara>Convert the rule to Base64 encoded by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ base64 /path/to/file/</programlisting>
</listitem>
<listitem>
<simpara>Copy the following MCO sample profile into a YAML file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
   labels:
     machineconfiguration.openshift.io/role: worker0 <co xml:id="CO177-1"/>
   name: 99-worker0-devices
spec:
   config:
     ignition:
       version: 3.2.0
     storage:
       files:
       - contents:
           source: data:text/plain;base64,&lt;encoded_base64_string&gt; <co xml:id="CO177-2"/>
         filesystem: root
         mode: 420
         path: /etc/udev/rules.d/41-dasd-eckd-0.0.4444.rules <co xml:id="CO177-3"/></programlisting>
<calloutlist>
<callout arearefs="CO177-1">
<para>The role you have defined in the machine config file.</para>
</callout>
<callout arearefs="CO177-2">
<para>The Base64 encoded string that you have generated in the previous step.</para>
</callout>
<callout arearefs="CO177-3">
<para>The path where the udev rule is located.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara><link xlink:href="../storage/persistent_storage/persistent_storage_local/persistent-storage-local.xml#persistent-storage-using-local-volume">Install and configure the Local Storage Operator</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.xml#k8s-nmstate-updating-node-network-config">Updating node network configuration</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configure-additional-devices-manually_post-install-configure-additional-devices-ibmz">
<title>Configuring additional devices manually</title>
<simpara>Tasks in this section describe how to manually configure additional devices in an IBM Z&#174; or IBM&#174; LinuxONE environment. This configuration method is persistent over node restarts but not OpenShift Container Platform native and you need to redo the steps if you replace the node.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster as a user with administrative privileges.</simpara>
</listitem>
<listitem>
<simpara>The device must be available to the node.</simpara>
</listitem>
<listitem>
<simpara>In a z/VM environment, the device must be attached to the z/VM guest.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Connect to the node via SSH by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh &lt;user&gt;@&lt;node_ip_address&gt;</programlisting>
<simpara>You can also start a debug session to the node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>To enable the devices with the <literal>chzdev</literal> command, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo chzdev -e 0.0.8000
  sudo chzdev -e 1000-1002
  sude chzdev -e 4444
  sudo chzdev -e 0.0.8000:0x500507680d760026:0x00bc000000000000</programlisting>
</listitem>
</orderedlist>
<formalpara role="_additional-resources">
<title>Additional resources</title>
<para>See <link xlink:href="https://www.ibm.com/docs/en/linux-on-systems?topic=linuxonibm/com.ibm.linux.z.ludd/ludd_c_perscfg.html">Persistent device configuration</link> in IBM&#174; Documentation.</para>
</formalpara>
</section>
<section xml:id="roce-network-cards">
<title>RoCE network Cards</title>
<simpara>RoCE (RDMA over Converged Ethernet) network cards do not need to be enabled and their interfaces can be configured with the Kubernetes NMState Operator whenever they are available in the node. For example, RoCE network cards are available if they are attached in a z/VM environment or passed through in a RHEL KVM environment.</simpara>
</section>
<section xml:id="enabling-multipathing-fcp-luns_post-install-configure-additional-devices-ibmz">
<title>Enabling multipathing for FCP LUNs</title>
<simpara>Tasks in this section describe how to manually configure additional devices in an IBM Z&#174; or IBM&#174; LinuxONE environment. This configuration method is persistent over node restarts but not OpenShift Container Platform native and you need to redo the steps if you replace the node.</simpara>
<important>
<simpara>On IBM Z&#174; and IBM&#174; LinuxONE, you can enable multipathing only if you configured your cluster for it during installation. For more information, see "Installing RHCOS and starting the OpenShift Container Platform bootstrap process" in <emphasis>Installing a cluster with z/VM on IBM Z&#174; and IBM&#174; LinuxONE</emphasis>.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to the cluster as a user with administrative privileges.</simpara>
</listitem>
<listitem>
<simpara>You have configured multiple paths to a LUN with either method explained above.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Connect to the node via SSH by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh &lt;user&gt;@&lt;node_ip_address&gt;</programlisting>
<simpara>You can also start a debug session to the node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>To enable multipathing, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo /sbin/mpathconf --enable</programlisting>
</listitem>
<listitem>
<simpara>To start the <literal>multipathd</literal> daemon, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo multipath</programlisting>
</listitem>
<listitem>
<simpara>Optional: To format your multipath device with fdisk, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo fdisk /dev/mapper/mpatha</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To verify that the devices have been grouped, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo multipath -II</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">mpatha (20017380030290197) dm-1 IBM,2810XIV
   size=512G features='1 queue_if_no_path' hwhandler='1 alua' wp=rw
	-+- policy='service-time 0' prio=50 status=enabled
 	|- 1:0:0:6  sde 68:16  active ready running
 	|- 1:0:1:6  sdf 69:24  active ready running
 	|- 0:0:0:6  sdg  8:80  active ready running
 	`- 0:0:1:6  sdh 66:48  active ready running</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara><link xlink:href="../storage/persistent_storage/persistent_storage_local/persistent-storage-local.xml#persistent-storage-using-local-volume">Install and configure the Local Storage Operator</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../networking/k8s_nmstate/k8s-nmstate-updating-node-network-config.xml#k8s-nmstate-updating-node-network-config">Updating node network configuration</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="post-install-vsphere-zones-regions-configuration">
<title>Multiple regions and zones configuration for a cluster on vSphere</title>

<simpara>As an administrator, you can specify multiple regions and zones for your OpenShift Container Platform cluster that runs on a VMware vSphere instance. This configuration reduces the risk of a hardware failure or network outage causing your cluster to fail.</simpara>
<simpara>A failure domain configuration lists parameters that create a topology. The following list states some of these parameters:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>computeCluster</literal></simpara>
</listitem>
<listitem>
<simpara><literal>datacenter</literal></simpara>
</listitem>
<listitem>
<simpara><literal>datastore</literal></simpara>
</listitem>
<listitem>
<simpara><literal>networks</literal></simpara>
</listitem>
<listitem>
<simpara><literal>resourcePool</literal></simpara>
</listitem>
</itemizedlist>
<simpara>After you define multiple regions and zones for your OpenShift Container Platform cluster, you can create or migrate nodes to another failure domain.</simpara>
<important>
<simpara>If you want to migrate pre-existing OpenShift Container Platform cluster compute nodes to a failure domain, you must define a new compute machine set for the compute node. This new machine set can scale up a compute node according to the topology of the failure domain, and scale down the pre-existing compute node.</simpara>
<simpara>The cloud provider adds <literal>topology.kubernetes.io/zone</literal> and <literal>topology.kubernetes.io/region</literal> labels to any compute node provisioned by a machine set resource.</simpara>
<simpara>For more information, see <link xlink:href="../machine_management/creating_machinesets/creating-machineset-vsphere.xml">Creating a compute machine set</link>.</simpara>
</important>
<section xml:id="specifying-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration">
<title>Specifying multiple regions and zones for your cluster on vSphere</title>
<simpara>You can configure the <literal>infrastructures.config.openshift.io</literal> configuration resource to specify multiple regions and zones for your OpenShift Container Platform cluster that runs on a VMware vSphere instance.</simpara>
<simpara>Topology-aware features for the cloud controller manager and the vSphere Container Storage Interface (CSI) Operator Driver require information about the vSphere topology where you host your OpenShift Container Platform cluster. This topology information exists in the <literal>infrastructures.config.openshift.io</literal> configuration resource.</simpara>
<simpara>Before you specify regions and zones for your cluster, you must ensure that all datacenters and compute clusters contain tags, so that the cloud provider can add labels to your node. For example, if <literal>datacenter-1</literal> represents <literal>region-a</literal> and <literal>compute-cluster-1</literal> represents <literal>zone-1</literal>, the cloud provider adds an <literal>openshift-region</literal> category label with a value of <literal>region-a</literal> to <literal>datacenter-1</literal>.  Additionally, the cloud provider adds an <literal>openshift-zone</literal> category tag with a value of <literal>zone-1</literal> to <literal>compute-cluster-1</literal>.</simpara>
<note>
<simpara>You can migrate control plane nodes with vMotion capabilities to a failure domain. After you add these nodes to a failure domain, the cloud provider adds <literal>topology.kubernetes.io/zone</literal> and <literal>topology.kubernetes.io/region</literal> labels to these nodes.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created the <literal>openshift-region</literal> and <literal>openshift-zone</literal> tag categories on the vCenter server.</simpara>
</listitem>
<listitem>
<simpara>You ensured that each datacenter and compute cluster contains tags that represent the name of their associated region or zone, or both.</simpara>
</listitem>
<listitem>
<simpara>Optional: If you defined <emphasis role="strong">API</emphasis> and <emphasis role="strong">Ingress</emphasis> static IP addresses to the installation program, you must ensure that all regions and zones share a common layer 2 network. This configuration ensures that API and Ingress Virtual IP (VIP) addresses can interact with your cluster.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>If you do not supply tags to all datacenters and compute clusters before you create a node or migrate a node, the cloud provider cannot add the <literal>topology.kubernetes.io/zone</literal> and <literal>topology.kubernetes.io/region</literal> labels to the node. This means that services cannot route traffic to your node.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>infrastructures.config.openshift.io</literal> custom resource definition (CRD) of your cluster to specify multiple regions and zones in the <literal>failureDomains</literal> section of the resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit infrastructures.config.openshift.io cluster</programlisting>
<formalpara>
<title>Example <literal>infrastructures.config.openshift.io</literal> CRD for a instance named <literal>cluster</literal> with multiple regions and zones defined in its configuration</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
  platformSpec:
    type: vSphere
    vsphere:
      vcenters:
        - datacenters:
            - &lt;region_a_datacenter&gt;
            - &lt;region_b_datacenter&gt;
          port: 443
          server: &lt;your_vcenter_server&gt;
      failureDomains:
        - name: &lt;failure_domain_1&gt;
          region: &lt;region_a&gt;
          zone: &lt;zone_a&gt;
          server: &lt;your_vcenter_server&gt;
          topology:
            datacenter: &lt;region_a_dc&gt;
            computeCluster: "&lt;/region_a_dc/host/zone_a_cluster&gt;"
            resourcePool: "&lt;/region_a_dc/host/zone_a_cluster/Resources/resource_pool&gt;"
            datastore: "&lt;/region_a_dc/datastore/datastore_a&gt;"
            networks:
            - port-group
        - name: &lt;failure_domain_2&gt;
          region: &lt;region_a&gt;
          zone: &lt;zone_b&gt;
          server: &lt;your_vcenter_server&gt;
          topology:
            computeCluster: &lt;/region_a_dc/host/zone_b_cluster&gt;
            datacenter: &lt;region_a_dc&gt;
            datastore: &lt;/region_a_dc/datastore/datastore_a&gt;
            networks:
            - port-group
        - name: &lt;failure_domain_3&gt;
          region: &lt;region_b&gt;
          zone: &lt;zone_a&gt;
          server: &lt;your_vcenter_server&gt;
          topology:
            computeCluster: &lt;/region_b_dc/host/zone_a_cluster&gt;
            datacenter: &lt;region_b_dc&gt;
            datastore: &lt;/region_b_dc/datastore/datastore_b&gt;
            networks:
            - port-group
      nodeNetworking:
        external: {}
        internal: {}</programlisting>
</para>
</formalpara>
<important>
<simpara>After you create a failure domain and you define it in a CRD for a VMware vSphere cluster, you must not modify or delete the failure domain. Doing any of these actions with this configuration can impact the availability and fault tolerance of a control plane machine.</simpara>
</important>
</listitem>
<listitem>
<simpara>Save the resource file to apply the changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/post-install-vsphere-zones-regions-configuration.xml#references-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration">Parameters for the cluster-wide infrastructure CRD</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="vsphere-enabling-multiple-layer2-network_post-install-vsphere-zones-regions-configuration">
<title>Enabling a multiple layer 2 network for your cluster</title>
<simpara>You can configure your cluster to use a multiple layer 2 network configuration so that data transfer among nodes can span across multiple networks.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured network connectivity among machines so that cluster components can communicate with each other.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>If you installed your cluster with installer-provisioned infrastructure, you must ensure that all control plane nodes share a common layer 2 network. Additionally, ensure compute nodes that are configured for Ingress pod scheduling share a common layer 2 network.</simpara>
<itemizedlist>
<listitem>
<simpara>If you need compute nodes to span multiple layer 2 networks, you can create infrastructure nodes that can host Ingress pods.</simpara>
</listitem>
<listitem>
<simpara>If you need to provision workloads across additional layer 2 networks, you can create compute machine sets on vSphere and then move these workloads to your target layer 2 networks.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If you installed your cluster on infrastructure that you provided, which is defined as a user-provisioned infrastructure, complete the following actions to meet your needs:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure your API load balancer and network so that the load balancer can reach the API and Machine Config Server on the control plane nodes.</simpara>
</listitem>
<listitem>
<simpara>Configure your Ingress load balancer and network so that the load balancer can reach the Ingress pods on the compute or infrastructure nodes.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/installing_vsphere/upi/installing-vsphere-network-customizations.xml#installation-network-connectivity-user-infra_installing-vsphere-network-customizations">Network connectivity requirements</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../machine_management/creating-infrastructure-machinesets.xml#creating-infrastructure-machinesets-production">Creating infrastructure machine sets for production environments</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../machine_management/creating_machinesets/creating-machineset-vsphere.xml#machineset-creating_creating-machineset-vsphere">Creating a compute machine set</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="references-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration">
<title>Parameters for the cluster-wide infrastructure CRD</title>
<simpara>You must set values for specific parameters in the cluster-wide infrastructure, <literal>infrastructures.config.openshift.io</literal>, Custom Resource Definition (CRD) to define multiple regions and zones for your OpenShift Container Platform cluster that runs on a VMware vSphere instance.</simpara>
<simpara>The following table lists mandatory parameters for defining multiple regions and zones for your OpenShift Container Platform cluster:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="66.6667*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>vcenters</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The vCenter server for your OpenShift Container Platform cluster. You can only specify one vCenter for your cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>datacenters</literal></simpara></entry>
<entry align="left" valign="top"><simpara>vCenter datacenters where VMs associated with the OpenShift Container Platform cluster will be created or presently exist.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>port</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The TCP port of the vCenter server.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>server</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The fully qualified domain name (FQDN) of the vCenter server.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>failureDomains</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The list of failure domains.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>region</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The value of the <literal>openshift-region</literal> tag assigned to the topology for the failure failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>zone</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The value of the <literal>openshift-zone</literal> tag assigned to the topology for the failure failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>topology</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The vCenter reources associated with the failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>datacenter</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The datacenter associated with the failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>computeCluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The full path of the compute cluster associated with the failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>resourcePool</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The full path of the resource pool associated with the failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>datastore</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The full path of the datastore associated with the failure domain.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>networks</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A list of port groups associated with the failure domain. Only one portgroup may be defined.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../post_installation_configuration/post-install-vsphere-zones-regions-configuration.xml#specifying-regions-zones-infrastructure-vsphere_post-install-vsphere-zones-regions-configuration">Specifying multiple regions and zones for your cluster on vSphere</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="coreos-layering">
<title>RHCOS image layering</title>

<simpara>Red Hat Enterprise Linux CoreOS (RHCOS) image layering allows you to easily extend the functionality of your base RHCOS image by <emphasis>layering</emphasis> additional images onto the base image. This layering does not modify the base RHCOS image. Instead, it creates a <emphasis>custom layered image</emphasis> that includes all RHCOS functionality and adds additional functionality to specific nodes in the cluster.</simpara>
<simpara>You create a custom layered image by using a Containerfile and applying it to nodes by using a <literal>MachineConfig</literal> object. The Machine Config Operator overrides the base RHCOS image, as specified by the <literal>osImageURL</literal> value in the associated machine config, and boots the new image. You can remove the custom layered image by deleting the machine config, The MCO reboots the nodes back to the base RHCOS image.</simpara>
<simpara>With RHCOS image layering, you can install RPMs into your base image, and your custom content will be booted alongside RHCOS. The Machine Config Operator (MCO) can roll out these custom layered images and monitor these custom containers in the same way it does for the default RHCOS image. RHCOS image layering gives you greater flexibility in how you manage your RHCOS nodes.</simpara>
<important>
<simpara>Installing realtime kernel and extensions RPMs as custom layered content is not recommended. This is because these RPMs can conflict with RPMs installed by using a machine config. If there is a conflict, the MCO enters a <literal>degraded</literal> state when it tries to install the machine config RPM. You need to remove the conflicting extension from your machine config before proceeding.</simpara>
</important>
<simpara>As soon as you apply the custom layered image to your cluster, you effectively <emphasis>take ownership</emphasis> of your custom layered images and those nodes. While Red Hat remains responsible for maintaining and updating the base RHCOS image on standard nodes, you are responsible for maintaining and updating images on nodes that use a custom layered image. You assume the responsibility for the package you applied with the custom layered image and any issues that might arise with the package.</simpara>
<simpara>To apply a custom layered image, you create a Containerfile that references an OpenShift Container Platform image and the RPM that you want to apply. You then push the resulting custom layered image to an image registry. In a non-production OpenShift Container Platform cluster, create a <literal>MachineConfig</literal> object for the targeted node pool that points to the new image.</simpara>
<note>
<simpara>Use the same base RHCOS image installed on the rest of your cluster. Use the <literal>oc adm release info --image-for rhel-coreos</literal> command to obtain the base image used in your cluster.</simpara>
</note>
<simpara>RHCOS image layering allows you to use the following types of images to create custom layered images:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">OpenShift Container Platform Hotfixes</emphasis>. You can work with Customer Experience and Engagement (CEE) to obtain and apply <link xlink:href="https://access.redhat.com/solutions/2996001">Hotfix packages</link> on top of your RHCOS image. In some instances, you might want a bug fix or enhancement before it is included in an official OpenShift Container Platform release. RHCOS image layering allows you to easily add the Hotfix before it is officially released and remove the Hotfix when the underlying RHCOS image incorporates the fix.</simpara>
<important>
<simpara>Some Hotfixes require a Red Hat Support Exception and are outside of the normal scope of OpenShift Container Platform support coverage or life cycle policies.</simpara>
</important>
<simpara>In the event you want a Hotfix, it will be provided to you based on <link xlink:href="https://access.redhat.com/solutions/2996001">Red Hat Hotfix policy</link>. Apply it on top of the base image and test that new custom layered image in a non-production environment. When you are satisfied that the custom layered image is safe to use in production, you can roll it out on your own schedule to specific node pools. For any reason, you can easily roll back the custom layered image and return to using the default RHCOS.</simpara>
<formalpara>
<title>Example Containerfile to apply a Hotfix</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># Using a 4.12.0 image
FROM quay.io/openshift-release-dev/ocp-release@sha256...
#Install hotfix rpm
RUN rpm-ostree override replace https://example.com/myrepo/haproxy-1.0.16-5.el8.src.rpm &amp;&amp; \
    rpm-ostree cleanup -m &amp;&amp; \
    ostree container commit</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">RHEL packages</emphasis>. You can download Red Hat Enterprise Linux (RHEL) packages from the <link xlink:href="https://access.redhat.com/downloads/content/479/ver=/rhel---9/9.1/x86_64/packages">Red Hat Customer Portal</link>, such as chrony, firewalld, and iputils.</simpara>
<formalpara>
<title>Example Containerfile to apply the firewalld utility</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">FROM quay.io/openshift-release-dev/ocp-release@sha256...
ADD configure-firewall-playbook.yml .
RUN rpm-ostree install firewalld ansible &amp;&amp; \
    ansible-playbook configure-firewall-playbook.yml &amp;&amp; \
    rpm -e ansible &amp;&amp; \
    ostree container commit</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example Containerfile to apply the libreswan utility</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">link:https://raw.githubusercontent.com/openshift/rhcos-image-layering-examples/master/libreswan/Containerfile[role=include]</programlisting>
</para>
</formalpara>
<simpara>Because libreswan requires additional RHEL packages, the image must be built on an entitled RHEL host.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Third-party packages</emphasis>. You can download and install RPMs from third-party organizations, such as the following types of packages:</simpara>
<itemizedlist>
<listitem>
<simpara>Bleeding edge drivers and kernel enhancements to improve performance or add capabilities.</simpara>
</listitem>
<listitem>
<simpara>Forensic client tools to investigate possible and actual break-ins.</simpara>
</listitem>
<listitem>
<simpara>Security agents.</simpara>
</listitem>
<listitem>
<simpara>Inventory agents that provide a coherent view of the entire cluster.</simpara>
</listitem>
<listitem>
<simpara>SSH Key management packages.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Example Containerfile to apply a third-party package from EPEL</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">link:https://raw.githubusercontent.com/openshift/rhcos-image-layering-examples/master/htop/Containerfile[role=include]</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example Containerfile to apply a third-party package that has RHEL dependencies</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">link:https://raw.githubusercontent.com/openshift/rhcos-image-layering-examples/master/fish/Containerfile[role=include]</programlisting>
</para>
</formalpara>
<simpara>This Containerfile installs the Linux fish program. Because fish requires additional RHEL packages, the image must be built on an entitled RHEL host.</simpara>
</listitem>
</itemizedlist>
<simpara>After you create the machine config, the Machine Config Operator (MCO) performs the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Renders a new machine config for the specified pool or pools.</simpara>
</listitem>
<listitem>
<simpara>Performs cordon and drain operations on the nodes in the pool or pools.</simpara>
</listitem>
<listitem>
<simpara>Writes the rest of the machine config parameters onto the nodes.</simpara>
</listitem>
<listitem>
<simpara>Applies the custom layered image to the node.</simpara>
</listitem>
<listitem>
<simpara>Reboots the node using the new image.</simpara>
</listitem>
</orderedlist>
<important>
<simpara>It is strongly recommended that you test your images outside of your production environment before rolling out to your cluster.</simpara>
</important>
<section xml:id="coreos-layering-configuring_coreos-layering">
<title>Applying a RHCOS custom layered image</title>
<simpara>You can easily configure Red Hat Enterprise Linux CoreOS (RHCOS) image layering on the nodes in specific machine config pools. The Machine Config Operator (MCO) reboots those nodes with the new custom layered image, overriding the base Red Hat Enterprise Linux CoreOS (RHCOS) image.</simpara>
<simpara>To apply a custom layered image to your cluster, you must have the custom layered image in a repository that your cluster can access. Then, create a <literal>MachineConfig</literal> object that points to the custom layered image. You need a separate <literal>MachineConfig</literal> object for each machine config pool that you want to configure.</simpara>
<important>
<simpara>When you configure a custom layered image, OpenShift Container Platform no longer automatically updates any node that uses the custom layered image. You become responsible for manually updating your nodes as appropriate. If you roll back the custom layer, OpenShift Container Platform will again automatically update the node. See the Additional resources section that follows for important information about updating nodes that use a custom layered image.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must create a custom layered image that is based on an OpenShift Container Platform image digest, not a tag.</simpara>
<note>
<simpara>You should use the same base RHCOS image that is installed on the rest of your cluster. Use the <literal>oc adm release info --image-for rhel-coreos</literal> command to obtain the base image being used in your cluster.</simpara>
</note>
<simpara>For example, the following Containerfile creates a custom layered image from an OpenShift Container Platform 4.14 image and overrides the kernel package with one from CentOS 9 Stream:</simpara>
<formalpara>
<title>Example Containerfile for a custom layer image</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># Using a 4.14.0 image
FROM quay.io/openshift-release/ocp-release@sha256... <co xml:id="CO178-1"/>
#Install hotfix rpm
RUN rpm-ostree cliwrap install-to-root / &amp;&amp; \ <co xml:id="CO178-2"/>
    rpm-ostree override replace http://mirror.stream.centos.org/9-stream/BaseOS/x86_64/os/Packages/kernel-{,core-,modules-,modules-core-,modules-extra-}5.14.0-295.el9.x86_64.rpm &amp;&amp; \ <co xml:id="CO178-3"/>
    rpm-ostree cleanup -m &amp;&amp; \
    ostree container commit</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO178-1">
<para>Specifies the RHCOS base image of your cluster.</para>
</callout>
<callout arearefs="CO178-2">
<para>Enables <literal>cliwrap</literal>. This is currently required to intercept some command invocations made from kernel scripts.</para>
</callout>
<callout arearefs="CO178-3">
<para>Replaces the kernel packages.</para>
</callout>
</calloutlist>
<note>
<simpara>Instructions on how to create a Containerfile are beyond the scope of this documentation.</simpara>
</note>
</listitem>
<listitem>
<simpara>Because the process for building a custom layered image is performed outside of the cluster, you must use the <literal>--authfile /path/to/pull-secret</literal> option with Podman or Buildah. Alternatively, to have the pull secret read by these tools automatically, you can add it to one of the default file locations: <literal>~/.docker/config.json</literal>, <literal>$XDG_RUNTIME_DIR/containers/auth.json</literal>, <literal>~/.docker/config.json</literal>, or <literal>~/.dockercfg</literal>. Refer to the <literal>containers-auth.json</literal> man page for more information.</simpara>
</listitem>
<listitem>
<simpara>You must push the custom layered image to a repository that your cluster can access.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a machine config file.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a YAML file similar to the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker <co xml:id="CO179-1"/>
  name: os-layer-custom
spec:
  osImageURL: quay.io/my-registry/custom-image@sha256... <co xml:id="CO179-2"/></programlisting>
<calloutlist>
<callout arearefs="CO179-1">
<para>Specifies the machine config pool to apply the custom layered image.</para>
</callout>
<callout arearefs="CO179-2">
<para>Specifies the path to the custom layered image in the repository.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>MachineConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
<important>
<simpara>It is strongly recommended that you test your images outside of your production environment before rolling out to your cluster.</simpara>
</important>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>You can verify that the custom layered image is applied by performing any of the following checks:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check that the worker machine config pool has rolled out with the new machine config:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check that the new machine config is created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mc</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
00-worker                                          5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-master-container-runtime                        5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-master-kubelet                                  5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-worker-container-runtime                        5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
01-worker-kubelet                                  5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
99-master-generated-registries                     5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
99-master-ssh                                                                                 3.2.0             98m
99-worker-generated-registries                     5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
99-worker-ssh                                                                                 3.2.0             98m
os-layer-custom                                                                                                 10s <co xml:id="CO180-1"/>
rendered-master-15961f1da260f7be141006404d17d39b   5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
rendered-worker-5aff604cb1381a4fe07feaf1595a797e   5bdb57489b720096ef912f738b46330a8f577803   3.2.0             95m
rendered-worker-5de4837625b1cbc237de6b22bc0bc873   5bdb57489b720096ef912f738b46330a8f577803   3.2.0             4s  <co xml:id="CO180-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO180-1">
<para>New machine config</para>
</callout>
<callout arearefs="CO180-2">
<para>New rendered machine config</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check that the <literal>osImageURL</literal> value in the new machine config points to the expected image:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe mc rendered-master-4e8be63aef68b843b546827b6ebe0913</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name:         rendered-master-4e8be63aef68b843b546827b6ebe0913
Namespace:
Labels:       &lt;none&gt;
Annotations:  machineconfiguration.openshift.io/generated-by-controller-version: 8276d9c1f574481043d3661a1ace1f36cd8c3b62
              machineconfiguration.openshift.io/release-image-version: 4.14.0-ec.3
API Version:  machineconfiguration.openshift.io/v1
Kind:         MachineConfig
...
  Os Image URL: quay.io/my-registry/custom-image@sha256...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the associated machine config pool is updating with the new machine config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-6faecdfa1b25c114a58cf178fbaa45e2   True      False      False      3              3                   3                     0                      39m
worker   rendered-worker-6b000dbc31aaee63c6a2d56d04cd4c1b   False     True       False      3              0                   0                     0                      39m <co xml:id="CO181-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO181-1">
<para>When the <literal>UPDATING</literal> field is <literal>True</literal>, the machine config pool is updating with the new machine config. When the field becomes <literal>False</literal>, the worker machine config pool has rolled out to the new machine config.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-148-79.us-west-1.compute.internal    Ready                      worker                 32m   v1.28.5
ip-10-0-155-125.us-west-1.compute.internal   Ready,SchedulingDisabled   worker                 35m   v1.28.5
ip-10-0-170-47.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.28.5
ip-10-0-174-77.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.28.5
ip-10-0-211-49.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.28.5
ip-10-0-218-151.us-west-1.compute.internal   Ready                      worker                 31m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>When the node is back in the <literal>Ready</literal> state, check that the node is using the custom layered image:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Open an <literal>oc debug</literal> session to the node. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-155-125.us-west-1.compute.internal</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Run the <literal>rpm-ostree status</literal> command to view that the custom layered image is in use:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# sudo rpm-ostree status</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>State: idle
Deployments:
* ostree-unverified-registry:quay.io/my-registry/...
                   Digest: sha256:...</screen>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<formalpara>
<title>Additional resources</title>
<para><link xlink:href="../post_installation_configuration/coreos-layering.xml#coreos-layering-updating_coreos-layering">Updating with a RHCOS custom layered image</link></para>
</formalpara>
</section>
<section xml:id="coreos-layering-removing_coreos-layering">
<title>Removing a RHCOS custom layered image</title>
<simpara>You can easily revert Red Hat Enterprise Linux CoreOS (RHCOS) image layering from the nodes in specific machine config pools. The Machine Config Operator (MCO) reboots those nodes with the cluster base Red Hat Enterprise Linux CoreOS (RHCOS) image, overriding the custom layered image.</simpara>
<simpara>To remove a Red Hat Enterprise Linux CoreOS (RHCOS) custom layered image from your cluster, you need to delete the machine config that applied the image.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the machine config that applied the custom layered image.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete mc os-layer-custom</programlisting>
<simpara>After deleting the machine config, the nodes reboot.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>You can verify that the custom layered image is removed by performing any of the following checks:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check that the worker machine config pool is updating with the previous machine config:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
master   rendered-master-6faecdfa1b25c114a58cf178fbaa45e2   True      False      False      3              3                   3                     0                      39m
worker   rendered-worker-6b000dbc31aaee63c6a2d56d04cd4c1b   False     True       False      3              0                   0                     0                      39m <co xml:id="CO182-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO182-1">
<para>When the <literal>UPDATING</literal> field is <literal>True</literal>, the machine config pool is updating with the previous machine config. When the field becomes <literal>False</literal>, the worker machine config pool has rolled out to the previous machine config.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                         STATUS                     ROLES                  AGE   VERSION
ip-10-0-148-79.us-west-1.compute.internal    Ready                      worker                 32m   v1.28.5
ip-10-0-155-125.us-west-1.compute.internal   Ready,SchedulingDisabled   worker                 35m   v1.28.5
ip-10-0-170-47.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.28.5
ip-10-0-174-77.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.28.5
ip-10-0-211-49.us-west-1.compute.internal    Ready                      control-plane,master   42m   v1.28.5
ip-10-0-218-151.us-west-1.compute.internal   Ready                      worker                 31m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>When the node is back in the <literal>Ready</literal> state, check that the node is using the base image:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Open an <literal>oc debug</literal> session to the node. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-155-125.us-west-1.compute.internal</programlisting>
</listitem>
<listitem>
<simpara>Set <literal>/host</literal> as the root directory within the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Run the <literal>rpm-ostree status</literal> command to view that the custom layered image is in use:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# sudo rpm-ostree status</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>State: idle
Deployments:
* ostree-unverified-registry:podman pull quay.io/openshift-release-dev/ocp-release@sha256:e2044c3cfebe0ff3a99fc207ac5efe6e07878ad59fd4ad5e41f88cb016dacd73
                   Digest: sha256:e2044c3cfebe0ff3a99fc207ac5efe6e07878ad59fd4ad5e41f88cb016dacd73</screen>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="coreos-layering-updating_coreos-layering">
<title>Updating with a RHCOS custom layered image</title>
<simpara>When you configure Red Hat Enterprise Linux CoreOS (RHCOS) image layering, OpenShift Container Platform no longer automatically updates the node pool that uses the custom layered image. You become responsible to manually update your nodes as appropriate.</simpara>
<simpara>To update a node that uses a custom layered image, follow these general steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>The cluster automatically upgrades to version x.y.z+1, except for the nodes that use the custom layered image.</simpara>
</listitem>
<listitem>
<simpara>You could then create a new Containerfile that references the updated OpenShift Container Platform image and the RPM that you had previously applied.</simpara>
</listitem>
<listitem>
<simpara>Create a new machine config that points to the updated custom layered image.</simpara>
</listitem>
</orderedlist>
<simpara>Updating a node with a custom layered image is not required. However, if that node gets too far behind the current OpenShift Container Platform version, you could experience unexpected results.</simpara>
</section>
</chapter>
<chapter xml:id="aws-compute-edge-tasks">
<title>AWS Local Zone tasks</title>

<simpara>After installing OpenShift Container Platform on Amazon Web Services (AWS), you can further configure AWS Local Zones and an edge compute pool, so that you can expand and customize your cluster to meet your needs.</simpara>
<section xml:id="post-install-edge-aws-extend-cluster_aws-compute-edge-tasks">
<title>Extend existing clusters to use AWS Local Zones</title>
<simpara>As a postinstallation task, you can extend an existing OpenShift Container Platform cluster on Amazon Web Services (AWS) to use AWS Local Zones.</simpara>
<simpara>Extending nodes to Local Zone locations comprises the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara>Adjusting the cluster-network maximum transmission unit (MTU)</simpara>
</listitem>
<listitem>
<simpara>Opting in the Local Zone group to AWS Local Zones</simpara>
</listitem>
<listitem>
<simpara>Creating a subnet in the existing VPC for a Local Zone location</simpara>
</listitem>
<listitem>
<simpara>Creating the machine set manifest, and then creating a node in each Local Zone location</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Before you extend an existing OpenShift Container Platform cluster on AWS to use Local Zones, check that the existing VPC contains available Classless Inter-Domain Routing (CIDR) blocks. These blocks are needed for creating the subnets.</simpara>
</important>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information about AWS Local Zones, the supported instances types, and services, see <link xlink:href="https://aws.amazon.com/about-aws/global-infrastructure/localzones/features/">AWS Local Zones features</link> in the AWS documentation.</simpara>
</listitem>
</itemizedlist>
<section xml:id="edge-machine-pools-aws-local-zones_aws-compute-edge-tasks">
<title>Edge compute pools and AWS Local Zones</title>
<simpara>Edge worker nodes are tainted worker nodes that run in AWS Local Zones locations.</simpara>
<simpara>When deploying a cluster that uses Local Zones, consider the following points:</simpara>
<itemizedlist>
<listitem>
<simpara>Amazon EC2 instances in the Local Zones are more expensive than Amazon EC2 instances in the Availability Zones.</simpara>
</listitem>
<listitem>
<simpara>Latency between applications and end users is lower in Local Zones, and latency might vary by location. A latency impact exists for some workloads if, for example, ingress traffic is mixed between Local Zones and Availability Zones.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Generally, the maximum transmission unit (MTU) between an Amazon EC2 instance in a Local Zone and an Amazon EC2 instance in the Region is 1300. For more information, see <link xlink:href="https://docs.aws.amazon.com/local-zones/latest/ug/how-local-zones-work.html">How Local Zones work</link> in the AWS documentation.
The cluster network MTU must be always less than the EC2 MTU to account for the overhead. The specific overhead is determined by the network plugin, for example:</simpara>
<itemizedlist>
<listitem>
<simpara>OVN-Kubernetes: <literal>100 bytes</literal></simpara>
</listitem>
<listitem>
<simpara>OpenShift SDN: <literal>50 bytes</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The network plugin can provide additional features, like IPsec, that also must be decreased the MTU. For additional information, see the documentation.</simpara>
</important>
<simpara>OpenShift Container Platform 4.12 introduced a new compute pool, <emphasis>edge</emphasis>, that is designed for use in remote zones. The edge compute pool configuration is common between AWS Local Zones locations. Because of the type and size limitations of resources like EC2 and EBS on Local Zone resources, the default instance type can vary from the traditional worker pool.</simpara>
<simpara>The default Elastic Block Store (EBS) for Local Zone locations is <literal>gp2</literal>, which differs from the regular worker pool. The instance type used for each Local Zone on edge compute pool also might differ from worker pools, depending on the instance offerings on the zone.</simpara>
<simpara>The edge compute pool creates new labels that developers can use to deploy applications onto AWS Local Zones nodes. The new labels are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>node-role.kubernetes.io/edge=''</literal></simpara>
</listitem>
<listitem>
<simpara><literal>machine.openshift.io/zone-type=local-zone</literal></simpara>
</listitem>
<listitem>
<simpara><literal>machine.openshift.io/zone-group=$ZONE_GROUP_NAME</literal></simpara>
</listitem>
</itemizedlist>
<simpara>By default, the machine sets for the edge compute pool defines the taint of <literal>NoSchedule</literal> to prevent regular workloads from spreading on Local Zone instances. Users can only run user workloads if they define tolerations in the pod specification.</simpara>
</section>
<section xml:id="post-install-extend-existing-to-local-zones-mtu">
<title>Changing the Cluster Network MTU to support AWS Local Zones subnets</title>
<simpara>You might need to change the maximum transmission unit (MTU) value for the cluster
network so that your cluster infrastructure can support Local Zone subnets.</simpara>
<section xml:id="nw-cluster-mtu-change-about_aws-compute-edge-tasks">
<title>About the cluster MTU</title>
<simpara>During installation the maximum transmission unit (MTU) for the cluster network is detected automatically based on the MTU of the primary network interface of nodes in the cluster. You do not usually need to override the detected MTU.</simpara>
<simpara>You might want to change the MTU of the cluster network for several reasons:</simpara>
<itemizedlist>
<listitem>
<simpara>The MTU detected during cluster installation is not correct for your infrastructure.</simpara>
</listitem>
<listitem>
<simpara>Your cluster infrastructure now requires a different MTU, such as from the addition of nodes that need a different MTU for optimal performance.</simpara>
</listitem>
</itemizedlist>
<simpara>You can change the cluster MTU for only the OVN-Kubernetes and OpenShift SDN cluster network plugins.</simpara>
<section xml:id="service-interruption-considerations_aws-compute-edge-tasks">
<title>Service interruption considerations</title>
<simpara>When you initiate an MTU change on your cluster the following effects might impact service availability:</simpara>
<itemizedlist>
<listitem>
<simpara>At least two rolling reboots are required to complete the migration to a new MTU. During this time, some nodes are not available as they restart.</simpara>
</listitem>
<listitem>
<simpara>Specific applications deployed to the cluster with shorter timeout intervals than the absolute TCP timeout interval might experience disruption during the MTU change.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="mtu-value-selection_aws-compute-edge-tasks">
<title>MTU value selection</title>
<simpara>When planning your MTU migration there are two related but distinct MTU values to consider.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Hardware MTU</emphasis>: This MTU value is set based on the specifics of your network infrastructure.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Cluster network MTU</emphasis>: This MTU value is always less than your hardware MTU to account for the cluster network overlay overhead. The specific overhead is determined by your network plugin:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">OVN-Kubernetes</emphasis>: <literal>100</literal> bytes</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OpenShift SDN</emphasis>: <literal>50</literal> bytes</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>If your cluster requires different MTU values for different nodes, you must subtract the overhead value for your network plugin from the lowest MTU value that is used by any node in your cluster. For example, if some nodes in your cluster have an MTU of <literal>9001</literal>, and some have an MTU of <literal>1500</literal>, you must set this value to <literal>1400</literal>.</simpara>
</section>
<section xml:id="how-the-migration-process-works_aws-compute-edge-tasks">
<title>How the migration process works</title>
<simpara>The following table summarizes the migration process by segmenting between the user-initiated steps in the process and the actions that the migration performs in response.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Live migration of the cluster MTU</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">User-initiated steps</entry>
<entry align="left" valign="top">OpenShift Container Platform activity</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Set the following values in the Cluster Network Operator configuration:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>spec.migration.mtu.machine.to</literal></simpara>
</listitem>
<listitem>
<simpara><literal>spec.migration.mtu.network.from</literal></simpara>
</listitem>
<listitem>
<simpara><literal>spec.migration.mtu.network.to</literal></simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cluster Network Operator (CNO)</emphasis>: Confirms that each field is set to a valid value.</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>mtu.machine.to</literal> must be set to either the new hardware MTU or to the current hardware MTU if the MTU for the hardware is not changing. This value is transient and is used as part of the migration process. Separately, if you specify a hardware MTU that is different from your existing hardware MTU value, you must manually configure the MTU to persist by other means, such as with a machine config, DHCP setting, or a Linux kernel command line.</simpara>
</listitem>
<listitem>
<simpara>The <literal>mtu.network.from</literal> field must equal the <literal>network.status.clusterNetworkMTU</literal> field, which is the current MTU of the cluster network.</simpara>
</listitem>
<listitem>
<simpara>The <literal>mtu.network.to</literal> field must be set to the target cluster network MTU and must be lower than the hardware MTU to allow for the overlay overhead of the network plugin. For OVN-Kubernetes, the overhead is <literal>100</literal> bytes and for OpenShift SDN the overhead is <literal>50</literal> bytes.</simpara>
</listitem>
</itemizedlist>
<simpara>If the values provided are valid, the CNO writes out a new temporary configuration with the MTU for the cluster network set to the value of the <literal>mtu.network.to</literal> field.</simpara>
<simpara><emphasis role="strong">Machine Config Operator (MCO)</emphasis>: Performs a rolling reboot of each node in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reconfigure the MTU of the primary network interface for the nodes on the cluster. You can use a variety of methods to accomplish this, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Deploying a new NetworkManager connection profile with the MTU change</simpara>
</listitem>
<listitem>
<simpara>Changing the MTU through a DHCP server setting</simpara>
</listitem>
<listitem>
<simpara>Changing the MTU through boot parameters</simpara>
</listitem>
</itemizedlist></entry>
<entry align="left" valign="top"><simpara>N/A</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Set the <literal>mtu</literal> value in the CNO configuration for the network plugin and set <literal>spec.migration</literal> to <literal>null</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Machine Config Operator (MCO)</emphasis>: Performs a rolling reboot of each node in the cluster with the new MTU configuration.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="nw-cluster-mtu-change_aws-compute-edge-tasks">
<title>Changing the cluster MTU</title>
<simpara>As a cluster administrator, you can change the maximum transmission unit (MTU) for your cluster. The migration is disruptive and nodes in your cluster might be temporarily unavailable as the MTU update rolls out.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster with a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You identified the target MTU for your cluster. The correct MTU varies depending on the network plugin that your cluster uses:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">OVN-Kubernetes</emphasis>: The cluster MTU must be set to <literal>100</literal> less than the lowest hardware MTU value in your cluster.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">OpenShift SDN</emphasis>: The cluster MTU must be set to <literal>50</literal> less than the lowest hardware MTU value in your cluster.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To increase or decrease the MTU for the cluster network complete the following procedure.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To obtain the current MTU for the cluster network, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe network.config cluster</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">...
Status:
  Cluster Network:
    Cidr:               10.217.0.0/22
    Host Prefix:        23
  Cluster Network MTU:  1400
  Network Type:         OpenShiftSDN
  Service Network:
    10.217.4.0/23
...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To begin the MTU migration, specify the migration configuration by entering the following command. The Machine Config Operator performs a rolling reboot of the nodes in the cluster in preparation for the MTU change.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": &lt;overlay_from&gt;, "to": &lt;overlay_to&gt; } , "machine": { "to" : &lt;machine_to&gt; } } } } }'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;overlay_from&gt;</literal></term>
<listitem>
<simpara>Specifies the current cluster network MTU value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;overlay_to&gt;</literal></term>
<listitem>
<simpara>Specifies the target MTU for the cluster network. This value is set relative to the value for <literal>&lt;machine_to&gt;</literal> and for OVN-Kubernetes must be <literal>100</literal> less and for OpenShift SDN must be <literal>50</literal> less.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;machine_to&gt;</literal></term>
<listitem>
<simpara>Specifies the MTU for the primary network interface on the underlying host network.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example that increases the cluster MTU</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": { "mtu": { "network": { "from": 1400, "to": 9000 } , "machine": { "to" : 9100} } } } }'</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>As the MCO updates machines in each machine config pool, it reboots each node one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
<note>
<simpara>By default, the MCO updates one machine per pool at a time, causing the total time the migration takes to increase with the size of the cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Confirm the status of the new machine configuration on the hosts:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the machine configuration state and the name of the applied machine configuration, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node | egrep "hostname|machineconfig"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">kubernetes.io/hostname=master-0
machineconfiguration.openshift.io/currentConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/desiredConfig: rendered-master-c53e221d9d24e1c8bb6ee89dd3d8ad7b
machineconfiguration.openshift.io/reason:
machineconfiguration.openshift.io/state: Done</programlisting>
</para>
</formalpara>
<simpara>Verify that the following statements are true:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>machineconfiguration.openshift.io/state</literal> field is <literal>Done</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>machineconfiguration.openshift.io/currentConfig</literal> field is equal to the value of the <literal>machineconfiguration.openshift.io/desiredConfig</literal> field.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To confirm that the machine config is correct, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineconfig &lt;config_name&gt; -o yaml | grep ExecStart</programlisting>
<simpara>where <literal>&lt;config_name&gt;</literal> is the name of the machine config from the <literal>machineconfiguration.openshift.io/currentConfig</literal> field.</simpara>
<simpara>The machine config must include the following update to the systemd configuration:</simpara>
<programlisting language="plain" linenumbering="unnumbered">ExecStart=/usr/local/bin/mtu-migration.sh</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To finalize the MTU migration, enter one of the following commands:</simpara>
<itemizedlist>
<listitem>
<simpara>If you are using the OVN-Kubernetes network plugin:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "ovnKubernetesConfig": { "mtu": &lt;mtu&gt; }}}}'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;mtu&gt;</literal></term>
<listitem>
<simpara>Specifies the new cluster network MTU that you specified with <literal>&lt;overlay_to&gt;</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>If you are using the OpenShift SDN network plugin:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch Network.operator.openshift.io cluster --type=merge --patch \
  '{"spec": { "migration": null, "defaultNetwork":{ "openshiftSDNConfig": { "mtu": &lt;mtu&gt; }}}}'</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;mtu&gt;</literal></term>
<listitem>
<simpara>Specifies the new cluster network MTU that you specified with <literal>&lt;overlay_to&gt;</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After finalizing the MTU migration, each MCP node is rebooted one by one. You must wait until all the nodes are updated. Check the machine config pool status by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mcp</programlisting>
<simpara>A successfully updated node has the following status: <literal>UPDATED=true</literal>, <literal>UPDATING=false</literal>, <literal>DEGRADED=false</literal>.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the node in your cluster uses the MTU that you specified in the previous procedure by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe network.config cluster</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="installation-aws-add-local-zone-locations_aws-compute-edge-tasks">
<title>Opting in to AWS Local Zones</title>
<simpara>If you plan to create the subnets in AWS Local Zones, you must opt in to each zone group separately.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the AWS CLI.</simpara>
</listitem>
<listitem>
<simpara>You have determined an AWS Region for where you want to deploy your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>You have attached a permissive IAM policy to a user or role account that opts in to the zone group. Consider the following configuration as an example IAM policy:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": [
        "ec2:ModifyAvailabilityZoneGroup"
      ],
      "Effect": "Allow",
      "Resource": "*"
    }
  ]
}</programlisting>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>List the zones that are available in your AWS Region by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws --region "&lt;value_of_AWS_Region&gt;" ec2 describe-availability-zones \
    --query 'AvailabilityZones[].[{ZoneName: ZoneName, GroupName: GroupName, Status: OptInStatus}]' \
    --filters Name=zone-type,Values=local-zone \
    --all-availability-zones</programlisting>
<simpara>Depending on the AWS Region, the list of available zones can be long. The command returns the following fields:</simpara>
<variablelist>
<varlistentry>
<term><literal>ZoneName</literal></term>
<listitem>
<simpara>The name of the Local Zone.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>GroupName</literal></term>
<listitem>
<simpara>The group that comprises the zone. To opt in to the region, save the name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Status</literal></term>
<listitem>
<simpara>The status of the Local Zone group. If the status is <literal>not-opted-in</literal>, you must opt in the <literal>GroupName</literal> by running the commands that follow.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Opt in to the zone group on your AWS account by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws ec2 modify-availability-zone-group \
    --group-name "&lt;value_of_GroupName&gt;" \<co xml:id="CO183-1"/>
    --opt-in-status opted-in</programlisting>
<calloutlist>
<callout arearefs="CO183-1">
<para>For <literal>&lt;value_of_GroupName&gt;</literal>, specify the name of the group of the Local Zone where you want to create subnets. For example, specify <literal>us-east-1-nyc-1</literal> to use the zone <literal>us-east-1-nyc-1a</literal> (US East New York).</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="post-install-existing-local-zone-subnet_aws-compute-edge-tasks">
<title>Extend existing clusters to use AWS Local Zones</title>
<simpara>If you want a Machine API to create an Amazon EC2 instance in a remote zone location, you must create a subnet in a Local Zone location. You can use any provisioning tool, such as Ansible or Terraform, to create subnets in the existing Virtual Private Cloud (VPC). You can configure the CloudFormation template to meet your requirements.</simpara>
<simpara>The following subsections include steps that use CloudFormation templates. Considering the limitation of NAT Gateways in AWS Local Zones, CloudFormation templates support only public subnets. You can reuse the template to create public subnets for each edge location to where you need to extend your cluster.</simpara>
<section xml:id="installation-creating-aws-subnet-localzone_aws-compute-edge-tasks">
<title>Creating a subnet in AWS Local Zones</title>
<simpara>You must create a subnet in AWS Local Zones before you configure a worker machineset for your OpenShift Container Platform cluster.</simpara>
<simpara>You must repeat the following process for each Local Zone you want to deploy worker nodes to.</simpara>
<simpara>You can use the provided CloudFormation template and a custom parameter file to create a stack of AWS resources that represent the subnet.</simpara>
<note>
<simpara>If you do not use the provided CloudFormation template to create your AWS
infrastructure, you must review the provided information and manually create
the infrastructure. If your cluster does not initialize correctly, you might
have to contact Red Hat support with your installation logs.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured an AWS account.</simpara>
</listitem>
<listitem>
<simpara>You added your AWS keys and region to your local AWS profile by running <literal>aws configure</literal>.</simpara>
</listitem>
<listitem>
<simpara>You opted in to the Local Zone group.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a JSON file that contains the parameter values that the template
requires:</simpara>
<programlisting language="json" linenumbering="unnumbered">[
  {
    "ParameterKey": "VpcId",
    "ParameterValue": "&lt;value_of_VpcId&gt;" <co xml:id="CO184-1"/>
  },
  {
    "ParameterKey": "PublicRouteTableId",
    "ParameterValue": "&lt;value_of_PublicRouteTableId&gt;" <co xml:id="CO184-2"/>
  },
  {
    "ParameterKey": "ZoneName",
    "ParameterValue": "&lt;value_of_ZoneName&gt;" <co xml:id="CO184-3"/>
  },
  {
    "ParameterKey": "SubnetName",
    "ParameterValue": "&lt;value_of_SubnetName&gt;"
  },
  {
    "ParameterKey": "PublicSubnetCidr",
    "ParameterValue": "10.0.192.0/20" <co xml:id="CO184-4"/>
  }
]</programlisting>
<calloutlist>
<callout arearefs="CO184-1">
<para>Specify the VPC ID, which is the value <literal>VpcID</literal> in the output of the CloudFormation template.
for the VPC.</para>
</callout>
<callout arearefs="CO184-2">
<para>Specify the Route Table ID, which is the value of the <literal>PublicRouteTableId</literal> in the CloudFormation stack
for the VPC.</para>
</callout>
<callout arearefs="CO184-3">
<para>Specify the AWS Local Zone name, which is the value of the <literal>ZoneName</literal> field in the <literal>AvailabilityZones</literal> object that you retrieve in the section "Opting in to AWS Local Zones".</para>
</callout>
<callout arearefs="CO184-4">
<para>Specify a CIDR block that is used to create the Local Zone subnet. This block must be part of the VPC CIDR block <literal>VpcCidr</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Copy the template from the <emphasis role="strong">CloudFormation template for the subnet</emphasis>
section of this topic and save it as a YAML file on your computer. This template
describes the VPC that your cluster requires.</simpara>
</listitem>
<listitem>
<simpara>Launch the CloudFormation template to create a stack of AWS resources that represent the VPC by running the following command:</simpara>
<important>
<simpara>You must enter the command on a single line.</simpara>
</important>
<programlisting language="terminal" linenumbering="unnumbered">$ aws cloudformation create-stack --stack-name &lt;subnet_stack_name&gt; \ <co xml:id="CO185-1"/>
     --template-body file://&lt;template&gt;.yaml \ <co xml:id="CO185-2"/>
     --parameters file://&lt;parameters&gt;.json <co xml:id="CO185-3"/></programlisting>
<calloutlist>
<callout arearefs="CO185-1">
<para><literal>&lt;subnet_stack_name&gt;</literal> is the name for the CloudFormation stack, such as <literal>cluster-lz-&lt;local_zone_shortname&gt;</literal>.
You need the name of this stack if you remove the cluster.</para>
</callout>
<callout arearefs="CO185-2">
<para><literal>&lt;template&gt;</literal> is the relative path to and name of the CloudFormation template
YAML file that you saved.</para>
</callout>
<callout arearefs="CO185-3">
<para><literal>&lt;parameters&gt;</literal> is the relative path to and name of the CloudFormation
parameters JSON file.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">arn:aws:cloudformation:us-east-1:123456789012:stack/&lt;subnet_stack_name&gt;/dbedae40-2fd3-11eb-820e-12a48460849f</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Confirm that the template components exist by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws cloudformation describe-stacks --stack-name &lt;subnet_stack_name&gt;</programlisting>
<simpara>After the <literal>StackStatus</literal> displays <literal>CREATE_COMPLETE</literal>, the output displays values
for the following parameters. You must provide these parameter values to
the other CloudFormation templates that you run to create your cluster:</simpara>
<informaltable tabstyle="horizontal" frame="none" colsep="0" rowsep="0">
<tgroup cols="2">
<colspec colwidth="15*"/>
<colspec colwidth="85*"/>
<tbody valign="top">
<row>
<entry>
<simpara><literal>PublicSubnetIds</literal></simpara>
</entry>
<entry>
<simpara>The IDs of the new public subnets.</simpara>
</entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="installation-cloudformation-subnet-localzone_aws-compute-edge-tasks">
<title>CloudFormation template for the subnet that uses AWS Local Zones</title>
<simpara>You can use the following CloudFormation template to deploy the subnet that
you need for your OpenShift Container Platform cluster that uses AWS Local Zones.</simpara>
<example>
<title>CloudFormation template for the subnet</title>
<programlisting language="yaml" linenumbering="unnumbered"># CloudFormation template used to create Local Zone subnets and dependencies
AWSTemplateFormatVersion: 2010-09-09
Description: Template for create Public Local Zone subnets

Parameters:
  VpcId:
    Description: VPC Id
    Type: String
  ZoneName:
    Description: Local Zone Name (Example us-east-1-nyc-1a)
    Type: String
  SubnetName:
    Description: Local Zone Name (Example cluster-public-us-east-1-nyc-1a)
    Type: String
  PublicRouteTableId:
    Description: Public Route Table ID to associate the Local Zone subnet
    Type: String
  PublicSubnetCidr:
    AllowedPattern: ^(([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])\.){3}([0-9]|[1-9][0-9]|1[0-9]{2}|2[0-4][0-9]|25[0-5])(\/(1[6-9]|2[0-4]))$
    ConstraintDescription: CIDR block parameter must be in the form x.x.x.x/16-24.
    Default: 10.0.128.0/20
    Description: CIDR block for Public Subnet
    Type: String

Resources:
  PublicSubnet:
    Type: "AWS::EC2::Subnet"
    Properties:
      VpcId: !Ref VpcId
      CidrBlock: !Ref PublicSubnetCidr
      AvailabilityZone: !Ref ZoneName
      Tags:
      - Key: Name
        Value: !Ref SubnetName
      - Key: kubernetes.io/cluster/unmanaged
        Value: "true"

  PublicSubnetRouteTableAssociation:
    Type: "AWS::EC2::SubnetRouteTableAssociation"
    Properties:
      SubnetId: !Ref PublicSubnet
      RouteTableId: !Ref PublicRouteTableId

Outputs:
  PublicSubnetIds:
    Description: Subnet IDs of the public subnets.
    Value:
      !Join ["", [!Ref PublicSubnet]]</programlisting>
</example>
</section>
</section>
<section xml:id="post-install-edge-aws-extend-machineset">
<title>Creating a machine set manifest for an AWS Local Zones node</title>
<simpara>After you create subnets in AWS Local Zones, you can create a machine set manifest.</simpara>
<simpara>The installation program sets the following labels for the <literal>edge</literal> machine pools at cluster installation time:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>machine.openshift.io/parent-zone-name: &lt;value_of_ParentZoneName&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>machine.openshift.io/zone-group: &lt;value_of_ZoneGroup&gt;</literal></simpara>
</listitem>
<listitem>
<simpara><literal>machine.openshift.io/zone-type: &lt;value_of_ZoneType&gt;</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The following procedure details how you can create a machine set configuraton that matches the <literal>edge</literal> compute pool configuration.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have created subnets in AWS Local Zones.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Manually preserve <literal>edge</literal> machine pool labels when creating the machine set manifest by gathering the AWS API. To complete this action, enter the following command in your command-line interface (CLI):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws ec2 describe-availability-zones --region &lt;value_of_Region&gt; \<co xml:id="CO186-1"/>
    --query 'AvailabilityZones[].{
	ZoneName: ZoneName,
	ParentZoneName: ParentZoneName,
	GroupName: GroupName,
	ZoneType: ZoneType}' \
    --filters Name=zone-name,Values=&lt;value_of_ZoneName&gt; \<co xml:id="CO186-2"/>
    --all-availability-zones</programlisting>
<calloutlist>
<callout arearefs="CO186-1">
<para>For <literal>&lt;value_of_Region&gt;</literal>, specify the name of the region for the zone.</para>
</callout>
<callout arearefs="CO186-2">
<para>For <literal>&lt;value_of_ZoneName&gt;</literal>, specify the name of the Local Zone.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Example output for Local Zone <literal>us-east-1-nyc-1a</literal></title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">[
    {
        "ZoneName": "us-east-1-nyc-1a",
        "ParentZoneName": "us-east-1f",
        "GroupName": "us-east-1-nyc-1",
        "ZoneType": "local-zone"
    }
]</programlisting>
</para>
</formalpara>
<section xml:id="machineset-yaml-aws_aws-compute-edge-tasks">
<title>Sample YAML for a compute machine set custom resource on AWS</title>
<simpara>This sample YAML defines a compute machine set that runs in the <literal>us-east-1-nyc-1a</literal> Amazon Web Services (AWS) zone and creates nodes that are labeled with <literal>node-role.kubernetes.io/edge: ""</literal>.</simpara>
<simpara>In this sample, <literal>&lt;infrastructure_id&gt;</literal> is the infrastructure ID label that is based on the cluster ID that you set when you provisioned the cluster, and
<literal>&lt;edge&gt;</literal>
is the node label to add.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO187-1"/>
  name: &lt;infrastructure_id&gt;-edge-&lt;zone&gt; <co xml:id="CO187-2"/>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO187-3"/>
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-edge-&lt;zone&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO187-4"/>
        machine.openshift.io/cluster-api-machine-role: edge <co xml:id="CO187-5"/>
        machine.openshift.io/cluster-api-machine-type: edge <co xml:id="CO187-6"/>
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-edge-&lt;zone&gt; <co xml:id="CO187-7"/>
    spec:
      metadata:
        labels:
          machine.openshift.io/parent-zone-name: &lt;value_of_ParentZoneName&gt;
          machine.openshift.io/zone-group: &lt;value_of_GroupName&gt;
          machine.openshift.io/zone-type: &lt;value_of_ZoneType&gt;
          node-role.kubernetes.io/edge: "" <co xml:id="CO187-8"/>
      providerSpec:
        value:
          ami:
            id: ami-046fe691f52a953f9 <co xml:id="CO187-9"/>
          apiVersion: machine.openshift.io/v1beta1
          blockDevices:
            - ebs:
                iops: 0
                volumeSize: 120
                volumeType: gp2
          credentialsSecret:
            name: aws-cloud-credentials
          deviceIndex: 0
          iamInstanceProfile:
            id: &lt;infrastructure_id&gt;-worker-profile <co xml:id="CO187-10"/>
          instanceType: m6i.large
          kind: AWSMachineProviderConfig
          placement:
            availabilityZone: &lt;zone&gt; <co xml:id="CO187-11"/>
            region: &lt;region&gt; <co xml:id="CO187-12"/>
          securityGroups:
            - filters:
                - name: tag:Name
                  values:
                    - &lt;infrastructure_id&gt;-worker-sg <co xml:id="CO187-13"/>
          subnet:
              id: &lt;value_of_PublicSubnetIds&gt; <co xml:id="CO187-14"/>
          publicIp: true
          tags:
            - name: kubernetes.io/cluster/&lt;infrastructure_id&gt; <co xml:id="CO187-15"/>
              value: owned
            - name: &lt;custom_tag_name&gt; <co xml:id="CO187-16"/>
              value: &lt;custom_tag_value&gt; <co xml:id="CO187-17"/>
          userDataSecret:
            name: worker-user-data
      taints: <co xml:id="CO187-18"/>
        - key: node-role.kubernetes.io/edge
          effect: NoSchedule</programlisting>
<calloutlist>
<callout arearefs="CO187-1 CO187-3 CO187-4 CO187-10 CO187-13 CO187-15">
<para>Specify the infrastructure ID that is based on the cluster ID that you set when you provisioned the cluster. If you have the OpenShift CLI installed, you can obtain the infrastructure ID by running the following command:</para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster</programlisting>
</callout>
<callout arearefs="CO187-2 CO187-7">
<para>Specify the infrastructure ID, <literal>edge</literal> role node label, and zone name.</para>
</callout>
<callout arearefs="CO187-5 CO187-6 CO187-8">
<para>Specify the <literal>edge</literal> role node label.</para>
</callout>
<callout arearefs="CO187-9">
<para>Specify a valid Red Hat Enterprise Linux CoreOS (RHCOS) Amazon
Machine Image (AMI) for your AWS zone for your OpenShift Container Platform nodes. If you want to use an AWS Marketplace image, you must complete the OpenShift Container Platform subscription from the <link xlink:href="https://aws.amazon.com/marketplace/fulfillment?productId=59ead7de-2540-4653-a8b0-fa7926d5c845">AWS Marketplace</link> to obtain an AMI ID for your region.</para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-machine-api \
    -o jsonpath='{.spec.template.spec.providerSpec.value.ami.id}{"\n"}' \
    get machineset/&lt;infrastructure_id&gt;-&lt;role&gt;-&lt;zone&gt;</programlisting>
</callout>
<callout arearefs="CO187-16 CO187-17">
<para>Optional: Specify custom tag data for your cluster. For example, you might add an admin contact email address by specifying a <literal>name:value</literal> pair of <literal>Email:admin-email@example.com</literal>.</para>
<note>
<simpara>Custom tags can also be specified during installation in the <literal>install-config.yml</literal> file. If the <literal>install-config.yml</literal> file and the machine set include a tag with the same <literal>name</literal> data, the value for the tag from the machine set takes priority over the value for the tag in the <literal>install-config.yml</literal> file.</simpara>
</note>
</callout>
<callout arearefs="CO187-11">
<para>Specify the zone name, for example, <literal>us-east-1-nyc-1a</literal>.</para>
</callout>
<callout arearefs="CO187-12">
<para>Specify the region, for example, <literal>us-east-1</literal>.</para>
</callout>
<callout arearefs="CO187-14">
<para>The ID of the public subnet that you created in AWS Local Zones. You created this public subnet ID on completing the procedure for "Creating a subnet in AWS Local Zones".</para>
</callout>
<callout arearefs="CO187-18">
<para>Specify a taint to prevent user workloads from being scheduled on
<literal>edge</literal>
nodes.</para>
<note>
<simpara>After adding the <literal>NoSchedule</literal> taint on the infrastructure node, existing DNS pods running on that node are marked as <literal>misscheduled</literal>. You must either delete or <link xlink:href="https://access.redhat.com/solutions/6592171">add toleration on <literal>misscheduled</literal> DNS pods</link>.</simpara>
</note>
</callout>
</calloutlist>
</section>
<section xml:id="machineset-creating_aws-compute-edge-tasks">
<title>Creating a compute machine set</title>
<simpara>In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Deploy an OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in to <literal>oc</literal> as a user with <literal>cluster-admin</literal> permission.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named <literal>&lt;file_name&gt;.yaml</literal>.</simpara>
<simpara>Ensure that you set the <literal>&lt;clusterID&gt;</literal> and <literal>&lt;role&gt;</literal> parameter values.</simpara>
</listitem>
<listitem>
<simpara>Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To list the compute machine sets in your cluster, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machinesets -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To view values of a specific compute machine set custom resource (CR), run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset &lt;machineset_name&gt; \
  -n openshift-machine-api -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt; <co xml:id="CO188-1"/>
  name: &lt;infrastructure_id&gt;-&lt;role&gt; <co xml:id="CO188-2"/>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
      machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: &lt;infrastructure_id&gt;
        machine.openshift.io/cluster-api-machine-role: &lt;role&gt;
        machine.openshift.io/cluster-api-machine-type: &lt;role&gt;
        machine.openshift.io/cluster-api-machineset: &lt;infrastructure_id&gt;-&lt;role&gt;
    spec:
      providerSpec: <co xml:id="CO188-3"/>
        ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO188-1">
<para>The cluster infrastructure ID.</para>
</callout>
<callout arearefs="CO188-2">
<para>A default node label.</para>
<note>
<simpara>For clusters that have user-provisioned infrastructure, a compute machine set can only create <literal>worker</literal> and <literal>infra</literal> type machines.</simpara>
</note>
</callout>
<callout arearefs="CO188-3">
<para>The values in the <literal>&lt;providerSpec&gt;</literal> section of the compute machine set CR are platform-specific. For more information about <literal>&lt;providerSpec&gt;</literal> parameters in the CR, see the sample compute machine set CR configuration for your provider.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a <literal>MachineSet</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>View the list of compute machine sets by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machineset -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-edge-us-east-1-nyc-1a      1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a          1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b          1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c          1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d          0         0                             55m
agl030519-vplxk-worker-us-east-1e          0         0                             55m
agl030519-vplxk-worker-us-east-1f          0         0                             55m</programlisting>
</para>
</formalpara>
<simpara>When the new compute machine set is available, the <literal>DESIRED</literal> and <literal>CURRENT</literal> values match. If the compute machine set is not available, wait a few minutes and run the command again.</simpara>
</listitem>
<listitem>
<simpara>Optional: To check nodes that were created by the edge machine, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/edge</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS   ROLES         AGE    VERSION
ip-10-0-207-188.ec2.internal   Ready    edge,worker   172m   v1.25.2+d2e245f</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/installing_aws/installing-aws-localzone.xml#installing-aws-localzone">Installing a cluster on AWS with worker nodes on AWS Local Zones</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installation-extend-edge-nodes-aws-local-zones_aws-compute-edge-tasks">
<title>Creating user workloads in AWS Local Zones</title>
<simpara>After you create an Amazon Web Service (AWS) Local Zone environment, and you deploy your cluster, you can use edge worker nodes to create user workloads in Local Zone subnets.</simpara>
<simpara>After you run the installation program and create the cluster, the installation program automatically specifies a taint effect of <literal>NoSchedule</literal> to each edge worker node. This means that a scheduler does not add a new pod, or deployment, to a node if the pod does not match the specified tolerations for a taint. You can modify the taint for better control over how nodes create workloads in each Local Zone subnet.</simpara>
<simpara>The installation program creates the compute machine set manifests file with <literal>node-role.kubernetes.io/edge</literal> and <literal>node-role.kubernetes.io/worker</literal> labels applied to each edge worker node that is located in a Local Zone subnet.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You deployed your cluster in a Virtual Private Cloud (VPC) with defined Local Zone subnets.</simpara>
</listitem>
<listitem>
<simpara>You ensured that the compute machine set for the edge workers on Local Zone subnets specifies the taints for <literal>node-role.kubernetes.io/edge</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>deployment</literal> resource YAML file for an example application to be deployed in the edge worker node that operates in a Local Zone subnet. Ensure that you specify the correct tolerations that match the taints for the edge worker node.</simpara>
<formalpara>
<title>Example of a configured <literal>deployment</literal> resource for an edge worker node that operates in a Local Zone subnet</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: Namespace
apiVersion: v1
metadata:
  name: &lt;local_zone_application_namespace&gt;
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: &lt;pvc_name&gt;
  namespace: &lt;local_zone_application_namespace&gt;
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 10Gi
  storageClassName: gp2-csi <co xml:id="CO189-1"/>
  volumeMode: Filesystem
---
apiVersion: apps/v1
kind: Deployment <co xml:id="CO189-2"/>
metadata:
  name: &lt;local_zone_application&gt; <co xml:id="CO189-3"/>
  namespace: &lt;local_zone_application_namespace&gt; <co xml:id="CO189-4"/>
spec:
  selector:
    matchLabels:
      app: &lt;local_zone_application&gt;
  replicas: 1
  template:
    metadata:
      labels:
        app: &lt;local_zone_application&gt;
        zone-group: ${ZONE_GROUP_NAME} <co xml:id="CO189-5"/>
    spec:
      securityContext:
        seccompProfile:
          type: RuntimeDefault
      nodeSelector: <co xml:id="CO189-6"/>
        machine.openshift.io/zone-group: ${ZONE_GROUP_NAME}
      tolerations: <co xml:id="CO189-7"/>
      - key: "node-role.kubernetes.io/edge"
        operator: "Equal"
        value: ""
        effect: "NoSchedule"
      containers:
        - image: openshift/origin-node
          command:
           - "/bin/socat"
          args:
            - TCP4-LISTEN:8080,reuseaddr,fork
            - EXEC:'/bin/bash -c \"printf \\\"HTTP/1.0 200 OK\r\n\r\n\\\"; sed -e \\\"/^\r/q\\\"\"'
          imagePullPolicy: Always
          name: echoserver
          ports:
            - containerPort: 8080
          volumeMounts:
            - mountPath: "/mnt/storage"
              name: data
      volumes:
      - name: data
        persistentVolumeClaim:
          claimName: &lt;pvc_name&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO189-1">
<para><literal>storageClassName</literal>: For the Local Zone configuration, you must specify <literal>gp2-csi</literal>.</para>
</callout>
<callout arearefs="CO189-2">
<para><literal>kind</literal>: Defines the <literal>deployment</literal> resource.</para>
</callout>
<callout arearefs="CO189-3">
<para><literal>name</literal>: Specifies the name of your Local Zone application. For example, <literal>local-zone-demo-app-nyc-1</literal>.</para>
</callout>
<callout arearefs="CO189-4">
<para><literal>namespace:</literal> Defines the namespace for the AWS Local Zone where you want to run the user workload. For example: <literal>local-zone-app-nyc-1a</literal>.</para>
</callout>
<callout arearefs="CO189-5">
<para><literal>zone-group</literal>: Defines the group to where a zone belongs. For example, <literal>us-east-1-iah-1</literal>.</para>
</callout>
<callout arearefs="CO189-6">
<para><literal>nodeSelector</literal>: Targets edge worker nodes that match the specified labels.</para>
</callout>
<callout arearefs="CO189-7">
<para><literal>tolerations</literal>: Sets the values that match with the <literal>taints</literal> defined on the <literal>MachineSet</literal> manifest for the Local Zone node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>service</literal> resource YAML file for the node. This resource exposes a pod from a targeted edge worker node to services that run inside your Local Zone network.</simpara>
<formalpara>
<title>Example of a configured <literal>service</literal> resource for an edge worker node that operates in a Local Zone subnet</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service <co xml:id="CO190-1"/>
metadata:
  name:  &lt;local_zone_application&gt;
  namespace: &lt;local_zone_application_namespace&gt;
spec:
  ports:
    - port: 80
      targetPort: 8080
      protocol: TCP
  type: NodePort
  selector: <co xml:id="CO190-2"/>
    app: &lt;local_zone_application&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO190-1">
<para><literal>kind</literal>: Defines the <literal>service</literal> resource.</para>
</callout>
<callout arearefs="CO190-2">
<para><literal>selector:</literal> Specifies the label type applied to managed pods.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../installing/installing_aws/installing-aws-localzone.xml#installing-aws-localzone">Installing a cluster on AWS with worker nodes on AWS Local Zones</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../nodes/scheduling/nodes-scheduler-taints-tolerations.xml#nodes-scheduler-taints-tolerations-about_nodes-scheduler-taints-tolerations">Understanding taints and tolerations</link></simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara>Optional: Use the AWS Load Balancer (ALB) Operator to expose a pod from a targeted edge worker node to services that run inside of a Local Zone subnet from a public network.
See <link xlink:href="../networking/aws_load_balancer_operator/install-aws-load-balancer-operator.xml#nw-installing-aws-load-balancer-operator_aws-load-balancer-operator">Installing the AWS Load Balancer Operator</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="adding-failure-domains-to-an-existing-nutanix-cluster">
<title>Adding failure domains to an existing Nutanix cluster</title>

<simpara>By default, the installation program installs control plane and compute machines into a single Nutanix Prism Element (cluster). After an OpenShift Container Platform cluster is deployed, you can improve its fault tolerance by adding additional Prism Element instances to the deployment using failure domains.</simpara>
<simpara>A failure domain represents a single Prism Element instance to which:</simpara>
<itemizedlist>
<listitem>
<simpara>New control plane and compute machines can be deployed.</simpara>
</listitem>
<listitem>
<simpara>Existing control plane and compute machines can be distributed.</simpara>
</listitem>
</itemizedlist>
<section xml:id="installation-nutanix-failure-domains-req_adding-failure-domains-to-an-existing-nutanix-cluster">
<title>Failure domain requirements</title>
<simpara>When planning to use failure domains, consider the following requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>All Nutanix Prism Element instances must be managed by the same instance of Prism Central. A deployment that is comprised of multiple Prism Central instances is not supported.</simpara>
</listitem>
<listitem>
<simpara>The machines that make up the Prism Element clusters must reside on the same Ethernet network for failure domains to be able to communicate with each other.</simpara>
</listitem>
<listitem>
<simpara>A subnet is required in each Prism Element that will be used as a failure domain in the OpenShift Container Platform cluster. When defining these subnets, they must share the same IP address prefix (CIDR) and should contain the virtual IP addresses that the OpenShift Container Platform cluster uses.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="post-installation-configuring-nutanix-failure-domains_adding-failure-domains-to-an-existing-nutanix-cluster">
<title>Adding failure domains to the Infrastructure CR</title>
<simpara>You add failure domains to an existing Nutanix cluster by modifying its Infrastructure custom resource (CR) (<literal>infrastructures.config.openshift.io</literal>).</simpara>
<tip>
<simpara>It is recommended that you configure three failure domains to ensure high-availability.</simpara>
</tip>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the Infrastructure CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit infrastructures.config.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>Configure the failure domains.</simpara>
<formalpara>
<title>Example Infrastructure CR with Nutanix failure domains</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  cloudConfig:
    key: config
    name: cloud-provider-config
#...
  platformSpec:
    nutanix:
      failureDomains:
      - cluster:
         type: UUID
         uuid: &lt;uuid&gt;
        name: &lt;failure_domain_name&gt;
        subnets:
        - type: UUID
          uuid: &lt;network_uuid&gt;
      - cluster:
         type: UUID
         uuid: &lt;uuid&gt;
        name: &lt;failure_domain_name&gt;
        subnets:
        - type: UUID
          uuid: &lt;network_uuid&gt;
      - cluster:
          type: UUID
          uuid: &lt;uuid&gt;
        name: &lt;failure_domain_name&gt;
        subnets:
        - type: UUID
          uuid: &lt;network_uuid&gt;
# ...</programlisting>
</para>
</formalpara>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;uuid&gt;</literal></term>
<listitem>
<simpara>Specifies the universally unique identifier (UUID) of the Prism Element.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;failure_domain_name&gt;</literal></term>
<listitem>
<simpara>Specifies a unique name for the failure domain. The name is limited to 64 or fewer characters, which can include lower-case letters, digits, and a dash (<literal>-</literal>). The dash cannot be in the leading or ending position of the name.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>&lt;network_uuid&gt;</literal></term>
<listitem>
<simpara>Specifies the UUID of the Prism Element subnet object. The subnet&#8217;s IP address prefix (CIDR) should contain the virtual IP addresses that the OpenShift Container Platform cluster uses. Only one subnet per failure domain (Prism Element) in an OpenShift Container Platform cluster is supported.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Save the CR to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="post-installation-adding-nutanix-failure-domains-control-planes_adding-failure-domains-to-an-existing-nutanix-cluster">
<title>Distributing control planes across failure domains</title>
<simpara>You distribute control planes across Nutanix failure domains by modifying the control plane machine set custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured the failure domains in the cluster&#8217;s Infrastructure custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>The control plane machine set custom resource (CR) is in an active state.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information on checking the control plane machine set custom resource state, see "Additional resources".</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the control plane machine set CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit controlplanemachineset.machine.openshift.io cluster -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Configure the control plane machine set to use failure domains by adding a <literal>spec.template.machines_v1beta1_machine_openshift_io.failureDomains</literal> stanza.</simpara>
<formalpara>
<title>Example control plane machine set with Nutanix failure domains</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1
kind: ControlPlaneMachineSet
  metadata:
    creationTimestamp: null
    labels:
      machine.openshift.io/cluster-api-cluster: &lt;cluster_name&gt;
    name: cluster
    namespace: openshift-machine-api
spec:
# ...
  template:
    machineType: machines_v1beta1_machine_openshift_io
    machines_v1beta1_machine_openshift_io:
      failureDomains:
        platform: Nutanix
        nutanix:
        - name: &lt;failure_domain_name_1&gt;
        - name: &lt;failure_domain_name_2&gt;
        - name: &lt;failure_domain_name_3&gt;
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Save your changes.</simpara>
</listitem>
</orderedlist>
<simpara>By default, the control plane machine set propagates changes to your control plane configuration automatically. If the cluster is configured to use the <literal>OnDelete</literal> update strategy, you must replace your control planes manually. For more information, see "Additional resources".</simpara>
</section>
<section xml:id="post-installation-adding-nutanix-failure-domains-compute-machines_adding-failure-domains-to-an-existing-nutanix-cluster">
<title>Distributing compute machines across failure domains</title>
<simpara>You can distribute compute machines across Nutanix failure domains by performing either of the following tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Modifying existing compute machine sets.</simpara>
</listitem>
<listitem>
<simpara>Creating new compute machine sets.</simpara>
</listitem>
</itemizedlist>
<simpara>The following procedure details how to distribute compute machines across failure domains by modifying existing compute machine sets. For more information on creating a compute machine set, see "Additional resources".</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured the failure domains in the cluster&#8217;s Infrastructure custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to view the cluster&#8217;s Infrastructure CR.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe infrastructures.config.openshift.io cluster</programlisting>
</listitem>
<listitem>
<simpara>For each failure domain (<literal>platformSpec.nutanix.failureDomains</literal>), note the cluster&#8217;s UUID, name, and subnet object UUID. These values are required to add a failure domain to a compute machine set.</simpara>
</listitem>
<listitem>
<simpara>List the compute machine sets in your cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machinesets -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Edit the first compute machine set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machineset &lt;machineset_name&gt; -n openshift-machine-api</programlisting>
</listitem>
<listitem>
<simpara>Configure the compute machine set to use the first failure domain by adding the following to the <literal>spec.template.spec.providerSpec.value</literal> stanza:</simpara>
<note>
<simpara>Be sure that the values you specify for the <literal>cluster</literal> and <literal>subnets</literal> fields match the values that were configured in the <literal>failureDomains</literal> stanza in the cluster&#8217;s Infrastructure CR.</simpara>
</note>
<formalpara>
<title>Example compute machine set with Nutanix failure domains</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1
kind: MachineSet
metadata:
  creationTimestamp: null
  labels:
    machine.openshift.io/cluster-api-cluster: &lt;cluster_name&gt;
  name: &lt;machineset_name&gt;
  namespace: openshift-machine-api
spec:
  replicas: 2
# ...
  template:
    spec:
# ...
      providerSpec:
        value:
          apiVersion: machine.openshift.io/v1
          failureDomain:
            name: &lt;failure_domain_name_1&gt;
          cluster:
            type: uuid
            uuid: &lt;prism_element_uuid_1&gt;
          subnets:
          - type: uuid
            uuid: &lt;prism_element_network_uuid_1&gt;
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Note the value of <literal>spec.replicas</literal>, as you need it when scaling the machine set to apply the changes.</simpara>
</listitem>
<listitem>
<simpara>Save your changes.</simpara>
</listitem>
<listitem>
<simpara>List the machines that are managed by the updated compute machine set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-machine-api machines -l machine.openshift.io/cluster-api-machineset=&lt;machine_set_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>For each machine that is managed by the updated compute machine set, set the <literal>delete</literal> annotation by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate machine/&lt;machine_name_original_1&gt; \
  -n openshift-machine-api \
  machine.openshift.io/delete-machine="true"</programlisting>
</listitem>
<listitem>
<simpara>Scale the compute machine set to twice the number of replicas by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=&lt;twice_the_number_of_replicas&gt; \<co xml:id="CO191-1"/>
  machineset &lt;machine_set_name&gt; \
  -n openshift-machine-api</programlisting>
<calloutlist>
<callout arearefs="CO191-1">
<para>For example, if the original number of replicas in the compute machine set is <literal>2</literal>, scale the replicas to <literal>4</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>List the machines that are managed by the updated compute machine set by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get -n openshift-machine-api machines -l machine.openshift.io/cluster-api-machineset=&lt;machine_set_name&gt;</programlisting>
<simpara>When the new machines are in the <literal>Running</literal> phase, you can scale the compute machine set to the original number of replicas.</simpara>
</listitem>
<listitem>
<simpara>Scale the compute machine set to the original number of replicas by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc scale --replicas=&lt;original_number_of_replicas&gt; \<co xml:id="CO192-1"/>
  machineset &lt;machine_set_name&gt; \
  -n openshift-machine-api</programlisting>
<calloutlist>
<callout arearefs="CO192-1">
<para>For example, if the original number of replicas in the compute machine set is <literal>2</literal>, scale the replicas to <literal>2</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>As required, continue to modify machine sets to reference the additional failure domains that are available to the deployment.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_adding-nutanix-failure-domains" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../machine_management/control_plane_machine_management/cpmso-getting-started.xml#cpmso-checking-status_cpmso-getting-started">Checking the control plane machine set custom resource state</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../machine_management/control_plane_machine_management/cpmso-using.xml#cpmso-feat-replace_cpmso-using">Replacing a control plane machine</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../machine_management/creating_machinesets/creating-machineset-nutanix.xml#creating-machineset-nutanix">Creating a compute machine set on Nutanix</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
</book>
<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>CLI tools</title>
<date>2024-02-23</date>
<title>CLI tools</title>
<productname>OpenShift Container Platform</productname>
<productnumber>4.14</productnumber>
<subtitle>Enter a short description here.</subtitle>
<abstract>
    <para>A short overview and summary of the book's subject and purpose, traditionally no more than one paragraph long.</para>
</abstract>
<authorgroup>
    <orgname>Red Hat OpenShift Documentation Team</orgname>
</authorgroup>
<xi:include href="Common_Content/Legal_Notice.xml" xmlns:xi="http://www.w3.org/2001/XInclude" />
</info>
<chapter xml:id="cli-tools-overview">
<title>OpenShift Container Platform CLI tools overview</title>

<simpara>A user performs a range of operations while working on OpenShift Container Platform
such as the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Managing clusters</simpara>
</listitem>
<listitem>
<simpara>Building, deploying, and managing applications</simpara>
</listitem>
<listitem>
<simpara>Managing deployment processes</simpara>
</listitem>
<listitem>
<simpara>Developing Operators</simpara>
</listitem>
<listitem>
<simpara>Creating and maintaining Operator catalogs</simpara>
</listitem>
</itemizedlist>
<simpara>OpenShift Container Platform
offers a set of command-line interface (CLI) tools that simplify these tasks by enabling users to perform various administration and development operations from the terminal.
These tools expose simple commands to manage the applications, as well as interact with each component of the system.</simpara>
<section xml:id="cli-tools-list">
<title>List of CLI tools</title>
<simpara>The following set of CLI tools are available in
OpenShift Container Platform:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="cli-getting-started">OpenShift CLI (<literal>oc</literal>)</link>:
This is the most commonly used CLI tool by OpenShift Container Platform users.
It helps both cluster administrators and developers to perform end-to-end operations across
OpenShift Container Platform
using the terminal. Unlike the web console, it allows the user to work directly with the project source code using command scripts.</simpara>
</listitem>
<listitem>
<simpara><link linkend="kn-cli-tools">Knative CLI (kn)</link>: The Knative (<literal>kn</literal>) CLI tool provides simple and intuitive terminal commands that can be used to interact with OpenShift Serverless components, such as Knative Serving and Eventing.</simpara>
</listitem>
<listitem>
<simpara><link linkend="installing-tkn">Pipelines CLI (tkn)</link>: OpenShift Pipelines is a continuous integration and continuous delivery (CI/CD) solution in OpenShift Container Platform, which internally uses Tekton. The <literal>tkn</literal> CLI tool provides simple and intuitive commands to interact with OpenShift Pipelines using the terminal.</simpara>
</listitem>
<listitem>
<simpara><link linkend="cli-opm-install">opm CLI</link>: The <literal>opm</literal> CLI tool helps the Operator developers and cluster administrators to create and maintain the catalogs of Operators from the terminal.</simpara>
</listitem>
<listitem>
<simpara><link linkend="cli-osdk-install">Operator SDK</link>: The Operator SDK, a component of the Operator Framework, provides a CLI tool that Operator developers can use to build, test, and deploy an Operator from the terminal. It simplifies the process of building Kubernetes-native applications, which can require deep, application-specific operational knowledge.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="_openshift-cli-oc">
<title>OpenShift CLI (oc)</title>
<section xml:id="cli-getting-started">
<title>Getting started with the OpenShift CLI</title>

<section xml:id="cli-about-cli_cli-developer-commands">
<title>About the OpenShift CLI</title>
<simpara>With the OpenShift CLI (<literal>oc</literal>), you can create applications and manage OpenShift Container Platform
projects from a terminal. The OpenShift CLI is ideal in the following situations:</simpara>
<itemizedlist>
<listitem>
<simpara>Working directly with project source code</simpara>
</listitem>
<listitem>
<simpara>Scripting
OpenShift Container Platform
operations</simpara>
</listitem>
<listitem>
<simpara>Managing projects while restricted by bandwidth resources and the web console is unavailable</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installing-openshift-cli">
<title>Installing the OpenShift CLI</title>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) either by downloading the binary or by using an RPM.</simpara>
<section xml:id="cli-installing-cli_cli-developer-commands">
<title>Installing the OpenShift CLI by downloading the binary</title>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) to interact with
OpenShift Container Platform
from a command-line interface. You can install <literal>oc</literal> on Linux, Windows, or macOS.</simpara>
<important>
<simpara>If you installed an earlier version of <literal>oc</literal>, you cannot use it to complete all of the commands in
OpenShift Container Platform 4.14.
Download and install the new version of <literal>oc</literal>.</simpara>
</important>
<bridgehead xml:id="_installing-the-openshift-cli-on-linux" renderas="sect5">Installing the OpenShift CLI on Linux</bridgehead>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) binary on Linux by using the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <link xlink:href="https://access.redhat.com/downloads/content/290">OpenShift Container Platform downloads page</link> on the Red Hat Customer Portal.</simpara>
</listitem>
<listitem>
<simpara>Select the architecture from the <emphasis role="strong">Product Variant</emphasis> drop-down list.</simpara>
</listitem>
<listitem>
<simpara>Select the appropriate version from the <emphasis role="strong">Version</emphasis> drop-down list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Download Now</emphasis> next to the <emphasis role="strong">OpenShift v4.14 Linux Client</emphasis> entry and save the file.</simpara>
</listitem>
<listitem>
<simpara>Unpack the archive:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvf &lt;file&gt;</programlisting>
</listitem>
<listitem>
<simpara>Place the <literal>oc</literal> binary in a directory that is on your <literal>PATH</literal>.</simpara>
<simpara>To check your <literal>PATH</literal>, execute the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After you install the OpenShift CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc &lt;command&gt;</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="_installing-the-openshift-cli-on-windows" renderas="sect5">Installing the OpenShift CLI on Windows</bridgehead>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) binary on Windows by using the following procedure.
.Procedure</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to the <link xlink:href="https://access.redhat.com/downloads/content/290">OpenShift Container Platform downloads page</link> on the Red Hat Customer Portal.</simpara>
</listitem>
<listitem>
<simpara>Select the appropriate version from the <emphasis role="strong">Version</emphasis> drop-down list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Download Now</emphasis> next to the <emphasis role="strong">OpenShift v4.14 Windows Client</emphasis> entry and save the file.</simpara>
</listitem>
<listitem>
<simpara>Unzip the archive with a ZIP program.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>oc</literal> binary to a directory that is on your <literal>PATH</literal>.</simpara>
<simpara>To check your <literal>PATH</literal>, open the command prompt and execute the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; path</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After you install the OpenShift CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; oc &lt;command&gt;</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="_installing-the-openshift-cli-on-macos" renderas="sect5">Installing the OpenShift CLI on macOS</bridgehead>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) binary on macOS by using the following procedure.
.Procedure</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to the <link xlink:href="https://access.redhat.com/downloads/content/290">OpenShift Container Platform downloads page</link> on the Red Hat Customer Portal.</simpara>
</listitem>
<listitem>
<simpara>Select the appropriate version from the <emphasis role="strong">Version</emphasis> drop-down list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Download Now</emphasis> next to the <emphasis role="strong">OpenShift v4.14 macOS Client</emphasis> entry and save the file.</simpara>
<note>
<simpara>For macOS arm64, choose the <emphasis role="strong">OpenShift v4.14 macOS arm64 Client</emphasis> entry.</simpara>
</note>
</listitem>
<listitem>
<simpara>Unpack and unzip the archive.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>oc</literal> binary to a directory on your PATH.</simpara>
<simpara>To check your <literal>PATH</literal>, open a terminal and execute the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After you install the OpenShift CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc &lt;command&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="cli-installing-cli-web-console_cli-developer-commands">
<title>Installing the OpenShift CLI by using the web console</title>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) to interact with OpenShift Container Platform
from a web console. You can install <literal>oc</literal> on Linux, Windows, or macOS.</simpara>
<important>
<simpara>If you installed an earlier version of <literal>oc</literal>, you cannot use it to complete all
of the commands in
OpenShift Container Platform 4.14.
Download and
install the new version of <literal>oc</literal>.</simpara>
</important>
<section xml:id="cli-installing-cli-web-console-macos-linux_cli-developer-commands">
<title>Installing the OpenShift CLI on Linux using the web console</title>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) binary on Linux by using the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the web console, click <emphasis role="strong">?</emphasis>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/click-question-mark.png"/>
</imageobject>
<textobject><phrase>click question mark</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Command Line Tools</emphasis>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/CLI-list.png"/>
</imageobject>
<textobject><phrase>CLI list</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Select appropriate <literal>oc</literal> binary for your Linux platform, and then click <emphasis role="strong">Download oc for Linux</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
<listitem>
<simpara>Unpack the archive.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvf &lt;file&gt;</programlisting>
</listitem>
<listitem>
<simpara>Move the <literal>oc</literal> binary to a directory that is on your <literal>PATH</literal>.</simpara>
<simpara>To check your <literal>PATH</literal>, execute the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
</orderedlist>
<simpara>After you install the OpenShift CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc &lt;command&gt;</programlisting>
</section>
<section xml:id="cli-installing-cli-web-console-macos-windows_cli-developer-commands">
<title>Installing the OpenShift CLI on Windows using the web console</title>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) binary on Windows by using the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the web console, click <emphasis role="strong">?</emphasis>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/click-question-mark.png"/>
</imageobject>
<textobject><phrase>click question mark</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Command Line Tools</emphasis>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/CLI-list.png"/>
</imageobject>
<textobject><phrase>CLI list</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Select the <literal>oc</literal> binary for Windows platform, and then click <emphasis role="strong">Download oc for Windows for x86_64</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
<listitem>
<simpara>Unzip the archive with a ZIP program.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>oc</literal> binary to a directory that is on your <literal>PATH</literal>.</simpara>
<simpara>To check your <literal>PATH</literal>, open the command prompt and execute the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; path</programlisting>
</listitem>
</orderedlist>
<simpara>After you install the OpenShift CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; oc &lt;command&gt;</programlisting>
</section>
<section xml:id="cli-installing-cli-web-console-macos_cli-developer-commands">
<title>Installing the OpenShift CLI on macOS using the web console</title>
<simpara>You can install the OpenShift CLI (<literal>oc</literal>) binary on macOS by using the following procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the web console, click <emphasis role="strong">?</emphasis>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/click-question-mark.png"/>
</imageobject>
<textobject><phrase>click question mark</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Command Line Tools</emphasis>.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/CLI-list.png"/>
</imageobject>
<textobject><phrase>CLI list</phrase></textobject>
</mediaobject>
</informalfigure>
</listitem>
<listitem>
<simpara>Select the <literal>oc</literal> binary for macOS platform, and then click <emphasis role="strong">Download oc for Mac for x86_64</emphasis>.</simpara>
<note>
<simpara>For macOS arm64, click <emphasis role="strong">Download oc for Mac for ARM 64</emphasis>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
<listitem>
<simpara>Unpack and unzip the archive.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>oc</literal> binary to a directory on your PATH.</simpara>
<simpara>To check your <literal>PATH</literal>, open a terminal and execute the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
</orderedlist>
<simpara>After you install the OpenShift CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc &lt;command&gt;</programlisting>
</section>
</section>
<section xml:id="cli-installing-cli-rpm_cli-developer-commands">
<title>Installing the OpenShift CLI by using an RPM</title>
<simpara>For Red Hat Enterprise Linux (RHEL), you can install the OpenShift CLI (<literal>oc</literal>) as an RPM if you have an active OpenShift Container Platform
subscription on your Red Hat account.</simpara>
<note>
<simpara>It is not supported to install the OpenShift CLI (<literal>oc</literal>) as an RPM for Red Hat Enterprise Linux (RHEL) 9. You must install the OpenShift CLI for RHEL 9 by downloading the binary.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Must have root or sudo privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Register with Red Hat Subscription Manager:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager register</programlisting>
</listitem>
<listitem>
<simpara>Pull the latest subscription data:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager refresh</programlisting>
</listitem>
<listitem>
<simpara>List the available subscriptions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager list --available --matches '*OpenShift*'</programlisting>
</listitem>
<listitem>
<simpara>In the output for the previous command, find the pool ID for
an OpenShift Container Platform
subscription and attach the subscription to the registered system:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager attach --pool=&lt;pool_id&gt;</programlisting>
</listitem>
<listitem>
<simpara>Enable the repositories required by
OpenShift Container Platform 4.14.</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --enable="rhocp-4.14-for-rhel-8-x86_64-rpms"</programlisting>
</listitem>
<listitem>
<simpara>Install the <literal>openshift-clients</literal> package:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum install openshift-clients</programlisting>
</listitem>
</orderedlist>
<simpara>After you install the CLI, it is available using the <literal>oc</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc &lt;command&gt;</programlisting>
</section>
<section xml:id="cli-installing-cli-brew_cli-developer-commands">
<title>Installing the OpenShift CLI by using Homebrew</title>
<simpara>For macOS, you can install the OpenShift CLI (<literal>oc</literal>) by using the <link xlink:href="https://brew.sh">Homebrew</link> package manager.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have Homebrew (<literal>brew</literal>) installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to install the <link xlink:href="https://formulae.brew.sh/formula/openshift-cli">openshift-cli</link> package:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ brew install openshift-cli</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cli-logging-in_cli-developer-commands">
<title>Logging in to the OpenShift CLI</title>
<simpara>You can log in to the OpenShift CLI (<literal>oc</literal>) to access and manage your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to
an OpenShift Container Platform
cluster.</simpara>
</listitem>
<listitem>
<simpara>The OpenShift CLI (<literal>oc</literal>) is installed.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>To access a cluster that is accessible only over an HTTP proxy server, you can set the <literal>HTTP_PROXY</literal>, <literal>HTTPS_PROXY</literal> and <literal>NO_PROXY</literal> variables.
These environment variables are respected by the <literal>oc</literal> CLI so that all communication with the cluster goes through the HTTP proxy.</simpara>
<simpara>Authentication headers are sent only when using HTTPS transport.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enter the <literal>oc login</literal> command and pass in a user name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login -u user1</programlisting>
</listitem>
<listitem>
<simpara>When prompted, enter the required information:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Server [https://localhost:8443]: https://openshift.example.com:6443 <co xml:id="CO1-1"/>
The server uses a certificate signed by an unknown authority.
You can bypass the certificate check, but any data you send to the server could be intercepted by others.
Use insecure connections? (y/n): y <co xml:id="CO1-2"/>

Authentication required for https://openshift.example.com:6443 (openshift)
Username: user1
Password: <co xml:id="CO1-3"/>
Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project &lt;projectname&gt;

Welcome! See 'oc help' to get started.</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO1-1">
<para>Enter the
OpenShift Container Platform
server URL.</para>
</callout>
<callout arearefs="CO1-2">
<para>Enter whether to use insecure connections.</para>
</callout>
<callout arearefs="CO1-3">
<para>Enter the user&#8217;s password.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<note>
<simpara>If you are logged in to the web console, you can generate an <literal>oc login</literal> command that includes your token and server information. You can use the command to log in to the
OpenShift Container Platform
CLI without the interactive prompts. To generate the command, select <emphasis role="strong">Copy login command</emphasis> from the username drop-down menu at the top right of the web console.</simpara>
</note>
<simpara>You can now create a project or issue other commands for managing your cluster.</simpara>
</section>
<section xml:id="cli-logging-in-web_cli-developer-commands">
<title>Logging in to the OpenShift CLI using a web browser</title>
<simpara>You can log in to the OpenShift CLI (<literal>oc</literal>) with the help of a web browser to access and manage your cluster. This allows users to avoid inserting their access token into the command line.</simpara>
<warning>
<simpara>Logging in to the CLI through the web browser runs a server on localhost with HTTP, not HTTPS; use with caution on multi-user workstations.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to an OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>You must have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You must have a browser installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enter the <literal>oc login</literal> command with the <literal>--web</literal> flag:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login &lt;cluster_url&gt; --web <co xml:id="CO2-1"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>Optionally, you can specify the server URL and callback port. For example, <literal>oc login &lt;cluster_url&gt; --web --callback-port 8280 localhost:8443</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>The web browser opens automatically. If it does not, click the link in the command output. If you do not specify the OpenShift Container Platform server <literal>oc</literal> tries to open the web console of the cluster specified in the current <literal>oc</literal> configuration file. If no <literal>oc</literal> configuration exists, <literal>oc</literal> prompts interactively for the server URL.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Opening login URL in the default browser: https://openshift.example.com
Opening in existing browser session.</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>If more than one identity provider is available, select your choice from the options provided.</simpara>
</listitem>
<listitem>
<simpara>Enter your username and password into the corresponding browser fields. After you are logged in, the browser displays the text <literal>access token received successfully; please return to your terminal</literal>.</simpara>
</listitem>
<listitem>
<simpara>Check the CLI for a login confirmation.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Login successful.

You don't have any projects. You can try to create a new project, by running

    oc new-project &lt;projectname&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>The web console defaults to the profile used in the previous session. To switch between Administrator and Developer profiles, log out of the OpenShift Container Platform web console and clear the cache.</simpara>
</note>
<simpara>You can now create a project or issue other commands for managing your cluster.</simpara>
</section>
<section xml:id="cli-using-cli_cli-developer-commands">
<title>Using the OpenShift CLI</title>
<simpara>Review the following sections to learn how to complete common tasks using the CLI.</simpara>
<section xml:id="_creating-a-project">
<title>Creating a project</title>
<simpara>Use the <literal>oc new-project</literal> command to create a new project.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-project my-project</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Now using project "my-project" on server "https://openshift.example.com:6443".</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_creating-a-new-app">
<title>Creating a new app</title>
<simpara>Use the <literal>oc new-app</literal> command to create a new application.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-app https://github.com/sclorg/cakephp-ex</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">--&gt; Found image 40de956 (9 days old) in imagestream "openshift/php" under tag "7.2" for "php"

...

    Run 'oc status' to view your app.</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_viewing-pods">
<title>Viewing pods</title>
<simpara>Use the <literal>oc get pods</literal> command to view the pods for the current project.</simpara>
<note>
<simpara>When you run <literal>oc</literal> inside a pod and do not specify a namespace, the namespace of the pod is used by default.</simpara>
</note>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                  READY   STATUS      RESTARTS   AGE     IP            NODE                           NOMINATED NODE
cakephp-ex-1-build    0/1     Completed   0          5m45s   10.131.0.10   ip-10-0-141-74.ec2.internal    &lt;none&gt;
cakephp-ex-1-deploy   0/1     Completed   0          3m44s   10.129.2.9    ip-10-0-147-65.ec2.internal    &lt;none&gt;
cakephp-ex-1-ktz97    1/1     Running     0          3m33s   10.128.2.11   ip-10-0-168-105.ec2.internal   &lt;none&gt;</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_viewing-pod-logs">
<title>Viewing pod logs</title>
<simpara>Use the <literal>oc logs</literal> command to view logs for a particular pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs cakephp-ex-1-deploy</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">--&gt; Scaling cakephp-ex-1 to 1
--&gt; Success</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_viewing-the-current-project">
<title>Viewing the current project</title>
<simpara>Use the <literal>oc project</literal> command to view the current project.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Using project "my-project" on server "https://openshift.example.com:6443".</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_viewing-the-status-for-the-current-project">
<title>Viewing the status for the current project</title>
<simpara>Use the <literal>oc status</literal> command to view information about the current project, such
as services, deployments, and build configs.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc status</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">In project my-project on server https://openshift.example.com:6443

svc/cakephp-ex - 172.30.236.80 ports 8080, 8443
  dc/cakephp-ex deploys istag/cakephp-ex:latest &lt;-
    bc/cakephp-ex source builds https://github.com/sclorg/cakephp-ex on openshift/php:7.2
    deployment #1 deployed 2 minutes ago - 1 pod

3 infos identified, use 'oc status --suggest' to see details.</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_listing-supported-api-resources">
<title>Listing supported API resources</title>
<simpara>Use the <literal>oc api-resources</literal> command to view the list of supported API resources
on the server.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc api-resources</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                  SHORTNAMES       APIGROUP                              NAMESPACED   KIND
bindings                                                                                     true         Binding
componentstatuses                     cs                                                     false        ComponentStatus
configmaps                            cm                                                     true         ConfigMap
...</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="cli-getting-help_cli-developer-commands">
<title>Getting help</title>
<simpara>You can get help with CLI commands and
OpenShift Container Platform
resources in the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara>Use <literal>oc help</literal> to get a list and description of all available CLI commands:</simpara>
<formalpara>
<title>Example: Get general help for the CLI</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc help</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">OpenShift Client

This client helps you develop, build, deploy, and run your applications on any OpenShift or Kubernetes compatible
platform. It also includes the administrative commands for managing a cluster under the 'adm' subcommand.

Usage:
  oc [flags]

Basic Commands:
  login           Log in to a server
  new-project     Request a new project
  new-app         Create a new application

...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>--help</literal> flag to get help about a specific CLI command:</simpara>
<formalpara>
<title>Example: Get help for the <literal>oc create</literal> command</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create --help</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Create a resource by filename or stdin

JSON and YAML formats are accepted.

Usage:
  oc create -f FILENAME [flags]

...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>oc explain</literal> command to view the description and fields for a
particular resource:</simpara>
<formalpara>
<title>Example: View documentation for the <literal>Pod</literal> resource</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc explain pods</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">KIND:     Pod
VERSION:  v1

DESCRIPTION:
     Pod is a collection of containers that can run on a host. This resource is
     created by clients and scheduled onto hosts.

FIELDS:
   apiVersion	&lt;string&gt;
     APIVersion defines the versioned schema of this representation of an
     object. Servers should convert recognized schemas to the latest internal
     value, and may reject unrecognized values. More info:
     https://git.k8s.io/community/contributors/devel/api-conventions.md#resources

...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cli-logging-out_cli-developer-commands">
<title>Logging out of the OpenShift CLI</title>
<simpara>You can log out the OpenShift CLI to end your current session.</simpara>
<itemizedlist>
<listitem>
<simpara>Use the <literal>oc logout</literal> command.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logout</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Logged "user1" out on "https://openshift.example.com"</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<simpara>This deletes the saved authentication token from the server and removes it from
your configuration file.</simpara>
</section>
</section>
<section xml:id="cli-configuring-cli">
<title>Configuring the OpenShift CLI</title>

<section xml:id="cli-enabling-tab-completion">
<title>Enabling tab completion</title>
<simpara>You can enable tab completion for the Bash or Zsh shells.</simpara>
<section xml:id="cli-enabling-tab-completion_cli-configuring-cli">
<title>Enabling tab completion for Bash</title>
<simpara>After you install the OpenShift CLI (<literal>oc</literal>), you can enable tab completion to automatically complete <literal>oc</literal> commands or suggest options when you press Tab. The following procedure enables tab completion for the Bash shell.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
<listitem>
<simpara>You must have the package <literal>bash-completion</literal> installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Save the Bash completion code to a file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc completion bash &gt; oc_bash_completion</programlisting>
</listitem>
<listitem>
<simpara>Copy the file to <literal>/etc/bash_completion.d/</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo cp oc_bash_completion /etc/bash_completion.d/</programlisting>
<simpara>You can also save the file to a local directory and source it from your <literal>.bashrc</literal> file instead.</simpara>
</listitem>
</orderedlist>
<simpara>Tab completion is enabled when you open a new terminal.</simpara>
</section>
<section xml:id="cli-enabling-tab-completion-zsh_cli-configuring-cli">
<title>Enabling tab completion for Zsh</title>
<simpara>After you install the OpenShift CLI (<literal>oc</literal>), you can enable tab completion to automatically complete <literal>oc</literal> commands or suggest options when you press Tab. The following procedure enables tab completion for the Zsh shell.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To add tab completion for <literal>oc</literal> to your <literal>.zshrc</literal> file, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &gt;&gt;~/.zshrc&lt;&lt;EOF
if [ $commands[oc] ]; then
  source &lt;(oc completion zsh)
  compdef _oc oc
fi
EOF</programlisting>
</listitem>
</itemizedlist>
<simpara>Tab completion is enabled when you open a new terminal.</simpara>
</section>
</section>
</section>
<section xml:id="usage-oc-kubectl">
<title>Usage of oc and kubectl commands</title>
<simpara>The Kubernetes command-line interface (CLI), <literal>kubectl</literal>, can be used to run commands against a Kubernetes cluster. Because OpenShift Container Platform
is a certified Kubernetes distribution, you can use the supported <literal>kubectl</literal> binaries that ship with
OpenShift Container Platform
, or you can gain extended functionality by using the <literal>oc</literal> binary.</simpara>
<section xml:id="_the-oc-binary">
<title>The oc binary</title>
<simpara>The <literal>oc</literal> binary offers the same capabilities as the <literal>kubectl</literal> binary, but it extends to natively support additional
OpenShift Container Platform
features, including:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Full support for
OpenShift Container Platform
resources</emphasis></simpara>
<simpara>Resources such as <literal>DeploymentConfig</literal>, <literal>BuildConfig</literal>, <literal>Route</literal>, <literal>ImageStream</literal>, and <literal>ImageStreamTag</literal> objects are specific to
OpenShift Container Platform
distributions, and build upon standard Kubernetes primitives.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Authentication</emphasis></simpara>
<simpara>The <literal>oc</literal> binary offers a built-in <literal>login</literal> command for authentication and lets you work with projects, which map Kubernetes namespaces to authenticated users.
Read <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#understanding-authentication">Understanding authentication</link> for more information.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Additional commands</emphasis></simpara>
<simpara>The additional command <literal>oc new-app</literal>, for example, makes it easier to get new applications started using existing source code or pre-built images. Similarly, the additional command <literal>oc new-project</literal> makes it easier to start a project that you can switch to as your default.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>If you installed an earlier version of the <literal>oc</literal> binary, you cannot use it to complete all of the commands in
OpenShift Container Platform 4.14
. If you want the latest features, you must download and install the latest version of the <literal>oc</literal> binary corresponding to your
OpenShift Container Platform
server version.</simpara>
</important>
<simpara>Non-security API changes will involve, at minimum, two minor releases (4.1 to 4.2 to 4.3, for example) to allow older <literal>oc</literal> binaries to update. Using new capabilities might require newer <literal>oc</literal> binaries. A 4.3 server might have additional capabilities that a 4.2 <literal>oc</literal> binary cannot use and a 4.3 <literal>oc</literal> binary might have additional capabilities that are unsupported by a 4.2 server.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Compatibility Matrix</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<tbody>
<row>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">X.Y</emphasis> (<literal>oc</literal> Client)</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">X.Y+N</emphasis> <footnote xml:id="versionpolicyn"><simpara>Where <emphasis role="strong">N</emphasis> is a number greater than or equal to 1.</simpara></footnote> (<literal>oc</literal> Client)</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">X.Y</emphasis> (Server)</simpara></entry>
<entry align="left" valign="top"><simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-1.png"/>
</imageobject>
<textobject><phrase>redcircle 1</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-3.png"/>
</imageobject>
<textobject><phrase>redcircle 3</phrase></textobject>
</inlinemediaobject></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">X.Y+N</emphasis> <footnoteref linkend="versionpolicyn"/> (Server)</simpara></entry>
<entry align="left" valign="top"><simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-2.png"/>
</imageobject>
<textobject><phrase>redcircle 2</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-1.png"/>
</imageobject>
<textobject><phrase>redcircle 1</phrase></textobject>
</inlinemediaobject></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-1.png"/>
</imageobject>
<textobject><phrase>redcircle 1</phrase></textobject>
</inlinemediaobject> Fully compatible.</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-2.png"/>
</imageobject>
<textobject><phrase>redcircle 2</phrase></textobject>
</inlinemediaobject> <literal>oc</literal> client might not be able to access server features.</simpara>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/redcircle-3.png"/>
</imageobject>
<textobject><phrase>redcircle 3</phrase></textobject>
</inlinemediaobject> <literal>oc</literal> client might provide options and features that might not be compatible with the accessed server.</simpara>
</section>
<section xml:id="_the-kubectl-binary">
<title>The kubectl binary</title>
<simpara>The <literal>kubectl</literal> binary is provided as a means to support existing workflows and scripts for new
OpenShift Container Platform
users coming from a standard Kubernetes environment, or for those who prefer to use the <literal>kubectl</literal> CLI. Existing users of <literal>kubectl</literal> can continue to use the binary to interact with Kubernetes primitives, with no changes required to the
OpenShift Container Platform
cluster.</simpara>
<simpara>You can install the supported <literal>kubectl</literal> binary by following the steps to <link linkend="cli-installing-cli_cli-developer-commands">Install the OpenShift CLI</link>. The <literal>kubectl</literal> binary is included in the archive if you download the binary, or is installed when you install the CLI by using an RPM.</simpara>
<simpara>For more information, see the <link xlink:href="https://kubernetes.io/docs/reference/kubectl/overview/">kubectl documentation</link>.</simpara>
</section>
</section>
<section xml:id="managing-cli-profiles">
<title>Managing CLI profiles</title>

<simpara>A CLI configuration file allows you to configure different profiles, or contexts, for use with the <link linkend="cli-tools-overview">CLI tools overview</link>. A context consists of
<link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#understanding-authentication">user authentication</link>
an OpenShift Container Platform
server information associated with a <emphasis>nickname</emphasis>.</simpara>
<section xml:id="about-switches-between-cli-profiles_managing-cli-profiles">
<title>About switches between CLI profiles</title>
<simpara>Contexts allow you to easily switch between multiple users across multiple
OpenShift Container Platform
servers, or clusters, when using CLI operations. Nicknames make managing CLI configurations easier by providing short-hand references to contexts, user credentials, and cluster details.
After a user logs in with the <literal>oc</literal> CLI for the first time,
OpenShift Container Platform
creates a <literal>~/.kube/config</literal> file if one does not already exist. As more authentication and connection details are provided to the CLI, either automatically during an <literal>oc login</literal> operation or by manually configuring CLI profiles, the updated information is stored in the configuration file:</simpara>
<formalpara>
<title>CLI config file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
clusters: <co xml:id="CO3-1"/>
- cluster:
    insecure-skip-tls-verify: true
    server: https://openshift1.example.com:8443
  name: openshift1.example.com:8443
- cluster:
    insecure-skip-tls-verify: true
    server: https://openshift2.example.com:8443
  name: openshift2.example.com:8443
contexts: <co xml:id="CO3-2"/>
- context:
    cluster: openshift1.example.com:8443
    namespace: alice-project
    user: alice/openshift1.example.com:8443
  name: alice-project/openshift1.example.com:8443/alice
- context:
    cluster: openshift1.example.com:8443
    namespace: joe-project
    user: alice/openshift1.example.com:8443
  name: joe-project/openshift1/alice
current-context: joe-project/openshift1.example.com:8443/alice <co xml:id="CO3-3"/>
kind: Config
preferences: {}
users: <co xml:id="CO3-4"/>
- name: alice/openshift1.example.com:8443
  user:
    token: xZHd2piv5_9vQrg-SKXRJ2Dsl9SceNJdhNTljEKTb8k</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO3-1">
<para>The <literal>clusters</literal> section defines connection details for
OpenShift Container Platform
clusters, including the address for their master server. In this example, one cluster is nicknamed <literal>openshift1.example.com:8443</literal> and another is nicknamed <literal>openshift2.example.com:8443</literal>.</para>
</callout>
<callout arearefs="CO3-2">
<para>This <literal>contexts</literal> section defines two contexts: one nicknamed <literal>alice-project/openshift1.example.com:8443/alice</literal>, using the <literal>alice-project</literal> project, <literal>openshift1.example.com:8443</literal> cluster, and <literal>alice</literal> user, and another nicknamed <literal>joe-project/openshift1.example.com:8443/alice</literal>, using the <literal>joe-project</literal> project, <literal>openshift1.example.com:8443</literal> cluster and <literal>alice</literal> user.</para>
</callout>
<callout arearefs="CO3-3">
<para>The <literal>current-context</literal> parameter shows that the <literal>joe-project/openshift1.example.com:8443/alice</literal> context is currently in use, allowing the <literal>alice</literal> user to work in the <literal>joe-project</literal> project on the <literal>openshift1.example.com:8443</literal> cluster.</para>
</callout>
<callout arearefs="CO3-4">
<para>The <literal>users</literal> section defines user credentials. In this example, the user nickname <literal>alice/openshift1.example.com:8443</literal> uses an access token.</para>
</callout>
</calloutlist>
<simpara>The CLI can support multiple configuration files which are loaded at runtime and merged together along with any override options specified from the command line. After you are logged in, you can use the <literal>oc status</literal> or <literal>oc project</literal> command to verify your current working environment:</simpara>
<formalpara>
<title>Verify the current working environment</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc status</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">oc status
In project Joe's Project (joe-project)

service database (172.30.43.12:5434 -&gt; 3306)
  database deploys docker.io/openshift/mysql-55-centos7:latest
    #1 deployed 25 minutes ago - 1 pod

service frontend (172.30.159.137:5432 -&gt; 8080)
  frontend deploys origin-ruby-sample:latest &lt;-
    builds https://github.com/openshift/ruby-hello-world with joe-project/ruby-20-centos7:latest
    #1 deployed 22 minutes ago - 2 pods

To see more information about a service or deployment, use 'oc describe service &lt;name&gt;' or 'oc describe dc &lt;name&gt;'.
You can use 'oc get all' to see lists of each of the types described in this example.</programlisting>
</para>
</formalpara>
<formalpara>
<title>List the current project</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Using project "joe-project" from context named "joe-project/openshift1.example.com:8443/alice" on server "https://openshift1.example.com:8443".</programlisting>
</para>
</formalpara>
<simpara>You can run the <literal>oc login</literal> command again and supply the required information during the interactive process, to log in using any other combination of user credentials and cluster details. A context is constructed based on the supplied information if one does not already exist. If you are already logged in and want to switch to another project the current user already has access to, use the <literal>oc project</literal> command and enter the name of the project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc project alice-project</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Now using project "alice-project" on server "https://openshift1.example.com:8443".</programlisting>
</para>
</formalpara>
<simpara>At any time, you can use the <literal>oc config view</literal> command to view your current CLI configuration, as seen in the output. Additional CLI configuration commands are also available for more advanced usage.</simpara>
<note>
<simpara>If you have access to administrator credentials but are no longer logged in as the default system user <literal>system:admin</literal>, you can log back in as this user at any time as long as the credentials are still present in your CLI config file. The following command logs in and switches to the default project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login -u system:admin -n default</programlisting>
</note>
</section>
<section xml:id="manual-configuration-of-cli-profiles_managing-cli-profiles">
<title>Manual configuration of CLI profiles</title>
<note>
<simpara>This section covers more advanced usage of CLI configurations. In most situations, you can use the <literal>oc login</literal> and <literal>oc project</literal> commands to log in and switch between contexts and projects.</simpara>
</note>
<simpara>If you want to manually configure your CLI config files, you can use the <literal>oc config</literal> command instead of directly modifying the files. The <literal>oc config</literal> command includes a number of helpful sub-commands for this purpose:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>CLI configuration subcommands</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="11.1111*"/>
<colspec colname="col_2" colwidth="88.8889*"/>
<thead>
<row>
<entry align="left" valign="top">Subcommand</entry>
<entry align="left" valign="top">Usage</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>set-cluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets a cluster entry in the CLI config file. If the referenced cluster
nickname already exists, the specified information is merged in.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config set-cluster &lt;cluster_nickname&gt; [--server=&lt;master_ip_or_fqdn&gt;]
[--certificate-authority=&lt;path/to/certificate/authority&gt;]
[--api-version=&lt;apiversion&gt;] [--insecure-skip-tls-verify=true]</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>set-context</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets a context entry in the CLI config file. If the referenced context
nickname already exists, the specified information is merged in.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config set-context &lt;context_nickname&gt; [--cluster=&lt;cluster_nickname&gt;]
[--user=&lt;user_nickname&gt;] [--namespace=&lt;namespace&gt;]</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>use-context</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets the current context using the specified context nickname.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config use-context &lt;context_nickname&gt;</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>set</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Sets an individual value in the CLI config file.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config set &lt;property_name&gt; &lt;property_value&gt;</programlisting>
<simpara>The <literal>&lt;property_name&gt;</literal> is a dot-delimited name where each token represents either an attribute name or a map key. The <literal>&lt;property_value&gt;</literal> is the new value being set.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>unset</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Unsets individual values in the CLI config file.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config unset &lt;property_name&gt;</programlisting>
<simpara>The <literal>&lt;property_name&gt;</literal> is a dot-delimited name where each token represents either an attribute name or a map key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>view</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Displays the merged CLI configuration currently in use.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config view</programlisting>
<simpara>Displays the result of the specified CLI config file.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config view --config=&lt;specific_filename&gt;</programlisting></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist>
<title>Example usage</title>
<listitem>
<simpara>Log in as a user that uses an access token.
This token is used by the <literal>alice</literal> user:</simpara>
</listitem>
</itemizedlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login https://openshift1.example.com --token=ns7yVhuRNpDM9cgzfhhxQ7bM5s7N2ZVrkZepSRf4LC0</programlisting>
<itemizedlist>
<listitem>
<simpara>View the cluster entry automatically created:</simpara>
</listitem>
</itemizedlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config view</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: v1
clusters:
- cluster:
    insecure-skip-tls-verify: true
    server: https://openshift1.example.com
  name: openshift1-example-com
contexts:
- context:
    cluster: openshift1-example-com
    namespace: default
    user: alice/openshift1-example-com
  name: default/openshift1-example-com/alice
current-context: default/openshift1-example-com/alice
kind: Config
preferences: {}
users:
- name: alice/openshift1.example.com
  user:
    token: ns7yVhuRNpDM9cgzfhhxQ7bM5s7N2ZVrkZepSRf4LC0</programlisting>
</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>Update the current context to have users log in to the desired namespace:</simpara>
</listitem>
</itemizedlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc config set-context `oc config current-context` --namespace=&lt;project_name&gt;</programlisting>
<itemizedlist>
<listitem>
<simpara>Examine the current context, to confirm that the changes are implemented:</simpara>
</listitem>
</itemizedlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc whoami -c</programlisting>
<simpara>All subsequent CLI operations uses the new context, unless otherwise specified by overriding CLI options or until the context is switched.</simpara>
</section>
<section xml:id="load-and-merge-rules_managing-cli-profiles">
<title>Load and merge rules</title>
<simpara>You can follow these rules, when issuing CLI operations for the loading and merging order for the CLI configuration:</simpara>
<itemizedlist>
<listitem>
<simpara>CLI config files are retrieved from your workstation, using the following hierarchy and merge rules:</simpara>
<itemizedlist>
<listitem>
<simpara>If the <literal>--config</literal> option is set, then only that file is loaded. The flag is set once and no merging takes place.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>$KUBECONFIG</literal> environment variable is set, then it is used. The variable can be a list of paths, and if so the paths are merged together. When a value is modified, it is modified in the file that defines the stanza. When a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the last file in the list.</simpara>
</listitem>
<listitem>
<simpara>Otherwise, the <literal><emphasis>~/.kube/config</emphasis></literal> file is used and no merging takes place.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The context to use is determined based on the first match in the following flow:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of the <literal>--context</literal> option.</simpara>
</listitem>
<listitem>
<simpara>The <literal>current-context</literal> value from the CLI config file.</simpara>
</listitem>
<listitem>
<simpara>An empty value is allowed at this stage.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The user and cluster to use is determined. At this point, you may or may not have a context; they are built based on the first match in the following flow, which is run once for the user and once for the cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of the <literal>--user</literal> for user name and  <literal>--cluster</literal> option for
cluster name.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>--context</literal> option is present, then use the context&#8217;s value.</simpara>
</listitem>
<listitem>
<simpara>An empty value is allowed at this stage.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The actual cluster information to use is determined. At this point, you may or may not have cluster information. Each piece of the cluster information is built based on the first match in the following flow:</simpara>
<itemizedlist>
<listitem>
<simpara>The values of any of the following command line options:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--server</literal>,</simpara>
</listitem>
<listitem>
<simpara><literal>--api-version</literal></simpara>
</listitem>
<listitem>
<simpara><literal>--certificate-authority</literal></simpara>
</listitem>
<listitem>
<simpara><literal>--insecure-skip-tls-verify</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If cluster information and a value for the attribute is present, then use it.</simpara>
</listitem>
<listitem>
<simpara>If you do not have a server location, then there is an error.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The actual user information to use is determined. Users are built using the same rules as clusters, except that you can only have one authentication technique per user; conflicting techniques cause the operation to fail. Command line options take precedence over config file values. Valid command line options are:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--auth-path</literal></simpara>
</listitem>
<listitem>
<simpara><literal>--client-certificate</literal></simpara>
</listitem>
<listitem>
<simpara><literal>--client-key</literal></simpara>
</listitem>
<listitem>
<simpara><literal>--token</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>For any information that is still missing, default values are used and prompts are given for additional information.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cli-extend-plugins">
<title>Extending the OpenShift CLI with plugins</title>

<simpara>You can write and install plugins to build on the default <literal>oc</literal> commands,
allowing you to perform new and more complex tasks with the
OpenShift Container Platform
CLI.</simpara>
<section xml:id="cli-writing-plugins_cli-extend-plugins">
<title>Writing CLI plugins</title>
<simpara>You can write a plugin for the
OpenShift Container Platform
CLI in any programming language
or script that allows you to write command-line commands. Note that you can not
use a plugin to overwrite an existing <literal>oc</literal> command.</simpara>
<formalpara>
<title>Procedure</title>
<para>This procedure creates a simple Bash plugin that prints a message to the
terminal when the <literal>oc foo</literal> command is issued.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a file called <literal>oc-foo</literal>.</simpara>
<simpara>When naming your plugin file, keep the following in mind:</simpara>
<itemizedlist>
<listitem>
<simpara>The file must begin with <literal>oc-</literal> or <literal>kubectl-</literal> to be recognized as a
plugin.</simpara>
</listitem>
<listitem>
<simpara>The file name determines the command that invokes the plugin. For example, a
plugin with the file name <literal>oc-foo-bar</literal> can be invoked by a command of
<literal>oc foo bar</literal>. You can also use underscores if you want the command to contain
dashes. For example, a plugin with the file name <literal>oc-foo_bar</literal> can be invoked
by a command of <literal>oc foo-bar</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Add the following contents to the file.</simpara>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash

# optional argument handling
if [[ "$1" == "version" ]]
then
    echo "1.0.0"
    exit 0
fi

# optional argument handling
if [[ "$1" == "config" ]]
then
    echo $KUBECONFIG
    exit 0
fi

echo "I am a plugin named kubectl-foo"</programlisting>
</listitem>
</orderedlist>
<simpara>After you install this plugin for the
OpenShift Container Platform
CLI, it can be invoked
using the <literal>oc foo</literal> command.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>Review the <link xlink:href="https://github.com/kubernetes/sample-cli-plugin">Sample plugin repository</link>
for an example of a plugin written in Go.</simpara>
</listitem>
<listitem>
<simpara>Review the <link xlink:href="https://github.com/kubernetes/cli-runtime/">CLI runtime repository</link> for a set of utilities to assist in writing plugins in Go.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cli-installing-plugins_cli-extend-plugins">
<title>Installing and using CLI plugins</title>
<simpara>After you write a custom plugin for the
OpenShift Container Platform
CLI, you must install
the plugin before use.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <literal>oc</literal> CLI tool installed.</simpara>
</listitem>
<listitem>
<simpara>You must have a CLI plugin file that begins with <literal>oc-</literal> or <literal>kubectl-</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If necessary, update the plugin file to be executable.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chmod +x &lt;plugin_file&gt;</programlisting>
</listitem>
<listitem>
<simpara>Place the file anywhere in your <literal>PATH</literal>, such as <literal>/usr/local/bin/</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv &lt;plugin_file&gt; /usr/local/bin/.</programlisting>
</listitem>
<listitem>
<simpara>Run <literal>oc plugin list</literal> to make sure that the plugin is listed.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc plugin list</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">The following compatible plugins are available:

/usr/local/bin/&lt;plugin_file&gt;</programlisting>
</para>
</formalpara>
<simpara>If your plugin is not listed here, verify that the file begins with <literal>oc-</literal>
or <literal>kubectl-</literal>, is executable, and is on your <literal>PATH</literal>.</simpara>
</listitem>
<listitem>
<simpara>Invoke the new command or option introduced by the plugin.</simpara>
<simpara>For example, if you built and installed the <literal>kubectl-ns</literal> plugin from the
 <link xlink:href="https://github.com/kubernetes/sample-cli-plugin">Sample plugin repository</link>,
  you can use the following command to view the current namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc ns</programlisting>
<simpara>Note that the command to invoke the plugin depends on the plugin file name.
For example, a plugin with the file name of <literal>oc-foo-bar</literal> is invoked by the <literal>oc foo bar</literal>
command.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="managing-cli-plugin-krew">
<title>Managing CLI plugins with Krew</title>

<simpara>You can use Krew to install and manage plugins for the OpenShift CLI (<literal>oc</literal>).</simpara>
<important>
<simpara>Using Krew to install and manage plugins for the OpenShift CLI is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="cli-krew-install-plugin_managing-cli-plugins-krew">
<title>Installing a CLI plugin with Krew</title>
<simpara>You can install a plugin for the OpenShift CLI (<literal>oc</literal>) with Krew.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed Krew by following the <link xlink:href="https://krew.sigs.k8s.io/docs/user-guide/setup/install/">installation procedure</link> in the Krew documentation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To list all available plugins, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew search</programlisting>
</listitem>
<listitem>
<simpara>To get information about a plugin, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew info &lt;plugin_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>To install a plugin, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew install &lt;plugin_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>To list all plugins that were installed by Krew, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew list</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="cli-krew-update-plugin_managing-cli-plugins-krew">
<title>Updating a CLI plugin with Krew</title>
<simpara>You can update a plugin that was installed for the OpenShift CLI (<literal>oc</literal>) with Krew.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed Krew by following the <link xlink:href="https://krew.sigs.k8s.io/docs/user-guide/setup/install/">installation procedure</link> in the Krew documentation.</simpara>
</listitem>
<listitem>
<simpara>You have installed a plugin for the OpenShift CLI with Krew.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To update a single plugin, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew upgrade &lt;plugin_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>To update all plugins that were installed by Krew, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew upgrade</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="cli-krew-remove-plugin_managing-cli-plugins-krew">
<title>Uninstalling a CLI plugin with Krew</title>
<simpara>You can uninstall a plugin that was installed for the OpenShift CLI (<literal>oc</literal>) with Krew.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed Krew by following the <link xlink:href="https://krew.sigs.k8s.io/docs/user-guide/setup/install/">installation procedure</link> in the Krew documentation.</simpara>
</listitem>
<listitem>
<simpara>You have installed a plugin for the OpenShift CLI with Krew.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To uninstall a plugin, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc krew uninstall &lt;plugin_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional-resources_managing-cli-plugins-krew" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://krew.sigs.k8s.io/">Krew</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="cli-extend-plugins">Extending the OpenShift CLI with plugins</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cli-developer-commands">
<title>OpenShift CLI developer command reference</title>

<simpara>This reference provides descriptions and example commands for OpenShift CLI (<literal>oc</literal>) developer commands.
For administrator commands, see the <link linkend="cli-administrator-commands">OpenShift CLI administrator command reference</link>.</simpara>
<simpara>Run <literal>oc help</literal> to list all commands or run <literal>oc &lt;command&gt; --help</literal> to get additional details for a specific command.</simpara>
<section xml:id="openshift-cli-developer_cli-developer-commands">
<title>OpenShift CLI (oc) developer commands</title>
<section xml:id="_oc-annotate">
<title>oc annotate</title>
<simpara>Update the annotations on a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Update pod 'foo' with the annotation 'description' and the value 'my frontend'
  # If the same annotation is set multiple times, only the last value will be applied
  oc annotate pods foo description='my frontend'

  # Update a pod identified by type and name in "pod.json"
  oc annotate -f pod.json description='my frontend'

  # Update pod 'foo' with the annotation 'description' and the value 'my frontend running nginx', overwriting any existing value
  oc annotate --overwrite pods foo description='my frontend running nginx'

  # Update all pods in the namespace
  oc annotate pods --all description='my frontend running nginx'

  # Update pod 'foo' only if the resource is unchanged from version 1
  oc annotate pods foo description='my frontend running nginx' --resource-version=1

  # Update pod 'foo' by removing an annotation named 'description' if it exists
  # Does not require the --overwrite flag
  oc annotate pods foo description-</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-api-resources">
<title>oc api-resources</title>
<simpara>Print the supported API resources on the server</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print the supported API resources
  oc api-resources

  # Print the supported API resources with more information
  oc api-resources -o wide

  # Print the supported API resources sorted by a column
  oc api-resources --sort-by=name

  # Print the supported namespaced resources
  oc api-resources --namespaced=true

  # Print the supported non-namespaced resources
  oc api-resources --namespaced=false

  # Print the supported API resources with a specific APIGroup
  oc api-resources --api-group=rbac.authorization.k8s.io</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-api-versions">
<title>oc api-versions</title>
<simpara>Print the supported API versions on the server, in the form of "group/version"</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print the supported API versions
  oc api-versions</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-apply">
<title>oc apply</title>
<simpara>Apply a configuration to a resource by file name or stdin</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Apply the configuration in pod.json to a pod
  oc apply -f ./pod.json

  # Apply resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  oc apply -k dir/

  # Apply the JSON passed into stdin to a pod
  cat pod.json | oc apply -f -

  # Apply the configuration from all files that end with '.json'
  oc apply -f '*.json'

  # Note: --prune is still in Alpha
  # Apply the configuration in manifest.yaml that matches label app=nginx and delete all other resources that are not in the file and match label app=nginx
  oc apply --prune -f manifest.yaml -l app=nginx

  # Apply the configuration in manifest.yaml and delete all the other config maps that are not in the file
  oc apply --prune -f manifest.yaml --all --prune-allowlist=core/v1/ConfigMap</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-apply-edit-last-applied">
<title>oc apply edit-last-applied</title>
<simpara>Edit latest last-applied-configuration annotations of a resource/object</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Edit the last-applied-configuration annotations by type/name in YAML
  oc apply edit-last-applied deployment/nginx

  # Edit the last-applied-configuration annotations by file in JSON
  oc apply edit-last-applied -f deploy.yaml -o json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-apply-set-last-applied">
<title>oc apply set-last-applied</title>
<simpara>Set the last-applied-configuration annotation on a live object to match the contents of a file</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set the last-applied-configuration of a resource to match the contents of a file
  oc apply set-last-applied -f deploy.yaml

  # Execute set-last-applied against each configuration file in a directory
  oc apply set-last-applied -f path/

  # Set the last-applied-configuration of a resource to match the contents of a file; will create the annotation if it does not already exist
  oc apply set-last-applied -f deploy.yaml --create-annotation=true</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-apply-view-last-applied">
<title>oc apply view-last-applied</title>
<simpara>View the latest last-applied-configuration annotations of a resource/object</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # View the last-applied-configuration annotations by type/name in YAML
  oc apply view-last-applied deployment/nginx

  # View the last-applied-configuration annotations by file in JSON
  oc apply view-last-applied -f deploy.yaml -o json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-attach">
<title>oc attach</title>
<simpara>Attach to a running container</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Get output from running pod mypod; use the 'oc.kubernetes.io/default-container' annotation
  # for selecting the container to be attached or the first container in the pod will be chosen
  oc attach mypod

  # Get output from ruby-container from pod mypod
  oc attach mypod -c ruby-container

  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  oc attach mypod -c ruby-container -i -t

  # Get output from the first pod of a replica set named nginx
  oc attach rs/nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-auth-can-i">
<title>oc auth can-i</title>
<simpara>Check whether an action is allowed</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Check to see if I can create pods in any namespace
  oc auth can-i create pods --all-namespaces

  # Check to see if I can list deployments in my current namespace
  oc auth can-i list deployments.apps

  # Check to see if service account "foo" of namespace "dev" can list pods
  # in the namespace "prod".
  # You must be allowed to use impersonation for the global option "--as".
  oc auth can-i list pods --as=system:serviceaccount:dev:foo -n prod

  # Check to see if I can do everything in my current namespace ("*" means all)
  oc auth can-i '*' '*'

  # Check to see if I can get the job named "bar" in namespace "foo"
  oc auth can-i list jobs.batch/bar -n foo

  # Check to see if I can read pod logs
  oc auth can-i get pods --subresource=log

  # Check to see if I can access the URL /logs/
  oc auth can-i get /logs/

  # List all allowed actions in namespace "foo"
  oc auth can-i --list --namespace=foo</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-auth-reconcile">
<title>oc auth reconcile</title>
<simpara>Reconciles rules for RBAC role, role binding, cluster role, and cluster role binding objects</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Reconcile RBAC resources from a file
  oc auth reconcile -f my-rbac-rules.yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-auth-whoami">
<title>oc auth whoami</title>
<simpara>Experimental: Check self subject attributes</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Get your subject attributes.
  oc auth whoami

  # Get your subject attributes in JSON format.
  oc auth whoami -o json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-autoscale">
<title>oc autoscale</title>
<simpara>Autoscale a deployment config, deployment, replica set, stateful set, or replication controller</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Auto scale a deployment "foo", with the number of pods between 2 and 10, no target CPU utilization specified so a default autoscaling policy will be used
  oc autoscale deployment foo --min=2 --max=10

  # Auto scale a replication controller "foo", with the number of pods between 1 and 5, target CPU utilization at 80%
  oc autoscale rc foo --max=5 --cpu-percent=80</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-cancel-build">
<title>oc cancel-build</title>
<simpara>Cancel running, pending, or new builds</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Cancel the build with the given name
  oc cancel-build ruby-build-2

  # Cancel the named build and print the build logs
  oc cancel-build ruby-build-2 --dump-logs

  # Cancel the named build and create a new one with the same parameters
  oc cancel-build ruby-build-2 --restart

  # Cancel multiple builds
  oc cancel-build ruby-build-1 ruby-build-2 ruby-build-3

  # Cancel all builds created from the 'ruby-build' build config that are in the 'new' state
  oc cancel-build bc/ruby-build --state=new</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-cluster-info">
<title>oc cluster-info</title>
<simpara>Display cluster information</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print the address of the control plane and cluster services
  oc cluster-info</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-cluster-info-dump">
<title>oc cluster-info dump</title>
<simpara>Dump relevant information for debugging and diagnosis</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Dump current cluster state to stdout
  oc cluster-info dump

  # Dump current cluster state to /path/to/cluster-state
  oc cluster-info dump --output-directory=/path/to/cluster-state

  # Dump all namespaces to stdout
  oc cluster-info dump --all-namespaces

  # Dump a set of namespaces to /path/to/cluster-state
  oc cluster-info dump --namespaces default,kube-system --output-directory=/path/to/cluster-state</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-completion">
<title>oc completion</title>
<simpara>Output shell completion code for the specified shell (bash, zsh, fish, or powershell)</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Installing bash completion on macOS using homebrew
  ## If running Bash 3.2 included with macOS
  brew install bash-completion
  ## or, if running Bash 4.1+
  brew install bash-completion@2
  ## If oc is installed via homebrew, this should start working immediately
  ## If you've installed via other means, you may need add the completion to your completion directory
  oc completion bash &gt; $(brew --prefix)/etc/bash_completion.d/oc


  # Installing bash completion on Linux
  ## If bash-completion is not installed on Linux, install the 'bash-completion' package
  ## via your distribution's package manager.
  ## Load the oc completion code for bash into the current shell
  source &lt;(oc completion bash)
  ## Write bash completion code to a file and source it from .bash_profile
  oc completion bash &gt; ~/.kube/completion.bash.inc
  printf "
  # oc shell completion
  source '$HOME/.kube/completion.bash.inc'
  " &gt;&gt; $HOME/.bash_profile
  source $HOME/.bash_profile

  # Load the oc completion code for zsh[1] into the current shell
  source &lt;(oc completion zsh)
  # Set the oc completion code for zsh[1] to autoload on startup
  oc completion zsh &gt; "${fpath[1]}/_oc"


  # Load the oc completion code for fish[2] into the current shell
  oc completion fish | source
  # To load completions for each session, execute once:
  oc completion fish &gt; ~/.config/fish/completions/oc.fish

  # Load the oc completion code for powershell into the current shell
  oc completion powershell | Out-String | Invoke-Expression
  # Set oc completion code for powershell to run on startup
  ## Save completion code to a script and execute in the profile
  oc completion powershell &gt; $HOME\.kube\completion.ps1
  Add-Content $PROFILE "$HOME\.kube\completion.ps1"
  ## Execute completion code in the profile
  Add-Content $PROFILE "if (Get-Command oc -ErrorAction SilentlyContinue) {
  oc completion powershell | Out-String | Invoke-Expression
  }"
  ## Add completion code directly to the $PROFILE script
  oc completion powershell &gt;&gt; $PROFILE</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-current-context">
<title>oc config current-context</title>
<simpara>Display the current-context</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Display the current-context
  oc config current-context</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-delete-cluster">
<title>oc config delete-cluster</title>
<simpara>Delete the specified cluster from the kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Delete the minikube cluster
  oc config delete-cluster minikube</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-delete-context">
<title>oc config delete-context</title>
<simpara>Delete the specified context from the kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Delete the context for the minikube cluster
  oc config delete-context minikube</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-delete-user">
<title>oc config delete-user</title>
<simpara>Delete the specified user from the kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Delete the minikube user
  oc config delete-user minikube</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-get-clusters">
<title>oc config get-clusters</title>
<simpara>Display clusters defined in the kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List the clusters that oc knows about
  oc config get-clusters</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-get-contexts">
<title>oc config get-contexts</title>
<simpara>Describe one or many contexts</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List all the contexts in your kubeconfig file
  oc config get-contexts

  # Describe one context in your kubeconfig file
  oc config get-contexts my-context</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-get-users">
<title>oc config get-users</title>
<simpara>Display users defined in the kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List the users that oc knows about
  oc config get-users</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-new-admin-kubeconfig">
<title>oc config new-admin-kubeconfig</title>
<simpara>Generate, make the server trust, and display a new admin.kubeconfig.</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Generate a new admin kubeconfig
  oc config new-admin-kubeconfig</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-new-kubelet-bootstrap-kubeconfig">
<title>oc config new-kubelet-bootstrap-kubeconfig</title>
<simpara>Generate, make the server trust, and display a new kubelet /etc/kubernetes/kubeconfig.</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Generate a new kubelet bootstrap kubeconfig
  oc config new-kubelet-bootstrap-kubeconfig</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-refresh-ca-bundle">
<title>oc config refresh-ca-bundle</title>
<simpara>Update the OpenShift CA bundle by contacting the apiserver.</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Refresh the CA bundle for the current context's cluster
  oc config refresh-ca-bundle

  # Refresh the CA bundle for the cluster named e2e in your kubeconfig
  oc config refresh-ca-bundle e2e

  # Print the CA bundle from the current OpenShift cluster's apiserver.
  oc config refresh-ca-bundle --dry-run</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-rename-context">
<title>oc config rename-context</title>
<simpara>Rename a context from the kubeconfig file</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Rename the context 'old-name' to 'new-name' in your kubeconfig file
  oc config rename-context old-name new-name</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-set">
<title>oc config set</title>
<simpara>Set an individual value in a kubeconfig file</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set the server field on the my-cluster cluster to https://1.2.3.4
  oc config set clusters.my-cluster.server https://1.2.3.4

  # Set the certificate-authority-data field on the my-cluster cluster
  oc config set clusters.my-cluster.certificate-authority-data $(echo "cert_data_here" | base64 -i -)

  # Set the cluster field in the my-context context to my-cluster
  oc config set contexts.my-context.cluster my-cluster

  # Set the client-key-data field in the cluster-admin user using --set-raw-bytes option
  oc config set users.cluster-admin.client-key-data cert_data_here --set-raw-bytes=true</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-set-cluster">
<title>oc config set-cluster</title>
<simpara>Set a cluster entry in kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set only the server field on the e2e cluster entry without touching other values
  oc config set-cluster e2e --server=https://1.2.3.4

  # Embed certificate authority data for the e2e cluster entry
  oc config set-cluster e2e --embed-certs --certificate-authority=~/.kube/e2e/kubernetes.ca.crt

  # Disable cert checking for the e2e cluster entry
  oc config set-cluster e2e --insecure-skip-tls-verify=true

  # Set the custom TLS server name to use for validation for the e2e cluster entry
  oc config set-cluster e2e --tls-server-name=my-cluster-name

  # Set the proxy URL for the e2e cluster entry
  oc config set-cluster e2e --proxy-url=https://1.2.3.4</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-set-context">
<title>oc config set-context</title>
<simpara>Set a context entry in kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set the user field on the gce context entry without touching other values
  oc config set-context gce --user=cluster-admin</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-set-credentials">
<title>oc config set-credentials</title>
<simpara>Set a user entry in kubeconfig</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set only the "client-key" field on the "cluster-admin"
  # entry, without touching other values
  oc config set-credentials cluster-admin --client-key=~/.kube/admin.key

  # Set basic auth for the "cluster-admin" entry
  oc config set-credentials cluster-admin --username=admin --password=uXFGweU9l35qcif

  # Embed client certificate data in the "cluster-admin" entry
  oc config set-credentials cluster-admin --client-certificate=~/.kube/admin.crt --embed-certs=true

  # Enable the Google Compute Platform auth provider for the "cluster-admin" entry
  oc config set-credentials cluster-admin --auth-provider=gcp

  # Enable the OpenID Connect auth provider for the "cluster-admin" entry with additional arguments
  oc config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-id=foo --auth-provider-arg=client-secret=bar

  # Remove the "client-secret" config value for the OpenID Connect auth provider for the "cluster-admin" entry
  oc config set-credentials cluster-admin --auth-provider=oidc --auth-provider-arg=client-secret-

  # Enable new exec auth plugin for the "cluster-admin" entry
  oc config set-credentials cluster-admin --exec-command=/path/to/the/executable --exec-api-version=client.authentication.k8s.io/v1beta1

  # Define new exec auth plugin arguments for the "cluster-admin" entry
  oc config set-credentials cluster-admin --exec-arg=arg1 --exec-arg=arg2

  # Create or update exec auth plugin environment variables for the "cluster-admin" entry
  oc config set-credentials cluster-admin --exec-env=key1=val1 --exec-env=key2=val2

  # Remove exec auth plugin environment variables for the "cluster-admin" entry
  oc config set-credentials cluster-admin --exec-env=var-to-remove-</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-unset">
<title>oc config unset</title>
<simpara>Unset an individual value in a kubeconfig file</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Unset the current-context
  oc config unset current-context

  # Unset namespace in foo context
  oc config unset contexts.foo.namespace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-use-context">
<title>oc config use-context</title>
<simpara>Set the current-context in a kubeconfig file</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Use the context for the minikube cluster
  oc config use-context minikube</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-config-view">
<title>oc config view</title>
<simpara>Display merged kubeconfig settings or a specified kubeconfig file</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show merged kubeconfig settings
  oc config view

  # Show merged kubeconfig settings, raw certificate data, and exposed secrets
  oc config view --raw

  # Get the password for the e2e user
  oc config view -o jsonpath='{.users[?(@.name == "e2e")].user.password}'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-cp">
<title>oc cp</title>
<simpara>Copy files and directories to and from containers</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # !!!Important Note!!!
  # Requires that the 'tar' binary is present in your container
  # image.  If 'tar' is not present, 'oc cp' will fail.
  #
  # For advanced use cases, such as symlinks, wildcard expansion or
  # file mode preservation, consider using 'oc exec'.

  # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace &lt;some-namespace&gt;
  tar cf - /tmp/foo | oc exec -i -n &lt;some-namespace&gt; &lt;some-pod&gt; -- tar xf - -C /tmp/bar

  # Copy /tmp/foo from a remote pod to /tmp/bar locally
  oc exec -n &lt;some-namespace&gt; &lt;some-pod&gt; -- tar cf - /tmp/foo | tar xf - -C /tmp/bar

  # Copy /tmp/foo_dir local directory to /tmp/bar_dir in a remote pod in the default namespace
  oc cp /tmp/foo_dir &lt;some-pod&gt;:/tmp/bar_dir

  # Copy /tmp/foo local file to /tmp/bar in a remote pod in a specific container
  oc cp /tmp/foo &lt;some-pod&gt;:/tmp/bar -c &lt;specific-container&gt;

  # Copy /tmp/foo local file to /tmp/bar in a remote pod in namespace &lt;some-namespace&gt;
  oc cp /tmp/foo &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/bar

  # Copy /tmp/foo from a remote pod to /tmp/bar locally
  oc cp &lt;some-namespace&gt;/&lt;some-pod&gt;:/tmp/foo /tmp/bar</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create">
<title>oc create</title>
<simpara>Create a resource from a file or from stdin</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a pod using the data in pod.json
  oc create -f ./pod.json

  # Create a pod based on the JSON passed into stdin
  cat pod.json | oc create -f -

  # Edit the data in registry.yaml in JSON then create the resource using the edited data
  oc create -f registry.yaml --edit -o json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-build">
<title>oc create build</title>
<simpara>Create a new build</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new build
  oc create build myapp</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-clusterresourcequota">
<title>oc create clusterresourcequota</title>
<simpara>Create a cluster resource quota</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a cluster resource quota limited to 10 pods
  oc create clusterresourcequota limit-bob --project-annotation-selector=openshift.io/requester=user-bob --hard=pods=10</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-clusterrole">
<title>oc create clusterrole</title>
<simpara>Create a cluster role</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a cluster role named "pod-reader" that allows user to perform "get", "watch" and "list" on pods
  oc create clusterrole pod-reader --verb=get,list,watch --resource=pods

  # Create a cluster role named "pod-reader" with ResourceName specified
  oc create clusterrole pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod

  # Create a cluster role named "foo" with API Group specified
  oc create clusterrole foo --verb=get,list,watch --resource=rs.apps

  # Create a cluster role named "foo" with SubResource specified
  oc create clusterrole foo --verb=get,list,watch --resource=pods,pods/status

  # Create a cluster role name "foo" with NonResourceURL specified
  oc create clusterrole "foo" --verb=get --non-resource-url=/logs/*

  # Create a cluster role name "monitoring" with AggregationRule specified
  oc create clusterrole monitoring --aggregation-rule="rbac.example.com/aggregate-to-monitoring=true"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-clusterrolebinding">
<title>oc create clusterrolebinding</title>
<simpara>Create a cluster role binding for a particular cluster role</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a cluster role binding for user1, user2, and group1 using the cluster-admin cluster role
  oc create clusterrolebinding cluster-admin --clusterrole=cluster-admin --user=user1 --user=user2 --group=group1</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-configmap">
<title>oc create configmap</title>
<simpara>Create a config map from a local file, directory or literal value</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new config map named my-config based on folder bar
  oc create configmap my-config --from-file=path/to/bar

  # Create a new config map named my-config with specified keys instead of file basenames on disk
  oc create configmap my-config --from-file=key1=/path/to/bar/file1.txt --from-file=key2=/path/to/bar/file2.txt

  # Create a new config map named my-config with key1=config1 and key2=config2
  oc create configmap my-config --from-literal=key1=config1 --from-literal=key2=config2

  # Create a new config map named my-config from the key=value pairs in the file
  oc create configmap my-config --from-file=path/to/bar

  # Create a new config map named my-config from an env file
  oc create configmap my-config --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-cronjob">
<title>oc create cronjob</title>
<simpara>Create a cron job with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a cron job
  oc create cronjob my-job --image=busybox --schedule="*/1 * * * *"

  # Create a cron job with a command
  oc create cronjob my-job --image=busybox --schedule="*/1 * * * *" -- date</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-deployment">
<title>oc create deployment</title>
<simpara>Create a deployment with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a deployment named my-dep that runs the busybox image
  oc create deployment my-dep --image=busybox

  # Create a deployment with a command
  oc create deployment my-dep --image=busybox -- date

  # Create a deployment named my-dep that runs the nginx image with 3 replicas
  oc create deployment my-dep --image=nginx --replicas=3

  # Create a deployment named my-dep that runs the busybox image and expose port 5701
  oc create deployment my-dep --image=busybox --port=5701</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-deploymentconfig">
<title>oc create deploymentconfig</title>
<simpara>Create a deployment config with default options that uses a given image</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create an nginx deployment config named my-nginx
  oc create deploymentconfig my-nginx --image=nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-identity">
<title>oc create identity</title>
<simpara>Manually create an identity (only needed if automatic creation is disabled)</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create an identity with identity provider "acme_ldap" and the identity provider username "adamjones"
  oc create identity acme_ldap:adamjones</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-imagestream">
<title>oc create imagestream</title>
<simpara>Create a new empty image stream</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new image stream
  oc create imagestream mysql</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-imagestreamtag">
<title>oc create imagestreamtag</title>
<simpara>Create a new image stream tag</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new image stream tag based on an image in a remote registry
  oc create imagestreamtag mysql:latest --from-image=myregistry.local/mysql/mysql:5.0</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-ingress">
<title>oc create ingress</title>
<simpara>Create an ingress with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a single ingress called 'simple' that directs requests to foo.com/bar to svc
  # svc1:8080 with a TLS secret "my-cert"
  oc create ingress simple --rule="foo.com/bar=svc1:8080,tls=my-cert"

  # Create a catch all ingress of "/path" pointing to service svc:port and Ingress Class as "otheringress"
  oc create ingress catch-all --class=otheringress --rule="/path=svc:port"

  # Create an ingress with two annotations: ingress.annotation1 and ingress.annotations2
  oc create ingress annotated --class=default --rule="foo.com/bar=svc:port" \
  --annotation ingress.annotation1=foo \
  --annotation ingress.annotation2=bla

  # Create an ingress with the same host and multiple paths
  oc create ingress multipath --class=default \
  --rule="foo.com/=svc:port" \
  --rule="foo.com/admin/=svcadmin:portadmin"

  # Create an ingress with multiple hosts and the pathType as Prefix
  oc create ingress ingress1 --class=default \
  --rule="foo.com/path*=svc:8080" \
  --rule="bar.com/admin*=svc2:http"

  # Create an ingress with TLS enabled using the default ingress certificate and different path types
  oc create ingress ingtls --class=default \
  --rule="foo.com/=svc:https,tls" \
  --rule="foo.com/path/subpath*=othersvc:8080"

  # Create an ingress with TLS enabled using a specific secret and pathType as Prefix
  oc create ingress ingsecret --class=default \
  --rule="foo.com/*=svc:8080,tls=secret1"

  # Create an ingress with a default backend
  oc create ingress ingdefault --class=default \
  --default-backend=defaultsvc:http \
  --rule="foo.com/*=svc:8080,tls=secret1"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-job">
<title>oc create job</title>
<simpara>Create a job with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a job
  oc create job my-job --image=busybox

  # Create a job with a command
  oc create job my-job --image=busybox -- date

  # Create a job from a cron job named "a-cronjob"
  oc create job test-job --from=cronjob/a-cronjob</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-namespace">
<title>oc create namespace</title>
<simpara>Create a namespace with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new namespace named my-namespace
  oc create namespace my-namespace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-poddisruptionbudget">
<title>oc create poddisruptionbudget</title>
<simpara>Create a pod disruption budget with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a pod disruption budget named my-pdb that will select all pods with the app=rails label
  # and require at least one of them being available at any point in time
  oc create poddisruptionbudget my-pdb --selector=app=rails --min-available=1

  # Create a pod disruption budget named my-pdb that will select all pods with the app=nginx label
  # and require at least half of the pods selected to be available at any point in time
  oc create pdb my-pdb --selector=app=nginx --min-available=50%</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-priorityclass">
<title>oc create priorityclass</title>
<simpara>Create a priority class with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a priority class named high-priority
  oc create priorityclass high-priority --value=1000 --description="high priority"

  # Create a priority class named default-priority that is considered as the global default priority
  oc create priorityclass default-priority --value=1000 --global-default=true --description="default priority"

  # Create a priority class named high-priority that cannot preempt pods with lower priority
  oc create priorityclass high-priority --value=1000 --description="high priority" --preemption-policy="Never"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-quota">
<title>oc create quota</title>
<simpara>Create a quota with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new resource quota named my-quota
  oc create quota my-quota --hard=cpu=1,memory=1G,pods=2,services=3,replicationcontrollers=2,resourcequotas=1,secrets=5,persistentvolumeclaims=10

  # Create a new resource quota named best-effort
  oc create quota best-effort --hard=pods=100 --scopes=BestEffort</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-role">
<title>oc create role</title>
<simpara>Create a role with single rule</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a role named "pod-reader" that allows user to perform "get", "watch" and "list" on pods
  oc create role pod-reader --verb=get --verb=list --verb=watch --resource=pods

  # Create a role named "pod-reader" with ResourceName specified
  oc create role pod-reader --verb=get --resource=pods --resource-name=readablepod --resource-name=anotherpod

  # Create a role named "foo" with API Group specified
  oc create role foo --verb=get,list,watch --resource=rs.apps

  # Create a role named "foo" with SubResource specified
  oc create role foo --verb=get,list,watch --resource=pods,pods/status</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-rolebinding">
<title>oc create rolebinding</title>
<simpara>Create a role binding for a particular role or cluster role</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a role binding for user1, user2, and group1 using the admin cluster role
  oc create rolebinding admin --clusterrole=admin --user=user1 --user=user2 --group=group1

  # Create a role binding for serviceaccount monitoring:sa-dev using the admin role
  oc create rolebinding admin-binding --role=admin --serviceaccount=monitoring:sa-dev</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-route-edge">
<title>oc create route edge</title>
<simpara>Create a route that uses edge TLS termination</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create an edge route named "my-route" that exposes the frontend service
  oc create route edge my-route --service=frontend

  # Create an edge route that exposes the frontend service and specify a path
  # If the route name is omitted, the service name will be used
  oc create route edge --service=frontend --path /assets</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-route-passthrough">
<title>oc create route passthrough</title>
<simpara>Create a route that uses passthrough TLS termination</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a passthrough route named "my-route" that exposes the frontend service
  oc create route passthrough my-route --service=frontend

  # Create a passthrough route that exposes the frontend service and specify
  # a host name. If the route name is omitted, the service name will be used
  oc create route passthrough --service=frontend --hostname=www.example.com</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-route-reencrypt">
<title>oc create route reencrypt</title>
<simpara>Create a route that uses reencrypt TLS termination</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a route named "my-route" that exposes the frontend service
  oc create route reencrypt my-route --service=frontend --dest-ca-cert cert.cert

  # Create a reencrypt route that exposes the frontend service, letting the
  # route name default to the service name and the destination CA certificate
  # default to the service CA
  oc create route reencrypt --service=frontend</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-secret-docker-registry">
<title>oc create secret docker-registry</title>
<simpara>Create a secret for use with a Docker registry</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # If you do not already have a .dockercfg file, create a dockercfg secret directly
  oc create secret docker-registry my-secret --docker-server=DOCKER_REGISTRY_SERVER --docker-username=DOCKER_USER --docker-password=DOCKER_PASSWORD --docker-email=DOCKER_EMAIL

  # Create a new secret named my-secret from ~/.docker/config.json
  oc create secret docker-registry my-secret --from-file=.dockerconfigjson=path/to/.docker/config.json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-secret-generic">
<title>oc create secret generic</title>
<simpara>Create a secret from a local file, directory, or literal value</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new secret named my-secret with keys for each file in folder bar
  oc create secret generic my-secret --from-file=path/to/bar

  # Create a new secret named my-secret with specified keys instead of names on disk
  oc create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-file=ssh-publickey=path/to/id_rsa.pub

  # Create a new secret named my-secret with key1=supersecret and key2=topsecret
  oc create secret generic my-secret --from-literal=key1=supersecret --from-literal=key2=topsecret

  # Create a new secret named my-secret using a combination of a file and a literal
  oc create secret generic my-secret --from-file=ssh-privatekey=path/to/id_rsa --from-literal=passphrase=topsecret

  # Create a new secret named my-secret from env files
  oc create secret generic my-secret --from-env-file=path/to/foo.env --from-env-file=path/to/bar.env</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-secret-tls">
<title>oc create secret tls</title>
<simpara>Create a TLS secret</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new TLS secret named tls-secret with the given key pair
  oc create secret tls tls-secret --cert=path/to/tls.cert --key=path/to/tls.key</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-service-clusterip">
<title>oc create service clusterip</title>
<simpara>Create a ClusterIP service</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new ClusterIP service named my-cs
  oc create service clusterip my-cs --tcp=5678:8080

  # Create a new ClusterIP service named my-cs (in headless mode)
  oc create service clusterip my-cs --clusterip="None"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-service-externalname">
<title>oc create service externalname</title>
<simpara>Create an ExternalName service</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new ExternalName service named my-ns
  oc create service externalname my-ns --external-name bar.com</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-service-loadbalancer">
<title>oc create service loadbalancer</title>
<simpara>Create a LoadBalancer service</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new LoadBalancer service named my-lbs
  oc create service loadbalancer my-lbs --tcp=5678:8080</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-service-nodeport">
<title>oc create service nodeport</title>
<simpara>Create a NodePort service</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new NodePort service named my-ns
  oc create service nodeport my-ns --tcp=5678:8080</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-serviceaccount">
<title>oc create serviceaccount</title>
<simpara>Create a service account with the specified name</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new service account named my-service-account
  oc create serviceaccount my-service-account</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-token">
<title>oc create token</title>
<simpara>Request a service account token</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Request a token to authenticate to the kube-apiserver as the service account "myapp" in the current namespace
  oc create token myapp

  # Request a token for a service account in a custom namespace
  oc create token myapp --namespace myns

  # Request a token with a custom expiration
  oc create token myapp --duration 10m

  # Request a token with a custom audience
  oc create token myapp --audience https://example.com

  # Request a token bound to an instance of a Secret object
  oc create token myapp --bound-object-kind Secret --bound-object-name mysecret

  # Request a token bound to an instance of a Secret object with a specific UID
  oc create token myapp --bound-object-kind Secret --bound-object-name mysecret --bound-object-uid 0d4691ed-659b-4935-a832-355f77ee47cc</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-user">
<title>oc create user</title>
<simpara>Manually create a user (only needed if automatic creation is disabled)</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a user with the username "ajones" and the display name "Adam Jones"
  oc create user ajones --full-name="Adam Jones"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-create-useridentitymapping">
<title>oc create useridentitymapping</title>
<simpara>Manually map an identity to a user</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Map the identity "acme_ldap:adamjones" to the user "ajones"
  oc create useridentitymapping acme_ldap:adamjones ajones</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-debug">
<title>oc debug</title>
<simpara>Launch a new instance of a pod for debugging</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Start a shell session into a pod using the OpenShift tools image
  oc debug

  # Debug a currently running deployment by creating a new pod
  oc debug deploy/test

  # Debug a node as an administrator
  oc debug node/master-1

  # Debug a Windows Node
  # Note: the chosen image must match the Windows Server version (2019, 2022) of the Node
  oc debug node/win-worker-1 --image=mcr.microsoft.com/powershell:lts-nanoserver-ltsc2022

  # Launch a shell in a pod using the provided image stream tag
  oc debug istag/mysql:latest -n openshift

  # Test running a job as a non-root user
  oc debug job/test --as-user=1000000

  # Debug a specific failing container by running the env command in the 'second' container
  oc debug daemonset/test -c second -- /bin/env

  # See the pod that would be created to debug
  oc debug mypod-9xbc -o yaml

  # Debug a resource but launch the debug pod in another namespace
  # Note: Not all resources can be debugged using --to-namespace without modification. For example,
  # volumes and service accounts are namespace-dependent. Add '-o yaml' to output the debug pod definition
  # to disk.  If necessary, edit the definition then run 'oc debug -f -' or run without --to-namespace
  oc debug mypod-9xbc --to-namespace testns</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-delete">
<title>oc delete</title>
<simpara>Delete resources by file names, stdin, resources and names, or by resources and label selector</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Delete a pod using the type and name specified in pod.json
  oc delete -f ./pod.json

  # Delete resources from a directory containing kustomization.yaml - e.g. dir/kustomization.yaml
  oc delete -k dir

  # Delete resources from all files that end with '.json'
  oc delete -f '*.json'

  # Delete a pod based on the type and name in the JSON passed into stdin
  cat pod.json | oc delete -f -

  # Delete pods and services with same names "baz" and "foo"
  oc delete pod,service baz foo

  # Delete pods and services with label name=myLabel
  oc delete pods,services -l name=myLabel

  # Delete a pod with minimal delay
  oc delete pod foo --now

  # Force delete a pod on a dead node
  oc delete pod foo --force

  # Delete all pods
  oc delete pods --all</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-describe">
<title>oc describe</title>
<simpara>Show details of a specific resource or group of resources</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Describe a node
  oc describe nodes kubernetes-node-emt8.c.myproject.internal

  # Describe a pod
  oc describe pods/nginx

  # Describe a pod identified by type and name in "pod.json"
  oc describe -f pod.json

  # Describe all pods
  oc describe pods

  # Describe pods by label name=myLabel
  oc describe pods -l name=myLabel

  # Describe all pods managed by the 'frontend' replication controller
  # (rc-created pods get the name of the rc as a prefix in the pod name)
  oc describe pods frontend</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-diff">
<title>oc diff</title>
<simpara>Diff the live version against a would-be applied version</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Diff resources included in pod.json
  oc diff -f pod.json

  # Diff file read from stdin
  cat service.yaml | oc diff -f -</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-edit">
<title>oc edit</title>
<simpara>Edit a resource on the server</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Edit the service named 'registry'
  oc edit svc/registry

  # Use an alternative editor
  KUBE_EDITOR="nano" oc edit svc/registry

  # Edit the job 'myjob' in JSON using the v1 API format
  oc edit job.v1.batch/myjob -o json

  # Edit the deployment 'mydeployment' in YAML and save the modified config in its annotation
  oc edit deployment/mydeployment -o yaml --save-config

  # Edit the 'status' subresource for the 'mydeployment' deployment
  oc edit deployment mydeployment --subresource='status'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-events">
<title>oc events</title>
<simpara>List events</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List recent events in the default namespace
  oc events

  # List recent events in all namespaces
  oc events --all-namespaces

  # List recent events for the specified pod, then wait for more events and list them as they arrive
  oc events --for pod/web-pod-13je7 --watch

  # List recent events in YAML format
  oc events -oyaml

  # List recent only events of type 'Warning' or 'Normal'
  oc events --types=Warning,Normal</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-exec">
<title>oc exec</title>
<simpara>Execute a command in a container</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Get output from running the 'date' command from pod mypod, using the first container by default
  oc exec mypod -- date

  # Get output from running the 'date' command in ruby-container from pod mypod
  oc exec mypod -c ruby-container -- date

  # Switch to raw terminal mode; sends stdin to 'bash' in ruby-container from pod mypod
  # and sends stdout/stderr from 'bash' back to the client
  oc exec mypod -c ruby-container -i -t -- bash -il

  # List contents of /usr from the first container of pod mypod and sort by modification time
  # If the command you want to execute in the pod has any flags in common (e.g. -i),
  # you must use two dashes (--) to separate your command's flags/arguments
  # Also note, do not surround your command and its flags/arguments with quotes
  # unless that is how you would execute it normally (i.e., do ls -t /usr, not "ls -t /usr")
  oc exec mypod -i -t -- ls -t /usr

  # Get output from running 'date' command from the first pod of the deployment mydeployment, using the first container by default
  oc exec deploy/mydeployment -- date

  # Get output from running 'date' command from the first pod of the service myservice, using the first container by default
  oc exec svc/myservice -- date</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-explain">
<title>oc explain</title>
<simpara>Get documentation for a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Get the documentation of the resource and its fields
  oc explain pods

  # Get all the fields in the resource
  oc explain pods --recursive

  # Get the explanation for deployment in supported api versions
  oc explain deployments --api-version=apps/v1

  # Get the documentation of a specific field of a resource
  oc explain pods.spec.containers

  # Get the documentation of resources in different format
  oc explain deployment --output=plaintext-openapiv2</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-expose">
<title>oc expose</title>
<simpara>Expose a replicated application as a service or route</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a route based on service nginx. The new route will reuse nginx's labels
  oc expose service nginx

  # Create a route and specify your own label and route name
  oc expose service nginx -l name=myroute --name=fromdowntown

  # Create a route and specify a host name
  oc expose service nginx --hostname=www.example.com

  # Create a route with a wildcard
  oc expose service nginx --hostname=x.example.com --wildcard-policy=Subdomain
  # This would be equivalent to *.example.com. NOTE: only hosts are matched by the wildcard; subdomains would not be included

  # Expose a deployment configuration as a service and use the specified port
  oc expose dc ruby-hello-world --port=8080

  # Expose a service as a route in the specified path
  oc expose service nginx --path=/nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-extract">
<title>oc extract</title>
<simpara>Extract secrets or config maps to disk</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Extract the secret "test" to the current directory
  oc extract secret/test

  # Extract the config map "nginx" to the /tmp directory
  oc extract configmap/nginx --to=/tmp

  # Extract the config map "nginx" to STDOUT
  oc extract configmap/nginx --to=-

  # Extract only the key "nginx.conf" from config map "nginx" to the /tmp directory
  oc extract configmap/nginx --to=/tmp --keys=nginx.conf</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-get">
<title>oc get</title>
<simpara>Display one or many resources</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List all pods in ps output format
  oc get pods

  # List all pods in ps output format with more information (such as node name)
  oc get pods -o wide

  # List a single replication controller with specified NAME in ps output format
  oc get replicationcontroller web

  # List deployments in JSON output format, in the "v1" version of the "apps" API group
  oc get deployments.v1.apps -o json

  # List a single pod in JSON output format
  oc get -o json pod web-pod-13je7

  # List a pod identified by type and name specified in "pod.yaml" in JSON output format
  oc get -f pod.yaml -o json

  # List resources from a directory with kustomization.yaml - e.g. dir/kustomization.yaml
  oc get -k dir/

  # Return only the phase value of the specified pod
  oc get -o template pod/web-pod-13je7 --template={{.status.phase}}

  # List resource information in custom columns
  oc get pod test-pod -o custom-columns=CONTAINER:.spec.containers[0].name,IMAGE:.spec.containers[0].image

  # List all replication controllers and services together in ps output format
  oc get rc,services

  # List one or more resources by their type and names
  oc get rc/web service/frontend pods/web-pod-13je7

  # List the 'status' subresource for a single pod
  oc get pod web-pod-13je7 --subresource status</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-get-token">
<title>oc get-token</title>
<simpara>Experimental: Get token from external OIDC issuer as credentials exec plugin</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Starts an auth code flow to the issuer url with the client id and the given extra scopes
  oc get-token --client-id=client-id --issuer-url=test.issuer.url --extra-scopes=email,profile

  # Starts an authe code flow to the issuer url with a different callback address.
  oc get-token --client-id=client-id --issuer-url=test.issuer.url --callback-address=127.0.0.1:8343</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-idle">
<title>oc idle</title>
<simpara>Idle scalable resources</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Idle the scalable controllers associated with the services listed in to-idle.txt
  $ oc idle --resource-names-file to-idle.txt</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-image-append">
<title>oc image append</title>
<simpara>Add layers to images and push them to a registry</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Remove the entrypoint on the mysql:latest image
  oc image append --from mysql:latest --to myregistry.com/myimage:latest --image '{"Entrypoint":null}'

  # Add a new layer to the image
  oc image append --from mysql:latest --to myregistry.com/myimage:latest layer.tar.gz

  # Add a new layer to the image and store the result on disk
  # This results in $(pwd)/v2/mysql/blobs,manifests
  oc image append --from mysql:latest --to file://mysql:local layer.tar.gz

  # Add a new layer to the image and store the result on disk in a designated directory
  # This will result in $(pwd)/mysql-local/v2/mysql/blobs,manifests
  oc image append --from mysql:latest --to file://mysql:local --dir mysql-local layer.tar.gz

  # Add a new layer to an image that is stored on disk (~/mysql-local/v2/image exists)
  oc image append --from-dir ~/mysql-local --to myregistry.com/myimage:latest layer.tar.gz

  # Add a new layer to an image that was mirrored to the current directory on disk ($(pwd)/v2/image exists)
  oc image append --from-dir v2 --to myregistry.com/myimage:latest layer.tar.gz

  # Add a new layer to a multi-architecture image for an os/arch that is different from the system's os/arch
  # Note: The first image in the manifest list that matches the filter will be returned when --keep-manifest-list is not specified
  oc image append --from docker.io/library/busybox:latest --filter-by-os=linux/s390x --to myregistry.com/myimage:latest layer.tar.gz

  # Add a new layer to a multi-architecture image for all the os/arch manifests when keep-manifest-list is specified
  oc image append --from docker.io/library/busybox:latest --keep-manifest-list --to myregistry.com/myimage:latest layer.tar.gz

  # Add a new layer to a multi-architecture image for all the os/arch manifests that is specified by the filter, while preserving the manifestlist
  oc image append --from docker.io/library/busybox:latest --filter-by-os=linux/s390x --keep-manifest-list --to myregistry.com/myimage:latest layer.tar.gz</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-image-extract">
<title>oc image extract</title>
<simpara>Copy files from an image to the file system</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Extract the busybox image into the current directory
  oc image extract docker.io/library/busybox:latest

  # Extract the busybox image into a designated directory (must exist)
  oc image extract docker.io/library/busybox:latest --path /:/tmp/busybox

  # Extract the busybox image into the current directory for linux/s390x platform
  # Note: Wildcard filter is not supported with extract; pass a single os/arch to extract
  oc image extract docker.io/library/busybox:latest --filter-by-os=linux/s390x

  # Extract a single file from the image into the current directory
  oc image extract docker.io/library/centos:7 --path /bin/bash:.

  # Extract all .repo files from the image's /etc/yum.repos.d/ folder into the current directory
  oc image extract docker.io/library/centos:7 --path /etc/yum.repos.d/*.repo:.

  # Extract all .repo files from the image's /etc/yum.repos.d/ folder into a designated directory (must exist)
  # This results in /tmp/yum.repos.d/*.repo on local system
  oc image extract docker.io/library/centos:7 --path /etc/yum.repos.d/*.repo:/tmp/yum.repos.d

  # Extract an image stored on disk into the current directory ($(pwd)/v2/busybox/blobs,manifests exists)
  # --confirm is required because the current directory is not empty
  oc image extract file://busybox:local --confirm

  # Extract an image stored on disk in a directory other than $(pwd)/v2 into the current directory
  # --confirm is required because the current directory is not empty ($(pwd)/busybox-mirror-dir/v2/busybox exists)
  oc image extract file://busybox:local --dir busybox-mirror-dir --confirm

  # Extract an image stored on disk in a directory other than $(pwd)/v2 into a designated directory (must exist)
  oc image extract file://busybox:local --dir busybox-mirror-dir --path /:/tmp/busybox

  # Extract the last layer in the image
  oc image extract docker.io/library/centos:7[-1]

  # Extract the first three layers of the image
  oc image extract docker.io/library/centos:7[:3]

  # Extract the last three layers of the image
  oc image extract docker.io/library/centos:7[-3:]</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-image-info">
<title>oc image info</title>
<simpara>Display information about an image</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show information about an image
  oc image info quay.io/openshift/cli:latest

  # Show information about images matching a wildcard
  oc image info quay.io/openshift/cli:4.*

  # Show information about a file mirrored to disk under DIR
  oc image info --dir=DIR file://library/busybox:latest

  # Select which image from a multi-OS image to show
  oc image info library/busybox:latest --filter-by-os=linux/arm64</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-image-mirror">
<title>oc image mirror</title>
<simpara>Mirror images from one repository to another</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Copy image to another tag
  oc image mirror myregistry.com/myimage:latest myregistry.com/myimage:stable

  # Copy image to another registry
  oc image mirror myregistry.com/myimage:latest docker.io/myrepository/myimage:stable

  # Copy all tags starting with mysql to the destination repository
  oc image mirror myregistry.com/myimage:mysql* docker.io/myrepository/myimage

  # Copy image to disk, creating a directory structure that can be served as a registry
  oc image mirror myregistry.com/myimage:latest file://myrepository/myimage:latest

  # Copy image to S3 (pull from &lt;bucket&gt;.s3.amazonaws.com/image:latest)
  oc image mirror myregistry.com/myimage:latest s3://s3.amazonaws.com/&lt;region&gt;/&lt;bucket&gt;/image:latest

  # Copy image to S3 without setting a tag (pull via @&lt;digest&gt;)
  oc image mirror myregistry.com/myimage:latest s3://s3.amazonaws.com/&lt;region&gt;/&lt;bucket&gt;/image

  # Copy image to multiple locations
  oc image mirror myregistry.com/myimage:latest docker.io/myrepository/myimage:stable \
  docker.io/myrepository/myimage:dev

  # Copy multiple images
  oc image mirror myregistry.com/myimage:latest=myregistry.com/other:test \
  myregistry.com/myimage:new=myregistry.com/other:target

  # Copy manifest list of a multi-architecture image, even if only a single image is found
  oc image mirror myregistry.com/myimage:latest=myregistry.com/other:test \
  --keep-manifest-list=true

  # Copy specific os/arch manifest of a multi-architecture image
  # Run 'oc image info myregistry.com/myimage:latest' to see available os/arch for multi-arch images
  # Note that with multi-arch images, this results in a new manifest list digest that includes only
  # the filtered manifests
  oc image mirror myregistry.com/myimage:latest=myregistry.com/other:test \
  --filter-by-os=os/arch

  # Copy all os/arch manifests of a multi-architecture image
  # Run 'oc image info myregistry.com/myimage:latest' to see list of os/arch manifests that will be mirrored
  oc image mirror myregistry.com/myimage:latest=myregistry.com/other:test \
  --keep-manifest-list=true

  # Note the above command is equivalent to
  oc image mirror myregistry.com/myimage:latest=myregistry.com/other:test \
  --filter-by-os=.*

  # Copy specific os/arch manifest of a multi-architecture image
  # Run 'oc image info myregistry.com/myimage:latest' to see available os/arch for multi-arch images
  # Note that the target registry may reject a manifest list if the platform specific images do not all
  # exist. You must use a registry with sparse registry support enabled.
  oc image mirror myregistry.com/myimage:latest=myregistry.com/other:test \
  --filter-by-os=linux/386 \
  --keep-manifest-list=true</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-import-image">
<title>oc import-image</title>
<simpara>Import images from a container image registry</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Import tag latest into a new image stream
  oc import-image mystream --from=registry.io/repo/image:latest --confirm

  # Update imported data for tag latest in an already existing image stream
  oc import-image mystream

  # Update imported data for tag stable in an already existing image stream
  oc import-image mystream:stable

  # Update imported data for all tags in an existing image stream
  oc import-image mystream --all

  # Update imported data for a tag that points to a manifest list to include the full manifest list
  oc import-image mystream --import-mode=PreserveOriginal

  # Import all tags into a new image stream
  oc import-image mystream --from=registry.io/repo/image --all --confirm

  # Import all tags into a new image stream using a custom timeout
  oc --request-timeout=5m import-image mystream --from=registry.io/repo/image --all --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-kustomize">
<title>oc kustomize</title>
<simpara>Build a kustomization target from a directory or URL</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Build the current working directory
  oc kustomize

  # Build some shared configuration directory
  oc kustomize /home/config/production

  # Build from github
  oc kustomize https://github.com/kubernetes-sigs/kustomize.git/examples/helloWorld?ref=v1.0.6</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-label">
<title>oc label</title>
<simpara>Update the labels on a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Update pod 'foo' with the label 'unhealthy' and the value 'true'
  oc label pods foo unhealthy=true

  # Update pod 'foo' with the label 'status' and the value 'unhealthy', overwriting any existing value
  oc label --overwrite pods foo status=unhealthy

  # Update all pods in the namespace
  oc label pods --all status=unhealthy

  # Update a pod identified by the type and name in "pod.json"
  oc label -f pod.json status=unhealthy

  # Update pod 'foo' only if the resource is unchanged from version 1
  oc label pods foo status=unhealthy --resource-version=1

  # Update pod 'foo' by removing a label named 'bar' if it exists
  # Does not require the --overwrite flag
  oc label pods foo bar-</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-login">
<title>oc login</title>
<simpara>Log in to a server</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Log in interactively
  oc login --username=myuser

  # Log in to the given server with the given certificate authority file
  oc login localhost:8443 --certificate-authority=/path/to/cert.crt

  # Log in to the given server with the given credentials (will not prompt interactively)
  oc login localhost:8443 --username=myuser --password=mypass

  # Log in to the given server through a browser
  oc login localhost:8443 --web --callback-port 8280

  # Log in to the external OIDC issuer through Auth Code + PKCE by starting a local server listening port 8080
  oc login localhost:8443 --exec-plugin=oc-oidc --client-id=client-id --extra-scopes=email,profile --callback-port=8080</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-logout">
<title>oc logout</title>
<simpara>End the current server session</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Log out
  oc logout</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-logs">
<title>oc logs</title>
<simpara>Print the logs for a container in a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Start streaming the logs of the most recent build of the openldap build config
  oc logs -f bc/openldap

  # Start streaming the logs of the latest deployment of the mysql deployment config
  oc logs -f dc/mysql

  # Get the logs of the first deployment for the mysql deployment config. Note that logs
  # from older deployments may not exist either because the deployment was successful
  # or due to deployment pruning or manual deletion of the deployment
  oc logs --version=1 dc/mysql

  # Return a snapshot of ruby-container logs from pod backend
  oc logs backend -c ruby-container

  # Start streaming of ruby-container logs from pod backend
  oc logs -f pod/backend -c ruby-container</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-new-app">
<title>oc new-app</title>
<simpara>Create a new application</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List all local templates and image streams that can be used to create an app
  oc new-app --list

  # Create an application based on the source code in the current git repository (with a public remote) and a container image
  oc new-app . --image=registry/repo/langimage

  # Create an application myapp with Docker based build strategy expecting binary input
  oc new-app  --strategy=docker --binary --name myapp

  # Create a Ruby application based on the provided [image]~[source code] combination
  oc new-app centos/ruby-25-centos7~https://github.com/sclorg/ruby-ex.git

  # Use the public container registry MySQL image to create an app. Generated artifacts will be labeled with db=mysql
  oc new-app mysql MYSQL_USER=user MYSQL_PASSWORD=pass MYSQL_DATABASE=testdb -l db=mysql

  # Use a MySQL image in a private registry to create an app and override application artifacts' names
  oc new-app --image=myregistry.com/mycompany/mysql --name=private

  # Use an image with the full manifest list to create an app and override application artifacts' names
  oc new-app --image=myregistry.com/mycompany/image --name=private --import-mode=PreserveOriginal

  # Create an application from a remote repository using its beta4 branch
  oc new-app https://github.com/openshift/ruby-hello-world#beta4

  # Create an application based on a stored template, explicitly setting a parameter value
  oc new-app --template=ruby-helloworld-sample --param=MYSQL_USER=admin

  # Create an application from a remote repository and specify a context directory
  oc new-app https://github.com/youruser/yourgitrepo --context-dir=src/build

  # Create an application from a remote private repository and specify which existing secret to use
  oc new-app https://github.com/youruser/yourgitrepo --source-secret=yoursecret

  # Create an application based on a template file, explicitly setting a parameter value
  oc new-app --file=./example/myapp/template.json --param=MYSQL_USER=admin

  # Search all templates, image streams, and container images for the ones that match "ruby"
  oc new-app --search ruby

  # Search for "ruby", but only in stored templates (--template, --image-stream and --image
  # can be used to filter search results)
  oc new-app --search --template=ruby

  # Search for "ruby" in stored templates and print the output as YAML
  oc new-app --search --template=ruby --output=yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-new-build">
<title>oc new-build</title>
<simpara>Create a new build configuration</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a build config based on the source code in the current git repository (with a public
  # remote) and a container image
  oc new-build . --image=repo/langimage

  # Create a NodeJS build config based on the provided [image]~[source code] combination
  oc new-build centos/nodejs-8-centos7~https://github.com/sclorg/nodejs-ex.git

  # Create a build config from a remote repository using its beta2 branch
  oc new-build https://github.com/openshift/ruby-hello-world#beta2

  # Create a build config using a Dockerfile specified as an argument
  oc new-build -D $'FROM centos:7\nRUN yum install -y httpd'

  # Create a build config from a remote repository and add custom environment variables
  oc new-build https://github.com/openshift/ruby-hello-world -e RACK_ENV=development

  # Create a build config from a remote private repository and specify which existing secret to use
  oc new-build https://github.com/youruser/yourgitrepo --source-secret=yoursecret

  # Create a build config using  an image with the full manifest list to create an app and override application artifacts' names
  oc new-build --image=myregistry.com/mycompany/image --name=private --import-mode=PreserveOriginal

  # Create a build config from a remote repository and inject the npmrc into a build
  oc new-build https://github.com/openshift/ruby-hello-world --build-secret npmrc:.npmrc

  # Create a build config from a remote repository and inject environment data into a build
  oc new-build https://github.com/openshift/ruby-hello-world --build-config-map env:config

  # Create a build config that gets its input from a remote repository and another container image
  oc new-build https://github.com/openshift/ruby-hello-world --source-image=openshift/jenkins-1-centos7 --source-image-path=/var/lib/jenkins:tmp</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-new-project">
<title>oc new-project</title>
<simpara>Request a new project</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new project with minimal information
  oc new-project web-team-dev

  # Create a new project with a display name and description
  oc new-project web-team-dev --display-name="Web Team Development" --description="Development project for the web team."</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-observe">
<title>oc observe</title>
<simpara>Observe changes to resources and react to them (experimental)</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Observe changes to services
  oc observe services

  # Observe changes to services, including the clusterIP and invoke a script for each
  oc observe services --template '{ .spec.clusterIP }' -- register_dns.sh

  # Observe changes to services filtered by a label selector
  oc observe services -l regist-dns=true --template '{ .spec.clusterIP }' -- register_dns.sh</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-patch">
<title>oc patch</title>
<simpara>Update fields of a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Partially update a node using a strategic merge patch, specifying the patch as JSON
  oc patch node k8s-node-1 -p '{"spec":{"unschedulable":true}}'

  # Partially update a node using a strategic merge patch, specifying the patch as YAML
  oc patch node k8s-node-1 -p $'spec:\n unschedulable: true'

  # Partially update a node identified by the type and name specified in "node.json" using strategic merge patch
  oc patch -f node.json -p '{"spec":{"unschedulable":true}}'

  # Update a container's image; spec.containers[*].name is required because it's a merge key
  oc patch pod valid-pod -p '{"spec":{"containers":[{"name":"kubernetes-serve-hostname","image":"new image"}]}}'

  # Update a container's image using a JSON patch with positional arrays
  oc patch pod valid-pod --type='json' -p='[{"op": "replace", "path": "/spec/containers/0/image", "value":"new image"}]'

  # Update a deployment's replicas through the 'scale' subresource using a merge patch
  oc patch deployment nginx-deployment --subresource='scale' --type='merge' -p '{"spec":{"replicas":2}}'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-plugin-list">
<title>oc plugin list</title>
<simpara>List all visible plugin executables on a user&#8217;s PATH</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List all available plugins
  oc plugin list</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-policy-add-role-to-user">
<title>oc policy add-role-to-user</title>
<simpara>Add a role to users or service accounts for the current project</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add the 'view' role to user1 for the current project
  oc policy add-role-to-user view user1

  # Add the 'edit' role to serviceaccount1 for the current project
  oc policy add-role-to-user edit -z serviceaccount1</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-policy-scc-review">
<title>oc policy scc-review</title>
<simpara>Check which service account can create a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Check whether service accounts sa1 and sa2 can admit a pod with a template pod spec specified in my_resource.yaml
  # Service Account specified in myresource.yaml file is ignored
  oc policy scc-review -z sa1,sa2 -f my_resource.yaml

  # Check whether service accounts system:serviceaccount:bob:default can admit a pod with a template pod spec specified in my_resource.yaml
  oc policy scc-review -z system:serviceaccount:bob:default -f my_resource.yaml

  # Check whether the service account specified in my_resource_with_sa.yaml can admit the pod
  oc policy scc-review -f my_resource_with_sa.yaml

  # Check whether the default service account can admit the pod; default is taken since no service account is defined in myresource_with_no_sa.yaml
  oc policy scc-review -f myresource_with_no_sa.yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-policy-scc-subject-review">
<title>oc policy scc-subject-review</title>
<simpara>Check whether a user or a service account can create a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Check whether user bob can create a pod specified in myresource.yaml
  oc policy scc-subject-review -u bob -f myresource.yaml

  # Check whether user bob who belongs to projectAdmin group can create a pod specified in myresource.yaml
  oc policy scc-subject-review -u bob -g projectAdmin -f myresource.yaml

  # Check whether a service account specified in the pod template spec in myresourcewithsa.yaml can create the pod
  oc policy scc-subject-review -f myresourcewithsa.yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-port-forward">
<title>oc port-forward</title>
<simpara>Forward one or more local ports to a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in the pod
  oc port-forward pod/mypod 5000 6000

  # Listen on ports 5000 and 6000 locally, forwarding data to/from ports 5000 and 6000 in a pod selected by the deployment
  oc port-forward deployment/mydeployment 5000 6000

  # Listen on port 8443 locally, forwarding to the targetPort of the service's port named "https" in a pod selected by the service
  oc port-forward service/myservice 8443:https

  # Listen on port 8888 locally, forwarding to 5000 in the pod
  oc port-forward pod/mypod 8888:5000

  # Listen on port 8888 on all addresses, forwarding to 5000 in the pod
  oc port-forward --address 0.0.0.0 pod/mypod 8888:5000

  # Listen on port 8888 on localhost and selected IP, forwarding to 5000 in the pod
  oc port-forward --address localhost,10.19.21.23 pod/mypod 8888:5000

  # Listen on a random port locally, forwarding to 5000 in the pod
  oc port-forward pod/mypod :5000</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-process">
<title>oc process</title>
<simpara>Process a template into list of resources</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Convert the template.json file into a resource list and pass to create
  oc process -f template.json | oc create -f -

  # Process a file locally instead of contacting the server
  oc process -f template.json --local -o yaml

  # Process template while passing a user-defined label
  oc process -f template.json -l name=mytemplate

  # Convert a stored template into a resource list
  oc process foo

  # Convert a stored template into a resource list by setting/overriding parameter values
  oc process foo PARM1=VALUE1 PARM2=VALUE2

  # Convert a template stored in different namespace into a resource list
  oc process openshift//foo

  # Convert template.json into a resource list
  cat template.json | oc process -f -</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-project">
<title>oc project</title>
<simpara>Switch to another project</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Switch to the 'myapp' project
  oc project myapp

  # Display the project currently in use
  oc project</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-projects">
<title>oc projects</title>
<simpara>Display existing projects</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List all projects
  oc projects</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-proxy">
<title>oc proxy</title>
<simpara>Run a proxy to the Kubernetes API server</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # To proxy all of the Kubernetes API and nothing else
  oc proxy --api-prefix=/

  # To proxy only part of the Kubernetes API and also some static files
  # You can get pods info with 'curl localhost:8001/api/v1/pods'
  oc proxy --www=/my/files --www-prefix=/static/ --api-prefix=/api/

  # To proxy the entire Kubernetes API at a different root
  # You can get pods info with 'curl localhost:8001/custom/api/v1/pods'
  oc proxy --api-prefix=/custom/

  # Run a proxy to the Kubernetes API server on port 8011, serving static content from ./local/www/
  oc proxy --port=8011 --www=./local/www/

  # Run a proxy to the Kubernetes API server on an arbitrary local port
  # The chosen port for the server will be output to stdout
  oc proxy --port=0

  # Run a proxy to the Kubernetes API server, changing the API prefix to k8s-api
  # This makes e.g. the pods API available at localhost:8001/k8s-api/v1/pods/
  oc proxy --api-prefix=/k8s-api</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-registry-login">
<title>oc registry login</title>
<simpara>Log in to the integrated registry</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Log in to the integrated registry
  oc registry login

  # Log in to different registry using BASIC auth credentials
  oc registry login --registry quay.io/myregistry --auth-basic=USER:PASS</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-replace">
<title>oc replace</title>
<simpara>Replace a resource by file name or stdin</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Replace a pod using the data in pod.json
  oc replace -f ./pod.json

  # Replace a pod based on the JSON passed into stdin
  cat pod.json | oc replace -f -

  # Update a single-container pod's image version (tag) to v4
  oc get pod mypod -o yaml | sed 's/\(image: myimage\):.*$/\1:v4/' | oc replace -f -

  # Force replace, delete and then re-create the resource
  oc replace --force -f ./pod.json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollback">
<title>oc rollback</title>
<simpara>Revert part of an application back to a previous deployment</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Perform a rollback to the last successfully completed deployment for a deployment config
  oc rollback frontend

  # See what a rollback to version 3 will look like, but do not perform the rollback
  oc rollback frontend --to-version=3 --dry-run

  # Perform a rollback to a specific deployment
  oc rollback frontend-2

  # Perform the rollback manually by piping the JSON of the new config back to oc
  oc rollback frontend -o json | oc replace dc/frontend -f -

  # Print the updated deployment configuration in JSON format instead of performing the rollback
  oc rollback frontend -o json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-cancel">
<title>oc rollout cancel</title>
<simpara>Cancel the in-progress deployment</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Cancel the in-progress deployment based on 'nginx'
  oc rollout cancel dc/nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-history">
<title>oc rollout history</title>
<simpara>View rollout history</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # View the rollout history of a deployment
  oc rollout history dc/nginx

  # View the details of deployment revision 3
  oc rollout history dc/nginx --revision=3</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-latest">
<title>oc rollout latest</title>
<simpara>Start a new rollout for a deployment config with the latest state from its triggers</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Start a new rollout based on the latest images defined in the image change triggers
  oc rollout latest dc/nginx

  # Print the rolled out deployment config
  oc rollout latest dc/nginx -o json</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-pause">
<title>oc rollout pause</title>
<simpara>Mark the provided resource as paused</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Mark the nginx deployment as paused. Any current state of
  # the deployment will continue its function, new updates to the deployment will not
  # have an effect as long as the deployment is paused
  oc rollout pause dc/nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-restart">
<title>oc rollout restart</title>
<simpara>Restart a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Restart a deployment
  oc rollout restart deployment/nginx

  # Restart a daemon set
  oc rollout restart daemonset/abc

  # Restart deployments with the app=nginx label
  oc rollout restart deployment --selector=app=nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-resume">
<title>oc rollout resume</title>
<simpara>Resume a paused resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Resume an already paused deployment
  oc rollout resume dc/nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-retry">
<title>oc rollout retry</title>
<simpara>Retry the latest failed rollout</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Retry the latest failed deployment based on 'frontend'
  # The deployer pod and any hook pods are deleted for the latest failed deployment
  oc rollout retry dc/frontend</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-status">
<title>oc rollout status</title>
<simpara>Show the status of the rollout</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Watch the status of the latest rollout
  oc rollout status dc/nginx</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rollout-undo">
<title>oc rollout undo</title>
<simpara>Undo a previous rollout</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Roll back to the previous deployment
  oc rollout undo dc/nginx

  # Roll back to deployment revision 3. The replication controller for that version must exist
  oc rollout undo dc/nginx --to-revision=3</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rsh">
<title>oc rsh</title>
<simpara>Start a shell session in a container</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Open a shell session on the first container in pod 'foo'
  oc rsh foo

  # Open a shell session on the first container in pod 'foo' and namespace 'bar'
  # (Note that oc client specific arguments must come before the resource name and its arguments)
  oc rsh -n bar foo

  # Run the command 'cat /etc/resolv.conf' inside pod 'foo'
  oc rsh foo cat /etc/resolv.conf

  # See the configuration of your internal registry
  oc rsh dc/docker-registry cat config.yml

  # Open a shell session on the container named 'index' inside a pod of your job
  oc rsh -c index job/scheduled</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-rsync">
<title>oc rsync</title>
<simpara>Copy files between a local file system and a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Synchronize a local directory with a pod directory
  oc rsync ./local/dir/ POD:/remote/dir

  # Synchronize a pod directory with a local directory
  oc rsync POD:/remote/dir/ ./local/dir</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-run">
<title>oc run</title>
<simpara>Run a particular image on the cluster</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Start a nginx pod
  oc run nginx --image=nginx

  # Start a hazelcast pod and let the container expose port 5701
  oc run hazelcast --image=hazelcast/hazelcast --port=5701

  # Start a hazelcast pod and set environment variables "DNS_DOMAIN=cluster" and "POD_NAMESPACE=default" in the container
  oc run hazelcast --image=hazelcast/hazelcast --env="DNS_DOMAIN=cluster" --env="POD_NAMESPACE=default"

  # Start a hazelcast pod and set labels "app=hazelcast" and "env=prod" in the container
  oc run hazelcast --image=hazelcast/hazelcast --labels="app=hazelcast,env=prod"

  # Dry run; print the corresponding API objects without creating them
  oc run nginx --image=nginx --dry-run=client

  # Start a nginx pod, but overload the spec with a partial set of values parsed from JSON
  oc run nginx --image=nginx --overrides='{ "apiVersion": "v1", "spec": { ... } }'

  # Start a busybox pod and keep it in the foreground, don't restart it if it exits
  oc run -i -t busybox --image=busybox --restart=Never

  # Start the nginx pod using the default command, but use custom arguments (arg1 .. argN) for that command
  oc run nginx --image=nginx -- &lt;arg1&gt; &lt;arg2&gt; ... &lt;argN&gt;

  # Start the nginx pod using a different command and custom arguments
  oc run nginx --image=nginx --command -- &lt;cmd&gt; &lt;arg1&gt; ... &lt;argN&gt;</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-scale">
<title>oc scale</title>
<simpara>Set a new size for a deployment, replica set, or replication controller</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Scale a replica set named 'foo' to 3
  oc scale --replicas=3 rs/foo

  # Scale a resource identified by type and name specified in "foo.yaml" to 3
  oc scale --replicas=3 -f foo.yaml

  # If the deployment named mysql's current size is 2, scale mysql to 3
  oc scale --current-replicas=2 --replicas=3 deployment/mysql

  # Scale multiple replication controllers
  oc scale --replicas=5 rc/example1 rc/example2 rc/example3

  # Scale stateful set named 'web' to 3
  oc scale --replicas=3 statefulset/web</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-secrets-link">
<title>oc secrets link</title>
<simpara>Link secrets to a service account</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add an image pull secret to a service account to automatically use it for pulling pod images
  oc secrets link serviceaccount-name pull-secret --for=pull

  # Add an image pull secret to a service account to automatically use it for both pulling and pushing build images
  oc secrets link builder builder-image-secret --for=pull,mount</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-secrets-unlink">
<title>oc secrets unlink</title>
<simpara>Detach secrets from a service account</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Unlink a secret currently associated with a service account
  oc secrets unlink serviceaccount-name secret-name another-secret-name ...</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-build-hook">
<title>oc set build-hook</title>
<simpara>Update a build hook on a build config</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Clear post-commit hook on a build config
  oc set build-hook bc/mybuild --post-commit --remove

  # Set the post-commit hook to execute a test suite using a new entrypoint
  oc set build-hook bc/mybuild --post-commit --command -- /bin/bash -c /var/lib/test-image.sh

  # Set the post-commit hook to execute a shell script
  oc set build-hook bc/mybuild --post-commit --script="/var/lib/test-image.sh param1 param2 &amp;&amp; /var/lib/done.sh"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-build-secret">
<title>oc set build-secret</title>
<simpara>Update a build secret on a build config</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Clear the push secret on a build config
  oc set build-secret --push --remove bc/mybuild

  # Set the pull secret on a build config
  oc set build-secret --pull bc/mybuild mysecret

  # Set the push and pull secret on a build config
  oc set build-secret --push --pull bc/mybuild mysecret

  # Set the source secret on a set of build configs matching a selector
  oc set build-secret --source -l app=myapp gitsecret</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-data">
<title>oc set data</title>
<simpara>Update the data within a config map or secret</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set the 'password' key of a secret
  oc set data secret/foo password=this_is_secret

  # Remove the 'password' key from a secret
  oc set data secret/foo password-

  # Update the 'haproxy.conf' key of a config map from a file on disk
  oc set data configmap/bar --from-file=../haproxy.conf

  # Update a secret with the contents of a directory, one key per file
  oc set data secret/foo --from-file=secret-dir</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-deployment-hook">
<title>oc set deployment-hook</title>
<simpara>Update a deployment hook on a deployment config</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Clear pre and post hooks on a deployment config
  oc set deployment-hook dc/myapp --remove --pre --post

  # Set the pre deployment hook to execute a db migration command for an application
  # using the data volume from the application
  oc set deployment-hook dc/myapp --pre --volumes=data -- /var/lib/migrate-db.sh

  # Set a mid deployment hook along with additional environment variables
  oc set deployment-hook dc/myapp --mid --volumes=data -e VAR1=value1 -e VAR2=value2 -- /var/lib/prepare-deploy.sh</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-env">
<title>oc set env</title>
<simpara>Update environment variables on a pod template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Update deployment config 'myapp' with a new environment variable
  oc set env dc/myapp STORAGE_DIR=/local

  # List the environment variables defined on a build config 'sample-build'
  oc set env bc/sample-build --list

  # List the environment variables defined on all pods
  oc set env pods --all --list

  # Output modified build config in YAML
  oc set env bc/sample-build STORAGE_DIR=/data -o yaml

  # Update all containers in all replication controllers in the project to have ENV=prod
  oc set env rc --all ENV=prod

  # Import environment from a secret
  oc set env --from=secret/mysecret dc/myapp

  # Import environment from a config map with a prefix
  oc set env --from=configmap/myconfigmap --prefix=MYSQL_ dc/myapp

  # Remove the environment variable ENV from container 'c1' in all deployment configs
  oc set env dc --all --containers="c1" ENV-

  # Remove the environment variable ENV from a deployment config definition on disk and
  # update the deployment config on the server
  oc set env -f dc.json ENV-

  # Set some of the local shell environment into a deployment config on the server
  oc set env | grep RAILS_ | oc env -e - dc/myapp</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-image">
<title>oc set image</title>
<simpara>Update the image of a pod template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set a deployment config's nginx container image to 'nginx:1.9.1', and its busybox container image to 'busybox'.
  oc set image dc/nginx busybox=busybox nginx=nginx:1.9.1

  # Set a deployment config's app container image to the image referenced by the imagestream tag 'openshift/ruby:2.3'.
  oc set image dc/myapp app=openshift/ruby:2.3 --source=imagestreamtag

  # Update all deployments' and rc's nginx container's image to 'nginx:1.9.1'
  oc set image deployments,rc nginx=nginx:1.9.1 --all

  # Update image of all containers of daemonset abc to 'nginx:1.9.1'
  oc set image daemonset abc *=nginx:1.9.1

  # Print result (in YAML format) of updating nginx container image from local file, without hitting the server
  oc set image -f path/to/file.yaml nginx=nginx:1.9.1 --local -o yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-image-lookup">
<title>oc set image-lookup</title>
<simpara>Change how images are resolved when deploying applications</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print all of the image streams and whether they resolve local names
  oc set image-lookup

  # Use local name lookup on image stream mysql
  oc set image-lookup mysql

  # Force a deployment to use local name lookup
  oc set image-lookup deploy/mysql

  # Show the current status of the deployment lookup
  oc set image-lookup deploy/mysql --list

  # Disable local name lookup on image stream mysql
  oc set image-lookup mysql --enabled=false

  # Set local name lookup on all image streams
  oc set image-lookup --all</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-probe">
<title>oc set probe</title>
<simpara>Update a probe on a pod template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Clear both readiness and liveness probes off all containers
  oc set probe dc/myapp --remove --readiness --liveness

  # Set an exec action as a liveness probe to run 'echo ok'
  oc set probe dc/myapp --liveness -- echo ok

  # Set a readiness probe to try to open a TCP socket on 3306
  oc set probe rc/mysql --readiness --open-tcp=3306

  # Set an HTTP startup probe for port 8080 and path /healthz over HTTP on the pod IP
  oc set probe dc/webapp --startup --get-url=http://:8080/healthz

  # Set an HTTP readiness probe for port 8080 and path /healthz over HTTP on the pod IP
  oc set probe dc/webapp --readiness --get-url=http://:8080/healthz

  # Set an HTTP readiness probe over HTTPS on 127.0.0.1 for a hostNetwork pod
  oc set probe dc/router --readiness --get-url=https://127.0.0.1:1936/stats

  # Set only the initial-delay-seconds field on all deployments
  oc set probe dc --all --readiness --initial-delay-seconds=30</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-resources">
<title>oc set resources</title>
<simpara>Update resource requests/limits on objects with pod templates</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set a deployments nginx container CPU limits to "200m and memory to 512Mi"
  oc set resources deployment nginx -c=nginx --limits=cpu=200m,memory=512Mi

  # Set the resource request and limits for all containers in nginx
  oc set resources deployment nginx --limits=cpu=200m,memory=512Mi --requests=cpu=100m,memory=256Mi

  # Remove the resource requests for resources on containers in nginx
  oc set resources deployment nginx --limits=cpu=0,memory=0 --requests=cpu=0,memory=0

  # Print the result (in YAML format) of updating nginx container limits locally, without hitting the server
  oc set resources -f path/to/file.yaml --limits=cpu=200m,memory=512Mi --local -o yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-route-backends">
<title>oc set route-backends</title>
<simpara>Update the backends for a route</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print the backends on the route 'web'
  oc set route-backends web

  # Set two backend services on route 'web' with 2/3rds of traffic going to 'a'
  oc set route-backends web a=2 b=1

  # Increase the traffic percentage going to b by 10%% relative to a
  oc set route-backends web --adjust b=+10%%

  # Set traffic percentage going to b to 10%% of the traffic going to a
  oc set route-backends web --adjust b=10%%

  # Set weight of b to 10
  oc set route-backends web --adjust b=10

  # Set the weight to all backends to zero
  oc set route-backends web --zero</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-selector">
<title>oc set selector</title>
<simpara>Set the selector on a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set the labels and selector before creating a deployment/service pair.
  oc create service clusterip my-svc --clusterip="None" -o yaml --dry-run | oc set selector --local -f - 'environment=qa' -o yaml | oc create -f -
  oc create deployment my-dep -o yaml --dry-run | oc label --local -f - environment=qa -o yaml | oc create -f -</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-serviceaccount">
<title>oc set serviceaccount</title>
<simpara>Update the service account of a resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Set deployment nginx-deployment's service account to serviceaccount1
  oc set serviceaccount deployment nginx-deployment serviceaccount1

  # Print the result (in YAML format) of updated nginx deployment with service account from a local file, without hitting the API server
  oc set sa -f nginx-deployment.yaml serviceaccount1 --local --dry-run -o yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-subject">
<title>oc set subject</title>
<simpara>Update the user, group, or service account in a role binding or cluster role binding</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Update a cluster role binding for serviceaccount1
  oc set subject clusterrolebinding admin --serviceaccount=namespace:serviceaccount1

  # Update a role binding for user1, user2, and group1
  oc set subject rolebinding admin --user=user1 --user=user2 --group=group1

  # Print the result (in YAML format) of updating role binding subjects locally, without hitting the server
  oc create rolebinding admin --role=admin --user=admin -o yaml --dry-run | oc set subject --local -f - --user=foo -o yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-triggers">
<title>oc set triggers</title>
<simpara>Update the triggers on one or more objects</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print the triggers on the deployment config 'myapp'
  oc set triggers dc/myapp

  # Set all triggers to manual
  oc set triggers dc/myapp --manual

  # Enable all automatic triggers
  oc set triggers dc/myapp --auto

  # Reset the GitHub webhook on a build to a new, generated secret
  oc set triggers bc/webapp --from-github
  oc set triggers bc/webapp --from-webhook

  # Remove all triggers
  oc set triggers bc/webapp --remove-all

  # Stop triggering on config change
  oc set triggers dc/myapp --from-config --remove

  # Add an image trigger to a build config
  oc set triggers bc/webapp --from-image=namespace1/image:latest

  # Add an image trigger to a stateful set on the main container
  oc set triggers statefulset/db --from-image=namespace1/image:latest -c main</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-set-volumes">
<title>oc set volumes</title>
<simpara>Update volumes on a pod template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # List volumes defined on all deployment configs in the current project
  oc set volume dc --all

  # Add a new empty dir volume to deployment config (dc) 'myapp' mounted under
  # /var/lib/myapp
  oc set volume dc/myapp --add --mount-path=/var/lib/myapp

  # Use an existing persistent volume claim (PVC) to overwrite an existing volume 'v1'
  oc set volume dc/myapp --add --name=v1 -t pvc --claim-name=pvc1 --overwrite

  # Remove volume 'v1' from deployment config 'myapp'
  oc set volume dc/myapp --remove --name=v1

  # Create a new persistent volume claim that overwrites an existing volume 'v1'
  oc set volume dc/myapp --add --name=v1 -t pvc --claim-size=1G --overwrite

  # Change the mount point for volume 'v1' to /data
  oc set volume dc/myapp --add --name=v1 -m /data --overwrite

  # Modify the deployment config by removing volume mount "v1" from container "c1"
  # (and by removing the volume "v1" if no other containers have volume mounts that reference it)
  oc set volume dc/myapp --remove --name=v1 --containers=c1

  # Add new volume based on a more complex volume source (AWS EBS, GCE PD,
  # Ceph, Gluster, NFS, ISCSI, ...)
  oc set volume dc/myapp --add -m /data --source=&lt;json-string&gt;</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-start-build">
<title>oc start-build</title>
<simpara>Start a new build</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Starts build from build config "hello-world"
  oc start-build hello-world

  # Starts build from a previous build "hello-world-1"
  oc start-build --from-build=hello-world-1

  # Use the contents of a directory as build input
  oc start-build hello-world --from-dir=src/

  # Send the contents of a Git repository to the server from tag 'v2'
  oc start-build hello-world --from-repo=../hello-world --commit=v2

  # Start a new build for build config "hello-world" and watch the logs until the build
  # completes or fails
  oc start-build hello-world --follow

  # Start a new build for build config "hello-world" and wait until the build completes. It
  # exits with a non-zero return code if the build fails
  oc start-build hello-world --wait</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-status">
<title>oc status</title>
<simpara>Show an overview of the current project</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # See an overview of the current project
  oc status

  # Export the overview of the current project in an svg file
  oc status -o dot | dot -T svg -o project.svg

  # See an overview of the current project including details for any identified issues
  oc status --suggest</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-tag">
<title>oc tag</title>
<simpara>Tag existing images into image streams</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Tag the current image for the image stream 'openshift/ruby' and tag '2.0' into the image stream 'yourproject/ruby with tag 'tip'
  oc tag openshift/ruby:2.0 yourproject/ruby:tip

  # Tag a specific image
  oc tag openshift/ruby@sha256:6b646fa6bf5e5e4c7fa41056c27910e679c03ebe7f93e361e6515a9da7e258cc yourproject/ruby:tip

  # Tag an external container image
  oc tag --source=docker openshift/origin-control-plane:latest yourproject/ruby:tip

  # Tag an external container image and request pullthrough for it
  oc tag --source=docker openshift/origin-control-plane:latest yourproject/ruby:tip --reference-policy=local

  # Tag an external container image and include the full manifest list
  oc tag --source=docker openshift/origin-control-plane:latest yourproject/ruby:tip --import-mode=PreserveOriginal

  # Remove the specified spec tag from an image stream
  oc tag openshift/origin-control-plane:latest -d</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-version">
<title>oc version</title>
<simpara>Print the client and server version information</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Print the OpenShift client, kube-apiserver, and openshift-apiserver version information for the current context
  oc version

  # Print the OpenShift client, kube-apiserver, and openshift-apiserver version numbers for the current context in json format
  oc version --output json

  # Print the OpenShift client version information for the current context
  oc version --client</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-wait">
<title>oc wait</title>
<simpara>Experimental: Wait for a specific condition on one or many resources</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Wait for the pod "busybox1" to contain the status condition of type "Ready"
  oc wait --for=condition=Ready pod/busybox1

  # The default value of status condition is true; you can wait for other targets after an equal delimiter (compared after Unicode simple case folding, which is a more general form of case-insensitivity)
  oc wait --for=condition=Ready=false pod/busybox1

  # Wait for the pod "busybox1" to contain the status phase to be "Running"
  oc wait --for=jsonpath='{.status.phase}'=Running pod/busybox1

  # Wait for the service "loadbalancer" to have ingress.
  oc wait --for=jsonpath='{.status.loadBalancer.ingress}' service/loadbalancer

  # Wait for the pod "busybox1" to be deleted, with a timeout of 60s, after having issued the "delete" command
  oc delete pod/busybox1
  oc wait --for=delete pod/busybox1 --timeout=60s</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-whoami">
<title>oc whoami</title>
<simpara>Return information about the current session</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Display the currently authenticated user
  oc whoami</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="additional-resources_cli-developer-commands" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="cli-administrator-commands">OpenShift CLI administrator command reference</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cli-administrator-commands">
<title>OpenShift CLI administrator command reference</title>

<simpara>This reference provides descriptions and example commands for OpenShift CLI (<literal>oc</literal>) administrator commands. You must have <literal>cluster-admin</literal> or equivalent permissions to use these commands.</simpara>
<simpara>For developer commands, see the <link linkend="cli-developer-commands">OpenShift CLI developer command reference</link>.</simpara>
<simpara>Run <literal>oc adm -h</literal> to list all administrator commands or run <literal>oc &lt;command&gt; --help</literal> to get additional details for a specific command.</simpara>
<section xml:id="openshift-cli-admin_cli-administrator-commands">
<title>OpenShift CLI (oc) administrator commands</title>
<section xml:id="_oc-adm-build-chain">
<title>oc adm build-chain</title>
<simpara>Output the inputs and dependencies of your builds</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Build the dependency tree for the 'latest' tag in &lt;image-stream&gt;
  oc adm build-chain &lt;image-stream&gt;

  # Build the dependency tree for the 'v2' tag in dot format and visualize it via the dot utility
  oc adm build-chain &lt;image-stream&gt;:v2 -o dot | dot -T svg -o deps.svg

  # Build the dependency tree across all namespaces for the specified image stream tag found in the 'test' namespace
  oc adm build-chain &lt;image-stream&gt; -n test --all</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-catalog-mirror">
<title>oc adm catalog mirror</title>
<simpara>Mirror an operator-registry catalog</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Mirror an operator-registry image and its contents to a registry
  oc adm catalog mirror quay.io/my/image:latest myregistry.com

  # Mirror an operator-registry image and its contents to a particular namespace in a registry
  oc adm catalog mirror quay.io/my/image:latest myregistry.com/my-namespace

  # Mirror to an airgapped registry by first mirroring to files
  oc adm catalog mirror quay.io/my/image:latest file:///local/index
  oc adm catalog mirror file:///local/index/my/image:latest my-airgapped-registry.com

  # Configure a cluster to use a mirrored registry
  oc apply -f manifests/imageDigestMirrorSet.yaml

  # Edit the mirroring mappings and mirror with "oc image mirror" manually
  oc adm catalog mirror --manifests-only quay.io/my/image:latest myregistry.com
  oc image mirror -f manifests/mapping.txt

  # Delete all ImageDigestMirrorSets generated by oc adm catalog mirror
  oc delete imagedigestmirrorset -l operators.openshift.org/catalog=true</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-certificate-approve">
<title>oc adm certificate approve</title>
<simpara>Approve a certificate signing request</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Approve CSR 'csr-sqgzp'
  oc adm certificate approve csr-sqgzp</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-certificate-deny">
<title>oc adm certificate deny</title>
<simpara>Deny a certificate signing request</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Deny CSR 'csr-sqgzp'
  oc adm certificate deny csr-sqgzp</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-copy-to-node">
<title>oc adm copy-to-node</title>
<simpara>Copies specified files to the node.</simpara>
</section>
<section xml:id="_oc-adm-cordon">
<title>oc adm cordon</title>
<simpara>Mark node as unschedulable</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Mark node "foo" as unschedulable
  oc adm cordon foo</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-create-bootstrap-project-template">
<title>oc adm create-bootstrap-project-template</title>
<simpara>Create a bootstrap project template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Output a bootstrap project template in YAML format to stdout
  oc adm create-bootstrap-project-template -o yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-create-error-template">
<title>oc adm create-error-template</title>
<simpara>Create an error page template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Output a template for the error page to stdout
  oc adm create-error-template</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-create-login-template">
<title>oc adm create-login-template</title>
<simpara>Create a login template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Output a template for the login page to stdout
  oc adm create-login-template</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-create-provider-selection-template">
<title>oc adm create-provider-selection-template</title>
<simpara>Create a provider selection template</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Output a template for the provider selection page to stdout
  oc adm create-provider-selection-template</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-drain">
<title>oc adm drain</title>
<simpara>Drain node in preparation for maintenance</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Drain node "foo", even if there are pods not managed by a replication controller, replica set, job, daemon set, or stateful set on it
  oc adm drain foo --force

  # As above, but abort if there are pods not managed by a replication controller, replica set, job, daemon set, or stateful set, and use a grace period of 15 minutes
  oc adm drain foo --grace-period=900</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-groups-add-users">
<title>oc adm groups add-users</title>
<simpara>Add users to a group</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add user1 and user2 to my-group
  oc adm groups add-users my-group user1 user2</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-groups-new">
<title>oc adm groups new</title>
<simpara>Create a new group</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add a group with no users
  oc adm groups new my-group

  # Add a group with two users
  oc adm groups new my-group user1 user2

  # Add a group with one user and shorter output
  oc adm groups new my-group user1 -o name</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-groups-prune">
<title>oc adm groups prune</title>
<simpara>Remove old OpenShift groups referencing missing records from an external provider</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Prune all orphaned groups
  oc adm groups prune --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Prune all orphaned groups except the ones from the denylist file
  oc adm groups prune --blacklist=/path/to/denylist.txt --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Prune all orphaned groups from a list of specific groups specified in an allowlist file
  oc adm groups prune --whitelist=/path/to/allowlist.txt --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Prune all orphaned groups from a list of specific groups specified in a list
  oc adm groups prune groups/group_name groups/other_name --sync-config=/path/to/ldap-sync-config.yaml --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-groups-remove-users">
<title>oc adm groups remove-users</title>
<simpara>Remove users from a group</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Remove user1 and user2 from my-group
  oc adm groups remove-users my-group user1 user2</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-groups-sync">
<title>oc adm groups sync</title>
<simpara>Sync OpenShift groups with records from an external provider</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Sync all groups with an LDAP server
  oc adm groups sync --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Sync all groups except the ones from the blacklist file with an LDAP server
  oc adm groups sync --blacklist=/path/to/blacklist.txt --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Sync specific groups specified in an allowlist file with an LDAP server
  oc adm groups sync --whitelist=/path/to/allowlist.txt --sync-config=/path/to/sync-config.yaml --confirm

  # Sync all OpenShift groups that have been synced previously with an LDAP server
  oc adm groups sync --type=openshift --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Sync specific OpenShift groups if they have been synced previously with an LDAP server
  oc adm groups sync groups/group1 groups/group2 groups/group3 --sync-config=/path/to/sync-config.yaml --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-inspect">
<title>oc adm inspect</title>
<simpara>Collect debugging data for a given resource</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Collect debugging data for the "openshift-apiserver" clusteroperator
  oc adm inspect clusteroperator/openshift-apiserver

  # Collect debugging data for the "openshift-apiserver" and "kube-apiserver" clusteroperators
  oc adm inspect clusteroperator/openshift-apiserver clusteroperator/kube-apiserver

  # Collect debugging data for all clusteroperators
  oc adm inspect clusteroperator

  # Collect debugging data for all clusteroperators and clusterversions
  oc adm inspect clusteroperators,clusterversions</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-migrate-icsp">
<title>oc adm migrate icsp</title>
<simpara>Update imagecontentsourcepolicy file(s) to imagedigestmirrorset file(s)</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Update the imagecontentsourcepolicy.yaml file to a new imagedigestmirrorset file under the mydir directory
  oc adm migrate icsp imagecontentsourcepolicy.yaml --dest-dir mydir</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-migrate-template-instances">
<title>oc adm migrate template-instances</title>
<simpara>Update template instances to point to the latest group-version-kinds</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Perform a dry-run of updating all objects
  oc adm migrate template-instances

  # To actually perform the update, the confirm flag must be appended
  oc adm migrate template-instances --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-must-gather">
<title>oc adm must-gather</title>
<simpara>Launch a new instance of a pod for gathering debug information</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Gather information using the default plug-in image and command, writing into ./must-gather.local.&lt;rand&gt;
  oc adm must-gather

  # Gather information with a specific local folder to copy to
  oc adm must-gather --dest-dir=/local/directory

  # Gather audit information
  oc adm must-gather -- /usr/bin/gather_audit_logs

  # Gather information using multiple plug-in images
  oc adm must-gather --image=quay.io/kubevirt/must-gather --image=quay.io/openshift/origin-must-gather

  # Gather information using a specific image stream plug-in
  oc adm must-gather --image-stream=openshift/must-gather:latest

  # Gather information using a specific image, command, and pod directory
  oc adm must-gather --image=my/image:tag --source-dir=/pod/directory -- myspecial-command.sh</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-new-project">
<title>oc adm new-project</title>
<simpara>Create a new project</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a new project using a node selector
  oc adm new-project myproject --node-selector='type=user-node,region=east'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-node-logs">
<title>oc adm node-logs</title>
<simpara>Display and filter node logs</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show kubelet logs from all masters
  oc adm node-logs --role master -u kubelet

  # See what logs are available in masters in /var/log
  oc adm node-logs --role master --path=/

  # Display cron log file from all masters
  oc adm node-logs --role master --path=cron</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-ocp-certificates-monitor-certificates">
<title>oc adm ocp-certificates monitor-certificates</title>
<simpara>Watch platform certificates.</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Watch platform certificates.
  oc adm ocp-certificates monitor-certificates</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-ocp-certificates-regenerate-leaf">
<title>oc adm ocp-certificates regenerate-leaf</title>
<simpara>Regenerate client and serving certificates of an OpenShift cluster</simpara>
</section>
<section xml:id="_oc-adm-ocp-certificates-regenerate-machine-config-server-serving-cert">
<title>oc adm ocp-certificates regenerate-machine-config-server-serving-cert</title>
<simpara>Regenerate the machine config operator certificates in an OpenShift cluster</simpara>
</section>
<section xml:id="_oc-adm-ocp-certificates-regenerate-top-level">
<title>oc adm ocp-certificates regenerate-top-level</title>
<simpara>Regenerate the top level certificates in an OpenShift cluster</simpara>
</section>
<section xml:id="_oc-adm-ocp-certificates-remove-old-trust">
<title>oc adm ocp-certificates remove-old-trust</title>
<simpara>Remove old CAs from ConfigMaps representing platform trust bundles in an OpenShift cluster</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  #  Remove only CA certificates created before a certain date from all trust bundles
  oc adm ocp-certificates remove-old-trust configmaps -A --all --created-before 2023-06-05T14:44:06Z</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-ocp-certificates-update-ignition-ca-bundle-for-machine-config-server">
<title>oc adm ocp-certificates update-ignition-ca-bundle-for-machine-config-server</title>
<simpara>Update user-data secrets in an OpenShift cluster to use updated MCO certfs</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Regenerate the MCO certs without modifying user-data secrets
  oc adm certificates regenerate-machine-config-server-serving-cert --update-ignition=false

  # Update the user-data secrets to use new MCS certs
  oc adm certificates update-ignition-ca-bundle-for-machine-config-server</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-pod-network-isolate-projects">
<title>oc adm pod-network isolate-projects</title>
<simpara>Isolate project network</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Provide isolation for project p1
  oc adm pod-network isolate-projects &lt;p1&gt;

  # Allow all projects with label name=top-secret to have their own isolated project network
  oc adm pod-network isolate-projects --selector='name=top-secret'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-pod-network-join-projects">
<title>oc adm pod-network join-projects</title>
<simpara>Join project network</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Allow project p2 to use project p1 network
  oc adm pod-network join-projects --to=&lt;p1&gt; &lt;p2&gt;

  # Allow all projects with label name=top-secret to use project p1 network
  oc adm pod-network join-projects --to=&lt;p1&gt; --selector='name=top-secret'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-pod-network-make-projects-global">
<title>oc adm pod-network make-projects-global</title>
<simpara>Make project network global</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Allow project p1 to access all pods in the cluster and vice versa
  oc adm pod-network make-projects-global &lt;p1&gt;

  # Allow all projects with label name=share to access all pods in the cluster and vice versa
  oc adm pod-network make-projects-global --selector='name=share'</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-policy-add-role-to-user">
<title>oc adm policy add-role-to-user</title>
<simpara>Add a role to users or service accounts for the current project</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add the 'view' role to user1 for the current project
  oc adm policy add-role-to-user view user1

  # Add the 'edit' role to serviceaccount1 for the current project
  oc adm policy add-role-to-user edit -z serviceaccount1</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-policy-add-scc-to-group">
<title>oc adm policy add-scc-to-group</title>
<simpara>Add a security context constraint to groups</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add the 'restricted' security context constraint to group1 and group2
  oc adm policy add-scc-to-group restricted group1 group2</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-policy-add-scc-to-user">
<title>oc adm policy add-scc-to-user</title>
<simpara>Add a security context constraint to users or a service account</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Add the 'restricted' security context constraint to user1 and user2
  oc adm policy add-scc-to-user restricted user1 user2

  # Add the 'privileged' security context constraint to serviceaccount1 in the current namespace
  oc adm policy add-scc-to-user privileged -z serviceaccount1</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-policy-scc-review">
<title>oc adm policy scc-review</title>
<simpara>Check which service account can create a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Check whether service accounts sa1 and sa2 can admit a pod with a template pod spec specified in my_resource.yaml
  # Service Account specified in myresource.yaml file is ignored
  oc adm policy scc-review -z sa1,sa2 -f my_resource.yaml

  # Check whether service accounts system:serviceaccount:bob:default can admit a pod with a template pod spec specified in my_resource.yaml
  oc adm policy scc-review -z system:serviceaccount:bob:default -f my_resource.yaml

  # Check whether the service account specified in my_resource_with_sa.yaml can admit the pod
  oc adm policy scc-review -f my_resource_with_sa.yaml

  # Check whether the default service account can admit the pod; default is taken since no service account is defined in myresource_with_no_sa.yaml
  oc adm policy scc-review -f myresource_with_no_sa.yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-policy-scc-subject-review">
<title>oc adm policy scc-subject-review</title>
<simpara>Check whether a user or a service account can create a pod</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Check whether user bob can create a pod specified in myresource.yaml
  oc adm policy scc-subject-review -u bob -f myresource.yaml

  # Check whether user bob who belongs to projectAdmin group can create a pod specified in myresource.yaml
  oc adm policy scc-subject-review -u bob -g projectAdmin -f myresource.yaml

  # Check whether a service account specified in the pod template spec in myresourcewithsa.yaml can create the pod
  oc adm policy scc-subject-review -f myresourcewithsa.yaml</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-prune-builds">
<title>oc adm prune builds</title>
<simpara>Remove old completed and failed builds</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Dry run deleting older completed and failed builds and also including
  # all builds whose associated build config no longer exists
  oc adm prune builds --orphans

  # To actually perform the prune operation, the confirm flag must be appended
  oc adm prune builds --orphans --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-prune-deployments">
<title>oc adm prune deployments</title>
<simpara>Remove old completed and failed deployment configs</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Dry run deleting all but the last complete deployment for every deployment config
  oc adm prune deployments --keep-complete=1

  # To actually perform the prune operation, the confirm flag must be appended
  oc adm prune deployments --keep-complete=1 --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-prune-groups">
<title>oc adm prune groups</title>
<simpara>Remove old OpenShift groups referencing missing records from an external provider</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Prune all orphaned groups
  oc adm prune groups --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Prune all orphaned groups except the ones from the denylist file
  oc adm prune groups --blacklist=/path/to/denylist.txt --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Prune all orphaned groups from a list of specific groups specified in an allowlist file
  oc adm prune groups --whitelist=/path/to/allowlist.txt --sync-config=/path/to/ldap-sync-config.yaml --confirm

  # Prune all orphaned groups from a list of specific groups specified in a list
  oc adm prune groups groups/group_name groups/other_name --sync-config=/path/to/ldap-sync-config.yaml --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-prune-images">
<title>oc adm prune images</title>
<simpara>Remove unreferenced images</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # See what the prune command would delete if only images and their referrers were more than an hour old
  # and obsoleted by 3 newer revisions under the same tag were considered
  oc adm prune images --keep-tag-revisions=3 --keep-younger-than=60m

  # To actually perform the prune operation, the confirm flag must be appended
  oc adm prune images --keep-tag-revisions=3 --keep-younger-than=60m --confirm

  # See what the prune command would delete if we are interested in removing images
  # exceeding currently set limit ranges ('openshift.io/Image')
  oc adm prune images --prune-over-size-limit

  # To actually perform the prune operation, the confirm flag must be appended
  oc adm prune images --prune-over-size-limit --confirm

  # Force the insecure HTTP protocol with the particular registry host name
  oc adm prune images --registry-url=http://registry.example.org --confirm

  # Force a secure connection with a custom certificate authority to the particular registry host name
  oc adm prune images --registry-url=registry.example.org --certificate-authority=/path/to/custom/ca.crt --confirm</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-reboot-machine-config-pool">
<title>oc adm reboot-machine-config-pool</title>
<simpara>Initiate reboot of the specified MachineConfigPool.</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Reboot all MachineConfigPools
  oc adm reboot-machine-config-pool mcp/worker mcp/master

  # Reboot all MachineConfigPools that inherit from worker.  This include all custom MachineConfigPools and infra.
  oc adm reboot-machine-config-pool mcp/worker

  # Reboot masters
  oc adm reboot-machine-config-pool mcp/master</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-release-extract">
<title>oc adm release extract</title>
<simpara>Extract the contents of an update payload to disk</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Use git to check out the source code for the current cluster release to DIR
  oc adm release extract --git=DIR

  # Extract cloud credential requests for AWS
  oc adm release extract --credentials-requests --cloud=aws

  # Use git to check out the source code for the current cluster release to DIR from linux/s390x image
  # Note: Wildcard filter is not supported; pass a single os/arch to extract
  oc adm release extract --git=DIR quay.io/openshift-release-dev/ocp-release:4.11.2 --filter-by-os=linux/s390x</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-release-info">
<title>oc adm release info</title>
<simpara>Display information about a release</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show information about the cluster's current release
  oc adm release info

  # Show the source code that comprises a release
  oc adm release info 4.11.2 --commit-urls

  # Show the source code difference between two releases
  oc adm release info 4.11.0 4.11.2 --commits

  # Show where the images referenced by the release are located
  oc adm release info quay.io/openshift-release-dev/ocp-release:4.11.2 --pullspecs

  # Show information about linux/s390x image
  # Note: Wildcard filter is not supported; pass a single os/arch to extract
  oc adm release info quay.io/openshift-release-dev/ocp-release:4.11.2 --filter-by-os=linux/s390x</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-release-mirror">
<title>oc adm release mirror</title>
<simpara>Mirror a release to a different image registry location</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Perform a dry run showing what would be mirrored, including the mirror objects
  oc adm release mirror 4.11.0 --to myregistry.local/openshift/release \
  --release-image-signature-to-dir /tmp/releases --dry-run

  # Mirror a release into the current directory
  oc adm release mirror 4.11.0 --to file://openshift/release \
  --release-image-signature-to-dir /tmp/releases

  # Mirror a release to another directory in the default location
  oc adm release mirror 4.11.0 --to-dir /tmp/releases

  # Upload a release from the current directory to another server
  oc adm release mirror --from file://openshift/release --to myregistry.com/openshift/release \
  --release-image-signature-to-dir /tmp/releases

  # Mirror the 4.11.0 release to repository registry.example.com and apply signatures to connected cluster
  oc adm release mirror --from=quay.io/openshift-release-dev/ocp-release:4.11.0-x86_64 \
  --to=registry.example.com/your/repository --apply-release-image-signature</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-release-new">
<title>oc adm release new</title>
<simpara>Create a new OpenShift release</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Create a release from the latest origin images and push to a DockerHub repository
  oc adm release new --from-image-stream=4.11 -n origin --to-image docker.io/mycompany/myrepo:latest

  # Create a new release with updated metadata from a previous release
  oc adm release new --from-release registry.ci.openshift.org/origin/release:v4.11 --name 4.11.1 \
  --previous 4.11.0 --metadata ... --to-image docker.io/mycompany/myrepo:latest

  # Create a new release and override a single image
  oc adm release new --from-release registry.ci.openshift.org/origin/release:v4.11 \
  cli=docker.io/mycompany/cli:latest --to-image docker.io/mycompany/myrepo:latest

  # Run a verification pass to ensure the release can be reproduced
  oc adm release new --from-release registry.ci.openshift.org/origin/release:v4.11</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-restart-kubelet">
<title>oc adm restart-kubelet</title>
<simpara>Restarts kubelet on the specified nodes</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Restart all the nodes,  10% at a time
  oc adm restart-kubelet nodes --all --directive=RemoveKubeletKubeconfig

  # Restart all the nodes,  20 nodes at a time
  oc adm restart-kubelet nodes --all --parallelism=20 --directive=RemoveKubeletKubeconfig

  # Restart all the nodes,  15% at a time
  oc adm restart-kubelet nodes --all --parallelism=15% --directive=RemoveKubeletKubeconfig

  # Restart all the masters at the same time
  oc adm restart-kubelet nodes -l node-role.kubernetes.io/master --parallelism=100% --directive=RemoveKubeletKubeconfig</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-taint">
<title>oc adm taint</title>
<simpara>Update the taints on one or more nodes</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Update node 'foo' with a taint with key 'dedicated' and value 'special-user' and effect 'NoSchedule'
  # If a taint with that key and effect already exists, its value is replaced as specified
  oc adm taint nodes foo dedicated=special-user:NoSchedule

  # Remove from node 'foo' the taint with key 'dedicated' and effect 'NoSchedule' if one exists
  oc adm taint nodes foo dedicated:NoSchedule-

  # Remove from node 'foo' all the taints with key 'dedicated'
  oc adm taint nodes foo dedicated-

  # Add a taint with key 'dedicated' on nodes having label myLabel=X
  oc adm taint node -l myLabel=X  dedicated=foo:PreferNoSchedule

  # Add to node 'foo' a taint with key 'bar' and no value
  oc adm taint nodes foo bar:NoSchedule</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-top-images">
<title>oc adm top images</title>
<simpara>Show usage statistics for images</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show usage statistics for images
  oc adm top images</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-top-imagestreams">
<title>oc adm top imagestreams</title>
<simpara>Show usage statistics for image streams</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show usage statistics for image streams
  oc adm top imagestreams</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-top-node">
<title>oc adm top node</title>
<simpara>Display resource (CPU/memory) usage of nodes</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show metrics for all nodes
  oc adm top node

  # Show metrics for a given node
  oc adm top node NODE_NAME</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-top-pod">
<title>oc adm top pod</title>
<simpara>Display resource (CPU/memory) usage of pods</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Show metrics for all pods in the default namespace
  oc adm top pod

  # Show metrics for all pods in the given namespace
  oc adm top pod --namespace=NAMESPACE

  # Show metrics for a given pod and its containers
  oc adm top pod POD_NAME --containers

  # Show metrics for the pods defined by label name=myLabel
  oc adm top pod -l name=myLabel</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-uncordon">
<title>oc adm uncordon</title>
<simpara>Mark node as schedulable</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Mark node "foo" as schedulable
  oc adm uncordon foo</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-upgrade">
<title>oc adm upgrade</title>
<simpara>Upgrade a cluster or adjust the upgrade channel</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # View the update status and available cluster updates
  oc adm upgrade

  # Update to the latest version
  oc adm upgrade --to-latest=true</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-verify-image-signature">
<title>oc adm verify-image-signature</title>
<simpara>Verify the image identity contained in the image signature</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Verify the image signature and identity using the local GPG keychain
  oc adm verify-image-signature sha256:c841e9b64e4579bd56c794bdd7c36e1c257110fd2404bebbb8b613e4935228c4 \
  --expected-identity=registry.local:5000/foo/bar:v1

  # Verify the image signature and identity using the local GPG keychain and save the status
  oc adm verify-image-signature sha256:c841e9b64e4579bd56c794bdd7c36e1c257110fd2404bebbb8b613e4935228c4 \
  --expected-identity=registry.local:5000/foo/bar:v1 --save

  # Verify the image signature and identity via exposed registry route
  oc adm verify-image-signature sha256:c841e9b64e4579bd56c794bdd7c36e1c257110fd2404bebbb8b613e4935228c4 \
  --expected-identity=registry.local:5000/foo/bar:v1 \
  --registry-url=docker-registry.foo.com

  # Remove all signature verifications from the image
  oc adm verify-image-signature sha256:c841e9b64e4579bd56c794bdd7c36e1c257110fd2404bebbb8b613e4935228c4 --remove-all</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-wait-for-node-reboot">
<title>oc adm wait-for-node-reboot</title>
<simpara>Wait for nodes to reboot after running <literal>oc adm reboot-machine-config-pool</literal></simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Wait for all nodes to complete a requested reboot from 'oc adm reboot-machine-config-pool mcp/worker mcp/master'
  oc adm wait-for-node-reboot nodes --all

  # Wait for masters to complete a requested reboot from 'oc adm reboot-machine-config-pool mcp/master'
  oc adm wait-for-node-reboot nodes -l node-role.kubernetes.io/master

  # Wait for masters to complete a specific reboot
  oc adm wait-for-node-reboot nodes -l node-role.kubernetes.io/master --reboot-number=4</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_oc-adm-wait-for-stable-cluster">
<title>oc adm wait-for-stable-cluster</title>
<simpara>wait for the platform operators to become stable</simpara>
<formalpara>
<title>Example usage</title>
<para>
<programlisting language="bash" linenumbering="unnumbered">  # Wait for all clusteroperators to become stable
  oc adm wait-for-stable-cluster

  # Consider operators to be stable if they report as such for 5 minutes straight
  oc adm wait-for-stable-cluster --minimum-stable-period 5m</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="additional-resources_cli-administrator-commands" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="cli-developer-commands">OpenShift CLI developer command reference</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="odo-important_update_cli-administrator-commands">
<title>Important update on <literal>odo</literal></title>

<simpara>Red Hat does not provide information about <literal>odo</literal> on the OpenShift Container Platform documentation site. See the <link xlink:href="https://odo.dev/docs/introduction">documentation</link> maintained by Red Hat and the upstream community for documentation information related to <literal>odo</literal>.</simpara>
<important>
<simpara>For the materials maintained by the upstream community, Red Hat provides support under <link xlink:href="https://access.redhat.com/solutions/5893251">Cooperative Community Support</link>.</simpara>
</important>
</chapter>
<chapter xml:id="kn-cli-tools">
<title>Knative CLI for use with OpenShift Serverless</title>

<simpara>The Knative (<literal>kn</literal>) CLI enables simple interaction with Knative components on OpenShift Container Platform.</simpara>
<section xml:id="kn-cli-tools-key-features">
<title>Key features</title>
<simpara>The Knative (<literal>kn</literal>) CLI is designed to make serverless computing tasks simple and concise.
Key features of the Knative CLI include:</simpara>
<itemizedlist>
<listitem>
<simpara>Deploy serverless applications from the command line.</simpara>
</listitem>
<listitem>
<simpara>Manage features of Knative Serving, such as services, revisions, and traffic-splitting.</simpara>
</listitem>
<listitem>
<simpara>Create and manage Knative Eventing components, such as event sources and triggers.</simpara>
</listitem>
<listitem>
<simpara>Create sink bindings to connect existing Kubernetes applications and Knative services.</simpara>
</listitem>
<listitem>
<simpara>Extend the Knative CLI with flexible plugin architecture, similar to the <literal>kubectl</literal> CLI.</simpara>
</listitem>
<listitem>
<simpara>Configure autoscaling parameters for Knative services.</simpara>
</listitem>
<listitem>
<simpara>Scripted usage, such as waiting for the results of an operation, or deploying custom rollout and rollback strategies.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="kn-cli-tools-installing-kn">
<title>Installing the Knative CLI</title>
<simpara>See <link xlink:href="https://docs.openshift.com/serverless/1.28/install/installing-kn.html#installing-kn">Installing the Knative CLI</link>.</simpara>
</section>
</chapter>
<chapter xml:id="_pipelines-cli-tkn">
<title>Pipelines CLI (tkn)</title>
<section xml:id="installing-tkn">
<title>Installing tkn</title>

<simpara>Use the CLI tool to manage Red Hat OpenShift Pipelines from a terminal. The following section describes how to install the CLI tool on different platforms.</simpara>
<simpara>You can also find the URL to the latest binaries from the OpenShift Container Platform web console by clicking the <emphasis role="strong">?</emphasis> icon in the upper-right corner and selecting <emphasis role="strong">Command Line Tools</emphasis>.
:FeatureName: Running Red Hat OpenShift Pipelines on ARM hardware</simpara>
<important>
<simpara>{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<note>
<simpara>Both the archives and the RPMs contain the following executables:</simpara>
<itemizedlist>
<listitem>
<simpara>tkn</simpara>
</listitem>
<listitem>
<simpara>tkn-pac</simpara>
</listitem>
<listitem>
<simpara>opc</simpara>
</listitem>
</itemizedlist>
</note>
<important>
<simpara>Running Red Hat OpenShift Pipelines with the <literal>opc</literal> CLI tool is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="installing-tkn-on-linux">
<title>Installing the Red Hat OpenShift Pipelines CLI on Linux</title>
<simpara role="_abstract">For Linux distributions, you can download the CLI as a <literal>tar.gz</literal> archive.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Download the relevant CLI tool.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-linux-amd64.tar.gz">Linux (x86_64, amd64)</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-linux-s390x.tar.gz">Linux on IBM Z&#174; and IBM&#174; LinuxONE (s390x)</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-linux-ppc64le.tar.gz">Linux on IBM Power&#174; (ppc64le)</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-linux-arm64.tar.gz">Linux on ARM (aarch64, arm64)</link></simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<listitem>
<simpara>Unpack the archive:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvzf &lt;file&gt;</programlisting>
</listitem>
<listitem>
<simpara>Add the location of your <literal>tkn</literal>, <literal>tkn-pac</literal>, and <literal>opc</literal> files to your <literal>PATH</literal> environment variable.</simpara>
</listitem>
<listitem>
<simpara>To check your <literal>PATH</literal>, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="installing-tkn-on-linux-using-rpm">
<title>Installing the Red Hat OpenShift Pipelines CLI on Linux using an RPM</title>
<simpara role="_abstract">For Red Hat Enterprise Linux (RHEL) version 8, you can install the Red Hat OpenShift Pipelines CLI as an RPM.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have an active OpenShift Container Platform subscription on your Red Hat account.</simpara>
</listitem>
<listitem>
<simpara>You have root or sudo privileges on your local system.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Register with Red Hat Subscription Manager:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager register</programlisting>
</listitem>
<listitem>
<simpara>Pull the latest subscription data:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager refresh</programlisting>
</listitem>
<listitem>
<simpara>List the available subscriptions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager list --available --matches '*pipelines*'</programlisting>
</listitem>
<listitem>
<simpara>In the output for the previous command, find the pool ID for your OpenShift Container Platform subscription and attach the subscription to the registered system:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager attach --pool=&lt;pool_id&gt;</programlisting>
</listitem>
<listitem>
<simpara>Enable the repositories required by Red Hat OpenShift Pipelines:</simpara>
<itemizedlist>
<listitem>
<simpara>Linux (x86_64, amd64)</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --enable="pipelines-1.13-for-rhel-8-x86_64-rpms"</programlisting>
</listitem>
<listitem>
<simpara>Linux on IBM Z&#174; and IBM&#174; LinuxONE (s390x)</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --enable="pipelines-1.13-for-rhel-8-s390x-rpms"</programlisting>
</listitem>
<listitem>
<simpara>Linux on IBM Power&#174; (ppc64le)</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --enable="pipelines-1.13-for-rhel-8-ppc64le-rpms"</programlisting>
</listitem>
<listitem>
<simpara>Linux on ARM (aarch64, arm64)</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --enable="pipelines-1.13-for-rhel-8-aarch64-rpms"</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Install the <literal>openshift-pipelines-client</literal> package:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum install openshift-pipelines-client</programlisting>
</listitem>
</orderedlist>
<simpara>After you install the CLI, it is available using the <literal>tkn</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn version</programlisting>
</section>
<section xml:id="installing-tkn-on-windows">
<title>Installing the Red Hat OpenShift Pipelines CLI on Windows</title>
<simpara role="_abstract">For Windows, you can download the CLI as a <literal>zip</literal> archive.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Download the <link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-windows-amd64.zip">CLI tool</link>.</simpara>
</listitem>
<listitem>
<simpara>Extract the archive with a ZIP program.</simpara>
</listitem>
<listitem>
<simpara>Add the location of your <literal>tkn</literal>, <literal>tkn-pac</literal>, and <literal>opc</literal> files to your <literal>PATH</literal> environment variable.</simpara>
</listitem>
<listitem>
<simpara>To check your <literal>PATH</literal>, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; path</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="installing-tkn-on-macos">
<title>Installing the Red Hat OpenShift Pipelines CLI on macOS</title>
<simpara role="_abstract">For macOS, you can download the CLI as a <literal>tar.gz</literal> archive.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Download the relevant CLI tool.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-macos-amd64.tar.gz">macOS</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://mirror.openshift.com/pub/openshift-v4/clients/pipelines/1.13.0/tkn-macos-arm64.tar.gz">macOS on ARM</link></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Unpack and extract the archive.</simpara>
</listitem>
<listitem>
<simpara>Add the location of your <literal>tkn</literal>, <literal>tkn-pac</literal>, and <literal>opc</literal> files to your <literal>PATH</literal> environment variable.</simpara>
</listitem>
<listitem>
<simpara>To check your <literal>PATH</literal>, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="op-configuring-tkn">
<title>Configuring the OpenShift Pipelines tkn CLI</title>

<simpara>Configure the Red Hat OpenShift Pipelines <literal>tkn</literal> CLI to enable tab completion.</simpara>
<section xml:id="op-tkn-enabling-tab-completion_configuring-tkn">
<title>Enabling tab completion</title>
<simpara>After you install the <literal>tkn</literal> CLI, you can enable tab completion to automatically complete <literal>tkn</literal> commands or suggest options when you press Tab.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <literal>tkn</literal> CLI tool installed.</simpara>
</listitem>
<listitem>
<simpara>You must have <literal>bash-completion</literal> installed on your local system.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>The following procedure enables tab completion for Bash.</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Save the Bash completion code to a file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn completion bash &gt; tkn_bash_completion</programlisting>
</listitem>
<listitem>
<simpara>Copy the file to <literal>/etc/bash_completion.d/</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo cp tkn_bash_completion /etc/bash_completion.d/</programlisting>
<simpara>Alternatively, you can save the file to a local directory and source it from your <literal>.bashrc</literal> file instead.</simpara>
</listitem>
</orderedlist>
<simpara>Tab completion is enabled when you open a new terminal.</simpara>
</section>
</section>
<section xml:id="op-tkn-reference">
<title>OpenShift Pipelines tkn reference</title>

<simpara>This section lists the basic <literal>tkn</literal> CLI commands.</simpara>
<section xml:id="_basic-syntax">
<title>Basic syntax</title>
<simpara><literal>tkn [command or options] [arguments&#8230;&#8203;]</literal></simpara>
</section>
<section xml:id="_global-options">
<title>Global options</title>
<simpara><literal>--help, -h</literal></simpara>
</section>
<section xml:id="op-tkn-utility-commands_op-tkn-reference">
<title>Utility commands</title>
<section xml:id="_tkn">
<title>tkn</title>
<simpara>Parent command for <literal>tkn</literal> CLI.</simpara>
<formalpara>
<title>Example: Display all options</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_completion-shell">
<title>completion [shell]</title>
<simpara>Print shell completion code which must be evaluated to provide interactive completion. Supported shells are <literal>bash</literal> and <literal>zsh</literal>.</simpara>
<formalpara>
<title>Example: Completion code for <literal>bash</literal> shell</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn completion bash</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_version">
<title>version</title>
<simpara>Print version information of the <literal>tkn</literal> CLI.</simpara>
<formalpara>
<title>Example: Check the <literal>tkn</literal> version</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn version</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-pipeline-management_op-tkn-reference">
<title>Pipelines management commands</title>
<section xml:id="_pipeline">
<title>pipeline</title>
<simpara>Manage pipelines.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipeline --help</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipeline-delete">
<title>pipeline delete</title>
<simpara>Delete a pipeline.</simpara>
<formalpara>
<title>Example: Delete the <literal>mypipeline</literal> pipeline from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipeline delete mypipeline -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipeline-describe">
<title>pipeline describe</title>
<simpara>Describe a pipeline.</simpara>
<formalpara>
<title>Example: Describe the <literal>mypipeline</literal> pipeline</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipeline describe mypipeline</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipeline-list">
<title>pipeline list</title>
<simpara>Display a list of pipelines.</simpara>
<formalpara>
<title>Example: Display a list of pipelines</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipeline list</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipeline-logs">
<title>pipeline logs</title>
<simpara>Display the logs for a specific pipeline.</simpara>
<formalpara>
<title>Example: Stream the live logs for the <literal>mypipeline</literal> pipeline</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipeline logs -f mypipeline</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipeline-start">
<title>pipeline start</title>
<simpara>Start a pipeline.</simpara>
<formalpara>
<title>Example: Start the <literal>mypipeline</literal> pipeline</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipeline start mypipeline</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-pipeline-run_op-tkn-reference">
<title>Pipeline run commands</title>
<section xml:id="_pipelinerun">
<title>pipelinerun</title>
<simpara>Manage pipeline runs.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipelinerun-cancel">
<title>pipelinerun cancel</title>
<simpara>Cancel a pipeline run.</simpara>
<formalpara>
<title>Example: Cancel the <literal>mypipelinerun</literal> pipeline run from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun cancel mypipelinerun -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipelinerun-delete">
<title>pipelinerun delete</title>
<simpara>Delete a pipeline run.</simpara>
<formalpara>
<title>Example: Delete pipeline runs from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun delete mypipelinerun1 mypipelinerun2 -n myspace</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example: Delete all pipeline runs from a namespace, except the five most recently executed pipeline runs</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun delete -n myspace --keep 5 <co xml:id="CO4-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO4-1">
<para>Replace <literal>5</literal> with the number of most recently executed pipeline runs you want to retain.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example: Delete all pipelines</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun delete --all</programlisting>
</para>
</formalpara>
<note>
<simpara>Starting with Red Hat OpenShift Pipelines 1.6, the <literal>tkn pipelinerun delete --all</literal> command does not delete any resources that are in the running state.</simpara>
</note>
</section>
<section xml:id="_pipelinerun-describe">
<title>pipelinerun describe</title>
<simpara>Describe a pipeline run.</simpara>
<formalpara>
<title>Example: Describe the <literal>mypipelinerun</literal> pipeline run in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun describe mypipelinerun -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipelinerun-list">
<title>pipelinerun list</title>
<simpara>List pipeline runs.</simpara>
<formalpara>
<title>Example: Display a list of pipeline runs in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun list -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_pipelinerun-logs">
<title>pipelinerun logs</title>
<simpara>Display the logs of a pipeline run.</simpara>
<formalpara>
<title>Example: Display the logs of the <literal>mypipelinerun</literal> pipeline run with all tasks and steps in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn pipelinerun logs mypipelinerun -a -n myspace</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-task-management_op-tkn-reference">
<title>Task management commands</title>
<section xml:id="_task">
<title>task</title>
<simpara>Manage tasks.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn task -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_task-delete">
<title>task delete</title>
<simpara>Delete a task.</simpara>
<formalpara>
<title>Example: Delete <literal>mytask1</literal> and <literal>mytask2</literal> tasks from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn task delete mytask1 mytask2 -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_task-describe">
<title>task describe</title>
<simpara>Describe a task.</simpara>
<formalpara>
<title>Example: Describe the <literal>mytask</literal> task in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn task describe mytask -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_task-list">
<title>task list</title>
<simpara>List tasks.</simpara>
<formalpara>
<title>Example: List all the tasks in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn task list -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_task-logs">
<title>task logs</title>
<simpara>Display task logs.</simpara>
<formalpara>
<title>Example: Display logs for the <literal>mytaskrun</literal> task run of the <literal>mytask</literal> task</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn task logs mytask mytaskrun -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_task-start">
<title>task start</title>
<simpara>Start a task.</simpara>
<formalpara>
<title>Example: Start the <literal>mytask</literal> task in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn task start mytask -s &lt;ServiceAccountName&gt; -n myspace</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-task-run_op-tkn-reference">
<title>Task run commands</title>
<section xml:id="_taskrun">
<title>taskrun</title>
<simpara>Manage task runs.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_taskrun-cancel">
<title>taskrun cancel</title>
<simpara>Cancel a task run.</simpara>
<formalpara>
<title>Example: Cancel the <literal>mytaskrun</literal> task run from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun cancel mytaskrun -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_taskrun-delete">
<title>taskrun delete</title>
<simpara>Delete a TaskRun.</simpara>
<formalpara>
<title>Example: Delete the <literal>mytaskrun1</literal> and <literal>mytaskrun2</literal> task runs from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun delete mytaskrun1 mytaskrun2 -n myspace</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example: Delete all but the five most recently executed task runs from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun delete -n myspace --keep 5 <co xml:id="CO5-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO5-1">
<para>Replace <literal>5</literal> with the number of most recently executed task runs you want to retain.</para>
</callout>
</calloutlist>
</section>
<section xml:id="_taskrun-describe">
<title>taskrun describe</title>
<simpara>Describe a task run.</simpara>
<formalpara>
<title>Example: Describe the <literal>mytaskrun</literal> task run in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun describe mytaskrun -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_taskrun-list">
<title>taskrun list</title>
<simpara>List task runs.</simpara>
<formalpara>
<title>Example: List all the task runs in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun list -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_taskrun-logs">
<title>taskrun logs</title>
<simpara>Display task run logs.</simpara>
<formalpara>
<title>Example: Display live logs for the <literal>mytaskrun</literal> task run in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn taskrun logs -f mytaskrun -n myspace</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-condition-management_op-tkn-reference">
<title>Condition management commands</title>
<section xml:id="_condition">
<title>condition</title>
<simpara>Manage Conditions.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn condition --help</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_condition-delete">
<title>condition delete</title>
<simpara>Delete a Condition.</simpara>
<formalpara>
<title>Example: Delete the <literal>mycondition1</literal> Condition from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn condition delete mycondition1 -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_condition-describe">
<title>condition describe</title>
<simpara>Describe a Condition.</simpara>
<formalpara>
<title>Example: Describe the <literal>mycondition1</literal> Condition in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn condition describe mycondition1 -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_condition-list">
<title>condition list</title>
<simpara>List Conditions.</simpara>
<formalpara>
<title>Example: List Conditions in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn condition list -n myspace</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-pipeline-resource-management_op-tkn-reference">
<title>Pipeline Resource management commands</title>
<section xml:id="_resource">
<title>resource</title>
<simpara>Manage Pipeline Resources.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn resource -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_resource-create">
<title>resource create</title>
<simpara>Create a Pipeline Resource.</simpara>
<formalpara>
<title>Example: Create a Pipeline Resource in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn resource create -n myspace</programlisting>
</para>
</formalpara>
<simpara>This is an interactive command that asks for input on the name of the Resource, type of the Resource, and the values based on the type of the Resource.</simpara>
</section>
<section xml:id="_resource-delete">
<title>resource delete</title>
<simpara>Delete a Pipeline Resource.</simpara>
<formalpara>
<title>Example: Delete the <literal>myresource</literal> Pipeline Resource from a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn resource delete myresource -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_resource-describe">
<title>resource describe</title>
<simpara>Describe a Pipeline Resource.</simpara>
<formalpara>
<title>Example: Describe the <literal>myresource</literal> Pipeline Resource</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn resource describe myresource -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_resource-list">
<title>resource list</title>
<simpara>List Pipeline Resources.</simpara>
<formalpara>
<title>Example: List all Pipeline Resources in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn resource list -n myspace</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-clustertask-management-commands_op-tkn-reference">
<title>ClusterTask management commands</title>
<important>
<simpara>In Red Hat OpenShift Pipelines 1.10, ClusterTask functionality of the <literal>tkn</literal> command line utility is deprecated and is planned to be removed in a future release.</simpara>
</important>
<section xml:id="_clustertask">
<title>clustertask</title>
<simpara>Manage ClusterTasks.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertask --help</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertask-delete">
<title>clustertask delete</title>
<simpara>Delete a ClusterTask resource in a cluster.</simpara>
<formalpara>
<title>Example: Delete <literal>mytask1</literal> and <literal>mytask2</literal> ClusterTasks</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertask delete mytask1 mytask2</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertask-describe">
<title>clustertask describe</title>
<simpara>Describe a ClusterTask.</simpara>
<formalpara>
<title>Example: Describe the <literal>mytask</literal> ClusterTask</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertask describe mytask1</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertask-list">
<title>clustertask list</title>
<simpara>List ClusterTasks.</simpara>
<formalpara>
<title>Example: List ClusterTasks</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertask list</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertask-start">
<title>clustertask start</title>
<simpara>Start ClusterTasks.</simpara>
<formalpara>
<title>Example: Start the <literal>mytask</literal> ClusterTask</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertask start mytask</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-trigger-management_op-tkn-reference">
<title>Trigger management commands</title>
<section xml:id="_eventlistener">
<title>eventlistener</title>
<simpara>Manage EventListeners.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn eventlistener -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_eventlistener-delete">
<title>eventlistener delete</title>
<simpara>Delete an EventListener.</simpara>
<formalpara>
<title>Example: Delete <literal>mylistener1</literal> and <literal>mylistener2</literal> EventListeners in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn eventlistener delete mylistener1 mylistener2 -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_eventlistener-describe">
<title>eventlistener describe</title>
<simpara>Describe an EventListener.</simpara>
<formalpara>
<title>Example: Describe the <literal>mylistener</literal> EventListener in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn eventlistener describe mylistener -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_eventlistener-list">
<title>eventlistener list</title>
<simpara>List EventListeners.</simpara>
<formalpara>
<title>Example: List all the EventListeners in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn eventlistener list -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_eventlistener-logs">
<title>eventlistener logs</title>
<simpara>Display logs of an EventListener.</simpara>
<formalpara>
<title>Example: Display the logs of the <literal>mylistener</literal> EventListener in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn eventlistener logs mylistener -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggerbinding">
<title>triggerbinding</title>
<simpara>Manage TriggerBindings.</simpara>
<formalpara>
<title>Example: Display TriggerBindings help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggerbinding -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggerbinding-delete">
<title>triggerbinding delete</title>
<simpara>Delete a TriggerBinding.</simpara>
<formalpara>
<title>Example: Delete <literal>mybinding1</literal> and <literal>mybinding2</literal> TriggerBindings in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggerbinding delete mybinding1 mybinding2 -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggerbinding-describe">
<title>triggerbinding describe</title>
<simpara>Describe a TriggerBinding.</simpara>
<formalpara>
<title>Example: Describe the <literal>mybinding</literal> TriggerBinding in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggerbinding describe mybinding -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggerbinding-list">
<title>triggerbinding list</title>
<simpara>List TriggerBindings.</simpara>
<formalpara>
<title>Example: List all the TriggerBindings in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggerbinding list -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggertemplate">
<title>triggertemplate</title>
<simpara>Manage TriggerTemplates.</simpara>
<formalpara>
<title>Example: Display TriggerTemplate help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggertemplate -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggertemplate-delete">
<title>triggertemplate delete</title>
<simpara>Delete a TriggerTemplate.</simpara>
<formalpara>
<title>Example: Delete <literal>mytemplate1</literal> and <literal>mytemplate2</literal> TriggerTemplates in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggertemplate delete mytemplate1 mytemplate2 -n `myspace`</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggertemplate-describe">
<title>triggertemplate describe</title>
<simpara>Describe a TriggerTemplate.</simpara>
<formalpara>
<title>Example: Describe the <literal>mytemplate</literal> TriggerTemplate in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggertemplate describe mytemplate -n `myspace`</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_triggertemplate-list">
<title>triggertemplate list</title>
<simpara>List TriggerTemplates.</simpara>
<formalpara>
<title>Example: List all the TriggerTemplates in a namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn triggertemplate list -n myspace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertriggerbinding">
<title>clustertriggerbinding</title>
<simpara>Manage ClusterTriggerBindings.</simpara>
<formalpara>
<title>Example: Display ClusterTriggerBindings help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertriggerbinding -h</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertriggerbinding-delete">
<title>clustertriggerbinding delete</title>
<simpara>Delete a ClusterTriggerBinding.</simpara>
<formalpara>
<title>Example: Delete <literal>myclusterbinding1</literal> and <literal>myclusterbinding2</literal> ClusterTriggerBindings</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertriggerbinding delete myclusterbinding1 myclusterbinding2</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertriggerbinding-describe">
<title>clustertriggerbinding describe</title>
<simpara>Describe a ClusterTriggerBinding.</simpara>
<formalpara>
<title>Example: Describe the <literal>myclusterbinding</literal> ClusterTriggerBinding</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertriggerbinding describe myclusterbinding</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_clustertriggerbinding-list">
<title>clustertriggerbinding list</title>
<simpara>List ClusterTriggerBindings.</simpara>
<formalpara>
<title>Example: List all ClusterTriggerBindings</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn clustertriggerbinding list</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="op-tkn-hub-interaction_op-tkn-reference">
<title>Hub interaction commands</title>
<simpara>Interact with Tekton Hub for resources such as tasks and pipelines.</simpara>
<section xml:id="_hub">
<title>hub</title>
<simpara>Interact with hub.</simpara>
<formalpara>
<title>Example: Display help</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub -h</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example: Interact with a hub API server</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub --api-server https://api.hub.tekton.dev</programlisting>
</para>
</formalpara>
<note>
<simpara>For each example, to get the corresponding sub-commands and flags, run <literal>tkn hub &lt;command&gt; --help</literal>.</simpara>
</note>
</section>
<section xml:id="_hub-downgrade">
<title>hub downgrade</title>
<simpara>Downgrade an installed resource.</simpara>
<formalpara>
<title>Example: Downgrade the <literal>mytask</literal> task in the <literal>mynamespace</literal> namespace to it&#8217;s older version</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub downgrade task mytask --to version -n mynamespace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_hub-get">
<title>hub get</title>
<simpara>Get a resource manifest by its name, kind, catalog, and version.</simpara>
<formalpara>
<title>Example: Get the manifest for a specific version of the <literal>myresource</literal> pipeline or task from the <literal>tekton</literal> catalog</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub get [pipeline | task] myresource --from tekton --version version</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_hub-info">
<title>hub info</title>
<simpara>Display information about a resource by its name, kind, catalog, and version.</simpara>
<formalpara>
<title>Example: Display information about a specific version of the <literal>mytask</literal> task from the <literal>tekton</literal> catalog</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub info task mytask --from tekton --version version</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_hub-install">
<title>hub install</title>
<simpara>Install a resource from a catalog by its kind, name, and version.</simpara>
<formalpara>
<title>Example: Install a specific version of the <literal>mytask</literal> task from the <literal>tekton</literal> catalog in the <literal>mynamespace</literal> namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub install task mytask --from tekton --version version -n mynamespace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_hub-reinstall">
<title>hub reinstall</title>
<simpara>Reinstall a resource by its kind and name.</simpara>
<formalpara>
<title>Example: Reinstall a specific version of the <literal>mytask</literal> task from the <literal>tekton</literal> catalog in the <literal>mynamespace</literal> namespace</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub reinstall task mytask --from tekton --version version -n mynamespace</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_hub-search">
<title>hub search</title>
<simpara>Search a resource by a combination of name, kind, and tags.</simpara>
<formalpara>
<title>Example: Search a resource with a tag <literal>cli</literal></title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub search --tags cli</programlisting>
</para>
</formalpara>
</section>
<section xml:id="_hub-upgrade">
<title>hub upgrade</title>
<simpara>Upgrade an installed resource.</simpara>
<formalpara>
<title>Example: Upgrade the installed <literal>mytask</literal> task in the <literal>mynamespace</literal> namespace to a new version</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ tkn hub upgrade task mytask --to version -n mynamespace</programlisting>
</para>
</formalpara>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_opm-cli">
<title>opm CLI</title>
<section xml:id="cli-opm-install">
<title>Installing the opm CLI</title>

<section xml:id="olm-about-opm_cli-opm-install">
<title>About the opm CLI</title>
<simpara>The <literal>opm</literal> CLI tool is provided by the Operator Framework for use with the Operator bundle format. This tool allows you to create and maintain catalogs of Operators from a list of Operator bundles that are similar to software repositories. The result is a container image which can be stored in a container registry and then installed on a cluster.</simpara>
<simpara>A catalog contains a database of pointers to Operator manifest content that can be queried through an included API that is served when the container image is run. On OpenShift Container Platform, Operator Lifecycle Manager (OLM) can reference the image in a catalog source, defined by a <literal>CatalogSource</literal> object, which polls the image at regular intervals to enable frequent updates to installed Operators on the cluster.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-bundle-format_olm-packaging-format">Operator Framework packaging format</link> for more information about the bundle format.</simpara>
</listitem>
<listitem>
<simpara>To create a bundle image using the Operator SDK, see
<link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#osdk-working-bundle-images">Working with bundle images</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="olm-installing-opm_cli-opm-install">
<title>Installing the opm CLI</title>
<simpara>You can install the <literal>opm</literal> CLI tool on your Linux, macOS, or Windows workstation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>For Linux, you must provide the following packages. RHEL 8 meets these requirements:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>podman</literal> version 1.9.3+ (version 2.0+ recommended)</simpara>
</listitem>
<listitem>
<simpara><literal>glibc</literal> version 2.28+</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <link xlink:href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/ocp/latest-4.14/">OpenShift mirror site</link> and download the latest version of the tarball that matches your operating system.</simpara>
</listitem>
<listitem>
<simpara>Unpack the archive.</simpara>
<itemizedlist>
<listitem>
<simpara>For Linux or macOS:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvf &lt;file&gt;</programlisting>
</listitem>
<listitem>
<simpara>For Windows, unzip the archive with a ZIP program.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Place the file anywhere in your <literal>PATH</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>For Linux or macOS:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check your <literal>PATH</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
<listitem>
<simpara>Move the file. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv ./opm /usr/local/bin/</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For Windows:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check your <literal>PATH</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; path</programlisting>
</listitem>
<listitem>
<simpara>Move the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; move opm.exe &lt;directory&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After you install the <literal>opm</literal> CLI, verify that it is available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ opm version</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="opm-addtl-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-managing-custom-catalogs">Managing custom catalogs</link> for <literal>opm</literal> procedures including creating, updating, and pruning catalogs.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cli-opm-ref">
<title>opm CLI reference</title>

<simpara>The <literal>opm</literal> command-line interface (CLI) is a tool for creating and maintaining Operator catalogs.</simpara>
<formalpara>
<title><literal>opm</literal> CLI syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm &lt;command&gt; [&lt;subcommand&gt;] [&lt;argument&gt;] [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title>Global flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-skip-tls-verify</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Skip TLS certificate verification for container image registries while pulling bundles or indexes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--use-http</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When you pull bundles, use plain HTTP for container image registries.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<important>
<simpara>The SQLite-based catalog format, including the related CLI commands, is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.</simpara>
<simpara>For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <emphasis>Deprecated and removed features</emphasis> section of the OpenShift Container Platform release notes.</simpara>
</important>
<section xml:id="opm-cli-ref-generate_cli-opm-ref">
<title>generate</title>
<simpara>Generate various artifacts for declarative config indexes.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm generate &lt;subcommand&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>generate</literal> subcommands</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Subcommand</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>dockerfile</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Generate a Dockerfile for a declarative config index.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>generate</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flags</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help for generate.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="opm-cli-ref-generate-dockerfile_cli-opm-ref">
<title>dockerfile</title>
<simpara>Generate a Dockerfile for a declarative config index.</simpara>
<important>
<simpara>This command creates a Dockerfile in the same directory as the <literal>&lt;dcRootDir&gt;</literal> (named <literal>&lt;dcDirName&gt;.Dockerfile</literal>) that is used to build the index. If a Dockerfile with the same name already exists, this command fails.</simpara>
<simpara>When specifying extra labels, if duplicate keys exist, only the last value of each duplicate key gets added to the generated Dockerfile.</simpara>
</important>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm generate dockerfile &lt;dcRootDir&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>generate dockerfile</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-i,</literal> <literal>--binary-image</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Image in which to build catalog. The default value is <literal>quay.io/operator-framework/opm:latest</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-l</literal>, <literal>--extra-labels</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Extra labels to include in the generated Dockerfile. Labels have the form <literal>key=value</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help for Dockerfile.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>To build with the official Red Hat image, use the <literal>registry.redhat.io/openshift4/ose-operator-registry:v4.14</literal> value with the <literal>-i</literal> flag.</simpara>
</note>
</section>
</section>
<section xml:id="opm-cli-ref-index_cli-opm-ref">
<title>index</title>
<simpara>Generate Operator index for SQLite database format container images from pre-existing Operator bundles.</simpara>
<important>
<simpara>As of OpenShift Container Platform 4.11, the default Red Hat-provided Operator catalog releases in the file-based catalog format. The default Red Hat-provided Operator catalogs for OpenShift Container Platform 4.6 through 4.10 released in the deprecated SQLite database format.</simpara>
<simpara>The <literal>opm</literal> subcommands, flags, and functionality related to the SQLite database format are also deprecated and will be removed in a future release. The features are still supported and must be used for catalogs that use the deprecated SQLite database format.</simpara>
<simpara>Many of the <literal>opm</literal> subcommands and flags for working with the SQLite database format, such as <literal>opm index prune</literal>, do not work with the file-based catalog format.</simpara>
<simpara>For more information about working with file-based catalogs, see "Additional resources".</simpara>
</important>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm index &lt;subcommand&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>index</literal> subcommands</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Subcommand</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>add</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Add Operator bundles to an index.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>prune</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Prune an index of all but specified packages.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>prune-stranded</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Prune an index of stranded bundles, which are bundles that are not associated with a particular image.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>rm</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Delete an entire Operator from an index.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="opm-cli-ref-index-add_cli-opm-ref">
<title>add</title>
<simpara>Add Operator bundles to an index.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm index add [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>index add</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-i</literal>, <literal>--binary-image</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Container image for on-image <literal>opm</literal> command</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-u</literal>, <literal>--build-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to build container images: <literal>podman</literal> (the default value) or <literal>docker</literal>. Overrides part of the <literal>--container-tool</literal> flag.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-b</literal>, <literal>--bundles</literal> (strings)</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of bundles to add.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-c</literal>, <literal>--container-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to interact with container images, such as for saving and building: <literal>docker</literal> or <literal>podman</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-f</literal>, <literal>--from-index</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Previous index to add to.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--generate</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If enabled, only creates the Dockerfile and saves it to local disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--mode</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Graph update mode that defines how channel graphs are updated: <literal>replaces</literal> (the default value), <literal>semver</literal>, or <literal>semver-skippatch</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-d</literal>, <literal>--out-dockerfile</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Optional: If generating the Dockerfile, specify a file name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--permissive</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allow registry load errors.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-p</literal>, <literal>--pull-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to pull container images: <literal>none</literal> (the default value), <literal>docker</literal>, or <literal>podman</literal>. Overrides part of the <literal>--container-tool</literal> flag.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-t</literal>, <literal>--tag</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Custom tag for container image being built.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-index-prune_cli-opm-ref">
<title>prune</title>
<simpara>Prune an index of all but specified packages.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm index prune [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>index prune</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-i</literal>, <literal>--binary-image</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Container image for on-image <literal>opm</literal> command</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-c</literal>, <literal>--container-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to interact with container images, such as for saving and building: <literal>docker</literal> or <literal>podman</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-f</literal>, <literal>--from-index</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Index to prune.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--generate</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If enabled, only creates the Dockerfile and saves it to local disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-d</literal>, <literal>--out-dockerfile</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Optional: If generating the Dockerfile, specify a file name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-p</literal>, <literal>--packages</literal> (strings)</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of packages to keep.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--permissive</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allow registry load errors.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-t</literal>, <literal>--tag</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Custom tag for container image being built.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-index-prune-stranded_cli-opm-ref">
<title>prune-stranded</title>
<simpara>Prune an index of stranded bundles, which are bundles that are not associated with a particular image.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm index prune-stranded [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>index prune-stranded</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-i</literal>, <literal>--binary-image</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Container image for on-image <literal>opm</literal> command</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-c</literal>, <literal>--container-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to interact with container images, such as for saving and building: <literal>docker</literal> or <literal>podman</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-f</literal>, <literal>--from-index</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Index to prune.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--generate</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If enabled, only creates the Dockerfile and saves it to local disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-d</literal>, <literal>--out-dockerfile</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Optional: If generating the Dockerfile, specify a file name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-p</literal>, <literal>--packages</literal> (strings)</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of packages to keep.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--permissive</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allow registry load errors.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-t</literal>, <literal>--tag</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Custom tag for container image being built.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-index-rm_cli-opm-ref">
<title>rm</title>
<simpara>Delete an entire Operator from an index.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm index rm [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>index rm</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-i</literal>, <literal>--binary-image</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Container image for on-image <literal>opm</literal> command</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-u</literal>, <literal>--build-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to build container images: <literal>podman</literal> (the default value) or <literal>docker</literal>. Overrides part of the <literal>--container-tool</literal> flag.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-c</literal>, <literal>--container-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to interact with container images, such as for saving and building: <literal>docker</literal> or <literal>podman</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-f</literal>, <literal>--from-index</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Previous index to delete from.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--generate</literal></simpara></entry>
<entry align="left" valign="top"><simpara>If enabled, only creates the Dockerfile and saves it to local disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-o</literal>, <literal>--operators</literal> (strings)</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of Operators to delete.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-d</literal>, <literal>--out-dockerfile</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Optional: If generating the Dockerfile, specify a file name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-p</literal>, <literal>--packages</literal> (strings)</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of packages to keep.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--permissive</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Allow registry load errors.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-p</literal>, <literal>--pull-tool</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to pull container images: <literal>none</literal> (the default value), <literal>docker</literal>, or <literal>podman</literal>. Overrides part of the <literal>--container-tool</literal> flag.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-t</literal>, <literal>--tag</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Custom tag for container image being built.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-file-based-catalogs_olm-packaging-format">Operator Framework packaging format</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-managing-custom-catalogs-fb">Managing custom catalogs</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-mirroring-disconnected">Mirroring images for a disconnected installation using the oc-mirror plugin</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="opm-cli-ref-init_cli-opm-ref">
<title>init</title>
<simpara>Generate an <literal>olm.package</literal> declarative config blob.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm init &lt;package_name&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>init</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-c</literal>, <literal>--default-channel</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>The channel that subscriptions will default to if unspecified.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-d</literal>, <literal>--description</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to the Operator&#8217;s <literal>README.md</literal> or other documentation.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-i</literal>, <literal>--icon</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to package&#8217;s icon.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-o</literal>, <literal>--output</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Output format: <literal>json</literal> (the default value) or <literal>yaml</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-migrate_cli-opm-ref">
<title>migrate</title>
<simpara>Migrate a SQLite database format index image or database file to a file-based catalog.</simpara>
<important>
<simpara>The SQLite-based catalog format, including the related CLI commands, is a deprecated feature. Deprecated functionality is still included in OpenShift Container Platform and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.</simpara>
<simpara>For the most recent list of major functionality that has been deprecated or removed within OpenShift Container Platform, refer to the <emphasis>Deprecated and removed features</emphasis> section of the OpenShift Container Platform release notes.</simpara>
</important>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm migrate &lt;index_ref&gt; &lt;output_dir&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>migrate</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-o</literal>, <literal>--output</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Output format: <literal>json</literal> (the default value) or <literal>yaml</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-render_cli-opm-ref">
<title>render</title>
<simpara>Generate a declarative config blob from the provided index images, bundle images, and SQLite database files.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm render &lt;index_image | bundle_image | sqlite_file&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>render</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-o</literal>, <literal>--output</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Output format: <literal>json</literal> (the default value) or <literal>yaml</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-server_cli-opm-ref">
<title>serve</title>
<simpara>Serve declarative configs via a GRPC server.</simpara>
<note>
<simpara>The declarative config directory is loaded by the <literal>serve</literal> command at startup. Changes made to the declarative config after this command starts are not reflected in the served content.</simpara>
</note>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm serve &lt;source_path&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>serve</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--cache-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>If this flag is set, it syncs and persists the server cache directory.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--cache-enforce-integrity</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Exits with an error if the cache is not present or is invalidated. The default value is <literal>true</literal> when the <literal>--cache-dir</literal> flag is set and the <literal>--cache-only</literal> flag is <literal>false</literal>. Otherwise, the default is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--cache-only</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Syncs the serve cache and exits without serving.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--debug</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Enables debug logging.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help for serve.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-p</literal>, <literal>--port</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>The port number for the service. The default value is <literal>50051</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--pprof-addr</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>The address of the startup profiling endpoint. The format is <literal>Addr:Port</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-t</literal>, <literal>--termination-log</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>The path to a container termination log file. The default value is <literal>/dev/termination-log</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="opm-cli-ref-validate_cli-opm-ref">
<title>validate</title>
<simpara>Validate the declarative config JSON file(s) in a given directory.</simpara>
<formalpara>
<title>Command syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ opm validate &lt;directory&gt; [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
</section>
</section>
</chapter>
<chapter xml:id="_operator-sdk">
<title>Operator SDK</title>
<section xml:id="cli-osdk-install">
<title>Installing the Operator SDK CLI</title>

<simpara>The Operator SDK provides a command-line interface (CLI) tool that Operator developers can use to build, test, and deploy an Operator. You can install the Operator SDK CLI on your workstation so that you are prepared to start authoring your own Operators.</simpara>
<simpara>Operator authors with cluster administrator access to a Kubernetes-based cluster, such as OpenShift Container Platform, can use the Operator SDK CLI to develop their own Operators based on Go, Ansible, Java, or Helm. <link xlink:href="https://kubebuilder.io/">Kubebuilder</link> is embedded into the Operator SDK as the scaffolding solution for Go-based Operators, which means existing Kubebuilder projects can be used as is with the Operator SDK and continue to work.
See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#osdk-about">Developing Operators</link> for full documentation on the Operator SDK.</simpara>
<note>
<simpara>OpenShift Container Platform 4.14 supports Operator SDK 1.31.0.</simpara>
</note>
<section xml:id="osdk-installing-cli-linux-macos_cli-osdk-install">
<title>Installing the Operator SDK CLI on Linux</title>
<simpara>You can install the OpenShift SDK CLI tool on Linux.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><link xlink:href="https://golang.org/dl/">Go</link> v1.19+</simpara>
</listitem>
<listitem>
<simpara><literal>docker</literal> v17.03+, <literal>podman</literal> v1.9.3+, or <literal>buildah</literal> v1.7+</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <link xlink:href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/operator-sdk/">OpenShift mirror site</link>.</simpara>
</listitem>
<listitem>
<simpara>From the latest 4.14 directory, download the latest version of the tarball for Linux.</simpara>
</listitem>
<listitem>
<simpara>Unpack the archive:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvf operator-sdk-v1.31.0-ocp-linux-x86_64.tar.gz</programlisting>
</listitem>
<listitem>
<simpara>Make the file executable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chmod +x operator-sdk</programlisting>
</listitem>
<listitem>
<simpara>Move the extracted <literal>operator-sdk</literal> binary to a directory that is on your <literal>PATH</literal>.</simpara>
<tip>
<simpara>To check your <literal>PATH</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</tip>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv ./operator-sdk /usr/local/bin/operator-sdk</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After you install the Operator SDK CLI, verify that it is available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ operator-sdk version</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">operator-sdk version: "v1.31.0-ocp", ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="osdk-installing-cli-macos_cli-osdk-install">
<title>Installing the Operator SDK CLI on macOS</title>
<simpara>You can install the OpenShift SDK CLI tool on macOS.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><link xlink:href="https://golang.org/dl/">Go</link> v1.19+</simpara>
</listitem>
<listitem>
<simpara><literal>docker</literal> v17.03+, <literal>podman</literal> v1.9.3+, or <literal>buildah</literal> v1.7+</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>For the <literal>amd64</literal> and <literal>arm64</literal> architectures, navigate to the <link xlink:href="https://mirror.openshift.com/pub/openshift-v4/x86_64/clients/operator-sdk/">OpenShift mirror site for the <literal>amd64</literal> architecture</link> and <link xlink:href="https://mirror.openshift.com/pub/openshift-v4/arm64/clients/operator-sdk/">OpenShift mirror site for the <literal>arm64</literal> architecture</link> respectively.</simpara>
</listitem>
<listitem>
<simpara>From the latest 4.14 directory, download the latest version of the tarball for macOS.</simpara>
</listitem>
<listitem>
<simpara>Unpack the Operator SDK archive for <literal>amd64</literal> architecture by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvf operator-sdk-v1.31.0-ocp-darwin-x86_64.tar.gz</programlisting>
</listitem>
<listitem>
<simpara>Unpack the Operator SDK archive for <literal>arm64</literal> architecture by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar xvf operator-sdk-v1.31.0-ocp-darwin-aarch64.tar.gz</programlisting>
</listitem>
<listitem>
<simpara>Make the file executable by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chmod +x operator-sdk</programlisting>
</listitem>
<listitem>
<simpara>Move the extracted <literal>operator-sdk</literal> binary to a directory that is on your <literal>PATH</literal> by running the following command:</simpara>
<tip>
<simpara>Check your <literal>PATH</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</tip>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv ./operator-sdk /usr/local/bin/operator-sdk</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After you install the Operator SDK CLI, verify that it is available by running the following command::</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ operator-sdk version</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">operator-sdk version: "v1.31.0-ocp", ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="cli-osdk-ref">
<title>Operator SDK CLI reference</title>

<simpara>The Operator SDK command-line interface (CLI) is a development kit designed to make writing Operators easier.</simpara>
<formalpara>
<title>Operator SDK CLI syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ operator-sdk &lt;command&gt; [&lt;subcommand&gt;] [&lt;argument&gt;] [&lt;flags&gt;]</programlisting>
</para>
</formalpara>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#osdk-about">Developing Operators</link> for full documentation on the Operator SDK.</simpara>
<section xml:id="osdk-cli-ref-bundle_cli-osdk-ref">
<title>bundle</title>
<simpara>The <literal>operator-sdk bundle</literal> command manages Operator bundle metadata.</simpara>
<section xml:id="osdk-cli-ref-bundle-validate_cli-osdk-ref">
<title>validate</title>
<simpara>The <literal>bundle validate</literal> subcommand validates an Operator bundle.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>bundle validate</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>bundle validate</literal> subcommand.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--index-builder</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Tool to pull and unpack bundle images. Only used when validating a bundle image. Available options are <literal>docker</literal>, which is the default, <literal>podman</literal>, or <literal>none</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--list-optional</literal></simpara></entry>
<entry align="left" valign="top"><simpara>List all optional validators available. When set, no validators are run.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--select-optional</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Label selector to select optional validators to run. When run with the <literal>--list-optional</literal> flag, lists available optional validators.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="osdk-cli-ref-cleanup_cli-osdk-ref">
<title>cleanup</title>
<simpara>The <literal>operator-sdk cleanup</literal> command destroys and removes resources that were created for an Operator that was deployed with the <literal>run</literal> command.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>cleanup</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>run bundle</literal> subcommand.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--kubeconfig</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to the <literal>kubeconfig</literal> file to use for CLI requests.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-n</literal>, <literal>--namespace</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>If present, namespace in which to run the CLI request.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--timeout &lt;duration&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Time to wait for the command to complete before failing. The default value is <literal>2m0s</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="osdk-cli-ref-completion_cli-osdk-ref">
<title>completion</title>
<simpara>The <literal>operator-sdk completion</literal> command generates shell completions to make issuing CLI commands quicker and easier.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>completion</literal> subcommands</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Subcommand</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>bash</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Generate bash completions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>zsh</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Generate zsh completions.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title><literal>completion</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-h, --help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Usage help output.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ operator-sdk completion bash</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># bash completion for operator-sdk                         -*- shell-script -*-
...
# ex: ts=4 sw=4 et filetype=sh</programlisting>
</para>
</formalpara>
</section>
<section xml:id="osdk-cli-ref-create_cli-osdk-ref">
<title>create</title>
<simpara>The <literal>operator-sdk create</literal> command is used to create, or <emphasis>scaffold</emphasis>, a Kubernetes API.</simpara>
<section xml:id="osdk-cli-ref-create-api_cli-osdk-ref">
<title>api</title>
<simpara>The <literal>create api</literal> subcommand scaffolds a Kubernetes API. The subcommand must be run in a project that was initialized with the <literal>init</literal> command.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>create api</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>run bundle</literal> subcommand.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="osdk-cli-ref-generate_cli-osdk-ref">
<title>generate</title>
<simpara>The <literal>operator-sdk generate</literal> command invokes a specific generator to generate code or manifests.</simpara>
<section xml:id="osdk-cli-ref-generate-bundle_cli-osdk-ref">
<title>bundle</title>
<simpara>The <literal>generate bundle</literal> subcommand generates a set of bundle manifests, metadata, and a <literal>bundle.Dockerfile</literal> file for your Operator project.</simpara>
<note>
<simpara>Typically, you run the <literal>generate kustomize manifests</literal> subcommand first to generate the input <link xlink:href="https://kustomize.io/">Kustomize</link> bases that are used by the <literal>generate bundle</literal> subcommand. However, you can use the <literal>make bundle</literal> command in an initialized project to automate running these commands in sequence.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title><literal>generate bundle</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--channels</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Comma-separated list of channels to which the bundle belongs. The default value is <literal>alpha</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--crds-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Root directory for <literal>CustomResoureDefinition</literal> manifests.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--default-channel</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>The default channel for the bundle.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--deploy-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Root directory for Operator manifests, such as deployments and RBAC. This directory is different from the directory passed to the <literal>--input-dir</literal> flag.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help for <literal>generate bundle</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--input-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Directory from which to read an existing bundle. This directory is the parent of your bundle <literal>manifests</literal> directory and is different from the <literal>--deploy-dir</literal> directory.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--kustomize-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Directory containing Kustomize bases and a <literal>kustomization.yaml</literal> file for bundle manifests. The default path is <literal>config/manifests</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--manifests</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Generate bundle manifests.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--metadata</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Generate bundle metadata and Dockerfile.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--output-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Directory to write the bundle to.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--overwrite</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Overwrite the bundle metadata and Dockerfile if they exist. The default value is <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--package</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Package name for the bundle.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-q</literal>, <literal>--quiet</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Run in quiet mode.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--stdout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Write bundle manifest to standard out.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--version</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Semantic version of the Operator in the generated bundle. Set only when creating a new bundle or upgrading the Operator.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#osdk-bundle-deploy-olm_osdk-working-bundle-images">Bundling an Operator and deploying with Operator Lifecycle Manager</link> for a full procedure that includes using the <literal>make bundle</literal> command to call the <literal>generate bundle</literal> subcommand.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="osdk-cli-ref-generate-kustomize_cli-osdk-ref">
<title>kustomize</title>
<simpara>The <literal>generate kustomize</literal> subcommand contains subcommands that generate <link xlink:href="https://kustomize.io/">Kustomize</link> data for the Operator.</simpara>
<section xml:id="osdk-cli-ref-generate-kustomize-manifests_cli-osdk-ref">
<title>manifests</title>
<simpara>The <literal>generate kustomize manifests</literal> subcommand generates or regenerates Kustomize bases and a <literal>kustomization.yaml</literal> file in the <literal>config/manifests</literal> directory, which are used to build bundle manifests by other Operator SDK commands. This command interactively asks for UI metadata, an important component of manifest bases, by default unless a base already exists or you set the <literal>--interactive=false</literal> flag.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>generate kustomize manifests</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--apis-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Root directory for API type definitions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help for <literal>generate kustomize manifests</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--input-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Directory containing existing Kustomize files.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--interactive</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>false</literal>, if no Kustomize base exists, an interactive command prompt is presented to accept custom metadata.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--output-dir</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Directory where to write Kustomize files.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--package</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Package name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-q</literal>, <literal>--quiet</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Run in quiet mode.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</section>
<section xml:id="osdk-cli-ref-init_cli-osdk-ref">
<title>init</title>
<simpara>The <literal>operator-sdk init</literal> command initializes an Operator project and generates, or <emphasis>scaffolds</emphasis>, a default project directory layout for the given plugin.</simpara>
<simpara>This command writes the following files:</simpara>
<itemizedlist>
<listitem>
<simpara>Boilerplate license file</simpara>
</listitem>
<listitem>
<simpara><literal>PROJECT</literal> file with the domain and repository</simpara>
</listitem>
<listitem>
<simpara><literal>Makefile</literal> to build the project</simpara>
</listitem>
<listitem>
<simpara><literal>go.mod</literal> file with project dependencies</simpara>
</listitem>
<listitem>
<simpara><literal>kustomization.yaml</literal> file for customizing manifests</simpara>
</listitem>
<listitem>
<simpara>Patch file for customizing images for manager manifests</simpara>
</listitem>
<listitem>
<simpara>Patch file for enabling Prometheus metrics</simpara>
</listitem>
<listitem>
<simpara><literal>main.go</literal> file to run</simpara>
</listitem>
</itemizedlist>
<table frame="all" rowsep="1" colsep="1">
<title><literal>init</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--help, -h</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>init</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--plugins</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Name and optionally version of the plugin to initialize the project with. Available plugins are <literal>ansible.sdk.operatorframework.io/v1</literal>, <literal>go.kubebuilder.io/v2</literal>, <literal>go.kubebuilder.io/v3</literal>, and <literal>helm.sdk.operatorframework.io/v1</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--project-version</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Project version. Available values are <literal>2</literal> and <literal>3-alpha</literal>, which is the default.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="osdk-cli-ref-run_cli-osdk-ref">
<title>run</title>
<simpara>The <literal>operator-sdk run</literal> command provides options that can launch the Operator in various environments.</simpara>
<section xml:id="osdk-cli-ref-run-bundle_cli-osdk-ref">
<title>bundle</title>
<simpara>The <literal>run bundle</literal> subcommand deploys an Operator in the bundle format with Operator Lifecycle Manager (OLM).</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>run bundle</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--index-image</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Index image in which to inject a bundle. The default image is <literal>quay.io/operator-framework/upstream-opm-builder:latest</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--install-mode &lt;install_mode_value&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Install mode supported by the cluster service version (CSV) of the Operator, for example <literal>AllNamespaces</literal> or <literal>SingleNamespace</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--timeout &lt;duration&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Install timeout. The default value is <literal>2m0s</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--kubeconfig</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to the <literal>kubeconfig</literal> file to use for CLI requests.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-n</literal>, <literal>--namespace</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>If present, namespace in which to run the CLI request.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--security-context-config &lt;security_context&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the security context to use for the catalog pod. Allowed values include <literal>restricted</literal> and <literal>legacy</literal>. The default value is <literal>legacy</literal>. <superscript>[1]</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>run bundle</literal> subcommand.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>The <literal>restricted</literal> security context is not compatible with the <literal>default</literal> namespace. To configure your Operator&#8217;s pod security admission in your production environment, see "Complying with pod security admission". For more information about pod security admission, see "Understanding and managing pod security admission".</simpara>
</listitem>
</orderedlist>
</para>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-operatorgroups-membership_olm-understanding-operatorgroups">Operator group membership</link> for details on possible install modes.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="osdk-cli-ref-run-bundle-upgrade_cli-osdk-ref">
<title>bundle-upgrade</title>
<simpara>The <literal>run bundle-upgrade</literal> subcommand upgrades an Operator that was previously installed in the bundle format with Operator Lifecycle Manager (OLM).</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>run bundle-upgrade</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--timeout &lt;duration&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Upgrade timeout. The default value is <literal>2m0s</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--kubeconfig</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to the <literal>kubeconfig</literal> file to use for CLI requests.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-n</literal>, <literal>--namespace</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>If present, namespace in which to run the CLI request.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--security-context-config &lt;security_context&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the security context to use for the catalog pod. Allowed values include <literal>restricted</literal> and <literal>legacy</literal>. The default value is <literal>legacy</literal>. <superscript>[1]</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>run bundle</literal> subcommand.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>The <literal>restricted</literal> security context is not compatible with the <literal>default</literal> namespace. To configure your Operator&#8217;s pod security admission in your production environment, see "Complying with pod security admission". For more information about pod security admission, see "Understanding and managing pod security admission".</simpara>
</listitem>
</orderedlist>
</para>
</section>
</section>
<section xml:id="osdk-cli-ref-scorecard_cli-osdk-ref">
<title>scorecard</title>
<simpara>The <literal>operator-sdk scorecard</literal> command runs the scorecard tool to validate an Operator bundle and provide suggestions for improvements. The command takes one argument, either a bundle image or directory containing manifests and metadata. If the argument holds an image tag, the image must be present remotely.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><literal>scorecard</literal> flags</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Flag</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>-c</literal>, <literal>--config</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to scorecard configuration file. The default path is <literal>bundle/tests/scorecard/config.yaml</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-h</literal>, <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Help output for the <literal>scorecard</literal> command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--kubeconfig</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Path to <literal>kubeconfig</literal> file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-L</literal>, <literal>--list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>List which tests are available to run.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-n</literal>, --namespace (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Namespace in which to run the test images.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-o</literal>, <literal>--output</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Output format for results. Available values are <literal>text</literal>, which is the default, and <literal>json</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--pod-security &lt;security_context&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Option to run scorecard with the specified security context. Allowed values include <literal>restricted</literal> and <literal>legacy</literal>. The default value is <literal>legacy</literal>. <superscript>[1]</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-l</literal>, <literal>--selector</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Label selector to determine which tests are run.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-s</literal>, <literal>--service-account</literal> (string)</simpara></entry>
<entry align="left" valign="top"><simpara>Service account to use for tests. The default value is <literal>default</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-x</literal>, <literal>--skip-cleanup</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Disable resource cleanup after tests are run.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-w</literal>, <literal>--wait-time &lt;duration&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Seconds to wait for tests to complete, for example <literal>35s</literal>. The default value is <literal>30s</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>The <literal>restricted</literal> security context is not compatible with the <literal>default</literal> namespace. To configure your Operator&#8217;s pod security admission in your production environment, see "Complying with pod security admission". For more information about pod security admission, see "Understanding and managing pod security admission".</simpara>
</listitem>
</orderedlist>
</para>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#osdk-scorecard">Validating Operators using the scorecard tool</link> for details about running the scorecard tool.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
</book>
<?xml version="1.0" encoding="UTF-8"?>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Monitoring</title>
<date>2024-02-13</date>
</info>
<chapter xml:id="monitoring-overview">
<title>Monitoring overview</title>

<section xml:id="about-openshift-monitoring">
<title>About {product-title} monitoring</title>
<simpara role="_abstract">{product-title} includes a preconfigured, preinstalled, and self-updating monitoring stack that provides monitoring for core platform components. You also have the option to <link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">enable monitoring for user-defined projects</link>.</simpara>
<simpara>A cluster administrator can <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#configuring-the-monitoring-stack">configure the monitoring stack</link> with the supported configurations. {product-title} delivers monitoring best practices out of the box.</simpara>
<simpara>A set of alerts are included by default that immediately notify administrators about issues with a cluster. Default dashboards in the {product-title} web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster. With the {product-title} web console, you can <link xl:href="../monitoring/managing-metrics.xml#managing-metrics">view and manage metrics</link>, <link xl:href="../monitoring/managing-alerts.xml#managing-alerts">alerts</link>, and <link xl:href="../monitoring/reviewing-monitoring-dashboards.xml#reviewing-monitoring-dashboards">review monitoring dashboards</link>.</simpara>
<simpara>In the <emphasis role="strong">Observe</emphasis> section of {product-title} web console, you can access and manage monitoring features such as <link xl:href="../monitoring/managing-metrics.xml#managing-metrics">metrics</link>, <link xl:href="../monitoring/managing-alerts.xml#managing-alerts">alerts</link>, <link xl:href="../monitoring/reviewing-monitoring-dashboards.xml#reviewing-monitoring-dashboards">monitoring dashboards</link>, and <link xl:href="../monitoring/managing-metrics.xml#getting-detailed-information-about-a-target_managing-metrics">metrics targets</link>.</simpara>
<simpara>After installing {product-title}, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects.
As a cluster administrator, you can find answers to common problems such as user metrics unavailability and high consumption of disk space by Prometheus in <link xl:href="../monitoring/troubleshooting-monitoring-issues.xml#troubleshooting-monitoring-issues">Troubleshooting monitoring issues</link>.</simpara>
</section>
<section xml:id="understanding-the-monitoring-stack_monitoring-overview">
<title>Understanding the monitoring stack</title>
<simpara>The {product-title}
monitoring stack is based on the <link xl:href="https://prometheus.io/">Prometheus</link> open source project and its wider ecosystem. The monitoring stack includes the following:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Default platform monitoring components</emphasis>.
A set of platform monitoring components are installed in the <literal>openshift-monitoring</literal> project by default during an OpenShift Container Platform installation. This provides monitoring for core cluster components including Kubernetes services. The default monitoring stack also enables remote health monitoring for clusters.</simpara>
<simpara>These components are illustrated in the <emphasis role="strong">Installed by default</emphasis> section in the following diagram.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Components for monitoring user-defined projects</emphasis>.
After optionally enabling monitoring for user-defined projects, additional monitoring components are installed in the <literal>openshift-user-workload-monitoring</literal> project. This provides monitoring for user-defined projects.
These components are illustrated in the <emphasis role="strong">User</emphasis> section in the following diagram.</simpara>
</listitem>
</itemizedlist>
<simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/monitoring-architecture.png"/>
</imageobject>
<textobject><phrase>{product-title} monitoring architecture</phrase></textobject>
</inlinemediaobject></simpara>
<section xml:id="default-monitoring-components_monitoring-overview">
<title>Default monitoring components</title>
<simpara>By default, the {product-title} {product-version} monitoring stack includes these components:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Default monitoring stack components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Cluster Monitoring Operator</simpara></entry>
<entry align="left" valign="top"><simpara>The Cluster Monitoring Operator (CMO) is a central component of the monitoring stack. It deploys, manages, and automatically updates Prometheus and Alertmanager instances, Thanos Querier, Telemeter Client, and metrics targets. The CMO is deployed by the Cluster Version Operator (CVO).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Prometheus Operator</simpara></entry>
<entry align="left" valign="top"><simpara>The Prometheus Operator (PO) in the <literal>openshift-monitoring</literal> project creates, configures, and manages platform Prometheus instances and Alertmanager instances. It also automatically generates monitoring target configurations based on Kubernetes label queries.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Prometheus</simpara></entry>
<entry align="left" valign="top"><simpara>Prometheus is the monitoring system on which the {product-title} monitoring stack is based. Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Prometheus Adapter</simpara></entry>
<entry align="left" valign="top"><simpara>The Prometheus Adapter (PA in the preceding diagram) translates Kubernetes node and pod queries for use in Prometheus. The resource metrics that are translated include CPU and memory utilization metrics. The Prometheus Adapter exposes the cluster resource metrics API for horizontal pod autoscaling. The Prometheus Adapter is also used by the <literal>oc adm top nodes</literal> and <literal>oc adm top pods</literal> commands.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Alertmanager</simpara></entry>
<entry align="left" valign="top"><simpara>The Alertmanager service handles alerts received from Prometheus. Alertmanager is also responsible for sending the alerts to external notification systems.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>kube-state-metrics agent</simpara></entry>
<entry align="left" valign="top"><simpara>The kube-state-metrics exporter agent (KSM in the preceding diagram) converts Kubernetes objects to metrics that Prometheus can use.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>monitoring-plugin</simpara></entry>
<entry align="left" valign="top"><simpara>The monitoring-plugin dynamic plugin component deploys the monitoring pages in the <emphasis role="strong">Observe</emphasis> section of the {product-title} web console.
You can use Cluster Monitoring Operator (CMO) config map settings to manage monitoring-plugin resources for the web console pages.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>openshift-state-metrics agent</simpara></entry>
<entry align="left" valign="top"><simpara>The openshift-state-metrics exporter (OSM in the preceding diagram) expands upon kube-state-metrics by adding metrics for {product-title}-specific resources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>node-exporter agent</simpara></entry>
<entry align="left" valign="top"><simpara>The node-exporter agent (NE in the preceding diagram) collects metrics about every node in a cluster. The node-exporter agent is deployed on every node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Thanos Querier</simpara></entry>
<entry align="left" valign="top"><simpara>Thanos Querier aggregates and optionally deduplicates core {product-title} metrics and metrics for user-defined projects under a single, multi-tenant interface.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Telemeter Client</simpara></entry>
<entry align="left" valign="top"><simpara>Telemeter Client sends a subsection of the data from platform Prometheus instances to Red Hat to facilitate Remote Health Monitoring for clusters.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>All of the components in the monitoring stack are monitored by the stack and are automatically updated when {product-title} is updated.</simpara>
<note>
<simpara>All components of the monitoring stack use the TLS security profile settings that are centrally configured by a cluster administrator.
If you configure a monitoring stack component that uses TLS security settings, the component uses the TLS security profile settings that already exist in the <literal>tlsSecurityProfile</literal> field in the global {product-title} <literal>apiservers.config.openshift.io/cluster</literal> resource.</simpara>
</note>
</section>
<section xml:id="default-monitoring-targets_monitoring-overview">
<title>Default monitoring targets</title>
<simpara>In addition to the components of the stack itself, the default monitoring stack monitors:</simpara>
<itemizedlist>
<listitem>
<simpara>CoreDNS</simpara>
</listitem>
<listitem>
<simpara>Elasticsearch (if Logging is installed)</simpara>
</listitem>
<listitem>
<simpara>etcd</simpara>
</listitem>
<listitem>
<simpara>Fluentd (if Logging is installed)</simpara>
</listitem>
<listitem>
<simpara>HAProxy</simpara>
</listitem>
<listitem>
<simpara>Image registry</simpara>
</listitem>
<listitem>
<simpara>Kubelets</simpara>
</listitem>
<listitem>
<simpara>Kubernetes API server</simpara>
</listitem>
<listitem>
<simpara>Kubernetes controller manager</simpara>
</listitem>
<listitem>
<simpara>Kubernetes scheduler</simpara>
</listitem>
<listitem>
<simpara>OpenShift API server</simpara>
</listitem>
<listitem>
<simpara>OpenShift Controller Manager</simpara>
</listitem>
<listitem>
<simpara>Operator Lifecycle Manager (OLM)</simpara>
</listitem>
<listitem>
<simpara>Vector (if Logging is installed)</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Each {product-title} component is responsible for its monitoring configuration. For problems with the monitoring of an {product-title} component, open a
<link xl:href="https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&amp;summary=Monitoring_issue&amp;issuetype=1&amp;priority=10200&amp;versions=12385624">Jira issue</link> against that component, not against the general monitoring component.</simpara>
</note>
<simpara>Other {product-title} framework components might be exposing metrics as well. For details, see their respective documentation.</simpara>
</section>
<section xml:id="components-for-monitoring-user-defined-projects_monitoring-overview">
<title>Components for monitoring user-defined projects</title>
<simpara>{product-title}
{product-version}
includes an optional enhancement to the monitoring stack that enables you to monitor services and pods in user-defined projects. This feature includes the following components:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Components for monitoring user-defined projects</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Prometheus Operator</simpara></entry>
<entry align="left" valign="top"><simpara>The Prometheus Operator (PO) in the <literal>openshift-user-workload-monitoring</literal> project creates, configures, and manages Prometheus and Thanos Ruler instances in the same project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Prometheus</simpara></entry>
<entry align="left" valign="top"><simpara>Prometheus is the monitoring system through which monitoring is provided for user-defined projects. Prometheus sends alerts to Alertmanager for processing.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Thanos Ruler</simpara></entry>
<entry align="left" valign="top"><simpara>The Thanos Ruler is a rule evaluation engine for Prometheus that is deployed as a separate process. In {product-title}
{product-version}
, Thanos Ruler provides rule and alerting evaluation for the monitoring of user-defined projects.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Alertmanager</simpara></entry>
<entry align="left" valign="top"><simpara>The Alertmanager service handles alerts received from Prometheus and Thanos Ruler. Alertmanager is also responsible for sending user-defined alerts to external notification systems. Deploying this service is optional.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>The components in the preceding table are deployed after monitoring is enabled for user-defined projects.</simpara>
</note>
<simpara>All of these components are monitored by the stack and are automatically updated when {product-title} is updated.</simpara>
</section>
<section xml:id="monitoring-targets-for-user-defined-projects_monitoring-overview">
<title>Monitoring targets for user-defined projects</title>
<simpara>When monitoring is enabled for user-defined projects, you can monitor:</simpara>
<itemizedlist>
<listitem>
<simpara>Metrics provided through service endpoints in user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>Pods running in user-defined projects.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="openshift-monitoring-common-terms_monitoring-overview">
<title>Glossary of common terms for {product-title} monitoring</title>
<simpara>This glossary defines common terms that are used in {product-title} architecture.</simpara>
<variablelist>
<varlistentry>
<term>Alertmanager</term>
<listitem>
<simpara>Alertmanager handles alerts received from Prometheus. Alertmanager is also responsible for sending the alerts to external notification systems.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Alerting rules</term>
<listitem>
<simpara>Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Cluster Monitoring Operator</term>
<listitem>
<simpara>The Cluster Monitoring Operator (CMO) is a central component of the monitoring stack. It deploys and manages Prometheus instances such as, the Thanos Querier, the Telemeter Client, and metrics targets to ensure that they are up to date. The CMO is deployed by the Cluster Version Operator (CVO).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Cluster Version Operator</term>
<listitem>
<simpara>The Cluster Version Operator (CVO) manages the lifecycle of cluster Operators, many of which are installed in {product-title} by default.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>config map</term>
<listitem>
<simpara>A config map provides a way to inject configuration data into pods. You can reference the data stored in a config map in a volume of type <literal>ConfigMap</literal>. Applications running in a pod can use this data.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Container</term>
<listitem>
<simpara>A container is a lightweight and executable image that includes software and all its dependencies. Containers virtualize the operating system. As a result, you can run containers anywhere from a data center to a public or private cloud as well as a developerâ€™s laptop.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>custom resource (CR)</term>
<listitem>
<simpara>A CR is an extension of the Kubernetes API. You can create custom resources.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>etcd</term>
<listitem>
<simpara>etcd is the key-value store for {product-title}, which stores the state of all resource objects.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Fluentd</term>
<listitem>
<simpara>Fluentd gathers logs from nodes and feeds them to Elasticsearch.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kubelets</term>
<listitem>
<simpara>Runs on nodes and reads the container manifests. Ensures that the defined containers have started and are running.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kubernetes API server</term>
<listitem>
<simpara>Kubernetes API server validates and configures data for the API objects.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kubernetes controller manager</term>
<listitem>
<simpara>Kubernetes controller manager governs the state of the cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Kubernetes scheduler</term>
<listitem>
<simpara>Kubernetes scheduler allocates pods to nodes.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>labels</term>
<listitem>
<simpara>Labels are key-value pairs that you can use to organize and select subsets of objects such as a pod.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>node</term>
<listitem>
<simpara>A worker machine in the {product-title} cluster. A node is either a virtual machine (VM) or a physical machine.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Operator</term>
<listitem>
<simpara>The preferred method of packaging, deploying, and managing a Kubernetes application in an {product-title} cluster. An Operator takes human operational knowledge and encodes it into software that is packaged and shared with customers.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Operator Lifecycle Manager (OLM)</term>
<listitem>
<simpara>OLM helps you install, update, and manage the lifecycle of Kubernetes native applications. OLM is an open source toolkit designed to manage Operators in an effective, automated, and scalable way.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Persistent storage</term>
<listitem>
<simpara>Stores the data even after the device is shut down. Kubernetes uses persistent volumes to store the application data.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Persistent volume claim (PVC)</term>
<listitem>
<simpara>You can use a PVC to mount a PersistentVolume into a Pod. You can access the storage without knowing the details of the cloud environment.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>pod</term>
<listitem>
<simpara>The pod is the smallest logical unit in Kubernetes. A pod is comprised of one or more containers to run in a worker node.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Prometheus</term>
<listitem>
<simpara>Prometheus is the monitoring system on which the {product-title} monitoring stack is based. Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Prometheus adapter</term>
<listitem>
<simpara>The Prometheus Adapter translates Kubernetes node and pod queries for use in Prometheus. The resource metrics that are translated include CPU and memory utilization. The Prometheus Adapter exposes the cluster resource metrics API for horizontal pod autoscaling.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Prometheus Operator</term>
<listitem>
<simpara>The Prometheus Operator (PO) in the <literal>openshift-monitoring</literal> project creates, configures, and manages platform Prometheus and Alertmanager instances. It also automatically generates monitoring target configurations based on Kubernetes label queries.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Silences</term>
<listitem>
<simpara>A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>storage</term>
<listitem>
<simpara>{product-title} supports many types of storage, both for on-premise and cloud providers.
You can manage container storage for persistent and non-persistent data in an {product-title} cluster.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Thanos Ruler</term>
<listitem>
<simpara>The Thanos Ruler is a rule evaluation engine for Prometheus that is deployed as a separate process. In {product-title}, Thanos Ruler provides rule and alerting evaluation for the monitoring of user-defined projects.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>web console</term>
<listitem>
<simpara>A user interface (UI) to manage {product-title}.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="additional-resources_monitoring-overview" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../support/remote_health_monitoring/about-remote-health-monitoring.xml#about-remote-health-monitoring">About remote health monitoring</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#granting-users-permission-to-monitor-user-defined-projects_enabling-monitoring-for-user-defined-projects">Granting users permission to monitor user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../security/tls-security-profiles.xml#tls-security-profiles">Configuring TLS security profiles</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="next-steps_monitoring-overview">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#configuring-the-monitoring-stack">Configuring the monitoring stack</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring-the-monitoring-stack">
<title>Configuring the monitoring stack</title>

<simpara>The {product-title} 4 installation program provides only a low number of configuration options before installation. Configuring most {product-title} framework components, including the cluster monitoring stack, happens postinstallation.</simpara>
<simpara>This section explains what configuration is supported,
shows how to configure the monitoring stack,
and demonstrates several common configuration scenarios.</simpara>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>The monitoring stack imposes additional resource requirements. Consult the computing resources recommendations in <link xl:href="../scalability_and_performance/recommended-performance-scale-practices/recommended-infrastructure-practices.xml#scaling-cluster-monitoring-operator">Scaling the Cluster Monitoring Operator</link> and verify that you have sufficient resources.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="maintenance-and-support_configuring-the-monitoring-stack">
<title>Maintenance and support for monitoring</title>
<simpara>The supported way of configuring {product-title} Monitoring is by configuring it using the options described in this document. <emphasis role="strong">Do not use other configurations, as they are unsupported.</emphasis> Configuration paradigms might change across Prometheus releases, and such cases can only be handled gracefully if all configuration possibilities are controlled. If you use configurations other than those described in this section, your changes will disappear because the <literal>cluster-monitoring-operator</literal> reconciles any differences. The Operator resets everything to the defined state by default and by design.</simpara>
<section xml:id="support-considerations_configuring-the-monitoring-stack">
<title>Support considerations for monitoring</title>
<simpara>The following modifications are explicitly not supported:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Creating additional <literal>ServiceMonitor</literal>, <literal>PodMonitor</literal>, and <literal>PrometheusRule</literal> objects in the <literal>openshift-&#42;</literal> and <literal>kube-&#42;</literal> projects.</emphasis></simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Modifying any resources or objects deployed in the <literal>openshift-monitoring</literal> or <literal>openshift-user-workload-monitoring</literal> projects.</emphasis> The resources created by the {product-title} monitoring stack are not meant to be used by any other resources, as there are no guarantees about their backward compatibility.</simpara>
<note>
<simpara>The Alertmanager configuration is deployed as a secret resource in the <literal>openshift-monitoring</literal> namespace.
If you have enabled a separate Alertmanager instance for user-defined alert routing, an Alertmanager configuration is also deployed as a secret resource in the <literal>openshift-user-workload-monitoring</literal> namespace.
To configure additional routes for any instance of Alertmanager, you need to decode, modify, and then encode that secret.
This procedure is a supported exception to the preceding statement.</simpara>
</note>
</listitem>
<listitem>
<simpara><emphasis role="strong">Modifying resources of the stack.</emphasis> The {product-title} monitoring stack ensures its resources are always in the state it expects them to be. If they are modified, the stack will reset them.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Deploying user-defined workloads to <literal>openshift-&#42;</literal>, and <literal>kube-&#42;</literal> projects.</emphasis> These projects are reserved for Red Hat provided components and they should not be used for user-defined workloads.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Enabling symptom based monitoring by using the <literal>Probe</literal> custom resource definition (CRD) in Prometheus Operator.</emphasis></simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Backward compatibility for metrics, recording rules, or alerting rules is not guaranteed.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Installing custom Prometheus instances on {product-title}.</emphasis> A custom instance is a Prometheus custom resource (CR) managed by the Prometheus Operator.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="unmanaged-monitoring-operators_configuring-the-monitoring-stack">
<title>Support policy for monitoring Operators</title>
<simpara>Monitoring Operators ensure that {product-title} monitoring resources function as designed and tested. If Cluster Version Operator (CVO) control of an Operator is overridden, the Operator does not respond to configuration changes, reconcile the intended state of cluster objects, or receive updates.</simpara>
<simpara>While overriding CVO control for an Operator can be helpful during debugging, this is  unsupported and the cluster administrator assumes full control of the individual component configurations and upgrades.</simpara>
<formalpara>
<title>Overriding the Cluster Version Operator</title>
<para>The <literal>spec.overrides</literal> parameter can be added to the configuration for the CVO to allow administrators to provide a list of overrides to the behavior of the CVO for a component. Setting the <literal>spec.overrides[].unmanaged</literal> parameter to <literal>true</literal> for a component blocks cluster upgrades and alerts the administrator after a CVO override has been set:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">Disabling ownership via cluster version overrides prevents upgrades. Please remove overrides before continuing.</programlisting>
<warning>
<simpara>Setting a CVO override puts the entire cluster in an unsupported state and prevents the monitoring stack from being reconciled to its intended state. This impacts the reliability features built into Operators and prevents updates from being received. Reported issues must be reproduced after removing any overrides for support to proceed.</simpara>
</warning>
</section>
</section>
<section xml:id="preparing-to-configure-the-monitoring-stack">
<title>Preparing to configure the monitoring stack</title>
<simpara>You can configure the monitoring stack by creating and updating monitoring config maps.</simpara>
<section xml:id="creating-cluster-monitoring-configmap_configuring-the-monitoring-stack">
<title>Creating a cluster monitoring config map</title>
<simpara>To configure core {product-title} monitoring components, you must create the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project.</simpara>
<note>
<simpara>When you save your changes to the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object, some or all of the pods in the <literal>openshift-monitoring</literal> project might be redeployed. It can sometimes take a while for these components to redeploy.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check whether the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object exists:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring get configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>If the <literal>ConfigMap</literal> object does not exist:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the following YAML manifest. In this example the file is called <literal>cluster-monitoring-config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration to create the <literal>ConfigMap</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f cluster-monitoring-config.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack">
<title>Creating a user-defined workload monitoring config map</title>
<simpara>To configure the components that monitor user-defined projects, you must create the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
<note>
<simpara>When you save your changes to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object, some or all of the pods in the <literal>openshift-user-workload-monitoring</literal> project might be redeployed. It can sometimes take a while for these components to redeploy. You can create and configure the config map before you first enable monitoring for user-defined projects, to prevent having to redeploy the pods often.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check whether the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object exists:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>If the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object does not exist:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the following YAML manifest. In this example the file is called <literal>user-workload-monitoring-config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |</programlisting>
</listitem>
<listitem>
<simpara>Apply the configuration to create the <literal>ConfigMap</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f user-workload-monitoring-config.yaml</programlisting>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-the-monitoring-stack_configuring-the-monitoring-stack">
<title>Configuring the monitoring stack</title>
<simpara>In {product-title} {product-version}, you can configure the monitoring stack using the <literal>cluster-monitoring-config</literal> or <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> objects. Config maps configure the Cluster Monitoring Operator (CMO), which in turn configures the components of the stack.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To configure core {product-title} monitoring components</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add your configuration under <literal>data/config.yaml</literal> as a key-value pair <literal>&lt;component_name&gt;:&#160;&lt;component_configuration&gt;</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      &lt;configuration_for_the_component&gt;</programlisting>
<simpara>Substitute <literal>&lt;component&gt;</literal> and <literal>&lt;configuration_for_the_component&gt;</literal> accordingly.</simpara>
<simpara>The following example <literal>ConfigMap</literal> object configures a persistent volume claim (PVC) for Prometheus. This relates to the Prometheus instance that monitors core {product-title} components only:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s: <co xml:id="CO1-1"/>
      volumeClaimTemplate:
        spec:
          storageClassName: fast
          volumeMode: Filesystem
          resources:
            requests:
              storage: 40Gi</programlisting>
<calloutlist>
<callout arearefs="CO1-1">
<para>Defines the Prometheus component and the subsequent lines define its configuration.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To configure components that monitor user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add your configuration under <literal>data/config.yaml</literal> as a key-value pair <literal>&lt;component_name&gt;:&#160;&lt;component_configuration&gt;</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      &lt;configuration_for_the_component&gt;</programlisting>
<simpara>Substitute <literal>&lt;component&gt;</literal> and <literal>&lt;configuration_for_the_component&gt;</literal> accordingly.</simpara>
<simpara>The following example <literal>ConfigMap</literal> object configures a data retention period and minimum container resource requests for Prometheus. This relates to the Prometheus instance that monitors user-defined projects only:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus: <co xml:id="CO2-1"/>
      retention: 24h <co xml:id="CO2-2"/>
      resources:
        requests:
          cpu: 200m <co xml:id="CO2-3"/>
          memory: 2Gi <co xml:id="CO2-4"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para>Defines the Prometheus component and the subsequent lines define its configuration.</para>
</callout>
<callout arearefs="CO2-2">
<para>Configures a twenty-four hour data retention period for the Prometheus instance that monitors user-defined projects.</para>
</callout>
<callout arearefs="CO2-3">
<para>Defines a minimum resource request of 200 millicores for the Prometheus container.</para>
</callout>
<callout arearefs="CO2-4">
<para>Defines a minimum pod resource request of 2 GiB of memory for the Prometheus container.</para>
</callout>
</calloutlist>
<note>
<simpara>The Prometheus config map component is called <literal>prometheusK8s</literal> in the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object and <literal>prometheus</literal> in the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes to the <literal>ConfigMap</literal> object. The pods affected by the new configuration are restarted automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>Configuration reference for the <link xl:href="../monitoring/config-map-reference-for-the-cluster-monitoring-operator.xml#clustermonitoringconfiguration"><literal>cluster-monitoring-config</literal></link> config map</simpara>
</listitem>
<listitem>
<simpara>Configuration reference for the <link xl:href="../monitoring/config-map-reference-for-the-cluster-monitoring-operator.xml#userworkloadconfiguration"><literal>user-workload-monitoring-config</literal></link> config map</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps</simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="configurable-monitoring-components_configuring-the-monitoring-stack">
<title>Configurable monitoring components</title>
<simpara>This table shows the monitoring components you can configure and the keys used to specify the components in the
<literal>cluster-monitoring-config</literal> and
<literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> objects.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Configurable monitoring components</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">cluster-monitoring-config config map key</entry>
<entry align="left" valign="top">user-workload-monitoring-config config map key</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Prometheus Operator</simpara></entry>
<entry align="left" valign="top"><simpara><literal>prometheusOperator</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>prometheusOperator</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Prometheus</simpara></entry>
<entry align="left" valign="top"><simpara><literal>prometheusK8s</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>prometheus</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Alertmanager</simpara></entry>
<entry align="left" valign="top"><simpara><literal>alertmanagerMain</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>alertmanager</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>kube-state-metrics</simpara></entry>
<entry align="left" valign="top"><simpara><literal>kubeStateMetrics</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>monitoring-plugin</simpara></entry>
<entry align="left" valign="top"><simpara><literal>monitoringPlugin</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>openshift-state-metrics</simpara></entry>
<entry align="left" valign="top"><simpara><literal>openshiftStateMetrics</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Telemeter Client</simpara></entry>
<entry align="left" valign="top"><simpara><literal>telemeterClient</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Prometheus Adapter</simpara></entry>
<entry align="left" valign="top"><simpara><literal>k8sPrometheusAdapter</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Thanos Querier</simpara></entry>
<entry align="left" valign="top"><simpara><literal>thanosQuerier</literal></simpara></entry>
<entry align="left" valign="top"></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Thanos Ruler</simpara></entry>
<entry align="left" valign="top"></entry>
<entry align="left" valign="top"><simpara><literal>thanosRuler</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>The Prometheus key is called <literal>prometheusK8s</literal> in the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object and <literal>prometheus</literal> in the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</note>
</section>
<section xml:id="using-node-selectors-to-move-monitoring-components_configuring-the-monitoring-stack">
<title>Using node selectors to move monitoring components</title>
<simpara>By using the <literal>nodeSelector</literal> constraint with labeled nodes, you can move any of the monitoring stack components to specific nodes.
By doing so, you can control the placement and distribution of the monitoring components across a cluster.</simpara>
<simpara>By controlling placement and distribution of monitoring components, you can optimize system resource use, improve performance, and segregate workloads based on specific requirements or policies.</simpara>
<section xml:id="how-node-selectors-work-with-other-constraints_configuring-the-monitoring-stack">
<title>How node selectors work with other constraints</title>
<simpara>If you move monitoring components by using node selector constraints, be aware that other constraints to control pod scheduling might exist for a cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Topology spread constraints might be in place to control pod placement.</simpara>
</listitem>
<listitem>
<simpara>Hard anti-affinity rules are in place for Prometheus, Thanos Querier, Alertmanager, and other monitoring components to ensure that multiple pods for these components are always spread across different nodes and are therefore always highly available.</simpara>
</listitem>
</itemizedlist>
<simpara>When scheduling pods onto nodes, the pod scheduler tries to satisfy all existing constraints when determining pod placement.
That is, all constraints compound when the pod scheduler determines which pods will be placed on which nodes.</simpara>
<simpara>Therefore, if you configure a node selector constraint but existing constraints cannot all be satisfied, the pod scheduler cannot match all constraints and will not schedule a pod for placement onto a node.</simpara>
<simpara>To maintain resilience and high availability for monitoring components, ensure that enough nodes are available and match all constraints when you configure a node selector constraint to move a component.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../nodes/nodes/nodes-nodes-working.xml#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../nodes/scheduling/nodes-scheduler-node-selectors.xml#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../nodes/scheduling/nodes-scheduler-pod-affinity.xml">Placing pods relative to other pods using affinity and anti-affinity rules</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../nodes/scheduling/nodes-scheduler-pod-topology-spread-constraints.xml">Controlling pod placement by using pod topology spread constraints</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#configuring_pod_topology_spread_constraintsfor_monitoring_configuring-the-monitoring-stack">Configuring pod topology spread constraints for monitoring</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">Kubernetes documentation about node selectors</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="moving-monitoring-components-to-different-nodes_configuring-the-monitoring-stack">
<title>Moving monitoring components to different nodes</title>
<simpara>To specify the nodes in your cluster on which monitoring stack components will run, configure the <literal>nodeSelector</literal> constraint in the component&#8217;s <literal>ConfigMap</literal> object to match labels assigned to the nodes.</simpara>
<note>
<simpara>You cannot add a node selector constraint directly to an existing scheduled pod.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If you have not done so yet, add a label to the nodes on which you want to run the monitoring components:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label nodes &lt;node-name&gt; &lt;node-label&gt;</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To move a component that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Specify the node labels for the <literal>nodeSelector</literal> constraint for the component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <co xml:id="CO3-1"/>
      nodeSelector:
        &lt;node-label-1&gt; <co xml:id="CO3-2"/>
        &lt;node-label-2&gt; <co xml:id="CO3-3"/>
        &lt;...&gt;</programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para>Substitute <literal>&lt;component&gt;</literal> with the appropriate monitoring stack component name.</para>
</callout>
<callout arearefs="CO3-2">
<para>Substitute <literal>&lt;node-label-1&gt;</literal> with the label you added to the node.</para>
</callout>
<callout arearefs="CO3-3">
<para>Optional: Specify additional labels.
If you specify additional labels, the pods for the component are only scheduled on the nodes that contain all of the specified labels.</para>
</callout>
</calloutlist>
<note>
<simpara>If monitoring components remain in a <literal>Pending</literal> state after configuring the <literal>nodeSelector</literal> constraint, check the pod events for errors relating to taints and tolerations.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To move a component that monitors user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Specify the node labels for the <literal>nodeSelector</literal> constraint for the component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <co xml:id="CO4-1"/>
      nodeSelector:
        &lt;node-label-1&gt; <co xml:id="CO4-2"/>
        &lt;node-label-2&gt; <co xml:id="CO4-3"/>
        &lt;...&gt;</programlisting>
<calloutlist>
<callout arearefs="CO4-1">
<para>Substitute <literal>&lt;component&gt;</literal> with the appropriate monitoring stack component name.</para>
</callout>
<callout arearefs="CO4-2">
<para>Substitute <literal>&lt;node-label-1&gt;</literal> with the label you added to the node.</para>
</callout>
<callout arearefs="CO4-3">
<para>Optional: Specify additional labels.
If you specify additional labels, the pods for the component are only scheduled on the nodes that contain all of the specified labels.</para>
</callout>
</calloutlist>
<note>
<simpara>If monitoring components remain in a <literal>Pending</literal> state after configuring the <literal>nodeSelector</literal> constraint, check the pod events for errors relating to taints and tolerations.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.
The components specified in the new configuration are moved to the new nodes automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When you save changes to a monitoring config map, the pods and other resources in the project might be redeployed.
The running monitoring processes in that project might also restart.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps</simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../nodes/nodes/nodes-nodes-working.xml#nodes-nodes-working-updating_nodes-nodes-working">Understanding how to update labels on nodes</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../nodes/scheduling/nodes-scheduler-node-selectors.xml#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</link></simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector">Kubernetes documentation</link> for details on the <literal>nodeSelector</literal> constraint</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="assigning-tolerations-to-monitoring-components_configuring-the-monitoring-stack">
<title>Assigning tolerations to monitoring components</title>
<simpara>You can assign tolerations to any of the monitoring stack components to enable moving them to tainted nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To assign tolerations to a component that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Specify <literal>tolerations</literal> for the component:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      tolerations:
        &lt;toleration_specification&gt;</programlisting>
<simpara>Substitute <literal>&lt;component&gt;</literal> and <literal>&lt;toleration_specification&gt;</literal> accordingly.</simpara>
<simpara>For example, <literal>oc adm taint nodes node1 key1=value1:NoSchedule</literal> adds a taint to <literal>node1</literal> with the key <literal>key1</literal> and the value <literal>value1</literal>. This prevents monitoring components from deploying pods on <literal>node1</literal> unless a toleration is configured for that taint. The following example configures the <literal>alertmanagerMain</literal> component to tolerate the example taint:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      tolerations:
      - key: "key1"
        operator: "Equal"
        value: "value1"
        effect: "NoSchedule"</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To assign tolerations to a component that monitors user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Specify <literal>tolerations</literal> for the component:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      tolerations:
        &lt;toleration_specification&gt;</programlisting>
<simpara>Substitute <literal>&lt;component&gt;</literal> and <literal>&lt;toleration_specification&gt;</literal> accordingly.</simpara>
<simpara>For example, <literal>oc adm taint nodes node1 key1=value1:NoSchedule</literal> adds a taint to <literal>node1</literal> with the key <literal>key1</literal> and the value <literal>value1</literal>. This prevents monitoring components from deploying pods on <literal>node1</literal> unless a toleration is configured for that taint. The following example configures the <literal>thanosRuler</literal> component to tolerate the example taint:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      tolerations:
      - key: "key1"
        operator: "Equal"
        value: "value1"
        effect: "NoSchedule"</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The new component placement configuration is applied automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps</simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="../nodes/scheduling/nodes-scheduler-taints-tolerations.xml#nodes-scheduler-taints-tolerations">{product-title} documentation</link> on taints and tolerations</simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">Kubernetes documentation</link> on taints and tolerations</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-the-body-size-limit-for-metrics-scraping_configuring-the-monitoring-stack">
<title>Setting the body size limit for metrics scraping</title>
<simpara>By default, no limit exists for the uncompressed body size for data returned from scraped metrics targets.
You can set a body size limit to help avoid situations in which Prometheus consumes excessive amounts of memory when scraped targets return a response that contains a large amount of data.
In addition, by setting a body size limit, you can reduce the impact that a malicious target might have on Prometheus and on the cluster as a whole.</simpara>
<simpara>After you set a value for <literal>enforcedBodySizeLimit</literal>, the alert <literal>PrometheusScrapeBodySizeLimitHit</literal> fires when at least one Prometheus scrape target replies with a response body larger than the configured value.</simpara>
<note>
<simpara>If metrics data scraped from a target has an uncompressed body size exceeding the configured size limit, the scrape fails.
Prometheus then considers this target to be down and sets its <literal>up</literal> metric value to <literal>0</literal>, which can trigger the <literal>TargetDown</literal> alert.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a value for <literal>enforcedBodySizeLimit</literal> to <literal>data/config.yaml/prometheusK8s</literal> to limit the body size that can be accepted per target scrape:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |-
    prometheusK8s:
      enforcedBodySizeLimit: 40MB <co xml:id="CO5-1"/></programlisting>
<calloutlist>
<callout arearefs="CO5-1">
<para>Specify the maximum body size for scraped metrics targets.
This <literal>enforcedBodySizeLimit</literal> example limits the uncompressed size per target scrape to 40 megabytes.
Valid numeric values use the Prometheus data size format: B (bytes), KB (kilobytes), MB (megabytes), GB (gigabytes), TB (terabytes), PB (petabytes), and EB (exabytes).
The default value is <literal>0</literal>, which specifies no limit.
You can also set the value to <literal>automatic</literal> to calculate the limit automatically based on cluster capacity.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes automatically.</simpara>
<warning>
<simpara>When you save changes to a <literal>cluster-monitoring-config</literal> config map, the pods and other resources in the <literal>openshift-monitoring</literal> project might be redeployed.
The running monitoring processes in that project might also restart.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config">Prometheus scrape configuration documentation</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="managing-cpu-and-memory-resources-for-monitoring-components">
<title>Managing CPU and memory resources for monitoring components</title>
<simpara>You can ensure that the containers that run monitoring components have enough CPU and memory resources by specifying values for resource limits and requests for those components.</simpara>
<simpara>You can configure these limits and requests for core platform monitoring components in the <literal>openshift-monitoring</literal> namespace and for the components that monitor user-defined projects in the <literal>openshift-user-workload-monitoring</literal> namespace.</simpara>
<section xml:id="about-specifying-limits-and-requests-for-monitoring-components_configuring-the-monitoring-stack">
<title>About specifying limits and requests for monitoring components</title>
<simpara>You can configure resource limits and request settings for core platform monitoring components and for the components that monitor user-defined projects, including the following components:</simpara>
<itemizedlist>
<listitem>
<simpara>Alertmanager (for core platform monitoring and for user-defined projects)</simpara>
</listitem>
<listitem>
<simpara>kube-state-metrics</simpara>
</listitem>
<listitem>
<simpara>monitoring-plugin</simpara>
</listitem>
<listitem>
<simpara>node-exporter</simpara>
</listitem>
<listitem>
<simpara>openshift-state-metrics</simpara>
</listitem>
<listitem>
<simpara>Prometheus (for core platform monitoring and for user-defined projects)</simpara>
</listitem>
<listitem>
<simpara>Prometheus Adapter</simpara>
</listitem>
<listitem>
<simpara>Prometheus Operator and its admission webhook service</simpara>
</listitem>
<listitem>
<simpara>Telemeter Client</simpara>
</listitem>
<listitem>
<simpara>Thanos Querier</simpara>
</listitem>
<listitem>
<simpara>Thanos Ruler</simpara>
</listitem>
</itemizedlist>
<simpara>By defining resource limits, you limit a container&#8217;s resource usage, which prevents the container from exceeding the specified maximum values for CPU and memory resources.</simpara>
<simpara>By defining resource requests, you specify that a container can be scheduled only on a node that has enough CPU and memory resources available to match the requested resources.</simpara>
</section>
<section xml:id="specifying-limits-and-resource-requests-for-monitoring-components_configuring-the-monitoring-stack">
<title>Specifying limits and requests for monitoring components</title>
<simpara>To configure CPU and memory resources, specify values for resource limits and requests in the appropriate <literal>ConfigMap</literal> object for the namespace in which the monitoring component is located:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> namespace for core platform monitoring</simpara>
</listitem>
<listitem>
<simpara>The <literal>user-workload-monitoring-config</literal> config map in the <literal>openshift-user-workload-monitoring</literal> namespace for components that monitor user-defined projects</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core platform monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>ConfigMap</literal> object named <literal>cluster-monitoring-config</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To configure core platform monitoring components, edit the <literal>cluster-monitoring-config</literal> config map object in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add values to define resource limits and requests for each core platform monitoring component you want to configure.</simpara>
<important>
<simpara>Make sure that the value set for a limit is always higher than the value set for a request.
Otherwise, an error will occur, and the container will not run.</simpara>
</important>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    k8sPrometheusAdapter:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    monitoringPlugin:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Save the file to apply the changes automatically.</simpara>
<important>
<simpara>When you save changes to the <literal>cluster-monitoring-config</literal> config map, the pods and other resources in the <literal>openshift-monitoring</literal> project might be redeployed.
The running monitoring processes in that project might also restart.</simpara>
</important>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits">Kubernetes requests and limits documentation</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring_persistent_storage_configuring-the-monitoring-stack">
<title>Configuring persistent storage</title>
<simpara>Running cluster monitoring with persistent storage means that your metrics are stored to a persistent volume (PV) and can survive a pod being restarted or recreated. This is ideal if you require your metrics or alerting data to be guarded from data loss. For production environments, it is highly recommended to configure persistent storage. Because of the high IO demands, it is advantageous to use local storage.</simpara>
<section xml:id="persistent-storage-prerequisites">
<title>Persistent storage prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Dedicate sufficient local persistent storage to ensure that the disk does not become full. How much storage you need depends on the number of pods.</simpara>
</listitem>
<listitem>
<simpara>Verify that you have a persistent volume (PV) ready to be claimed by the persistent volume claim (PVC), one PV for each replica. Because Prometheus and Alertmanager both have two replicas, you need four PVs to support the entire monitoring stack. The PVs are available from the Local Storage Operator, but not if you have enabled dynamically provisioned storage.</simpara>
</listitem>
<listitem>
<simpara>Use <literal>Filesystem</literal> as the storage type value for the <literal>volumeMode</literal> parameter when you configure the persistent volume.</simpara>
<note>
<simpara>If you use a local volume for persistent storage, do not use a raw block volume, which is described with <literal>volumeMode: Block</literal> in the <literal>LocalVolume</literal> object. Prometheus cannot use raw block volumes.</simpara>
</note>
<important>
<simpara>Prometheus does not support file systems that are not POSIX compliant.
For example, some NFS file system implementations are not POSIX compliant.
If you want to use an NFS file system for storage, verify with the vendor that their NFS implementation is fully POSIX compliant.</simpara>
</important>
</listitem>
</itemizedlist>
</section>
<section xml:id="configuring-a-local-persistent-volume-claim_configuring-the-monitoring-stack">
<title>Configuring a local persistent volume claim</title>
<simpara>For monitoring components to use a persistent volume (PV), you must configure a persistent volume claim (PVC).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To configure a PVC for a component that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add your PVC configuration for the component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      volumeClaimTemplate:
        spec:
          storageClassName: &lt;storage_class&gt;
          resources:
            requests:
              storage: &lt;amount_of_storage&gt;</programlisting>
<simpara>See the <link xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">Kubernetes documentation on PersistentVolumeClaims</link> for information on how to specify <literal>volumeClaimTemplate</literal>.</simpara>
<simpara>The following example configures a PVC that claims local persistent storage for the Prometheus instance that monitors core {product-title} components:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi</programlisting>
<simpara>In the above example, the storage class created by the Local Storage Operator is called <literal>local-storage</literal>.</simpara>
<simpara>The following example configures a PVC that claims local persistent storage for Alertmanager:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 10Gi</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To configure a PVC for a component that monitors user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add your PVC configuration for the component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      volumeClaimTemplate:
        spec:
          storageClassName: &lt;storage_class&gt;
          resources:
            requests:
              storage: &lt;amount_of_storage&gt;</programlisting>
<simpara>See the <link xl:href="https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims">Kubernetes documentation on PersistentVolumeClaims</link> for information on how to specify <literal>volumeClaimTemplate</literal>.</simpara>
<simpara>The following example configures a PVC that claims
local
persistent storage for the Prometheus instance that monitors user-defined projects:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi</programlisting>
<simpara>In the above example, the storage class created by the Local Storage Operator is called <literal>local-storage</literal>.</simpara>
<simpara>The following example configures a PVC that claims
local
persistent storage for Thanos Ruler:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 10Gi</programlisting>
<note>
<simpara>Storage requirements for the <literal>thanosRuler</literal> component depend on the number of rules that are evaluated and how many samples each rule generates.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods affected by the new configuration are restarted automatically and the new storage configuration is applied.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="resizing-a-persistent-storage-volume_configuring-the-monitoring-stack">
<title>Resizing a persistent storage volume</title>
<simpara>{product-title} does not support resizing an existing persistent storage volume used by <literal>StatefulSet</literal> resources, even if the underlying <literal>StorageClass</literal> resource used supports persistent volume sizing.
Therefore, even if you update the <literal>storage</literal> field for an existing persistent volume claim (PVC) with a larger size, this setting will not be propagated to the associated persistent volume (PV).</simpara>
<simpara>However, resizing a PV is still possible by using a manual process. If you want to resize a PV for a monitoring component such as Prometheus, Thanos Ruler, or Alertmanager, you can update the appropriate config map in which the component is configured. Then, patch the PVC, and delete and orphan the pods.
Orphaning the pods recreates the <literal>StatefulSet</literal> resource immediately and automatically updates the size of the volumes mounted in the pods with the new PVC settings.
No service disruption occurs during this process.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have configured at least one PVC for core {product-title} monitoring components.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have configured at least one PVC for components that monitor user-defined projects.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To resize a PVC for a component that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a new storage size for the PVC configuration for the component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <co xml:id="CO6-1"/>
      volumeClaimTemplate:
        spec:
          storageClassName: &lt;storage_class&gt; <co xml:id="CO6-2"/>
          resources:
            requests:
              storage: &lt;amount_of_storage&gt; <co xml:id="CO6-3"/></programlisting>
<calloutlist>
<callout arearefs="CO6-1">
<para>Specify the core monitoring component.</para>
</callout>
<callout arearefs="CO6-2">
<para>Specify the storage class.</para>
</callout>
<callout arearefs="CO6-3">
<para>Specify the new size for the storage volume.</para>
</callout>
</calloutlist>
<simpara>The following example configures a PVC that sets the local persistent storage to 100 gigabytes for the Prometheus instance that monitors core {product-title} components:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 100Gi</programlisting>
<simpara>The following example configures a PVC that sets the local persistent storage for Alertmanager to 40 gigabytes:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To resize a PVC for a component that monitors user-defined projects</emphasis>:</simpara>
<note>
<simpara>You can resize the volumes for the Thanos Ruler and Prometheus instances that monitor user-defined projects.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Update the PVC configuration for the monitoring component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <co xml:id="CO7-1"/>
      volumeClaimTemplate:
        spec:
          storageClassName: &lt;storage_class&gt; <co xml:id="CO7-2"/>
          resources:
            requests:
              storage: &lt;amount_of_storage&gt; <co xml:id="CO7-3"/></programlisting>
<calloutlist>
<callout arearefs="CO7-1">
<para>Specify the core monitoring component.</para>
</callout>
<callout arearefs="CO7-2">
<para>Specify the storage class.</para>
</callout>
<callout arearefs="CO7-3">
<para>Specify the new size for the storage volume.</para>
</callout>
</calloutlist>
<simpara>The following example configures the PVC size to 100 gigabytes for the Prometheus instance that monitors user-defined projects:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 100Gi</programlisting>
<simpara>The following example sets the PVC size to 20 gigabytes for Thanos Ruler:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 20Gi</programlisting>
<note>
<simpara>Storage requirements for the <literal>thanosRuler</literal> component depend on the number of rules that are evaluated and how many samples each rule generates.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods affected by the new configuration restart automatically.</simpara>
<warning>
<simpara>When you save changes to a monitoring config map, the pods and other resources in the related project might be redeployed. The monitoring processes running in that project might also be restarted.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Manually patch every PVC with the updated storage request. The following example resizes the storage size for the Prometheus component in the <literal>openshift-monitoring</literal> namespace to 100Gi:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for p in $(oc -n openshift-monitoring get pvc -l app.kubernetes.io/name=prometheus -o jsonpath='{range .items[*]}{.metadata.name} {end}'); do \
  oc -n openshift-monitoring patch pvc/${p} --patch '{"spec": {"resources": {"requests": {"storage":"100Gi"}}}}'; \
  done</programlisting>
</listitem>
<listitem>
<simpara>Delete the underlying StatefulSet with the <literal>--cascade=orphan</literal> parameter:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete statefulset -l app.kubernetes.io/name=prometheus --cascade=orphan</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying-retention-time-and-size-for-prometheus-metrics-data_configuring-the-monitoring-stack">
<title>Modifying the retention time and size for Prometheus metrics data</title>
<simpara>By default, Prometheus automatically retains metrics data for 11 days. You can modify the retention time for
Prometheus
to change how soon the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses. If the data reaches this size limit, Prometheus deletes the oldest data first until the disk space used is again below the limit.</simpara>
<simpara>Note the following behaviors of these data retention settings:</simpara>
<itemizedlist>
<listitem>
<simpara>The size-based retention policy applies to all data block directories in the <literal>/prometheus</literal> directory, including persistent blocks, write-ahead log (WAL) data, and m-mapped chunks.</simpara>
</listitem>
<listitem>
<simpara>Data in the <literal>/wal</literal> and <literal>/head_chunks</literal> directories counts toward the retention size limit, but Prometheus never purges data from these directories based on size- or time-based retention policies.
Thus, if you set a retention size limit lower than the maximum size set for the <literal>/wal</literal> and <literal>/head_chunks</literal> directories, you have configured the system not to retain any data blocks in the <literal>/prometheus</literal> data directories.</simpara>
</listitem>
<listitem>
<simpara>The size-based retention policy is applied only when Prometheus cuts a new data block, which occurs every two hours after the WAL contains at least three hours of data.</simpara>
</listitem>
<listitem>
<simpara>If you do not explicitly define values for either <literal>retention</literal> or <literal>retentionSize</literal>, retention time defaults to 11 days, and retention size is not set.</simpara>
</listitem>
<listitem>
<simpara>If you define values for both <literal>retention</literal> and <literal>retentionSize</literal>, both values apply.
If any data blocks exceed the defined retention time or the defined size limit, Prometheus purges these data blocks.</simpara>
</listitem>
<listitem>
<simpara>If you define a value for <literal>retentionSize</literal> and do not define <literal>retention</literal>, only the <literal>retentionSize</literal> value applies.</simpara>
</listitem>
<listitem>
<simpara>If you do not define a value for <literal>retentionSize</literal> and only define a value for <literal>retention</literal>, only the <literal>retention</literal> value applies.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>A cluster administrator has enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To modify the retention time and size for the Prometheus instance that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add the retention time and size configuration under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: &lt;time_specification&gt; <co xml:id="CO8-1"/>
      retentionSize: &lt;size_specification&gt; <co xml:id="CO8-2"/></programlisting>
<calloutlist>
<callout arearefs="CO8-1">
<para>The retention time: a number directly followed by <literal>ms</literal> (milliseconds), <literal>s</literal> (seconds), <literal>m</literal> (minutes), <literal>h</literal> (hours), <literal>d</literal> (days), <literal>w</literal> (weeks), or <literal>y</literal> (years). You can also combine time values for specific times, such as <literal>1h30m15s</literal>.</para>
</callout>
<callout arearefs="CO8-2">
<para>The retention size: a number directly followed by <literal>B</literal> (bytes), <literal>KB</literal> (kilobytes), <literal>MB</literal> (megabytes), <literal>GB</literal> (gigabytes), <literal>TB</literal> (terabytes), <literal>PB</literal> (petabytes), and <literal>EB</literal> (exabytes).</para>
</callout>
</calloutlist>
<simpara>The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance that monitors core {product-title} components:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: 24h
      retentionSize: 10GB</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To modify the retention time and size for the Prometheus instance that monitors user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add the retention time and size configuration under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: &lt;time_specification&gt; <co xml:id="CO9-1"/>
      retentionSize: &lt;size_specification&gt; <co xml:id="CO9-2"/></programlisting>
<calloutlist>
<callout arearefs="CO9-1">
<para>The retention time: a number directly followed by <literal>ms</literal> (milliseconds), <literal>s</literal> (seconds), <literal>m</literal> (minutes), <literal>h</literal> (hours), <literal>d</literal> (days), <literal>w</literal> (weeks), or <literal>y</literal> (years).
You can also combine time values for specific times, such as <literal>1h30m15s</literal>.</para>
</callout>
<callout arearefs="CO9-2">
<para>The retention size: a number directly followed by <literal>B</literal> (bytes), <literal>KB</literal> (kilobytes), <literal>MB</literal> (megabytes), <literal>GB</literal> (gigabytes), <literal>TB</literal> (terabytes), <literal>PB</literal> (petabytes), or <literal>EB</literal> (exabytes).</para>
</callout>
</calloutlist>
<simpara>The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance that monitors user-defined projects:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: 24h
      retentionSize: 10GB</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods affected by the new configuration restart automatically.</simpara>
<warning>
<simpara>When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying-the-retention-time-for-thanos-ruler-metrics-data_configuring-the-monitoring-stack">
<title>Modifying the retention time for Thanos Ruler metrics data</title>
<simpara>By default, for user-defined projects, Thanos Ruler automatically retains metrics data for 24 hours. You can modify the retention time to change how long this data is retained by specifying a time value in the <literal>user-workload-monitoring-config</literal> config map in the <literal>openshift-user-workload-monitoring</literal> namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>A cluster administrator has enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add the retention time configuration under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      retention: &lt;time_specification&gt; <co xml:id="CO10-1"/></programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>Specify the retention time in the following format: a number directly followed by <literal>ms</literal> (milliseconds), <literal>s</literal> (seconds), <literal>m</literal> (minutes), <literal>h</literal> (hours), <literal>d</literal> (days), <literal>w</literal> (weeks), or <literal>y</literal> (years).
You can also combine time values for specific times, such as <literal>1h30m15s</literal>.
The default is <literal>24h</literal>.</para>
</callout>
</calloutlist>
<simpara>The following example sets the retention time to 10 days for Thanos Ruler data:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      retention: 10d</programlisting>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods affected by the new configuration automatically restart.</simpara>
<warning>
<simpara>Saving changes to a monitoring config map might restart monitoring processes and redeploy the pods and other resources in the related project.
The running monitoring processes in that project might also restart.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#creating-cluster-monitoring-configmap_configuring-the-monitoring-stack">Creating a cluster monitoring config map</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../scalability_and_performance/recommended-performance-scale-practices/recommended-infrastructure-practices.xml#prometheus-database-storage-requirements_cluster-monitoring-operator">Prometheus database storage requirements</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../scalability_and_performance/optimization/optimizing-storage.xml#optimizing-storage">Recommended configurable storage technology</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../storage/understanding-persistent-storage.xml#understanding-persistent-storage">Understanding persistent storage</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../scalability_and_performance/optimization/optimizing-storage.xml#optimizing-storage">Optimizing storage</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../storage/persistent_storage/persistent_storage_local/persistent-storage-local.xml#persistent-storage-using-local-volume">Configure local persistent storage</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring_remote_write_storage_configuring-the-monitoring-stack">
<title>Configuring remote write storage</title>
<simpara role="_abstract">You can configure remote write storage to enable Prometheus to send ingested metrics to remote systems for long-term storage. Doing so has no impact on how or for how long Prometheus stores metrics.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components:</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects:</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have set up a remote write compatible endpoint (such as Thanos) and know the endpoint URL. See the <link xl:href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">Prometheus remote endpoints and storage documentation</link> for information about endpoints that are compatible with the remote write feature.</simpara>
</listitem>
<listitem>
<simpara>You have set up authentication credentials in a <literal>Secret</literal> object for the remote write endpoint. You must create the secret in the
same namespace as the Prometheus object for which you configure remote write: the <literal>openshift-monitoring</literal> namespace for default platform monitoring or the <literal>openshift-user-workload-monitoring</literal> namespace for user workload monitoring.</simpara>
<caution>
<simpara>To reduce security risks, use HTTPS and authentication to send metrics to an endpoint.</simpara>
</caution>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To configure remote write for the Prometheus instance that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a <literal>remoteWrite:</literal> section under <literal>data/config.yaml/prometheusK8s</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add an endpoint URL and authentication credentials in this section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com" <co xml:id="CO11-1"/>
        &lt;endpoint_authentication_credentials&gt; <co xml:id="CO11-2"/></programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>The URL of the remote write endpoint.</para>
</callout>
<callout arearefs="CO11-2">
<para>The authentication method and credentials for the endpoint.
Currently supported authentication methods are AWS Signature Version 4, authentication using HTTP in an <literal>Authorization</literal> request header, Basic authentication, OAuth 2.0, and TLS client.
See <emphasis>Supported remote write authentication settings</emphasis> for sample configurations of supported authentication methods.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Add write relabel configuration values after the authentication credentials:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        &lt;write_relabel_configs&gt; <co xml:id="CO12-1"/></programlisting>
<calloutlist>
<callout arearefs="CO12-1">
<para>The write relabel configuration settings.</para>
</callout>
</calloutlist>
<simpara>For <literal>&lt;write_relabel_configs&gt;</literal> substitute a list of write relabel configurations for metrics that you want to send to the remote endpoint.</simpara>
<simpara>The following sample shows how to forward a single metric called <literal>my_metric</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: 'my_metric'
          action: keep</programlisting>
<simpara>See the <link xl:href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config">Prometheus relabel_config documentation</link> for information about write relabel configuration options.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To configure remote write for the Prometheus instance that monitors user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a <literal>remoteWrite:</literal> section under <literal>data/config.yaml/prometheus</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add an endpoint URL and authentication credentials in this section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com" <co xml:id="CO13-1"/>
        &lt;endpoint_authentication_credentials&gt; <co xml:id="CO13-2"/></programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>The URL of the remote write endpoint.</para>
</callout>
<callout arearefs="CO13-2">
<para>The authentication method and credentials for the endpoint.
Currently supported authentication methods are AWS Signature Version 4, authentication using HTTP an <literal>Authorization</literal> request header, basic authentication, OAuth 2.0, and TLS client.
See <emphasis>Supported remote write authentication settings</emphasis> below for sample configurations of supported authentication methods.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Add write relabel configuration values after the authentication credentials:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        &lt;write_relabel_configs&gt; <co xml:id="CO14-1"/></programlisting>
<calloutlist>
<callout arearefs="CO14-1">
<para>The write relabel configuration settings.</para>
</callout>
</calloutlist>
<simpara>For <literal>&lt;write_relabel_configs&gt;</literal> substitute a list of write relabel configurations for metrics that you want to send to the remote endpoint.</simpara>
<simpara>The following sample shows how to forward a single metric called <literal>my_metric</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: 'my_metric'
          action: keep</programlisting>
<simpara>See the <link xl:href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config">Prometheus relabel_config documentation</link> for information about write relabel configuration options.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods affected by the new configuration restart automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>Saving changes to a monitoring <literal>ConfigMap</literal> object might redeploy the pods and other resources in the related project. Saving changes might also restart the running monitoring processes in that project.</simpara>
</warning>
</listitem>
</orderedlist>
<section xml:id="supported_remote_write_authentication_settings_configuring-the-monitoring-stack">
<title>Supported remote write authentication settings</title>
<simpara>You can use different methods to authenticate with a remote write endpoint. Currently supported authentication methods are AWS Signature Version 4, basic authentication, authorization, OAuth 2.0, and TLS client. The following table provides details about supported authentication methods for use with remote write.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Authentication method</entry>
<entry align="left" valign="top">Config map field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>AWS Signature Version 4</simpara></entry>
<entry align="left" valign="top"><simpara><literal>sigv4</literal></simpara></entry>
<entry align="left" valign="top"><simpara>This method uses AWS Signature Version 4 authentication to sign requests.
You cannot use this method simultaneously with authorization, OAuth 2.0, or Basic authentication.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Basic authentication</simpara></entry>
<entry align="left" valign="top"><simpara><literal>basicAuth</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Basic authentication sets the authorization header on every remote write request with the configured username and password.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>authorization</simpara></entry>
<entry align="left" valign="top"><simpara><literal>authorization</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Authorization sets the <literal>Authorization</literal> header on every remote write request using the configured token.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>OAuth 2.0</simpara></entry>
<entry align="left" valign="top"><simpara><literal>oauth2</literal></simpara></entry>
<entry align="left" valign="top"><simpara>An OAuth 2.0 configuration uses the client credentials grant type.
Prometheus fetches an access token from <literal>tokenUrl</literal> with the specified client ID and client secret to access the remote write endpoint.
You cannot use this method simultaneously with authorization, AWS Signature Version 4, or Basic authentication.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>TLS client</simpara></entry>
<entry align="left" valign="top"><simpara><literal>tlsConfig</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A TLS client configuration specifies the CA certificate, the client certificate, and the client key file information used to authenticate with the remote write endpoint server using TLS.
The sample configuration assumes that you have already created a CA certificate file, a client certificate file, and a client key file.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="example-remote-write-authentication-settings_configuring-the-monitoring-stack">
<title>Example remote write authentication settings</title>
<simpara>The following samples show different authentication settings you can use to connect to a remote write endpoint. Each sample also shows how to configure a corresponding <literal>Secret</literal> object that contains authentication credentials and other relevant settings. Each sample configures authentication for use with
default platform monitoring
in the <literal>openshift-monitoring</literal> namespace.</simpara>
<example>
<title>Sample YAML for AWS Signature Version 4 authentication</title>
<simpara>The following shows the settings for a <literal>sigv4</literal> secret named <literal>sigv4-credentials</literal> in the <literal>openshift-monitoring</literal> namespace.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: sigv4-credentials
  namespace: openshift-monitoring
stringData:
  accessKey: &lt;AWS_access_key&gt; <co xml:id="CO15-1"/>
  secretKey: &lt;AWS_secret_key&gt; <co xml:id="CO15-2"/>
type: Opaque</programlisting>
<calloutlist>
<callout arearefs="CO15-1">
<para>The AWS API access key.</para>
</callout>
<callout arearefs="CO15-2">
<para>The AWS API secret key.</para>
</callout>
</calloutlist>
<simpara>The following shows sample AWS Signature Version 4 remote write authentication settings that use a <literal>Secret</literal> object named <literal>sigv4-credentials</literal> in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        sigv4:
          region: &lt;AWS_region&gt; <co xml:id="CO16-1"/>
          accessKey:
            name: sigv4-credentials <co xml:id="CO16-2"/>
            key: accessKey <co xml:id="CO16-3"/>
          secretKey:
            name: sigv4-credentials <co xml:id="CO16-4"/>
            key: secretKey <co xml:id="CO16-5"/>
          profile: &lt;AWS_profile_name&gt; <co xml:id="CO16-6"/>
          roleArn: &lt;AWS_role_arn&gt; <co xml:id="CO16-7"/></programlisting>
<calloutlist>
<callout arearefs="CO16-1">
<para>The AWS region.</para>
</callout>
<callout arearefs="CO16-2 CO16-4">
<para>The name of the <literal>Secret</literal> object containing the AWS API access credentials.</para>
</callout>
<callout arearefs="CO16-3">
<para>The key that contains the AWS API access key in the specified <literal>Secret</literal> object.</para>
</callout>
<callout arearefs="CO16-5">
<para>The key that contains the AWS API secret key in the specified <literal>Secret</literal> object.</para>
</callout>
<callout arearefs="CO16-6">
<para>The name of the AWS profile that is being used to authenticate.</para>
</callout>
<callout arearefs="CO16-7">
<para>The unique identifier for the Amazon Resource Name (ARN) assigned to your role.</para>
</callout>
</calloutlist>
</example>
<example>
<title>Sample YAML for basic authentication</title>
<simpara>The following shows sample basic authentication settings for a <literal>Secret</literal> object named <literal>rw-basic-auth</literal> in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: rw-basic-auth
  namespace: openshift-monitoring
stringData:
  user: &lt;basic_username&gt; <co xml:id="CO17-1"/>
  password: &lt;basic_password&gt; <co xml:id="CO17-2"/>
type: Opaque</programlisting>
<calloutlist>
<callout arearefs="CO17-1">
<para>The username.</para>
</callout>
<callout arearefs="CO17-2">
<para>The password.</para>
</callout>
</calloutlist>
<simpara>The following sample shows a <literal>basicAuth</literal> remote write configuration that uses a <literal>Secret</literal> object named <literal>rw-basic-auth</literal> in the <literal>openshift-monitoring</literal> namespace.
It assumes that you have already set up authentication credentials for the endpoint.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://basicauth.example.com/api/write"
        basicAuth:
          username:
            name: rw-basic-auth <co xml:id="CO18-1"/>
            key: user <co xml:id="CO18-2"/>
          password:
            name: rw-basic-auth <co xml:id="CO18-3"/>
            key: password <co xml:id="CO18-4"/></programlisting>
<calloutlist>
<callout arearefs="CO18-1 CO18-3">
<para>The name of the <literal>Secret</literal> object that contains the authentication credentials.</para>
</callout>
<callout arearefs="CO18-2">
<para>The key that contains the username  in the specified <literal>Secret</literal> object.</para>
</callout>
<callout arearefs="CO18-4">
<para>The key that contains the password in the specified <literal>Secret</literal> object.</para>
</callout>
</calloutlist>
</example>
<example>
<title>Sample YAML for authentication with a bearer token using a <literal>Secret</literal> Object</title>
<simpara>The following shows bearer token settings for a <literal>Secret</literal> object named <literal>rw-bearer-auth</literal> in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: rw-bearer-auth
  namespace: openshift-monitoring
stringData:
  token: &lt;authentication_token&gt; <co xml:id="CO19-1"/>
type: Opaque</programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>The authentication token.</para>
</callout>
</calloutlist>
<simpara>The following shows sample bearer token config map settings that use a <literal>Secret</literal> object named <literal>rw-bearer-auth</literal> in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true
    prometheusK8s:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        authorization:
          type: Bearer <co xml:id="CO20-1"/>
          credentials:
            name: rw-bearer-auth <co xml:id="CO20-2"/>
            key: token <co xml:id="CO20-3"/></programlisting>
<calloutlist>
<callout arearefs="CO20-1">
<para>The authentication type of the request. The default value is <literal>Bearer</literal>.</para>
</callout>
<callout arearefs="CO20-2">
<para>The name of the <literal>Secret</literal> object that contains the authentication credentials.</para>
</callout>
<callout arearefs="CO20-3">
<para>The key that contains the authentication token in the specified <literal>Secret</literal> object.</para>
</callout>
</calloutlist>
</example>
<example>
<title>Sample YAML for OAuth 2.0 authentication</title>
<simpara>The following shows sample OAuth 2.0 settings for a <literal>Secret</literal> object named <literal>oauth2-credentials</literal> in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: oauth2-credentials
  namespace: openshift-monitoring
stringData:
  id: &lt;oauth2_id&gt; <co xml:id="CO21-1"/>
  secret: &lt;oauth2_secret&gt; <co xml:id="CO21-2"/>
  token: &lt;oauth2_authentication_token&gt; <co xml:id="CO21-3"/>
type: Opaque</programlisting>
<calloutlist>
<callout arearefs="CO21-1">
<para>The Oauth 2.0 ID.</para>
</callout>
<callout arearefs="CO21-2">
<para>The OAuth 2.0 secret.</para>
</callout>
<callout arearefs="CO21-3">
<para>The OAuth 2.0 token.</para>
</callout>
</calloutlist>
<simpara>The following shows an <literal>oauth2</literal> remote write authentication sample configuration that uses a <literal>Secret</literal> object named <literal>oauth2-credentials</literal> in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://test.example.com/api/write"
        oauth2:
          clientId:
            secret:
              name: oauth2-credentials <co xml:id="CO22-1"/>
              key: id <co xml:id="CO22-2"/>
          clientSecret:
            name: oauth2-credentials <co xml:id="CO22-3"/>
            key: secret <co xml:id="CO22-4"/>
          tokenUrl: https://example.com/oauth2/token <co xml:id="CO22-5"/>
          scopes: <co xml:id="CO22-6"/>
          - &lt;scope_1&gt;
          - &lt;scope_2&gt;
          endpointParams: <co xml:id="CO22-7"/>
            param1: &lt;parameter_1&gt;
            param2: &lt;parameter_2&gt;</programlisting>
<calloutlist>
<callout arearefs="CO22-1 CO22-3">
<para>The name of the corresponding <literal>Secret</literal> object. Note that <literal>ClientId</literal> can alternatively refer to a <literal>ConfigMap</literal> object, although <literal>clientSecret</literal> must refer to a <literal>Secret</literal> object.</para>
</callout>
<callout arearefs="CO22-2 CO22-4">
<para>The key that contains the OAuth 2.0 credentials in the specified <literal>Secret</literal> object.</para>
</callout>
<callout arearefs="CO22-5">
<para>The URL used to fetch a token with the specified <literal>clientId</literal> and <literal>clientSecret</literal>.</para>
</callout>
<callout arearefs="CO22-6">
<para>The OAuth 2.0 scopes for the authorization request. These scopes limit what data the tokens can access.</para>
</callout>
<callout arearefs="CO22-7">
<para>The OAuth 2.0 authorization request parameters required for the authorization server.</para>
</callout>
</calloutlist>
</example>
<example>
<title>Sample YAML for TLS client authentication</title>
<simpara>The following shows sample TLS client settings for a <literal>tls</literal> <literal>Secret</literal> object named <literal>mtls-bundle</literal> in the <literal>openshift-monitoring</literal> namespace.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: mtls-bundle
  namespace: openshift-monitoring
data:
  ca.crt: &lt;ca_cert&gt; <co xml:id="CO23-1"/>
  client.crt: &lt;client_cert&gt; <co xml:id="CO23-2"/>
  client.key: &lt;client_key&gt; <co xml:id="CO23-3"/>
type: tls</programlisting>
<calloutlist>
<callout arearefs="CO23-1">
<para>The CA certificate in the Prometheus container with which to validate the server certificate.</para>
</callout>
<callout arearefs="CO23-2">
<para>The client certificate for authentication with the server.</para>
</callout>
<callout arearefs="CO23-3">
<para>The client key.</para>
</callout>
</calloutlist>
<simpara>The following sample shows a <literal>tlsConfig</literal> remote write authentication configuration that uses a TLS <literal>Secret</literal> object named <literal>mtls-bundle</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        tlsConfig:
          ca:
            secret:
              name: mtls-bundle <co xml:id="CO24-1"/>
              key: ca.crt <co xml:id="CO24-2"/>
          cert:
            secret:
              name: mtls-bundle <co xml:id="CO24-3"/>
              key: client.crt <co xml:id="CO24-4"/>
          keySecret:
            name: mtls-bundle <co xml:id="CO24-5"/>
            key: client.key <co xml:id="CO24-6"/></programlisting>
<calloutlist>
<callout arearefs="CO24-1 CO24-3 CO24-5">
<para>The name of the corresponding <literal>Secret</literal> object that contains the TLS authentication credentials. Note that <literal>ca</literal> and <literal>cert</literal> can alternatively refer to a <literal>ConfigMap</literal> object, though <literal>keySecret</literal> must refer to a <literal>Secret</literal> object.</para>
</callout>
<callout arearefs="CO24-2">
<para>The key in the specified <literal>Secret</literal> object that contains the CA certificate for the endpoint.</para>
</callout>
<callout arearefs="CO24-4">
<para>The key in the specified <literal>Secret</literal> object that contains the client certificate for the endpoint.</para>
</callout>
<callout arearefs="CO24-6">
<para>The key in the specified <literal>Secret</literal> object that contains the client key secret.</para>
</callout>
</calloutlist>
</example>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage">Setting up remote write compatible endpoints</link> for steps to create a remote write compatible endpoint (such as Thanos).</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="https://prometheus.io/docs/practices/remote_write/#remote-write-tuning">Tuning remote write settings</link> for information about how to optimize remote write settings for different use cases.</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../nodes/pods/nodes-pods-secrets.xml#nodes-pods-secrets-about_nodes-pods-secrets">Understanding secrets</link> for steps to create and configure <literal>Secret</literal> objects in {product-title}.</simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="../rest_api/monitoring_apis/prometheus-monitoring-coreos-com-v1.xml#spec-remotewrite-2">Prometheus REST API reference for remote write</link> for information about additional optional fields.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="adding-cluster-id-labels-to-metrics_configuring-the-monitoring-stack">
<title>Adding cluster ID labels to metrics</title>
<simpara>If you manage multiple {product-title} clusters and use the remote write feature to send metrics data from these clusters to an external storage location, you can add cluster ID labels to identify the metrics data coming from different clusters. You can then query these labels to identify the source cluster for a metric and distinguish that data from similar metrics data sent by other clusters.</simpara>
<simpara>This way, if you manage many clusters for multiple customers and send metrics data to a single centralized storage system, you can use cluster ID labels to query metrics for a particular cluster or customer.</simpara>
<simpara>Creating and using cluster ID labels involves three general steps:</simpara>
<itemizedlist>
<listitem>
<simpara>Configuring the write relabel settings for remote write storage.</simpara>
</listitem>
<listitem>
<simpara>Adding cluster ID labels to the metrics.</simpara>
</listitem>
<listitem>
<simpara>Querying these labels to identify the source cluster or customer for a metric.</simpara>
</listitem>
</itemizedlist>
<section xml:id="creating-cluster-id-labels-for-metrics_configuring-the-monitoring-stack">
<title>Creating cluster ID labels for metrics</title>
<simpara>You can create cluster ID labels for metrics for default platform monitoring and for user workload monitoring.</simpara>
<simpara>For default platform monitoring, you add cluster ID labels for metrics in the <literal>write_relabel</literal> settings for remote write storage in the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> namespace.</simpara>
<simpara>For user workload monitoring, you edit the settings in the <literal>user-workload-monitoring-config</literal> config map in the <literal>openshift-user-workload-monitoring</literal> namespace.</simpara>
<note>
<simpara>When Prometheus scrapes user workload targets that expose a <literal>namespace</literal> label, the system stores this label as <literal>exported_namespace</literal>.
This behavior ensures that the final namespace label value is equal to the namespace of the target pod.
You cannot override this default configuration by setting the value of the <literal>honorLabels</literal> field to <literal>true</literal> for <literal>PodMonitor</literal> or <literal>ServiceMonitor</literal> objects.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring default platform monitoring components:</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects:</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have configured remote write storage.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To create cluster ID labels for core {product-title} metrics:</emphasis></simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>In the <literal>writeRelabelConfigs:</literal> section under <literal>data/config.yaml/prometheusK8s/remoteWrite</literal>, add cluster ID relabel configuration values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        writeRelabelConfigs: <co xml:id="CO25-1"/>
          - &lt;relabel_config&gt; <co xml:id="CO25-2"/></programlisting>
<calloutlist>
<callout arearefs="CO25-1">
<para>Add a list of write relabel configurations for metrics that you want to send to the remote endpoint.</para>
</callout>
<callout arearefs="CO25-2">
<para>Substitute the label configuration for the metrics sent to the remote write endpoint.</para>
</callout>
</calloutlist>
<simpara>The following sample shows how to forward a metric with the cluster ID label <literal>cluster_id</literal> in default platform monitoring:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__ <co xml:id="CO26-1"/>
          targetLabel: cluster_id <co xml:id="CO26-2"/>
          action: replace <co xml:id="CO26-3"/></programlisting>
<calloutlist>
<callout arearefs="CO26-1">
<para>The system initially applies a temporary cluster ID source label named <literal>__tmp_openshift_cluster_id__</literal>. This temporary label gets replaced by the cluster ID label name that you specify.</para>
</callout>
<callout arearefs="CO26-2">
<para>Specify the name of the cluster ID label for metrics sent to remote write storage.
If you use a label name that already exists for a metric, that value is overwritten with the name of this cluster ID label.
For the label name, do not use <literal>__tmp_openshift_cluster_id__</literal>. The final relabeling step removes labels that use this name.</para>
</callout>
<callout arearefs="CO26-3">
<para>The <literal>replace</literal> write relabel action replaces the temporary label with the target label for outgoing metrics.
This action is the default and is applied if no action is specified.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To create cluster ID labels for user-defined project metrics:</emphasis></simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>In the <literal>writeRelabelConfigs:</literal> section under <literal>data/config.yaml/prometheus/remoteWrite</literal>, add cluster ID relabel configuration values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        &lt;endpoint_authentication_credentials&gt;
        writeRelabelConfigs: <co xml:id="CO27-1"/>
          - &lt;relabel_config&gt; <co xml:id="CO27-2"/></programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para>Add a list of write relabel configurations for metrics that you want to send to the remote endpoint.</para>
</callout>
<callout arearefs="CO27-2">
<para>Substitute the label configuration for the metrics sent to the remote write endpoint.</para>
</callout>
</calloutlist>
<simpara>The following sample shows how to forward a metric with the cluster ID label <literal>cluster_id</literal> in user-workload monitoring:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__ <co xml:id="CO28-1"/>
          targetLabel: cluster_id <co xml:id="CO28-2"/>
          action: replace <co xml:id="CO28-3"/></programlisting>
<calloutlist>
<callout arearefs="CO28-1">
<para>The system initially applies a temporary cluster ID source label named <literal>__tmp_openshift_cluster_id__</literal>. This temporary label gets replaced by the cluster ID label name that you specify.</para>
</callout>
<callout arearefs="CO28-2">
<para>Specify the name of the cluster ID label for metrics sent to remote write storage. If you use a label name that already exists for a metric, that value is overwritten with the name of this cluster ID label. For the label name, do not use <literal>__tmp_openshift_cluster_id__</literal>. The final relabeling step removes labels that use this name.</para>
</callout>
<callout arearefs="CO28-3">
<para>The <literal>replace</literal> write relabel action replaces the temporary label with the target label for outgoing metrics. This action is the default and is applied if no action is specified.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes to the <literal>ConfigMap</literal> object.
The pods affected by the updated configuration automatically restart.</simpara>
<warning>
<simpara>Saving changes to a monitoring <literal>ConfigMap</literal> object might redeploy the pods and other resources in the related project. Saving changes might also restart the running monitoring processes in that project.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For details about write relabel configuration, see <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#configuring_remote_write_storage_configuring-the-monitoring-stack">Configuring remote write storage</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-metrics-collection-profiles_configuring-the-monitoring-stack">
<title>Configuring metrics collection profiles</title>
<important>
<simpara>Using a metrics collection profile is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete.
Red Hat does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xl:href="https://access.redhat.com/support/offerings/techpreview">https://access.redhat.com/support/offerings/techpreview</link>.</simpara>
</important>
<simpara>By default, Prometheus collects metrics exposed by all default metrics targets in {product-title} components.
However, you might want Prometheus to collect fewer metrics from a cluster in certain scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>If cluster administrators require only alert, telemetry, and console metrics and do not require other metrics to be available.</simpara>
</listitem>
<listitem>
<simpara>If a cluster increases in size, and the increased size of the default metrics data collected now requires a significant increase in CPU and memory resources.</simpara>
</listitem>
</itemizedlist>
<simpara>You can use a metrics collection profile to collect either the default amount of metrics data or a minimal amount of metrics data.
When you collect minimal metrics data, basic monitoring features such as alerting continue to work.
At the same time, the CPU and memory resources required by Prometheus decrease.</simpara>
<section xml:id="about-metrics-collection-profiles_configuring-the-monitoring-stack">
<title>About metrics collection profiles</title>
<simpara>You can enable one of two metrics collection profiles:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">full</emphasis>: Prometheus collects metrics data exposed by all platform components. This setting is the default.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">minimal</emphasis>: Prometheus collects only the metrics data required for platform alerts, recording rules, telemetry, and console dashboards.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="choosing-a-metrics-collection-profile_configuring-the-monitoring-stack">
<title>Choosing a metrics collection profile</title>
<simpara>To choose a metrics collection profile for core {product-title} monitoring components, edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have enabled Technology Preview features by using the <literal>FeatureGate</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
</itemizedlist>
<warning>
<simpara>Saving changes to a monitoring config map might restart monitoring processes and redeploy the pods and other resources in the related project.
The running monitoring processes in that project might also restart.</simpara>
</warning>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add the metrics collection profile setting under <literal>data/config.yaml/prometheusK8s</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      collectionProfile: &lt;metrics_collection_profile_name&gt; <co xml:id="CO29-1"/></programlisting>
<calloutlist>
<callout arearefs="CO29-1">
<para>The name of the metrics collection profile.
The available values are <literal>full</literal> or <literal>minimal</literal>.
If you do not specify a value or if the <literal>collectionProfile</literal> key name does not exist in the config map, the default setting of <literal>full</literal> is used.</para>
</callout>
</calloutlist>
<simpara>The following example sets the metrics collection profile to <literal>minimal</literal> for the core platform instance of Prometheus:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      collectionProfile: <emphasis role="strong">minimal</emphasis></programlisting>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods affected by the new configuration restart automatically.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/managing-metrics.xml#viewing-a-list-of-available-metrics_managing-metrics">Viewing a list of available metrics</link> for steps to view a list of metrics being collected for a cluster.</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../nodes/clusters/nodes-cluster-enabling-features.xml">Enabling features using feature gates</link> for steps to enable Technology Preview features.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="controlling-the-impact-of-unbound-attributes-in-user-defined-projects_configuring-the-monitoring-stack">
<title>Controlling the impact of unbound metrics attributes in user-defined projects</title>
<simpara>Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a <literal>customer_id</literal> attribute is unbound because it has an infinite number of possible values.</simpara>
<simpara>Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.</simpara>
<simpara>Cluster administrators
can use the following measures to control the impact of unbound metrics attributes in user-defined projects:</simpara>
<itemizedlist>
<listitem>
<simpara>Limit the number of samples that can be accepted per target scrape in user-defined projects</simpara>
</listitem>
<listitem>
<simpara>Limit the number of scraped labels, the length of label names, and the length of label values</simpara>
</listitem>
<listitem>
<simpara>Create alerts that fire when a scrape sample threshold is reached or when the target cannot be scraped</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Limiting scrape samples can help prevent the issues caused by adding many unbound attributes to labels. Developers can also prevent the underlying cause by limiting the number of unbound attributes that they define for metrics. Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.</simpara>
</note>
<section xml:id="setting-scrape-sample-and-label-limits-for-user-defined-projects_configuring-the-monitoring-stack">
<title>Setting scrape sample and label limits for user-defined projects</title>
<simpara>You can limit the number of samples that can be accepted per target scrape in user-defined projects. You can also limit the number of scraped labels, the length of label names, and the length of label values.</simpara>
<warning>
<simpara>If you set sample or label limits, no further sample data is ingested for that target scrape after the limit is reached.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>enforcedSampleLimit</literal> configuration to <literal>data/config.yaml</literal> to limit the number of samples that can be accepted per target scrape in user-defined projects:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      enforcedSampleLimit: 50000 <co xml:id="CO30-1"/></programlisting>
<calloutlist>
<callout arearefs="CO30-1">
<para>A value is required if this parameter is specified. This <literal>enforcedSampleLimit</literal> example limits the number of samples that can be accepted per target scrape in user-defined projects to 50,000.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Add the <literal>enforcedLabelLimit</literal>, <literal>enforcedLabelNameLengthLimit</literal>, and <literal>enforcedLabelValueLengthLimit</literal> configurations to <literal>data/config.yaml</literal> to limit the number of scraped labels, the length of label names, and the length of label values in user-defined projects:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      enforcedLabelLimit: 500 <co xml:id="CO31-1"/>
      enforcedLabelNameLengthLimit: 50 <co xml:id="CO31-2"/>
      enforcedLabelValueLengthLimit: 600 <co xml:id="CO31-3"/></programlisting>
<calloutlist>
<callout arearefs="CO31-1">
<para>Specifies the maximum number of labels per scrape.
The default value is <literal>0</literal>, which specifies no limit.</para>
</callout>
<callout arearefs="CO31-2">
<para>Specifies the maximum length in characters of a label name.
The default value is <literal>0</literal>, which specifies no limit.</para>
</callout>
<callout arearefs="CO31-3">
<para>Specifies the maximum length in characters of a label value.
The default value is <literal>0</literal>, which specifies no limit.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The limits are applied automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When changes are saved to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object, the pods and other resources in the <literal>openshift-user-workload-monitoring</literal> project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-scrape-sample-alerts_configuring-the-monitoring-stack">
<title>Creating scrape sample alerts</title>
<simpara>You can create alerts that notify you when:</simpara>
<itemizedlist>
<listitem>
<simpara>The target cannot be scraped or is not available for the specified <literal>for</literal> duration</simpara>
</listitem>
<listitem>
<simpara>A scrape sample threshold is reached or is exceeded for the specified <literal>for</literal> duration</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have limited the number of samples that can be accepted per target scrape in user-defined projects, by using <literal>enforcedSampleLimit</literal>.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file with alerts that inform you when the targets are down and when the enforced sample limit is approaching. The file in this example is called <literal>monitoring-stack-alerts.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: monitoring-stack-alerts <co xml:id="CO32-1"/>
  namespace: ns1 <co xml:id="CO32-2"/>
spec:
  groups:
  - name: general.rules
    rules:
    - alert: TargetDown <co xml:id="CO32-3"/>
      annotations:
        message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
          }} targets in {{ $labels.namespace }} namespace are down.' <co xml:id="CO32-4"/>
      expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,
        namespace, service)) &gt; 10
      for: 10m <co xml:id="CO32-5"/>
      labels:
        severity: warning <co xml:id="CO32-6"/>
    - alert: ApproachingEnforcedSamplesLimit <co xml:id="CO32-7"/>
      annotations:
        message: '{{ $labels.container }} container of the {{ $labels.pod }} pod in the {{ $labels.namespace }} namespace consumes {{ $value | humanizePercentage }} of the samples limit budget.' <co xml:id="CO32-8"/>
      expr: scrape_samples_scraped/50000 &gt; 0.8 <co xml:id="CO32-9"/>
      for: 10m <co xml:id="CO32-10"/>
      labels:
        severity: warning <co xml:id="CO32-11"/></programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>Defines the name of the alerting rule.</para>
</callout>
<callout arearefs="CO32-2">
<para>Specifies the user-defined project where the alerting rule will be deployed.</para>
</callout>
<callout arearefs="CO32-3">
<para>The <literal>TargetDown</literal> alert will fire if the target cannot be scraped or is not available for the <literal>for</literal> duration.</para>
</callout>
<callout arearefs="CO32-4">
<para>The message that will be output when the <literal>TargetDown</literal> alert fires.</para>
</callout>
<callout arearefs="CO32-5">
<para>The conditions for the <literal>TargetDown</literal> alert must be true for this duration before the alert is fired.</para>
</callout>
<callout arearefs="CO32-6">
<para>Defines the severity for the <literal>TargetDown</literal> alert.</para>
</callout>
<callout arearefs="CO32-7">
<para>The <literal>ApproachingEnforcedSamplesLimit</literal> alert will fire when the defined scrape sample threshold is reached or exceeded for the specified <literal>for</literal> duration.</para>
</callout>
<callout arearefs="CO32-8">
<para>The message that will be output when the <literal>ApproachingEnforcedSamplesLimit</literal> alert fires.</para>
</callout>
<callout arearefs="CO32-9">
<para>The threshold for the <literal>ApproachingEnforcedSamplesLimit</literal> alert. In this example the alert will fire when the number of samples per target scrape has exceeded 80% of the enforced sample limit of <literal>50000</literal>. The <literal>for</literal> duration must also have passed before the alert will fire. The <literal>&lt;number&gt;</literal> in the expression <literal>scrape_samples_scraped/&lt;number&gt; &gt; &lt;threshold&gt;</literal> must match the <literal>enforcedSampleLimit</literal> value defined in the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</para>
</callout>
<callout arearefs="CO32-10">
<para>The conditions for the <literal>ApproachingEnforcedSamplesLimit</literal> alert must be true for this duration before the alert is fired.</para>
</callout>
<callout arearefs="CO32-11">
<para>Defines the severity for the <literal>ApproachingEnforcedSamplesLimit</literal> alert.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration to the user-defined project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f monitoring-stack-alerts.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack">Creating a user-defined workload monitoring config map</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/troubleshooting-monitoring-issues.xml#determining-why-prometheus-is-consuming-disk-space_troubleshooting-monitoring-issues">Determining why Prometheus is consuming a lot of disk space</link> for steps to query which metrics have the highest number of scrape samples.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="monitoring-configuring-external-alertmanagers_configuring-the-monitoring-stack">
<title>Configuring external Alertmanager instances</title>
<simpara>The {product-title} monitoring stack includes a local Alertmanager instance that routes alerts from Prometheus.
You can add external Alertmanager instances to route alerts for core {product-title} projects or user-defined projects.</simpara>
<simpara>If you add the same external Alertmanager configuration for multiple clusters and disable the local instance for each cluster, you can then manage alert routing for multiple clusters by using a single external Alertmanager instance.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components in the <literal>openshift-monitoring</literal> project</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> config map.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> config map.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To configure additional Alertmanagers for routing alerts from core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add an <literal>additionalAlertmanagerConfigs:</literal> section under <literal>data/config.yaml/prometheusK8s</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add the configuration details for additional Alertmanagers in this section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      additionalAlertmanagerConfigs:
      - &lt;alertmanager_specification&gt;</programlisting>
<simpara>For <literal>&lt;alertmanager_specification&gt;</literal>, substitute authentication and other configuration details for additional Alertmanager instances.
Currently supported authentication methods are bearer token (<literal>bearerToken</literal>) and client TLS (<literal>tlsConfig</literal>).
The following sample config map configures an additional Alertmanager using a bearer token with client TLS authentication:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      additionalAlertmanagerConfigs:
      - scheme: https
        pathPrefix: /
        timeout: "30s"
        apiVersion: v1
        bearerToken:
          name: alertmanager-bearer-token
          key: token
        tlsConfig:
          key:
            name: alertmanager-tls
            key: tls.key
          cert:
            name: alertmanager-tls
            key: tls.crt
          ca:
            name: alertmanager-tls
            key: tls.ca
        staticConfigs:
        - external-alertmanager1-remote.com
        - external-alertmanager1-remote2.com</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To configure additional Alertmanager instances for routing alerts from user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> config map in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a <literal>&lt;component&gt;/additionalAlertmanagerConfigs:</literal> section under <literal>data/config.yaml/</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add the configuration details for additional Alertmanagers in this section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;:
      additionalAlertmanagerConfigs:
      - &lt;alertmanager_specification&gt;</programlisting>
<simpara>For <literal>&lt;component&gt;</literal>, substitute one of two supported external Alertmanager components: <literal>prometheus</literal> or <literal>thanosRuler</literal>.</simpara>
<simpara>For <literal>&lt;alertmanager_specification&gt;</literal>, substitute authentication and other configuration details for additional Alertmanager instances. Currently supported authentication methods are bearer token (<literal>bearerToken</literal>) and client TLS (<literal>tlsConfig</literal>). The following sample config map configures an additional Alertmanager using Thanos Ruler with a bearer token and client TLS authentication:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      additionalAlertmanagerConfigs:
      - scheme: https
        pathPrefix: /
        timeout: "30s"
        apiVersion: v1
        bearerToken:
          name: alertmanager-bearer-token
          key: token
        tlsConfig:
          key:
            name: alertmanager-tls
            key: tls.key
          cert:
            name: alertmanager-tls
            key: tls.crt
          ca:
            name: alertmanager-tls
            key: tls.ca
        staticConfigs:
        - external-alertmanager1-remote.com
        - external-alertmanager1-remote2.com</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes to the <literal>ConfigMap</literal> object. The new component placement configuration is applied automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save the file to apply the changes to the <literal>ConfigMap</literal> object. The new component placement configuration is applied automatically.</simpara>
</listitem>
</orderedlist>
</chapter>
<chapter xml:id="monitoring-configuring-secrets-for-alertmanager_configuring-the-monitoring-stack">
<title>Configuring secrets for Alertmanager</title>
<simpara>The {product-title} monitoring stack includes Alertmanager, which routes alerts from Prometheus to endpoint receivers.
If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can configure Alertmanager to use a secret that contains authentication credentials for the receiver.</simpara>
<simpara>For example, you can configure Alertmanager to use a secret to authenticate with an endpoint receiver that requires a certificate issued by a private Certificate Authority (CA).
You can also configure Alertmanager to use a secret to authenticate with a receiver that requires a password file for Basic HTTP authentication.
In either case, authentication details are contained in the <literal>Secret</literal> object rather than in the <literal>ConfigMap</literal> object.</simpara>
<section xml:id="monitoring-adding-a-secret-to-the-alertmanager-configuration_configuring-the-monitoring-stack">
<title>Adding a secret to the Alertmanager configuration</title>
<simpara>You can add secrets to the Alertmanager configuration for core platform monitoring components by editing the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> project.</simpara>
<simpara>After you add a secret to the config map, the secret is mounted as a volume at <literal>/etc/alertmanager/secrets/&lt;secret_name&gt;</literal> within the <literal>alertmanager</literal> container for the Alertmanager pods.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components in the <literal>openshift-monitoring</literal> project</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> config map.</simpara>
</listitem>
<listitem>
<simpara>You have created the secret to be configured in Alertmanager in the <literal>openshift-monitoring</literal> project.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>A cluster administrator has enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the secret to be configured in Alertmanager in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To add a secret configuration to Alertmanager for core platform monitoring</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a <literal>secrets:</literal> section under <literal>data/config.yaml/alertmanagerMain</literal> with the following configuration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      secrets: <co xml:id="CO33-1"/>
      - &lt;secret_name_1&gt; <co xml:id="CO33-2"/>
      - &lt;secret_name_2&gt;</programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>This section contains the secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object.</para>
</callout>
<callout arearefs="CO33-2">
<para>The name of the <literal>Secret</literal> object that contains authentication credentials for the receiver. If you add multiple secrets, place each one on a new line.</para>
</callout>
</calloutlist>
<simpara>The following sample config map settings configure Alertmanager to use two <literal>Secret</literal> objects named <literal>test-secret-basic-auth</literal> and <literal>test-secret-api-token</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      secrets:
      - test-secret-basic-auth
      - test-secret-api-token</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To add a secret configuration to Alertmanager for user-defined project monitoring</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> config map in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a <literal>secrets:</literal> section under <literal>data/config.yaml/alertmanager/secrets</literal> with the following configuration:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      secrets: <co xml:id="CO34-1"/>
      - &lt;secret_name_1&gt; <co xml:id="CO34-2"/>
      - &lt;secret_name_2&gt;</programlisting>
<calloutlist>
<callout arearefs="CO34-1">
<para>This section contains the secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object.</para>
</callout>
<callout arearefs="CO34-2">
<para>The name of the <literal>Secret</literal> object that contains authentication credentials for the receiver. If you add multiple secrets, place each one on a new line.</para>
</callout>
</calloutlist>
<simpara>The following sample config map settings configure Alertmanager to use two <literal>Secret</literal> objects named <literal>test-secret</literal> and <literal>test-secret-api-token</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      enabled: true
      secrets:
      - test-secret
      - test-api-receiver-token</programlisting>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes to the <literal>ConfigMap</literal> object. The new configuration is applied automatically.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="attaching-additional-labels-to-your-time-series-and-alerts_configuring-the-monitoring-stack">
<title>Attaching additional labels to your time series and alerts</title>
<simpara>Using the external labels feature of Prometheus, you can attach custom labels to all time series and alerts leaving Prometheus.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are configuring core {product-title} monitoring components</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are configuring components that monitor user-defined projects</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To attach custom labels to all time series and alerts leaving the Prometheus instance that monitors core {product-title} projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Define a map of labels you want to add for every metric under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      externalLabels:
        &lt;key&gt;: &lt;value&gt; <co xml:id="CO35-1"/></programlisting>
<calloutlist>
<callout arearefs="CO35-1">
<para>Substitute <literal>&lt;key&gt;: &lt;value&gt;</literal> with a map of key-value pairs where <literal>&lt;key&gt;</literal> is a unique name for the new label and <literal>&lt;value&gt;</literal> is its value.</para>
</callout>
</calloutlist>
<warning>
<simpara>Do not use <literal>prometheus</literal> or <literal>prometheus_replica</literal> as key names, because they are reserved and will be overwritten.</simpara>
</warning>
<simpara>For example, to add metadata about the region and environment to all time series and alerts, use:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      externalLabels:
        region: eu
        environment: prod</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To attach custom labels to all time series and alerts leaving the Prometheus instance that monitors user-defined projects</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Define a map of labels you want to add for every metric under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      externalLabels:
        &lt;key&gt;: &lt;value&gt; <co xml:id="CO36-1"/></programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>Substitute <literal>&lt;key&gt;: &lt;value&gt;</literal> with a map of key-value pairs where <literal>&lt;key&gt;</literal> is a unique name for the new label and <literal>&lt;value&gt;</literal> is its value.</para>
</callout>
</calloutlist>
<warning>
<simpara>Do not use <literal>prometheus</literal> or <literal>prometheus_replica</literal> as key names, because they are reserved and will be overwritten.</simpara>
</warning>
<note>
<simpara>In the <literal>openshift-user-workload-monitoring</literal> project, Prometheus handles metrics and Thanos Ruler handles alerting and recording rules. Setting <literal>externalLabels</literal> for <literal>prometheus</literal> in the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object will only configure external labels for metrics and not for any rules.</simpara>
</note>
<simpara>For example, to add metadata about the region and environment to all time series and alerts related to user-defined projects, use:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      externalLabels:
        region: eu
        environment: prod</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The new configuration is applied automatically.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps.</simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="configuring_pod_topology_spread_constraintsfor_monitoring_configuring-the-monitoring-stack">
<title>Configuring pod topology spread constraints for monitoring</title>
<simpara>You can use pod topology spread constraints to control how
Prometheus, Thanos Ruler, and Alertmanager
pods are spread across a network topology when {product-title} pods are deployed in multiple availability zones.</simpara>
<simpara>Pod topology spread constraints are suitable for controlling pod scheduling within hierarchical topologies in which nodes are spread across different infrastructure levels, such as regions and zones within those regions.
Additionally, by being able to schedule pods in different zones, you can improve network latency in certain scenarios.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../nodes/scheduling/nodes-scheduler-pod-topology-spread-constraints.xml#nodes-scheduler-pod-topology-spread-constraints-about">Controlling pod placement by using pod topology spread constraints</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/">Kubernetes Pod Topology Spread Constraints documentation</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="setting-up-pod-topology-spread-constraints-for-prometheus_configuring-the-monitoring-stack">
<title>Setting up pod topology spread constraints for Prometheus</title>
<simpara>For core {product-title} platform monitoring, you can set up pod topology spread constraints for Prometheus to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Prometheus pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.</simpara>
<simpara>You configure pod topology spread constraints for Prometheus in the <literal>cluster-monitoring-config</literal> config map.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add  values for the following settings under <literal>data/config.yaml/prometheusK8s</literal> to configure pod topology spread constraints:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      topologySpreadConstraints:
      - maxSkew: 1 <co xml:id="CO37-1"/>
        topologyKey: monitoring <co xml:id="CO37-2"/>
        whenUnsatisfiable: DoNotSchedule <co xml:id="CO37-3"/>
        labelSelector:
          matchLabels: <co xml:id="CO37-4"/>
            app.kubernetes.io/name: prometheus</programlisting>
<calloutlist>
<callout arearefs="CO37-1">
<para>Specify a numeric value for <literal>maxSkew</literal>, which defines the degree to which pods are allowed to be unevenly distributed.
This field is required, and the value must be greater than zero.
The value specified has a different effect depending on what value you specify for <literal>whenUnsatisfiable</literal>.</para>
</callout>
<callout arearefs="CO37-2">
<para>Specify a key of node labels for <literal>topologyKey</literal>.
This field is required.
Nodes that have a label with this key and identical values are considered to be in the same topology.
The scheduler will try to put a balanced number of pods into each domain.</para>
</callout>
<callout arearefs="CO37-3">
<para>Specify a value for <literal>whenUnsatisfiable</literal>.
This field is required.
Available options are <literal>DoNotSchedule</literal> and <literal>ScheduleAnyway</literal>.
Specify <literal>DoNotSchedule</literal> if you want the <literal>maxSkew</literal> value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.
Specify <literal>ScheduleAnyway</literal> if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.</para>
</callout>
<callout arearefs="CO37-4">
<para>Specify a value for <literal>matchLabels</literal>. This value is used to identify the set of matching pods to which to apply the constraints.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes automatically.</simpara>
<warning>
<simpara>When you save changes to the <literal>cluster-monitoring-config</literal> config map, the pods and other resources in the <literal>openshift-monitoring</literal> project might be redeployed.
The running monitoring processes in that project might also restart.</simpara>
</warning>
</listitem>
</orderedlist>
</section>
<section xml:id="setting-up-pod-topology-spread-constraints-for-alertmanager_configuring-the-monitoring-stack">
<title>Setting up pod topology spread constraints for Alertmanager</title>
<simpara>For core {product-title} platform monitoring, you can set up pod topology spread constraints for Alertmanager to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Alertmanager pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.</simpara>
<simpara>You configure pod topology spread constraints for Alertmanager in the <literal>cluster-monitoring-config</literal> config map.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add values for the following settings under <literal>data/config.yaml/alertmanagermain</literal> to configure pod topology spread constraints:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      topologySpreadConstraints:
      - maxSkew: 1 <co xml:id="CO38-1"/>
        topologyKey: monitoring <co xml:id="CO38-2"/>
        whenUnsatisfiable: DoNotSchedule <co xml:id="CO38-3"/>
        labelSelector:
          matchLabels: <co xml:id="CO38-4"/>
            app.kubernetes.io/name: alertmanager</programlisting>
<calloutlist>
<callout arearefs="CO38-1">
<para>Specify a numeric value for <literal>maxSkew</literal>, which defines the degree to which pods are allowed to be unevenly distributed.
This field is required, and the value must be greater than zero.
The value specified has a different effect depending on what value you specify for <literal>whenUnsatisfiable</literal>.</para>
</callout>
<callout arearefs="CO38-2">
<para>Specify a key of node labels for <literal>topologyKey</literal>.
This field is required.
Nodes that have a label with this key and identical values are considered to be in the same topology.
The scheduler will try to put a balanced number of pods into each domain.</para>
</callout>
<callout arearefs="CO38-3">
<para>Specify a value for <literal>whenUnsatisfiable</literal>.
This field is required.
Available options are <literal>DoNotSchedule</literal> and <literal>ScheduleAnyway</literal>.
Specify <literal>DoNotSchedule</literal> if you want the <literal>maxSkew</literal> value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.
Specify <literal>ScheduleAnyway</literal> if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.</para>
</callout>
<callout arearefs="CO38-4">
<para>Specify a value for <literal>matchLabels</literal>. This value is used to identify the set of matching pods to which to apply the constraints.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes automatically.</simpara>
<warning>
<simpara>When you save changes to the <literal>cluster-monitoring-config</literal> config map, the pods and other resources in the <literal>openshift-monitoring</literal> project might be redeployed.
The running monitoring processes in that project might also restart.</simpara>
</warning>
</listitem>
</orderedlist>
</section>
<section xml:id="setting-up-pod-topology-spread-constraints-for-thanos-ruler_configuring-the-monitoring-stack">
<title>Setting up pod topology spread constraints for Thanos Ruler</title>
<simpara>For user-defined monitoring, you can set up pod topology spread constraints for Thanos Ruler to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Thanos Ruler pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.</simpara>
<simpara>You configure pod topology spread constraints for Thanos Ruler in the <literal>user-workload-monitoring-config</literal> config map.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster administrator has enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> config map in the <literal>openshift-user-workload-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add values for the following settings under <literal>data/config.yaml/thanosRuler</literal> to configure pod topology spread constraints:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      topologySpreadConstraints:
      - maxSkew: 1 <co xml:id="CO39-1"/>
        topologyKey: monitoring <co xml:id="CO39-2"/>
        whenUnsatisfiable: ScheduleAnyway <co xml:id="CO39-3"/>
        labelSelector:
          matchLabels: <co xml:id="CO39-4"/>
            app.kubernetes.io/name: thanos-ruler</programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>Specify a numeric value for <literal>maxSkew</literal>, which defines the degree to which pods are allowed to be unevenly distributed. This field is required, and the value must be greater than zero. The value specified has a different effect depending on what value you specify for <literal>whenUnsatisfiable</literal>.</para>
</callout>
<callout arearefs="CO39-2">
<para>Specify a key of node labels for <literal>topologyKey</literal>. This field is required. Nodes that have a label with this key and identical values are considered to be in the same topology. The scheduler will try to put a balanced number of pods into each domain.</para>
</callout>
<callout arearefs="CO39-3">
<para>Specify a value for <literal>whenUnsatisfiable</literal>. This field is required. Available options are <literal>DoNotSchedule</literal> and <literal>ScheduleAnyway</literal>. Specify <literal>DoNotSchedule</literal> if you want the <literal>maxSkew</literal> value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.  Specify <literal>ScheduleAnyway</literal> if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.</para>
</callout>
<callout arearefs="CO39-4">
<para>Specify a value for <literal>matchLabels</literal>. This value is used to identify the set of matching pods to which to apply the constraints.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes automatically.</simpara>
<warning>
<simpara>When you save changes to the <literal>user-workload-monitoring-config</literal> config map, the pods and other resources in the <literal>openshift-user-workload-monitoring</literal> project might be redeployed.
The running monitoring processes in that project might also restart.</simpara>
</warning>
</listitem>
</orderedlist>
</section>
<section xml:id="setting-log-levels-for-monitoring-components_configuring-the-monitoring-stack">
<title>Setting log levels for monitoring components</title>
<simpara>You can configure the log level for
Alertmanager, Prometheus Operator, Prometheus, Thanos Querier, and Thanos Ruler.</simpara>
<simpara>The following log levels can be applied to the relevant component in the
<literal>cluster-monitoring-config</literal> and
<literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> objects:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>debug</literal>. Log debug, informational, warning, and error messages.</simpara>
</listitem>
<listitem>
<simpara><literal>info</literal>. Log informational, warning, and error messages.</simpara>
</listitem>
<listitem>
<simpara><literal>warn</literal>. Log warning and error messages only.</simpara>
</listitem>
<listitem>
<simpara><literal>error</literal>. Log error messages only.</simpara>
</listitem>
</itemizedlist>
<simpara>The default log level is <literal>info</literal>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are setting a log level for Alertmanager, Prometheus Operator, Prometheus, or Thanos Querier in the <literal>openshift-monitoring</literal> project</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are setting a log level for Prometheus Operator, Prometheus, or Thanos Ruler in the <literal>openshift-user-workload-monitoring</literal> project</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>ConfigMap</literal> object:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">To set a log level for a component in the <literal>openshift-monitoring</literal> project</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>logLevel: &lt;log_level&gt;</literal> for a component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <co xml:id="CO40-1"/>
      logLevel: &lt;log_level&gt; <co xml:id="CO40-2"/></programlisting>
<calloutlist>
<callout arearefs="CO40-1">
<para>The monitoring stack component for which you are setting a log level.
For default platform monitoring, available component values are <literal>prometheusK8s</literal>, <literal>alertmanagerMain</literal>, <literal>prometheusOperator</literal>, and <literal>thanosQuerier</literal>.</para>
</callout>
<callout arearefs="CO40-2">
<para>The log level to set for the component.
The available values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>.
The default value is <literal>info</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To set a log level for a component in the <literal>openshift-user-workload-monitoring</literal> project</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>logLevel: &lt;log_level&gt;</literal> for a component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    &lt;component&gt;: <co xml:id="CO41-1"/>
      logLevel: &lt;log_level&gt; <co xml:id="CO41-2"/></programlisting>
<calloutlist>
<callout arearefs="CO41-1">
<para>The monitoring stack component for which you are setting a log level.
For user workload monitoring, available component values are <literal>alertmanager</literal>, <literal>prometheus</literal>, <literal>prometheusOperator</literal>, and <literal>thanosRuler</literal>.</para>
</callout>
<callout arearefs="CO41-2">
<para>The log level to apply to the component. The available values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default value is <literal>info</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods for the component restart automatically when you apply the log-level change.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Confirm that the log-level has been applied by reviewing the deployment or pod configuration in the related project. The following example checks the log level in the <literal>prometheus-operator</literal> deployment in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml | grep "log-level"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">        - --log-level=debug</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the pods for the component are running. The following example lists the status of pods in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get pods</programlisting>
<note>
<simpara>If an unrecognized <literal>logLevel</literal> value is included in the <literal>ConfigMap</literal> object, the pods for the component might not restart successfully.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="setting-query-log-file-for-prometheus_configuring-the-monitoring-stack">
<title>Enabling the query log file for Prometheus</title>
<simpara role="_abstract">You can configure Prometheus to write all queries that have been run by the engine to a log file.
You can do so for default platform monitoring and for user-defined workload monitoring.</simpara>
<important>
<simpara>Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the <literal>ConfigMap</literal> object to enable the feature.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><emphasis role="strong">If you are enabling the query log file feature for Prometheus in the <literal>openshift-monitoring</literal> project</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">If you are enabling the query log file feature for Prometheus in the <literal>openshift-user-workload-monitoring</literal> project</emphasis>:</simpara>
<itemizedlist>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role, or as a user with the <literal>user-workload-monitoring-config-edit</literal> role in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara><emphasis role="strong">To set the query log file for Prometheus in the <literal>openshift-monitoring</literal> project</emphasis>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>queryLogFile: &lt;path&gt;</literal> for <literal>prometheusK8s</literal> under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      queryLogFile: &lt;path&gt; <co xml:id="CO42-1"/></programlisting>
<calloutlist>
<callout arearefs="CO42-1">
<para>The full path to the file in which queries will be logged.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<warning>
<simpara>When you save changes to a monitoring config map, pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Verify that the pods for the component are running. The following sample command lists the status of pods in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring get pods</programlisting>
</listitem>
<listitem>
<simpara>Read the query log:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring exec prometheus-k8s-0 -- cat &lt;path&gt;</programlisting>
<important>
<simpara>Revert the setting in the config map after you have examined the logged query information.</simpara>
</important>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">To set the query log file for Prometheus in the <literal>openshift-user-workload-monitoring</literal> project</emphasis>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>queryLogFile: &lt;path&gt;</literal> for <literal>prometheus</literal> under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      queryLogFile: &lt;path&gt; <co xml:id="CO43-1"/></programlisting>
<calloutlist>
<callout arearefs="CO43-1">
<para>The full path to the file in which queries will be logged.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<note>
<simpara>Configurations applied to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.</simpara>
</note>
<warning>
<simpara>When you save changes to a monitoring config map, pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Verify that the pods for the component are running. The following example command lists the status of pods in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get pods</programlisting>
</listitem>
<listitem>
<simpara>Read the query log:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring exec prometheus-user-workload-0 -- cat &lt;path&gt;</programlisting>
<important>
<simpara>Revert the setting in the config map after you have examined the logged query information.</simpara>
</important>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link> for steps to enable user-defined monitoring.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="enabling-query-logging-for-thanos-querier_configuring-the-monitoring-stack">
<title>Enabling query logging for Thanos Querier</title>
<simpara role="_abstract">For default platform monitoring in the <literal>openshift-monitoring</literal> project, you can enable the Cluster Monitoring Operator to log all queries run by Thanos Querier.</simpara>
<important>
<simpara>Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the <literal>ConfigMap</literal> object to enable the feature.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>You can enable query logging for Thanos Querier in the <literal>openshift-monitoring</literal> project:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add a <literal>thanosQuerier</literal> section under <literal>data/config.yaml</literal> and add values as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    thanosQuerier:
      enableRequestLogging: &lt;value&gt; <co xml:id="CO44-1"/>
      logLevel: &lt;value&gt; <co xml:id="CO44-2"/></programlisting>
<calloutlist>
<callout arearefs="CO44-1">
<para>Set the value to <literal>true</literal> to enable logging and <literal>false</literal> to disable logging. The default value is <literal>false</literal>.</para>
</callout>
<callout arearefs="CO44-2">
<para>Set the value to <literal>debug</literal>, <literal>info</literal>, <literal>warn</literal>, or <literal>error</literal>. If no value exists for <literal>logLevel</literal>, the log level defaults to <literal>error</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<warning>
<simpara>When you save changes to a monitoring config map, pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the Thanos Querier pods are running. The following sample command lists the status of pods in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring get pods</programlisting>
</listitem>
<listitem>
<simpara>Run a test query using the following sample commands as a model:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ token=`oc create token prometheus-k8s -n openshift-monitoring`
$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -k -H "Authorization: Bearer $token" 'https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query?query=cluster_version'</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to read the query log:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring logs &lt;thanos_querier_pod_name&gt; -c thanos-query</programlisting>
<note>
<simpara>Because the <literal>thanos-querier</literal> pods are highly available (HA) pods, you might be able to see logs in only one pod.</simpara>
</note>
</listitem>
<listitem>
<simpara>After you examine the logged query information, disable query logging by changing the <literal>enableRequestLogging</literal> value to <literal>false</literal> in the config map.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="setting-audit-log-levels-for-the-prometheus-adapter_configuring-the-monitoring-stack">
<title>Setting audit log levels for the Prometheus Adapter</title>
<simpara role="_abstract">In default platform monitoring, you can configure the audit log level for the Prometheus Adapter.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>You can set an audit log level for the Prometheus Adapter in the default <literal>openshift-monitoring</literal> project:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>profile:</literal> in the <literal>k8sPrometheusAdapter/audit</literal> section under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    k8sPrometheusAdapter:
      audit:
        profile: &lt;audit_log_level&gt; <co xml:id="CO45-1"/></programlisting>
<calloutlist>
<callout arearefs="CO45-1">
<para>The audit log level to apply to the Prometheus Adapter.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Set the audit log level by using one of the following values for the <literal>profile:</literal> parameter:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>None</literal>: Do not log events.</simpara>
</listitem>
<listitem>
<simpara><literal>Metadata</literal>: Log only the metadata for the request, such as user, timestamp, and so forth. Do not log the request text and the response text. <literal>Metadata</literal> is the default audit log level.</simpara>
</listitem>
<listitem>
<simpara><literal>Request</literal>: Log only the metadata and the request text but not the response text. This option does not apply for non-resource requests.</simpara>
</listitem>
<listitem>
<simpara><literal>RequestResponse</literal>: Log event metadata, request text, and response text. This option does not apply for non-resource requests.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The pods for the Prometheus Adapter restart automatically when you apply the change.</simpara>
<warning>
<simpara>When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>In the config map, under <literal>k8sPrometheusAdapter/audit/profile</literal>, set the log level to <literal>Request</literal> and save the file.</simpara>
</listitem>
<listitem>
<simpara>Confirm that the pods for the Prometheus Adapter are running. The following example lists the status of pods in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring get pods</programlisting>
</listitem>
<listitem>
<simpara>Confirm that the audit log level and audit log file path are correctly configured:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring get deploy prometheus-adapter -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
  - --audit-policy-file=/etc/audit/request-profile.yaml
  - --audit-log-path=/var/log/adapter/audit.log</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Confirm that the correct log level has been applied in the <literal>prometheus-adapter</literal> deployment in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring exec deploy/prometheus-adapter -c prometheus-adapter -- cat /etc/audit/request-profile.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"apiVersion": "audit.k8s.io/v1"
"kind": "Policy"
"metadata":
  "name": "Request"
"omitStages":
- "RequestReceived"
"rules":
- "level": "Request"</programlisting>
</para>
</formalpara>
<note>
<simpara>If you enter an unrecognized <literal>profile</literal> value for the Prometheus Adapter in the <literal>ConfigMap</literal> object, no changes are made to the Prometheus Adapter, and an error is logged by the Cluster Monitoring Operator.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the audit log for the Prometheus Adapter:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring exec -c &lt;prometheus_adapter_pod_name&gt; -- cat /var/log/adapter/audit.log</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#preparing-to-configure-the-monitoring-stack">Preparing to configure the monitoring stack</link> for steps to create monitoring config maps.</simpara>
</listitem>
</itemizedlist>
<section xml:id="monitoring-disabling-the-local-alertmanager_configuring-the-monitoring-stack">
<title>Disabling the local Alertmanager</title>
<simpara>A local Alertmanager that routes alerts from Prometheus instances is enabled by default in the <literal>openshift-monitoring</literal> project of the {product-title} monitoring stack.</simpara>
<simpara>If you do not need the local Alertmanager, you can disable it by configuring the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> config map.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>enabled: false</literal> for the <literal>alertmanagerMain</literal> component under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      enabled: false</programlisting>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The Alertmanager instance is disabled automatically when you apply the change.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="https://prometheus.io/docs/alerting/latest/alertmanager/">Prometheus Alertmanager documentation</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/managing-alerts.xml">Managing alerts</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_next_steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara>Learn about <link xl:href="../support/remote_health_monitoring/opting-out-of-remote-health-reporting.xml#opting-out-remote-health-reporting_opting-out-remote-health-reporting">remote health reporting</link> and, if necessary, opt out of it.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="enabling-monitoring-for-user-defined-projects">
<title>Enabling monitoring for user-defined projects</title>

<simpara>In {product-title} {product-version}, you can enable monitoring for user-defined projects in addition to the default platform monitoring. You can monitor your own projects in {product-title} without the need for an additional monitoring solution. Using this feature centralizes monitoring for core platform components and user-defined projects.</simpara>
<note>
<simpara>Versions of Prometheus Operator installed using Operator Lifecycle Manager (OLM) are not compatible with user-defined monitoring. Therefore, custom Prometheus instances installed as a Prometheus custom resource (CR) managed by the OLM Prometheus Operator are not supported in {product-title}.</simpara>
</note>
<section xml:id="enabling-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects">
<title>Enabling monitoring for user-defined projects</title>
<simpara>Cluster administrators can enable monitoring for user-defined projects by setting the <literal>enableUserWorkload: true</literal> field in the cluster monitoring <literal>ConfigMap</literal> object.</simpara>
<important>
<simpara>In {product-title} {product-version} you must remove any custom Prometheus instances before enabling monitoring for user-defined projects.</simpara>
</important>
<note>
<simpara>You must have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role to enable monitoring for user-defined projects in {product-title}. Cluster administrators can then optionally grant users permission to configure the components that are responsible for monitoring user-defined projects.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have optionally created and configured the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project. You can add configuration options to this <literal>ConfigMap</literal> object for the components that monitor user-defined projects.</simpara>
<note>
<simpara>Every time you save configuration changes to the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object, the pods in the <literal>openshift-user-workload-monitoring</literal> project are redeployed. It can sometimes take a while for these components to redeploy. You can create and configure the <literal>ConfigMap</literal> object before you first enable monitoring for user-defined projects, to prevent having to redeploy the pods often.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>enableUserWorkload: true</literal> under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true <co xml:id="CO46-1"/></programlisting>
<calloutlist>
<callout arearefs="CO46-1">
<para>When set to <literal>true</literal>, the <literal>enableUserWorkload</literal> parameter enables monitoring for user-defined projects in a cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. Monitoring for user-defined projects is then enabled automatically.</simpara>
<warning>
<simpara>When changes are saved to the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object, the pods and other resources in the <literal>openshift-monitoring</literal> project might be redeployed. The running monitoring processes in that project might also be restarted.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Check that the <literal>prometheus-operator</literal>, <literal>prometheus-user-workload</literal> and <literal>thanos-ruler-user-workload</literal> pods are running in the <literal>openshift-user-workload-monitoring</literal> project. It might take a short while for the pods to start:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get pod</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                   READY   STATUS        RESTARTS   AGE
prometheus-operator-6f7b748d5b-t7nbg   2/2     Running       0          3h
prometheus-user-workload-0             4/4     Running       1          3h
prometheus-user-workload-1             4/4     Running       1          3h
thanos-ruler-user-workload-0           3/3     Running       0          3h
thanos-ruler-user-workload-1           3/3     Running       0          3h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#creating-cluster-monitoring-configmap_configuring-the-monitoring-stack">Creating a cluster monitoring config map</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#configuring-the-monitoring-stack">Configuring the monitoring stack</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#granting-users-permission-to-configure-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects">Granting users permission to configure monitoring for user-defined projects</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="granting-users-permission-to-monitor-user-defined-projects_enabling-monitoring-for-user-defined-projects">
<title>Granting users permission to monitor user-defined projects</title>
<simpara>Cluster administrators can monitor all core {product-title} and user-defined projects.</simpara>
<simpara>Cluster administrators can grant developers and other users permission to monitor their own projects. Privileges are granted by assigning one of the following monitoring roles:</simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">monitoring-rules-view</emphasis> cluster role provides read access to <literal>PrometheusRule</literal> custom resources for a project.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">monitoring-rules-edit</emphasis> cluster role grants a user permission to create, modify, and deleting <literal>PrometheusRule</literal> custom resources for a project.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">monitoring-edit</emphasis> cluster role grants the same privileges as the <literal>monitoring-rules-edit</literal> cluster role. Additionally, it enables a user to create new scrape targets for services or pods. With this role, you can also create, modify, and delete <literal>ServiceMonitor</literal> and <literal>PodMonitor</literal> resources.</simpara>
</listitem>
</itemizedlist>
<simpara>You can also grant users permission to configure the components that are responsible for monitoring user-defined projects:</simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">user-workload-monitoring-config-edit</emphasis> role in the <literal>openshift-user-workload-monitoring</literal> project enables you to edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object. With this role, you can edit the <literal>ConfigMap</literal> object to configure Prometheus, Prometheus Operator, and Thanos Ruler for user-defined workload monitoring.</simpara>
</listitem>
</itemizedlist>
<simpara>You can also grant users permission to configure alert routing for user-defined projects:</simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">alert-routing-edit</emphasis> cluster role grants a user permission to create, update, and delete <literal>AlertmanagerConfig</literal> custom resources for a project.</simpara>
</listitem>
</itemizedlist>
<simpara>This section provides details on how to assign these roles by using the {product-title} web console or the CLI.</simpara>
<section xml:id="granting-user-permissions-using-the-web-console_enabling-monitoring-for-user-defined-projects">
<title>Granting user permissions by using the web console</title>
<simpara>You can grant users permissions to monitor their own projects, by using the {product-title} web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>The user account that you are assigning the role to already exists.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective within the {product-title} web console, navigate to <emphasis role="strong">User Management</emphasis> &#8594; <emphasis role="strong">RoleBindings</emphasis> &#8594; <emphasis role="strong">Create binding</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Binding Type</emphasis> section, select the "Namespace Role Binding" type.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Name</emphasis> field, enter a name for the role binding.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Namespace</emphasis> field, select the user-defined project where you want to grant the access.</simpara>
<important>
<simpara>The monitoring role will be bound to the project that you apply in the <emphasis role="strong">Namespace</emphasis> field. The permissions that you grant to a user by using this procedure will apply only to the selected project.</simpara>
</important>
</listitem>
<listitem>
<simpara>Select <literal>monitoring-rules-view</literal>, <literal>monitoring-rules-edit</literal>, or <literal>monitoring-edit</literal> in the <emphasis role="strong">Role Name</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Subject</emphasis> section, select <emphasis role="strong">User</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Subject Name</emphasis> field, enter the name of the user.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Create</emphasis> to apply the role binding.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="granting-user-permissions-using-the-cli_enabling-monitoring-for-user-defined-projects">
<title>Granting user permissions by using the CLI</title>
<simpara>You can grant users permissions to monitor their own projects, by using the OpenShift CLI (<literal>oc</literal>).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>The user account that you are assigning the role to already exists.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Assign a monitoring role to a user for a project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc policy add-role-to-user &lt;role&gt; &lt;user&gt; -n &lt;namespace&gt; <co xml:id="CO47-1"/></programlisting>
<calloutlist>
<callout arearefs="CO47-1">
<para>Substitute <literal>&lt;role&gt;</literal> with <literal>monitoring-rules-view</literal>, <literal>monitoring-rules-edit</literal>, or <literal>monitoring-edit</literal>.</para>
</callout>
</calloutlist>
<important>
<simpara>Whichever role you choose, you must bind it against a specific project as a cluster administrator.</simpara>
</important>
<simpara>As an example, substitute <literal>&lt;role&gt;</literal> with <literal>monitoring-edit</literal>, <literal>&lt;user&gt;</literal> with <literal>johnsmith</literal>, and <literal>&lt;namespace&gt;</literal> with <literal>ns1</literal>. This assigns the user <literal>johnsmith</literal> permission to set up metrics collection and to create alerting rules in the <literal>ns1</literal> namespace.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="granting-users-permission-to-configure-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects">
<title>Granting users permission to configure monitoring for user-defined projects</title>
<simpara>You can grant users permission to configure monitoring for user-defined projects.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>The user account that you are assigning the role to already exists.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Assign the <literal>user-workload-monitoring-config-edit</literal> role to a user in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring adm policy add-role-to-user \
  user-workload-monitoring-config-edit &lt;user&gt; \
  --role-namespace openshift-user-workload-monitoring</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="accessing-metrics-from-outside-cluster_enabling-monitoring-for-user-defined-projects">
<title>Accessing metrics from outside the cluster for custom applications</title>
<simpara>Learn how to query Prometheus statistics from the command line when monitoring your own services. You can access monitoring data from outside the cluster with the <literal>thanos-querier</literal> route.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You deployed your own service, following the <emphasis>Enabling monitoring for user-defined projects</emphasis> procedure.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Extract a token to connect to Prometheus:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ SECRET=`oc get secret -n openshift-user-workload-monitoring | grep  prometheus-user-workload-token | head -n 1 | awk '{print $1 }'`</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ TOKEN=`echo $(oc get secret $SECRET -n openshift-user-workload-monitoring -o json | jq -r '.data.token') | base64 -d`</programlisting>
</listitem>
<listitem>
<simpara>Extract your route host:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ THANOS_QUERIER_HOST=`oc get route thanos-querier -n openshift-monitoring -o json | jq -r '.spec.host'`</programlisting>
</listitem>
<listitem>
<simpara>Query the metrics of your own services in the command line. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ NAMESPACE=ns1</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -X GET -kG "https://$THANOS_QUERIER_HOST/api/v1/query?" --data-urlencode "query=up{namespace='$NAMESPACE'}" -H "Authorization: Bearer $TOKEN"</programlisting>
<simpara>The output will show you the duration that your application pods have been up.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{"status":"success","data":{"resultType":"vector","result":[{"metric":{"__name__":"up","endpoint":"web","instance":"10.129.0.46:8080","job":"prometheus-example-app","namespace":"ns1","pod":"prometheus-example-app-68d47c4fb6-jztp2","service":"prometheus-example-app"},"value":[1591881154.748,"1"]}]}}</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="excluding-a-user-defined-project-from-monitoring_enabling-monitoring-for-user-defined-projects">
<title>Excluding a user-defined project from monitoring</title>
<simpara>Individual user-defined projects can be excluded from user workload monitoring. To do so, add the <literal>openshift.io/user-monitoring</literal> label to the project&#8217;s namespace with a value of <literal>false</literal>.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the label to the project namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace my-project 'openshift.io/user-monitoring=false'</programlisting>
</listitem>
<listitem>
<simpara>To re-enable monitoring, remove the label from the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace my-project 'openshift.io/user-monitoring-'</programlisting>
<note>
<simpara>If there were any active monitoring targets for the project, it may take a few minutes for Prometheus to stop scraping them after adding the label.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="disabling-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects">
<title>Disabling monitoring for user-defined projects</title>
<simpara>After enabling monitoring for user-defined projects, you can disable it again by setting <literal>enableUserWorkload: false</literal> in the cluster monitoring <literal>ConfigMap</literal> object.</simpara>
<note>
<simpara>Alternatively, you can remove <literal>enableUserWorkload: true</literal> to disable monitoring for user-defined projects.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Set <literal>enableUserWorkload:</literal> to <literal>false</literal> under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: false</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. Monitoring for user-defined projects is then disabled automatically.</simpara>
</listitem>
<listitem>
<simpara>Check that the <literal>prometheus-operator</literal>, <literal>prometheus-user-workload</literal> and <literal>thanos-ruler-user-workload</literal> pods are terminated in the <literal>openshift-user-workload-monitoring</literal> project. This might take a short while:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get pod</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">No resources found in openshift-user-workload-monitoring project.</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>The <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project is not automatically deleted when monitoring for user-defined projects is disabled. This is to preserve any custom configurations that you may have created in the <literal>ConfigMap</literal> object.</simpara>
</note>
</section>
<section xml:id="_next_steps_2">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/managing-metrics.xml#managing-metrics">Managing metrics</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="enabling-alert-routing-for-user-defined-projects">
<title>Enabling alert routing for user-defined projects</title>

<simpara role="_abstract">In {product-title} {product-version}, a cluster administrator can enable alert routing for user-defined projects.
This process consists of two general steps:</simpara>
<itemizedlist>
<listitem>
<simpara>Enable alert routing for user-defined projects to use the default platform Alertmanager instance or, optionally, a separate Alertmanager instance only for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>Grant users permission to configure alert routing for user-defined projects.</simpara>
</listitem>
</itemizedlist>
<simpara>After you complete these steps, developers and other users can configure custom alerts and alert routing for their user-defined projects.</simpara>
<section xml:id="understanding-alert-routing-for-user-defined-projects_enabling-alert-routing-for-user-defined-projects">
<title>Understanding alert routing for user-defined projects</title>
<simpara role="_abstract">As a cluster administrator, you can enable alert routing for user-defined projects.
With this feature, you can allow users with the <emphasis role="strong">alert-routing-edit</emphasis> role to configure alert notification routing and receivers for user-defined projects.
These notifications are routed by the default Alertmanager instance or, if enabled, an optional Alertmanager instance dedicated to user-defined monitoring.</simpara>
<simpara>Users can then create and configure user-defined alert routing by creating or editing the <literal>AlertmanagerConfig</literal> objects for their user-defined projects without the help of an administrator.</simpara>
<simpara>After a user has defined alert routing for a user-defined project, user-defined alert notifications are routed as follows:</simpara>
<itemizedlist>
<listitem>
<simpara>To the <literal>alertmanager-main</literal> pods in the <literal>openshift-monitoring</literal> namespace if using the default platform Alertmanager instance.</simpara>
</listitem>
<listitem>
<simpara>To the <literal>alertmanager-user-workload</literal> pods in the <literal>openshift-user-workload-monitoring</literal> namespace if you have enabled a separate instance of Alertmanager for user-defined projects.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The following are limitations of alert routing for user-defined projects:</simpara>
<itemizedlist>
<listitem>
<simpara>For user-defined alerting rules, user-defined routing is scoped to the namespace in which the resource is defined. For example, a routing configuration in namespace <literal>ns1</literal> only applies to <literal>PrometheusRules</literal> resources in the same namespace.</simpara>
</listitem>
<listitem>
<simpara>When a namespace is excluded from user-defined monitoring, <literal>AlertmanagerConfig</literal> resources in the namespace cease to be part of the Alertmanager configuration.</simpara>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="enabling-the-platform-alertmanager-instance-for-user-defined-alert-routing_enabling-alert-routing-for-user-defined-projects">
<title>Enabling the platform Alertmanager instance for user-defined alert routing</title>
<simpara>You can allow users to create user-defined alert routing configurations that use the main platform instance of Alertmanager.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring edit configmap cluster-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>enableUserAlertmanagerConfig: true</literal> in the <literal>alertmanagerMain</literal> section under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      enableUserAlertmanagerConfig: true <co xml:id="CO48-1"/></programlisting>
<calloutlist>
<callout arearefs="CO48-1">
<para>Set the <literal>enableUserAlertmanagerConfig</literal> value to <literal>true</literal> to allow users to create user-defined alert routing configurations that use the main platform instance of Alertmanager.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="enabling-a-separate-alertmanager-instance-for-user-defined-alert-routing_enabling-alert-routing-for-user-defined-projects">
<title>Enabling a separate Alertmanager instance for user-defined alert routing</title>
<simpara>In some clusters, you might want to deploy a dedicated Alertmanager instance for user-defined projects, which can help reduce the load on the default platform Alertmanager instance and can better separate user-defined alerts from default platform alerts.
In these cases, you can optionally enable a separate instance of Alertmanager to send alerts for user-defined projects only.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects in the <literal>cluster-monitoring-config</literal> config map for the <literal>openshift-monitoring</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>enabled: true</literal> and <literal>enableAlertmanagerConfig: true</literal> in the <literal>alertmanager</literal> section under <literal>data/config.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      enabled: true <co xml:id="CO49-1"/>
      enableAlertmanagerConfig: true <co xml:id="CO49-2"/></programlisting>
<calloutlist>
<callout arearefs="CO49-1">
<para>Set the <literal>enabled</literal> value to <literal>true</literal> to enable a dedicated instance of the Alertmanager for user-defined projects in a cluster. Set the value to <literal>false</literal> or omit the key entirely to disable the Alertmanager for user-defined projects.
If you set this value to <literal>false</literal> or if the key is omitted, user-defined alerts are routed to the default platform Alertmanager instance.</para>
</callout>
<callout arearefs="CO49-2">
<para>Set the <literal>enableAlertmanagerConfig</literal> value to <literal>true</literal> to enable users to define their own alert routing configurations with <literal>AlertmanagerConfig</literal> objects.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file to apply the changes. The dedicated instance of Alertmanager for user-defined projects starts automatically.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the <literal>user-workload</literal> Alertmanager instance has started:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># oc -n openshift-user-workload-monitoring get alertmanager</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME            VERSION   REPLICAS   AGE
user-workload   0.24.0    2          100s</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="granting-users-permission-to-configure-alert-routing-for-user-defined-projects_enabling-alert-routing-for-user-defined-projects">
<title>Granting users permission to configure alert routing for user-defined projects</title>
<simpara role="_abstract">You can grant users permission to configure alert routing for user-defined projects.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects in the <literal>cluster-monitoring-config</literal> config map for the <literal>openshift-monitoring</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>The user account that you are assigning the role to already exists.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Assign the <literal>alert-routing-edit</literal> cluster role to a user in the user-defined project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;namespace&gt; adm policy add-role-to-user alert-routing-edit &lt;user&gt; <co xml:id="CO50-1"/></programlisting>
<calloutlist>
<callout arearefs="CO50-1">
<para>For <literal>&lt;namespace&gt;</literal>, substitute the namespace for the user-defined project, such as <literal>ns1</literal>. For <literal>&lt;user&gt;</literal>, substitute the username for the account to which you want to assign the role.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user defined projects</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/managing-alerts.xml#creating-alert-routing-for-user-defined-projects_managing-alerts">Creating alert routing for user-defined projects</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_next_steps_3">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/managing-alerts.xml#managing-alerts">Managing alerts</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="managing-metrics">
<title>Managing metrics</title>

<simpara role="_abstract">You can collect metrics to monitor how cluster components and your own workloads are performing.</simpara>
<section xml:id="understanding-metrics_managing-metrics">
<title>Understanding metrics</title>
<simpara role="_abstract">In {product-title} {product-version},
cluster components are monitored by scraping metrics exposed through service endpoints. You can also configure metrics collection for user-defined projects. Metrics enable you to monitor how cluster components and your own workloads are performing.</simpara>
<simpara>You can define the metrics that you want to provide for your own workloads by using Prometheus client libraries at the application level.</simpara>
<simpara>In {product-title}, metrics are exposed through an HTTP service endpoint under the <literal>/metrics</literal> canonical name. You can list all available metrics for a service by running a <literal>curl</literal> query against <literal>http://&lt;endpoint&gt;/metrics</literal>. For instance, you can expose a route to the <literal>prometheus-example-app</literal> example application and then run the following to view all of its available metrics:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl http://&lt;example_app_endpoint&gt;/metrics</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># HELP http_requests_total Count of all HTTP requests
# TYPE http_requests_total counter
http_requests_total{code="200",method="get"} 4
http_requests_total{code="404",method="get"} 2
# HELP version Version information about this binary
# TYPE version gauge
version{version="v0.1.0"} 1</programlisting>
</para>
</formalpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="https://prometheus.io/docs/instrumenting/clientlibs/">Prometheus client library documentation</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="setting-up-metrics-collection-for-user-defined-projects_managing-metrics">
<title>Setting up metrics collection for user-defined projects</title>
<simpara>You can create a <literal>ServiceMonitor</literal> resource to scrape metrics from a service endpoint in a user-defined project. This assumes that your application uses a Prometheus client library to expose metrics to the <literal>/metrics</literal> canonical name.</simpara>
<simpara>This section describes how to deploy a sample service in a user-defined project and then create a <literal>ServiceMonitor</literal> resource that defines how that service should be monitored.</simpara>
<section xml:id="deploying-a-sample-service_managing-metrics">
<title>Deploying a sample service</title>
<simpara>To test monitoring of a service in a user-defined project, you can deploy a sample service.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the service configuration. In this example, it is called <literal>prometheus-example-app.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add the following deployment and service configuration details to the file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: ns1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-example-app
  template:
    metadata:
      labels:
        app: prometheus-example-app
    spec:
      containers:
      - image: ghcr.io/rhobs/prometheus-example-app:0.4.2
        imagePullPolicy: IfNotPresent
        name: prometheus-example-app
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    name: web
  selector:
    app: prometheus-example-app
  type: ClusterIP</programlisting>
<simpara>This configuration deploys a service named <literal>prometheus-example-app</literal> in the user-defined <literal>ns1</literal> project. This service exposes the custom <literal>version</literal> metric.</simpara>
</listitem>
<listitem>
<simpara>Apply the configuration to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f prometheus-example-app.yaml</programlisting>
<simpara>It takes some time to deploy the service.</simpara>
</listitem>
<listitem>
<simpara>You can check that the pod is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n ns1 get pod</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                      READY     STATUS    RESTARTS   AGE
prometheus-example-app-7857545cb7-sbgwq   1/1       Running   0          81m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="specifying-how-a-service-is-monitored_managing-metrics">
<title>Specifying how a service is monitored</title>
<simpara role="_abstract">To use the metrics exposed by your service, you must configure {product-title} monitoring to scrape metrics from the <literal>/metrics</literal> endpoint. You can do this using a <literal>ServiceMonitor</literal> custom resource definition (CRD) that specifies how a service should be monitored, or a <literal>PodMonitor</literal> CRD that specifies how a pod should be monitored. The former requires a <literal>Service</literal> object, while the latter does not, allowing Prometheus to directly scrape metrics from the metrics endpoint exposed by a pod.</simpara>
<simpara>This procedure shows you how to create a <literal>ServiceMonitor</literal> resource for a service in a user-defined project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or the <literal>monitoring-edit</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>For this example, you have deployed the <literal>prometheus-example-app</literal> sample service in the <literal>ns1</literal> project.</simpara>
<note>
<simpara>The <literal>prometheus-example-app</literal> sample service does not support TLS authentication.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the <literal>ServiceMonitor</literal> resource configuration. In this example, the file is called <literal>example-app-service-monitor.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add the following <literal>ServiceMonitor</literal> resource configuration details:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: prometheus-example-monitor
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app</programlisting>
<simpara>This defines a <literal>ServiceMonitor</literal> resource that scrapes the metrics exposed by the <literal>prometheus-example-app</literal> sample service, which includes the <literal>version</literal> metric.</simpara>
<note>
<simpara>A <literal>ServiceMonitor</literal> resource in a user-defined namespace can only discover services in the same namespace. That is, the <literal>namespaceSelector</literal> field of the <literal>ServiceMonitor</literal> resource is always ignored.</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the configuration to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-app-service-monitor.yaml</programlisting>
<simpara>It takes some time to deploy the <literal>ServiceMonitor</literal> resource.</simpara>
</listitem>
<listitem>
<simpara>You can check that the <literal>ServiceMonitor</literal> resource is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n ns1 get servicemonitor</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                         AGE
prometheus-example-monitor   81m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/enabling-monitoring-for-user-defined-projects.xml#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="https://access.redhat.com/articles/6675491">How to scrape metrics using TLS in a ServiceMonitor configuration in a user-defined project</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../rest_api/monitoring_apis/podmonitor-monitoring-coreos-com-v1.xml">PodMonitor API</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../rest_api/monitoring_apis/servicemonitor-monitoring-coreos-com-v1.xml">ServiceMonitor API</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="viewing-a-list-of-available-metrics_managing-metrics">
<title>Viewing a list of available metrics</title>
<simpara>As a cluster administrator or as a user with view permissions for all projects, you can view a list of metrics available in a cluster and output the list in JSON format.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are a cluster administrator, or you have access to the cluster as a user with the <literal>cluster-monitoring-view</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the {product-title} CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have obtained the {product-title} API route for Thanos Querier.</simpara>
</listitem>
<listitem>
<simpara>You are able to get a bearer token by using the <literal>oc whoami -t</literal> command.</simpara>
<important>
<simpara>You can only use bearer token authentication to access the Thanos Querier API route.</simpara>
</important>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If you have not obtained the {product-title} API route for Thanos Querier, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get routes -n openshift-monitoring thanos-querier -o jsonpath='{.status.ingress[0].host}'</programlisting>
</listitem>
<listitem>
<simpara>Retrieve a list of metrics in JSON format from the Thanos Querier API route by running the following command. This command uses <literal>oc</literal> to authenticate with a bearer token.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -k -H "Authorization: Bearer $(oc whoami -t)" https://&lt;thanos_querier_route&gt;/api/v1/metadata <co xml:id="CO51-1"/></programlisting>
<calloutlist>
<callout arearefs="CO51-1">
<para>Replace <literal>&lt;thanos_querier_route&gt;</literal> with the {product-title} API route for Thanos Querier.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="about-querying-metrics_managing-metrics">
<title>Querying metrics</title>
<simpara>The {product-title} monitoring dashboard enables you to run Prometheus Query Language (PromQL) queries to examine metrics visualized on a plot. This functionality provides information about the state of a cluster and any user-defined workloads that you are monitoring.</simpara>
<simpara>As a cluster administrator, you can query metrics for all core {product-title} and user-defined projects.</simpara>
<simpara>As a developer, you must specify a project name when querying metrics. You must have the required privileges to view metrics for the selected project.</simpara>
<section xml:id="querying-metrics-for-all-projects-as-an-administrator_managing-metrics">
<title>Querying metrics for all projects as a cluster administrator</title>
<simpara>As a
cluster administrator
or as a user with view permissions for all projects, you can access metrics for all default {product-title} and user-defined projects in the Metrics UI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or with view permissions for all projects.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the <emphasis role="strong">Administrator</emphasis> perspective in the {product-title} web console, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>To add one or more queries, do any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Create a custom query.</simpara></entry>
<entry align="left" valign="top"><simpara>Add your Prometheus Query Language (PromQL) query to the <emphasis role="strong">Expression</emphasis> field.</simpara><simpara>As you type a PromQL expression, autocomplete suggestions appear in a drop-down list. These suggestions include functions, metrics, labels, and time tokens.
You can use the keyboard arrows to select one of these suggested items and then press Enter to add the item to your expression. You can also move your mouse pointer over a suggested item to view a brief description of that item.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Add multiple queries.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Add query</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Duplicate an existing query.</simpara></entry>
<entry align="left" valign="top"><simpara>Select the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to the query, then choose <emphasis role="strong">Duplicate query</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Disable a query from being run.</simpara></entry>
<entry align="left" valign="top"><simpara>Select the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to the query and choose <emphasis role="strong">Disable query</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>To run queries that you created, select <emphasis role="strong">Run queries</emphasis>. The metrics from the queries are visualized on the plot. If a query is invalid, the UI shows an error message.</simpara>
<note>
<simpara>Queries that operate on large amounts of data might time out or overload the browser when drawing time series graphs. To avoid this, select <emphasis role="strong">Hide graph</emphasis> and calibrate your query using only the metrics table. Then, after finding a feasible query, enable the plot to draw the graphs.</simpara>
</note>
<note>
<simpara>By default, the query table shows an expanded view that lists every metric and its current value. You can select <emphasis role="strong">Ë…</emphasis> to minimize the expanded view for a query.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: The page URL now contains the queries you ran. To use this set of queries again in the future, save this URL.</simpara>
</listitem>
<listitem>
<simpara>Explore the visualized metrics. Initially, all metrics from all enabled queries are shown on the plot. You can select which metrics are shown by doing any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Hide all metrics from a query.</simpara></entry>
<entry align="left" valign="top"><simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> for the query and click <emphasis role="strong">Hide all series</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hide a specific metric.</simpara></entry>
<entry align="left" valign="top"><simpara>Go to the query table and click the colored square near the metric name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Zoom into the plot and change the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Either:</simpara>
<itemizedlist>
<listitem>
<simpara>Visually select the time range by clicking and dragging on the plot horizontally.</simpara>
</listitem>
<listitem>
<simpara>Use the menu in the left upper corner to select the time range.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reset the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Reset zoom</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Display outputs for all queries at a specific point in time.</simpara></entry>
<entry align="left" valign="top"><simpara>Hold the mouse cursor on the plot at that point. The query outputs will appear in a pop-up box.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hide the plot.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Hide graph</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information about creating PromQL queries, see the <link xl:href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus query documentation</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="querying-metrics-for-user-defined-projects-as-a-developer_managing-metrics">
<title>Querying metrics for user-defined projects as a developer</title>
<simpara>You can access metrics for a user-defined project as a developer or as a user with view permissions for the project.</simpara>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, the Metrics UI includes some predefined CPU, memory, bandwidth, and network packet queries for the selected project. You can also run custom Prometheus Query Language (PromQL) queries for CPU, memory, bandwidth, network packet and application metrics for the project.</simpara>
<note>
<simpara>Developers can only use the <emphasis role="strong">Developer</emphasis> perspective and not the <emphasis role="strong">Administrator</emphasis> perspective. As a developer, you can only query metrics for one project at a time.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have deployed a service in a user-defined project.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>ServiceMonitor</literal> custom resource definition (CRD) for the service to define how the service is monitored.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the <emphasis role="strong">Developer</emphasis> perspective in the {product-title} web console, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the project that you want to view metrics for in the <emphasis role="strong">Project:</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Select a query from the <emphasis role="strong">Select query</emphasis> list, or create a custom PromQL query based on the selected query by selecting <emphasis role="strong">Show PromQL</emphasis>. The metrics from the queries are visualized on the plot.</simpara>
<note>
<simpara>In the Developer perspective, you can only run one query at a time.</simpara>
</note>
</listitem>
<listitem>
<simpara>Explore the visualized metrics by doing any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Zoom into the plot and change the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Either:</simpara>
<itemizedlist>
<listitem>
<simpara>Visually select the time range by clicking and dragging on the plot horizontally.</simpara>
</listitem>
<listitem>
<simpara>Use the menu in the left upper corner to select the time range.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reset the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Reset zoom</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Display outputs for all queries at a specific point in time.</simpara></entry>
<entry align="left" valign="top"><simpara>Hold the mouse cursor on the plot at that point. The query outputs appear in a pop-up box.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>For more information about creating PromQL queries, see the <link xl:href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus query documentation</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="getting-detailed-information-about-a-target_managing-metrics">
<title>Getting detailed information about a metrics target</title>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective in the {product-title} web console, you can use the <emphasis role="strong">Metrics targets</emphasis> page to view, search, and filter the endpoints that are currently targeted for scraping, which helps you to identify and troubleshoot problems. For example, you can view the current status of targeted endpoints to see when {product-title} Monitoring is not able to scrape metrics from a targeted component.</simpara>
<simpara>The <emphasis role="strong">Metrics targets</emphasis> page shows targets for default {product-title} projects and for user-defined projects.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as an administrator for the project for which you want to view metrics targets.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Targets</emphasis>. The <emphasis role="strong">Metrics targets</emphasis> page opens with a list of all service endpoint targets that are being scraped for metrics.</simpara>
<simpara>This page shows details about targets for default {product-title} and user-defined projects. This page lists the following information for each target:</simpara>
<itemizedlist>
<listitem>
<simpara>Service endpoint URL being scraped</simpara>
</listitem>
<listitem>
<simpara>ServiceMonitor component being monitored</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">up</emphasis> or <emphasis role="strong">down</emphasis> status of the target</simpara>
</listitem>
<listitem>
<simpara>Namespace</simpara>
</listitem>
<listitem>
<simpara>Last scrape time</simpara>
</listitem>
<listitem>
<simpara>Duration of the last scrape</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Optional: The list of metrics targets can be long. To find a specific target, do any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Filter the targets by status and source.</simpara></entry>
<entry align="left" valign="top"><simpara>Select filters in the <emphasis role="strong">Filter</emphasis> list.</simpara>
<simpara>The following filtering options are available:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Status</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Up</emphasis>. The target is currently up and being actively scraped for metrics.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Down</emphasis>. The target is currently down and not being scraped for metrics.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Source</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Platform</emphasis>. Platform-level targets relate only to default Red Hat OpenShift Service on AWS projects. These projects provide core Red Hat OpenShift Service on AWS functionality.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">User</emphasis>. User targets relate to user-defined projects. These projects are user-created and can be customized.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search for a target by name or label.</simpara></entry>
<entry align="left" valign="top"><simpara>Enter a search term in the <emphasis role="strong">Text</emphasis> or <emphasis role="strong">Label</emphasis> field next to the search box.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Sort the targets.</simpara></entry>
<entry align="left" valign="top"><simpara>Click one or more of the <emphasis role="strong">Endpoint Status</emphasis>, <emphasis role="strong">Namespace</emphasis>, <emphasis role="strong">Last Scrape</emphasis>, and <emphasis role="strong">Scrape Duration</emphasis> column headers.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>Click the URL in the <emphasis role="strong">Endpoint</emphasis> column for a target to navigate to its <emphasis role="strong">Target details</emphasis> page. This page provides information about the target, including the following:</simpara>
<itemizedlist>
<listitem>
<simpara>The endpoint URL being scraped for metrics</simpara>
</listitem>
<listitem>
<simpara>The current <emphasis role="strong">Up</emphasis> or <emphasis role="strong">Down</emphasis> status of the target</simpara>
</listitem>
<listitem>
<simpara>A link to the namespace</simpara>
</listitem>
<listitem>
<simpara>A link to the ServiceMonitor details</simpara>
</listitem>
<listitem>
<simpara>Labels attached to the target</simpara>
</listitem>
<listitem>
<simpara>The most recent time that the target was scraped for metrics</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</chapter>
<chapter xml:id="managing-alerts">
<title>Managing alerts</title>

<simpara>In {product-title} {product-version}, the Alerting UI enables you to manage alerts, silences, and alerting rules.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Alerting rules</emphasis>. Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Alerts</emphasis>. An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an {product-title} cluster.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Silences</emphasis>. A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The alerts, silences, and alerting rules that are available in the Alerting UI relate to the projects that you have access to. For example, if you are logged in as a user with the <literal>cluster-admin</literal> role, you can access all alerts, silences, and alerting rules.</simpara>
<simpara>If you are a non-administrator user, you can create and silence alerts if you are assigned the following user roles:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>cluster-monitoring-view</literal> cluster role, which allows you to access Alertmanager</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-alertmanager-edit</literal> role, which permits you to create and silence alerts in the <emphasis role="strong">Administrator</emphasis> perspective in the web console</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-rules-edit</literal> cluster role, which permits you to create and silence alerts in the <emphasis role="strong">Developer</emphasis> perspective in the web console</simpara>
</listitem>
</itemizedlist>
</note>
<section xml:id="monitoring-accessing-the-alerting-ui_managing-alerts">
<title>Accessing the Alerting UI in the Administrator and Developer perspectives</title>
<simpara>The Alerting UI is accessible through the Administrator perspective and the Developer perspective in the {product-title} web console.</simpara>
<itemizedlist>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis>. The three main pages in the Alerting UI in this perspective are the <emphasis role="strong">Alerts</emphasis>, <emphasis role="strong">Silences</emphasis>, and <emphasis role="strong">Alerting Rules</emphasis> pages.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<listitem>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">&lt;project_name&gt;</emphasis> &#8594; <emphasis role="strong">Alerts</emphasis>. In this perspective, alerts, silences, and alerting rules are all managed from the <emphasis role="strong">Alerts</emphasis> page. The results shown in the <emphasis role="strong">Alerts</emphasis> page are specific to the selected project.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, you can select from core {product-title} and user-defined projects that you have access to in the <emphasis role="strong">Project:</emphasis> list. However, alerts, silences, and alerting rules relating to core {product-title} projects are not displayed if you are not logged in as a cluster administrator.</simpara>
</note>
</section>
<section xml:id="searching-alerts-silences-and-alerting-rules_managing-alerts">
<title>Searching and filtering alerts, silences, and alerting rules</title>
<simpara>You can filter the alerts, silences, and alerting rules that are displayed in the Alerting UI. This section provides a description of each of the available filtering options.</simpara>
<bridgehead xml:id="_understanding_alert_filters" renderas="sect3">Understanding alert filters</bridgehead>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, the <emphasis role="strong">Alerts</emphasis> page in the Alerting UI provides details about alerts relating to default {product-title} and user-defined projects. The page includes a summary of severity, state, and source for each alert. The time at which an alert went into its current state is also shown.</simpara>
<simpara>You can filter by alert state, severity, and source. By default, only <emphasis role="strong">Platform</emphasis> alerts that are <emphasis role="strong">Firing</emphasis> are displayed. The following describes each alert filtering option:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Alert State</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Firing</emphasis>. The alert is firing because the alert condition is true and the optional <literal>for</literal> duration has passed. The alert will continue to fire as long as the condition remains true.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Pending</emphasis>. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Silenced</emphasis>. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications will not be sent for alerts that match all the listed values or regular expressions.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Severity</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Critical</emphasis>. The condition that triggered the alert could have a critical impact. The alert requires immediate attention when fired and is typically paged to an individual or to a critical response team.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Warning</emphasis>. The alert provides a warning notification about something that might require attention to prevent a problem from occurring. Warnings are typically routed to a ticketing system for non-immediate review.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Info</emphasis>. The alert is provided for informational purposes only.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">None</emphasis>. The alert has no defined severity.</simpara>
</listitem>
<listitem>
<simpara>You can also create custom severity definitions for alerts relating to user-defined projects.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Source</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Platform</emphasis>. Platform-level alerts relate only to default {product-title} projects. These projects provide core {product-title} functionality.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">User</emphasis>. User alerts relate to user-defined projects. These alerts are user-created and are customizable. User-defined workload monitoring can be enabled postinstallation to provide observability into your own workloads.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<bridgehead xml:id="_understanding_silence_filters" renderas="sect3">Understanding silence filters</bridgehead>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, the <emphasis role="strong">Silences</emphasis> page in the Alerting UI provides details about silences applied to alerts in default {product-title} and user-defined projects. The page includes a summary of the state of each silence and the time at which a silence ends.</simpara>
<simpara>You can filter by silence state. By default, only <emphasis role="strong">Active</emphasis> and <emphasis role="strong">Pending</emphasis> silences are displayed. The following describes each silence state filter option:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Silence State</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Active</emphasis>. The silence is active and the alert will be muted until the silence is expired.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Pending</emphasis>. The silence has been scheduled and it is not yet active.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Expired</emphasis>. The silence has expired and notifications will be sent if the conditions for an alert are true.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<bridgehead xml:id="_understanding_alerting_rule_filters" renderas="sect3">Understanding alerting rule filters</bridgehead>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, the <emphasis role="strong">Alerting Rules</emphasis> page in the Alerting UI provides details about alerting rules relating to default {product-title} and user-defined projects. The page includes a summary of the state, severity, and source for each alerting rule.</simpara>
<simpara>You can filter alerting rules by alert state, severity, and source. By default, only <emphasis role="strong">Platform</emphasis> alerting rules are displayed. The following describes each alerting rule filtering option:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Alert State</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Firing</emphasis>. The alert is firing because the alert condition is true and the optional <literal>for</literal> duration has passed. The alert will continue to fire as long as the condition remains true.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Pending</emphasis>. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Silenced</emphasis>. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications will not be sent for alerts that match all the listed values or regular expressions.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Not Firing</emphasis>. The alert is not firing.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Severity</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Critical</emphasis>. The conditions defined in the alerting rule could have a critical impact. When true, these conditions require immediate attention. Alerts relating to the rule are typically paged to an individual or to a critical response team.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Warning</emphasis>. The conditions defined in the alerting rule might require attention to prevent a problem from occurring. Alerts relating to the rule are typically routed to a ticketing system for non-immediate review.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Info</emphasis>. The alerting rule provides informational alerts only.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">None</emphasis>. The alerting rule has no defined severity.</simpara>
</listitem>
<listitem>
<simpara>You can also create custom severity definitions for alerting rules relating to user-defined projects.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Source</emphasis> filters:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Platform</emphasis>. Platform-level alerting rules relate only to default {product-title} projects. These projects provide core {product-title} functionality.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">User</emphasis>. User-defined workload alerting rules relate to user-defined projects. These alerting rules are user-created and are customizable. User-defined workload monitoring can be enabled postinstallation to provide observability into your own workloads.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<bridgehead xml:id="_searching_and_filtering_alerts_silences_and_alerting_rules_in_the_developer_perspective" renderas="sect3">Searching and filtering alerts, silences, and alerting rules in the Developer perspective</bridgehead>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, the Alerts page in the Alerting UI provides a combined view of alerts and silences relating to the selected project. A link to the governing alerting rule is provided for each displayed alert.</simpara>
<simpara>In this view, you can filter by alert state and severity. By default, all alerts in the selected project are displayed if you have permission to access the project. These filters are the same as those described for the <emphasis role="strong">Administrator</emphasis> perspective.</simpara>
</section>
<section xml:id="getting-information-about-alerts-silences-and-alerting-rules_managing-alerts">
<title>Getting information about alerts, silences, and alerting rules</title>
<simpara>The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para><emphasis role="strong">To obtain information about alerts in the Administrator perspective</emphasis>:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Open the {product-title} web console and navigate to the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Alerts</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Optional: Search for alerts by name using the <emphasis role="strong">Name</emphasis> field in the search list.</simpara>
</listitem>
<listitem>
<simpara>Optional: Filter alerts by state, severity, and source by selecting filters in the <emphasis role="strong">Filter</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Optional: Sort the alerts by clicking one or more of the <emphasis role="strong">Name</emphasis>, <emphasis role="strong">Severity</emphasis>, <emphasis role="strong">State</emphasis>, and <emphasis role="strong">Source</emphasis> column headers.</simpara>
</listitem>
<listitem>
<simpara>Select the name of an alert to navigate to its <emphasis role="strong">Alert Details</emphasis> page. The page includes a graph that illustrates alert time series data. It also provides information about the alert, including:</simpara>
<itemizedlist>
<listitem>
<simpara>A description of the alert</simpara>
</listitem>
<listitem>
<simpara>Messages associated with the alerts</simpara>
</listitem>
<listitem>
<simpara>Labels attached to the alert</simpara>
</listitem>
<listitem>
<simpara>A link to its governing alerting rule</simpara>
</listitem>
<listitem>
<simpara>Silences for the alert, if any exist</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<simpara><emphasis role="strong">To obtain information about silences in the Administrator perspective</emphasis>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Silences</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Optional: Filter the silences by name using the <emphasis role="strong">Search by name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: Filter silences by state by selecting filters in the <emphasis role="strong">Filter</emphasis> list. By default, <emphasis role="strong">Active</emphasis> and <emphasis role="strong">Pending</emphasis> filters are applied.</simpara>
</listitem>
<listitem>
<simpara>Optional: Sort the silences by clicking one or more of the <emphasis role="strong">Name</emphasis>, <emphasis role="strong">Firing Alerts</emphasis>, and <emphasis role="strong">State</emphasis> column headers.</simpara>
</listitem>
<listitem>
<simpara>Select the name of a silence to navigate to its <emphasis role="strong">Silence Details</emphasis> page. The page includes the following details:</simpara>
<itemizedlist>
<listitem>
<simpara>Alert specification</simpara>
</listitem>
<listitem>
<simpara>Start time</simpara>
</listitem>
<listitem>
<simpara>End time</simpara>
</listitem>
<listitem>
<simpara>Silence state</simpara>
</listitem>
<listitem>
<simpara>Number and list of firing alerts</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<simpara><emphasis role="strong">To obtain information about alerting rules in the Administrator perspective</emphasis>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Alerting Rules</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Optional: Filter alerting rules by state, severity, and source by selecting filters in the <emphasis role="strong">Filter</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Optional: Sort the alerting rules by clicking one or more of the <emphasis role="strong">Name</emphasis>, <emphasis role="strong">Severity</emphasis>, <emphasis role="strong">Alert State</emphasis>, and <emphasis role="strong">Source</emphasis> column headers.</simpara>
</listitem>
<listitem>
<simpara>Select the name of an alerting rule to navigate to its <emphasis role="strong">Alerting Rule Details</emphasis> page. The page provides the following details about the alerting rule:</simpara>
<itemizedlist>
<listitem>
<simpara>Alerting rule name, severity, and description</simpara>
</listitem>
<listitem>
<simpara>The expression that defines the condition for firing the alert</simpara>
</listitem>
<listitem>
<simpara>The time for which the condition should be true for an alert to fire</simpara>
</listitem>
<listitem>
<simpara>A graph for each alert governed by the alerting rule, showing the value with which the alert is firing</simpara>
</listitem>
<listitem>
<simpara>A table of all alerts governed by the alerting rule</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<simpara><emphasis role="strong">To obtain information about alerts, silences, and alerting rules in the Developer perspective</emphasis>:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">&lt;project_name&gt;</emphasis> &#8594; <emphasis role="strong">Alerts</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>View details for an alert, silence, or an alerting rule:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Alert Details</emphasis> can be viewed by selecting <emphasis role="strong">&gt;</emphasis> to the left of an alert name and then selecting the alert in the list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Silence Details</emphasis> can be viewed by selecting a silence in the <emphasis role="strong">Silenced By</emphasis> section of the <emphasis role="strong">Alert Details</emphasis> page. The <emphasis role="strong">Silence Details</emphasis> page includes the following information:</simpara>
<itemizedlist>
<listitem>
<simpara>Alert specification</simpara>
</listitem>
<listitem>
<simpara>Start time</simpara>
</listitem>
<listitem>
<simpara>End time</simpara>
</listitem>
<listitem>
<simpara>Silence state</simpara>
</listitem>
<listitem>
<simpara>Number and list of firing alerts</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Alerting Rule Details</emphasis> can be viewed by selecting <emphasis role="strong">View Alerting Rule</emphasis> in the <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> menu on the right of an alert in the <emphasis role="strong">Alerts</emphasis> page.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<note>
<simpara>Only alerts, silences, and alerting rules relating to the selected project are displayed in the <emphasis role="strong">Developer</emphasis> perspective.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See the <link xl:href="https://github.com/openshift/runbooks/tree/master/alerts/cluster-monitoring-operator">Cluster Monitoring Operator runbooks</link> to help diagnose and resolve issues that trigger specific {product-title} monitoring alerts.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="managing-silences_managing-alerts">
<title>Managing silences</title>
<simpara>You can create a silence for an alert in the {product-title} web console in both the <emphasis role="strong">Administrator</emphasis> and <emphasis role="strong">Developer</emphasis> perspectives.
After you create a silence, you will not receive notifications about an alert when the alert fires.</simpara>
<simpara>Creating silences is useful in scenarios where you have received an initial alert notification, and you do not want to receive further notifications during the time in which you resolve the underlying issue causing the alert to fire.</simpara>
<simpara>When creating a silence, you must specify whether it becomes active immediately or at a later time. You must also set a duration period after which the silence expires.</simpara>
<simpara>After you create silences, you can view, edit, and expire them.</simpara>
<section xml:id="silencing-alerts_managing-alerts">
<title>Silencing alerts</title>
<simpara>You can silence a specific alert or silence alerts that match a specification that you define.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>If you are a cluster administrator, you have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>If you are a non-administrator user, you have access to the cluster as a user with the following user roles:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>cluster-monitoring-view</literal> cluster role, which allows you to access Alertmanager.</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-alertmanager-edit</literal> role, which permits you to create and silence alerts in the <emphasis role="strong">Administrator</emphasis> perspective in the web console.</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-rules-edit</literal> cluster role, which permits you to create and silence alerts in the <emphasis role="strong">Developer</emphasis> perspective in the web console.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To silence a specific alert in the <emphasis role="strong">Administrator</emphasis> perspective:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Alerts</emphasis> in the {product-title} web console.</simpara>
</listitem>
<listitem>
<simpara>For the alert that you want to silence, click <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Silence alert</emphasis> to open the <emphasis role="strong">Silence alert</emphasis> page with a default configuration for the chosen alert.</simpara>
</listitem>
<listitem>
<simpara>Optional: Change the default configuration details for the silence.</simpara>
<note>
<simpara>You must add a comment before saving a silence.</simpara>
</note>
</listitem>
<listitem>
<simpara>To save the silence, click <emphasis role="strong">Silence</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>To silence a specific alert in the <emphasis role="strong">Developer</emphasis> perspective:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">&lt;project_name&gt;</emphasis> &#8594; <emphasis role="strong">Alerts</emphasis> in the {product-title} web console.</simpara>
</listitem>
<listitem>
<simpara>If necessary, expand the details for the alert by selecting <emphasis role="strong">&gt;</emphasis> next to the alert name.</simpara>
</listitem>
<listitem>
<simpara>Click the alert message in the expanded view to open the <emphasis role="strong">Alert details</emphasis> page for the alert.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Silence alert</emphasis> to open the <emphasis role="strong">Silence alert</emphasis> page with a default configuration for the alert.</simpara>
</listitem>
<listitem>
<simpara>Optional: Change the default configuration details for the silence.</simpara>
<note>
<simpara>You must add a comment before saving a silence.</simpara>
</note>
</listitem>
<listitem>
<simpara>To save the silence, click <emphasis role="strong">Silence</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>To silence a set of alerts by creating a silence configuration in the <emphasis role="strong">Administrator</emphasis> perspective:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Silences</emphasis> in the {product-title} web console.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create silence</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Create silence</emphasis> page, set the schedule, duration, and label details for an alert.</simpara>
<note>
<simpara>You must add a comment before saving a silence.</simpara>
</note>
</listitem>
<listitem>
<simpara>To create silences for alerts that match the labels that you entered, click <emphasis role="strong">Silence</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>To silence a set of alerts by creating a silence configuration in the <emphasis role="strong">Developer</emphasis> perspective:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">&lt;project_name&gt;</emphasis> &#8594; <emphasis role="strong">Silences</emphasis> in the {product-title} web console.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create silence</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Create silence</emphasis> page, set the duration and label details for an alert.</simpara>
<note>
<simpara>You must add a comment before saving a silence.</simpara>
</note>
</listitem>
<listitem>
<simpara>To create silences for alerts that match the labels that you entered, click <emphasis role="strong">Silence</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="editing-silences_managing-alerts">
<title>Editing silences</title>
<simpara>You can edit a silence, which expires the existing silence and creates a new one with the changed configuration.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>If you are a cluster administrator, you have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>If you are a non-administrator user, you have access to the cluster as a user with the following user roles:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>cluster-monitoring-view</literal> cluster role, which allows you to access Alertmanager.</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-alertmanager-edit</literal> role, which permits you to create and silence alerts in the <emphasis role="strong">Administrator</emphasis> perspective in the web console.</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-rules-edit</literal> cluster role, which permits you to create and silence alerts in the <emphasis role="strong">Developer</emphasis> perspective in the web console.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To edit a silence in the <emphasis role="strong">Administrator</emphasis> perspective:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Silences</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>For the silence you want to modify, click <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Edit silence</emphasis>.</simpara>
<simpara>Alternatively, you can click <emphasis role="strong">Actions</emphasis> and select <emphasis role="strong">Edit silence</emphasis> on the <emphasis role="strong">Silence details</emphasis> page for a silence.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Edit silence</emphasis> page, make changes and click <emphasis role="strong">Silence</emphasis>. Doing so expires the existing silence and creates one with the updated configuration.</simpara>
</listitem>
</orderedlist>
<simpara>To edit a silence in the <emphasis role="strong">Developer</emphasis> perspective:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">&lt;project_name&gt;</emphasis> &#8594; <emphasis role="strong">Silences</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>For the silence you want to modify, click <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Edit silence</emphasis>.</simpara>
<simpara>Alternatively, you can click <emphasis role="strong">Actions</emphasis> and select <emphasis role="strong">Edit silence</emphasis> on the <emphasis role="strong">Silence details</emphasis> page for a silence.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Edit silence</emphasis> page, make changes and click <emphasis role="strong">Silence</emphasis>. Doing so expires the existing silence and creates one with the updated configuration.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="expiring-silences_managing-alerts">
<title>Expiring silences</title>
<simpara>You can expire a single silence or multiple silences. Expiring a silence deactivates it permanently.</simpara>
<note>
<simpara>You cannot delete expired, silenced alerts.
Expired silences older than 120 hours are garbage collected.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>If you are a cluster administrator, you have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>If you are a non-administrator user, you have access to the cluster as a user with the following user roles:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>cluster-monitoring-view</literal> cluster role, which allows you to access Alertmanager.</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-alertmanager-edit</literal> role, which permits you to create and silence alerts in the <emphasis role="strong">Administrator</emphasis> perspective in the web console.</simpara>
</listitem>
<listitem>
<simpara>The <literal>monitoring-rules-edit</literal> cluster role, which permits you to create and silence alerts in the <emphasis role="strong">Developer</emphasis> perspective in the web console.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To expire a silence or silences in the <emphasis role="strong">Administrator</emphasis> perspective:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Silences</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>For the silence or silences you want to expire, select the checkbox in the corresponding row.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Expire 1 silence</emphasis> to expire a single selected silence or <emphasis role="strong">Expire <emphasis>&lt;n&gt;</emphasis> silences</emphasis> to expire multiple selected silences, where <emphasis>&lt;n&gt;</emphasis> is the number of silences you selected.</simpara>
<simpara>Alternatively, to expire a single silence you can click <emphasis role="strong">Actions</emphasis> and select <emphasis role="strong">Expire silence</emphasis> on the <emphasis role="strong">Silence details</emphasis> page for a silence.</simpara>
</listitem>
</orderedlist>
<simpara>To expire a silence in the <emphasis role="strong">Developer</emphasis> perspective:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Go to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">&lt;project_name&gt;</emphasis> &#8594; <emphasis role="strong">Silences</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>For the silence or silences you want to expire, select the checkbox in the corresponding row.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Expire 1 silence</emphasis> to expire a single selected silence or <emphasis role="strong">Expire <emphasis>&lt;n&gt;</emphasis> silences</emphasis> to expire multiple selected silences, where <emphasis>&lt;n&gt;</emphasis> is the number of silences you selected.</simpara>
<simpara>Alternatively, to expire a single silence you can click <emphasis role="strong">Actions</emphasis> and select <emphasis role="strong">Expire silence</emphasis> on the <emphasis role="strong">Silence details</emphasis> page for a silence.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="managing-core-platform-alerting-rules_managing-alerts">
<title>Managing alerting rules for core platform monitoring</title>
<simpara>{product-title} {product-version} monitoring ships with a large set of default alerting rules for platform metrics.
As a cluster administrator, you can customize this set of rules in two ways:</simpara>
<itemizedlist>
<listitem>
<simpara>Modify the settings for existing platform alerting rules by adjusting thresholds or by adding and modifying labels.
For example, you can change the <literal>severity</literal> label for an alert from <literal>warning</literal> to <literal>critical</literal> to help you route and triage issues flagged by an alert.</simpara>
</listitem>
<listitem>
<simpara>Define and add new custom alerting rules by constructing a query expression based on core platform metrics in the <literal>openshift-monitoring</literal> namespace.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Core platform alerting rule considerations</title>
<listitem>
<simpara>New alerting rules must be based on the default {product-title} monitoring metrics.</simpara>
</listitem>
<listitem>
<simpara>You can only add and modify alerting rules. You cannot create new recording rules or modify existing recording rules.</simpara>
</listitem>
<listitem>
<simpara>If you modify existing platform alerting rules by using an <literal>AlertRelabelConfig</literal> object, your modifications are not reflected in the Prometheus alerts API.
Therefore, any dropped alerts still appear in the {product-title} web console even though they are no longer forwarded to Alertmanager.
Additionally, any modifications to alerts, such as a changed <literal>severity</literal> label, do not appear in the web console.</simpara>
</listitem>
</itemizedlist>
<section xml:id="tips-for-optimizing-alerting-rules-for-core-platform-monitoring_managing-alerts">
<title>Tips for optimizing alerting rules for core platform monitoring</title>
<simpara>If you customize core platform alerting rules to meet your organization&#8217;s specific needs, follow these guidelines to help ensure that the customized rules are efficient and effective.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Minimize the number of new rules</emphasis>.
Create only rules that are essential to your specific requirements.
By minimizing the number of rules, you create a more manageable and focused alerting system in your monitoring environment.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Focus on symptoms rather than causes</emphasis>.
Create rules that notify users of symptoms instead of underlying causes.
This approach ensures that users are promptly notified of a relevant symptom so that they can investigate the root cause after an alert has triggered.
This tactic also significantly reduces the overall number of rules you need to create.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Plan and assess your needs before implementing changes</emphasis>.
First, decide what symptoms are important and what actions you want users to take if these symptoms occur.
Then, assess existing rules and decide if you can modify any of them to meet your needs instead of creating entirely new rules for each symptom.
By modifying existing rules and creating new ones judiciously, you help to streamline your alerting system.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Provide clear alert messaging</emphasis>.
When you create alert messages, describe the symptom, possible causes, and recommended actions.
Include unambiguous, concise explanations along with troubleshooting steps or links to more information.
Doing so helps users quickly assess the situation and respond appropriately.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Include severity levels</emphasis>.
Assign severity levels to your rules to indicate how a user needs to react when a symptom occurs and triggers an alert.
For example, classifying an alert as <emphasis role="strong">Critical</emphasis> signals that an individual or a critical response team needs to respond immediately.
By defining severity levels, you help users know how to respond to an alert and help ensure that the most urgent issues receive prompt attention.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-new-alerting-rules_managing-alerts">
<title>Creating new alerting rules</title>
<simpara>As a cluster administrator, you can create new alerting rules based on platform metrics.
These alerting rules trigger alerts based on the values of chosen metrics.</simpara>
<note>
<simpara>If you create a customized <literal>AlertingRule</literal> resource based on an existing platform alerting rule, silence the original alert to avoid receiving conflicting alerts.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user that has the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new YAML configuration file named <literal>example-alerting-rule.yaml</literal> in the <literal>openshift-monitoring</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>Add an <literal>AlertingRule</literal> resource to the YAML file.
The following example creates a new alerting rule named <literal>example</literal>, similar to the default <literal>watchdog</literal> alert:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.openshift.io/v1
kind: AlertingRule
metadata:
  name: example
  namespace: openshift-monitoring
spec:
  groups:
  - name: example-rules
    rules:
    - alert: ExampleAlert <co xml:id="CO52-1"/>
      expr: vector(1) <co xml:id="CO52-2"/></programlisting>
<calloutlist>
<callout arearefs="CO52-1">
<para>The name of the alerting rule you want to create.</para>
</callout>
<callout arearefs="CO52-2">
<para>The PromQL query expression that defines the new rule.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration file to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-alerting-rule.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="modifying-core-platform-alerting-rules_managing-alerts">
<title>Modifying core platform alerting rules</title>
<simpara>As a cluster administrator, you can modify core platform alerts before Alertmanager routes them to a receiver.
For example, you can change the severity label of an alert, add a custom label, or exclude an alert from being sent to Alertmanager.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a new YAML configuration file named <literal>example-modified-alerting-rule.yaml</literal> in the <literal>openshift-monitoring</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>Add an <literal>AlertRelabelConfig</literal> resource to the YAML file.
The following example modifies the <literal>severity</literal> setting to <literal>critical</literal> for the default platform <literal>watchdog</literal> alerting rule:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.openshift.io/v1
kind: AlertRelabelConfig
metadata:
  name: watchdog
  namespace: openshift-monitoring
spec:
  configs:
  - sourceLabels: [alertname,severity] <co xml:id="CO53-1"/>
    regex: "Watchdog;none" <co xml:id="CO53-2"/>
    targetLabel: severity <co xml:id="CO53-3"/>
    replacement: critical <co xml:id="CO53-4"/>
    action: Replace <co xml:id="CO53-5"/></programlisting>
<calloutlist>
<callout arearefs="CO53-1">
<para>The source labels for the values you want to modify.</para>
</callout>
<callout arearefs="CO53-2">
<para>The regular expression against which the value of <literal>sourceLabels</literal> is matched.</para>
</callout>
<callout arearefs="CO53-3">
<para>The target label of the value you want to modify.</para>
</callout>
<callout arearefs="CO53-4">
<para>The new value to replace the target label.</para>
</callout>
<callout arearefs="CO53-5">
<para>The relabel action that replaces the old value based on regex matching.
The default action is <literal>Replace</literal>.
Other possible values are <literal>Keep</literal>, <literal>Drop</literal>, <literal>HashMod</literal>, <literal>LabelMap</literal>, <literal>LabelDrop</literal>, and <literal>LabelKeep</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration file to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-modified-alerting-rule.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/monitoring-overview.xml#monitoring-overview">Monitoring overview</link> for details about {product-title} {product-version} monitoring architecture.</simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager documentation</link> for information about alerting rules.</simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config">Prometheus relabeling documentation</link> for information about how relabeling works.</simpara>
</listitem>
<listitem>
<simpara>See the <link xl:href="https://prometheus.io/docs/practices/alerting/">Prometheus alerting documentation</link> for further guidelines on optimizing alerts.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="managing-alerting-rules-for-user-defined-projects_managing-alerts">
<title>Managing alerting rules for user-defined projects</title>
<simpara>{product-title} monitoring ships with a set of default alerting rules. As a cluster administrator, you can view the default alerting rules.</simpara>
<simpara>In {product-title} {product-version}, you can create, view, edit, and remove alerting rules in user-defined projects.</simpara>
<itemizedlist>
<title>Alerting rule considerations</title>
<listitem>
<simpara>The default alerting rules are used specifically for the {product-title} cluster.</simpara>
</listitem>
<listitem>
<simpara>Some alerting rules intentionally have identical names. They send alerts about the same event with different thresholds, different severity, or both.</simpara>
</listitem>
<listitem>
<simpara>Inhibition rules prevent notifications for lower severity alerts that are firing when a higher severity alert is also firing.</simpara>
</listitem>
</itemizedlist>
<section xml:id="Optimizing-alerting-for-user-defined-projects_managing-alerts">
<title>Optimizing alerting for user-defined projects</title>
<simpara>You can optimize alerting for your own projects by considering the following recommendations when creating alerting rules:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Minimize the number of alerting rules that you create for your project</emphasis>. Create alerting rules that notify you of conditions that impact you. It is more difficult to notice relevant alerts if you generate many alerts for conditions that do not impact you.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Create alerting rules for symptoms instead of causes</emphasis>. Create alerting rules that notify you of conditions regardless of the underlying cause. The cause can then be investigated. You will need many more alerting rules if each relates only to a specific cause. Some causes are then likely to be missed.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Plan before you write your alerting rules</emphasis>. Determine what symptoms are important to you and what actions you want to take if they occur. Then build an alerting rule for each symptom.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Provide clear alert messaging</emphasis>. State the symptom and recommended actions in the alert message.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Include severity levels in your alerting rules</emphasis>. The severity of an alert depends on how you need to react if the reported symptom occurs. For example, a critical alert should be triggered if a symptom requires immediate attention by an individual or a critical response team.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See the <link xl:href="https://prometheus.io/docs/practices/alerting/">Prometheus alerting documentation</link> for further guidelines on optimizing alerts</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/monitoring-overview.xml#monitoring-overview">Monitoring overview</link> for details about {product-title} {product-version} monitoring architecture</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-creating-alerting-rules-for-user-defined-projects_managing-alerts">
<title>About creating alerting rules for user-defined projects</title>
<simpara>If you create alerting rules for a user-defined project, consider the following key behaviors and important limitations when you define the new rules:</simpara>
<itemizedlist>
<listitem>
<simpara>A user-defined alerting rule can include metrics exposed by its own project in addition to the default metrics from core platform monitoring.
You cannot include metrics from another user-defined project.</simpara>
<simpara>For example, an alerting rule for the <literal>ns1</literal> user-defined project can use metrics exposed by the <literal>ns1</literal> project in addition to core platform metrics, such as CPU and memory metrics.
However, the rule cannot include metrics from a different <literal>ns2</literal> user-defined project.</simpara>
</listitem>
<listitem>
<simpara>To reduce latency and to minimize the load on core platform monitoring components, you can add the <literal>openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus</literal> label to a rule.
This label forces only the Prometheus instance deployed in the <literal>openshift-user-workload-monitoring</literal> project to evaluate the alerting rule and prevents the Thanos Ruler instance from doing so.</simpara>
<important>
<simpara>If an alerting rule has this label, your alerting rule can use only those metrics exposed by your user-defined project.
Alerting rules you create based on default platform metrics might not trigger alerts.</simpara>
</important>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-alerting-rules-for-user-defined-projects_managing-alerts">
<title>Creating alerting rules for user-defined projects</title>
<simpara>You can create alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user that has the <literal>monitoring-rules-edit</literal> cluster role for the project where you want to create an alerting rule.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for alerting rules. In this example, it is called <literal>example-app-alerting-rule.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add an alerting rule configuration to the YAML file. For example:</simpara>
<note>
<simpara>When you create an alerting rule, a project label is enforced on it if a rule with the same name exists in another project.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-alert
  namespace: ns1
spec:
  groups:
  - name: example
    rules:
    - alert: VersionAlert
      expr: version{job="prometheus-example-app"} == 0</programlisting>
<simpara>This configuration creates an alerting rule named <literal>example-alert</literal>. The alerting rule fires an alert when the <literal>version</literal> metric exposed by the sample service becomes <literal>0</literal>.</simpara>
</listitem>
<listitem>
<simpara>Apply the configuration file to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-app-alerting-rule.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/monitoring-overview.xml#monitoring-overview">Monitoring overview</link> for details about {product-title} {product-version} monitoring architecture.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="accessing-alerting-rules-for-your-project_managing-alerts">
<title>Accessing alerting rules for user-defined projects</title>
<simpara>To list alerting rules for a user-defined project, you must have been assigned the <literal>monitoring-rules-view</literal> cluster role for the project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user that has the <literal>monitoring-rules-view</literal> cluster role for your project.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To list alerting rules in <literal>&lt;project&gt;</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;project&gt; get prometheusrule</programlisting>
</listitem>
<listitem>
<simpara>To list the configuration of an alerting rule, run the following:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;project&gt; get prometheusrule &lt;rule&gt; -o yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="listing-alerting-rules-for-all-projects-in-a-single-view_managing-alerts">
<title>Listing alerting rules for all projects in a single view</title>
<simpara>As a cluster administrator,
you can list alerting rules for core {product-title} and user-defined projects together in a single view.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, navigate to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> &#8594; <emphasis role="strong">Alerting rules</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Platform</emphasis> and <emphasis role="strong">User</emphasis> sources in the <emphasis role="strong">Filter</emphasis> drop-down menu.</simpara>
<note>
<simpara>The <emphasis role="strong">Platform</emphasis> source is selected by default.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="removing-alerting-rules-for-user-defined-projects_managing-alerts">
<title>Removing alerting rules for user-defined projects</title>
<simpara>You can remove alerting rules for user-defined projects.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user that has the <literal>monitoring-rules-edit</literal> cluster role for the project where you want to create an alerting rule.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To remove rule <literal>&lt;foo&gt;</literal> in <literal>&lt;namespace&gt;</literal>, run the following:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;namespace&gt; delete prometheusrule &lt;foo&gt;</programlisting>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See the <link xl:href="https://prometheus.io/docs/alerting/alertmanager/">Alertmanager documentation</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="sending-notifications-to-external-systems_managing-alerts">
<title>Sending notifications to external systems</title>
<simpara>In {product-title} {product-version}, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure {product-title} to send alerts to the following receiver types:</simpara>
<itemizedlist>
<listitem>
<simpara>PagerDuty</simpara>
</listitem>
<listitem>
<simpara>Webhook</simpara>
</listitem>
<listitem>
<simpara>Email</simpara>
</listitem>
<listitem>
<simpara>Slack</simpara>
</listitem>
</itemizedlist>
<simpara>Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review.</simpara>
<formalpara>
<title>Checking that alerting is operational by using the watchdog alert</title>
<para>{product-title} monitoring includes a watchdog alert that fires continuously. Alertmanager repeatedly sends watchdog alert notifications to configured notification providers. The provider is usually configured to notify an administrator when it stops receiving the watchdog alert. This mechanism helps you quickly identify any communication issues between Alertmanager and the notification provider.</para>
</formalpara>
<section xml:id="configuring-alert-receivers_managing-alerts">
<title>Configuring alert receivers</title>
<simpara>You can configure alert receivers to ensure that you learn about important issues with your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, navigate to <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Cluster Settings</emphasis> &#8594; <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Alertmanager</emphasis>.</simpara>
<note>
<simpara>Alternatively, you can navigate to the same page through the notification drawer. Select the bell icon at the top right of the {product-title} web console and choose <emphasis role="strong">Configure</emphasis> in the <emphasis role="strong">AlertmanagerReceiverNotConfigured</emphasis> alert.</simpara>
</note>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Create Receiver</emphasis> in the <emphasis role="strong">Receivers</emphasis> section of the page.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Create Receiver</emphasis> form, add a <emphasis role="strong">Receiver Name</emphasis> and choose a <emphasis role="strong">Receiver Type</emphasis> from the list.</simpara>
</listitem>
<listitem>
<simpara>Edit the receiver configuration:</simpara>
<itemizedlist>
<listitem>
<simpara>For PagerDuty receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Choose an integration type and add a PagerDuty integration key.</simpara>
</listitem>
<listitem>
<simpara>Add the URL of your PagerDuty installation.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the client and incident details or the severity specification.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For webhook receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the endpoint to send HTTP POST requests to.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the default option to send resolved alerts to the receiver.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For email receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the email address to send notifications to.</simpara>
</listitem>
<listitem>
<simpara>Add SMTP configuration details, including the address to send notifications from, the smarthost and port number used for sending emails, the hostname of the SMTP server, and authentication details.</simpara>
</listitem>
<listitem>
<simpara>Choose whether TLS is required.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the default option not to send resolved alerts to the receiver or edit the body of email notifications configuration.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For Slack receivers:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the URL of the Slack webhook.</simpara>
</listitem>
<listitem>
<simpara>Add the Slack channel or user name to send notifications to.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Show advanced configuration</emphasis> if you want to edit the default option not to send resolved alerts to the receiver or edit the icon and username configuration. You can also choose whether to find and link channel names and usernames.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>By default, firing alerts with labels that match all of the selectors will be sent to the receiver. If you want label values for firing alerts to be matched exactly before they are sent to the receiver:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add routing label names and values in the <emphasis role="strong">Routing Labels</emphasis> section of the form.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Regular Expression</emphasis> if want to use a regular expression.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Add Label</emphasis> to add further routing labels.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Create</emphasis> to create the receiver.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-alert-routing-for-user-defined-projects_managing-alerts">
<title>Creating alert routing for user-defined projects</title>
<simpara role="_abstract">If you are a non-administrator user who has been given the <literal>alert-routing-edit</literal> cluster role, you can create or edit alert routing for user-defined projects.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster administrator has enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>A cluster administrator has enabled alert routing for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user that has the <literal>alert-routing-edit</literal> cluster role for the project for which you want to create alert routing.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for alert routing. The example in this procedure uses a file called <literal>example-app-alert-routing.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add an <literal>AlertmanagerConfig</literal> YAML definition to the file. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1beta1
kind: AlertmanagerConfig
metadata:
  name: example-routing
  namespace: ns1
spec:
  route:
    receiver: default
    groupBy: [job]
  receivers:
  - name: default
    webhookConfigs:
    - url: https://example.org/post</programlisting>
<note>
<simpara>For user-defined alerting rules, user-defined routing is scoped to the namespace in which the resource is defined.
For example, a routing configuration defined in the <literal>AlertmanagerConfig</literal> object for namespace <literal>ns1</literal> only applies to <literal>PrometheusRules</literal> resources in the same namespace.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
<listitem>
<simpara>Apply the resource to the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-app-alert-routing.yaml</programlisting>
<simpara>The configuration is automatically applied to the Alertmanager pods.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="applying-custom-alertmanager-configuration_managing-alerts">
<title>Applying a custom Alertmanager configuration</title>
<simpara>You can overwrite the default Alertmanager configuration by editing the <literal>alertmanager-main</literal> secret in the <literal>openshift-monitoring</literal> namespace for the platform instance of Alertmanager.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>Procedure</title>
<para>To change the Alertmanager configuration from the CLI:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Print the currently active Alertmanager configuration into file <literal>alertmanager.yaml</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode &gt; alertmanager.yaml</programlisting>
</listitem>
<listitem>
<simpara>Edit the configuration in <literal>alertmanager.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">global:
  resolve_timeout: 5m
route:
  group_wait: 30s <co xml:id="CO54-1"/>
  group_interval: 5m <co xml:id="CO54-2"/>
  repeat_interval: 12h <co xml:id="CO54-3"/>
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=&lt;your_service&gt;" <co xml:id="CO54-4"/>
    routes:
    - matchers:
      - &lt;your_matching_rules&gt; <co xml:id="CO54-5"/>
      receiver: &lt;receiver&gt; <co xml:id="CO54-6"/>
receivers:
- name: default
- name: watchdog
- name: &lt;receiver&gt;
#  &lt;receiver_configuration&gt;</programlisting>
<calloutlist>
<callout arearefs="CO54-1">
<para>The <literal>group_wait</literal> value specifies how long Alertmanager waits before sending an initial notification for a group of alerts.
This value controls how long Alertmanager waits while collecting initial alerts for the same group before sending a notification.</para>
</callout>
<callout arearefs="CO54-2">
<para>The <literal>group_interval</literal> value specifies how much time must elapse before Alertmanager sends a notification about new alerts added to a group of alerts for which an initial notification was already sent.</para>
</callout>
<callout arearefs="CO54-3">
<para>The <literal>repeat_interval</literal> value specifies the minimum amount of time that must pass before an alert notification is repeated.
If you want a notification to repeat at each group interval, set the <literal>repeat_interval</literal> value to less than the <literal>group_interval</literal> value.
However, the repeated notification can still be delayed, for example, when certain Alertmanager pods are restarted or rescheduled.</para>
</callout>
<callout arearefs="CO54-4">
<para>The <literal>service</literal> value specifies the service that fires the alerts.</para>
</callout>
<callout arearefs="CO54-5">
<para>The <literal>&lt;your_matching_rules&gt;</literal> value specifies the target alerts.</para>
</callout>
<callout arearefs="CO54-6">
<para>The <literal>receiver</literal> value specifies the receiver to use for the alert.</para>
</callout>
</calloutlist>
<note>
<simpara>Use the <literal>matchers</literal> key name to indicate the matchers that an alert has to fulfill to match the node.
Do not use the <literal>match</literal> or <literal>match_re</literal> key names, which are both deprecated and planned for removal in a future release.</simpara>
<simpara>In addition, if you define inhibition rules, use the <literal>target_matchers</literal> key name to indicate the target matchers and the <literal>source_matchers</literal> key name to indicate the source matchers.
Do not use the <literal>target_match</literal>, <literal>target_match_re</literal>, <literal>source_match</literal>, or <literal>source_match_re</literal> key names, which are deprecated and planned for removal in a future release.</simpara>
</note>
<simpara>The following Alertmanager configuration example configures PagerDuty as an alert receiver:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">global:
  resolve_timeout: 5m
route:
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=example-app"
    routes:
    - matchers:
      - "severity=critical"
      receiver: team-frontend-page*
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  pagerduty_configs:
  - service_key: "_your-key_"</programlisting>
<simpara>With this configuration, alerts of <literal>critical</literal> severity that are fired by the <literal>example-app</literal> service are sent using the <literal>team-frontend-page</literal> receiver. Typically these types of alerts would be paged to an individual or a critical response team.</simpara>
</listitem>
<listitem>
<simpara>Apply the new configuration in the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-monitoring replace secret --filename=-</programlisting>
</listitem>
</orderedlist>
<simpara>To change the Alertmanager configuration from the {product-title} web console:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Cluster Settings</emphasis> &#8594; <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Alertmanager</emphasis> &#8594; <emphasis role="strong">YAML</emphasis> page of the web console.</simpara>
</listitem>
<listitem>
<simpara>Modify the YAML configuration file.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="applying-a-custom-configuration-to-alertmanager-for-user-defined-alert-routing_managing-alerts">
<title>Applying a custom configuration to Alertmanager for user-defined alert routing</title>
<simpara>If you have enabled a separate instance of Alertmanager dedicated to user-defined alert routing, you can overwrite the configuration for this instance of Alertmanager by editing the <literal>alertmanager-user-workload</literal> secret in the <literal>openshift-user-workload-monitoring</literal> namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Print the currently active Alertmanager configuration into the file <literal>alertmanager.yaml</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get secret alertmanager-user-workload --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode &gt; alertmanager.yaml</programlisting>
</listitem>
<listitem>
<simpara>Edit the configuration in <literal>alertmanager.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">route:
  receiver: Default
  group_by:
  - name: Default
  routes:
  - matchers:
    - "service = prometheus-example-monitor" <co xml:id="CO55-1"/>
    receiver: &lt;receiver&gt; <co xml:id="CO55-2"/>
receivers:
- name: Default
- name: &lt;receiver&gt;
#  &lt;receiver_configuration&gt;</programlisting>
<calloutlist>
<callout arearefs="CO55-1">
<para>Specifies which alerts match the route. This example shows all alerts that have the <literal>service="prometheus-example-monitor"</literal> label.</para>
</callout>
<callout arearefs="CO55-2">
<para>Specifies the receiver to use for the alerts group.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the new configuration in the file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring create secret generic alertmanager-user-workload --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-user-workload-monitoring replace secret --filename=-</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="https://www.pagerduty.com/">the PagerDuty official site</link> for more information on PagerDuty.</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="https://www.pagerduty.com/docs/guides/prometheus-integration-guide/">the PagerDuty Prometheus Integration Guide</link> to learn how to retrieve the <literal>service_key</literal>.</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="https://prometheus.io/docs/alerting/configuration/">Alertmanager configuration</link> for configuring alerting through different alert receivers.</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/enabling-alert-routing-for-user-defined-projects.xml#enabling-alert-routing-for-user-defined-projects">Enabling alert routing for user-defined projects</link> to learn how to enable a dedicated instance of Alertmanager for user-defined alert routing.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="_next_steps_4">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/reviewing-monitoring-dashboards.xml#reviewing-monitoring-dashboards">Reviewing monitoring dashboards</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="reviewing-monitoring-dashboards">
<title>Reviewing monitoring dashboards</title>

<simpara>{product-title} {product-version} provides a comprehensive set of monitoring dashboards that help you understand the state of cluster components and user-defined workloads.</simpara>
<simpara>Use the <emphasis role="strong">Administrator</emphasis> perspective to access dashboards for the core {product-title} components, including the following items:</simpara>
<itemizedlist>
<listitem>
<simpara>API performance</simpara>
</listitem>
<listitem>
<simpara>etcd</simpara>
</listitem>
<listitem>
<simpara>Kubernetes compute resources</simpara>
</listitem>
<listitem>
<simpara>Kubernetes network resources</simpara>
</listitem>
<listitem>
<simpara>Prometheus</simpara>
</listitem>
<listitem>
<simpara>USE method dashboards relating to cluster and node performance</simpara>
</listitem>
<listitem>
<simpara>Node performance metrics</simpara>
</listitem>
</itemizedlist>
<figure>
<title>Example dashboard in the Administrator perspective</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/monitoring-dashboard-administrator.png"/>
</imageobject>
<textobject><phrase>monitoring dashboard administrator</phrase></textobject>
</mediaobject>
</figure>
<simpara>Use the <emphasis role="strong">Developer</emphasis> perspective to access Kubernetes compute resources dashboards that provide the following application metrics for a selected project:</simpara>
<itemizedlist>
<listitem>
<simpara>CPU usage</simpara>
</listitem>
<listitem>
<simpara>Memory usage</simpara>
</listitem>
<listitem>
<simpara>Bandwidth information</simpara>
</listitem>
<listitem>
<simpara>Packet rate information</simpara>
</listitem>
</itemizedlist>
<figure>
<title>Example dashboard in the Developer perspective</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/observe-dashboard-developer.png"/>
</imageobject>
<textobject><phrase>observe dashboard developer</phrase></textobject>
</mediaobject>
</figure>
<note>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, you can view dashboards for only one project at a time.</simpara>
</note>
<section xml:id="reviewing-monitoring-dashboards-admin_reviewing-monitoring-dashboards">
<title>Reviewing monitoring dashboards as a cluster administrator</title>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, you can view dashboards relating to core {product-title} cluster components.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective in the {product-title} web console, navigate to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Dashboards</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Choose a dashboard in the <emphasis role="strong">Dashboard</emphasis> list. Some dashboards, such as <emphasis role="strong">etcd</emphasis> and <emphasis role="strong">Prometheus</emphasis> dashboards, produce additional sub-menus when selected.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select a time range for the graphs in the <emphasis role="strong">Time Range</emphasis> list.</simpara>
<itemizedlist>
<listitem>
<simpara>Select a pre-defined time period.</simpara>
</listitem>
<listitem>
<simpara>Set a custom time range by selecting <emphasis role="strong">Custom time range</emphasis> in the <emphasis role="strong">Time Range</emphasis> list.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Input or select the <emphasis role="strong">From</emphasis> and <emphasis role="strong">To</emphasis> dates and times.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to save the custom time range.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Optional: Select a <emphasis role="strong">Refresh Interval</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Hover over each of the graphs within a dashboard to display detailed information about specific items.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="reviewing-monitoring-dashboards-developer_reviewing-monitoring-dashboards">
<title>Reviewing monitoring dashboards as a developer</title>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, you can view dashboards relating to a selected project. You must have access to monitor a project to view dashboard information for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a developer or as a user.</simpara>
</listitem>
<listitem>
<simpara>You have view permissions for the project that you are viewing the dashboard for.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the Developer perspective in the {product-title} web console, navigate to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Dashboard</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a project from the <emphasis role="strong">Project:</emphasis> drop-down list.</simpara>
</listitem>
<listitem>
<simpara>Select a dashboard from the <emphasis role="strong">Dashboard</emphasis> drop-down list to see the filtered metrics.</simpara>
<note>
<simpara>All dashboards produce additional sub-menus when selected, except <emphasis role="strong">Kubernetes / Compute Resources / Namespace (Pods)</emphasis>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: Select a time range for the graphs in the <emphasis role="strong">Time Range</emphasis> list.</simpara>
<itemizedlist>
<listitem>
<simpara>Select a pre-defined time period.</simpara>
</listitem>
<listitem>
<simpara>Set a custom time range by selecting <emphasis role="strong">Custom time range</emphasis> in the <emphasis role="strong">Time Range</emphasis> list.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Input or select the <emphasis role="strong">From</emphasis> and <emphasis role="strong">To</emphasis> dates and times.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis> to save the custom time range.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Optional: Select a <emphasis role="strong">Refresh Interval</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Hover over each of the graphs within a dashboard to display detailed information about specific items.</simpara>
</listitem>
</orderedlist>
<itemizedlist xml:id="additional-resources-reviewing-monitoring-dashboards" role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../applications/odc-monitoring-project-and-application-metrics-using-developer-perspective.xml#monitoring-project-and-application-metrics-using-developer-perspective">Monitoring project and application metrics using the Developer perspective</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="next-steps_reviewing-monitoring-dashboards">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/accessing-third-party-monitoring-apis.xml#accessing-third-party-monitoring-apis">Accessing third-party monitoring APIs</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="accessing-third-party-monitoring-apis">
<title>Accessing third-party monitoring APIs</title>

<simpara role="_abstract">In {product-title} {product-version}, you can access web service APIs for some third-party monitoring components from the command line interface (CLI).</simpara>
<section xml:id="accessing-third-party-monitoring-web-service-apis_accessing-third-party-monitoring-apis">
<title>Accessing third-party monitoring web service APIs</title>
<simpara role="_abstract">You can directly access third-party web service APIs from the command line for the following monitoring stack components: Prometheus, Alertmanager, Thanos Ruler, and Thanos Querier.</simpara>
<simpara>The following example commands show how to query the service API receivers for Alertmanager.
This example requires that the associated user account be bound against the <literal>monitoring-alertmanager-edit</literal> role in the <literal>openshift-monitoring</literal> namespace and that the account has the privilege to view the route.
This access only supports using a Bearer Token for authentication.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login -u &lt;username&gt; -p &lt;password&gt;</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ host=$(oc -n openshift-monitoring get route alertmanager-main -ojsonpath={.spec.host})</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ token=$(oc whoami -t)</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -H "Authorization: Bearer $token" -k "https://$host/api/v2/receivers"</programlisting>
<note>
<simpara>To access Thanos Ruler and Thanos Querier service APIs, the requesting account must have <literal>get</literal> permission on the namespaces resource, which can be done by granting the <literal>cluster-monitoring-view</literal> cluster role to the account.</simpara>
</note>
</section>
<section xml:id="monitoring-querying-metrics-by-using-the-federation-endpoint-for-prometheus_accessing-third-party-monitoring-apis">
<title>Querying metrics by using the federation endpoint for Prometheus</title>
<simpara>You can use the federation endpoint to scrape platform and user-defined metrics from a network location outside the cluster.
To do so, access the Prometheus <literal>/federate</literal> endpoint for the cluster via an {product-title} route.</simpara>
<warning>
<simpara>A delay in retrieving metrics data occurs when you use federation.
This delay can affect the accuracy and timeliness of the scraped metrics.</simpara>
<simpara>Using the federation endpoint can also degrade the performance and scalability of your cluster, especially if you use the federation endpoint to retrieve large amounts of metrics data.
To avoid these issues, follow these recommendations:</simpara>
<itemizedlist>
<listitem>
<simpara>Do not try to retrieve all metrics data via the federation endpoint.
Query it only when you want to retrieve a limited, aggregated data set.
For example, retrieving fewer than 1,000 samples for each request helps minimize the risk of performance degradation.</simpara>
</listitem>
<listitem>
<simpara>Avoid querying the federation endpoint frequently.
Limit queries to a maximum of one every 30 seconds.</simpara>
</listitem>
</itemizedlist>
<simpara>If you need to forward large amounts of data outside the cluster, use remote write instead. For more information, see the <emphasis>Configuring remote write storage</emphasis> section.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have obtained the host URL for the {product-title} route.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-monitoring-view</literal> cluster role or have obtained a bearer token with <literal>get</literal> permission on the <literal>namespaces</literal> resource.</simpara>
<note>
<simpara>You can only use bearer token authentication to access the federation endpoint.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the bearer token:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ token=`oc whoami -t`</programlisting>
</listitem>
<listitem>
<simpara>Query metrics from the <literal>/federate</literal> route.
The following example queries <literal>up</literal> metrics:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -G -s -k -H "Authorization: Bearer $token" \
    'https://&lt;federation_host&gt;/federate' \ <co xml:id="CO56-1"/>
    --data-urlencode 'match[]=up'</programlisting>
<calloutlist>
<callout arearefs="CO56-1">
<para>For &lt;federation_host&gt;, substitute the host URL for the federation route.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered"># TYPE up untyped
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.143.148:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035322214
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.148.166:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035338597
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.173.16:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035343834
...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_accessing-third-party-monitoring-apis" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#configuring_remote_write_storage_configuring-the-monitoring-stack">Configuring remote write storage</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/managing-metrics.xml#managing-metrics">Managing metrics</link></simpara>
</listitem>
<listitem>
<simpara><link xl:href="../monitoring/managing-alerts.xml#managing-alerts">Managing alerts</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="troubleshooting-monitoring-issues">
<title>Troubleshooting monitoring issues</title>

<simpara>Find troubleshooting steps for common issues with core platform and user-defined project monitoring.</simpara>
<section xml:id="investigating-why-user-defined-metrics-are-unavailable_troubleshooting-monitoring-issues">
<title>Investigating why user-defined project metrics are unavailable</title>
<simpara><literal>ServiceMonitor</literal> resources enable you to determine how to use the metrics exposed by a service in user-defined projects. Follow the steps outlined in this procedure if you have created a <literal>ServiceMonitor</literal> resource but cannot see any corresponding metrics in the Metrics UI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have enabled and configured monitoring for user-defined workloads.</simpara>
</listitem>
<listitem>
<simpara>You have created the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>ServiceMonitor</literal> resource.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara><emphasis role="strong">Check that the corresponding labels match</emphasis> in the service and <literal>ServiceMonitor</literal> resource configurations.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Obtain the label defined in the service. The following example queries the <literal>prometheus-example-app</literal> service in the <literal>ns1</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n ns1 get service prometheus-example-app -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">  labels:
    app: prometheus-example-app</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the <literal>matchLabels</literal> <literal>app</literal> label in the <literal>ServiceMonitor</literal> resource configuration matches the label output in the preceding step:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n ns1 get servicemonitor prometheus-example-monitor -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>apiVersion: v1
kind: Service
# ...
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app
# ...</screen>
</para>
</formalpara>
<note>
<simpara>You can check service and <literal>ServiceMonitor</literal> resource labels as a developer with view permissions for the project.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Inspect the logs for the Prometheus Operator</emphasis> in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>List the pods in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get pods</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-776fcbbd56-2nbfm   2/2     Running   0          132m
prometheus-user-workload-0             5/5     Running   1          132m
prometheus-user-workload-1             5/5     Running   1          132m
thanos-ruler-user-workload-0           3/3     Running   0          132m
thanos-ruler-user-workload-1           3/3     Running   0          132m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Obtain the logs from the <literal>prometheus-operator</literal> container in the <literal>prometheus-operator</literal> pod. In the following example, the pod is called <literal>prometheus-operator-776fcbbd56-2nbfm</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring logs prometheus-operator-776fcbbd56-2nbfm -c prometheus-operator</programlisting>
<simpara>If there is a issue with the service monitor, the logs might include an error similar to this example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">level=warn ts=2020-08-10T11:48:20.906739623Z caller=operator.go:1829 component=prometheusoperator msg="skipping servicemonitor" error="it accesses file system via bearer token file which Prometheus specification prohibits" servicemonitor=eagle/eagle namespace=openshift-user-workload-monitoring prometheus=user-workload</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Review the target status for your endpoint</emphasis> on the <emphasis role="strong">Metrics targets</emphasis> page in the {product-title} web console UI.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Log in to the {product-title} web console and navigate to <emphasis role="strong">Observe</emphasis> â†’ <emphasis role="strong">Targets</emphasis> in the <emphasis role="strong">Administrator</emphasis> perspective.</simpara>
</listitem>
<listitem>
<simpara>Locate the metrics endpoint in the list, and review the status of the target in the <emphasis role="strong">Status</emphasis> column.</simpara>
</listitem>
<listitem>
<simpara>If the <emphasis role="strong">Status</emphasis> is <emphasis role="strong">Down</emphasis>, click the URL for the endpoint to view more information on the <emphasis role="strong">Target Details</emphasis> page for that metrics target.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara><emphasis role="strong">Configure debug level logging for the Prometheus Operator</emphasis> in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config</programlisting>
</listitem>
<listitem>
<simpara>Add <literal>logLevel: debug</literal> for <literal>prometheusOperator</literal> under <literal>data/config.yaml</literal> to set the log level to <literal>debug</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheusOperator:
      logLevel: debug
# ...</programlisting>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
<note>
<simpara>The <literal>prometheus-operator</literal> in the <literal>openshift-user-workload-monitoring</literal> project restarts automatically when you apply the log-level change.</simpara>
</note>
</listitem>
<listitem>
<simpara>Confirm that the <literal>debug</literal> log-level has been applied to the <literal>prometheus-operator</literal> deployment in the <literal>openshift-user-workload-monitoring</literal> project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml |  grep "log-level"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">        - --log-level=debug</programlisting>
</para>
</formalpara>
<simpara>Debug level logging will show all calls made by the Prometheus Operator.</simpara>
</listitem>
<listitem>
<simpara>Check that the <literal>prometheus-operator</literal> pod is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-user-workload-monitoring get pods</programlisting>
<note>
<simpara>If an unrecognized Prometheus Operator <literal>loglevel</literal> value is included in the config map, the <literal>prometheus-operator</literal> pod might not restart successfully.</simpara>
</note>
</listitem>
<listitem>
<simpara>Review the debug logs to see if the Prometheus Operator is using the <literal>ServiceMonitor</literal> resource. Review the logs for other related errors.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="../monitoring/configuring-the-monitoring-stack.xml#creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack">Creating a user-defined workload monitoring config map</link></simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/managing-metrics.xml#specifying-how-a-service-is-monitored_managing-metrics">Specifying how a service is monitored</link> for details on how to create a <literal>ServiceMonitor</literal> or <literal>PodMonitor</literal> resource</simpara>
</listitem>
<listitem>
<simpara>See <link xl:href="../monitoring/managing-metrics.xml#getting-detailed-information-about-a-target_managing-metrics">Getting detailed information about metrics targets</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="determining-why-prometheus-is-consuming-disk-space_troubleshooting-monitoring-issues">
<title>Determining why Prometheus is consuming a lot of disk space</title>
<simpara>Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a <literal>customer_id</literal> attribute is unbound because it has an infinite number of possible values.</simpara>
<simpara>Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.</simpara>
<simpara>You can use the following measures when Prometheus consumes a lot of disk:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Check the number of scrape samples</emphasis> that are being collected.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Check the time series database (TSDB) status using the Prometheus HTTP API</emphasis> for more information about which labels are creating the most time series. Doing so requires cluster administrator privileges.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Reduce the number of unique time series that are created</emphasis> by reducing the number of unbound attributes that are assigned to user-defined metrics.</simpara>
<note>
<simpara>Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.</simpara>
</note>
</listitem>
<listitem>
<simpara><emphasis role="strong">Enforce limits on the number of samples that can be scraped</emphasis> across user-defined projects. This requires cluster administrator privileges.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective, navigate to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Run the following Prometheus Query Language (PromQL) query in the <emphasis role="strong">Expression</emphasis> field. This returns the ten metrics that have the highest number of scrape samples:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">topk(10,count by (job)({__name__=~".+"}))</programlisting>
</listitem>
<listitem>
<simpara>Investigate the number of unbound label values assigned to metrics with higher than expected scrape sample counts.</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">If the metrics relate to a user-defined project</emphasis>, review the metrics key-value pairs assigned to your workload. These are implemented through Prometheus client libraries at the application level. Try to limit the number of unbound attributes referenced in your labels.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">If the metrics relate to a core {product-title} project</emphasis>, create a Red Hat support case on the <link xl:href="https://access.redhat.com/">Red Hat Customer Portal</link>.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Review the TSDB status using the Prometheus HTTP API by running the following commands as a
cluster administrator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login -u &lt;username&gt; -p &lt;password&gt;</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ host=$(oc -n openshift-monitoring get route prometheus-k8s -ojsonpath={.spec.host})</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ token=$(oc whoami -t)</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -H "Authorization: Bearer $token" -k "https://$host/api/v1/status/tsdb"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">"status": "success",</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xl:href="../monitoring/configuring-the-monitoring-stack.xml#setting-scrape-sample-and-label-limits-for-user-defined-projects_configuring-the-monitoring-stack">Setting a scrape sample limit for user-defined projects</link> for details on how to set a scrape sample limit and create related alerting rules</simpara>
</listitem>
<listitem>
<simpara><link xl:href="../support/getting-support.xml#support-submitting-a-case_getting-support">Submitting a support case</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="config-map-reference-for-the-cluster-monitoring-operator">
<title>Config map reference for the Cluster Monitoring Operator</title>

<section xml:id="cluster-monitoring-operator-configuration-reference">
<title>Cluster Monitoring Operator configuration reference</title>
<simpara role="_abstract">Parts of {product-title} cluster monitoring are configurable.
The API is accessible by setting parameters defined in various config maps.</simpara>
<itemizedlist>
<listitem>
<simpara>To configure monitoring components, edit the <literal>ConfigMap</literal> object named <literal>cluster-monitoring-config</literal> in the <literal>openshift-monitoring</literal> namespace.
These configurations are defined by <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link>.</simpara>
</listitem>
<listitem>
<simpara>To configure monitoring components that monitor user-defined projects, edit the <literal>ConfigMap</literal> object named <literal>user-workload-monitoring-config</literal> in the <literal>openshift-user-workload-monitoring</literal> namespace.
These configurations are defined by <link xl:href="#userworkloadconfiguration">UserWorkloadConfiguration</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>The configuration file is always defined under the <literal>config.yaml</literal> key in the config map data.</simpara>
<note>
<itemizedlist>
<listitem>
<simpara>Not all configuration parameters are exposed.</simpara>
</listitem>
<listitem>
<simpara>Configuring cluster monitoring is optional.</simpara>
</listitem>
<listitem>
<simpara>If a configuration does not exist or is empty, default values are used.</simpara>
</listitem>
<listitem>
<simpara>If the configuration is invalid YAML data, the Cluster Monitoring Operator stops reconciling the resources and reports <literal>Degraded=True</literal> in the status conditions of the Operator.</simpara>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="_additionalalertmanagerconfig">
<title>AdditionalAlertmanagerConfig</title>
<section xml:id="_description">
<title>Description</title>
<simpara>The <literal>AdditionalAlertmanagerConfig</literal> resource defines settings for how a component communicates with additional Alertmanager instances.</simpara>
</section>
<section xml:id="_required">
<title>Required</title>
<itemizedlist>
<listitem>
<simpara><literal>apiVersion</literal></simpara>
</listitem>
</itemizedlist>
<simpara>Appears in: <link xl:href="#prometheusk8sconfig">PrometheusK8sConfig</link>,
<link xl:href="#prometheusrestrictedconfig">PrometheusRestrictedConfig</link>,
<link xl:href="#thanosrulerconfig">ThanosRulerConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>apiVersion</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the API version of Alertmanager. Possible values are <literal>v1</literal> or <literal>v2</literal>. The default is <literal>v2</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>bearerToken</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.SecretKeySelector</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the secret key reference containing the bearer token to use when authenticating to Alertmanager.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>pathPrefix</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the path prefix to add in front of the push endpoint path.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>scheme</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the URL scheme to use when communicating with Alertmanager instances. Possible values are <literal>http</literal> or <literal>https</literal>. The default value is <literal>http</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>staticConfigs</simpara></entry>
<entry align="left" valign="top"><simpara>[]string</simpara></entry>
<entry align="left" valign="top"><simpara>A list of statically configured Alertmanager endpoints in the form of <literal>&lt;hosts&gt;:&lt;port&gt;</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>timeout</simpara></entry>
<entry align="left" valign="top"><simpara>*string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the timeout value used when sending alerts.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tlsConfig</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#tlsconfig">TLSConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the TLS settings to use for Alertmanager connections.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_alertmanagermainconfig">
<title>AlertmanagerMainConfig</title>
<section xml:id="_description_2">
<title>Description</title>
<simpara>The <literal>AlertmanagerMainConfig</literal> resource defines settings for the Alertmanager component in the <literal>openshift-monitoring</literal> namespace.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>*bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the main Alertmanager instance in the <literal>openshift-monitoring</literal> namespace. The default value is <literal>true</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enableUserAlertmanagerConfig</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables user-defined namespaces to be selected for <literal>AlertmanagerConfig</literal> lookups. This setting only applies if the user workload monitoring instance of Alertmanager is not enabled. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level setting for Alertmanager. The possible values are: <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, <literal>debug</literal>. The default value is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the Pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the Alertmanager container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>secrets</simpara></entry>
<entry align="left" valign="top"><simpara>[]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a list of secrets to be mounted into Alertmanager. The secrets must reside within the same namespace as the Alertmanager object. They are added as volumes named <literal>secret-&lt;secret-name&gt;</literal> and mounted at <literal>/etc/alertmanager/secrets/&lt;secret-name&gt;</literal> in the <literal>alertmanager</literal> container of the Alertmanager pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>volumeClaimTemplate</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.EmbeddedPersistentVolumeClaim</simpara></entry>
<entry align="left" valign="top"><simpara>Defines persistent storage for Alertmanager. Use this setting to configure the persistent volume claim, including storage class, volume size, and name.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_alertmanageruserworkloadconfig">
<title>AlertmanagerUserWorkloadConfig</title>
<section xml:id="_description_3">
<title>Description</title>
<simpara>The <literal>AlertmanagerUserWorkloadConfig</literal> resource defines the settings for the Alertmanager instance used for user-defined projects.</simpara>
<simpara>Appears in: <link xl:href="#userworkloadconfiguration">UserWorkloadConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables a dedicated instance of Alertmanager for user-defined alerts in the <literal>openshift-user-workload-monitoring</literal> namespace. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enableAlertmanagerConfig</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag to enable or disable user-defined namespaces to be selected for <literal>AlertmanagerConfig</literal> lookup. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level setting for Alertmanager for user workload monitoring. The possible values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default value is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the Alertmanager container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>secrets</simpara></entry>
<entry align="left" valign="top"><simpara>[]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a list of secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object. They are added as volumes named <literal>secret-&lt;secret-name&gt;</literal> and mounted at <literal>/etc/alertmanager/secrets/&lt;secret-name&gt;</literal> in the <literal>alertmanager</literal> container of the Alertmanager pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>volumeClaimTemplate</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.EmbeddedPersistentVolumeClaim</simpara></entry>
<entry align="left" valign="top"><simpara>Defines persistent storage for Alertmanager. Use this setting to configure the persistent volume claim, including storage class, volume size and name.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_clustermonitoringconfiguration">
<title>ClusterMonitoringConfiguration</title>
<section xml:id="_description_4">
<title>Description</title>
<simpara>The <literal>ClusterMonitoringConfiguration</literal> resource defines settings that customize the default platform monitoring stack through the <literal>cluster-monitoring-config</literal> config map in the <literal>openshift-monitoring</literal> namespace.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>alertmanagerMain</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#alertmanagermainconfig">AlertmanagerMainConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>AlertmanagerMainConfig</literal> defines settings for the Alertmanager component in the <literal>openshift-monitoring</literal> namespace.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enableUserWorkload</simpara></entry>
<entry align="left" valign="top"><simpara>*bool</simpara></entry>
<entry align="left" valign="top"><simpara><literal>UserWorkloadEnabled</literal> is a Boolean flag that enables monitoring for user-defined projects.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>k8sPrometheusAdapter</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#k8sprometheusadapter">K8sPrometheusAdapter</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>K8sPrometheusAdapter</literal> defines settings for the Prometheus Adapter component.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>kubeStateMetrics</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#kubestatemetricsconfig">KubeStateMetricsConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>KubeStateMetricsConfig</literal> defines settings for the <literal>kube-state-metrics</literal> agent.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>metricsServer</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#metricsserverconfig">MetricsServerConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>MetricsServer</literal> defines settings for the Metrics Server component.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>prometheusK8s</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#prometheusk8sconfig">PrometheusK8sConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>PrometheusK8sConfig</literal> defines settings for the Prometheus component.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>prometheusOperator</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#prometheusoperatorconfig">PrometheusOperatorConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>PrometheusOperatorConfig</literal> defines settings for the Prometheus Operator component.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>prometheusOperatorAdmissionWebhook</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#prometheusoperatoradmissionwebhookconfig">PrometheusOperatorAdmissionWebhookConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>PrometheusOperatorAdmissionWebhookConfig</literal> defines settings for the admission webhook component of Prometheus Operator.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>openshiftStateMetrics</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#openshiftstatemetricsconfig">OpenShiftStateMetricsConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>OpenShiftMetricsConfig</literal> defines settings for the <literal>openshift-state-metrics</literal> agent.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>telemeterClient</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#telemeterclientconfig">TelemeterClientConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>TelemeterClientConfig</literal> defines settings for the Telemeter Client component.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>thanosQuerier</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#thanosquerierconfig">ThanosQuerierConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>ThanosQuerierConfig</literal> defines settings for the Thanos Querier component.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeExporter</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexporterconfig">NodeExporterConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>NodeExporterConfig</literal> defines settings for the <literal>node-exporter</literal> agent.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>monitoringPlugin</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#monitoringpluginconfig">MonitoringPluginConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara><literal>MonitoringPluginConfig</literal> defines settings for the monitoring <literal>console-plugin</literal> component.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_dedicatedservicemonitors">
<title>DedicatedServiceMonitors</title>
<section xml:id="_description_5">
<title>Description</title>
<important>
<simpara>This setting is deprecated and is planned to be removed in a future {product-title} version.
In the current version, this setting still exists but has no effect.</simpara>
</important>
<simpara>You can use the <literal>DedicatedServiceMonitors</literal> resource to configure dedicated Service Monitors for the Prometheus Adapter</simpara>
<simpara>Appears in: <link xl:href="#k8sprometheusadapter">K8sPrometheusAdapter</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>When <literal>enabled</literal> is set to <literal>true</literal>, the Cluster Monitoring Operator (CMO) deploys a dedicated Service Monitor that exposes the kubelet <literal>/metrics/resource</literal> endpoint. This Service Monitor sets <literal>honorTimestamps: true</literal> and only keeps metrics that are relevant for the pod resource queries of Prometheus Adapter. Additionally, Prometheus Adapter is configured to use these dedicated metrics. Overall, this feature improves the consistency of Prometheus Adapter-based CPU usage measurements used by, for example, the <literal>oc adm top pod</literal> command or the Horizontal Pod Autoscaler.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_k8sprometheusadapter">
<title>K8sPrometheusAdapter</title>
<section xml:id="_description_6">
<title>Description</title>
<simpara>The <literal>K8sPrometheusAdapter</literal> resource defines settings for the Prometheus Adapter component.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>audit</simpara></entry>
<entry align="left" valign="top"><simpara>*Audit</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the audit configuration used by the Prometheus Adapter instance. Possible profile values are: <literal>metadata</literal>, <literal>request</literal>, <literal>requestresponse</literal>, and <literal>none</literal>. The default value is <literal>metadata</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>PrometheusAdapter</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>dedicatedServiceMonitors</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#dedicatedservicemonitors">DedicatedServiceMonitors</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines dedicated service monitors.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_kubestatemetricsconfig">
<title>KubeStateMetricsConfig</title>
<section xml:id="_description_7">
<title>Description</title>
<simpara>The <literal>KubeStateMetricsConfig</literal> resource defines settings for the <literal>kube-state-metrics</literal> agent.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>KubeStateMetrics</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_metricsserverconfig">
<title>MetricsServerConfig</title>
<section xml:id="_description_8">
<title>Description</title>
<important>
<simpara>Metrics Server is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xl:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>The <literal>MetricsServerConfig</literal> resource defines settings for the Metrics Server component. Note that this setting only applies when the <literal>TechPreviewNoUpgrade</literal> feature gate is enabled.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the Metrics Server container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_prometheusoperatoradmissionwebhookconfig">
<title>PrometheusOperatorAdmissionWebhookConfig</title>
<section xml:id="_description_9">
<title>Description</title>
<simpara>The <literal>PrometheusOperatorAdmissionWebhookConfig</literal> resource defines settings for the admission webhook workload for Prometheus Operator.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>prometheus-operator-admission-webhook</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_monitoringpluginconfig">
<title>MonitoringPluginConfig</title>
<section xml:id="_description_10">
<title>Description</title>
<simpara>The <literal>MonitoringPluginConfig</literal> resource defines settings for the web console plugin component in the <literal>openshift-monitoring</literal> namespace.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>console-plugin</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines a pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectorbuddyinfoconfig">
<title>NodeExporterCollectorBuddyInfoConfig</title>
<section xml:id="_description_11">
<title>Description</title>
<simpara>The <literal>NodeExporterCollectorBuddyInfoConfig</literal> resource works as an on/off switch for the <literal>buddyinfo</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>buddyinfo</literal> collector is disabled.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>buddyinfo</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectorconfig">
<title>NodeExporterCollectorConfig</title>
<section xml:id="_description_12">
<title>Description</title>
<simpara>The <literal>NodeExporterCollectorConfig</literal> resource defines settings for individual collectors of the <literal>node-exporter</literal> agent.</simpara>
<simpara>Appears in: <link xl:href="#nodeexporterconfig">NodeExporterConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>cpufreq</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectorcpufreqconfig">NodeExporterCollectorCpufreqConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>cpufreq</literal> collector, which collects CPU frequency statistics. Disabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tcpstat</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectortcpstatconfig">NodeExporterCollectorTcpStatConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>tcpstat</literal> collector, which collects TCP connection statistics. Disabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>netdev</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectornetdevconfig">NodeExporterCollectorNetDevConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>netdev</literal> collector, which collects network devices statistics. Enabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>netclass</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectornetclassconfig">NodeExporterCollectorNetClassConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>netclass</literal> collector, which collects information about network devices. Enabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>buddyinfo</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectorbuddyinfoconfig">NodeExporterCollectorBuddyInfoConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>buddyinfo</literal> collector, which collects statistics about memory fragmentation from the <literal>node_buddyinfo_blocks</literal> metric. This metric collects data from <literal>/proc/buddyinfo</literal>. Disabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>mountstats</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectormountstatsconfig">NodeExporterCollectorMountStatsConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>mountstats</literal> collector, which collects statistics about NFS volume I/O activities. Disabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ksmd</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectorksmdconfig">NodeExporterCollectorKSMDConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>ksmd</literal> collector, which collects statistics from the kernel same-page merger daemon. Disabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>processes</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectorprocessesconfig">NodeExporterCollectorProcessesConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>processes</literal> collector, which collects statistics from processes and threads running in the system. Disabled by default.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>systemd</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectorsystemdconfig">NodeExporterCollectorSystemdConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>systemd</literal> collector, which collects statistics on the systemd daemon and its managed services. Disabled by default.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectorcpufreqconfig">
<title>NodeExporterCollectorCpufreqConfig</title>
<section xml:id="_description_13">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorCpufreqConfig</literal> resource to enable or disable the <literal>cpufreq</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>cpufreq</literal> collector is disabled. Under certain circumstances, enabling the <literal>cpufreq</literal> collector increases CPU usage on machines with many cores. If you enable this collector and have machines with many cores, monitor your systems closely for excessive CPU usage.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>cpufreq</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectorksmdconfig">
<title>NodeExporterCollectorKSMDConfig</title>
<section xml:id="_description_14">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorKSMDConfig</literal> resource to enable or disable the <literal>ksmd</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>ksmd</literal> collector is disabled.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>ksmd</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectormountstatsconfig">
<title>NodeExporterCollectorMountStatsConfig</title>
<section xml:id="_description_15">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorMountStatsConfig</literal> resource to enable or disable the <literal>mountstats</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>mountstats</literal> collector is disabled. If you enable the collector, the following metrics become available: <literal>node_mountstats_nfs_read_bytes_total</literal>, <literal>node_mountstats_nfs_write_bytes_total</literal>, and <literal>node_mountstats_nfs_operations_requests_total</literal>. Be aware that these metrics can have a high cardinality. If you enable this collector, closely monitor any increases in memory usage for the <literal>prometheus-k8s</literal> pods.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>mountstats</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectornetclassconfig">
<title>NodeExporterCollectorNetClassConfig</title>
<section xml:id="_description_16">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorNetClassConfig</literal> resource to enable or disable the <literal>netclass</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>netclass</literal> collector is enabled. If you disable this collector, these metrics become unavailable: <literal>node_network_info</literal>, <literal>node_network_address_assign_type</literal>, <literal>node_network_carrier</literal>, <literal>node_network_carrier_changes_total</literal>, <literal>node_network_carrier_up_changes_total</literal>, <literal>node_network_carrier_down_changes_total</literal>, <literal>node_network_device_id</literal>, <literal>node_network_dormant</literal>, <literal>node_network_flags</literal>, <literal>node_network_iface_id</literal>, <literal>node_network_iface_link</literal>, <literal>node_network_iface_link_mode</literal>, <literal>node_network_mtu_bytes</literal>, <literal>node_network_name_assign_type</literal>, <literal>node_network_net_dev_group</literal>, <literal>node_network_speed_bytes</literal>, <literal>node_network_transmit_queue_length</literal>, and <literal>node_network_protocol_type</literal>.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>netclass</literal> collector.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>useNetlink</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that activates the <literal>netlink</literal> implementation of the <literal>netclass</literal> collector. The default value is <literal>true</literal>, which activates the <literal>netlink</literal> mode. This implementation improves the performance of the <literal>netclass</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectornetdevconfig">
<title>NodeExporterCollectorNetDevConfig</title>
<section xml:id="_description_17">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorNetDevConfig</literal> resource to enable or disable the <literal>netdev</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>netdev</literal> collector is enabled. If disabled, these metrics become unavailable: <literal>node_network_receive_bytes_total</literal>, <literal>node_network_receive_compressed_total</literal>, <literal>node_network_receive_drop_total</literal>, <literal>node_network_receive_errs_total</literal>, <literal>node_network_receive_fifo_total</literal>, <literal>node_network_receive_frame_total</literal>, <literal>node_network_receive_multicast_total</literal>, <literal>node_network_receive_nohandler_total</literal>, <literal>node_network_receive_packets_total</literal>, <literal>node_network_transmit_bytes_total</literal>, <literal>node_network_transmit_carrier_total</literal>, <literal>node_network_transmit_colls_total</literal>, <literal>node_network_transmit_compressed_total</literal>, <literal>node_network_transmit_drop_total</literal>, <literal>node_network_transmit_errs_total</literal>, <literal>node_network_transmit_fifo_total</literal>, and <literal>node_network_transmit_packets_total</literal>.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>netdev</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectorprocessesconfig">
<title>NodeExporterCollectorProcessesConfig</title>
<section xml:id="_description_18">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorProcessesConfig</literal> resource to enable or disable the <literal>processes</literal> collector of the <literal>node-exporter</literal> agent. If the collector is enabled, the following metrics become available: <literal>node_processes_max_processes</literal>, <literal>node_processes_pids</literal>, <literal>node_processes_state</literal>, <literal>node_processes_threads</literal>, <literal>node_processes_threads_state</literal>. The metric <literal>node_processes_state</literal> and <literal>node_processes_threads_state</literal> can have up to five series each, depending on the state of the processes and threads. The possible states of a process or a thread are: <literal>D</literal> (UNINTERRUPTABLE_SLEEP), <literal>R</literal> (RUNNING &amp; RUNNABLE), <literal>S</literal> (INTERRUPTABLE_SLEEP), <literal>T</literal> (STOPPED), or <literal>Z</literal> (ZOMBIE). By default, the <literal>processes</literal> collector is disabled.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>processes</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectorsystemdconfig">
<title>NodeExporterCollectorSystemdConfig</title>
<section xml:id="_description_19">
<title>Description</title>
<simpara>Use the <literal>NodeExporterCollectorSystemdConfig</literal> resource to enable or disable the <literal>systemd</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>systemd</literal> collector is disabled. If enabled, the following metrics become available: <literal>node_systemd_system_running</literal>, <literal>node_systemd_units</literal>, <literal>node_systemd_version</literal>. If the unit uses a socket, it also generates the following metrics: <literal>node_systemd_socket_accepted_connections_total</literal>, <literal>node_systemd_socket_current_connections</literal>, <literal>node_systemd_socket_refused_connections_total</literal>.  You can use the <literal>units</literal> parameter to select the <literal>systemd</literal> units to be included by the <literal>systemd</literal> collector. The selected units are used to generate the <literal>node_systemd_unit_state</literal> metric, which shows the state of each <literal>systemd</literal> unit. However, this metric&#8217;s cardinality might be high (at least five series per unit per node). If you enable this collector with a long list of selected units, closely monitor the <literal>prometheus-k8s</literal> deployment for excessive memory usage. Note that the <literal>node_systemd_timer_last_trigger_seconds</literal> metric is only shown if you have configured the value of the <literal>units</literal> parameter as <literal>logrotate.timer</literal>.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>systemd</literal> collector.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>units</simpara></entry>
<entry align="left" valign="top"><simpara>[]string</simpara></entry>
<entry align="left" valign="top"><simpara>A list of regular expression (regex) patterns that match systemd units to be included by the <literal>systemd</literal> collector. By default, the list is empty, so the collector exposes no metrics for systemd units.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexportercollectortcpstatconfig">
<title>NodeExporterCollectorTcpStatConfig</title>
<section xml:id="_description_20">
<title>Description</title>
<simpara>The <literal>NodeExporterCollectorTcpStatConfig</literal> resource works as an on/off switch for the <literal>tcpstat</literal> collector of the <literal>node-exporter</literal> agent. By default, the <literal>tcpstat</literal> collector is disabled.</simpara>
<simpara>Appears in: <link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enabled</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables the <literal>tcpstat</literal> collector.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_nodeexporterconfig">
<title>NodeExporterConfig</title>
<section xml:id="_description_21">
<title>Description</title>
<simpara>The <literal>NodeExporterConfig</literal> resource defines settings for the <literal>node-exporter</literal> agent.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>collectors</simpara></entry>
<entry align="left" valign="top"><simpara><link xl:href="#nodeexportercollectorconfig">NodeExporterCollectorConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines which collectors are enabled and their additional configuration parameters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>maxProcs</simpara></entry>
<entry align="left" valign="top"><simpara>uint32</simpara></entry>
<entry align="left" valign="top"><simpara>The target number of CPUs on which the node-exporter&#8217;s process will run. The default value is <literal>0</literal>, which means that node-exporter runs on all CPUs. If a kernel deadlock occurs or if performance degrades when reading from <literal>sysfs</literal> concurrently, you can change this value to <literal>1</literal>, which limits node-exporter to running on one CPU. For nodes with a high CPU count, you can set the limit to a low number, which  saves resources by preventing Go routines from being scheduled to run on all CPUs. However, I/O performance degrades if the <literal>maxProcs</literal> value is set too low and there are many metrics to collect.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ignoredNetworkDevices</simpara></entry>
<entry align="left" valign="top"><simpara>*[]string</simpara></entry>
<entry align="left" valign="top"><simpara>A list of network devices, defined as regular expressions, that you want to exclude from the relevant collector configuration such as <literal>netdev</literal> and <literal>netclass</literal>. If no list is specified, the Cluster Monitoring Operator uses a predefined list of devices to be excluded to minimize the impact on memory usage. If the list is empty, no devices are excluded. If you modify this setting, monitor the <literal>prometheus-k8s</literal> deployment closely for excessive memory usage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>NodeExporter</literal> container.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_openshiftstatemetricsconfig">
<title>OpenShiftStateMetricsConfig</title>
<section xml:id="_description_22">
<title>Description</title>
<simpara>The <literal>OpenShiftStateMetricsConfig</literal> resource defines settings for the <literal>openshift-state-metrics</literal> agent.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>OpenShiftStateMetrics</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_prometheusk8sconfig">
<title>PrometheusK8sConfig</title>
<section xml:id="_description_23">
<title>Description</title>
<simpara>The <literal>PrometheusK8sConfig</literal> resource defines settings for the Prometheus component.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>additionalAlertmanagerConfigs</simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xl:href="#additionalalertmanagerconfig">AdditionalAlertmanagerConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configures additional Alertmanager instances that receive alerts from the Prometheus component. By default, no additional Alertmanager instances are configured.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enforcedBodySizeLimit</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Enforces a body size limit for Prometheus scraped metrics. If a scraped target&#8217;s body response is larger than the limit, the scrape will fail. The following values are valid: an empty value to specify no limit, a numeric value in Prometheus size format (such as <literal>64MB</literal>), or the string <literal>automatic</literal>, which indicates that the limit will be automatically calculated based on cluster capacity. The default value is empty, which indicates no limit.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>externalLabels</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines labels to be added to any time series or alerts when communicating with external systems such as federation, remote storage, and Alertmanager. By default, no labels are added.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level setting for Prometheus. The possible values are: <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default value is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>queryLogFile</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the file to which PromQL queries are logged. This setting can be either a filename, in which case the queries are saved to an <literal>emptyDir</literal> volume at <literal>/var/log/prometheus</literal>, or a full path to a location where an <literal>emptyDir</literal> volume will be mounted and the queries saved. Writing to <literal>/dev/stderr</literal>, <literal>/dev/stdout</literal> or <literal>/dev/null</literal> is supported, but writing to any other <literal>/dev/</literal> path is not supported. Relative paths are also not supported. By default, PromQL queries are not logged.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>remoteWrite</simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xl:href="#remotewritespec">RemoteWriteSpec</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the remote write configuration, including URL, authentication, and relabeling settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>Prometheus</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>retention</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: <literal>[0-9]+(ms|s|m|h|d|w|y)</literal> (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is <literal>15d</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>retentionSize</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the maximum amount of disk space used by data blocks plus the write-ahead log (WAL). Supported values are <literal>B</literal>, <literal>KB</literal>, <literal>KiB</literal>, <literal>MB</literal>, <literal>MiB</literal>, <literal>GB</literal>, <literal>GiB</literal>, <literal>TB</literal>, <literal>TiB</literal>, <literal>PB</literal>, <literal>PiB</literal>, <literal>EB</literal>, and <literal>EiB</literal>. By default, no limit is defined.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>collectionProfile</simpara></entry>
<entry align="left" valign="top"><simpara>CollectionProfile</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the metrics collection profile that Prometheus uses to collect metrics from the platform components. Supported values are <literal>full</literal> or <literal>minimal</literal>. In the <literal>full</literal> profile (default), Prometheus collects all metrics that are exposed by the platform components. In the <literal>minimal</literal> profile, Prometheus only collects metrics necessary for the default platform alerts, recording rules, telemetry, and console dashboards.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>volumeClaimTemplate</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.EmbeddedPersistentVolumeClaim</simpara></entry>
<entry align="left" valign="top"><simpara>Defines persistent storage for Prometheus. Use this setting to configure the persistent volume claim, including storage class, volume size and name.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_prometheusoperatorconfig">
<title>PrometheusOperatorConfig</title>
<section xml:id="_description_24">
<title>Description</title>
<simpara>The <literal>PrometheusOperatorConfig</literal> resource defines settings for the Prometheus Operator component.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link>,
<link xl:href="#userworkloadconfiguration">UserWorkloadConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level settings for Prometheus Operator. The possible values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default value is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>PrometheusOperator</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_prometheusrestrictedconfig">
<title>PrometheusRestrictedConfig</title>
<section xml:id="_description_25">
<title>Description</title>
<simpara>The <literal>PrometheusRestrictedConfig</literal> resource defines the settings for the Prometheus component that monitors user-defined projects.</simpara>
<simpara>Appears in: <link xl:href="#userworkloadconfiguration">UserWorkloadConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>additionalAlertmanagerConfigs</simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xl:href="#additionalalertmanagerconfig">AdditionalAlertmanagerConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configures additional Alertmanager instances that receive alerts from the Prometheus component. By default, no additional Alertmanager instances are configured.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enforcedLabelLimit</simpara></entry>
<entry align="left" valign="top"><simpara>*uint64</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a per-scrape limit on the number of labels accepted for a sample. If the number of labels exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is <literal>0</literal>, which means that no limit is set.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enforcedLabelNameLengthLimit</simpara></entry>
<entry align="left" valign="top"><simpara>*uint64</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a per-scrape limit on the length of a label name for a sample. If the length of a label name exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is <literal>0</literal>, which means that no limit is set.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enforcedLabelValueLengthLimit</simpara></entry>
<entry align="left" valign="top"><simpara>*uint64</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a per-scrape limit on the length of a label value for a sample. If the length of a label value exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is <literal>0</literal>, which means that no limit is set.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enforcedSampleLimit</simpara></entry>
<entry align="left" valign="top"><simpara>*uint64</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a global limit on the number of scraped samples that will be accepted. This setting overrides the <literal>SampleLimit</literal> value set in any user-defined <literal>ServiceMonitor</literal> or <literal>PodMonitor</literal> object if the value is greater than <literal>enforcedTargetLimit</literal>. Administrators can use this setting to keep the overall number of samples under control. The default value is <literal>0</literal>, which means that no limit is set.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enforcedTargetLimit</simpara></entry>
<entry align="left" valign="top"><simpara>*uint64</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies a global limit on the number of scraped targets. This setting overrides the <literal>TargetLimit</literal> value set in any user-defined <literal>ServiceMonitor</literal> or <literal>PodMonitor</literal> object if the value is greater than <literal>enforcedSampleLimit</literal>. Administrators can use this setting to keep the overall number of targets under control. The default value is <literal>0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>externalLabels</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines labels to be added to any time series or alerts when communicating with external systems such as federation, remote storage, and Alertmanager. By default, no labels are added.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level setting for Prometheus. The possible values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default setting is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>queryLogFile</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the file to which PromQL queries are logged. This setting can be either a filename, in which case the queries are saved to an <literal>emptyDir</literal> volume at <literal>/var/log/prometheus</literal>, or a full path to a location where an <literal>emptyDir</literal> volume will be mounted and the queries saved. Writing to <literal>/dev/stderr</literal>, <literal>/dev/stdout</literal> or <literal>/dev/null</literal> is supported, but writing to any other <literal>/dev/</literal> path is not supported. Relative paths are also not supported. By default, PromQL queries are not logged.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>remoteWrite</simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xl:href="#remotewritespec">RemoteWriteSpec</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the remote write configuration, including URL, authentication, and relabeling settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the Prometheus container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>retention</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: <literal>[0-9]+(ms|s|m|h|d|w|y)</literal> (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is <literal>15d</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>retentionSize</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the maximum amount of disk space used by data blocks plus the write-ahead log (WAL). Supported values are <literal>B</literal>, <literal>KB</literal>, <literal>KiB</literal>, <literal>MB</literal>, <literal>MiB</literal>, <literal>GB</literal>, <literal>GiB</literal>, <literal>TB</literal>, <literal>TiB</literal>, <literal>PB</literal>, <literal>PiB</literal>, <literal>EB</literal>, and <literal>EiB</literal>. The default value is <literal>nil</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>volumeClaimTemplate</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.EmbeddedPersistentVolumeClaim</simpara></entry>
<entry align="left" valign="top"><simpara>Defines persistent storage for Prometheus. Use this setting to configure the storage class and size of a volume.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_remotewritespec">
<title>RemoteWriteSpec</title>
<section xml:id="_description_26">
<title>Description</title>
<simpara>The <literal>RemoteWriteSpec</literal> resource defines the settings for remote write storage.</simpara>
</section>
<section xml:id="_required_2">
<title>Required</title>
<itemizedlist>
<listitem>
<simpara><literal>url</literal></simpara>
</listitem>
</itemizedlist>
<simpara>Appears in: <link xl:href="#prometheusk8sconfig">PrometheusK8sConfig</link>,
<link xl:href="#prometheusrestrictedconfig">PrometheusRestrictedConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>authorization</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.SafeAuthorization</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the authorization settings for remote write storage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>basicAuth</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.BasicAuth</simpara></entry>
<entry align="left" valign="top"><simpara>Defines Basic authentication settings for the remote write endpoint URL.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>bearerTokenFile</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the file that contains the bearer token for the remote write endpoint. However, because you cannot mount secrets in a pod, in practice you can only reference the token of the service account.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>headers</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Specifies the custom HTTP headers to be sent along with each remote write request. Headers set by Prometheus cannot be overwritten.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>metadataConfig</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.MetadataConfig</simpara></entry>
<entry align="left" valign="top"><simpara>Defines settings for sending series metadata to remote write storage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>name</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the name of the remote write queue. This name is used in metrics and logging to differentiate queues. If specified, this name must be unique.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>oauth2</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.OAuth2</simpara></entry>
<entry align="left" valign="top"><simpara>Defines OAuth2 authentication settings for the remote write endpoint.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>proxyUrl</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines an optional proxy URL.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>queueConfig</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.QueueConfig</simpara></entry>
<entry align="left" valign="top"><simpara>Allows tuning configuration for remote write queue parameters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>remoteTimeout</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the timeout value for requests to the remote write endpoint.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>sendExemplars</simpara></entry>
<entry align="left" valign="top"><simpara>*bool</simpara></entry>
<entry align="left" valign="top"><simpara>Enables sending exemplars via remote write. When enabled, this setting configures Prometheus to store a maximum of 100,000 exemplars in memory.
This setting only applies to user-defined monitoring and is not applicable to core platform monitoring.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>sigv4</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.Sigv4</simpara></entry>
<entry align="left" valign="top"><simpara>Defines AWS Signature Version 4 authentication settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tlsConfig</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.SafeTLSConfig</simpara></entry>
<entry align="left" valign="top"><simpara>Defines TLS authentication settings for the remote write endpoint.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>url</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the URL of the remote write endpoint to which samples will be sent.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>writeRelabelConfigs</simpara></entry>
<entry align="left" valign="top"><simpara>[]monv1.RelabelConfig</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the list of remote write relabel configurations.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_tlsconfig">
<title>TLSConfig</title>
<section xml:id="_description_27">
<title>Description</title>
<simpara>The <literal>TLSConfig</literal> resource configures the settings for TLS connections.</simpara>
</section>
<section xml:id="_required_3">
<title>Required</title>
<itemizedlist>
<listitem>
<simpara><literal>insecureSkipVerify</literal></simpara>
</listitem>
</itemizedlist>
<simpara>Appears in: <link xl:href="#additionalalertmanagerconfig">AdditionalAlertmanagerConfig</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>ca</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.SecretKeySelector</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the secret key reference containing the Certificate Authority (CA) to use for the remote host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>cert</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.SecretKeySelector</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the secret key reference containing the public certificate to use for the remote host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>key</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.SecretKeySelector</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the secret key reference containing the private key to use for the remote host.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>serverName</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Used to verify the hostname on the returned certificate.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>insecureSkipVerify</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, disables the verification of the remote host&#8217;s certificate and name.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_telemeterclientconfig">
<title>TelemeterClientConfig</title>
<section xml:id="_description_28">
<title>Description</title>
<simpara><literal>TelemeterClientConfig</literal> defines settings for the Telemeter Client component.</simpara>
</section>
<section xml:id="_required_4">
<title>Required</title>
<itemizedlist>
<listitem>
<simpara><literal>nodeSelector</literal></simpara>
</listitem>
<listitem>
<simpara><literal>tolerations</literal></simpara>
</listitem>
</itemizedlist>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the <literal>TelemeterClient</literal> container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_thanosquerierconfig">
<title>ThanosQuerierConfig</title>
<section xml:id="_description_29">
<title>Description</title>
<simpara>The <literal>ThanosQuerierConfig</literal> resource defines settings for the Thanos Querier component.</simpara>
<simpara>Appears in: <link xl:href="#clustermonitoringconfiguration">ClusterMonitoringConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>enableRequestLogging</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables or disables request logging. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level setting for Thanos Querier. The possible values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default value is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>enableCORS</simpara></entry>
<entry align="left" valign="top"><simpara>bool</simpara></entry>
<entry align="left" valign="top"><simpara>A Boolean flag that enables setting CORS headers. The headers allow access from any origin. The default value is <literal>false</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the Thanos Querier container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_thanosrulerconfig">
<title>ThanosRulerConfig</title>
<section xml:id="_description_30">
<title>Description</title>
<simpara>The <literal>ThanosRulerConfig</literal> resource defines configuration for the Thanos Ruler instance for user-defined projects.</simpara>
<simpara>Appears in: <link xl:href="#userworkloadconfiguration">UserWorkloadConfiguration</link></simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>additionalAlertmanagerConfigs</simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xl:href="#additionalalertmanagerconfig">AdditionalAlertmanagerConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Configures how the Thanos Ruler component communicates with additional Alertmanager instances. The default value is <literal>nil</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>logLevel</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the log level setting for Thanos Ruler. The possible values are <literal>error</literal>, <literal>warn</literal>, <literal>info</literal>, and <literal>debug</literal>. The default value is <literal>info</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>nodeSelector</simpara></entry>
<entry align="left" valign="top"><simpara>map[string]string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the nodes on which the Pods are scheduled.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>resources</simpara></entry>
<entry align="left" valign="top"><simpara>*v1.ResourceRequirements</simpara></entry>
<entry align="left" valign="top"><simpara>Defines resource requests and limits for the Alertmanager container.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>retention</simpara></entry>
<entry align="left" valign="top"><simpara>string</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: <literal>[0-9]+(ms|s|m|h|d|w|y)</literal> (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is <literal>15d</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>tolerations</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.Toleration</simpara></entry>
<entry align="left" valign="top"><simpara>Defines tolerations for the pods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>topologySpreadConstraints</simpara></entry>
<entry align="left" valign="top"><simpara>[]v1.TopologySpreadConstraint</simpara></entry>
<entry align="left" valign="top"><simpara>Defines the pod&#8217;s topology spread constraints.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>volumeClaimTemplate</simpara></entry>
<entry align="left" valign="top"><simpara>*monv1.EmbeddedPersistentVolumeClaim</simpara></entry>
<entry align="left" valign="top"><simpara>Defines persistent storage for Thanos Ruler. Use this setting to configure the storage class and size of a volume.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="_userworkloadconfiguration">
<title>UserWorkloadConfiguration</title>
<section xml:id="_description_31">
<title>Description</title>
<simpara>The <literal>UserWorkloadConfiguration</literal> resource defines the settings responsible for user-defined projects in the <literal>user-workload-monitoring-config</literal> config map  in the <literal>openshift-user-workload-monitoring</literal> namespace. You can only enable <literal>UserWorkloadConfiguration</literal> after you have set <literal>enableUserWorkload</literal> to <literal>true</literal> in the <literal>cluster-monitoring-config</literal> config map under the <literal>openshift-monitoring</literal> namespace.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>alertmanager</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#alertmanageruserworkloadconfig">AlertmanagerUserWorkloadConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the settings for the Alertmanager component in user workload monitoring.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>prometheus</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#prometheusrestrictedconfig">PrometheusRestrictedConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the settings for the Prometheus component in user workload monitoring.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>prometheusOperator</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#prometheusoperatorconfig">PrometheusOperatorConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the settings for the Prometheus Operator component in user workload monitoring.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>thanosRuler</simpara></entry>
<entry align="left" valign="top"><simpara>*<link xl:href="#thanosrulerconfig">ThanosRulerConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the settings for the Thanos Ruler component in user workload monitoring.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
</chapter>
<chapter xml:id="_cluster_observability_operator">
<title>Cluster Observability Operator</title>
<section xml:id="cluster-observability-operator-release-notes">
<title>Cluster Observability Operator release notes</title>

<important>
<simpara>The Cluster Observability Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xl:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>The Cluster Observability Operator (COO) is an optional {product-title} Operator that enables administrators to create standalone monitoring stacks that are independently configurable for use by different services and users.</simpara>
<simpara>The COO complements the built-in monitoring capabilities of {product-title}. You can deploy it in parallel with the default platform and user workload monitoring stacks managed by the Cluster Monitoring Operator (CMO).</simpara>
<simpara>These release notes track the development of the Cluster Observability Operator in {product-title}.</simpara>
<section xml:id="cluster-observability-operator-release-notes-0-1-1">
<title>Cluster Observability Operator 0.1.1</title>
<simpara>This release updates the Cluster Observability Operator to support installing the Operator in restricted networks or disconnected environments.</simpara>
</section>
<section xml:id="cluster-observability-operator-release-notes-0-1">
<title>Cluster Observability Operator 0.1</title>
<simpara>This release makes a Technology Preview version of the Cluster Observability Operator available on OperatorHub.</simpara>
</section>
</section>
<section xml:id="cluster-observability-operator-overview">
<title>Cluster Observability Operator overview</title>

<important>
<simpara>The Cluster Observability Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xl:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>The Cluster Observability Operator (COO) is an optional component of the {product-title}. You can deploy it to create standalone monitoring stacks that are independently configurable for use by different services and users.</simpara>
<simpara>The COO deploys the following monitoring components:</simpara>
<itemizedlist>
<listitem>
<simpara>Prometheus</simpara>
</listitem>
<listitem>
<simpara>Thanos Querier (optional)</simpara>
</listitem>
<listitem>
<simpara>Alertmanager (optional)</simpara>
</listitem>
</itemizedlist>
<simpara>The COO components function independently of the default in-cluster monitoring stack, which is deployed and managed by the Cluster Monitoring Operator (CMO).
Monitoring stacks deployed by the two Operators do not conflict. You can use a COO monitoring stack in addition to the default platform monitoring components deployed by the CMO.</simpara>
<section xml:id="understanding-the-cluster-observability-operator_cluster_observability_operator_overview">
<title>Understanding the Cluster Observability Operator</title>
<simpara>A default monitoring stack created by the Cluster Observability Operator (COO) includes a highly available Prometheus instance capable of sending metrics to an external endpoint by using remote write.</simpara>
<simpara>Each COO stack also includes an optional Thanos Querier component, which you can use to query a highly available Prometheus instance from a central location, and an optional Alertmanager component, which you can use to set up alert configurations for different services.</simpara>
<section xml:id="advantages-of-using-cluster-observability-operator_cluster_observability_operator_overview">
<title>Advantages of using the Cluster Observability Operator</title>
<simpara>The <literal>MonitoringStack</literal> CRD used by the COO offers an opinionated default monitoring configuration for COO-deployed monitoring components, but you can customize it to suit more complex requirements.</simpara>
<simpara>Deploying a COO-managed monitoring stack can help meet monitoring needs that are difficult or impossible to address by using the core platform monitoring stack deployed by the Cluster Monitoring Operator (CMO).
A monitoring stack deployed using COO has the following advantages over core platform and user workload monitoring:</simpara>
<variablelist>
<varlistentry>
<term>Extendability</term>
<listitem>
<simpara>Users can add more metrics to a COO-deployed monitoring stack, which is not possible with core platform monitoring without losing support.
In addition, COO-managed stacks can receive certain cluster-specific metrics from core platform monitoring by using federation.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Multi-tenancy support</term>
<listitem>
<simpara>The COO can create a monitoring stack per user namespace.
You can also deploy multiple stacks per namespace or a single stack for multiple namespaces.
For example, cluster administrators, SRE teams, and development teams can all deploy their own monitoring stacks on a single cluster, rather than having to use a single shared stack of monitoring components.
Users on different teams can then independently configure features such as separate alerts, alert routing, and alert receivers for their applications and services.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Scalability</term>
<listitem>
<simpara>You can create COO-managed monitoring stacks as needed.
Multiple monitoring stacks can run on a single cluster, which can facilitate the monitoring of very large clusters by using manual sharding. This ability addresses cases where the number of metrics exceeds the monitoring capabilities of a single Prometheus instance.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Flexibility</term>
<listitem>
<simpara>Deploying the COO with Operator Lifecycle Manager (OLM) decouples COO releases from {product-title} release cycles.
This method of deployment enables faster release iterations and the ability to respond rapidly to changing requirements and issues.
Additionally, by deploying a COO-managed monitoring stack, users can manage alerting rules independently of {product-title} release cycles.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Highly customizable</term>
<listitem>
<simpara>The COO can delegate ownership of single configurable fields in custom resources to users by using Server-Side Apply (SSA), which enhances customization.</simpara>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xl:href="https://kubernetes.io/docs/reference/using-api/server-side-apply/">Kubernetes documentation for Server-Side Apply (SSA)</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installing-cluster-observability-operators">
<title>Installing the Cluster Observability Operator</title>

<important>
<simpara>The Cluster Observability Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xl:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>As a cluster administrator, you can install the Cluster Observability Operator (COO) from OperatorHub by using the {product-title} web console or CLI.
OperatorHub is a user interface that works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster.</simpara>
<simpara>To install the COO using OperatorHub, follow the procedure described in <link xl:href="../../operators/admin/olm-adding-operators-to-cluster.xml">Adding Operators to a cluster</link>.</simpara>
<section xml:id="uninstalling-the-cluster-observability-operator-using-the-web-console_installing_the_cluster_observability_operator">
<title>Uninstalling the Cluster Observability Operator using the web console</title>
<simpara>If you have installed the Cluster Observability Operator (COO) by using OperatorHub, you can uninstall it in the {product-title} web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role.</simpara>
</listitem>
<listitem>
<simpara>You have logged in to the {product-title} web console.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Go to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Locate the <emphasis role="strong">Cluster Observability Operator</emphasis> entry in the list.</simpara>
</listitem>
<listitem>
<simpara>Click <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> for this entry and select <emphasis role="strong">Uninstall Operator</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-the-cluster-observability-operator-to-monitor-a-service">
<title>Configuring the Cluster Observability Operator to monitor a service</title>

<important>
<simpara>The Cluster Observability Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xl:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You can monitor metrics for a service by configuring monitoring stacks managed by the Cluster Observability Operator (COO).</simpara>
<simpara>To test monitoring a service, follow these steps:</simpara>
<itemizedlist>
<listitem>
<simpara>Deploy a sample service that defines a service endpoint.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>ServiceMonitor</literal> object that specifies how the service is to be monitored by the COO.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>MonitoringStack</literal> object to discover the <literal>ServiceMonitor</literal> object.</simpara>
</listitem>
</itemizedlist>
<section xml:id="deploying-a-sample-service-for-cluster-observability-operator_configuring_the_cluster_observability_operator_to_monitor_a_service">
<title>Deploying a sample service for Cluster Observability Operator</title>
<simpara>This configuration deploys a sample service named <literal>prometheus-coo-example-app</literal> in the user-defined <literal>ns1-coo</literal> project.
The service exposes the custom <literal>version</literal> metric.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with administrative permissions for the namespace.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file named <literal>prometheus-coo-example-app.yaml</literal> that contains the following configuration details for a namespace, deployment, and service:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: ns1-coo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-coo-example-app
  name: prometheus-coo-example-app
  namespace: ns1-coo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-coo-example-app
  template:
    metadata:
      labels:
        app: prometheus-coo-example-app
    spec:
      containers:
      - image: ghcr.io/rhobs/prometheus-example-app:0.4.2
        imagePullPolicy: IfNotPresent
        name: prometheus-coo-example-app
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus-coo-example-app
  name: prometheus-coo-example-app
  namespace: ns1-coo
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    name: web
  selector:
    app: prometheus-coo-example-app
  type: ClusterIP</programlisting>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
<listitem>
<simpara>Apply the configuration to the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f prometheus-coo-example-app.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the pod is running by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n -ns1-coo get pod</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                      READY     STATUS    RESTARTS   AGE
prometheus-coo-example-app-0927545cb7-anskj   1/1       Running   0          81m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="specifying-how-a-service-is-monitored-by-cluster-observability-operator_configuring_the_cluster_observability_operator_to_monitor_a_service">
<title>Specifying how a service is monitored by Cluster Observability Operator</title>
<simpara>To use the metrics exposed by the sample service you created in the "Deploying a sample service for Cluster Observability Operator" section, you must configure monitoring components to scrape metrics from the <literal>/metrics</literal> endpoint.</simpara>
<simpara>You can create this configuration by using a <literal>ServiceMonitor</literal> object that specifies how the service is to be monitored, or a <literal>PodMonitor</literal> object that specifies how a pod is to be monitored.
The <literal>ServiceMonitor</literal> object requires a <literal>Service</literal> object. The <literal>PodMonitor</literal> object does not, which enables the <literal>MonitoringStack</literal> object to scrape metrics directly from the metrics endpoint exposed by a pod.</simpara>
<simpara>This procedure shows how to create a <literal>ServiceMonitor</literal> object for a sample service named <literal>prometheus-coo-example-app</literal> in the <literal>ns1-coo</literal> namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with administrative permissions for the namespace.</simpara>
</listitem>
<listitem>
<simpara>You have installed the Cluster Observability Operator.</simpara>
</listitem>
<listitem>
<simpara>You have deployed the <literal>prometheus-coo-example-app</literal> sample service in the <literal>ns1-coo</literal> namespace.</simpara>
<note>
<simpara>The <literal>prometheus-coo-example-app</literal> sample service does not support TLS authentication.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file named <literal>example-coo-app-service-monitor.yaml</literal> that contains the following <literal>ServiceMonitor</literal> object configuration details:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.rhobs/v1alpha1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: prometheus-coo-example-monitor
  name: prometheus-coo-example-monitor
  namespace: ns1-coo
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-coo-example-app</programlisting>
<simpara>This configuration defines a <literal>ServiceMonitor</literal> object that the <literal>MonitoringStack</literal> object will reference to scrape the metrics data exposed by the <literal>prometheus-coo-example-app</literal> sample service.</simpara>
</listitem>
<listitem>
<simpara>Apply the configuration to the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-app-service-monitor.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>ServiceMonitor</literal> resource is created by running the following command and observing the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n ns1-coo get servicemonitor</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                         AGE
prometheus-coo-example-monitor   81m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-a-monitoringstack-object-for-cluster-observability-operator_configuring_the_cluster_observability_operator_to_monitor_a_service">
<title>Creating a MonitoringStack object for the Cluster Observability Operator</title>
<simpara>To scrape the metrics data exposed by the target <literal>prometheus-coo-example-app</literal> service, create a <literal>MonitoringStack</literal> object that references the <literal>ServiceMonitor</literal> object you created in the "Specifying how a service is monitored for Cluster Observability Operator" section.
This <literal>MonitoringStack</literal> object can then discover the service and scrape the exposed metrics data from it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or as a user with administrative permissions for the namespace.</simpara>
</listitem>
<listitem>
<simpara>You have installed the Cluster Observability Operator.</simpara>
</listitem>
<listitem>
<simpara>You have deployed the <literal>prometheus-coo-example-app</literal> sample service in the <literal>ns1-coo</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>ServiceMonitor</literal> object named <literal>prometheus-coo-example-monitor</literal> in the <literal>ns1-coo</literal> namespace.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the <literal>MonitoringStack</literal> object configuration. For this example, name the file <literal>example-coo-monitoring-stack.yaml</literal>.</simpara>
</listitem>
<listitem>
<simpara>Add the following <literal>MonitoringStack</literal> object configuration details:</simpara>
<formalpara>
<title>Example <literal>MonitoringStack</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.rhobs/v1alpha1
kind: MonitoringStack
metadata:
  name: example-coo-monitoring-stack
  namespace: ns1-coo
spec:
  logLevel: debug
  retention: 1d
  resourceSelector:
    matchLabels:
      k8s-app: prometheus-coo-example-monitor</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the <literal>MonitoringStack</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-coo-monitoring-stack.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>MonitoringStack</literal> object is available by running the following command and inspecting the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n ns1-coo get monitoringstack</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                         AGE
example-coo-monitoring-stack   81m</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</chapter>
</book>
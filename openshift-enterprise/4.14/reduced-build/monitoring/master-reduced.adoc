= Monitoring

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="monitoring-overview"]
= Monitoring overview
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: monitoring-overview

toc::[]

[id="about-openshift-monitoring"]
== About {product-title} monitoring

[role="_abstract"]
{product-title} includes a preconfigured, preinstalled, and self-updating monitoring stack that provides monitoring for core platform components. You also have the option to xref:enabling-monitoring-for-user-defined-projects[enable monitoring for user-defined projects].

A cluster administrator can xref:configuring-the-monitoring-stack[configure the monitoring stack] with the supported configurations. {product-title} delivers monitoring best practices out of the box.

A set of alerts are included by default that immediately notify administrators about issues with a cluster. Default dashboards in the {product-title} web console include visual representations of cluster metrics to help you to quickly understand the state of your cluster. With the {product-title} web console, you can xref:managing-metrics[view and manage metrics], xref:managing-alerts[alerts], and xref:reviewing-monitoring-dashboards[review monitoring dashboards].

In the *Observe* section of {product-title} web console, you can access and manage monitoring features such as xref:managing-metrics[metrics], xref:managing-alerts[alerts], xref:reviewing-monitoring-dashboards[monitoring dashboards], and xref:getting-detailed-information-about-a-target_managing-metrics[metrics targets].

After installing {product-title}, cluster administrators can optionally enable monitoring for user-defined projects. By using this feature, cluster administrators, developers, and other users can specify how services and pods are monitored in their own projects.
As a cluster administrator, you can find answers to common problems such as user metrics unavailability and high consumption of disk space by Prometheus in xref:troubleshooting-monitoring-issues[Troubleshooting monitoring issues].


// Understanding the monitoring stack
:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/support/virt-openshift-cluster-monitoring.adoc
// * monitoring/monitoring-overview.adoc

// This module uses a conditionalized title so that the module
// can be re-used in associated products but the title is not
// included in the existing OpenShift assembly.

:_mod-docs-content-type: CONCEPT
[id="understanding-the-monitoring-stack_{context}"]
= Understanding the monitoring stack

The {product-title}
monitoring stack is based on the link:https://prometheus.io/[Prometheus] open source project and its wider ecosystem. The monitoring stack includes the following:

* *Default platform monitoring components*.
A set of platform monitoring components are installed in the `openshift-monitoring` project by default during an OpenShift Container Platform installation. This provides monitoring for core cluster components including Kubernetes services. The default monitoring stack also enables remote health monitoring for clusters.
+
These components are illustrated in the *Installed by default* section in the following diagram.

* *Components for monitoring user-defined projects*.
After optionally enabling monitoring for user-defined projects, additional monitoring components are installed in the `openshift-user-workload-monitoring` project. This provides monitoring for user-defined projects.
These components are illustrated in the *User* section in the following diagram.

image:monitoring-architecture.png[{product-title} monitoring architecture]

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/monitoring-overview.adoc

:_mod-docs-content-type: REFERENCE
[id="default-monitoring-components_{context}"]
= Default monitoring components

By default, the {product-title} {product-version} monitoring stack includes these components:

.Default monitoring stack components
[options="header"]
|===

|Component|Description

|Cluster Monitoring Operator
|The Cluster Monitoring Operator (CMO) is a central component of the monitoring stack. It deploys, manages, and automatically updates Prometheus and Alertmanager instances, Thanos Querier, Telemeter Client, and metrics targets. The CMO is deployed by the Cluster Version Operator (CVO).

|Prometheus Operator
|The Prometheus Operator (PO) in the `openshift-monitoring` project creates, configures, and manages platform Prometheus instances and Alertmanager instances. It also automatically generates monitoring target configurations based on Kubernetes label queries.

|Prometheus
|Prometheus is the monitoring system on which the {product-title} monitoring stack is based. Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.

|Prometheus Adapter
|The Prometheus Adapter (PA in the preceding diagram) translates Kubernetes node and pod queries for use in Prometheus. The resource metrics that are translated include CPU and memory utilization metrics. The Prometheus Adapter exposes the cluster resource metrics API for horizontal pod autoscaling. The Prometheus Adapter is also used by the `oc adm top nodes` and `oc adm top pods` commands.

|Alertmanager
|The Alertmanager service handles alerts received from Prometheus. Alertmanager is also responsible for sending the alerts to external notification systems.

|kube-state-metrics agent
|The kube-state-metrics exporter agent (KSM in the preceding diagram) converts Kubernetes objects to metrics that Prometheus can use.

|monitoring-plugin
|The monitoring-plugin dynamic plugin component deploys the monitoring pages in the *Observe* section of the {product-title} web console.
You can use Cluster Monitoring Operator (CMO) config map settings to manage monitoring-plugin resources for the web console pages.

|openshift-state-metrics agent
|The openshift-state-metrics exporter (OSM in the preceding diagram) expands upon kube-state-metrics by adding metrics for {product-title}-specific resources.

|node-exporter agent
|The node-exporter agent (NE in the preceding diagram) collects metrics about every node in a cluster. The node-exporter agent is deployed on every node.

|Thanos Querier
|Thanos Querier aggregates and optionally deduplicates core {product-title} metrics and metrics for user-defined projects under a single, multi-tenant interface.

|Telemeter Client
|Telemeter Client sends a subsection of the data from platform Prometheus instances to Red Hat to facilitate Remote Health Monitoring for clusters.

|===

All of the components in the monitoring stack are monitored by the stack and are automatically updated when {product-title} is updated.

[NOTE]
====
All components of the monitoring stack use the TLS security profile settings that are centrally configured by a cluster administrator.
If you configure a monitoring stack component that uses TLS security settings, the component uses the TLS security profile settings that already exist in the `tlsSecurityProfile` field in the global {product-title} `apiservers.config.openshift.io/cluster` resource.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/monitoring-overview.adoc

:_mod-docs-content-type: REFERENCE
[id="default-monitoring-targets_{context}"]
= Default monitoring targets

In addition to the components of the stack itself, the default monitoring stack monitors:


* CoreDNS
* Elasticsearch (if Logging is installed)
* etcd
* Fluentd (if Logging is installed)
* HAProxy
* Image registry
* Kubelets
* Kubernetes API server
* Kubernetes controller manager
* Kubernetes scheduler
* OpenShift API server
* OpenShift Controller Manager
* Operator Lifecycle Manager (OLM)
* Vector (if Logging is installed)

[NOTE]
====
Each {product-title} component is responsible for its monitoring configuration. For problems with the monitoring of an {product-title} component, open a
link:https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&summary=Monitoring_issue&issuetype=1&priority=10200&versions=12385624[Jira issue] against that component, not against the general monitoring component.
====

Other {product-title} framework components might be exposing metrics as well. For details, see their respective documentation.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/monitoring-overview.adoc

:_mod-docs-content-type: REFERENCE
[id="components-for-monitoring-user-defined-projects_{context}"]
= Components for monitoring user-defined projects

{product-title}
{product-version}
includes an optional enhancement to the monitoring stack that enables you to monitor services and pods in user-defined projects. This feature includes the following components:

.Components for monitoring user-defined projects
[options="header"]
|===

|Component|Description

|Prometheus Operator
|The Prometheus Operator (PO) in the `openshift-user-workload-monitoring` project creates, configures, and manages Prometheus and Thanos Ruler instances in the same project.

|Prometheus
|Prometheus is the monitoring system through which monitoring is provided for user-defined projects. Prometheus sends alerts to Alertmanager for processing.

|Thanos Ruler
|The Thanos Ruler is a rule evaluation engine for Prometheus that is deployed as a separate process. In {product-title}
{product-version}
, Thanos Ruler provides rule and alerting evaluation for the monitoring of user-defined projects.

|Alertmanager
|The Alertmanager service handles alerts received from Prometheus and Thanos Ruler. Alertmanager is also responsible for sending user-defined alerts to external notification systems. Deploying this service is optional.

|===

[NOTE]
====
The components in the preceding table are deployed after monitoring is enabled for user-defined projects.
====

All of these components are monitored by the stack and are automatically updated when {product-title} is updated.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/monitoring-overview.adoc

:_mod-docs-content-type: CONCEPT
[id="monitoring-targets-for-user-defined-projects_{context}"]
= Monitoring targets for user-defined projects

When monitoring is enabled for user-defined projects, you can monitor:


* Metrics provided through service endpoints in user-defined projects.
* Pods running in user-defined projects.

:leveloffset: 1
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/monitoring-overview.adoc

:_mod-docs-content-type: REFERENCE
[id="openshift-monitoring-common-terms_{context}"]
= Glossary of common terms for {product-title} monitoring

This glossary defines common terms that are used in {product-title} architecture.

Alertmanager::
Alertmanager handles alerts received from Prometheus. Alertmanager is also responsible for sending the alerts to external notification systems.

Alerting rules::
Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.

Cluster Monitoring Operator::
The Cluster Monitoring Operator (CMO) is a central component of the monitoring stack. It deploys and manages Prometheus instances such as, the Thanos Querier, the Telemeter Client, and metrics targets to ensure that they are up to date. The CMO is deployed by the Cluster Version Operator (CVO).

Cluster Version Operator::
The Cluster Version Operator (CVO) manages the lifecycle of cluster Operators, many of which are installed in {product-title} by default.

config map::
A config map provides a way to inject configuration data into pods. You can reference the data stored in a config map in a volume of type `ConfigMap`. Applications running in a pod can use this data.

Container::
A container is a lightweight and executable image that includes software and all its dependencies. Containers virtualize the operating system. As a result, you can run containers anywhere from a data center to a public or private cloud as well as a developer’s laptop.

custom resource (CR)::
A CR is an extension of the Kubernetes API. You can create custom resources.

etcd::
etcd is the key-value store for {product-title}, which stores the state of all resource objects.

Fluentd::
Fluentd gathers logs from nodes and feeds them to Elasticsearch.

Kubelets::
Runs on nodes and reads the container manifests. Ensures that the defined containers have started and are running.

Kubernetes API server::
Kubernetes API server validates and configures data for the API objects.

Kubernetes controller manager::
Kubernetes controller manager governs the state of the cluster.

Kubernetes scheduler::
Kubernetes scheduler allocates pods to nodes.

labels::
Labels are key-value pairs that you can use to organize and select subsets of objects such as a pod.

node::
A worker machine in the {product-title} cluster. A node is either a virtual machine (VM) or a physical machine.

Operator::
The preferred method of packaging, deploying, and managing a Kubernetes application in an {product-title} cluster. An Operator takes human operational knowledge and encodes it into software that is packaged and shared with customers.

Operator Lifecycle Manager (OLM)::
OLM helps you install, update, and manage the lifecycle of Kubernetes native applications. OLM is an open source toolkit designed to manage Operators in an effective, automated, and scalable way.

Persistent storage::
Stores the data even after the device is shut down. Kubernetes uses persistent volumes to store the application data.

Persistent volume claim (PVC)::
You can use a PVC to mount a PersistentVolume into a Pod. You can access the storage without knowing the details of the cloud environment.

pod::
The pod is the smallest logical unit in Kubernetes. A pod is comprised of one or more containers to run in a worker node.

Prometheus::
Prometheus is the monitoring system on which the {product-title} monitoring stack is based. Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.

Prometheus adapter::
The Prometheus Adapter translates Kubernetes node and pod queries for use in Prometheus. The resource metrics that are translated include CPU and memory utilization. The Prometheus Adapter exposes the cluster resource metrics API for horizontal pod autoscaling.

Prometheus Operator::
The Prometheus Operator (PO) in the `openshift-monitoring` project creates, configures, and manages platform Prometheus and Alertmanager instances. It also automatically generates monitoring target configurations based on Kubernetes label queries.

Silences::
A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.

storage::
{product-title} supports many types of storage, both for on-premise and cloud providers.
You can manage container storage for persistent and non-persistent data in an {product-title} cluster.

Thanos Ruler::
The Thanos Ruler is a rule evaluation engine for Prometheus that is deployed as a separate process. In {product-title}, Thanos Ruler provides rule and alerting evaluation for the monitoring of user-defined projects.

web console::
A user interface (UI) to manage {product-title}.

:leveloffset: 1

[role="_additional-resources"]
[id="additional-resources_monitoring-overview"]
== Additional resources

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#about-remote-health-monitoring[About remote health monitoring]
* xref:granting-users-permission-to-monitor-user-defined-projects_enabling-monitoring-for-user-defined-projects[Granting users permission to monitor user-defined projects]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#tls-security-profiles[Configuring TLS security profiles]

[id="next-steps_monitoring-overview"]
== Next steps

* xref:configuring-the-monitoring-stack[Configuring the monitoring stack]


:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="configuring-the-monitoring-stack"]
= Configuring the monitoring stack
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: configuring-the-monitoring-stack

toc::[]

The {product-title} 4 installation program provides only a low number of configuration options before installation. Configuring most {product-title} framework components, including the cluster monitoring stack, happens postinstallation.

This section explains what configuration is supported,
shows how to configure the monitoring stack,
and demonstrates several common configuration scenarios.

== Prerequisites

* The monitoring stack imposes additional resource requirements. Consult the computing resources recommendations in link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#scaling-cluster-monitoring-operator[Scaling the Cluster Monitoring Operator] and verify that you have sufficient resources.

// Maintenance and support for monitoring
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

[id="maintenance-and-support_{context}"]
= Maintenance and support for monitoring

The supported way of configuring {product-title} Monitoring is by configuring it using the options described in this document. *Do not use other configurations, as they are unsupported.* Configuration paradigms might change across Prometheus releases, and such cases can only be handled gracefully if all configuration possibilities are controlled. If you use configurations other than those described in this section, your changes will disappear because the `cluster-monitoring-operator` reconciles any differences. The Operator resets everything to the defined state by default and by design.


:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="support-considerations_{context}"]
= Support considerations for monitoring

The following modifications are explicitly not supported:

* *Creating additional `ServiceMonitor`, `PodMonitor`, and `PrometheusRule` objects in the `openshift-&#42;` and `kube-&#42;` projects.*
* *Modifying any resources or objects deployed in the `openshift-monitoring` or `openshift-user-workload-monitoring` projects.* The resources created by the {product-title} monitoring stack are not meant to be used by any other resources, as there are no guarantees about their backward compatibility.
+
[NOTE]
====
The Alertmanager configuration is deployed as a secret resource in the `openshift-monitoring` namespace.
If you have enabled a separate Alertmanager instance for user-defined alert routing, an Alertmanager configuration is also deployed as a secret resource in the `openshift-user-workload-monitoring` namespace.
To configure additional routes for any instance of Alertmanager, you need to decode, modify, and then encode that secret.
This procedure is a supported exception to the preceding statement.
====
+
* *Modifying resources of the stack.* The {product-title} monitoring stack ensures its resources are always in the state it expects them to be. If they are modified, the stack will reset them.
* *Deploying user-defined workloads to `openshift-&#42;`, and `kube-&#42;` projects.* These projects are reserved for Red Hat provided components and they should not be used for user-defined workloads.
* *Enabling symptom based monitoring by using the `Probe` custom resource definition (CRD) in Prometheus Operator.*

[NOTE]
====
Backward compatibility for metrics, recording rules, or alerting rules is not guaranteed.
====

* *Installing custom Prometheus instances on {product-title}.* A custom instance is a Prometheus custom resource (CR) managed by the Prometheus Operator.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="unmanaged-monitoring-operators_{context}"]
= Support policy for monitoring Operators

Monitoring Operators ensure that {product-title} monitoring resources function as designed and tested. If Cluster Version Operator (CVO) control of an Operator is overridden, the Operator does not respond to configuration changes, reconcile the intended state of cluster objects, or receive updates.

While overriding CVO control for an Operator can be helpful during debugging, this is  unsupported and the cluster administrator assumes full control of the individual component configurations and upgrades.

.Overriding the Cluster Version Operator

The `spec.overrides` parameter can be added to the configuration for the CVO to allow administrators to provide a list of overrides to the behavior of the CVO for a component. Setting the `spec.overrides[].unmanaged` parameter to `true` for a component blocks cluster upgrades and alerts the administrator after a CVO override has been set:

[source,terminal]
----
Disabling ownership via cluster version overrides prevents upgrades. Please remove overrides before continuing.
----

[WARNING]
====
Setting a CVO override puts the entire cluster in an unsupported state and prevents the monitoring stack from being reconciled to its intended state. This impacts the reliability features built into Operators and prevents updates from being received. Reported issues must be reproduced after removing any overrides for support to proceed.
====

:leveloffset: 1

// Preparing to configure the monitoring stack
[id="preparing-to-configure-the-monitoring-stack"]
== Preparing to configure the monitoring stack

You can configure the monitoring stack by creating and updating monitoring config maps.

:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-cluster-monitoring-configmap_{context}"]
= Creating a cluster monitoring config map

To configure core {product-title} monitoring components, you must create the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project.

[NOTE]
====
When you save your changes to the `cluster-monitoring-config` `ConfigMap` object, some or all of the pods in the `openshift-monitoring` project might be redeployed. It can sometimes take a while for these components to redeploy.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Check whether the `cluster-monitoring-config` `ConfigMap` object exists:
+
[source,terminal]
----
$ oc -n openshift-monitoring get configmap cluster-monitoring-config
----

. If the `ConfigMap` object does not exist:
.. Create the following YAML manifest. In this example the file is called `cluster-monitoring-config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
----
+
.. Apply the configuration to create the `ConfigMap` object:
+
[source,terminal]
----
$ oc apply -f cluster-monitoring-config.yaml
----

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-user-defined-workload-monitoring-configmap_{context}"]
= Creating a user-defined workload monitoring config map

To configure the components that monitor user-defined projects, you must create the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project.

[NOTE]
====
When you save your changes to the `user-workload-monitoring-config` `ConfigMap` object, some or all of the pods in the `openshift-user-workload-monitoring` project might be redeployed. It can sometimes take a while for these components to redeploy. You can create and configure the config map before you first enable monitoring for user-defined projects, to prevent having to redeploy the pods often.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Check whether the `user-workload-monitoring-config` `ConfigMap` object exists:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get configmap user-workload-monitoring-config
----

. If the `user-workload-monitoring-config` `ConfigMap` object does not exist:
.. Create the following YAML manifest. In this example the file is called `user-workload-monitoring-config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
----
+
.. Apply the configuration to create the `ConfigMap` object:
+
[source,terminal]
----
$ oc apply -f user-workload-monitoring-config.yaml
----
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]

// Configuring the monitoring stack
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-the-monitoring-stack_{context}"]
= Configuring the monitoring stack

In {product-title} {product-version}, you can configure the monitoring stack using the `cluster-monitoring-config` or `user-workload-monitoring-config` `ConfigMap` objects. Config maps configure the Cluster Monitoring Operator (CMO), which in turn configures the components of the stack.

.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object.
** *To configure core {product-title} monitoring components*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add your configuration under `data/config.yaml` as a key-value pair `<component_name>:{nbsp}<component_configuration>`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>:
      <configuration_for_the_component>
----
+
Substitute `<component>` and `<configuration_for_the_component>` accordingly.
+
The following example `ConfigMap` object configures a persistent volume claim (PVC) for Prometheus. This relates to the Prometheus instance that monitors core {product-title} components only:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s: <1>
      volumeClaimTemplate:
        spec:
          storageClassName: fast
          volumeMode: Filesystem
          resources:
            requests:
              storage: 40Gi
----
<1> Defines the Prometheus component and the subsequent lines define its configuration.

** *To configure components that monitor user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add your configuration under `data/config.yaml` as a key-value pair `<component_name>:{nbsp}<component_configuration>`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>:
      <configuration_for_the_component>
----
+
Substitute `<component>` and `<configuration_for_the_component>` accordingly.
+
The following example `ConfigMap` object configures a data retention period and minimum container resource requests for Prometheus. This relates to the Prometheus instance that monitors user-defined projects only:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus: <1>
      retention: 24h <2>
      resources:
        requests:
          cpu: 200m <3>
          memory: 2Gi <4>
----
<1> Defines the Prometheus component and the subsequent lines define its configuration.
<2> Configures a twenty-four hour data retention period for the Prometheus instance that monitors user-defined projects.
<3> Defines a minimum resource request of 200 millicores for the Prometheus container.
<4> Defines a minimum pod resource request of 2 GiB of memory for the Prometheus container.
+
[NOTE]
====
The Prometheus config map component is called `prometheusK8s` in the `cluster-monitoring-config` `ConfigMap` object and `prometheus` in the `user-workload-monitoring-config` `ConfigMap` object.
====

. Save the file to apply the changes to the `ConfigMap` object. The pods affected by the new configuration are restarted automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* Configuration reference for the xref:clustermonitoringconfiguration[`cluster-monitoring-config`] config map
* Configuration reference for the xref:userworkloadconfiguration[`user-workload-monitoring-config`] config map
* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps
* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]

// Configurable monitoring components
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

[id="configurable-monitoring-components_{context}"]
= Configurable monitoring components

This table shows the monitoring components you can configure and the keys used to specify the components in the
`cluster-monitoring-config` and
`user-workload-monitoring-config` `ConfigMap` objects.


.Configurable monitoring components
[options="header"]
|====
|Component |cluster-monitoring-config config map key |user-workload-monitoring-config config map key
|Prometheus Operator |`prometheusOperator` |`prometheusOperator`
|Prometheus |`prometheusK8s` |`prometheus`
|Alertmanager |`alertmanagerMain` | `alertmanager`
|kube-state-metrics |`kubeStateMetrics` |
|monitoring-plugin | `monitoringPlugin` |
|openshift-state-metrics |`openshiftStateMetrics` |
|Telemeter Client |`telemeterClient` |
|Prometheus Adapter |`k8sPrometheusAdapter` |
|Thanos Querier |`thanosQuerier` |
|Thanos Ruler | |`thanosRuler`
|====

[NOTE]
====
The Prometheus key is called `prometheusK8s` in the `cluster-monitoring-config` `ConfigMap` object and `prometheus` in the `user-workload-monitoring-config` `ConfigMap` object.
====


:leveloffset: 1

// Moving monitoring components to different nodes
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="using-node-selectors-to-move-monitoring-components_{context}"]
= Using node selectors to move monitoring components

By using the `nodeSelector` constraint with labeled nodes, you can move any of the monitoring stack components to specific nodes.
By doing so, you can control the placement and distribution of the monitoring components across a cluster.

By controlling placement and distribution of monitoring components, you can optimize system resource use, improve performance, and segregate workloads based on specific requirements or policies.

[id="how-node-selectors-work-with-other-constraints_{context}"]
== How node selectors work with other constraints


If you move monitoring components by using node selector constraints, be aware that other constraints to control pod scheduling might exist for a cluster:

* Topology spread constraints might be in place to control pod placement.
* Hard anti-affinity rules are in place for Prometheus, Thanos Querier, Alertmanager, and other monitoring components to ensure that multiple pods for these components are always spread across different nodes and are therefore always highly available.

When scheduling pods onto nodes, the pod scheduler tries to satisfy all existing constraints when determining pod placement.
That is, all constraints compound when the pod scheduler determines which pods will be placed on which nodes.

Therefore, if you configure a node selector constraint but existing constraints cannot all be satisfied, the pod scheduler cannot match all constraints and will not schedule a pod for placement onto a node.

To maintain resilience and high availability for monitoring components, ensure that enough nodes are available and match all constraints when you configure a node selector constraint to move a component.

:leveloffset: 1
[role="_additional-resources"]
.Additional resources
// The nodes topics may apply to OSD/ROSA when that content is ported from OCP.
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working[Understanding how to update labels on nodes]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-selectors[Placing pods on specific nodes using node selectors]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#[Placing pods relative to other pods using affinity and anti-affinity rules]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#[Controlling pod placement by using pod topology spread constraints]
* xref:configuring_pod_topology_spread_constraintsfor_monitoring_configuring-the-monitoring-stack[Configuring pod topology spread constraints for monitoring]
* link:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector[Kubernetes documentation about node selectors]

:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="moving-monitoring-components-to-different-nodes_{context}"]
= Moving monitoring components to different nodes

To specify the nodes in your cluster on which monitoring stack components will run, configure the `nodeSelector` constraint in the component's `ConfigMap` object to match labels assigned to the nodes.

[NOTE]
====
You cannot add a node selector constraint directly to an existing scheduled pod.
====


.Prerequisites
* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. If you have not done so yet, add a label to the nodes on which you want to run the monitoring components:
+
[source,terminal]
----
$ oc label nodes <node-name> <node-label>
----
. Edit the `ConfigMap` object:
** *To move a component that monitors core {product-title} projects*:

.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Specify the node labels for the `nodeSelector` constraint for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>: <1>
      nodeSelector:
        <node-label-1> <2>
        <node-label-2> <3>
        <...>
----
<1> Substitute `<component>` with the appropriate monitoring stack component name.
<2> Substitute `<node-label-1>` with the label you added to the node.
<3> Optional: Specify additional labels.
If you specify additional labels, the pods for the component are only scheduled on the nodes that contain all of the specified labels.
+
[NOTE]
====
If monitoring components remain in a `Pending` state after configuring the `nodeSelector` constraint, check the pod events for errors relating to taints and tolerations.
====

** *To move a component that monitors user-defined projects*:

.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Specify the node labels for the `nodeSelector` constraint for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>: <1>
      nodeSelector:
        <node-label-1> <2>
        <node-label-2> <3>
        <...>
----
<1> Substitute `<component>` with the appropriate monitoring stack component name.
<2> Substitute `<node-label-1>` with the label you added to the node.
<3> Optional: Specify additional labels.
If you specify additional labels, the pods for the component are only scheduled on the nodes that contain all of the specified labels.
+
[NOTE]
====
If monitoring components remain in a `Pending` state after configuring the `nodeSelector` constraint, check the pod events for errors relating to taints and tolerations.
====

. Save the file to apply the changes.
The components specified in the new configuration are moved to the new nodes automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When you save changes to a monitoring config map, the pods and other resources in the project might be redeployed.
The running monitoring processes in that project might also restart.
====

:leveloffset: 1


[role="_additional-resources"]
.Additional resources

* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps
* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]
// This xref might be relevant for ROSA/OSD if the Node content is reused:
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-nodes-working-updating_nodes-nodes-working[Understanding how to update labels on nodes]
// This xref might be relevant for ROSA/OSD if the Node content is reused:
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-selectors[Placing pods on specific nodes using node selectors]
* See the link:https://kubernetes.io/docs/concepts/configuration/assign-pod-node/#nodeselector[Kubernetes documentation] for details on the `nodeSelector` constraint

// Assigning tolerations to monitoring components
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="assigning-tolerations-to-monitoring-components_{context}"]
= Assigning tolerations to monitoring components

You can assign tolerations to any of the monitoring stack components to enable moving them to tainted nodes.


.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object:
** *To assign tolerations to a component that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Specify `tolerations` for the component:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>:
      tolerations:
        <toleration_specification>
----
+
Substitute `<component>` and `<toleration_specification>` accordingly.
+
For example, `oc adm taint nodes node1 key1=value1:NoSchedule` adds a taint to `node1` with the key `key1` and the value `value1`. This prevents monitoring components from deploying pods on `node1` unless a toleration is configured for that taint. The following example configures the `alertmanagerMain` component to tolerate the example taint:
+
[source,yaml,subs=quotes]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      tolerations:
      - key: "key1"
        operator: "Equal"
        value: "value1"
        effect: "NoSchedule"
----

** *To assign tolerations to a component that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Specify `tolerations` for the component:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>:
      tolerations:
        <toleration_specification>
----
+
Substitute `<component>` and `<toleration_specification>` accordingly.
+
For example, `oc adm taint nodes node1 key1=value1:NoSchedule` adds a taint to `node1` with the key `key1` and the value `value1`. This prevents monitoring components from deploying pods on `node1` unless a toleration is configured for that taint. The following example configures the `thanosRuler` component to tolerate the example taint:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      tolerations:
      - key: "key1"
        operator: "Equal"
        value: "value1"
        effect: "NoSchedule"
----

. Save the file to apply the changes. The new component placement configuration is applied automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps
* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]
// This xref might be relevant for ROSA/OSD if the Node content is reused:
* See the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-taints-tolerations[{product-title} documentation] on taints and tolerations
* See the link:https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/[Kubernetes documentation] on taints and tolerations

// Setting the body size limit for metrics scraping
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-the-body-size-limit-for-metrics-scraping_{context}"]
= Setting the body size limit for metrics scraping

By default, no limit exists for the uncompressed body size for data returned from scraped metrics targets.
You can set a body size limit to help avoid situations in which Prometheus consumes excessive amounts of memory when scraped targets return a response that contains a large amount of data.
In addition, by setting a body size limit, you can reduce the impact that a malicious target might have on Prometheus and on the cluster as a whole.

After you set a value for `enforcedBodySizeLimit`, the alert `PrometheusScrapeBodySizeLimitHit` fires when at least one Prometheus scrape target replies with a response body larger than the configured value.

[NOTE]
====
If metrics data scraped from a target has an uncompressed body size exceeding the configured size limit, the scrape fails.
Prometheus then considers this target to be down and sets its `up` metric value to `0`, which can trigger the `TargetDown` alert.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` namespace:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add a value for `enforcedBodySizeLimit` to `data/config.yaml/prometheusK8s` to limit the body size that can be accepted per target scrape:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |-
    prometheusK8s:
      enforcedBodySizeLimit: 40MB <1>
----
<1> Specify the maximum body size for scraped metrics targets.
This `enforcedBodySizeLimit` example limits the uncompressed size per target scrape to 40 megabytes.
Valid numeric values use the Prometheus data size format: B (bytes), KB (kilobytes), MB (megabytes), GB (gigabytes), TB (terabytes), PB (petabytes), and EB (exabytes).
The default value is `0`, which specifies no limit.
You can also set the value to `automatic` to calculate the limit automatically based on cluster capacity.

. Save the file to apply the changes automatically.
+
[WARNING]
====
When you save changes to a `cluster-monitoring-config` config map, the pods and other resources in the `openshift-monitoring` project might be redeployed.
The running monitoring processes in that project might also restart.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/#scrape_config[Prometheus scrape configuration documentation]

// Configuring limits and resource requests for monitoring components

[id="managing-cpu-and-memory-resources-for-monitoring-components"]
== Managing CPU and memory resources for monitoring components

You can ensure that the containers that run monitoring components have enough CPU and memory resources by specifying values for resource limits and requests for those components.

You can configure these limits and requests for core platform monitoring components in the `openshift-monitoring` namespace and for the components that monitor user-defined projects in the `openshift-user-workload-monitoring` namespace.

:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="about-specifying-limits-and-requests-for-monitoring-components_{context}"]
= About specifying limits and requests for monitoring components

You can configure resource limits and request settings for core platform monitoring components and for the components that monitor user-defined projects, including the following components:

* Alertmanager (for core platform monitoring and for user-defined projects)
* kube-state-metrics
* monitoring-plugin
* node-exporter
* openshift-state-metrics
* Prometheus (for core platform monitoring and for user-defined projects)
* Prometheus Adapter
* Prometheus Operator and its admission webhook service
* Telemeter Client
* Thanos Querier
* Thanos Ruler

By defining resource limits, you limit a container's resource usage, which prevents the container from exceeding the specified maximum values for CPU and memory resources.

By defining resource requests, you specify that a container can be scheduled only on a node that has enough CPU and memory resources available to match the requested resources.



:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="specifying-limits-and-resource-requests-for-monitoring-components_{context}"]
= Specifying limits and requests for monitoring components

To configure CPU and memory resources, specify values for resource limits and requests in the appropriate `ConfigMap` object for the namespace in which the monitoring component is located:

* The `cluster-monitoring-config` config map in the `openshift-monitoring` namespace for core platform monitoring
* The `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` namespace for components that monitor user-defined projects

.Prerequisites

* *If you are configuring core platform monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created a `ConfigMap` object named `cluster-monitoring-config`.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. To configure core platform monitoring components, edit the `cluster-monitoring-config` config map object in the `openshift-monitoring` namespace:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add values to define resource limits and requests for each core platform monitoring component you want to configure.
+
[IMPORTANT]
====
Make sure that the value set for a limit is always higher than the value set for a request.
Otherwise, an error will occur, and the container will not run.
====
+
.Example
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusK8s:
      resources:
        limits:
          cpu: 500m
          memory: 3Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusOperator:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    k8sPrometheusAdapter:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    kubeStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    telemeterClient:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    openshiftStateMetrics:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    thanosQuerier:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    nodeExporter:
      resources:
        limits:
          cpu: 50m
          memory: 150Mi
        requests:
          cpu: 20m
          memory: 50Mi
    monitoringPlugin:
      resources:
        limits:
          cpu: 500m
          memory: 1Gi
        requests:
          cpu: 200m
          memory: 500Mi
    prometheusOperatorAdmissionWebhook:
      resources:
        limits:
          cpu: 50m
          memory: 100Mi
        requests:
          cpu: 20m
          memory: 50Mi
----

. Save the file to apply the changes automatically.
+
[IMPORTANT]
====
When you save changes to the `cluster-monitoring-config` config map, the pods and other resources in the `openshift-monitoring` project might be redeployed.
The running monitoring processes in that project might also restart.
====


:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* link:https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/#requests-and-limits[Kubernetes requests and limits documentation]

// Configuring persistent storage
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="configuring_persistent_storage_{context}"]
= Configuring persistent storage

Running cluster monitoring with persistent storage means that your metrics are stored to a persistent volume (PV) and can survive a pod being restarted or recreated. This is ideal if you require your metrics or alerting data to be guarded from data loss. For production environments, it is highly recommended to configure persistent storage. Because of the high IO demands, it is advantageous to use local storage.

[id="persistent-storage-prerequisites"]
== Persistent storage prerequisites


* Dedicate sufficient local persistent storage to ensure that the disk does not become full. How much storage you need depends on the number of pods.

* Verify that you have a persistent volume (PV) ready to be claimed by the persistent volume claim (PVC), one PV for each replica. Because Prometheus and Alertmanager both have two replicas, you need four PVs to support the entire monitoring stack. The PVs are available from the Local Storage Operator, but not if you have enabled dynamically provisioned storage.

* Use `Filesystem` as the storage type value for the `volumeMode` parameter when you configure the persistent volume.
+
[NOTE]
====
If you use a local volume for persistent storage, do not use a raw block volume, which is described with `volumeMode: Block` in the `LocalVolume` object. Prometheus cannot use raw block volumes.
====
+
[IMPORTANT]
====
Prometheus does not support file systems that are not POSIX compliant.
For example, some NFS file system implementations are not POSIX compliant.
If you want to use an NFS file system for storage, verify with the vendor that their NFS implementation is fully POSIX compliant.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-a-local-persistent-volume-claim_{context}"]
= Configuring a local persistent volume claim

For monitoring components to use a persistent volume (PV), you must configure a persistent volume claim (PVC).

.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object:
** *To configure a PVC for a component that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add your PVC configuration for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>:
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class>
          resources:
            requests:
              storage: <amount_of_storage>
----
+
See the link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[Kubernetes documentation on PersistentVolumeClaims] for information on how to specify `volumeClaimTemplate`.
+
The following example configures a PVC that claims local persistent storage for the Prometheus instance that monitors core {product-title} components:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi
----
+
In the above example, the storage class created by the Local Storage Operator is called `local-storage`.
+
The following example configures a PVC that claims local persistent storage for Alertmanager:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 10Gi
----

** *To configure a PVC for a component that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add your PVC configuration for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>:
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class>
          resources:
            requests:
              storage: <amount_of_storage>
----
+
See the link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/#persistentvolumeclaims[Kubernetes documentation on PersistentVolumeClaims] for information on how to specify `volumeClaimTemplate`.
+
The following example configures a PVC that claims
local
persistent storage for the Prometheus instance that monitors user-defined projects:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi
----
+
In the above example, the storage class created by the Local Storage Operator is called `local-storage`.
+
The following example configures a PVC that claims
local
persistent storage for Thanos Ruler:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 10Gi
----
+
[NOTE]
====
Storage requirements for the `thanosRuler` component depend on the number of rules that are evaluated and how many samples each rule generates.
====

. Save the file to apply the changes. The pods affected by the new configuration are restarted automatically and the new storage configuration is applied.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="resizing-a-persistent-storage-volume_{context}"]
= Resizing a persistent storage volume

{product-title} does not support resizing an existing persistent storage volume used by `StatefulSet` resources, even if the underlying `StorageClass` resource used supports persistent volume sizing.
Therefore, even if you update the `storage` field for an existing persistent volume claim (PVC) with a larger size, this setting will not be propagated to the associated persistent volume (PV).

However, resizing a PV is still possible by using a manual process. If you want to resize a PV for a monitoring component such as Prometheus, Thanos Ruler, or Alertmanager, you can update the appropriate config map in which the component is configured. Then, patch the PVC, and delete and orphan the pods.
Orphaning the pods recreates the `StatefulSet` resource immediately and automatically updates the size of the volumes mounted in the pods with the new PVC settings.
No service disruption occurs during this process.

.Prerequisites

* You have installed the OpenShift CLI (`oc`).
* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
** You have configured at least one PVC for core {product-title} monitoring components.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
** You have configured at least one PVC for components that monitor user-defined projects.

.Procedure

. Edit the `ConfigMap` object:
** *To resize a PVC for a component that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add a new storage size for the PVC configuration for the component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>: <1>
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class> <2>
          resources:
            requests:
              storage: <amount_of_storage> <3>
----
<1> Specify the core monitoring component.
<2> Specify the storage class.
<3> Specify the new size for the storage volume.
+
The following example configures a PVC that sets the local persistent storage to 100 gigabytes for the Prometheus instance that monitors core {product-title} components:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 100Gi
----
+
The following example configures a PVC that sets the local persistent storage for Alertmanager to 40 gigabytes:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 40Gi
----

** *To resize a PVC for a component that monitors user-defined projects*:
+
[NOTE]
====
You can resize the volumes for the Thanos Ruler and Prometheus instances that monitor user-defined projects.
====
+
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Update the PVC configuration for the monitoring component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>: <1>
      volumeClaimTemplate:
        spec:
          storageClassName: <storage_class> <2>
          resources:
            requests:
              storage: <amount_of_storage> <3>
----
<1> Specify the core monitoring component.
<2> Specify the storage class.
<3> Specify the new size for the storage volume.
+
The following example configures the PVC size to 100 gigabytes for the Prometheus instance that monitors user-defined projects:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 100Gi
----
+
The following example sets the PVC size to 20 gigabytes for Thanos Ruler:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      volumeClaimTemplate:
        spec:
          storageClassName: local-storage
          resources:
            requests:
              storage: 20Gi
----
+
[NOTE]
====
Storage requirements for the `thanosRuler` component depend on the number of rules that are evaluated and how many samples each rule generates.
====

. Save the file to apply the changes. The pods affected by the new configuration restart automatically.
+
[WARNING]
====
When you save changes to a monitoring config map, the pods and other resources in the related project might be redeployed. The monitoring processes running in that project might also be restarted.
====

. Manually patch every PVC with the updated storage request. The following example resizes the storage size for the Prometheus component in the `openshift-monitoring` namespace to 100Gi:
+
[source,terminal]
----
$ for p in $(oc -n openshift-monitoring get pvc -l app.kubernetes.io/name=prometheus -o jsonpath='{range .items[*]}{.metadata.name} {end}'); do \
  oc -n openshift-monitoring patch pvc/${p} --patch '{"spec": {"resources": {"requests": {"storage":"100Gi"}}}}'; \
  done

----

. Delete the underlying StatefulSet with the `--cascade=orphan` parameter:
+
[source,terminal]
----
$ oc delete statefulset -l app.kubernetes.io/name=prometheus --cascade=orphan
----

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="modifying-retention-time-and-size-for-prometheus-metrics-data_{context}"]
= Modifying the retention time and size for Prometheus metrics data

By default, Prometheus automatically retains metrics data for 11 days. You can modify the retention time for
Prometheus
to change how soon the data is deleted. You can also set the maximum amount of disk space the retained metrics data uses. If the data reaches this size limit, Prometheus deletes the oldest data first until the disk space used is again below the limit.

Note the following behaviors of these data retention settings:

* The size-based retention policy applies to all data block directories in the `/prometheus` directory, including persistent blocks, write-ahead log (WAL) data, and m-mapped chunks.
* Data in the `/wal` and `/head_chunks` directories counts toward the retention size limit, but Prometheus never purges data from these directories based on size- or time-based retention policies.
Thus, if you set a retention size limit lower than the maximum size set for the `/wal` and `/head_chunks` directories, you have configured the system not to retain any data blocks in the `/prometheus` data directories.
* The size-based retention policy is applied only when Prometheus cuts a new data block, which occurs every two hours after the WAL contains at least three hours of data.
* If you do not explicitly define values for either `retention` or `retentionSize`, retention time defaults to 11 days, and retention size is not set.
* If you define values for both `retention` and `retentionSize`, both values apply.
If any data blocks exceed the defined retention time or the defined size limit, Prometheus purges these data blocks.
* If you define a value for `retentionSize` and do not define `retention`, only the `retentionSize` value applies.
* If you do not define a value for `retentionSize` and only define a value for `retention`, only the `retention` value applies.

.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** A cluster administrator has enabled monitoring for user-defined projects.
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object:
** *To modify the retention time and size for the Prometheus instance that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add the retention time and size configuration under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: <time_specification> <1>
      retentionSize: <size_specification> <2>
----
+
<1> The retention time: a number directly followed by `ms` (milliseconds), `s` (seconds), `m` (minutes), `h` (hours), `d` (days), `w` (weeks), or `y` (years). You can also combine time values for specific times, such as `1h30m15s`.
<2> The retention size: a number directly followed by `B` (bytes), `KB` (kilobytes), `MB` (megabytes), `GB` (gigabytes), `TB` (terabytes), `PB` (petabytes), and `EB` (exabytes).
+
The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance that monitors core {product-title} components:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      retention: 24h
      retentionSize: 10GB
----

** *To modify the retention time and size for the Prometheus instance that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add the retention time and size configuration under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: <time_specification> <1>
      retentionSize: <size_specification> <2>
----
+
<1> The retention time: a number directly followed by `ms` (milliseconds), `s` (seconds), `m` (minutes), `h` (hours), `d` (days), `w` (weeks), or `y` (years).
You can also combine time values for specific times, such as `1h30m15s`.
<2> The retention size: a number directly followed by `B` (bytes), `KB` (kilobytes), `MB` (megabytes), `GB` (gigabytes), `TB` (terabytes), `PB` (petabytes), or `EB` (exabytes).
+
The following example sets the retention time to 24 hours and the retention size to 10 gigabytes for the Prometheus instance that monitors user-defined projects:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      retention: 24h
      retentionSize: 10GB
----

. Save the file to apply the changes. The pods affected by the new configuration restart automatically.
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="modifying-the-retention-time-for-thanos-ruler-metrics-data_{context}"]
= Modifying the retention time for Thanos Ruler metrics data

By default, for user-defined projects, Thanos Ruler automatically retains metrics data for 24 hours. You can modify the retention time to change how long this data is retained by specifying a time value in the `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` namespace.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
* A cluster administrator has enabled monitoring for user-defined projects.
* You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

. Add the retention time configuration under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      retention: <time_specification> <1>
----
+
<1> Specify the retention time in the following format: a number directly followed by `ms` (milliseconds), `s` (seconds), `m` (minutes), `h` (hours), `d` (days), `w` (weeks), or `y` (years).
You can also combine time values for specific times, such as `1h30m15s`.
The default is `24h`.
+
The following example sets the retention time to 10 days for Thanos Ruler data:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      retention: 10d
----

. Save the file to apply the changes. The pods affected by the new configuration automatically restart.
+
[WARNING]
====
Saving changes to a monitoring config map might restart monitoring processes and redeploy the pods and other resources in the related project.
The running monitoring processes in that project might also restart.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* xref:creating-cluster-monitoring-configmap_configuring-the-monitoring-stack[Creating a cluster monitoring config map]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#prometheus-database-storage-requirements_cluster-monitoring-operator[Prometheus database storage requirements]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#optimizing-storage[Recommended configurable storage technology]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#understanding-persistent-storage[Understanding persistent storage]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#optimizing-storage[Optimizing storage]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-using-local-volume[Configure local persistent storage]
* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]

// Configuring remote write storage for Prometheus
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring_remote_write_storage_{context}"]
= Configuring remote write storage

[role="_abstract"]
You can configure remote write storage to enable Prometheus to send ingested metrics to remote systems for long-term storage. Doing so has no impact on how or for how long Prometheus stores metrics.

.Prerequisites

* *If you are configuring core {product-title} monitoring components:*
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects:*
** You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).
* You have set up a remote write compatible endpoint (such as Thanos) and know the endpoint URL. See the link:https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage[Prometheus remote endpoints and storage documentation] for information about endpoints that are compatible with the remote write feature.
* You have set up authentication credentials in a `Secret` object for the remote write endpoint. You must create the secret in the
same namespace as the Prometheus object for which you configure remote write: the `openshift-monitoring` namespace for default platform monitoring or the `openshift-user-workload-monitoring` namespace for user workload monitoring.

+
[CAUTION]
====
To reduce security risks, use HTTPS and authentication to send metrics to an endpoint.
====

.Procedure

. Edit the `ConfigMap` object:
** *To configure remote write for the Prometheus instance that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add a `remoteWrite:` section under `data/config.yaml/prometheusK8s`.

.. Add an endpoint URL and authentication credentials in this section:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com" <1>
        <endpoint_authentication_credentials> <2>
----
+
<1> The URL of the remote write endpoint.
<2> The authentication method and credentials for the endpoint.
Currently supported authentication methods are AWS Signature Version 4, authentication using HTTP in an `Authorization` request header, Basic authentication, OAuth 2.0, and TLS client.
See _Supported remote write authentication settings_ for sample configurations of supported authentication methods.

.. Add write relabel configuration values after the authentication credentials:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        <endpoint_authentication_credentials>
        <write_relabel_configs> <1>
----
<1> The write relabel configuration settings.
+
For `<write_relabel_configs>` substitute a list of write relabel configurations for metrics that you want to send to the remote endpoint.
+
The following sample shows how to forward a single metric called `my_metric`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: 'my_metric'
          action: keep

----
+
See the link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config[Prometheus relabel_config documentation] for information about write relabel configuration options.

** *To configure remote write for the Prometheus instance that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add a `remoteWrite:` section under `data/config.yaml/prometheus`.

.. Add an endpoint URL and authentication credentials in this section:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com" <1>
        <endpoint_authentication_credentials> <2>
----
+
<1> The URL of the remote write endpoint.
<2> The authentication method and credentials for the endpoint.
Currently supported authentication methods are AWS Signature Version 4, authentication using HTTP an `Authorization` request header, basic authentication, OAuth 2.0, and TLS client.
See _Supported remote write authentication settings_ below for sample configurations of supported authentication methods.

.. Add write relabel configuration values after the authentication credentials:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        <endpoint_authentication_credentials>
        <write_relabel_configs> <1>
----
<1> The write relabel configuration settings.
+
For `<write_relabel_configs>` substitute a list of write relabel configurations for metrics that you want to send to the remote endpoint.
+
The following sample shows how to forward a single metric called `my_metric`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels: [__name__]
          regex: 'my_metric'
          action: keep

----
+
See the link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config[Prometheus relabel_config documentation] for information about write relabel configuration options.

. Save the file to apply the changes. The pods affected by the new configuration restart automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
Saving changes to a monitoring `ConfigMap` object might redeploy the pods and other resources in the related project. Saving changes might also restart the running monitoring processes in that project.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: REFERENCE
[id="supported_remote_write_authentication_settings_{context}"]
= Supported remote write authentication settings

You can use different methods to authenticate with a remote write endpoint. Currently supported authentication methods are AWS Signature Version 4, basic authentication, authorization, OAuth 2.0, and TLS client. The following table provides details about supported authentication methods for use with remote write.

[options="header"]
|===

|Authentication method|Config map field|Description

|AWS Signature Version 4|`sigv4`|This method uses AWS Signature Version 4 authentication to sign requests.
You cannot use this method simultaneously with authorization, OAuth 2.0, or Basic authentication.

|Basic authentication|`basicAuth`|Basic authentication sets the authorization header on every remote write request with the configured username and password.

|authorization|`authorization`|Authorization sets the `Authorization` header on every remote write request using the configured token.

|OAuth 2.0|`oauth2`|An OAuth 2.0 configuration uses the client credentials grant type.
Prometheus fetches an access token from `tokenUrl` with the specified client ID and client secret to access the remote write endpoint.
You cannot use this method simultaneously with authorization, AWS Signature Version 4, or Basic authentication.

|TLS client|`tlsConfig`|A TLS client configuration specifies the CA certificate, the client certificate, and the client key file information used to authenticate with the remote write endpoint server using TLS.
The sample configuration assumes that you have already created a CA certificate file, a client certificate file, and a client key file.

|===

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: REFERENCE
[id="example-remote-write-authentication-settings_{context}"]
= Example remote write authentication settings

// Set attributes to distinguish between cluster monitoring examples and user workload monitoring examples.
:configmap-name: cluster-monitoring-config
:namespace-name: openshift-monitoring
:prometheus-instance: prometheusK8s

The following samples show different authentication settings you can use to connect to a remote write endpoint. Each sample also shows how to configure a corresponding `Secret` object that contains authentication credentials and other relevant settings. Each sample configures authentication for use with
default platform monitoring
in the `{namespace-name}` namespace.

.Sample YAML for AWS Signature Version 4 authentication
====
The following shows the settings for a `sigv4` secret named `sigv4-credentials` in the `{namespace-name}` namespace.

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: Secret
metadata:
  name: sigv4-credentials
  namespace: {namespace-name}
stringData:
  accessKey: <AWS_access_key> <1>
  secretKey: <AWS_secret_key> <2>
type: Opaque
----
<1> The AWS API access key.
<2> The AWS API secret key.

The following shows sample AWS Signature Version 4 remote write authentication settings that use a `Secret` object named `sigv4-credentials` in the `{namespace-name}` namespace:

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {prometheus-instance}:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        sigv4:
          region: <AWS_region> <1>
          accessKey:
            name: sigv4-credentials <2>
            key: accessKey <3>
          secretKey:
            name: sigv4-credentials <2>
            key: secretKey <4>
          profile: <AWS_profile_name> <5>
          roleArn: <AWS_role_arn> <6>
----
<1> The AWS region.
<2> The name of the `Secret` object containing the AWS API access credentials.
<3> The key that contains the AWS API access key in the specified `Secret` object.
<4> The key that contains the AWS API secret key in the specified `Secret` object.
<5> The name of the AWS profile that is being used to authenticate.
<6> The unique identifier for the Amazon Resource Name (ARN) assigned to your role.
====

.Sample YAML for basic authentication
====
The following shows sample basic authentication settings for a `Secret` object named `rw-basic-auth` in the `{namespace-name}` namespace:

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: Secret
metadata:
  name: rw-basic-auth
  namespace: {namespace-name}
stringData:
  user: <basic_username> <1>
  password: <basic_password> <2>
type: Opaque
----
<1> The username.
<2> The password.

The following sample shows a `basicAuth` remote write configuration that uses a `Secret` object named `rw-basic-auth` in the `{namespace-name}` namespace.
It assumes that you have already set up authentication credentials for the endpoint.

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {prometheus-instance}:
      remoteWrite:
      - url: "https://basicauth.example.com/api/write"
        basicAuth:
          username:
            name: rw-basic-auth <1>
            key: user <2>
          password:
            name: rw-basic-auth <1>
            key: password <3>
----
<1> The name of the `Secret` object that contains the authentication credentials.
<2> The key that contains the username  in the specified `Secret` object.
<3> The key that contains the password in the specified `Secret` object.
====

.Sample YAML for authentication with a bearer token using a `Secret` Object
====
The following shows bearer token settings for a `Secret` object named `rw-bearer-auth` in the `{namespace-name}` namespace:

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: Secret
metadata:
  name: rw-bearer-auth
  namespace: {namespace-name}
stringData:
  token: <authentication_token> <1>
type: Opaque
----
<1> The authentication token.

The following shows sample bearer token config map settings that use a `Secret` object named `rw-bearer-auth` in the `{namespace-name}` namespace:

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    enableUserWorkload: true
    {prometheus-instance}:
      remoteWrite:
      - url: "https://authorization.example.com/api/write"
        authorization:
          type: Bearer <1>
          credentials:
            name: rw-bearer-auth <2>
            key: token <3>
----
<1> The authentication type of the request. The default value is `Bearer`.
<2> The name of the `Secret` object that contains the authentication credentials.
<3> The key that contains the authentication token in the specified `Secret` object.
====

.Sample YAML for OAuth 2.0 authentication
====
The following shows sample OAuth 2.0 settings for a `Secret` object named `oauth2-credentials` in the `{namespace-name}` namespace:

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: Secret
metadata:
  name: oauth2-credentials
  namespace: {namespace-name}
stringData:
  id: <oauth2_id> <1>
  secret: <oauth2_secret> <2>
  token: <oauth2_authentication_token> <3>
type: Opaque
----
<1> The Oauth 2.0 ID.
<2> The OAuth 2.0 secret.
<3> The OAuth 2.0 token.

The following shows an `oauth2` remote write authentication sample configuration that uses a `Secret` object named `oauth2-credentials` in the `{namespace-name}` namespace:

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {prometheus-instance}:
      remoteWrite:
      - url: "https://test.example.com/api/write"
        oauth2:
          clientId:
            secret:
              name: oauth2-credentials <1>
              key: id <2>
          clientSecret:
            name: oauth2-credentials <1>
            key: secret <2>
          tokenUrl: https://example.com/oauth2/token <3>
          scopes: <4>
          - <scope_1>
          - <scope_2>
          endpointParams: <5>
            param1: <parameter_1>
            param2: <parameter_2>
----
<1> The name of the corresponding `Secret` object. Note that `ClientId` can alternatively refer to a `ConfigMap` object, although `clientSecret` must refer to a `Secret` object.
<2> The key that contains the OAuth 2.0 credentials in the specified `Secret` object.
<3> The URL used to fetch a token with the specified `clientId` and `clientSecret`.
<4> The OAuth 2.0 scopes for the authorization request. These scopes limit what data the tokens can access.
<5> The OAuth 2.0 authorization request parameters required for the authorization server.
====

.Sample YAML for TLS client authentication
====
The following shows sample TLS client settings for a `tls` `Secret` object named `mtls-bundle` in the `{namespace-name}` namespace.

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: Secret
metadata:
  name: mtls-bundle
  namespace: {namespace-name}
data:
  ca.crt: <ca_cert> <1>
  client.crt: <client_cert> <2>
  client.key: <client_key> <3>
type: tls
----
<1> The CA certificate in the Prometheus container with which to validate the server certificate.
<2> The client certificate for authentication with the server.
<3> The client key.

The following sample shows a `tlsConfig` remote write authentication configuration that uses a TLS `Secret` object named `mtls-bundle`.

[source,yaml,subs="attributes+"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: {configmap-name}
  namespace: {namespace-name}
data:
  config.yaml: |
    {prometheus-instance}:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        tlsConfig:
          ca:
            secret:
              name: mtls-bundle <1>
              key: ca.crt <2>
          cert:
            secret:
              name: mtls-bundle <1>
              key: client.crt <3>
          keySecret:
            name: mtls-bundle <1>
            key: client.key <4>
----
<1> The name of the corresponding `Secret` object that contains the TLS authentication credentials. Note that `ca` and `cert` can alternatively refer to a `ConfigMap` object, though `keySecret` must refer to a `Secret` object.
<2> The key in the specified `Secret` object that contains the CA certificate for the endpoint.
<3> The key in the specified `Secret` object that contains the client certificate for the endpoint.
<4> The key in the specified `Secret` object that contains the client key secret.
====

// Unset the source code block attributes just to be safe.
:!namespace-name:
:!prometheus-instance:

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See link:https://prometheus.io/docs/operating/integrations/#remote-endpoints-and-storage[Setting up remote write compatible endpoints] for steps to create a remote write compatible endpoint (such as Thanos).
* See link:https://prometheus.io/docs/practices/remote_write/#remote-write-tuning[Tuning remote write settings] for information about how to optimize remote write settings for different use cases.
// This xref might be relevant for ROSA/OSD if this content is reused:
* See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-pods-secrets-about_nodes-pods-secrets[Understanding secrets] for steps to create and configure `Secret` objects in {product-title}.
* See the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#spec-remotewrite-2[Prometheus REST API reference for remote write] for information about additional optional fields.

// Configuring labels for outgoing metrics
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="adding-cluster-id-labels-to-metrics_{context}"]
= Adding cluster ID labels to metrics

If you manage multiple {product-title} clusters and use the remote write feature to send metrics data from these clusters to an external storage location, you can add cluster ID labels to identify the metrics data coming from different clusters. You can then query these labels to identify the source cluster for a metric and distinguish that data from similar metrics data sent by other clusters.

This way, if you manage many clusters for multiple customers and send metrics data to a single centralized storage system, you can use cluster ID labels to query metrics for a particular cluster or customer.

Creating and using cluster ID labels involves three general steps:

* Configuring the write relabel settings for remote write storage.

* Adding cluster ID labels to the metrics.

* Querying these labels to identify the source cluster or customer for a metric.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-cluster-id-labels-for-metrics_{context}"]
= Creating cluster ID labels for metrics

You can create cluster ID labels for metrics for default platform monitoring and for user workload monitoring.

For default platform monitoring, you add cluster ID labels for metrics in the `write_relabel` settings for remote write storage in the `cluster-monitoring-config` config map in the `openshift-monitoring` namespace.

For user workload monitoring, you edit the settings in the `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` namespace.

[NOTE]
====
When Prometheus scrapes user workload targets that expose a `namespace` label, the system stores this label as `exported_namespace`.
This behavior ensures that the final namespace label value is equal to the namespace of the target pod.
You cannot override this default configuration by setting the value of the `honorLabels` field to `true` for `PodMonitor` or `ServiceMonitor` objects.
====



.Prerequisites

* *If you are configuring default platform monitoring components:*
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects:*
** You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).
* You have configured remote write storage.

.Procedure

. Edit the `ConfigMap` object:
** *To create cluster ID labels for core {product-title} metrics:*
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. In the `writeRelabelConfigs:` section under `data/config.yaml/prometheusK8s/remoteWrite`, add cluster ID relabel configuration values:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        <endpoint_authentication_credentials>
        writeRelabelConfigs: <1>
          - <relabel_config> <2>
----
<1> Add a list of write relabel configurations for metrics that you want to send to the remote endpoint.
<2> Substitute the label configuration for the metrics sent to the remote write endpoint.
+
The following sample shows how to forward a metric with the cluster ID label `cluster_id` in default platform monitoring:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__ <1>
          targetLabel: cluster_id <2>
          action: replace <3>
----
<1> The system initially applies a temporary cluster ID source label named `+++__tmp_openshift_cluster_id__+++`. This temporary label gets replaced by the cluster ID label name that you specify.
<2> Specify the name of the cluster ID label for metrics sent to remote write storage.
If you use a label name that already exists for a metric, that value is overwritten with the name of this cluster ID label.
For the label name, do not use `+++__tmp_openshift_cluster_id__+++`. The final relabeling step removes labels that use this name.
<3> The `replace` write relabel action replaces the temporary label with the target label for outgoing metrics.
This action is the default and is applied if no action is specified.

** *To create cluster ID labels for user-defined project metrics:*
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. In the `writeRelabelConfigs:` section under `data/config.yaml/prometheus/remoteWrite`, add cluster ID relabel configuration values:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        <endpoint_authentication_credentials>
        writeRelabelConfigs: <1>
          - <relabel_config> <2>
----
<1> Add a list of write relabel configurations for metrics that you want to send to the remote endpoint.
<2> Substitute the label configuration for the metrics sent to the remote write endpoint.
+
The following sample shows how to forward a metric with the cluster ID label `cluster_id` in user-workload monitoring:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      remoteWrite:
      - url: "https://remote-write-endpoint.example.com"
        writeRelabelConfigs:
        - sourceLabels:
          - __tmp_openshift_cluster_id__ <1>
          targetLabel: cluster_id <2>
          action: replace <3>
----
<1> The system initially applies a temporary cluster ID source label named `+++__tmp_openshift_cluster_id__+++`. This temporary label gets replaced by the cluster ID label name that you specify.
<2> Specify the name of the cluster ID label for metrics sent to remote write storage. If you use a label name that already exists for a metric, that value is overwritten with the name of this cluster ID label. For the label name, do not use `+++__tmp_openshift_cluster_id__+++`. The final relabeling step removes labels that use this name.
<3> The `replace` write relabel action replaces the temporary label with the target label for outgoing metrics. This action is the default and is applied if no action is specified.

. Save the file to apply the changes to the `ConfigMap` object.
The pods affected by the updated configuration automatically restart.
+
[WARNING]
====
Saving changes to a monitoring `ConfigMap` object might redeploy the pods and other resources in the related project. Saving changes might also restart the running monitoring processes in that project.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* For details about write relabel configuration, see xref:configuring_remote_write_storage_configuring-the-monitoring-stack[Configuring remote write storage].

// Configuring metrics collection profiles
// TP features are excluded from OSD and ROSA. When this feature is GA, it can be included in the OSD/ROSA docs.
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="configuring-metrics-collection-profiles_{context}"]
= Configuring metrics collection profiles

[IMPORTANT]
====
[subs="attributes+"]
Using a metrics collection profile is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete.
Red Hat does not recommend using them in production.
These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview[https://access.redhat.com/support/offerings/techpreview].
====

By default, Prometheus collects metrics exposed by all default metrics targets in {product-title} components.
However, you might want Prometheus to collect fewer metrics from a cluster in certain scenarios:

* If cluster administrators require only alert, telemetry, and console metrics and do not require other metrics to be available.
* If a cluster increases in size, and the increased size of the default metrics data collected now requires a significant increase in CPU and memory resources.

You can use a metrics collection profile to collect either the default amount of metrics data or a minimal amount of metrics data.
When you collect minimal metrics data, basic monitoring features such as alerting continue to work.
At the same time, the CPU and memory resources required by Prometheus decrease.

[id="about-metrics-collection-profiles_{context}"]
== About metrics collection profiles

You can enable one of two metrics collection profiles:

* *full*: Prometheus collects metrics data exposed by all platform components. This setting is the default.
* *minimal*: Prometheus collects only the metrics data required for platform alerts, recording rules, telemetry, and console dashboards.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="choosing-a-metrics-collection-profile_{context}"]
= Choosing a metrics collection profile

To choose a metrics collection profile for core {product-title} monitoring components, edit the `cluster-monitoring-config` `ConfigMap` object.

.Prerequisites

* You have installed the OpenShift CLI (`oc`).
* You have enabled Technology Preview features by using the `FeatureGate` custom resource (CR).
* You have created the `cluster-monitoring-config` `ConfigMap` object.
* You have access to the cluster as a user with the `cluster-admin` cluster role.

[WARNING]
====
Saving changes to a monitoring config map might restart monitoring processes and redeploy the pods and other resources in the related project.
The running monitoring processes in that project might also restart.
====

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add the metrics collection profile setting under `data/config.yaml/prometheusK8s`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      collectionProfile: <metrics_collection_profile_name> <1>
----
+
<1> The name of the metrics collection profile.
The available values are `full` or `minimal`.
If you do not specify a value or if the `collectionProfile` key name does not exist in the config map, the default setting of `full` is used.
+
The following example sets the metrics collection profile to `minimal` for the core platform instance of Prometheus:
+
[source,yaml,subs=quotes]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      collectionProfile: *minimal*
----

. Save the file to apply the changes. The pods affected by the new configuration restart automatically.

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See xref:viewing-a-list-of-available-metrics_managing-metrics[Viewing a list of available metrics] for steps to view a list of metrics being collected for a cluster.
* See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#[Enabling features using feature gates] for steps to enable Technology Preview features.

// Managing scrape sample limits for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="controlling-the-impact-of-unbound-attributes-in-user-defined-projects_{context}"]
= Controlling the impact of unbound metrics attributes in user-defined projects

Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a `customer_id` attribute is unbound because it has an infinite number of possible values.

Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.

Cluster administrators
can use the following measures to control the impact of unbound metrics attributes in user-defined projects:

* Limit the number of samples that can be accepted per target scrape in user-defined projects
* Limit the number of scraped labels, the length of label names, and the length of label values
* Create alerts that fire when a scrape sample threshold is reached or when the target cannot be scraped

[NOTE]
====
Limiting scrape samples can help prevent the issues caused by adding many unbound attributes to labels. Developers can also prevent the underlying cause by limiting the number of unbound attributes that they define for metrics. Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-scrape-sample-and-label-limits-for-user-defined-projects_{context}"]
= Setting scrape sample and label limits for user-defined projects

You can limit the number of samples that can be accepted per target scrape in user-defined projects. You can also limit the number of scraped labels, the length of label names, and the length of label values.

[WARNING]
====
If you set sample or label limits, no further sample data is ingested for that target scrape after the limit is reached.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
* You have enabled monitoring for user-defined projects.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

. Add the `enforcedSampleLimit` configuration to `data/config.yaml` to limit the number of samples that can be accepted per target scrape in user-defined projects:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      enforcedSampleLimit: 50000 <1>
----
<1> A value is required if this parameter is specified. This `enforcedSampleLimit` example limits the number of samples that can be accepted per target scrape in user-defined projects to 50,000.

. Add the `enforcedLabelLimit`, `enforcedLabelNameLengthLimit`, and `enforcedLabelValueLengthLimit` configurations to `data/config.yaml` to limit the number of scraped labels, the length of label names, and the length of label values in user-defined projects:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      enforcedLabelLimit: 500 <1>
      enforcedLabelNameLengthLimit: 50 <2>
      enforcedLabelValueLengthLimit: 600 <3>
----
<1> Specifies the maximum number of labels per scrape.
The default value is `0`, which specifies no limit.
<2> Specifies the maximum length in characters of a label name.
The default value is `0`, which specifies no limit.
<3> Specifies the maximum length in characters of a label value.
The default value is `0`, which specifies no limit.

. Save the file to apply the changes. The limits are applied automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to the `user-workload-monitoring-config` `ConfigMap` object, the pods and other resources in the `openshift-user-workload-monitoring` project might be redeployed. The running monitoring processes in that project might also be restarted.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-scrape-sample-alerts_{context}"]
= Creating scrape sample alerts

You can create alerts that notify you when:

* The target cannot be scraped or is not available for the specified `for` duration
* A scrape sample threshold is reached or is exceeded for the specified `for` duration

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
* You have enabled monitoring for user-defined projects.
* You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have limited the number of samples that can be accepted per target scrape in user-defined projects, by using `enforcedSampleLimit`.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Create a YAML file with alerts that inform you when the targets are down and when the enforced sample limit is approaching. The file in this example is called `monitoring-stack-alerts.yaml`:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  labels:
    prometheus: k8s
    role: alert-rules
  name: monitoring-stack-alerts <1>
  namespace: ns1 <2>
spec:
  groups:
  - name: general.rules
    rules:
    - alert: TargetDown <3>
      annotations:
        message: '{{ printf "%.4g" $value }}% of the {{ $labels.job }}/{{ $labels.service
          }} targets in {{ $labels.namespace }} namespace are down.' <4>
      expr: 100 * (count(up == 0) BY (job, namespace, service) / count(up) BY (job,
        namespace, service)) > 10
      for: 10m <5>
      labels:
        severity: warning <6>
    - alert: ApproachingEnforcedSamplesLimit <7>
      annotations:
        message: '{{ $labels.container }} container of the {{ $labels.pod }} pod in the {{ $labels.namespace }} namespace consumes {{ $value | humanizePercentage }} of the samples limit budget.' <8>
      expr: scrape_samples_scraped/50000 > 0.8 <9>
      for: 10m <10>
      labels:
        severity: warning <11>
----
<1> Defines the name of the alerting rule.
<2> Specifies the user-defined project where the alerting rule will be deployed.
<3> The `TargetDown` alert will fire if the target cannot be scraped or is not available for the `for` duration.
<4> The message that will be output when the `TargetDown` alert fires.
<5> The conditions for the `TargetDown` alert must be true for this duration before the alert is fired.
<6> Defines the severity for the `TargetDown` alert.
<7> The `ApproachingEnforcedSamplesLimit` alert will fire when the defined scrape sample threshold is reached or exceeded for the specified `for` duration.
<8> The message that will be output when the `ApproachingEnforcedSamplesLimit` alert fires.
<9> The threshold for the `ApproachingEnforcedSamplesLimit` alert. In this example the alert will fire when the number of samples per target scrape has exceeded 80% of the enforced sample limit of `50000`. The `for` duration must also have passed before the alert will fire. The `<number>` in the expression `scrape_samples_scraped/<number> > <threshold>` must match the `enforcedSampleLimit` value defined in the `user-workload-monitoring-config` `ConfigMap` object.
<10> The conditions for the `ApproachingEnforcedSamplesLimit` alert must be true for this duration before the alert is fired.
<11> Defines the severity for the `ApproachingEnforcedSamplesLimit` alert.

. Apply the configuration to the user-defined project:
+
[source,terminal]
----
$ oc apply -f monitoring-stack-alerts.yaml
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* xref:creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack[Creating a user-defined workload monitoring config map]
* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]
* See xref:determining-why-prometheus-is-consuming-disk-space_troubleshooting-monitoring-issues[Determining why Prometheus is consuming a lot of disk space] for steps to query which metrics have the highest number of scrape samples.

//Configuring external alertmanagers
:leveloffset: 1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="monitoring-configuring-external-alertmanagers_{context}"]
= Configuring external Alertmanager instances

The {product-title} monitoring stack includes a local Alertmanager instance that routes alerts from Prometheus.
You can add external Alertmanager instances to route alerts for core {product-title} projects or user-defined projects.

If you add the same external Alertmanager configuration for multiple clusters and disable the local instance for each cluster, you can then manage alert routing for multiple clusters by using a single external Alertmanager instance.

.Prerequisites

* *If you are configuring core {product-title} monitoring components in the `openshift-monitoring` project*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` config map.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` config map.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object.
** *To configure additional Alertmanagers for routing alerts from core {product-title} projects*:
.. Edit the `cluster-monitoring-config` config map in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add an `additionalAlertmanagerConfigs:` section under `data/config.yaml/prometheusK8s`.

.. Add the configuration details for additional Alertmanagers in this section:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      additionalAlertmanagerConfigs:
      - <alertmanager_specification>
----
+
For `<alertmanager_specification>`, substitute authentication and other configuration details for additional Alertmanager instances.
Currently supported authentication methods are bearer token (`bearerToken`) and client TLS (`tlsConfig`).
The following sample config map configures an additional Alertmanager using a bearer token with client TLS authentication:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      additionalAlertmanagerConfigs:
      - scheme: https
        pathPrefix: /
        timeout: "30s"
        apiVersion: v1
        bearerToken:
          name: alertmanager-bearer-token
          key: token
        tlsConfig:
          key:
            name: alertmanager-tls
            key: tls.key
          cert:
            name: alertmanager-tls
            key: tls.crt
          ca:
            name: alertmanager-tls
            key: tls.ca
        staticConfigs:
        - external-alertmanager1-remote.com
        - external-alertmanager1-remote2.com
----

** *To configure additional Alertmanager instances for routing alerts from user-defined projects*:

.. Edit the `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add a `<component>/additionalAlertmanagerConfigs:` section under `data/config.yaml/`.

.. Add the configuration details for additional Alertmanagers in this section:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>:
      additionalAlertmanagerConfigs:
      - <alertmanager_specification>
----
+
For `<component>`, substitute one of two supported external Alertmanager components: `prometheus` or `thanosRuler`.
+
For `<alertmanager_specification>`, substitute authentication and other configuration details for additional Alertmanager instances. Currently supported authentication methods are bearer token (`bearerToken`) and client TLS (`tlsConfig`). The following sample config map configures an additional Alertmanager using Thanos Ruler with a bearer token and client TLS authentication:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      additionalAlertmanagerConfigs:
      - scheme: https
        pathPrefix: /
        timeout: "30s"
        apiVersion: v1
        bearerToken:
          name: alertmanager-bearer-token
          key: token
        tlsConfig:
          key:
            name: alertmanager-tls
            key: tls.key
          cert:
            name: alertmanager-tls
            key: tls.crt
          ca:
            name: alertmanager-tls
            key: tls.ca
        staticConfigs:
        - external-alertmanager1-remote.com
        - external-alertmanager1-remote2.com
----

. Save the file to apply the changes to the `ConfigMap` object. The new component placement configuration is applied automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====

. Save the file to apply the changes to the `ConfigMap` object. The new component placement configuration is applied automatically.



:leveloffset: 1

//Configuring secrets for Alertmanager
:leveloffset: 1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="monitoring-configuring-secrets-for-alertmanager_{context}"]
= Configuring secrets for Alertmanager

The {product-title} monitoring stack includes Alertmanager, which routes alerts from Prometheus to endpoint receivers.
If you need to authenticate with a receiver so that Alertmanager can send alerts to it, you can configure Alertmanager to use a secret that contains authentication credentials for the receiver.

For example, you can configure Alertmanager to use a secret to authenticate with an endpoint receiver that requires a certificate issued by a private Certificate Authority (CA).
You can also configure Alertmanager to use a secret to authenticate with a receiver that requires a password file for Basic HTTP authentication.
In either case, authentication details are contained in the `Secret` object rather than in the `ConfigMap` object.

:leveloffset: 1
:leveloffset: 2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="monitoring-adding-a-secret-to-the-alertmanager-configuration_{context}"]
= Adding a secret to the Alertmanager configuration

You can add secrets to the Alertmanager configuration for core platform monitoring components by editing the `cluster-monitoring-config` config map in the `openshift-monitoring` project.

After you add a secret to the config map, the secret is mounted as a volume at `/etc/alertmanager/secrets/<secret_name>` within the `alertmanager` container for the Alertmanager pods.

.Prerequisites

* *If you are configuring core {product-title} monitoring components in the `openshift-monitoring` project*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` config map.
** You have created the secret to be configured in Alertmanager in the `openshift-monitoring` project.
* *If you are configuring components that monitor user-defined projects*:
** A cluster administrator has enabled monitoring for user-defined projects.
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the secret to be configured in Alertmanager in the `openshift-user-workload-monitoring` project.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object.
** *To add a secret configuration to Alertmanager for core platform monitoring*:
.. Edit the `cluster-monitoring-config` config map in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add a `secrets:` section under `data/config.yaml/alertmanagerMain` with the following configuration:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      secrets: <1>
      - <secret_name_1> <2>
      - <secret_name_2>
----
<1> This section contains the secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object.
<2> The name of the `Secret` object that contains authentication credentials for the receiver. If you add multiple secrets, place each one on a new line.
+
The following sample config map settings configure Alertmanager to use two `Secret` objects named `test-secret-basic-auth` and `test-secret-api-token`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      secrets:
      - test-secret-basic-auth
      - test-secret-api-token
----

** *To add a secret configuration to Alertmanager for user-defined project monitoring*:

.. Edit the `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add a `secrets:` section under `data/config.yaml/alertmanager/secrets` with the following configuration:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      secrets: <1>
      - <secret_name_1> <2>
      - <secret_name_2>
----
<1> This section contains the secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object.
<2> The name of the `Secret` object that contains authentication credentials for the receiver. If you add multiple secrets, place each one on a new line.
+
The following sample config map settings configure Alertmanager to use two `Secret` objects named `test-secret` and `test-secret-api-token`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      enabled: true
      secrets:
      - test-secret
      - test-api-receiver-token
----
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====

. Save the file to apply the changes to the `ConfigMap` object. The new configuration is applied automatically.


:leveloffset: 1

//Attaching additional labels to your time series and alerts
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="attaching-additional-labels-to-your-time-series-and-alerts_{context}"]
= Attaching additional labels to your time series and alerts

Using the external labels feature of Prometheus, you can attach custom labels to all time series and alerts leaving Prometheus.

.Prerequisites

* *If you are configuring core {product-title} monitoring components*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are configuring components that monitor user-defined projects*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object:
** *To attach custom labels to all time series and alerts leaving the Prometheus instance that monitors core {product-title} projects*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Define a map of labels you want to add for every metric under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      externalLabels:
        <key>: <value> <1>
----
+
<1> Substitute `<key>: <value>` with a map of key-value pairs where `<key>` is a unique name for the new label and `<value>` is its value.
+
[WARNING]
====
Do not use `prometheus` or `prometheus_replica` as key names, because they are reserved and will be overwritten.
====
+
For example, to add metadata about the region and environment to all time series and alerts, use:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      externalLabels:
        region: eu
        environment: prod
----

** *To attach custom labels to all time series and alerts leaving the Prometheus instance that monitors user-defined projects*:
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Define a map of labels you want to add for every metric under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      externalLabels:
        <key>: <value> <1>
----
+
<1> Substitute `<key>: <value>` with a map of key-value pairs where `<key>` is a unique name for the new label and `<value>` is its value.
+
[WARNING]
====
Do not use `prometheus` or `prometheus_replica` as key names, because they are reserved and will be overwritten.
====
+
[NOTE]
====
In the `openshift-user-workload-monitoring` project, Prometheus handles metrics and Thanos Ruler handles alerting and recording rules. Setting `externalLabels` for `prometheus` in the `user-workload-monitoring-config` `ConfigMap` object will only configure external labels for metrics and not for any rules.
====
+
For example, to add metadata about the region and environment to all time series and alerts related to user-defined projects, use:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      externalLabels:
        region: eu
        environment: prod
----

. Save the file to apply the changes. The new configuration is applied automatically.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps.
* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]

// Configuring topology spread constraints for monitoring components
:leveloffset: 1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: CONCEPT
[id="configuring_pod_topology_spread_constraintsfor_monitoring_{context}"]
= Configuring pod topology spread constraints for monitoring

You can use pod topology spread constraints to control how
Prometheus, Thanos Ruler, and Alertmanager
pods are spread across a network topology when {product-title} pods are deployed in multiple availability zones.

Pod topology spread constraints are suitable for controlling pod scheduling within hierarchical topologies in which nodes are spread across different infrastructure levels, such as regions and zones within those regions.
Additionally, by being able to schedule pods in different zones, you can improve network latency in certain scenarios.

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

// This xref might be relevant to ROSA/OSD if the Node content is reused:
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-pod-topology-spread-constraints-about[Controlling pod placement by using pod topology spread constraints]
* link:https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/[Kubernetes Pod Topology Spread Constraints documentation]

:leveloffset: 2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-up-pod-topology-spread-constraints-for-prometheus_{context}"]
= Setting up pod topology spread constraints for Prometheus

For core {product-title} platform monitoring, you can set up pod topology spread constraints for Prometheus to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Prometheus pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.

You configure pod topology spread constraints for Prometheus in the `cluster-monitoring-config` config map.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have created the `cluster-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` namespace:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add  values for the following settings under `data/config.yaml/prometheusK8s` to configure pod topology spread constraints:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      topologySpreadConstraints:
      - maxSkew: 1 <1>
        topologyKey: monitoring <2>
        whenUnsatisfiable: DoNotSchedule <3>
        labelSelector:
          matchLabels: <4>
            app.kubernetes.io/name: prometheus
----
<1> Specify a numeric value for `maxSkew`, which defines the degree to which pods are allowed to be unevenly distributed.
This field is required, and the value must be greater than zero.
The value specified has a different effect depending on what value you specify for `whenUnsatisfiable`.
<2> Specify a key of node labels for `topologyKey`.
This field is required.
Nodes that have a label with this key and identical values are considered to be in the same topology.
The scheduler will try to put a balanced number of pods into each domain.
<3> Specify a value for `whenUnsatisfiable`.
This field is required.
Available options are `DoNotSchedule` and `ScheduleAnyway`.
Specify `DoNotSchedule` if you want the `maxSkew` value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.
Specify `ScheduleAnyway` if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.
<4> Specify a value for `matchLabels`. This value is used to identify the set of matching pods to which to apply the constraints.

. Save the file to apply the changes automatically.
+
[WARNING]
====
When you save changes to the `cluster-monitoring-config` config map, the pods and other resources in the `openshift-monitoring` project might be redeployed.
The running monitoring processes in that project might also restart.
====

:leveloffset: 1
:leveloffset: 2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-up-pod-topology-spread-constraints-for-alertmanager_{context}"]
= Setting up pod topology spread constraints for Alertmanager

For core {product-title} platform monitoring, you can set up pod topology spread constraints for Alertmanager to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Alertmanager pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.

You configure pod topology spread constraints for Alertmanager in the `cluster-monitoring-config` config map.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have created the `cluster-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` namespace:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add values for the following settings under `data/config.yaml/alertmanagermain` to configure pod topology spread constraints:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      topologySpreadConstraints:
      - maxSkew: 1 <1>
        topologyKey: monitoring <2>
        whenUnsatisfiable: DoNotSchedule <3>
        labelSelector:
          matchLabels: <4>
            app.kubernetes.io/name: alertmanager
----
<1> Specify a numeric value for `maxSkew`, which defines the degree to which pods are allowed to be unevenly distributed.
This field is required, and the value must be greater than zero.
The value specified has a different effect depending on what value you specify for `whenUnsatisfiable`.
<2> Specify a key of node labels for `topologyKey`.
This field is required.
Nodes that have a label with this key and identical values are considered to be in the same topology.
The scheduler will try to put a balanced number of pods into each domain.
<3> Specify a value for `whenUnsatisfiable`.
This field is required.
Available options are `DoNotSchedule` and `ScheduleAnyway`.
Specify `DoNotSchedule` if you want the `maxSkew` value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.
Specify `ScheduleAnyway` if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.
<4> Specify a value for `matchLabels`. This value is used to identify the set of matching pods to which to apply the constraints.

. Save the file to apply the changes automatically.
+
[WARNING]
====
When you save changes to the `cluster-monitoring-config` config map, the pods and other resources in the `openshift-monitoring` project might be redeployed.
The running monitoring processes in that project might also restart.
====

:leveloffset: 1

:leveloffset: 2

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-up-pod-topology-spread-constraints-for-thanos-ruler_{context}"]
= Setting up pod topology spread constraints for Thanos Ruler

For user-defined monitoring, you can set up pod topology spread constraints for Thanos Ruler to fine tune how pod replicas are scheduled to nodes across zones.
Doing so helps ensure that Thanos Ruler pods are highly available and run more efficiently, because workloads are spread across nodes in different data centers or hierarchical infrastructure zones.

You configure pod topology spread constraints for Thanos Ruler in the `user-workload-monitoring-config` config map.

.Prerequisites

* A cluster administrator has enabled monitoring for user-defined projects.
* You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
* You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `user-workload-monitoring-config` config map in the `openshift-user-workload-monitoring` namespace:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

. Add values for the following settings under `data/config.yaml/thanosRuler` to configure pod topology spread constraints:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    thanosRuler:
      topologySpreadConstraints:
      - maxSkew: 1 <1>
        topologyKey: monitoring <2>
        whenUnsatisfiable: ScheduleAnyway <3>
        labelSelector:
          matchLabels: <4>
            app.kubernetes.io/name: thanos-ruler
----
<1> Specify a numeric value for `maxSkew`, which defines the degree to which pods are allowed to be unevenly distributed. This field is required, and the value must be greater than zero. The value specified has a different effect depending on what value you specify for `whenUnsatisfiable`.
<2> Specify a key of node labels for `topologyKey`. This field is required. Nodes that have a label with this key and identical values are considered to be in the same topology. The scheduler will try to put a balanced number of pods into each domain.
<3> Specify a value for `whenUnsatisfiable`. This field is required. Available options are `DoNotSchedule` and `ScheduleAnyway`. Specify `DoNotSchedule` if you want the `maxSkew` value to define the maximum difference allowed between the number of matching pods in the target topology and the global minimum.  Specify `ScheduleAnyway` if you want the scheduler to still schedule the pod but to give higher priority to nodes that might reduce the skew.
<4> Specify a value for `matchLabels`. This value is used to identify the set of matching pods to which to apply the constraints.

. Save the file to apply the changes automatically.
+
[WARNING]
====
When you save changes to the `user-workload-monitoring-config` config map, the pods and other resources in the `openshift-user-workload-monitoring` project might be redeployed.
The running monitoring processes in that project might also restart.
====

:leveloffset: 1

// Setting log levels for monitoring components
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-log-levels-for-monitoring-components_{context}"]
= Setting log levels for monitoring components

You can configure the log level for
Alertmanager, Prometheus Operator, Prometheus, Thanos Querier, and Thanos Ruler.

The following log levels can be applied to the relevant component in the
`cluster-monitoring-config` and
`user-workload-monitoring-config` `ConfigMap` objects:

* `debug`. Log debug, informational, warning, and error messages.
* `info`. Log informational, warning, and error messages.
* `warn`. Log warning and error messages only.
* `error`. Log error messages only.

The default log level is `info`.

.Prerequisites

* *If you are setting a log level for Alertmanager, Prometheus Operator, Prometheus, or Thanos Querier in the `openshift-monitoring` project*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are setting a log level for Prometheus Operator, Prometheus, or Thanos Ruler in the `openshift-user-workload-monitoring` project*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `ConfigMap` object:
** *To set a log level for a component in the `openshift-monitoring` project*:
.. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

.. Add `logLevel: <log_level>` for a component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    <component>: <1>
      logLevel: <log_level> <2>
----
<1> The monitoring stack component for which you are setting a log level.
For default platform monitoring, available component values are `prometheusK8s`, `alertmanagerMain`, `prometheusOperator`, and `thanosQuerier`.
<2> The log level to set for the component.
The available values are `error`, `warn`, `info`, and `debug`.
The default value is `info`.

** *To set a log level for a component in the `openshift-user-workload-monitoring` project*:

.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----

.. Add `logLevel: <log_level>` for a component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    <component>: <1>
      logLevel: <log_level> <2>
----
<1> The monitoring stack component for which you are setting a log level.
For user workload monitoring, available component values are `alertmanager`, `prometheus`, `prometheusOperator`, and `thanosRuler`.
<2> The log level to apply to the component. The available values are `error`, `warn`, `info`, and `debug`. The default value is `info`.

. Save the file to apply the changes. The pods for the component restart automatically when you apply the log-level change.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

. Confirm that the log-level has been applied by reviewing the deployment or pod configuration in the related project. The following example checks the log level in the `prometheus-operator` deployment in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml | grep "log-level"
----
+
.Example output
[source,terminal]
----
        - --log-level=debug
----

. Check that the pods for the component are running. The following example lists the status of pods in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get pods
----
+
[NOTE]
====
If an unrecognized `logLevel` value is included in the `ConfigMap` object, the pods for the component might not restart successfully.
====

:leveloffset: 1

// Setting query log for Prometheus
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-query-log-file-for-prometheus_{context}"]
= Enabling the query log file for Prometheus

[role="_abstract"]
You can configure Prometheus to write all queries that have been run by the engine to a log file.
You can do so for default platform monitoring and for user-defined workload monitoring.

[IMPORTANT]
====
Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the `ConfigMap` object to enable the feature.
====

.Prerequisites

* *If you are enabling the query log file feature for Prometheus in the `openshift-monitoring` project*:
** You have access to the cluster as a user with the `cluster-admin` cluster role.
** You have created the `cluster-monitoring-config` `ConfigMap` object.
* *If you are enabling the query log file feature for Prometheus in the `openshift-user-workload-monitoring` project*:
** You have access to the cluster as a user with the `cluster-admin` cluster role, or as a user with the `user-workload-monitoring-config-edit` role in the `openshift-user-workload-monitoring` project.
** You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have installed the OpenShift CLI (`oc`).

.Procedure

** *To set the query log file for Prometheus in the `openshift-monitoring` project*:
. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----
+
. Add `queryLogFile: <path>` for `prometheusK8s` under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    prometheusK8s:
      queryLogFile: <path> <1>
----
<1> The full path to the file in which queries will be logged.
+
. Save the file to apply the changes.
+
[WARNING]
====
When you save changes to a monitoring config map, pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====
+
. Verify that the pods for the component are running. The following sample command lists the status of pods in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring get pods
----
+
. Read the query log:
+
[source,terminal]
----
$ oc -n openshift-monitoring exec prometheus-k8s-0 -- cat <path>
----
+
[IMPORTANT]
====
Revert the setting in the config map after you have examined the logged query information.
====

** *To set the query log file for Prometheus in the `openshift-user-workload-monitoring` project*:
. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----
+
. Add `queryLogFile: <path>` for `prometheus` under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheus:
      queryLogFile: <path> <1>
----
<1> The full path to the file in which queries will be logged.
+
. Save the file to apply the changes.
+
[NOTE]
====
Configurations applied to the `user-workload-monitoring-config` `ConfigMap` object are not activated unless a cluster administrator has enabled monitoring for user-defined projects.
====
+
[WARNING]
====
When you save changes to a monitoring config map, pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====
+
. Verify that the pods for the component are running. The following example command lists the status of pods in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get pods
----
+
. Read the query log:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring exec prometheus-user-workload-0 -- cat <path>
----
+
[IMPORTANT]
====
Revert the setting in the config map after you have examined the logged query information.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps
* See xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects] for steps to enable user-defined monitoring.

// Enabling query logging for Thanos Querier
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="enabling-query-logging-for-thanos-querier_{context}"]
= Enabling query logging for Thanos Querier

[role="_abstract"]
For default platform monitoring in the `openshift-monitoring` project, you can enable the Cluster Monitoring Operator to log all queries run by Thanos Querier.

[IMPORTANT]
====
Because log rotation is not supported, only enable this feature temporarily when you need to troubleshoot an issue. After you finish troubleshooting, disable query logging by reverting the changes you made to the `ConfigMap` object to enable the feature.
====

.Prerequisites

* You have installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have created the `cluster-monitoring-config` `ConfigMap` object.

.Procedure

You can enable query logging for Thanos Querier in the `openshift-monitoring` project:

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----
+
. Add a `thanosQuerier` section under `data/config.yaml` and add values as shown in the following example:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    thanosQuerier:
      enableRequestLogging: <value> <1>
      logLevel: <value> <2>


----
<1> Set the value to `true` to enable logging and `false` to disable logging. The default value is `false`.
<2> Set the value to `debug`, `info`, `warn`, or `error`. If no value exists for `logLevel`, the log level defaults to `error`.
+
. Save the file to apply the changes.
+
[WARNING]
====
When you save changes to a monitoring config map, pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

.Verification

. Verify that the Thanos Querier pods are running. The following sample command lists the status of pods in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring get pods
----
+
. Run a test query using the following sample commands as a model:
+
[source,terminal]
----
$ token=`oc create token prometheus-k8s -n openshift-monitoring`
$ oc -n openshift-monitoring exec -c prometheus prometheus-k8s-0 -- curl -k -H "Authorization: Bearer $token" 'https://thanos-querier.openshift-monitoring.svc:9091/api/v1/query?query=cluster_version'
----
. Run the following command to read the query log:
+
[source,terminal]
----
$ oc -n openshift-monitoring logs <thanos_querier_pod_name> -c thanos-query
----
+
[NOTE]
====
Because the `thanos-querier` pods are highly available (HA) pods, you might be able to see logs in only one pod.
====
+
. After you examine the logged query information, disable query logging by changing the `enableRequestLogging` value to `false` in the config map.


:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps.

// Setting audit log levels for the Prometheus Adapter
:leveloffset: 1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-audit-log-levels-for-the-prometheus-adapter_{context}"]
= Setting audit log levels for the Prometheus Adapter

[role=_abstract]
In default platform monitoring, you can configure the audit log level for the Prometheus Adapter.

.Prerequisites

* You have installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have created the `cluster-monitoring-config` `ConfigMap` object.

.Procedure

You can set an audit log level for the Prometheus Adapter in the default `openshift-monitoring` project:

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add `profile:` in the `k8sPrometheusAdapter/audit` section under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    k8sPrometheusAdapter:
      audit:
        profile: <audit_log_level> <1>
----
<1> The audit log level to apply to the Prometheus Adapter.

. Set the audit log level by using one of the following values for the `profile:` parameter:
+
* `None`: Do not log events.
* `Metadata`: Log only the metadata for the request, such as user, timestamp, and so forth. Do not log the request text and the response text. `Metadata` is the default audit log level.
* `Request`: Log only the metadata and the request text but not the response text. This option does not apply for non-resource requests.
* `RequestResponse`: Log event metadata, request text, and response text. This option does not apply for non-resource requests.

. Save the file to apply the changes. The pods for the Prometheus Adapter restart automatically when you apply the change.
+
[WARNING]
====
When changes are saved to a monitoring config map, the pods and other resources in the related project might be redeployed. The running monitoring processes in that project might also be restarted.
====

.Verification

. In the config map, under `k8sPrometheusAdapter/audit/profile`, set the log level to `Request` and save the file.

. Confirm that the pods for the Prometheus Adapter are running. The following example lists the status of pods in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring get pods
----

. Confirm that the audit log level and audit log file path are correctly configured:
+
[source,terminal]
----
$ oc -n openshift-monitoring get deploy prometheus-adapter -o yaml
----
+
.Example output
[source,terminal]
----
...
  - --audit-policy-file=/etc/audit/request-profile.yaml
  - --audit-log-path=/var/log/adapter/audit.log
----

. Confirm that the correct log level has been applied in the `prometheus-adapter` deployment in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring exec deploy/prometheus-adapter -c prometheus-adapter -- cat /etc/audit/request-profile.yaml
----
+
.Example output
[source,terminal]
----
"apiVersion": "audit.k8s.io/v1"
"kind": "Policy"
"metadata":
  "name": "Request"
"omitStages":
- "RequestReceived"
"rules":
- "level": "Request"
----
+
[NOTE]
====
If you enter an unrecognized `profile` value for the Prometheus Adapter in the `ConfigMap` object, no changes are made to the Prometheus Adapter, and an error is logged by the Cluster Monitoring Operator.
====

. Review the audit log for the Prometheus Adapter:
+
[source,terminal]
----
$ oc -n openshift-monitoring exec -c <prometheus_adapter_pod_name> -- cat /var/log/adapter/audit.log
----


:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See xref:preparing-to-configure-the-monitoring-stack[Preparing to configure the monitoring stack] for steps to create monitoring config maps.

// Disabling the local Alertmanager
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/configuring-the-monitoring-stack.adoc

:_mod-docs-content-type: PROCEDURE
[id="monitoring-disabling-the-local-alertmanager_{context}"]
= Disabling the local Alertmanager

A local Alertmanager that routes alerts from Prometheus instances is enabled by default in the `openshift-monitoring` project of the {product-title} monitoring stack.

If you do not need the local Alertmanager, you can disable it by configuring the `cluster-monitoring-config` config map in the `openshift-monitoring` project.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have created the `cluster-monitoring-config` config map.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `cluster-monitoring-config` config map in the `openshift-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add `enabled: false` for the `alertmanagerMain` component under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      enabled: false
----

. Save the file to apply the changes. The Alertmanager instance is disabled automatically when you apply the change.


:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* link:https://prometheus.io/docs/alerting/latest/alertmanager/[Prometheus Alertmanager documentation]
* xref:[Managing alerts]

== Next steps

* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]
* Learn about link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#opting-out-remote-health-reporting_opting-out-remote-health-reporting[remote health reporting] and, if necessary, opt out of it.

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="enabling-monitoring-for-user-defined-projects"]
= Enabling monitoring for user-defined projects
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: enabling-monitoring-for-user-defined-projects

toc::[]

In {product-title} {product-version}, you can enable monitoring for user-defined projects in addition to the default platform monitoring. You can monitor your own projects in {product-title} without the need for an additional monitoring solution. Using this feature centralizes monitoring for core platform components and user-defined projects.

// Text snippet included in the following modules:
//
// * modules/monitoring-enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: SNIPPET

[NOTE]
====
Versions of Prometheus Operator installed using Operator Lifecycle Manager (OLM) are not compatible with user-defined monitoring. Therefore, custom Prometheus instances installed as a Prometheus custom resource (CR) managed by the OLM Prometheus Operator are not supported in {product-title}.
====

// Enabling monitoring for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="enabling-monitoring-for-user-defined-projects_{context}"]
= Enabling monitoring for user-defined projects

Cluster administrators can enable monitoring for user-defined projects by setting the `enableUserWorkload: true` field in the cluster monitoring `ConfigMap` object.

[IMPORTANT]
====
In {product-title} {product-version} you must remove any custom Prometheus instances before enabling monitoring for user-defined projects.
====

[NOTE]
====
You must have access to the cluster as a user with the `cluster-admin` cluster role to enable monitoring for user-defined projects in {product-title}. Cluster administrators can then optionally grant users permission to configure the components that are responsible for monitoring user-defined projects.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).
* You have created the `cluster-monitoring-config` `ConfigMap` object.
* You have optionally created and configured the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project. You can add configuration options to this `ConfigMap` object for the components that monitor user-defined projects.
+
[NOTE]
====
Every time you save configuration changes to the `user-workload-monitoring-config` `ConfigMap` object, the pods in the `openshift-user-workload-monitoring` project are redeployed. It can sometimes take a while for these components to redeploy. You can create and configure the `ConfigMap` object before you first enable monitoring for user-defined projects, to prevent having to redeploy the pods often.
====

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----

. Add `enableUserWorkload: true` under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: true <1>
----
<1> When set to `true`, the `enableUserWorkload` parameter enables monitoring for user-defined projects in a cluster.

. Save the file to apply the changes. Monitoring for user-defined projects is then enabled automatically.
+
[WARNING]
====
When changes are saved to the `cluster-monitoring-config` `ConfigMap` object, the pods and other resources in the `openshift-monitoring` project might be redeployed. The running monitoring processes in that project might also be restarted.
====

. Check that the `prometheus-operator`, `prometheus-user-workload` and `thanos-ruler-user-workload` pods are running in the `openshift-user-workload-monitoring` project. It might take a short while for the pods to start:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get pod
----
+
.Example output
[source,terminal]
----
NAME                                   READY   STATUS        RESTARTS   AGE
prometheus-operator-6f7b748d5b-t7nbg   2/2     Running       0          3h
prometheus-user-workload-0             4/4     Running       1          3h
prometheus-user-workload-1             4/4     Running       1          3h
thanos-ruler-user-workload-0           3/3     Running       0          3h
thanos-ruler-user-workload-1           3/3     Running       0          3h
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* xref:creating-cluster-monitoring-configmap_configuring-the-monitoring-stack[Creating a cluster monitoring config map]
* xref:configuring-the-monitoring-stack[Configuring the monitoring stack]
* xref:granting-users-permission-to-configure-monitoring-for-user-defined-projects_enabling-monitoring-for-user-defined-projects[Granting users permission to configure monitoring for user-defined projects]

// Granting users permission to monitor user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

[id="granting-users-permission-to-monitor-user-defined-projects_{context}"]
= Granting users permission to monitor user-defined projects

Cluster administrators can monitor all core {product-title} and user-defined projects.

Cluster administrators can grant developers and other users permission to monitor their own projects. Privileges are granted by assigning one of the following monitoring roles:

* The *monitoring-rules-view* cluster role provides read access to `PrometheusRule` custom resources for a project.

* The *monitoring-rules-edit* cluster role grants a user permission to create, modify, and deleting `PrometheusRule` custom resources for a project.

* The *monitoring-edit* cluster role grants the same privileges as the `monitoring-rules-edit` cluster role. Additionally, it enables a user to create new scrape targets for services or pods. With this role, you can also create, modify, and delete `ServiceMonitor` and `PodMonitor` resources.

You can also grant users permission to configure the components that are responsible for monitoring user-defined projects:

* The *user-workload-monitoring-config-edit* role in the `openshift-user-workload-monitoring` project enables you to edit the `user-workload-monitoring-config` `ConfigMap` object. With this role, you can edit the `ConfigMap` object to configure Prometheus, Prometheus Operator, and Thanos Ruler for user-defined workload monitoring.

You can also grant users permission to configure alert routing for user-defined projects:

* The **alert-routing-edit** cluster role grants a user permission to create, update, and delete `AlertmanagerConfig` custom resources for a project.

This section provides details on how to assign these roles by using the {product-title} web console or the CLI.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="granting-user-permissions-using-the-web-console_{context}"]
= Granting user permissions by using the web console

You can grant users permissions to monitor their own projects, by using the {product-title} web console.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* The user account that you are assigning the role to already exists.

.Procedure

. In the *Administrator* perspective within the {product-title} web console, navigate to *User Management* -> *RoleBindings* -> *Create binding*.

. In the *Binding Type* section, select the "Namespace Role Binding" type.

. In the *Name* field, enter a name for the role binding.

. In the *Namespace* field, select the user-defined project where you want to grant the access.
+
[IMPORTANT]
====
The monitoring role will be bound to the project that you apply in the *Namespace* field. The permissions that you grant to a user by using this procedure will apply only to the selected project.
====

. Select `monitoring-rules-view`, `monitoring-rules-edit`, or `monitoring-edit` in the *Role Name* list.

. In the *Subject* section, select *User*.

. In the *Subject Name* field, enter the name of the user.

. Select *Create* to apply the role binding.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="granting-user-permissions-using-the-cli_{context}"]
= Granting user permissions by using the CLI

You can grant users permissions to monitor their own projects, by using the OpenShift CLI (`oc`).

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* The user account that you are assigning the role to already exists.
* You have installed the OpenShift CLI (`oc`).

.Procedure

* Assign a monitoring role to a user for a project:
+
[source,terminal]
----
$ oc policy add-role-to-user <role> <user> -n <namespace> <1>
----
<1> Substitute `<role>` with `monitoring-rules-view`, `monitoring-rules-edit`, or `monitoring-edit`.
+
[IMPORTANT]
====
Whichever role you choose, you must bind it against a specific project as a cluster administrator.
====
+
As an example, substitute `<role>` with `monitoring-edit`, `<user>` with `johnsmith`, and `<namespace>` with `ns1`. This assigns the user `johnsmith` permission to set up metrics collection and to create alerting rules in the `ns1` namespace.

:leveloffset: 1

// Granting users permission to configure monitoring for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="granting-users-permission-to-configure-monitoring-for-user-defined-projects_{context}"]
= Granting users permission to configure monitoring for user-defined projects

You can grant users permission to configure monitoring for user-defined projects.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* The user account that you are assigning the role to already exists.
* You have installed the OpenShift CLI (`oc`).

.Procedure

* Assign the `user-workload-monitoring-config-edit` role to a user in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring adm policy add-role-to-user \
  user-workload-monitoring-config-edit <user> \
  --role-namespace openshift-user-workload-monitoring
----

:leveloffset: 1

// Accessing metrics from outside the cluster for custom applications
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="accessing-metrics-from-outside-cluster_{context}"]
= Accessing metrics from outside the cluster for custom applications

Learn how to query Prometheus statistics from the command line when monitoring your own services. You can access monitoring data from outside the cluster with the `thanos-querier` route.

.Prerequisites

* You deployed your own service, following the _Enabling monitoring for user-defined projects_ procedure.

.Procedure

. Extract a token to connect to Prometheus:
+
[source,terminal]
----
$ SECRET=`oc get secret -n openshift-user-workload-monitoring | grep  prometheus-user-workload-token | head -n 1 | awk '{print $1 }'`
----
+
[source,terminal]
----
$ TOKEN=`echo $(oc get secret $SECRET -n openshift-user-workload-monitoring -o json | jq -r '.data.token') | base64 -d`
----

. Extract your route host:
+
[source,terminal]
----
$ THANOS_QUERIER_HOST=`oc get route thanos-querier -n openshift-monitoring -o json | jq -r '.spec.host'`
----

. Query the metrics of your own services in the command line. For example:
+
[source,terminal]
----
$ NAMESPACE=ns1
----
+
[source,terminal]
----
$ curl -X GET -kG "https://$THANOS_QUERIER_HOST/api/v1/query?" --data-urlencode "query=up{namespace='$NAMESPACE'}" -H "Authorization: Bearer $TOKEN"
----
+
The output will show you the duration that your application pods have been up.
+
.Example output
[source,terminal]
----
{"status":"success","data":{"resultType":"vector","result":[{"metric":{"__name__":"up","endpoint":"web","instance":"10.129.0.46:8080","job":"prometheus-example-app","namespace":"ns1","pod":"prometheus-example-app-68d47c4fb6-jztp2","service":"prometheus-example-app"},"value":[1591881154.748,"1"]}]}}
----

:leveloffset: 1

// Excluding a user-defined project from monitoring
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc
// * monitoring/sd-disabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="excluding-a-user-defined-project-from-monitoring_{context}"]
= Excluding a user-defined project from monitoring

Individual user-defined projects can be excluded from user workload monitoring. To do so, add the `openshift.io/user-monitoring` label to the project's namespace with a value of `false`.

.Procedure

. Add the label to the project namespace:
+
[source,terminal]
----
$ oc label namespace my-project 'openshift.io/user-monitoring=false'
----
+
. To re-enable monitoring, remove the label from the namespace:
+
[source,terminal]
----
$ oc label namespace my-project 'openshift.io/user-monitoring-'
----
+
[NOTE]
====
If there were any active monitoring targets for the project, it may take a few minutes for Prometheus to stop scraping them after adding the label.
====

:leveloffset: 1

// Disabling monitoring for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-monitoring-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="disabling-monitoring-for-user-defined-projects_{context}"]
= Disabling monitoring for user-defined projects

After enabling monitoring for user-defined projects, you can disable it again by setting `enableUserWorkload: false` in the cluster monitoring `ConfigMap` object.

[NOTE]
====
Alternatively, you can remove `enableUserWorkload: true` to disable monitoring for user-defined projects.
====

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----
+
.. Set `enableUserWorkload:` to `false` under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    enableUserWorkload: false
----

. Save the file to apply the changes. Monitoring for user-defined projects is then disabled automatically.

. Check that the `prometheus-operator`, `prometheus-user-workload` and `thanos-ruler-user-workload` pods are terminated in the `openshift-user-workload-monitoring` project. This might take a short while:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get pod
----
+
.Example output
[source,terminal]
----
No resources found in openshift-user-workload-monitoring project.
----

[NOTE]
====
The `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project is not automatically deleted when monitoring for user-defined projects is disabled. This is to preserve any custom configurations that you may have created in the `ConfigMap` object.
====

:leveloffset: 1

== Next steps

* xref:managing-metrics[Managing metrics]

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="enabling-alert-routing-for-user-defined-projects"]
= Enabling alert routing for user-defined projects
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: enabling-alert-routing-for-user-defined-projects

toc::[]

[role="_abstract"]
In {product-title} {product-version}, a cluster administrator can enable alert routing for user-defined projects.
This process consists of two general steps:

* Enable alert routing for user-defined projects to use the default platform Alertmanager instance or, optionally, a separate Alertmanager instance only for user-defined projects.
* Grant users permission to configure alert routing for user-defined projects.

After you complete these steps, developers and other users can configure custom alerts and alert routing for their user-defined projects.

// Overview of setting up alert routing for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-alert-routing-for-user-defined-projects.adoc

:_mod-docs-content-type: CONCEPT
[id="understanding-alert-routing-for-user-defined-projects_{context}"]
= Understanding alert routing for user-defined projects

[role="_abstract"]
As a cluster administrator, you can enable alert routing for user-defined projects.
With this feature, you can allow users with the **alert-routing-edit** role to configure alert notification routing and receivers for user-defined projects.
These notifications are routed by the default Alertmanager instance or, if enabled, an optional Alertmanager instance dedicated to user-defined monitoring.

Users can then create and configure user-defined alert routing by creating or editing the `AlertmanagerConfig` objects for their user-defined projects without the help of an administrator.

After a user has defined alert routing for a user-defined project, user-defined alert notifications are routed as follows:

* To the `alertmanager-main` pods in the `openshift-monitoring` namespace if using the default platform Alertmanager instance.

* To the `alertmanager-user-workload` pods in the `openshift-user-workload-monitoring` namespace if you have enabled a separate instance of Alertmanager for user-defined projects.

[NOTE]
====
The following are limitations of alert routing for user-defined projects:

* For user-defined alerting rules, user-defined routing is scoped to the namespace in which the resource is defined. For example, a routing configuration in namespace `ns1` only applies to `PrometheusRules` resources in the same namespace.

* When a namespace is excluded from user-defined monitoring, `AlertmanagerConfig` resources in the namespace cease to be part of the Alertmanager configuration.
====

:leveloffset: 1

// Enabling user-defined alerts using the default Alertmanager instance
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-alert-routing-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="enabling-the-platform-alertmanager-instance-for-user-defined-alert-routing_{context}"]
= Enabling the platform Alertmanager instance for user-defined alert routing

You can allow users to create user-defined alert routing configurations that use the main platform instance of Alertmanager.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object:
+
[source,terminal]
----
$ oc -n openshift-monitoring edit configmap cluster-monitoring-config
----
+
. Add `enableUserAlertmanagerConfig: true` in the `alertmanagerMain` section under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |
    alertmanagerMain:
      enableUserAlertmanagerConfig: true <1>
----
<1> Set the `enableUserAlertmanagerConfig` value to `true` to allow users to create user-defined alert routing configurations that use the main platform instance of Alertmanager.
+
. Save the file to apply the changes.

:leveloffset: 1

// Enabling a dedicated Alertmanager instance for use in user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-alert-routing-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="enabling-a-separate-alertmanager-instance-for-user-defined-alert-routing_{context}"]
= Enabling a separate Alertmanager instance for user-defined alert routing

In some clusters, you might want to deploy a dedicated Alertmanager instance for user-defined projects, which can help reduce the load on the default platform Alertmanager instance and can better separate user-defined alerts from default platform alerts.
In these cases, you can optionally enable a separate instance of Alertmanager to send alerts for user-defined projects only.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have enabled monitoring for user-defined projects in the `cluster-monitoring-config` config map for the `openshift-monitoring` namespace.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Edit the `user-workload-monitoring-config` `ConfigMap` object:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----
+
. Add `enabled: true` and `enableAlertmanagerConfig: true` in the `alertmanager` section under `data/config.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    alertmanager:
      enabled: true <1>
      enableAlertmanagerConfig: true <2>
----
<1> Set the `enabled` value to `true` to enable a dedicated instance of the Alertmanager for user-defined projects in a cluster. Set the value to `false` or omit the key entirely to disable the Alertmanager for user-defined projects.
If you set this value to `false` or if the key is omitted, user-defined alerts are routed to the default platform Alertmanager instance.
<2> Set the `enableAlertmanagerConfig` value to `true` to enable users to define their own alert routing configurations with `AlertmanagerConfig` objects.
+
. Save the file to apply the changes. The dedicated instance of Alertmanager for user-defined projects starts automatically.

.Verification

* Verify that the `user-workload` Alertmanager instance has started:
+
[source,terminal]
----
# oc -n openshift-user-workload-monitoring get alertmanager
----
+
.Example output
+
[source,terminal]
----
NAME            VERSION   REPLICAS   AGE
user-workload   0.24.0    2          100s
----

// In ROSA/OSD, a dedicated-admin doesn't have permission to view the alertmanager resource.

:leveloffset: 1

// Granting users permission to configure alert routing for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/enabling-alert-routing-for-user-defined-projects.adoc

:_mod-docs-content-type: PROCEDURE
[id="granting-users-permission-to-configure-alert-routing-for-user-defined-projects_{context}"]
= Granting users permission to configure alert routing for user-defined projects

[role="_abstract"]
You can grant users permission to configure alert routing for user-defined projects.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have enabled monitoring for user-defined projects in the `cluster-monitoring-config` config map for the `openshift-monitoring` namespace.
* The user account that you are assigning the role to already exists.
* You have installed the OpenShift CLI (`oc`).

.Procedure

* Assign the `alert-routing-edit` cluster role to a user in the user-defined project:
+
[source,terminal]
----
$ oc -n <namespace> adm policy add-role-to-user alert-routing-edit <user> <1>
----
<1> For `<namespace>`, substitute the namespace for the user-defined project, such as `ns1`. For `<user>`, substitute the username for the account to which you want to assign the role.

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user defined projects]
* xref:creating-alert-routing-for-user-defined-projects_managing-alerts[Creating alert routing for user-defined projects]

== Next steps

* xref:managing-alerts[Managing alerts]

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="managing-metrics"]
= Managing metrics
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: managing-metrics

toc::[]

[role="_abstract"]
You can collect metrics to monitor how cluster components and your own workloads are performing.

// Understanding metrics
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc

:_mod-docs-content-type: CONCEPT
[id="understanding-metrics_{context}"]
= Understanding metrics

[role="_abstract"]
In {product-title} {product-version},
cluster components are monitored by scraping metrics exposed through service endpoints. You can also configure metrics collection for user-defined projects. Metrics enable you to monitor how cluster components and your own workloads are performing.

You can define the metrics that you want to provide for your own workloads by using Prometheus client libraries at the application level.

In {product-title}, metrics are exposed through an HTTP service endpoint under the `/metrics` canonical name. You can list all available metrics for a service by running a `curl` query against `\http://<endpoint>/metrics`. For instance, you can expose a route to the `prometheus-example-app` example application and then run the following to view all of its available metrics:

[source,terminal]
----
$ curl http://<example_app_endpoint>/metrics
----

.Example output
[source,terminal]
----
# HELP http_requests_total Count of all HTTP requests
# TYPE http_requests_total counter
http_requests_total{code="200",method="get"} 4
http_requests_total{code="404",method="get"} 2
# HELP version Version information about this binary
# TYPE version gauge
version{version="v0.1.0"} 1
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* link:https://prometheus.io/docs/instrumenting/clientlibs/[Prometheus client library documentation]

// Setting up metrics collection for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc

:_mod-docs-content-type: CONCEPT
[id="setting-up-metrics-collection-for-user-defined-projects_{context}"]
= Setting up metrics collection for user-defined projects

You can create a `ServiceMonitor` resource to scrape metrics from a service endpoint in a user-defined project. This assumes that your application uses a Prometheus client library to expose metrics to the `/metrics` canonical name.

This section describes how to deploy a sample service in a user-defined project and then create a `ServiceMonitor` resource that defines how that service should be monitored.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc

:_mod-docs-content-type: PROCEDURE
[id="deploying-a-sample-service_{context}"]
= Deploying a sample service

To test monitoring of a service in a user-defined project, you can deploy a sample service.

.Procedure

. Create a YAML file for the service configuration. In this example, it is called `prometheus-example-app.yaml`.

. Add the following deployment and service configuration details to the file:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ns1
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-example-app
  template:
    metadata:
      labels:
        app: prometheus-example-app
    spec:
      containers:
      - image: ghcr.io/rhobs/prometheus-example-app:0.4.2
        imagePullPolicy: IfNotPresent
        name: prometheus-example-app
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus-example-app
  name: prometheus-example-app
  namespace: ns1
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    name: web
  selector:
    app: prometheus-example-app
  type: ClusterIP
----
+
This configuration deploys a service named `prometheus-example-app` in the user-defined `ns1` project. This service exposes the custom `version` metric.

. Apply the configuration to the cluster:
+
[source,terminal]
----
$ oc apply -f prometheus-example-app.yaml
----
+
It takes some time to deploy the service.

. You can check that the pod is running:
+
[source,terminal]
----
$ oc -n ns1 get pod
----
+
.Example output
[source,terminal]
----
NAME                                      READY     STATUS    RESTARTS   AGE
prometheus-example-app-7857545cb7-sbgwq   1/1       Running   0          81m
----

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc

:_mod-docs-content-type: PROCEDURE
[id="specifying-how-a-service-is-monitored_{context}"]
= Specifying how a service is monitored

[role="_abstract"]
To use the metrics exposed by your service, you must configure {product-title} monitoring to scrape metrics from the `/metrics` endpoint. You can do this using a `ServiceMonitor` custom resource definition (CRD) that specifies how a service should be monitored, or a `PodMonitor` CRD that specifies how a pod should be monitored. The former requires a `Service` object, while the latter does not, allowing Prometheus to directly scrape metrics from the metrics endpoint exposed by a pod.

This procedure shows you how to create a `ServiceMonitor` resource for a service in a user-defined project.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role or the `monitoring-edit` cluster role.
* You have enabled monitoring for user-defined projects.
* For this example, you have deployed the `prometheus-example-app` sample service in the `ns1` project.
+
[NOTE]
====
The `prometheus-example-app` sample service does not support TLS authentication.
====

.Procedure

. Create a YAML file for the `ServiceMonitor` resource configuration. In this example, the file is called `example-app-service-monitor.yaml`.

. Add the following `ServiceMonitor` resource configuration details:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: prometheus-example-monitor
  name: prometheus-example-monitor
  namespace: ns1
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app
----
+
This defines a `ServiceMonitor` resource that scrapes the metrics exposed by the `prometheus-example-app` sample service, which includes the `version` metric.
+
[NOTE]
====
A `ServiceMonitor` resource in a user-defined namespace can only discover services in the same namespace. That is, the `namespaceSelector` field of the `ServiceMonitor` resource is always ignored.
====

. Apply the configuration to the cluster:
+
[source,terminal]
----
$ oc apply -f example-app-service-monitor.yaml
----
+
It takes some time to deploy the `ServiceMonitor` resource.

. You can check that the `ServiceMonitor` resource is running:
+
[source,terminal]
----
$ oc -n ns1 get servicemonitor
----
+
.Example output
[source,terminal]
----
NAME                         AGE
prometheus-example-monitor   81m
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* xref:enabling-monitoring-for-user-defined-projects[Enabling monitoring for user-defined projects]
* link:https://access.redhat.com/articles/6675491[How to scrape metrics using TLS in a ServiceMonitor configuration in a user-defined project]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#[PodMonitor API]
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/api_reference/#[ServiceMonitor API]

// Viewing a list of available metrics for a cluster
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc

:_mod-docs-content-type: PROCEDURE
[id="viewing-a-list-of-available-metrics_{context}"]
= Viewing a list of available metrics

As a cluster administrator or as a user with view permissions for all projects, you can view a list of metrics available in a cluster and output the list in JSON format.

.Prerequisites
* You are a cluster administrator, or you have access to the cluster as a user with the `cluster-monitoring-view` cluster role.
* You have installed the {product-title} CLI (`oc`).
* You have obtained the {product-title} API route for Thanos Querier.
* You are able to get a bearer token by using the `oc whoami -t` command.
+
[IMPORTANT]
====
You can only use bearer token authentication to access the Thanos Querier API route.
====

.Procedure

. If you have not obtained the {product-title} API route for Thanos Querier, run the following command:
+
[source,terminal]
----
$ oc get routes -n openshift-monitoring thanos-querier -o jsonpath='{.status.ingress[0].host}'
----

. Retrieve a list of metrics in JSON format from the Thanos Querier API route by running the following command. This command uses `oc` to authenticate with a bearer token.
+
[source,terminal]
----
$ curl -k -H "Authorization: Bearer $(oc whoami -t)" https://<thanos_querier_route>/api/v1/metadata <1>
----
<1> Replace `<thanos_querier_route>` with the {product-title} API route for Thanos Querier.

:leveloffset: 1

// Querying metrics
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc
// * virt/support/virt-prometheus-queries.adoc

:_mod-docs-content-type: CONCEPT
[id="about-querying-metrics_{context}"]
= Querying metrics

The {product-title} monitoring dashboard enables you to run Prometheus Query Language (PromQL) queries to examine metrics visualized on a plot. This functionality provides information about the state of a cluster and any user-defined workloads that you are monitoring.

As a cluster administrator, you can query metrics for all core {product-title} and user-defined projects.

As a developer, you must specify a project name when querying metrics. You must have the required privileges to view metrics for the selected project.

:leveloffset: 1

// include::modules/monitoring-contents-of-the-metrics-ui.adoc[leveloffset=+2]

// Querying metrics for all projects as an administrator
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc
// * virt/support/virt-prometheus-queries.adoc

:_mod-docs-content-type: PROCEDURE
[id="querying-metrics-for-all-projects-as-an-administrator_{context}"]
= Querying metrics for all projects as a cluster administrator

As a
cluster administrator
or as a user with view permissions for all projects, you can access metrics for all default {product-title} and user-defined projects in the Metrics UI.


.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role or with view permissions for all projects.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. From the *Administrator* perspective in the {product-title} web console, select *Observe* -> *Metrics*.

. To add one or more queries, do any of the following:
+
|===
|Option |Description

|Create a custom query.
|Add your Prometheus Query Language (PromQL) query to the *Expression* field.

As you type a PromQL expression, autocomplete suggestions appear in a drop-down list. These suggestions include functions, metrics, labels, and time tokens.
You can use the keyboard arrows to select one of these suggested items and then press Enter to add the item to your expression. You can also move your mouse pointer over a suggested item to view a brief description of that item.

|Add multiple queries. |Select *Add query*.

|Duplicate an existing query. |Select the Options menu {kebab} next to the query, then choose *Duplicate query*.

|Disable a query from being run. |Select the Options menu {kebab} next to the query and choose *Disable query*.
|===

. To run queries that you created, select *Run queries*. The metrics from the queries are visualized on the plot. If a query is invalid, the UI shows an error message.
+
[NOTE]
====
Queries that operate on large amounts of data might time out or overload the browser when drawing time series graphs. To avoid this, select *Hide graph* and calibrate your query using only the metrics table. Then, after finding a feasible query, enable the plot to draw the graphs.
====
+
[NOTE]
====
By default, the query table shows an expanded view that lists every metric and its current value. You can select *˅* to minimize the expanded view for a query.
====

. Optional: The page URL now contains the queries you ran. To use this set of queries again in the future, save this URL.

. Explore the visualized metrics. Initially, all metrics from all enabled queries are shown on the plot. You can select which metrics are shown by doing any of the following:
+
|===
|Option |Description

|Hide all metrics from a query. |Click the Options menu {kebab} for the query and click *Hide all series*.

|Hide a specific metric. |Go to the query table and click the colored square near the metric name.

|Zoom into the plot and change the time range.
a|Either:

* Visually select the time range by clicking and dragging on the plot horizontally.
* Use the menu in the left upper corner to select the time range.

|Reset the time range. |Select *Reset zoom*.

|Display outputs for all queries at a specific point in time. |Hold the mouse cursor on the plot at that point. The query outputs will appear in a pop-up box.

|Hide the plot. |Select *Hide graph*.
|===

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* For more information about creating PromQL queries, see the link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus query documentation].

// Querying metrics for user-defined projects as a developer
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc
// * virt/support/virt-prometheus-queries.adoc

:_mod-docs-content-type: PROCEDURE
[id="querying-metrics-for-user-defined-projects-as-a-developer_{context}"]
= Querying metrics for user-defined projects as a developer

You can access metrics for a user-defined project as a developer or as a user with view permissions for the project.

In the *Developer* perspective, the Metrics UI includes some predefined CPU, memory, bandwidth, and network packet queries for the selected project. You can also run custom Prometheus Query Language (PromQL) queries for CPU, memory, bandwidth, network packet and application metrics for the project.

[NOTE]
====
Developers can only use the *Developer* perspective and not the *Administrator* perspective. As a developer, you can only query metrics for one project at a time.
====

.Prerequisites

* You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.
* You have enabled monitoring for user-defined projects.
* You have deployed a service in a user-defined project.
* You have created a `ServiceMonitor` custom resource definition (CRD) for the service to define how the service is monitored.

.Procedure

. From the *Developer* perspective in the {product-title} web console, select *Observe* -> *Metrics*.

. Select the project that you want to view metrics for in the *Project:* list.

. Select a query from the *Select query* list, or create a custom PromQL query based on the selected query by selecting *Show PromQL*. The metrics from the queries are visualized on the plot.
+
[NOTE]
====
In the Developer perspective, you can only run one query at a time.
====

. Explore the visualized metrics by doing any of the following:
+
|===
|Option |Description

|Zoom into the plot and change the time range.
a|Either:

* Visually select the time range by clicking and dragging on the plot horizontally.
* Use the menu in the left upper corner to select the time range.

|Reset the time range. |Select *Reset zoom*.

|Display outputs for all queries at a specific point in time. |Hold the mouse cursor on the plot at that point. The query outputs appear in a pop-up box.
|===

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* For more information about creating PromQL queries, see the link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus query documentation].

// Getting detailed information about metrics targets
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-metrics.adoc

:_mod-docs-content-type: PROCEDURE
[id="getting-detailed-information-about-a-target_{context}"]
= Getting detailed information about a metrics target

In the *Administrator* perspective in the {product-title} web console, you can use the *Metrics targets* page to view, search, and filter the endpoints that are currently targeted for scraping, which helps you to identify and troubleshoot problems. For example, you can view the current status of targeted endpoints to see when {product-title} Monitoring is not able to scrape metrics from a targeted component.

The *Metrics targets* page shows targets for default {product-title} projects and for user-defined projects.

.Prerequisites

* You have access to the cluster as an administrator for the project for which you want to view metrics targets.

.Procedure

. In the *Administrator* perspective, select *Observe* -> *Targets*. The *Metrics targets* page opens with a list of all service endpoint targets that are being scraped for metrics.
+
--
This page shows details about targets for default {product-title} and user-defined projects. This page lists the following information for each target:

* Service endpoint URL being scraped
* ServiceMonitor component being monitored
* The **up** or **down** status of the target
* Namespace
* Last scrape time
* Duration of the last scrape
--

. Optional: The list of metrics targets can be long. To find a specific target, do any of the following:
+
|===
|Option |Description

|Filter the targets by status and source.
a|Select filters in the *Filter* list.

The following filtering options are available:

* **Status** filters:
** **Up**. The target is currently up and being actively scraped for metrics.
** **Down**. The target is currently down and not being scraped for metrics.

* **Source** filters:
** **Platform**. Platform-level targets relate only to default Red Hat OpenShift Service on AWS projects. These projects provide core Red Hat OpenShift Service on AWS functionality.
** **User**. User targets relate to user-defined projects. These projects are user-created and can be customized.

|Search for a target by name or label. |Enter a search term in the **Text** or **Label** field next to the search box.

|Sort the targets. |Click one or more of the **Endpoint Status**, **Namespace**, **Last Scrape**, and **Scrape Duration** column headers.
|===

. Click the URL in the **Endpoint** column for a target to navigate to its **Target details** page. This page provides information about the target, including the following:
+
--
** The endpoint URL being scraped for metrics
** The current *Up* or *Down* status of the target
** A link to the namespace
** A link to the ServiceMonitor details
** Labels attached to the target
** The most recent time that the target was scraped for metrics
--

:leveloffset: 1

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="managing-alerts"]
= Managing alerts
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: managing-alerts

toc::[]

In {product-title} {product-version}, the Alerting UI enables you to manage alerts, silences, and alerting rules.

* *Alerting rules*. Alerting rules contain a set of conditions that outline a particular state within a cluster. Alerts are triggered when those conditions are true. An alerting rule can be assigned a severity that defines how the alerts are routed.
* *Alerts*. An alert is fired when the conditions defined in an alerting rule are true. Alerts provide a notification that a set of circumstances are apparent within an {product-title} cluster.
* *Silences*. A silence can be applied to an alert to prevent notifications from being sent when the conditions for an alert are true. You can mute an alert after the initial notification, while you work on resolving the underlying issue.

[NOTE]
====
The alerts, silences, and alerting rules that are available in the Alerting UI relate to the projects that you have access to. For example, if you are logged in as a user with the `cluster-admin` role, you can access all alerts, silences, and alerting rules.

If you are a non-administrator user, you can create and silence alerts if you are assigned the following user roles:

* The `cluster-monitoring-view` cluster role, which allows you to access Alertmanager
* The `monitoring-alertmanager-edit` role, which permits you to create and silence alerts in the *Administrator* perspective in the web console
* The `monitoring-rules-edit` cluster role, which permits you to create and silence alerts in the *Developer* perspective in the web console
====

// Accessing the Alerting UI in the Administrator and Developer perspectives
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc
// * logging/logging_alerts/log-storage-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="monitoring-accessing-the-alerting-ui_{context}"]
= Accessing the Alerting UI in the Administrator and Developer perspectives

The Alerting UI is accessible through the Administrator perspective and the Developer perspective in the {product-title} web console.

* In the *Administrator* perspective, select *Observe* -> *Alerting*. The three main pages in the Alerting UI in this perspective are the *Alerts*, *Silences*, and *Alerting Rules* pages.

//Next to the title of each of these pages is a link to the Alertmanager interface.

* In the *Developer* perspective, select *Observe* -> *<project_name>* -> *Alerts*. In this perspective, alerts, silences, and alerting rules are all managed from the *Alerts* page. The results shown in the *Alerts* page are specific to the selected project.

[NOTE]
====
In the *Developer* perspective, you can select from core {product-title} and user-defined projects that you have access to in the *Project:* list. However, alerts, silences, and alerting rules relating to core {product-title} projects are not displayed if you are not logged in as a cluster administrator.
====

:leveloffset: 1

// Searching and filtering alerts, silences, and alerting rules
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: CONCEPT
[id="searching-alerts-silences-and-alerting-rules_{context}"]
= Searching and filtering alerts, silences, and alerting rules

You can filter the alerts, silences, and alerting rules that are displayed in the Alerting UI. This section provides a description of each of the available filtering options.

[discrete]
== Understanding alert filters

In the *Administrator* perspective, the *Alerts* page in the Alerting UI provides details about alerts relating to default {product-title} and user-defined projects. The page includes a summary of severity, state, and source for each alert. The time at which an alert went into its current state is also shown.

You can filter by alert state, severity, and source. By default, only *Platform* alerts that are *Firing* are displayed. The following describes each alert filtering option:

* *Alert State* filters:
** *Firing*. The alert is firing because the alert condition is true and the optional `for` duration has passed. The alert will continue to fire as long as the condition remains true.
** *Pending*. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires.
** *Silenced*. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications will not be sent for alerts that match all the listed values or regular expressions.

* *Severity* filters:
** *Critical*. The condition that triggered the alert could have a critical impact. The alert requires immediate attention when fired and is typically paged to an individual or to a critical response team.
** *Warning*. The alert provides a warning notification about something that might require attention to prevent a problem from occurring. Warnings are typically routed to a ticketing system for non-immediate review.
** *Info*. The alert is provided for informational purposes only.
** *None*. The alert has no defined severity.
** You can also create custom severity definitions for alerts relating to user-defined projects.

* *Source* filters:
** *Platform*. Platform-level alerts relate only to default {product-title} projects. These projects provide core {product-title} functionality.
** *User*. User alerts relate to user-defined projects. These alerts are user-created and are customizable. User-defined workload monitoring can be enabled postinstallation to provide observability into your own workloads.

[discrete]
== Understanding silence filters

In the *Administrator* perspective, the *Silences* page in the Alerting UI provides details about silences applied to alerts in default {product-title} and user-defined projects. The page includes a summary of the state of each silence and the time at which a silence ends.

You can filter by silence state. By default, only *Active* and *Pending* silences are displayed. The following describes each silence state filter option:

* *Silence State* filters:
** *Active*. The silence is active and the alert will be muted until the silence is expired.
** *Pending*. The silence has been scheduled and it is not yet active.
** *Expired*. The silence has expired and notifications will be sent if the conditions for an alert are true.

[discrete]
== Understanding alerting rule filters

In the *Administrator* perspective, the *Alerting Rules* page in the Alerting UI provides details about alerting rules relating to default {product-title} and user-defined projects. The page includes a summary of the state, severity, and source for each alerting rule.

You can filter alerting rules by alert state, severity, and source. By default, only *Platform* alerting rules are displayed. The following describes each alerting rule filtering option:

* *Alert State* filters:
** *Firing*. The alert is firing because the alert condition is true and the optional `for` duration has passed. The alert will continue to fire as long as the condition remains true.
** *Pending*. The alert is active but is waiting for the duration that is specified in the alerting rule before it fires.
** *Silenced*. The alert is now silenced for a defined time period. Silences temporarily mute alerts based on a set of label selectors that you define. Notifications will not be sent for alerts that match all the listed values or regular expressions.
** *Not Firing*. The alert is not firing.

* *Severity* filters:
** *Critical*. The conditions defined in the alerting rule could have a critical impact. When true, these conditions require immediate attention. Alerts relating to the rule are typically paged to an individual or to a critical response team.
** *Warning*. The conditions defined in the alerting rule might require attention to prevent a problem from occurring. Alerts relating to the rule are typically routed to a ticketing system for non-immediate review.
** *Info*. The alerting rule provides informational alerts only.
** *None*. The alerting rule has no defined severity.
** You can also create custom severity definitions for alerting rules relating to user-defined projects.

* *Source* filters:
** *Platform*. Platform-level alerting rules relate only to default {product-title} projects. These projects provide core {product-title} functionality.
** *User*. User-defined workload alerting rules relate to user-defined projects. These alerting rules are user-created and are customizable. User-defined workload monitoring can be enabled postinstallation to provide observability into your own workloads.

[discrete]
== Searching and filtering alerts, silences, and alerting rules in the Developer perspective

In the *Developer* perspective, the Alerts page in the Alerting UI provides a combined view of alerts and silences relating to the selected project. A link to the governing alerting rule is provided for each displayed alert.

In this view, you can filter by alert state and severity. By default, all alerts in the selected project are displayed if you have permission to access the project. These filters are the same as those described for the *Administrator* perspective.

:leveloffset: 1

// Getting information about alerts, silences and alerting rules
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="getting-information-about-alerts-silences-and-alerting-rules_{context}"]
= Getting information about alerts, silences, and alerting rules

The Alerting UI provides detailed information about alerts and their governing alerting rules and silences.

.Prerequisites

* You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.

.Procedure

*To obtain information about alerts in the Administrator perspective*:

. Open the {product-title} web console and navigate to the *Observe* -> *Alerting* -> *Alerts* page.

. Optional: Search for alerts by name using the *Name* field in the search list.

. Optional: Filter alerts by state, severity, and source by selecting filters in the *Filter* list.

. Optional: Sort the alerts by clicking one or more of the *Name*, *Severity*, *State*, and *Source* column headers.

. Select the name of an alert to navigate to its *Alert Details* page. The page includes a graph that illustrates alert time series data. It also provides information about the alert, including:
+
--
** A description of the alert
** Messages associated with the alerts
** Labels attached to the alert
** A link to its governing alerting rule
** Silences for the alert, if any exist
--

*To obtain information about silences in the Administrator perspective*:

. Navigate to the *Observe* -> *Alerting* -> *Silences* page.

. Optional: Filter the silences by name using the *Search by name* field.

. Optional: Filter silences by state by selecting filters in the *Filter* list. By default, *Active* and *Pending* filters are applied.

. Optional: Sort the silences by clicking one or more of the *Name*, *Firing Alerts*, and *State* column headers.

. Select the name of a silence to navigate to its *Silence Details* page. The page includes the following details:
+
--
* Alert specification
* Start time
* End time
* Silence state
* Number and list of firing alerts
--

*To obtain information about alerting rules in the Administrator perspective*:

. Navigate to the *Observe* -> *Alerting* -> *Alerting Rules* page.

. Optional: Filter alerting rules by state, severity, and source by selecting filters in the *Filter* list.

. Optional: Sort the alerting rules by clicking one or more of the *Name*, *Severity*, *Alert State*, and *Source* column headers.

. Select the name of an alerting rule to navigate to its *Alerting Rule Details* page. The page provides the following details about the alerting rule:
+
--
** Alerting rule name, severity, and description
** The expression that defines the condition for firing the alert
** The time for which the condition should be true for an alert to fire
** A graph for each alert governed by the alerting rule, showing the value with which the alert is firing
** A table of all alerts governed by the alerting rule
--

*To obtain information about alerts, silences, and alerting rules in the Developer perspective*:

. Navigate to the *Observe* -> *<project_name>* -> *Alerts* page.

. View details for an alert, silence, or an alerting rule:

* *Alert Details* can be viewed by selecting *>* to the left of an alert name and then selecting the alert in the list.

* *Silence Details* can be viewed by selecting a silence in the *Silenced By* section of the *Alert Details* page. The *Silence Details* page includes the following information:
+
--
* Alert specification
* Start time
* End time
* Silence state
* Number and list of firing alerts
--

* *Alerting Rule Details* can be viewed by selecting *View Alerting Rule* in the {kebab} menu on the right of an alert in the *Alerts* page.

[NOTE]
====
Only alerts, silences, and alerting rules relating to the selected project are displayed in the *Developer* perspective.
====

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* See the link:https://github.com/openshift/runbooks/tree/master/alerts/cluster-monitoring-operator[Cluster Monitoring Operator runbooks] to help diagnose and resolve issues that trigger specific {product-title} monitoring alerts.

// Managing silences
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: CONCEPT
[id="managing-silences_{context}"]
= Managing silences

You can create a silence for an alert in the {product-title} web console in both the *Administrator* and *Developer* perspectives.
After you create a silence, you will not receive notifications about an alert when the alert fires.

Creating silences is useful in scenarios where you have received an initial alert notification, and you do not want to receive further notifications during the time in which you resolve the underlying issue causing the alert to fire.

When creating a silence, you must specify whether it becomes active immediately or at a later time. You must also set a duration period after which the silence expires.

After you create silences, you can view, edit, and expire them.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="silencing-alerts_{context}"]
= Silencing alerts

You can silence a specific alert or silence alerts that match a specification that you define.

.Prerequisites

* If you are a cluster administrator, you have access to the cluster as a user with the `cluster-admin` role.
* If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
** The `cluster-monitoring-view` cluster role, which allows you to access Alertmanager.
** The `monitoring-alertmanager-edit` role, which permits you to create and silence alerts in the *Administrator* perspective in the web console.
** The `monitoring-rules-edit` cluster role, which permits you to create and silence alerts in the *Developer* perspective in the web console.

.Procedure

To silence a specific alert in the *Administrator* perspective:

. Go to *Observe* -> *Alerting* -> *Alerts* in the {product-title} web console.

. For the alert that you want to silence, click {kebab} and select *Silence alert* to open the *Silence alert* page with a default configuration for the chosen alert.

. Optional: Change the default configuration details for the silence.
+
[NOTE]
====
You must add a comment before saving a silence.
====

. To save the silence, click *Silence*.

To silence a specific alert in the *Developer* perspective:

. Go to *Observe* -> *<project_name>* -> *Alerts* in the {product-title} web console.

. If necessary, expand the details for the alert by selecting *>* next to the alert name.

. Click the alert message in the expanded view to open the *Alert details* page for the alert.

. Click *Silence alert* to open the *Silence alert* page with a default configuration for the alert.

. Optional: Change the default configuration details for the silence.
+
[NOTE]
====
You must add a comment before saving a silence.
====

. To save the silence, click *Silence*.

To silence a set of alerts by creating a silence configuration in the *Administrator* perspective:

. Go to *Observe* -> *Alerting* -> *Silences* in the {product-title} web console.

. Click *Create silence*.

. On the *Create silence* page, set the schedule, duration, and label details for an alert.
+
[NOTE]
====
You must add a comment before saving a silence.
====

. To create silences for alerts that match the labels that you entered, click *Silence*.

To silence a set of alerts by creating a silence configuration in the *Developer* perspective:

. Go to *Observe* -> *<project_name>* -> *Silences* in the {product-title} web console.

. Click *Create silence*.

. On the *Create silence* page, set the duration and label details for an alert.
+
[NOTE]
====
You must add a comment before saving a silence.
====

. To create silences for alerts that match the labels that you entered, click *Silence*.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="editing-silences_{context}"]
= Editing silences

You can edit a silence, which expires the existing silence and creates a new one with the changed configuration.

.Prerequisites

* If you are a cluster administrator, you have access to the cluster as a user with the `cluster-admin` role.
* If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
** The `cluster-monitoring-view` cluster role, which allows you to access Alertmanager.
** The `monitoring-alertmanager-edit` role, which permits you to create and silence alerts in the *Administrator* perspective in the web console.
** The `monitoring-rules-edit` cluster role, which permits you to create and silence alerts in the *Developer* perspective in the web console.

.Procedure

To edit a silence in the *Administrator* perspective:

. Go to *Observe* -> *Alerting* -> *Silences*.

. For the silence you want to modify, click {kebab} and select *Edit silence*.
+
Alternatively, you can click *Actions* and select *Edit silence* on the *Silence details* page for a silence.

. On the *Edit silence* page, make changes and click *Silence*. Doing so expires the existing silence and creates one with the updated configuration.

To edit a silence in the *Developer* perspective:

. Go to *Observe* -> *<project_name>* -> *Silences*.

. For the silence you want to modify, click {kebab} and select *Edit silence*.
+
Alternatively, you can click *Actions* and select *Edit silence* on the *Silence details* page for a silence.

. On the *Edit silence* page, make changes and click *Silence*. Doing so expires the existing silence and creates one with the updated configuration.


:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="expiring-silences_{context}"]
= Expiring silences

You can expire a single silence or multiple silences. Expiring a silence deactivates it permanently.

[NOTE]
====
You cannot delete expired, silenced alerts.
Expired silences older than 120 hours are garbage collected.
====

.Prerequisites

* If you are a cluster administrator, you have access to the cluster as a user with the `cluster-admin` role.
* If you are a non-administrator user, you have access to the cluster as a user with the following user roles:
** The `cluster-monitoring-view` cluster role, which allows you to access Alertmanager.
** The `monitoring-alertmanager-edit` role, which permits you to create and silence alerts in the *Administrator* perspective in the web console.
** The `monitoring-rules-edit` cluster role, which permits you to create and silence alerts in the *Developer* perspective in the web console.

.Procedure

To expire a silence or silences in the *Administrator* perspective:

. Go to *Observe* -> *Alerting* -> *Silences*.

. For the silence or silences you want to expire, select the checkbox in the corresponding row.

. Click *Expire 1 silence* to expire a single selected silence or *Expire _<n>_ silences* to expire multiple selected silences, where _<n>_ is the number of silences you selected.
+
Alternatively, to expire a single silence you can click *Actions* and select *Expire silence* on the *Silence details* page for a silence.

To expire a silence in the *Developer* perspective:

. Go to *Observe* -> *<project_name>* -> *Silences*.

. For the silence or silences you want to expire, select the checkbox in the corresponding row.

. Click *Expire 1 silence* to expire a single selected silence or *Expire _<n>_ silences* to expire multiple selected silences, where _<n>_ is the number of silences you selected.
+
Alternatively, to expire a single silence you can click *Actions* and select *Expire silence* on the *Silence details* page for a silence.

:leveloffset: 1

// Managing core platform alerting rules
// Tech Preview features are not documented in the ROSA/OSD docs. However, even when GA, ROSA/OSD generally doesn't include information about core platform monitoring.
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: CONCEPT
[id="managing-core-platform-alerting-rules_{context}"]
= Managing alerting rules for core platform monitoring

{product-title} {product-version} monitoring ships with a large set of default alerting rules for platform metrics.
As a cluster administrator, you can customize this set of rules in two ways:

* Modify the settings for existing platform alerting rules by adjusting thresholds or by adding and modifying labels.
For example, you can change the `severity` label for an alert from `warning` to `critical` to help you route and triage issues flagged by an alert.

* Define and add new custom alerting rules by constructing a query expression based on core platform metrics in the `openshift-monitoring` namespace.

.Core platform alerting rule considerations

* New alerting rules must be based on the default {product-title} monitoring metrics.

* You can only add and modify alerting rules. You cannot create new recording rules or modify existing recording rules.

* If you modify existing platform alerting rules by using an `AlertRelabelConfig` object, your modifications are not reflected in the Prometheus alerts API.
Therefore, any dropped alerts still appear in the {product-title} web console even though they are no longer forwarded to Alertmanager.
Additionally, any modifications to alerts, such as a changed `severity` label, do not appear in the web console.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: CONCEPT
[id="tips-for-optimizing-alerting-rules-for-core-platform-monitoring_{context}"]
= Tips for optimizing alerting rules for core platform monitoring

If you customize core platform alerting rules to meet your organization's specific needs, follow these guidelines to help ensure that the customized rules are efficient and effective.

* *Minimize the number of new rules*.
Create only rules that are essential to your specific requirements.
By minimizing the number of rules, you create a more manageable and focused alerting system in your monitoring environment.

* *Focus on symptoms rather than causes*.
Create rules that notify users of symptoms instead of underlying causes.
This approach ensures that users are promptly notified of a relevant symptom so that they can investigate the root cause after an alert has triggered.
This tactic also significantly reduces the overall number of rules you need to create.

* *Plan and assess your needs before implementing changes*.
First, decide what symptoms are important and what actions you want users to take if these symptoms occur.
Then, assess existing rules and decide if you can modify any of them to meet your needs instead of creating entirely new rules for each symptom.
By modifying existing rules and creating new ones judiciously, you help to streamline your alerting system.

* *Provide clear alert messaging*.
When you create alert messages, describe the symptom, possible causes, and recommended actions.
Include unambiguous, concise explanations along with troubleshooting steps or links to more information.
Doing so helps users quickly assess the situation and respond appropriately.

* *Include severity levels*.
Assign severity levels to your rules to indicate how a user needs to react when a symptom occurs and triggers an alert.
For example, classifying an alert as *Critical* signals that an individual or a critical response team needs to respond immediately.
By defining severity levels, you help users know how to respond to an alert and help ensure that the most urgent issues receive prompt attention.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-new-alerting-rules_{context}"]
= Creating new alerting rules

As a cluster administrator, you can create new alerting rules based on platform metrics.
These alerting rules trigger alerts based on the values of chosen metrics.

[NOTE]
====
If you create a customized `AlertingRule` resource based on an existing platform alerting rule, silence the original alert to avoid receiving conflicting alerts.
====

.Prerequisites

* You have access to the cluster as a user that has the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Create a new YAML configuration file named `example-alerting-rule.yaml` in the `openshift-monitoring` namespace.

. Add an `AlertingRule` resource to the YAML file.
The following example creates a new alerting rule named `example`, similar to the default `watchdog` alert:
+
[source,yaml]
----
apiVersion: monitoring.openshift.io/v1
kind: AlertingRule
metadata:
  name: example
  namespace: openshift-monitoring
spec:
  groups:
  - name: example-rules
    rules:
    - alert: ExampleAlert <1>
      expr: vector(1) <2>
----
<1> The name of the alerting rule you want to create.
<2> The PromQL query expression that defines the new rule.

. Apply the configuration file to the cluster:
+
[source,terminal]
----
$ oc apply -f example-alerting-rule.yaml
----

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="modifying-core-platform-alerting-rules_{context}"]
= Modifying core platform alerting rules

As a cluster administrator, you can modify core platform alerts before Alertmanager routes them to a receiver.
For example, you can change the severity label of an alert, add a custom label, or exclude an alert from being sent to Alertmanager.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Create a new YAML configuration file named `example-modified-alerting-rule.yaml` in the `openshift-monitoring` namespace.

. Add an `AlertRelabelConfig` resource to the YAML file.
The following example modifies the `severity` setting to `critical` for the default platform `watchdog` alerting rule:
+
[source,yaml]
----
apiVersion: monitoring.openshift.io/v1
kind: AlertRelabelConfig
metadata:
  name: watchdog
  namespace: openshift-monitoring
spec:
  configs:
  - sourceLabels: [alertname,severity] <1>
    regex: "Watchdog;none" <2>
    targetLabel: severity <3>
    replacement: critical <4>
    action: Replace <5>
----
<1> The source labels for the values you want to modify.
<2> The regular expression against which the value of `sourceLabels` is matched.
<3> The target label of the value you want to modify.
<4> The new value to replace the target label.
<5> The relabel action that replaces the old value based on regex matching.
The default action is `Replace`.
Other possible values are `Keep`, `Drop`, `HashMod`, `LabelMap`, `LabelDrop`, and `LabelKeep`.

. Apply the configuration file to the cluster:
+
[source,terminal]
----
$ oc apply -f example-modified-alerting-rule.yaml
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* See xref:monitoring-overview[Monitoring overview] for details about {product-title} {product-version} monitoring architecture.
* See the link:https://prometheus.io/docs/alerting/alertmanager/[Alertmanager documentation] for information about alerting rules.
* See the link:https://prometheus.io/docs/prometheus/latest/configuration/configuration/#relabel_config[Prometheus relabeling documentation] for information about how relabeling works.
* See the link:https://prometheus.io/docs/practices/alerting/[Prometheus alerting documentation] for further guidelines on optimizing alerts.

// Managing alerting rules for user-defined projects
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc
//

:_mod-docs-content-type: CONCEPT
[id="managing-alerting-rules-for-user-defined-projects_{context}"]
= Managing alerting rules for user-defined projects

{product-title} monitoring ships with a set of default alerting rules. As a cluster administrator, you can view the default alerting rules.

In {product-title} {product-version}, you can create, view, edit, and remove alerting rules in user-defined projects.


.Alerting rule considerations

* The default alerting rules are used specifically for the {product-title} cluster.

* Some alerting rules intentionally have identical names. They send alerts about the same event with different thresholds, different severity, or both.

* Inhibition rules prevent notifications for lower severity alerts that are firing when a higher severity alert is also firing.

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: CONCEPT
[id="Optimizing-alerting-for-user-defined-projects_{context}"]
= Optimizing alerting for user-defined projects

You can optimize alerting for your own projects by considering the following recommendations when creating alerting rules:

* *Minimize the number of alerting rules that you create for your project*. Create alerting rules that notify you of conditions that impact you. It is more difficult to notice relevant alerts if you generate many alerts for conditions that do not impact you.

* *Create alerting rules for symptoms instead of causes*. Create alerting rules that notify you of conditions regardless of the underlying cause. The cause can then be investigated. You will need many more alerting rules if each relates only to a specific cause. Some causes are then likely to be missed.

* *Plan before you write your alerting rules*. Determine what symptoms are important to you and what actions you want to take if they occur. Then build an alerting rule for each symptom.

* *Provide clear alert messaging*. State the symptom and recommended actions in the alert message.

* *Include severity levels in your alerting rules*. The severity of an alert depends on how you need to react if the reported symptom occurs. For example, a critical alert should be triggered if a symptom requires immediate attention by an individual or a critical response team.

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* See the link:https://prometheus.io/docs/practices/alerting/[Prometheus alerting documentation] for further guidelines on optimizing alerts
* See xref:monitoring-overview[Monitoring overview] for details about {product-title} {product-version} monitoring architecture

// creating alerting rules for user defined projects
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: CONCEPT
[id="about-creating-alerting-rules-for-user-defined-projects_{context}"]
= About creating alerting rules for user-defined projects

If you create alerting rules for a user-defined project, consider the following key behaviors and important limitations when you define the new rules:

* A user-defined alerting rule can include metrics exposed by its own project in addition to the default metrics from core platform monitoring.
You cannot include metrics from another user-defined project.
+
For example, an alerting rule for the `ns1` user-defined project can use metrics exposed by the `ns1` project in addition to core platform metrics, such as CPU and memory metrics.
However, the rule cannot include metrics from a different `ns2` user-defined project.

* To reduce latency and to minimize the load on core platform monitoring components, you can add the `openshift.io/prometheus-rule-evaluation-scope: leaf-prometheus` label to a rule.
This label forces only the Prometheus instance deployed in the `openshift-user-workload-monitoring` project to evaluate the alerting rule and prevents the Thanos Ruler instance from doing so.
+
[IMPORTANT]
====
If an alerting rule has this label, your alerting rule can use only those metrics exposed by your user-defined project.
Alerting rules you create based on default platform metrics might not trigger alerts.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-alerting-rules-for-user-defined-projects_{context}"]
= Creating alerting rules for user-defined projects

You can create alerting rules for user-defined projects. Those alerting rules will trigger alerts based on the values of the chosen metrics.

.Prerequisites

* You have enabled monitoring for user-defined projects.
* You are logged in as a user that has the `monitoring-rules-edit` cluster role for the project where you want to create an alerting rule.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Create a YAML file for alerting rules. In this example, it is called `example-app-alerting-rule.yaml`.

. Add an alerting rule configuration to the YAML file. For example:
+
[NOTE]
====
When you create an alerting rule, a project label is enforced on it if a rule with the same name exists in another project.
====
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: example-alert
  namespace: ns1
spec:
  groups:
  - name: example
    rules:
    - alert: VersionAlert
      expr: version{job="prometheus-example-app"} == 0
----
+
This configuration creates an alerting rule named `example-alert`. The alerting rule fires an alert when the `version` metric exposed by the sample service becomes `0`.

. Apply the configuration file to the cluster:
+
[source,terminal]
----
$ oc apply -f example-app-alerting-rule.yaml
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources
* See xref:monitoring-overview[Monitoring overview] for details about {product-title} {product-version} monitoring architecture.

:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="accessing-alerting-rules-for-your-project_{context}"]
= Accessing alerting rules for user-defined projects

To list alerting rules for a user-defined project, you must have been assigned the `monitoring-rules-view` cluster role for the project.

.Prerequisites

* You have enabled monitoring for user-defined projects.
* You are logged in as a user that has the `monitoring-rules-view` cluster role for your project.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. To list alerting rules in `<project>`:
+
[source,terminal]
----
$ oc -n <project> get prometheusrule
----

. To list the configuration of an alerting rule, run the following:
+
[source,terminal]
----
$ oc -n <project> get prometheusrule <rule> -o yaml
----

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="listing-alerting-rules-for-all-projects-in-a-single-view_{context}"]
= Listing alerting rules for all projects in a single view

As a cluster administrator,
you can list alerting rules for core {product-title} and user-defined projects together in a single view.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. In the *Administrator* perspective, navigate to *Observe* -> *Alerting* -> *Alerting rules*.

. Select the *Platform* and *User* sources in the *Filter* drop-down menu.
+
[NOTE]
====
The *Platform* source is selected by default.
====

:leveloffset: 1
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="removing-alerting-rules-for-user-defined-projects_{context}"]
= Removing alerting rules for user-defined projects

You can remove alerting rules for user-defined projects.

.Prerequisites

* You have enabled monitoring for user-defined projects.
* You are logged in as a user that has the `monitoring-rules-edit` cluster role for the project where you want to create an alerting rule.
* You have installed the OpenShift CLI (`oc`).

.Procedure

* To remove rule `<foo>` in `<namespace>`, run the following:
+
[source,terminal]
----
$ oc -n <namespace> delete prometheusrule <foo>
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See the link:https://prometheus.io/docs/alerting/alertmanager/[Alertmanager documentation]

// Sending notifications to external systems
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc
// * post_installation_configuration/configuring-alert-notifications.adoc

:_mod-docs-content-type: CONCEPT
[id="sending-notifications-to-external-systems_{context}"]
= Sending notifications to external systems

In {product-title} {product-version}, firing alerts can be viewed in the Alerting UI. Alerts are not configured by default to be sent to any notification systems. You can configure {product-title} to send alerts to the following receiver types:

* PagerDuty
* Webhook
* Email
* Slack

Routing alerts to receivers enables you to send timely notifications to the appropriate teams when failures occur. For example, critical alerts require immediate attention and are typically paged to an individual or a critical response team. Alerts that provide non-critical warning notifications might instead be routed to a ticketing system for non-immediate review.

.Checking that alerting is operational by using the watchdog alert

{product-title} monitoring includes a watchdog alert that fires continuously. Alertmanager repeatedly sends watchdog alert notifications to configured notification providers. The provider is usually configured to notify an administrator when it stops receiving the watchdog alert. This mechanism helps you quickly identify any communication issues between Alertmanager and the notification provider.

:leveloffset: 1
// Configuring alert receivers
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc
// * post_installation_configuration/configuring-alert-notifications.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-alert-receivers_{context}"]
= Configuring alert receivers

You can configure alert receivers to ensure that you learn about important issues with your cluster.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.

.Procedure

. In the *Administrator* perspective, navigate to *Administration* -> *Cluster Settings* -> *Configuration* -> *Alertmanager*.
+
[NOTE]
====
Alternatively, you can navigate to the same page through the notification drawer. Select the bell icon at the top right of the {product-title} web console and choose *Configure* in the *AlertmanagerReceiverNotConfigured* alert.
====

. Select *Create Receiver* in the *Receivers* section of the page.

. In the *Create Receiver* form, add a *Receiver Name* and choose a *Receiver Type* from the list.

. Edit the receiver configuration:
+
* For PagerDuty receivers:
+
.. Choose an integration type and add a PagerDuty integration key.
+
.. Add the URL of your PagerDuty installation.
+
.. Select *Show advanced configuration* if you want to edit the client and incident details or the severity specification.
+
* For webhook receivers:
+
.. Add the endpoint to send HTTP POST requests to.
+
.. Select *Show advanced configuration* if you want to edit the default option to send resolved alerts to the receiver.
+
* For email receivers:
+
.. Add the email address to send notifications to.
+
.. Add SMTP configuration details, including the address to send notifications from, the smarthost and port number used for sending emails, the hostname of the SMTP server, and authentication details.
+
.. Choose whether TLS is required.
+
.. Select *Show advanced configuration* if you want to edit the default option not to send resolved alerts to the receiver or edit the body of email notifications configuration.
+
* For Slack receivers:
+
.. Add the URL of the Slack webhook.
+
.. Add the Slack channel or user name to send notifications to.
+
.. Select *Show advanced configuration* if you want to edit the default option not to send resolved alerts to the receiver or edit the icon and username configuration. You can also choose whether to find and link channel names and usernames.

. By default, firing alerts with labels that match all of the selectors will be sent to the receiver. If you want label values for firing alerts to be matched exactly before they are sent to the receiver:
.. Add routing label names and values in the *Routing Labels* section of the form.
+
.. Select *Regular Expression* if want to use a regular expression.
+
.. Select *Add Label* to add further routing labels.

. Select *Create* to create the receiver.

:leveloffset: 1
// Creating alert routing for user-defined projects
:leveloffset: +2

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-alert-routing-for-user-defined-projects_{context}"]
= Creating alert routing for user-defined projects

[role="_abstract"]
If you are a non-administrator user who has been given the `alert-routing-edit` cluster role, you can create or edit alert routing for user-defined projects.

.Prerequisites

* A cluster administrator has enabled monitoring for user-defined projects.
* A cluster administrator has enabled alert routing for user-defined projects.
* You are logged in as a user that has the `alert-routing-edit` cluster role for the project for which you want to create alert routing.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Create a YAML file for alert routing. The example in this procedure uses a file called `example-app-alert-routing.yaml`.

. Add an `AlertmanagerConfig` YAML definition to the file. For example:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1beta1
kind: AlertmanagerConfig
metadata:
  name: example-routing
  namespace: ns1
spec:
  route:
    receiver: default
    groupBy: [job]
  receivers:
  - name: default
    webhookConfigs:
    - url: https://example.org/post
----
+
[NOTE]
====
For user-defined alerting rules, user-defined routing is scoped to the namespace in which the resource is defined.
For example, a routing configuration defined in the `AlertmanagerConfig` object for namespace `ns1` only applies to `PrometheusRules` resources in the same namespace.
====
+
. Save the file.

. Apply the resource to the cluster:
+
[source,terminal]
----
$ oc apply -f example-app-alert-routing.yaml
----
+
The configuration is automatically applied to the Alertmanager pods.

:leveloffset: 1

// Applying a custom Alertmanager configuration
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="applying-custom-alertmanager-configuration_{context}"]
= Applying a custom Alertmanager configuration

You can overwrite the default Alertmanager configuration by editing the `alertmanager-main` secret in the `openshift-monitoring` namespace for the platform instance of Alertmanager.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.

.Procedure

To change the Alertmanager configuration from the CLI:

. Print the currently active Alertmanager configuration into file `alertmanager.yaml`:
+
[source,terminal]
----
$ oc -n openshift-monitoring get secret alertmanager-main --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode > alertmanager.yaml
----
+
. Edit the configuration in `alertmanager.yaml`:
+
[source,yaml]
----
global:
  resolve_timeout: 5m
route:
  group_wait: 30s <1>
  group_interval: 5m <2>
  repeat_interval: 12h <3>
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=<your_service>" <4>
    routes:
    - matchers:
      - <your_matching_rules> <5>
      receiver: <receiver> <6>
receivers:
- name: default
- name: watchdog
- name: <receiver>
#  <receiver_configuration>
----
<1> The `group_wait` value specifies how long Alertmanager waits before sending an initial notification for a group of alerts.
This value controls how long Alertmanager waits while collecting initial alerts for the same group before sending a notification.
<2> The `group_interval` value specifies how much time must elapse before Alertmanager sends a notification about new alerts added to a group of alerts for which an initial notification was already sent.
<3> The `repeat_interval` value specifies the minimum amount of time that must pass before an alert notification is repeated.
If you want a notification to repeat at each group interval, set the `repeat_interval` value to less than the `group_interval` value.
However, the repeated notification can still be delayed, for example, when certain Alertmanager pods are restarted or rescheduled.
<4> The `service` value specifies the service that fires the alerts.
<5> The `<your_matching_rules>` value specifies the target alerts.
<6> The `receiver` value specifies the receiver to use for the alert.
+
[NOTE]
====
Use the `matchers` key name to indicate the matchers that an alert has to fulfill to match the node.
Do not use the `match` or `match_re` key names, which are both deprecated and planned for removal in a future release.

In addition, if you define inhibition rules, use the `target_matchers` key name to indicate the target matchers and the `source_matchers` key name to indicate the source matchers.
Do not use the `target_match`, `target_match_re`, `source_match`, or `source_match_re` key names, which are deprecated and planned for removal in a future release.
====
+
The following Alertmanager configuration example configures PagerDuty as an alert receiver:
+
[source,yaml]
----
global:
  resolve_timeout: 5m
route:
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 12h
  receiver: default
  routes:
  - matchers:
    - "alertname=Watchdog"
    repeat_interval: 2m
    receiver: watchdog
  - matchers:
    - "service=example-app"
    routes:
    - matchers:
      - "severity=critical"
      receiver: team-frontend-page*
receivers:
- name: default
- name: watchdog
- name: team-frontend-page
  pagerduty_configs:
  - service_key: "_your-key_"
----
+
With this configuration, alerts of `critical` severity that are fired by the `example-app` service are sent using the `team-frontend-page` receiver. Typically these types of alerts would be paged to an individual or a critical response team.
+
. Apply the new configuration in the file:
+
[source,terminal]
----
$ oc -n openshift-monitoring create secret generic alertmanager-main --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-monitoring replace secret --filename=-
----

To change the Alertmanager configuration from the {product-title} web console:

. Navigate to the *Administration* -> *Cluster Settings* -> *Configuration* -> *Alertmanager* -> *YAML* page of the web console.

. Modify the YAML configuration file.

. Select *Save*.

:leveloffset: 1

// Applying a custom configuration to Alertmanager for user-defined alert routing
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/managing-alerts.adoc

:_mod-docs-content-type: PROCEDURE
[id="applying-a-custom-configuration-to-alertmanager-for-user-defined-alert-routing_{context}"]
= Applying a custom configuration to Alertmanager for user-defined alert routing

If you have enabled a separate instance of Alertmanager dedicated to user-defined alert routing, you can overwrite the configuration for this instance of Alertmanager by editing the `alertmanager-user-workload` secret in the `openshift-user-workload-monitoring` namespace.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. Print the currently active Alertmanager configuration into the file `alertmanager.yaml`:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get secret alertmanager-user-workload --template='{{ index .data "alertmanager.yaml" }}' | base64 --decode > alertmanager.yaml
----
+
. Edit the configuration in `alertmanager.yaml`:
+
[source,yaml]
----
route:
  receiver: Default
  group_by:
  - name: Default
  routes:
  - matchers:
    - "service = prometheus-example-monitor" <1>
    receiver: <receiver> <2>
receivers:
- name: Default
- name: <receiver>
#  <receiver_configuration>
----
<1> Specifies which alerts match the route. This example shows all alerts that have the `service="prometheus-example-monitor"` label.
<2> Specifies the receiver to use for the alerts group.
+
. Apply the new configuration in the file:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring create secret generic alertmanager-user-workload --from-file=alertmanager.yaml --dry-run=client -o=yaml |  oc -n openshift-user-workload-monitoring replace secret --filename=-
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See link:https://www.pagerduty.com/[the PagerDuty official site] for more information on PagerDuty.
* See link:https://www.pagerduty.com/docs/guides/prometheus-integration-guide/[the PagerDuty Prometheus Integration Guide] to learn how to retrieve the `service_key`.
* See link:https://prometheus.io/docs/alerting/configuration/[Alertmanager configuration] for configuring alerting through different alert receivers.
* See xref:enabling-alert-routing-for-user-defined-projects[Enabling alert routing for user-defined projects] to learn how to enable a dedicated instance of Alertmanager for user-defined alert routing.


== Next steps

* xref:reviewing-monitoring-dashboards[Reviewing monitoring dashboards]

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="reviewing-monitoring-dashboards"]
= Reviewing monitoring dashboards
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: reviewing-monitoring-dashboards

toc::[]

{product-title} {product-version} provides a comprehensive set of monitoring dashboards that help you understand the state of cluster components and user-defined workloads.

Use the *Administrator* perspective to access dashboards for the core {product-title} components, including the following items:

* API performance
* etcd
* Kubernetes compute resources
* Kubernetes network resources
* Prometheus
* USE method dashboards relating to cluster and node performance
* Node performance metrics

.Example dashboard in the Administrator perspective
image::monitoring-dashboard-administrator.png[]

Use the *Developer* perspective to access Kubernetes compute resources dashboards that provide the following application metrics for a selected project:

* CPU usage
* Memory usage
* Bandwidth information
* Packet rate information

.Example dashboard in the Developer perspective
image::observe-dashboard-developer.png[]

[NOTE]
====
In the *Developer* perspective, you can view dashboards for only one project at a time.
====

// Reviewing monitoring dashboards as a cluster administrator
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/reviewing-monitoring-dashboards.adoc

:_mod-docs-content-type: PROCEDURE
[id="reviewing-monitoring-dashboards-admin_{context}"]
= Reviewing monitoring dashboards as a cluster administrator

In the *Administrator* perspective, you can view dashboards relating to core {product-title} cluster components.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.

.Procedure

. In the *Administrator* perspective in the {product-title} web console, navigate to *Observe* -> *Dashboards*.

. Choose a dashboard in the *Dashboard* list. Some dashboards, such as *etcd* and *Prometheus* dashboards, produce additional sub-menus when selected.

. Optional: Select a time range for the graphs in the *Time Range* list.
+
** Select a pre-defined time period.
+
** Set a custom time range by selecting *Custom time range* in the *Time Range* list.
+
.. Input or select the *From* and *To* dates and times.
+
.. Click *Save* to save the custom time range.

. Optional: Select a *Refresh Interval*.

. Hover over each of the graphs within a dashboard to display detailed information about specific items.

:leveloffset: 1

// Reviewing monitoring dashboards as a developer
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/reviewing-monitoring-dashboards.adoc

:_mod-docs-content-type: PROCEDURE
[id="reviewing-monitoring-dashboards-developer_{context}"]
= Reviewing monitoring dashboards as a developer

In the *Developer* perspective, you can view dashboards relating to a selected project. You must have access to monitor a project to view dashboard information for it.

.Prerequisites

* You have access to the cluster as a developer or as a user.
* You have view permissions for the project that you are viewing the dashboard for.

.Procedure

. In the Developer perspective in the {product-title} web console, navigate to *Observe* -> *Dashboard*.

. Select a project from the *Project:* drop-down list.

. Select a dashboard from the *Dashboard* drop-down list to see the filtered metrics.
+
[NOTE]
====
All dashboards produce additional sub-menus when selected, except *Kubernetes / Compute Resources / Namespace (Pods)*.
====
+
. Optional: Select a time range for the graphs in the *Time Range* list.
+
** Select a pre-defined time period.
+
** Set a custom time range by selecting *Custom time range* in the *Time Range* list.
+
.. Input or select the *From* and *To* dates and times.
+
.. Click *Save* to save the custom time range.

. Optional: Select a *Refresh Interval*.

. Hover over each of the graphs within a dashboard to display detailed information about specific items.

:leveloffset: 1

// This additional resource might be valid for ROSA/OSD when the Building applications content is ported.
[role="_additional-resources"]
[id="additional-resources-reviewing-monitoring-dashboards"]
.Additional resources

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#monitoring-project-and-application-metrics-using-developer-perspective[Monitoring project and application metrics using the Developer perspective]

[id="next-steps_reviewing-monitoring-dashboards"]
== Next steps

* xref:accessing-third-party-monitoring-apis[Accessing third-party monitoring APIs]

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="accessing-third-party-monitoring-apis"]
= Accessing third-party monitoring APIs
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: accessing-third-party-monitoring-apis

toc::[]

[role="_abstract"]
In {product-title} {product-version}, you can access web service APIs for some third-party monitoring components from the command line interface (CLI).

// Accessing service APIs for third-party monitoring components
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/accessing-third-party-monitoring-uis-and-apis.adoc

:_mod-docs-content-type: PROCEDURE
[id="accessing-third-party-monitoring-web-service-apis_{context}"]
= Accessing third-party monitoring web service APIs

[role="_abstract"]
You can directly access third-party web service APIs from the command line for the following monitoring stack components: Prometheus, Alertmanager, Thanos Ruler, and Thanos Querier.

The following example commands show how to query the service API receivers for Alertmanager.
This example requires that the associated user account be bound against the `monitoring-alertmanager-edit` role in the `openshift-monitoring` namespace and that the account has the privilege to view the route.
This access only supports using a Bearer Token for authentication.

[source,terminal]
----
$ oc login -u <username> -p <password>
----

[source,terminal]
----
$ host=$(oc -n openshift-monitoring get route alertmanager-main -ojsonpath={.spec.host})
----

[source,terminal]
----
$ token=$(oc whoami -t)
----

[source,terminal]
----
$ curl -H "Authorization: Bearer $token" -k "https://$host/api/v2/receivers"
----

[NOTE]
====
To access Thanos Ruler and Thanos Querier service APIs, the requesting account must have `get` permission on the namespaces resource, which can be done by granting the `cluster-monitoring-view` cluster role to the account.
====

:leveloffset: 1

// Querying metrics by using the federation endpoint for Prometheus
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/accessing-third-party-monitoring-apis.adoc

:_mod-docs-content-type: PROCEDURE
[id="monitoring-querying-metrics-by-using-the-federation-endpoint-for-prometheus_{context}"]
= Querying metrics by using the federation endpoint for Prometheus

You can use the federation endpoint to scrape platform and user-defined metrics from a network location outside the cluster.
To do so, access the Prometheus `/federate` endpoint for the cluster via an {product-title} route.

[WARNING]
====
A delay in retrieving metrics data occurs when you use federation.
This delay can affect the accuracy and timeliness of the scraped metrics.

Using the federation endpoint can also degrade the performance and scalability of your cluster, especially if you use the federation endpoint to retrieve large amounts of metrics data.
To avoid these issues, follow these recommendations:

* Do not try to retrieve all metrics data via the federation endpoint.
Query it only when you want to retrieve a limited, aggregated data set.
For example, retrieving fewer than 1,000 samples for each request helps minimize the risk of performance degradation.

* Avoid querying the federation endpoint frequently.
Limit queries to a maximum of one every 30 seconds.

If you need to forward large amounts of data outside the cluster, use remote write instead. For more information, see the _Configuring remote write storage_ section.
====

.Prerequisites

* You have installed the OpenShift CLI (`oc`).
* You have obtained the host URL for the {product-title} route.
* You have access to the cluster as a user with the `cluster-monitoring-view` cluster role or have obtained a bearer token with `get` permission on the `namespaces` resource.
+
[NOTE]
====
You can only use bearer token authentication to access the federation endpoint.
====

.Procedure

. Retrieve the bearer token:
+
[source,terminal]
----
$ token=`oc whoami -t`
----

. Query metrics from the `/federate` route.
The following example queries `up` metrics:
+
[source,terminal]
----
$ curl -G -s -k -H "Authorization: Bearer $token" \
    'https://<federation_host>/federate' \ <1>
    --data-urlencode 'match[]=up'
----
+
<1> For <federation_host>, substitute the host URL for the federation route.
+
.Example output
+
[source,terminal]
----
# TYPE up untyped
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.143.148:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035322214
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.148.166:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035338597
up{apiserver="kube-apiserver",endpoint="https",instance="10.0.173.16:6443",job="apiserver",namespace="default",service="kubernetes",prometheus="openshift-monitoring/k8s",prometheus_replica="prometheus-k8s-0"} 1 1657035343834
...
----

:leveloffset: 1

[role="_additional-resources"]
[id="additional-resources_accessing-third-party-monitoring-apis"]
== Additional resources

* xref:configuring_remote_write_storage_configuring-the-monitoring-stack[Configuring remote write storage]
* xref:managing-metrics[Managing metrics]
* xref:managing-alerts[Managing alerts]

:leveloffset!:

:leveloffset: +1

:_mod-docs-content-type: ASSEMBLY
[id="troubleshooting-monitoring-issues"]
= Troubleshooting monitoring issues
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: troubleshooting-monitoring-issues

toc::[]

Find troubleshooting steps for common issues with core platform and user-defined project monitoring.

// Investigating why user-defined project metrics are unavailable (OCP)
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/troubleshooting-monitoring-issues.adoc
// * support/troubleshooting/investigating-monitoring-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="investigating-why-user-defined-metrics-are-unavailable_{context}"]
= Investigating why user-defined project metrics are unavailable

`ServiceMonitor` resources enable you to determine how to use the metrics exposed by a service in user-defined projects. Follow the steps outlined in this procedure if you have created a `ServiceMonitor` resource but cannot see any corresponding metrics in the Metrics UI.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the OpenShift CLI (`oc`).
* You have enabled and configured monitoring for user-defined workloads.
* You have created the `user-workload-monitoring-config` `ConfigMap` object.
* You have created a `ServiceMonitor` resource.

.Procedure

. *Check that the corresponding labels match* in the service and `ServiceMonitor` resource configurations.
.. Obtain the label defined in the service. The following example queries the `prometheus-example-app` service in the `ns1` project:
+
[source,terminal]
----
$ oc -n ns1 get service prometheus-example-app -o yaml
----
+
.Example output
[source,terminal]
----
  labels:
    app: prometheus-example-app
----
+
.. Check that the `matchLabels` `app` label in the `ServiceMonitor` resource configuration matches the label output in the preceding step:
+
[source,terminal]
----
$ oc -n ns1 get servicemonitor prometheus-example-monitor -o yaml
----
+
.Example output
----
apiVersion: v1
kind: Service
# ...
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-example-app
# ...
----
+
[NOTE]
====
You can check service and `ServiceMonitor` resource labels as a developer with view permissions for the project.
====

. *Inspect the logs for the Prometheus Operator* in the `openshift-user-workload-monitoring` project.
.. List the pods in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get pods
----
+
.Example output
[source,terminal]
----
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-776fcbbd56-2nbfm   2/2     Running   0          132m
prometheus-user-workload-0             5/5     Running   1          132m
prometheus-user-workload-1             5/5     Running   1          132m
thanos-ruler-user-workload-0           3/3     Running   0          132m
thanos-ruler-user-workload-1           3/3     Running   0          132m
----
+
.. Obtain the logs from the `prometheus-operator` container in the `prometheus-operator` pod. In the following example, the pod is called `prometheus-operator-776fcbbd56-2nbfm`:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring logs prometheus-operator-776fcbbd56-2nbfm -c prometheus-operator
----
+
If there is a issue with the service monitor, the logs might include an error similar to this example:
+
[source,terminal]
----
level=warn ts=2020-08-10T11:48:20.906739623Z caller=operator.go:1829 component=prometheusoperator msg="skipping servicemonitor" error="it accesses file system via bearer token file which Prometheus specification prohibits" servicemonitor=eagle/eagle namespace=openshift-user-workload-monitoring prometheus=user-workload
----

. *Review the target status for your endpoint* on the *Metrics targets* page in the {product-title} web console UI.
.. Log in to the {product-title} web console and navigate to *Observe* → *Targets* in the *Administrator* perspective.

.. Locate the metrics endpoint in the list, and review the status of the target in the *Status* column.

.. If the *Status* is *Down*, click the URL for the endpoint to view more information on the *Target Details* page for that metrics target.

. *Configure debug level logging for the Prometheus Operator* in the `openshift-user-workload-monitoring` project.
.. Edit the `user-workload-monitoring-config` `ConfigMap` object in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring edit configmap user-workload-monitoring-config
----
+
.. Add `logLevel: debug` for `prometheusOperator` under `data/config.yaml` to set the log level to `debug`:
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
    prometheusOperator:
      logLevel: debug
# ...
----
+
.. Save the file to apply the changes.
+
[NOTE]
====
The `prometheus-operator` in the `openshift-user-workload-monitoring` project restarts automatically when you apply the log-level change.
====
+
.. Confirm that the `debug` log-level has been applied to the `prometheus-operator` deployment in the `openshift-user-workload-monitoring` project:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get deploy prometheus-operator -o yaml |  grep "log-level"
----
+
.Example output
[source,terminal]
----
        - --log-level=debug
----
+
Debug level logging will show all calls made by the Prometheus Operator.
+
.. Check that the `prometheus-operator` pod is running:
+
[source,terminal]
----
$ oc -n openshift-user-workload-monitoring get pods
----
+
[NOTE]
====
If an unrecognized Prometheus Operator `loglevel` value is included in the config map, the `prometheus-operator` pod might not restart successfully.
====
+
.. Review the debug logs to see if the Prometheus Operator is using the `ServiceMonitor` resource. Review the logs for other related errors.

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* xref:creating-user-defined-workload-monitoring-configmap_configuring-the-monitoring-stack[Creating a user-defined workload monitoring config map]
* See xref:specifying-how-a-service-is-monitored_managing-metrics[Specifying how a service is monitored] for details on how to create a `ServiceMonitor` or `PodMonitor` resource
* See xref:getting-detailed-information-about-a-target_managing-metrics[Getting detailed information about metrics targets]

// Investigating why user-defined project metrics are unavailable (OSD/ROSA)

// Determining why Prometheus is consuming a lot of disk space
:leveloffset: +1

// Module included in the following assemblies:
//
// * monitoring/troubleshooting-monitoring-issues.adoc
// * support/troubleshooting/investigating-monitoring-issues.adoc

:_mod-docs-content-type: PROCEDURE
[id="determining-why-prometheus-is-consuming-disk-space_{context}"]
= Determining why Prometheus is consuming a lot of disk space

Developers can create labels to define attributes for metrics in the form of key-value pairs. The number of potential key-value pairs corresponds to the number of possible values for an attribute. An attribute that has an unlimited number of potential values is called an unbound attribute. For example, a `customer_id` attribute is unbound because it has an infinite number of possible values.

Every assigned key-value pair has a unique time series. The use of many unbound attributes in labels can result in an exponential increase in the number of time series created. This can impact Prometheus performance and can consume a lot of disk space.

You can use the following measures when Prometheus consumes a lot of disk:

* *Check the number of scrape samples* that are being collected.

* *Check the time series database (TSDB) status using the Prometheus HTTP API* for more information about which labels are creating the most time series. Doing so requires cluster administrator privileges.

* *Reduce the number of unique time series that are created* by reducing the number of unbound attributes that are assigned to user-defined metrics.
+
[NOTE]
====
Using attributes that are bound to a limited set of possible values reduces the number of potential key-value pair combinations.
====
+
* *Enforce limits on the number of samples that can be scraped* across user-defined projects. This requires cluster administrator privileges.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have installed the OpenShift CLI (`oc`).

.Procedure

. In the *Administrator* perspective, navigate to *Observe* -> *Metrics*.

. Run the following Prometheus Query Language (PromQL) query in the *Expression* field. This returns the ten metrics that have the highest number of scrape samples:
+
[source,terminal]
----
topk(10,count by (job)({__name__=~".+"}))
----

. Investigate the number of unbound label values assigned to metrics with higher than expected scrape sample counts.
** *If the metrics relate to a user-defined project*, review the metrics key-value pairs assigned to your workload. These are implemented through Prometheus client libraries at the application level. Try to limit the number of unbound attributes referenced in your labels.

** *If the metrics relate to a core {product-title} project*, create a Red Hat support case on the link:https://access.redhat.com/[Red Hat Customer Portal].

. Review the TSDB status using the Prometheus HTTP API by running the following commands as a
cluster administrator:
+
[source,terminal]
----
$ oc login -u <username> -p <password>
----
+
[source,terminal]
----
$ host=$(oc -n openshift-monitoring get route prometheus-k8s -ojsonpath={.spec.host})
----
+
[source,terminal]
----
$ token=$(oc whoami -t)
----
+
[source,terminal]
----
$ curl -H "Authorization: Bearer $token" -k "https://$host/api/v1/status/tsdb"
----
+
.Example output
[source,terminal]
----
"status": "success",
----

:leveloffset: 1

[role="_additional-resources"]
.Additional resources

* See xref:setting-scrape-sample-and-label-limits-for-user-defined-projects_configuring-the-monitoring-stack[Setting a scrape sample limit for user-defined projects] for details on how to set a scrape sample limit and create related alerting rules
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#support-submitting-a-case_getting-support[Submitting a support case]

:leveloffset!:

:leveloffset: +1

// DO NOT EDIT THE CONTENT IN THIS FILE. It is automatically generated from the
// source code for the Cluster Monitoring Operator. Any changes made to this
// file will be overwritten when the content is re-generated. If you wish to
// make edits, read the docgen utility instructions in the source code for the
// CMO.
:_mod-docs-content-type: REFERENCE
[id="config-map-reference-for-the-cluster-monitoring-operator"]
= Config map reference for the Cluster Monitoring Operator
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: config-map-reference-for-the-cluster-monitoring-operator

toc::[]

[id="cluster-monitoring-operator-configuration-reference"]
== Cluster Monitoring Operator configuration reference

[role="_abstract"]
Parts of {product-title} cluster monitoring are configurable.
The API is accessible by setting parameters defined in various config maps.

* To configure monitoring components, edit the `ConfigMap` object named `cluster-monitoring-config` in the `openshift-monitoring` namespace.
These configurations are defined by .
* To configure monitoring components that monitor user-defined projects, edit the `ConfigMap` object named `user-workload-monitoring-config` in the `openshift-user-workload-monitoring` namespace.
These configurations are defined by .

The configuration file is always defined under the `config.yaml` key in the config map data.

[NOTE]
====
* Not all configuration parameters are exposed.
* Configuring cluster monitoring is optional.
* If a configuration does not exist or is empty, default values are used.
* If the configuration is invalid YAML data, the Cluster Monitoring Operator stops reconciling the resources and reports `Degraded=True` in the status conditions of the Operator.
====

== AdditionalAlertmanagerConfig

=== Description

The `AdditionalAlertmanagerConfig` resource defines settings for how a component communicates with additional Alertmanager instances.

=== Required
* `apiVersion`

Appears in: ,
,


[options="header"]
|===
| Property | Type | Description
|apiVersion|string|Defines the API version of Alertmanager. Possible values are `v1` or `v2`. The default is `v2`.

|bearerToken|*v1.SecretKeySelector|Defines the secret key reference containing the bearer token to use when authenticating to Alertmanager.

|pathPrefix|string|Defines the path prefix to add in front of the push endpoint path.

|scheme|string|Defines the URL scheme to use when communicating with Alertmanager instances. Possible values are `http` or `https`. The default value is `http`.

|staticConfigs|[]string|A list of statically configured Alertmanager endpoints in the form of `<hosts>:<port>`.

|timeout|*string|Defines the timeout value used when sending alerts.

|tlsConfig||Defines the TLS settings to use for Alertmanager connections.

|===

== AlertmanagerMainConfig

=== Description

The `AlertmanagerMainConfig` resource defines settings for the Alertmanager component in the `openshift-monitoring` namespace.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|*bool|A Boolean flag that enables or disables the main Alertmanager instance in the `openshift-monitoring` namespace. The default value is `true`.

|enableUserAlertmanagerConfig|bool|A Boolean flag that enables or disables user-defined namespaces to be selected for `AlertmanagerConfig` lookups. This setting only applies if the user workload monitoring instance of Alertmanager is not enabled. The default value is `false`.

|logLevel|string|Defines the log level setting for Alertmanager. The possible values are: `error`, `warn`, `info`, `debug`. The default value is `info`.

|nodeSelector|map[string]string|Defines the nodes on which the Pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the Alertmanager container.

|secrets|[]string|Defines a list of secrets to be mounted into Alertmanager. The secrets must reside within the same namespace as the Alertmanager object. They are added as volumes named `secret-<secret-name>` and mounted at `/etc/alertmanager/secrets/<secret-name>` in the `alertmanager` container of the Alertmanager pods.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|volumeClaimTemplate|*monv1.EmbeddedPersistentVolumeClaim|Defines persistent storage for Alertmanager. Use this setting to configure the persistent volume claim, including storage class, volume size, and name.

|===

== AlertmanagerUserWorkloadConfig

=== Description

The `AlertmanagerUserWorkloadConfig` resource defines the settings for the Alertmanager instance used for user-defined projects.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables a dedicated instance of Alertmanager for user-defined alerts in the `openshift-user-workload-monitoring` namespace. The default value is `false`.

|enableAlertmanagerConfig|bool|A Boolean flag to enable or disable user-defined namespaces to be selected for `AlertmanagerConfig` lookup. The default value is `false`.

|logLevel|string|Defines the log level setting for Alertmanager for user workload monitoring. The possible values are `error`, `warn`, `info`, and `debug`. The default value is `info`.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the Alertmanager container.

|secrets|[]string|Defines a list of secrets to be mounted into Alertmanager. The secrets must be located within the same namespace as the Alertmanager object. They are added as volumes named `secret-<secret-name>` and mounted at `/etc/alertmanager/secrets/<secret-name>` in the `alertmanager` container of the Alertmanager pods.

|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|volumeClaimTemplate|*monv1.EmbeddedPersistentVolumeClaim|Defines persistent storage for Alertmanager. Use this setting to configure the persistent volume claim, including storage class, volume size and name.

|===

== ClusterMonitoringConfiguration

=== Description

The `ClusterMonitoringConfiguration` resource defines settings that customize the default platform monitoring stack through the `cluster-monitoring-config` config map in the `openshift-monitoring` namespace.

[options="header"]
|===
| Property | Type | Description
|alertmanagerMain|*|`AlertmanagerMainConfig` defines settings for the Alertmanager component in the `openshift-monitoring` namespace.

|enableUserWorkload|*bool|`UserWorkloadEnabled` is a Boolean flag that enables monitoring for user-defined projects.

|k8sPrometheusAdapter|*|`K8sPrometheusAdapter` defines settings for the Prometheus Adapter component.

|kubeStateMetrics|*|`KubeStateMetricsConfig` defines settings for the `kube-state-metrics` agent.

|metricsServer|*|`MetricsServer` defines settings for the Metrics Server component.

|prometheusK8s|*|`PrometheusK8sConfig` defines settings for the Prometheus component.

|prometheusOperator|*|`PrometheusOperatorConfig` defines settings for the Prometheus Operator component.

|prometheusOperatorAdmissionWebhook|*|`PrometheusOperatorAdmissionWebhookConfig` defines settings for the admission webhook component of Prometheus Operator.

|openshiftStateMetrics|*|`OpenShiftMetricsConfig` defines settings for the `openshift-state-metrics` agent.

|telemeterClient|*|`TelemeterClientConfig` defines settings for the Telemeter Client component.

|thanosQuerier|*|`ThanosQuerierConfig` defines settings for the Thanos Querier component.

|nodeExporter||`NodeExporterConfig` defines settings for the `node-exporter` agent.

|monitoringPlugin|*|`MonitoringPluginConfig` defines settings for the monitoring `console-plugin` component.

|===

== DedicatedServiceMonitors

=== Description

[IMPORTANT]
====
This setting is deprecated and is planned to be removed in a future {product-title} version.
In the current version, this setting still exists but has no effect.
====

You can use the `DedicatedServiceMonitors` resource to configure dedicated Service Monitors for the Prometheus Adapter

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|When `enabled` is set to `true`, the Cluster Monitoring Operator (CMO) deploys a dedicated Service Monitor that exposes the kubelet `/metrics/resource` endpoint. This Service Monitor sets `honorTimestamps: true` and only keeps metrics that are relevant for the pod resource queries of Prometheus Adapter. Additionally, Prometheus Adapter is configured to use these dedicated metrics. Overall, this feature improves the consistency of Prometheus Adapter-based CPU usage measurements used by, for example, the `oc adm top pod` command or the Horizontal Pod Autoscaler.

|===

== K8sPrometheusAdapter

=== Description

The `K8sPrometheusAdapter` resource defines settings for the Prometheus Adapter component.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|audit|*Audit|Defines the audit configuration used by the Prometheus Adapter instance. Possible profile values are: `metadata`, `request`, `requestresponse`, and `none`. The default value is `metadata`.

|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `PrometheusAdapter` container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|dedicatedServiceMonitors|*|Defines dedicated service monitors.

|===

== KubeStateMetricsConfig

=== Description

The `KubeStateMetricsConfig` resource defines settings for the `kube-state-metrics` agent.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `KubeStateMetrics` container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|===

== MetricsServerConfig

=== Description

:FeatureName: Metrics Server
:leveloffset: +1

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 1

The `MetricsServerConfig` resource defines settings for the Metrics Server component. Note that this setting only applies when the `TechPreviewNoUpgrade` feature gate is enabled.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the Metrics Server container.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|===

== PrometheusOperatorAdmissionWebhookConfig

=== Description

The `PrometheusOperatorAdmissionWebhookConfig` resource defines settings for the admission webhook workload for Prometheus Operator.

Appears in: 

[options="header"]
|===
| Property | Type | Description

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `prometheus-operator-admission-webhook` container.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|===

== MonitoringPluginConfig

=== Description

The `MonitoringPluginConfig` resource defines settings for the web console plugin component in the `openshift-monitoring` namespace.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `console-plugin` container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines a pod's topology spread constraints.

|===

== NodeExporterCollectorBuddyInfoConfig

=== Description

The `NodeExporterCollectorBuddyInfoConfig` resource works as an on/off switch for the `buddyinfo` collector of the `node-exporter` agent. By default, the `buddyinfo` collector is disabled.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `buddyinfo` collector.

|===

== NodeExporterCollectorConfig

=== Description

The `NodeExporterCollectorConfig` resource defines settings for individual collectors of the `node-exporter` agent.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|cpufreq||Defines the configuration of the `cpufreq` collector, which collects CPU frequency statistics. Disabled by default.

|tcpstat||Defines the configuration of the `tcpstat` collector, which collects TCP connection statistics. Disabled by default.

|netdev||Defines the configuration of the `netdev` collector, which collects network devices statistics. Enabled by default.

|netclass||Defines the configuration of the `netclass` collector, which collects information about network devices. Enabled by default.

|buddyinfo||Defines the configuration of the `buddyinfo` collector, which collects statistics about memory fragmentation from the `node_buddyinfo_blocks` metric. This metric collects data from `/proc/buddyinfo`. Disabled by default.

|mountstats||Defines the configuration of the `mountstats` collector, which collects statistics about NFS volume I/O activities. Disabled by default.

|ksmd||Defines the configuration of the `ksmd` collector, which collects statistics from the kernel same-page merger daemon. Disabled by default.

|processes||Defines the configuration of the `processes` collector, which collects statistics from processes and threads running in the system. Disabled by default.

|systemd||Defines the configuration of the `systemd` collector, which collects statistics on the systemd daemon and its managed services. Disabled by default.

|===

== NodeExporterCollectorCpufreqConfig

=== Description

Use the `NodeExporterCollectorCpufreqConfig` resource to enable or disable the `cpufreq` collector of the `node-exporter` agent. By default, the `cpufreq` collector is disabled. Under certain circumstances, enabling the `cpufreq` collector increases CPU usage on machines with many cores. If you enable this collector and have machines with many cores, monitor your systems closely for excessive CPU usage.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `cpufreq` collector.

|===

== NodeExporterCollectorKSMDConfig

=== Description

Use the `NodeExporterCollectorKSMDConfig` resource to enable or disable the `ksmd` collector of the `node-exporter` agent. By default, the `ksmd` collector is disabled.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `ksmd` collector.

|===

== NodeExporterCollectorMountStatsConfig

=== Description

Use the `NodeExporterCollectorMountStatsConfig` resource to enable or disable the `mountstats` collector of the `node-exporter` agent. By default, the `mountstats` collector is disabled. If you enable the collector, the following metrics become available: `node_mountstats_nfs_read_bytes_total`, `node_mountstats_nfs_write_bytes_total`, and `node_mountstats_nfs_operations_requests_total`. Be aware that these metrics can have a high cardinality. If you enable this collector, closely monitor any increases in memory usage for the `prometheus-k8s` pods.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `mountstats` collector.

|===

== NodeExporterCollectorNetClassConfig

=== Description

Use the `NodeExporterCollectorNetClassConfig` resource to enable or disable the `netclass` collector of the `node-exporter` agent. By default, the `netclass` collector is enabled. If you disable this collector, these metrics become unavailable: `node_network_info`, `node_network_address_assign_type`, `node_network_carrier`, `node_network_carrier_changes_total`, `node_network_carrier_up_changes_total`, `node_network_carrier_down_changes_total`, `node_network_device_id`, `node_network_dormant`, `node_network_flags`, `node_network_iface_id`, `node_network_iface_link`, `node_network_iface_link_mode`, `node_network_mtu_bytes`, `node_network_name_assign_type`, `node_network_net_dev_group`, `node_network_speed_bytes`, `node_network_transmit_queue_length`, and `node_network_protocol_type`.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `netclass` collector.

|useNetlink|bool|A Boolean flag that activates the `netlink` implementation of the `netclass` collector. The default value is `true`, which activates the `netlink` mode. This implementation improves the performance of the `netclass` collector.

|===

== NodeExporterCollectorNetDevConfig

=== Description

Use the `NodeExporterCollectorNetDevConfig` resource to enable or disable the `netdev` collector of the `node-exporter` agent. By default, the `netdev` collector is enabled. If disabled, these metrics become unavailable: `node_network_receive_bytes_total`, `node_network_receive_compressed_total`, `node_network_receive_drop_total`, `node_network_receive_errs_total`, `node_network_receive_fifo_total`, `node_network_receive_frame_total`, `node_network_receive_multicast_total`, `node_network_receive_nohandler_total`, `node_network_receive_packets_total`, `node_network_transmit_bytes_total`, `node_network_transmit_carrier_total`, `node_network_transmit_colls_total`, `node_network_transmit_compressed_total`, `node_network_transmit_drop_total`, `node_network_transmit_errs_total`, `node_network_transmit_fifo_total`, and `node_network_transmit_packets_total`.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `netdev` collector.

|===

== NodeExporterCollectorProcessesConfig

=== Description

Use the `NodeExporterCollectorProcessesConfig` resource to enable or disable the `processes` collector of the `node-exporter` agent. If the collector is enabled, the following metrics become available: `node_processes_max_processes`, `node_processes_pids`, `node_processes_state`, `node_processes_threads`, `node_processes_threads_state`. The metric `node_processes_state` and `node_processes_threads_state` can have up to five series each, depending on the state of the processes and threads. The possible states of a process or a thread are: `D` (UNINTERRUPTABLE_SLEEP), `R` (RUNNING & RUNNABLE), `S` (INTERRUPTABLE_SLEEP), `T` (STOPPED), or `Z` (ZOMBIE). By default, the `processes` collector is disabled.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `processes` collector.

|===

== NodeExporterCollectorSystemdConfig

=== Description

Use the `NodeExporterCollectorSystemdConfig` resource to enable or disable the `systemd` collector of the `node-exporter` agent. By default, the `systemd` collector is disabled. If enabled, the following metrics become available: `node_systemd_system_running`, `node_systemd_units`, `node_systemd_version`. If the unit uses a socket, it also generates the following metrics: `node_systemd_socket_accepted_connections_total`, `node_systemd_socket_current_connections`, `node_systemd_socket_refused_connections_total`.  You can use the `units` parameter to select the `systemd` units to be included by the `systemd` collector. The selected units are used to generate the `node_systemd_unit_state` metric, which shows the state of each `systemd` unit. However, this metric's cardinality might be high (at least five series per unit per node). If you enable this collector with a long list of selected units, closely monitor the `prometheus-k8s` deployment for excessive memory usage. Note that the `node_systemd_timer_last_trigger_seconds` metric is only shown if you have configured the value of the `units` parameter as `logrotate.timer`.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `systemd` collector.

|units|[]string|A list of regular expression (regex) patterns that match systemd units to be included by the `systemd` collector. By default, the list is empty, so the collector exposes no metrics for systemd units.

|===

== NodeExporterCollectorTcpStatConfig

=== Description

The `NodeExporterCollectorTcpStatConfig` resource works as an on/off switch for the `tcpstat` collector of the `node-exporter` agent. By default, the `tcpstat` collector is disabled.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enabled|bool|A Boolean flag that enables or disables the `tcpstat` collector.

|===

== NodeExporterConfig

=== Description

The `NodeExporterConfig` resource defines settings for the `node-exporter` agent.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|collectors||Defines which collectors are enabled and their additional configuration parameters.

|maxProcs|uint32|The target number of CPUs on which the node-exporter's process will run. The default value is `0`, which means that node-exporter runs on all CPUs. If a kernel deadlock occurs or if performance degrades when reading from `sysfs` concurrently, you can change this value to `1`, which limits node-exporter to running on one CPU. For nodes with a high CPU count, you can set the limit to a low number, which  saves resources by preventing Go routines from being scheduled to run on all CPUs. However, I/O performance degrades if the `maxProcs` value is set too low and there are many metrics to collect.

|ignoredNetworkDevices|*[]string|A list of network devices, defined as regular expressions, that you want to exclude from the relevant collector configuration such as `netdev` and `netclass`. If no list is specified, the Cluster Monitoring Operator uses a predefined list of devices to be excluded to minimize the impact on memory usage. If the list is empty, no devices are excluded. If you modify this setting, monitor the `prometheus-k8s` deployment closely for excessive memory usage.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `NodeExporter` container.

|===

== OpenShiftStateMetricsConfig

=== Description

The `OpenShiftStateMetricsConfig` resource defines settings for the `openshift-state-metrics` agent.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `OpenShiftStateMetrics` container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|===

== PrometheusK8sConfig

=== Description

The `PrometheusK8sConfig` resource defines settings for the Prometheus component.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|additionalAlertmanagerConfigs|[]|Configures additional Alertmanager instances that receive alerts from the Prometheus component. By default, no additional Alertmanager instances are configured.

|enforcedBodySizeLimit|string|Enforces a body size limit for Prometheus scraped metrics. If a scraped target's body response is larger than the limit, the scrape will fail. The following values are valid: an empty value to specify no limit, a numeric value in Prometheus size format (such as `64MB`), or the string `automatic`, which indicates that the limit will be automatically calculated based on cluster capacity. The default value is empty, which indicates no limit.

|externalLabels|map[string]string|Defines labels to be added to any time series or alerts when communicating with external systems such as federation, remote storage, and Alertmanager. By default, no labels are added.

|logLevel|string|Defines the log level setting for Prometheus. The possible values are: `error`, `warn`, `info`, and `debug`. The default value is `info`.

|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|queryLogFile|string|Specifies the file to which PromQL queries are logged. This setting can be either a filename, in which case the queries are saved to an `emptyDir` volume at `/var/log/prometheus`, or a full path to a location where an `emptyDir` volume will be mounted and the queries saved. Writing to `/dev/stderr`, `/dev/stdout` or `/dev/null` is supported, but writing to any other `/dev/` path is not supported. Relative paths are also not supported. By default, PromQL queries are not logged.

|remoteWrite|[]|Defines the remote write configuration, including URL, authentication, and relabeling settings.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `Prometheus` container.

|retention|string|Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: `[0-9]+(ms\|s\|m\|h\|d\|w\|y)` (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is `15d`.

|retentionSize|string|Defines the maximum amount of disk space used by data blocks plus the write-ahead log (WAL). Supported values are `B`, `KB`, `KiB`, `MB`, `MiB`, `GB`, `GiB`, `TB`, `TiB`, `PB`, `PiB`, `EB`, and `EiB`. By default, no limit is defined.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|collectionProfile|CollectionProfile|Defines the metrics collection profile that Prometheus uses to collect metrics from the platform components. Supported values are `full` or `minimal`. In the `full` profile (default), Prometheus collects all metrics that are exposed by the platform components. In the `minimal` profile, Prometheus only collects metrics necessary for the default platform alerts, recording rules, telemetry, and console dashboards.

|volumeClaimTemplate|*monv1.EmbeddedPersistentVolumeClaim|Defines persistent storage for Prometheus. Use this setting to configure the persistent volume claim, including storage class, volume size and name.

|===

== PrometheusOperatorConfig

=== Description

The `PrometheusOperatorConfig` resource defines settings for the Prometheus Operator component.

Appears in: ,


[options="header"]
|===
| Property | Type | Description
|logLevel|string|Defines the log level settings for Prometheus Operator. The possible values are `error`, `warn`, `info`, and `debug`. The default value is `info`.

|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `PrometheusOperator` container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|===

== PrometheusRestrictedConfig

=== Description

The `PrometheusRestrictedConfig` resource defines the settings for the Prometheus component that monitors user-defined projects.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|additionalAlertmanagerConfigs|[]|Configures additional Alertmanager instances that receive alerts from the Prometheus component. By default, no additional Alertmanager instances are configured.

|enforcedLabelLimit|*uint64|Specifies a per-scrape limit on the number of labels accepted for a sample. If the number of labels exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is `0`, which means that no limit is set.

|enforcedLabelNameLengthLimit|*uint64|Specifies a per-scrape limit on the length of a label name for a sample. If the length of a label name exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is `0`, which means that no limit is set.

|enforcedLabelValueLengthLimit|*uint64|Specifies a per-scrape limit on the length of a label value for a sample. If the length of a label value exceeds this limit after metric relabeling, the entire scrape is treated as failed. The default value is `0`, which means that no limit is set.

|enforcedSampleLimit|*uint64|Specifies a global limit on the number of scraped samples that will be accepted. This setting overrides the `SampleLimit` value set in any user-defined `ServiceMonitor` or `PodMonitor` object if the value is greater than `enforcedTargetLimit`. Administrators can use this setting to keep the overall number of samples under control. The default value is `0`, which means that no limit is set.

|enforcedTargetLimit|*uint64|Specifies a global limit on the number of scraped targets. This setting overrides the `TargetLimit` value set in any user-defined `ServiceMonitor` or `PodMonitor` object if the value is greater than `enforcedSampleLimit`. Administrators can use this setting to keep the overall number of targets under control. The default value is `0`.

|externalLabels|map[string]string|Defines labels to be added to any time series or alerts when communicating with external systems such as federation, remote storage, and Alertmanager. By default, no labels are added.

|logLevel|string|Defines the log level setting for Prometheus. The possible values are `error`, `warn`, `info`, and `debug`. The default setting is `info`.

|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|queryLogFile|string|Specifies the file to which PromQL queries are logged. This setting can be either a filename, in which case the queries are saved to an `emptyDir` volume at `/var/log/prometheus`, or a full path to a location where an `emptyDir` volume will be mounted and the queries saved. Writing to `/dev/stderr`, `/dev/stdout` or `/dev/null` is supported, but writing to any other `/dev/` path is not supported. Relative paths are also not supported. By default, PromQL queries are not logged.

|remoteWrite|[]|Defines the remote write configuration, including URL, authentication, and relabeling settings.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the Prometheus container.

|retention|string|Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: `[0-9]+(ms\|s\|m\|h\|d\|w\|y)` (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is `15d`.

|retentionSize|string|Defines the maximum amount of disk space used by data blocks plus the write-ahead log (WAL). Supported values are `B`, `KB`, `KiB`, `MB`, `MiB`, `GB`, `GiB`, `TB`, `TiB`, `PB`, `PiB`, `EB`, and `EiB`. The default value is `nil`.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|volumeClaimTemplate|*monv1.EmbeddedPersistentVolumeClaim|Defines persistent storage for Prometheus. Use this setting to configure the storage class and size of a volume.

|===

== RemoteWriteSpec

=== Description

The `RemoteWriteSpec` resource defines the settings for remote write storage.

=== Required
* `url`

Appears in: ,


[options="header"]
|===
| Property | Type | Description
|authorization|*monv1.SafeAuthorization|Defines the authorization settings for remote write storage.

|basicAuth|*monv1.BasicAuth|Defines Basic authentication settings for the remote write endpoint URL.

|bearerTokenFile|string|Defines the file that contains the bearer token for the remote write endpoint. However, because you cannot mount secrets in a pod, in practice you can only reference the token of the service account.

|headers|map[string]string|Specifies the custom HTTP headers to be sent along with each remote write request. Headers set by Prometheus cannot be overwritten.

|metadataConfig|*monv1.MetadataConfig|Defines settings for sending series metadata to remote write storage.

|name|string|Defines the name of the remote write queue. This name is used in metrics and logging to differentiate queues. If specified, this name must be unique.

|oauth2|*monv1.OAuth2|Defines OAuth2 authentication settings for the remote write endpoint.

|proxyUrl|string|Defines an optional proxy URL.

|queueConfig|*monv1.QueueConfig|Allows tuning configuration for remote write queue parameters.

|remoteTimeout|string|Defines the timeout value for requests to the remote write endpoint.

|sendExemplars|*bool|Enables sending exemplars via remote write. When enabled, this setting configures Prometheus to store a maximum of 100,000 exemplars in memory.
This setting only applies to user-defined monitoring and is not applicable to core platform monitoring.

|sigv4|*monv1.Sigv4|Defines AWS Signature Version 4 authentication settings.

|tlsConfig|*monv1.SafeTLSConfig|Defines TLS authentication settings for the remote write endpoint.

|url|string|Defines the URL of the remote write endpoint to which samples will be sent.

|writeRelabelConfigs|[]monv1.RelabelConfig|Defines the list of remote write relabel configurations.

|===

== TLSConfig

=== Description

The `TLSConfig` resource configures the settings for TLS connections.

=== Required
* `insecureSkipVerify`

Appears in: 

[options="header"]
|===
| Property | Type | Description
|ca|*v1.SecretKeySelector|Defines the secret key reference containing the Certificate Authority (CA) to use for the remote host.

|cert|*v1.SecretKeySelector|Defines the secret key reference containing the public certificate to use for the remote host.

|key|*v1.SecretKeySelector|Defines the secret key reference containing the private key to use for the remote host.

|serverName|string|Used to verify the hostname on the returned certificate.

|insecureSkipVerify|bool|When set to `true`, disables the verification of the remote host's certificate and name.

|===

== TelemeterClientConfig

=== Description

`TelemeterClientConfig` defines settings for the Telemeter Client component.

=== Required
* `nodeSelector`
* `tolerations`

Appears in: 

[options="header"]
|===
| Property | Type | Description
|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the `TelemeterClient` container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|===

== ThanosQuerierConfig

=== Description

The `ThanosQuerierConfig` resource defines settings for the Thanos Querier component.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|enableRequestLogging|bool|A Boolean flag that enables or disables request logging. The default value is `false`.

|logLevel|string|Defines the log level setting for Thanos Querier. The possible values are `error`, `warn`, `info`, and `debug`. The default value is `info`.

|enableCORS|bool|A Boolean flag that enables setting CORS headers. The headers allow access from any origin. The default value is `false`.

|nodeSelector|map[string]string|Defines the nodes on which the pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the Thanos Querier container.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|===

== ThanosRulerConfig

=== Description

The `ThanosRulerConfig` resource defines configuration for the Thanos Ruler instance for user-defined projects.

Appears in: 

[options="header"]
|===
| Property | Type | Description
|additionalAlertmanagerConfigs|[]|Configures how the Thanos Ruler component communicates with additional Alertmanager instances. The default value is `nil`.

|logLevel|string|Defines the log level setting for Thanos Ruler. The possible values are `error`, `warn`, `info`, and `debug`. The default value is `info`.

|nodeSelector|map[string]string|Defines the nodes on which the Pods are scheduled.

|resources|*v1.ResourceRequirements|Defines resource requests and limits for the Alertmanager container.

|retention|string|Defines the duration for which Prometheus retains data. This definition must be specified using the following regular expression pattern: `[0-9]+(ms\|s\|m\|h\|d\|w\|y)` (ms = milliseconds, s= seconds,m = minutes, h = hours, d = days, w = weeks, y = years). The default value is `15d`.

|tolerations|[]v1.Toleration|Defines tolerations for the pods.

|topologySpreadConstraints|[]v1.TopologySpreadConstraint|Defines the pod's topology spread constraints.

|volumeClaimTemplate|*monv1.EmbeddedPersistentVolumeClaim|Defines persistent storage for Thanos Ruler. Use this setting to configure the storage class and size of a volume.

|===

== UserWorkloadConfiguration

=== Description

The `UserWorkloadConfiguration` resource defines the settings responsible for user-defined projects in the `user-workload-monitoring-config` config map  in the `openshift-user-workload-monitoring` namespace. You can only enable `UserWorkloadConfiguration` after you have set `enableUserWorkload` to `true` in the `cluster-monitoring-config` config map under the `openshift-monitoring` namespace.

[options="header"]
|===
| Property | Type | Description
|alertmanager|*|Defines the settings for the Alertmanager component in user workload monitoring.

|prometheus|*|Defines the settings for the Prometheus component in user workload monitoring.

|prometheusOperator|*|Defines the settings for the Prometheus Operator component in user workload monitoring.

|thanosRuler|*|Defines the settings for the Thanos Ruler component in user workload monitoring.

|===

:leveloffset!:

== Cluster Observability Operator
:leveloffset: +2

// Cluster Observability Operator Release Notes
:_mod-docs-content-type: ASSEMBLY
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
[id="cluster-observability-operator-release-notes"]
= {coo-full} release notes
:context: cluster-observability-operator-release-notes

toc::[]

:FeatureName: The Cluster Observability Operator
:leveloffset: +2

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 2

The {coo-first} is an optional {product-title} Operator that enables administrators to create standalone monitoring stacks that are independently configurable for use by different services and users.

The {coo-short} complements the built-in monitoring capabilities of {product-title}. You can deploy it in parallel with the default platform and user workload monitoring stacks managed by the Cluster Monitoring Operator (CMO).

These release notes track the development of the {coo-full} in {product-title}.

[id="cluster-observability-operator-release-notes-0-1-1"]
== {coo-full} 0.1.1
This release updates the {coo-full} to support installing the Operator in restricted networks or disconnected environments.

[id="cluster-observability-operator-release-notes-0-1"]
== {coo-full} 0.1

This release makes a Technology Preview version of the {coo-full} available on OperatorHub.

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="cluster-observability-operator-overview"]
= {coo-full} overview
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: cluster_observability_operator_overview

toc::[]

:FeatureName: The Cluster Observability Operator
:leveloffset: +2

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 2

The {coo-first} is an optional component of the {product-title}. You can deploy it to create standalone monitoring stacks that are independently configurable for use by different services and users.

The {coo-short} deploys the following monitoring components:

* Prometheus
* Thanos Querier (optional)
* Alertmanager (optional)

The {coo-short} components function independently of the default in-cluster monitoring stack, which is deployed and managed by the Cluster Monitoring Operator (CMO).
Monitoring stacks deployed by the two Operators do not conflict. You can use a {coo-short} monitoring stack in addition to the default platform monitoring components deployed by the CMO.

:leveloffset: +1

//Module included in the following assemblies:
//
// monitoring/cluster_observability_operator/cluster-observability-operator-overview.adoc

:_mod-docs-content-type: CONCEPT
[id="understanding-the-cluster-observability-operator_{context}"]
= Understanding the {coo-full}

A default monitoring stack created by the {coo-first} includes a highly available Prometheus instance capable of sending metrics to an external endpoint by using remote write.

Each {coo-short} stack also includes an optional Thanos Querier component, which you can use to query a highly available Prometheus instance from a central location, and an optional Alertmanager component, which you can use to set up alert configurations for different services.

[id="advantages-of-using-cluster-observability-operator_{context}"]
== Advantages of using the {coo-full}

The `MonitoringStack` CRD used by the {coo-short} offers an opinionated default monitoring configuration for {coo-short}-deployed monitoring components, but you can customize it to suit more complex requirements.

Deploying a {coo-short}-managed monitoring stack can help meet monitoring needs that are difficult or impossible to address by using the core platform monitoring stack deployed by the Cluster Monitoring Operator (CMO).
A monitoring stack deployed using {coo-short} has the following advantages over core platform and user workload monitoring:

Extendability:: Users can add more metrics to a {coo-short}-deployed monitoring stack, which is not possible with core platform monitoring without losing support.
In addition, {coo-short}-managed stacks can receive certain cluster-specific metrics from core platform monitoring by using federation.
Multi-tenancy support:: The {coo-short} can create a monitoring stack per user namespace.
You can also deploy multiple stacks per namespace or a single stack for multiple namespaces.
For example, cluster administrators, SRE teams, and development teams can all deploy their own monitoring stacks on a single cluster, rather than having to use a single shared stack of monitoring components.
Users on different teams can then independently configure features such as separate alerts, alert routing, and alert receivers for their applications and services.
Scalability:: You can create {coo-short}-managed monitoring stacks as needed.
Multiple monitoring stacks can run on a single cluster, which can facilitate the monitoring of very large clusters by using manual sharding. This ability addresses cases where the number of metrics exceeds the monitoring capabilities of a single Prometheus instance.
Flexibility:: Deploying the {coo-short} with Operator Lifecycle Manager (OLM) decouples {coo-short} releases from {product-title} release cycles.
This method of deployment enables faster release iterations and the ability to respond rapidly to changing requirements and issues.
Additionally, by deploying a {coo-short}-managed monitoring stack, users can manage alerting rules independently of {product-title} release cycles.
Highly customizable:: The {coo-short} can delegate ownership of single configurable fields in custom resources to users by using Server-Side Apply (SSA), which enhances customization.

:leveloffset: 2

[role="_additional-resources"]
.Additional resources

* link:https://kubernetes.io/docs/reference/using-api/server-side-apply/[Kubernetes documentation for Server-Side Apply (SSA)]


:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="installing-cluster-observability-operators"]
= Installing the {coo-full}
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: installing_the_cluster_observability_operator

toc::[]

:FeatureName: The Cluster Observability Operator
:leveloffset: +2

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 2

As a cluster administrator, you can install the {coo-first} from OperatorHub by using the {product-title} web console or CLI.
OperatorHub is a user interface that works in conjunction with Operator Lifecycle Manager (OLM), which installs and manages Operators on a cluster.

To install the {coo-short} using OperatorHub, follow the procedure described in link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#[Adding Operators to a cluster].

// Uninstalling COO using the OCP web console
:leveloffset: +1

// Module included in the following assemblies:

// * monitoring/cluster_observability_operator/installing-the-cluster-observability-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="uninstalling-the-cluster-observability-operator-using-the-web-console_{context}"]
= Uninstalling the {coo-full} using the web console
If you have installed the {coo-first} by using OperatorHub, you can uninstall it in the {product-title} web console.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role.
* You have logged in to the {product-title} web console.

.Procedure

. Go to *Operators* -> *Installed Operators*.

. Locate the *{coo-full}* entry in the list.

. Click {kebab} for this entry and select *Uninstall Operator*.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="configuring-the-cluster-observability-operator-to-monitor-a-service"]
= Configuring the {coo-full} to monitor a service
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: configuring_the_cluster_observability_operator_to_monitor_a_service

toc::[]

:FeatureName: The Cluster Observability Operator
:leveloffset: +2

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 2

You can monitor metrics for a service by configuring monitoring stacks managed by the {coo-first}.

To test monitoring a service, follow these steps:

* Deploy a sample service that defines a service endpoint.
* Create a `ServiceMonitor` object that specifies how the service is to be monitored by the {coo-short}.
* Create a `MonitoringStack` object to discover the `ServiceMonitor` object.

// Deploy a sample service for Cluster Observability Operator
:leveloffset: +1

// Module included in the following assemblies:
//
// monitoring/cluster-observability-operator/configuring-the-cluster-observability-operator-to-monitor-a-service.adoc

:_mod-docs-content-type: PROCEDURE
[id="deploying-a-sample-service-for-cluster-observability-operator_{context}"]
= Deploying a sample service for {coo-full}

This configuration deploys a sample service named `prometheus-coo-example-app` in the user-defined `ns1-coo` project.
The service exposes the custom `version` metric.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with administrative permissions for the namespace.

.Procedure

. Create a YAML file named `prometheus-coo-example-app.yaml` that contains the following configuration details for a namespace, deployment, and service:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: ns1-coo
---
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: prometheus-coo-example-app
  name: prometheus-coo-example-app
  namespace: ns1-coo
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus-coo-example-app
  template:
    metadata:
      labels:
        app: prometheus-coo-example-app
    spec:
      containers:
      - image: ghcr.io/rhobs/prometheus-example-app:0.4.2
        imagePullPolicy: IfNotPresent
        name: prometheus-coo-example-app
---
apiVersion: v1
kind: Service
metadata:
  labels:
    app: prometheus-coo-example-app
  name: prometheus-coo-example-app
  namespace: ns1-coo
spec:
  ports:
  - port: 8080
    protocol: TCP
    targetPort: 8080
    name: web
  selector:
    app: prometheus-coo-example-app
  type: ClusterIP
----

. Save the file.

. Apply the configuration to the cluster by running the following command:
+
[source,terminal]
----
$ oc apply -f prometheus-coo-example-app.yaml
----

. Verify that the pod is running by running the following command and observing the output:
+
[source,terminal]
----
$ oc -n -ns1-coo get pod
----
+
.Example output
[source,terminal]
----
NAME                                      READY     STATUS    RESTARTS   AGE
prometheus-coo-example-app-0927545cb7-anskj   1/1       Running   0          81m
----

:leveloffset: 2

// Specify how the sample COO service is monitored
:leveloffset: +1

// Module included in the following assemblies:
//
// monitoring/cluster-observability-operator/configuring-the-cluster-observability-operator-to-monitor-a-service.adoc

:_mod-docs-content-type: PROCEDURE
[id="specifying-how-a-service-is-monitored-by-cluster-observability-operator_{context}"]
= Specifying how a service is monitored by {coo-full}

To use the metrics exposed by the sample service you created in the "Deploying a sample service for {coo-full}" section, you must configure monitoring components to scrape metrics from the `/metrics` endpoint.

You can create this configuration by using a `ServiceMonitor` object that specifies how the service is to be monitored, or a `PodMonitor` object that specifies how a pod is to be monitored.
The `ServiceMonitor` object requires a `Service` object. The `PodMonitor` object does not, which enables the `MonitoringStack` object to scrape metrics directly from the metrics endpoint exposed by a pod.

This procedure shows how to create a `ServiceMonitor` object for a sample service named `prometheus-coo-example-app` in the `ns1-coo` namespace.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with administrative permissions for the namespace.
* You have installed the {coo-full}.
* You have deployed the `prometheus-coo-example-app` sample service in the `ns1-coo` namespace.
+
[NOTE]
====
The `prometheus-coo-example-app` sample service does not support TLS authentication.
====

.Procedure

. Create a YAML file named `example-coo-app-service-monitor.yaml` that contains the following `ServiceMonitor` object configuration details:
+
[source,yaml]
----
apiVersion: monitoring.rhobs/v1alpha1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: prometheus-coo-example-monitor
  name: prometheus-coo-example-monitor
  namespace: ns1-coo
spec:
  endpoints:
  - interval: 30s
    port: web
    scheme: http
  selector:
    matchLabels:
      app: prometheus-coo-example-app
----
+
This configuration defines a `ServiceMonitor` object that the `MonitoringStack` object will reference to scrape the metrics data exposed by the `prometheus-coo-example-app` sample service.

. Apply the configuration to the cluster by running the following command:
+
[source,terminal]
----
$ oc apply -f example-app-service-monitor.yaml
----

. Verify that the `ServiceMonitor` resource is created by running the following command and observing the output:
+
[source,terminal]
----
$ oc -n ns1-coo get servicemonitor
----
+
.Example output
[source,terminal]
----
NAME                         AGE
prometheus-coo-example-monitor   81m
----

:leveloffset: 2

// Create a MonitoringStack object to discover the service monitor
:leveloffset: +1

// Module included in the following assemblies:
//
// monitoring/cluster-observability-operator/configuring-the-cluster-observability-operator-to-monitor-a-service.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-a-monitoringstack-object-for-cluster-observability-operator_{context}"]
= Creating a MonitoringStack object for the {coo-full}

To scrape the metrics data exposed by the target `prometheus-coo-example-app` service, create a `MonitoringStack` object that references the `ServiceMonitor` object you created in the "Specifying how a service is monitored for {coo-full}" section.
This `MonitoringStack` object can then discover the service and scrape the exposed metrics data from it.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` cluster role or as a user with administrative permissions for the namespace.
* You have installed the {coo-full}.
* You have deployed the `prometheus-coo-example-app` sample service in the `ns1-coo` namespace.
* You have created a `ServiceMonitor` object named `prometheus-coo-example-monitor` in the `ns1-coo` namespace.

.Procedure

. Create a YAML file for the `MonitoringStack` object configuration. For this example, name the file `example-coo-monitoring-stack.yaml`.

. Add the following `MonitoringStack` object configuration details:
+
.Example `MonitoringStack` object
+
[source,yaml]
----
apiVersion: monitoring.rhobs/v1alpha1
kind: MonitoringStack
metadata:
  name: example-coo-monitoring-stack
  namespace: ns1-coo
spec:
  logLevel: debug
  retention: 1d
  resourceSelector:
    matchLabels:
      k8s-app: prometheus-coo-example-monitor
----

. Apply the `MonitoringStack` object by running the following command:
+
[source,terminal]
----
$ oc apply -f example-coo-monitoring-stack.yaml
----

. Verify that the `MonitoringStack` object is available by running the following command and inspecting the output:
+
[source,terminal]
----
$ oc -n ns1-coo get monitoringstack
----
+
.Example output
[source,terminal]
----
NAME                         AGE
example-coo-monitoring-stack   81m
----

:leveloffset: 2

:leveloffset!:

//# includes=monitoring-overview,_attributes/common-attributes,modules/monitoring-understanding-the-monitoring-stack,modules/monitoring-default-monitoring-components,modules/monitoring-default-monitoring-targets,modules/monitoring-components-for-monitoring-user-defined-projects,modules/monitoring-targets-for-user-defined-projects,modules/monitoring-common-terms,configuring-the-monitoring-stack,modules/monitoring-maintenance-and-support,modules/monitoring-support-considerations,modules/monitoring-unmanaged-monitoring-operators,modules/monitoring-creating-cluster-monitoring-configmap,modules/monitoring-creating-user-defined-workload-monitoring-configmap,modules/monitoring-configuring-the-monitoring-stack,modules/monitoring-configurable-monitoring-components,modules/monitoring-using-node-selectors-to-move-monitoring-components,modules/monitoring-moving-monitoring-components-to-different-nodes,modules/monitoring-assigning-tolerations-to-monitoring-components,modules/monitoring-setting-the-body-size-limit-for-metrics-scraping,modules/monitoring-about-specifying-limits-and-requests-for-monitoring-components,modules/monitoring-specifying-limits-and-requests-for-monitoring-components,modules/monitoring-configuring-persistent-storage,modules/monitoring-configuring-a-local-persistent-volume-claim,modules/monitoring-resizing-a-persistent-storage-volume,modules/monitoring-modifying-retention-time-and-size-for-prometheus-metrics-data,modules/monitoring-modifying-the-retention-time-for-thanos-ruler-metrics-data,modules/monitoring-configuring-remote-write-storage,modules/monitoring-supported-remote-write-authentication-settings,modules/monitoring-example-remote-write-authentication-settings,modules/monitoring-adding-cluster-id-labels-to-metrics,modules/monitoring-creating-cluster-id-labels-for-metrics,modules/monitoring-configuring-metrics-collection-profiles,modules/monitoring-choosing-a-metrics-collection-profile,modules/monitoring-limiting-scrape-samples-in-user-defined-projects,modules/monitoring-setting-scrape-sample-and-label-limits-for-user-defined-projects,modules/monitoring-creating-scrape-sample-alerts,modules/monitoring-configuring-external-alertmanagers,modules/monitoring-configuring-secrets-for-alertmanager,modules/monitoring-adding-a-secret-to-the-alertmanager-configuration,modules/monitoring-attaching-additional-labels-to-your-time-series-and-alerts,modules/monitoring-configuring-pod-topology-spread-constraints-for-monitoring,modules/monitoring-setting-up-pod-topology-spread-constraints-for-prometheus,modules/monitoring-setting-up-pod-topology-spread-constraints-for-alertmanager,modules/monitoring-setting-up-pod-topology-spread-constraints-for-thanos-ruler,modules/monitoring-setting-log-levels-for-monitoring-components,modules/monitoring-setting-query-log-file-for-prometheus,modules/monitoring-enabling-query-logging-for-thanos-querier,modules/monitoring-setting-audit-log-levels-for-the-prometheus-adapter,modules/monitoring-disabling-the-local-alertmanager,enabling-monitoring-for-user-defined-projects,snippets/monitoring-custom-prometheus-note,modules/monitoring-enabling-monitoring-for-user-defined-projects,modules/monitoring-granting-users-permission-to-monitor-user-defined-projects,modules/monitoring-granting-user-permissions-using-the-web-console,modules/monitoring-granting-user-permissions-using-the-cli,modules/monitoring-granting-users-permission-to-configure-monitoring-for-user-defined-projects,modules/accessing-metrics-outside-cluster,modules/monitoring-excluding-a-user-defined-project-from-monitoring,modules/monitoring-disabling-monitoring-for-user-defined-projects,enabling-alert-routing-for-user-defined-projects,modules/monitoring-understanding-alert-routing-for-user-defined-projects,modules/monitoring-enabling-the-platform-alertmanager-instance-for-user-defined-alert-routing,modules/monitoring-enabling-a-separate-alertmanager-instance-for-user-defined-alert-routing,modules/monitoring-granting-users-permission-to-configure-alert-routing-for-user-defined-projects,managing-metrics,modules/monitoring-understanding-metrics,modules/monitoring-setting-up-metrics-collection-for-user-defined-projects,modules/monitoring-deploying-a-sample-service,modules/monitoring-specifying-how-a-service-is-monitored,modules/monitoring-viewing-a-list-of-available-metrics,modules/monitoring-about-querying-metrics,modules/monitoring-querying-metrics-for-all-projects-as-an-administrator,modules/monitoring-querying-metrics-for-user-defined-projects-as-a-developer,modules/monitoring-getting-detailed-information-about-a-target,managing-alerts,modules/monitoring-accessing-the-alerting-ui,modules/monitoring-searching-alerts-silences-and-alerting-rules,modules/monitoring-getting-information-about-alerts-silences-and-alerting-rules,modules/monitoring-managing-silences,modules/monitoring-silencing-alerts,modules/monitoring-editing-silences,modules/monitoring-expiring-silences,modules/monitoring-managing-core-platform-alerting-rules,modules/monitoring-tips-for-optimizing-alerting-rules-for-core-platform-monitoring,modules/monitoring-creating-new-alerting-rules,modules/monitoring-modifying-core-platform-alerting-rules,modules/monitoring-managing-alerting-rules-for-user-defined-projects,modules/monitoring-optimizing-alerting-for-user-defined-projects,modules/monitoring-about-creating-alerting-rules-for-user-defined-projects,modules/monitoring-creating-alerting-rules-for-user-defined-projects,modules/monitoring-accessing-alerting-rules-for-your-project,modules/monitoring-listing-alerting-rules-for-all-projects-in-a-single-view,modules/monitoring-removing-alerting-rules-for-user-defined-projects,modules/monitoring-sending-notifications-to-external-systems,modules/monitoring-configuring-alert-receivers,modules/monitoring-creating-alert-routing-for-user-defined-projects,modules/monitoring-applying-custom-alertmanager-configuration,modules/monitoring-applying-a-custom-configuration-to-alertmanager-for-user-defined-alert-routing,reviewing-monitoring-dashboards,modules/monitoring-reviewing-monitoring-dashboards-admin,modules/monitoring-reviewing-monitoring-dashboards-developer,accessing-third-party-monitoring-apis,modules/monitoring-accessing-third-party-monitoring-web-service-apis,modules/monitoring-querying-metrics-by-using-the-federation-endpoint-for-prometheus,troubleshooting-monitoring-issues,modules/monitoring-investigating-why-user-defined-metrics-are-unavailable,modules/monitoring-determining-why-prometheus-is-consuming-disk-space,config-map-reference-for-the-cluster-monitoring-operator,snippets/technology-preview,cluster_observability_operator/cluster-observability-operator-release-notes,cluster_observability_operator/_attributes/common-attributes,cluster_observability_operator/snippets/technology-preview,cluster_observability_operator/cluster-observability-operator-overview,cluster_observability_operator/modules/monitoring-understanding-the-cluster-observability-operator,cluster_observability_operator/installing-the-cluster-observability-operator,cluster_observability_operator/modules/monitoring-uninstalling-cluster-observability-operator-using-the-web-console,cluster_observability_operator/configuring-the-cluster-observability-operator-to-monitor-a-service,cluster_observability_operator/modules/monitoring-deploying-a-sample-service-for-cluster-observability-operator,cluster_observability_operator/modules/monitoring-specifying-how-a-service-is-monitored-by-cluster-observability-operator,cluster_observability_operator/modules/monitoring-creating-a-monitoringstack-object-for-cluster-observability-operator

= Service Mesh

== Service Mesh 2.x
:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-about"]
= About OpenShift Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-about

toc::[]

[NOTE]
====
Because {SMProductName} releases on a different cadence from {product-title} and because the {SMProductName} Operator supports deploying multiple versions of the `ServiceMeshControlPlane`, the {SMProductShortName} documentation does not maintain separate documentation sets for minor versions of the product.  The current documentation set applies to the most recent version of {SMProductShortName} unless version-specific limitations are called out in a particular topic or for a particular feature.

For additional information about the {SMProductName} life cycle and supported platforms, refer to the link:https://access.redhat.com/support/policy/updates/openshift#ossm[Platform Life Cycle Policy].
====

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-about.adoc
////

[id="ossm-servicemesh-overview_{context}"]
= Introduction to {SMProductName}

{SMProductName} addresses a variety of problems in a microservice architecture by creating a centralized point of control in an application. It adds a transparent layer on existing distributed applications without requiring any changes to the application code.

Microservice architectures split the work of enterprise applications into modular services, which can make scaling and maintenance easier. However, as an enterprise application built on a microservice architecture grows in size and complexity, it becomes difficult to understand and manage. {SMProductShortName} can address those architecture problems by capturing or intercepting traffic between services and can modify, redirect, or create new requests to other services.

{SMProductShortName}, which is based on the open source link:https://istio.io/[Istio project], provides an easy way to create a network of deployed services that provides discovery, load balancing, service-to-service authentication, failure recovery, metrics, and monitoring. A service mesh also provides more complex operational functionality, including A/B testing, canary releases, access control, and end-to-end authentication.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/servicemesh-release-notes.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-core-features_{context}"]
= Core features

{SMProductName} provides a number of key capabilities uniformly across a network of services:

* *Traffic Management* - Control the flow of traffic and API calls between services, make calls more reliable, and make the network more robust in the face of adverse conditions.
* *Service Identity and Security* - Provide services in the mesh with a verifiable identity and provide the ability to protect service traffic as it flows over networks of varying degrees of trustworthiness.
* *Policy Enforcement* - Apply organizational policy to the interaction between services, ensure access policies are enforced and resources are fairly distributed among consumers. Policy changes are made by configuring the mesh, not by changing application code.
* *Telemetry* - Gain understanding of the dependencies between services and the nature and flow of traffic between them, providing the ability to quickly identify issues.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="service-mesh-release-notes"]
= Service Mesh Release Notes
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-release-notes

toc::[]

// The following include statements pull in the module files that comprise 2.x release notes.

:leveloffset: +1

// Module included in the following assemblies:
//
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-0.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-1.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-2.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-3.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-4.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-5.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-6.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-7.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-8.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-9.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-3-0.adoc

:_mod-docs-content-type: CONCEPT

[id="making-open-source-more-inclusive_{context}"]
= Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[our CTO Chris Wright's message].

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/servicemesh-release-notes.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-rn-new-features_{context}"]
= New features and enhancements

////
*Feature* – Describe the new functionality available to the customer. For enhancements, try to describe as specifically as possible where the customer will see changes.
*Reason* – If known, include why has the enhancement been implemented (use case, performance, technology, etc.). For example, showcases integration of X with Y, demonstrates Z API feature, includes latest framework bug fixes. There may not have been a 'problem' previously, but system behavior may have changed.
*Result* – If changed, describe the current user experience
////

This release adds improvements related to the following components and concepts.

== New features {SMProductName} version 2.4.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.11 and later versions.

=== Component versions included in {SMProductName} version 2.4.5
//Component table updated for 2.4.5, 2.3.9, and 2.2.12
//Kiali updated to 1.65.11 Nov 8, 2023
//According to Distributed Tracing rel notes, Jaeger is unchanged
//Envoy remains unchanged
//Istio remains unchanged
|===
|Component |Version

|Istio
|1.16.7

|Envoy Proxy
|1.24.12

|Jaeger
|1.47.0

|Kiali
|1.65.11
|===
//Component table updated for 2.4.5, 2.3.9, and 2.2.12

== New features {SMProductName} version 2.4.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.11 and later versions.

=== Component versions included in {SMProductName} version 2.4.4
//Istio stays the same
//Envoy updated to 1.24.12
//According to distributed tracing release 2.9, the latest at time of OSSM 2.4.4, Jaeger has been updated to 1.47.0
//Kiali updated to 1.65.9
//Kiali updated to 1.65.10 on 10/31/2023 due to security fix
|===
|Component |Version

|Istio
|1.16.7

|Envoy Proxy
|1.24.12

|Jaeger
|1.47.0

|Kiali
|1.65.10
|===
//Components updated for 2.4.4

== New features {SMProductName} version 2.4.3

* The {SMProductName} Operator is now available on ARM-based clusters as a Technology Preview feature.
* The `envoyExtAuthzGrpc` field has been added, which is used to configure an external authorization provider using the gRPC API.
* Common Vulnerabilities and Exposures (CVEs) have been addressed.
* This release is supported on {product-title} 4.10 and newer versions.

=== Component versions included in {SMProductName} version 2.4.3
//THESE MAY NEED TO BE UPDATED FOR 2.4.3
//Kiali updated to 1.65.8 on 09/06/2023
//09/06/2023: According to distributed tracing release notes, Jaeger component version remains unchanged.
|===
|Component |Version

|Istio
|1.16.7

|Envoy Proxy
|1.24.10

|Jaeger
|1.42.0

|Kiali
|1.65.8
|===
//COMPONENTS ABOVE MAY NEED TO BE UPDATED FOR 2.4.3

=== {SMProductName} operator to ARM-based clusters
:FeatureName: {SMProductName} operator to ARM based clusters
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

This release makes the {SMProductName} Operator available on ARM-based clusters as a Technology Preview feature. Images are available for Istio, Envoy, Prometheus, Kiali, and Grafana. Images are not available for Jaeger, so Jaeger must be disabled as a {SMProductShortName} add-on.

=== Remote Procedure Calls (gRPC) API support for external authorization configuration

This enhancement adds the `envoyExtAuthzGrpc` field to configure an external authorization provider using the gRPC API.

== New features {SMProductName} version 2.4.2

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.4.2

|===
|Component |Version

|Istio
|1.16.7

|Envoy Proxy
|1.24.10

|Jaeger
|1.42.0

|Kiali
|1.65.7
|===

== New features {SMProductName} version 2.4.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.4.1

|===
|Component |Version

|Istio
|1.16.5

|Envoy Proxy
|1.24.8

|Jaeger
|1.42.0

|Kiali
|1.65.7
|===

== New features {SMProductName} version 2.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.4

|===
|Component |Version

|Istio
|1.16.5

|Envoy Proxy
|1.24.8

|Jaeger
|1.42.0

|Kiali
|1.65.6
|===

=== Cluster-wide deployments
This enhancement introduces a generally available version of cluster-wide deployments. A cluster-wide deployment contains a service mesh control plane that monitors resources for an entire cluster. The control plane uses a single query across all namespaces to monitor each Istio or Kubernetes resource that affects the mesh configuration. Reducing the number of queries the control plane performs in a cluster-wide deployment improves performance.

=== Support for discovery selectors
This enhancement introduces a generally available version of the `meshConfig.discoverySelectors` field, which can be used in cluster-wide deployments to limit the services the service mesh control plane can discover.

[source,yaml]
----
spec:
  meshConfig
    discoverySelectors:
    - matchLabels:
        env: prod
        region: us-east1
    - matchExpressions:
      - key: app
        operator: In
        values:
          - cassandra
          - spark
----

=== Integration with cert-manager istio-csr
With this update, {SMProductName} integrates with the `cert-manager` controller and the `istio-csr` agent. `cert-manager` adds certificates and certificate issuers as resource types in Kubernetes clusters, and simplifies the process of obtaining, renewing, and using those certificates. `cert-manager` provides and rotates an intermediate CA certificate for Istio. Integration with `istio-csr` enables users to delegate signing certificate requests from Istio proxies to `cert-manager`. `ServiceMeshControlPlane` v2.4 accepts CA certificates provided by `cert-manager` as `cacerts` secret.

[NOTE]
====
Integration with `cert-manager` and `istio-csr` is not supported on {ibm-power-name}, {ibm-z-name}, and {ibm-linuxone-name}.
====

=== Integration with external authorization systems
This enhancement introduces a generally available method of integrating {SMProductName} with external authorization systems by using the `action: CUSTOM` field of the `AuthorizationPolicy` resource. Use the `envoyExtAuthzHttp` field to delegate the access control to an external authorization system.

=== Integration with external Prometheus installation

This enhancement introduces a generally available version of the Prometheus extension provider. You can expose metrics to the {product-title} monitoring stack or a custom Prometheus installation by setting the value of the `extensionProviders` field to `prometheus` in the `spec.meshConfig` specification. The telemetry object configures Istio proxies to collect traffic metrics. {SMProductShortName} only supports the Telemetry API for Prometheus metrics.

[source,yaml]
----
spec:
  meshConfig:
    extensionProviders:
    - name: prometheus
      prometheus: {}
---
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: enable-prometheus-metrics
spec:
  metrics:
  - providers:
    - name: prometheus
----

=== Single stack IPv6 support

This enhancement introduces generally available support for single stack IPv6 clusters, providing access to a broader range of IP addresses. Dual stack IPv4 or IPv6 cluster is not supported.

[NOTE]
====
Single stack IPv6 support is not available on {ibm-power-name}, {ibm-z-name}, and {ibm-linuxone-name}.
====

=== {product-title} Gateway API support
:FeatureName: {product-title} Gateway API support
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

This enhancement introduces an updated Technology Preview version of the {product-title} Gateway API. By default, the {product-title} Gateway API is disabled.

==== Enabling {product-title} Gateway API
To enable the {product-title} Gateway API, set the value of the `enabled` field to `true` in the `techPreview.gatewayAPI` specification of the `ServiceMeshControlPlane` resource.

[source,yaml]
----
spec:
  techPreview:
    gatewayAPI:
      enabled: true
----

Previously, environment variables were used to enable the Gateway API.

[source,yaml]
----
spec:
  runtime:
    components:
      pilot:
        container:
          env:
            PILOT_ENABLE_GATEWAY_API: "true"
            PILOT_ENABLE_GATEWAY_API_STATUS: "true"
            PILOT_ENABLE_GATEWAY_API_DEPLOYMENT_CONTROLLER: "true"
----


=== Control plane deployment on infrastructure nodes
{SMProductShortName} control plane deployment is now supported and documented on OpenShift infrastructure nodes. For more information, see the following documentation:

* Configuring all {SMProductShortName} control plane components to run on infrastructure nodes
* Configuring individual {SMProductShortName} control plane components to run on infrastructure nodes


=== Istio 1.16 support
{SMProductShortName} 2.4 is based on Istio 1.16, which brings in new features and product enhancements. While many Istio 1.16 features are supported, the following exceptions should be noted:

* HBONE protocol for sidecars is an experimental feature that is not supported.
* {SMProductShortName} on ARM64 architecture is not supported.
* OpenTelemetry API remains a Technology Preview feature.

== New features {SMProductName} version 2.3.9
//Update with 2.4.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.11 and later versions.

=== Component versions included in {SMProductName} version 2.3.9
//Updated with 2.4.5
//Kiali updated to 1.57.14 Nov 8
|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.11

|Jaeger
|1.47.0

|Kiali
|1.57.14
|===


== New features {SMProductName} version 2.3.8
//Update with 2.4.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.11 and later versions.

=== Component versions included in {SMProductName} version 2.3.8
//Istio for 2.3.8 is now 1.14.5
//Kiali is 1.57.13
//Jaeger is 1.47.0
//Envoy stays the same
|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.11

|Jaeger
|1.47.0

|Kiali
|1.57.13
|===
//Components updated for 2.3.8 as part of 2.4.4 update

== New features {SMProductName} version 2.3.7

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.3.7

|===
|Component |Version

|Istio
|1.14.6

|Envoy Proxy
|1.22.11

|Jaeger
|1.42.0

|Kiali
|1.57.11
|===

== New features {SMProductName} version 2.3.6

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.3.6

|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.11

|Jaeger
|1.42.0

|Kiali
|1.57.10
|===

== New features {SMProductName} version 2.3.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.3.5

|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.9

|Jaeger
|1.42.0

|Kiali
|1.57.10
|===

== New features {SMProductName} version 2.3.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.3.4

|===
|Component |Version

|Istio
|1.14.6

|Envoy Proxy
|1.22.9

|Jaeger
|1.42.0

|Kiali
|1.57.9
|===

== New features {SMProductName} version 2.3.3

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.
//only Envoy Proxy changed 04/17/2023
//kiali and istio changed 04/19/2023
//Jaeger updated 04/20/2023

=== Component versions included in {SMProductName} version 2.3.3

|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.9

|Jaeger
|1.42.0

|Kiali
|1.57.7
|===

== New features {SMProductName} version 2.3.2

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.3.2

|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.7

|Jaeger
|1.39

|Kiali
|1.57.6
|===

== New features {SMProductName} version 2.3.1

This release of {SMProductName} introduces new features, addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.3.1

|===
|Component |Version

|Istio
|1.14.5

|Envoy Proxy
|1.22.4

|Jaeger
|1.39

|Kiali
|1.57.5
|===

== New features {SMProductName} version 2.3

This release of {SMProductName} introduces new features, addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.3

|===
|Component |Version

|Istio
|1.14.3

|Envoy Proxy
|1.22.4

|Jaeger
|1.38

|Kiali
|1.57.3
|===

=== New Container Network Interface (CNI) DaemonSet container and ConfigMap

The `openshift-operators` namespace includes a new istio CNI DaemonSet `istio-cni-node-v2-3` and a new `ConfigMap` resource, `istio-cni-config-v2-3`.

When upgrading to Service Mesh Control Plane 2.3, the existing `istio-cni-node` DaemonSet is not changed, and a new `istio-cni-node-v2-3` DaemonSet is created.

This name change does not affect previous releases or any `istio-cni-node` CNI DaemonSet associated with a Service Mesh Control Plane deployed using a previous release.

=== Gateway injection support

This release introduces generally available support for Gateway injection. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than the sidecar Envoy proxies running alongside your service workloads. This enables the ability to customize gateway options. When using gateway injection, you must create the following resources in the namespace where you want to run your gateway proxy: `Service`, `Deployment`, `Role`, and `RoleBinding`.

=== Istio 1.14 Support

{SMProductShortName} 2.3 is based on Istio 1.14, which brings in new features and product enhancements. While many Istio 1.14 features are supported, the following exceptions should be noted:

* ProxyConfig API is supported with the exception of the image field.
* Telemetry API is a Technology Preview feature.
* SPIRE runtime is not a supported feature.

=== OpenShift Service Mesh Console
:FeatureName: OpenShift Service Mesh Console
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

This release introduces a Technology Preview version of the {product-title} Service Mesh Console, which integrates the Kiali interface directly into the OpenShift web console. For additional information, see link:https://cloud.redhat.com/blog/introducing-the-openshift-service-mesh-console-a-developer-preview[Introducing the OpenShift Service Mesh Console (A Technology Preview)]

===  Cluster-wide deployment
:FeatureName: Cluster-wide deployment
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

This release introduces cluster-wide deployment as a Technology Preview feature. A cluster-wide deployment contains a Service Mesh Control Plane that monitors resources for an entire cluster. The control plane uses a single query across all namespaces to monitor each Istio or Kubernetes resource kind that affects the mesh configuration. In contrast, the multitenant approach uses a query per namespace for each resource kind. Reducing the number of queries the control plane performs in a cluster-wide deployment improves performance.

[NOTE]
====
This cluster-wide deployment documentation is only applicable for control planes deployed using SMCP v2.3. cluster-wide deployments created using SMCP v2.3 are not compatible with cluster-wide deployments created using SMCP v2.4.
====

==== Configuring cluster-wide deployment

The following example `ServiceMeshControlPlane` object configures a cluster-wide deployment.

To create an SMCP for cluster-wide deployment, a user must belong to the `cluster-admin` ClusterRole. If the SMCP is configured for cluster-wide deployment, it must be the only SMCP in the cluster. You cannot change the control plane mode from multitenant to cluster-wide (or from cluster-wide to multitenant). If a multitenant control plane already exists, delete it and create a new one.

This example configures the SMCP for cluster-wide deployment.

[source,yaml]
----
  apiVersion: maistra.io/v2
  kind: ServiceMeshControlPlane
  metadata:
    name: cluster-wide
    namespace: istio-system
  spec:
    version: v2.3
    techPreview:
      controlPlaneMode: ClusterScoped <1>
----
<1> Enables Istiod to monitor resources at the cluster level rather than monitor each individual namespace.

Additionally, the SMMR must also be configured for cluster-wide deployment. This example configures the SMMR for cluster-wide deployment.

[source,yaml]
----
  apiVersion: maistra.io/v1
  kind: ServiceMeshMemberRoll
  metadata:
    name: default
  spec:
    members:
    - '*' <1>
----
<1> Adds all namespaces to the mesh, including any namespaces you subsequently create. The following namespaces are not part of the mesh: kube, openshift, kube-* and openshift-*.

== New features {SMProductName} version 2.2.12

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.11 and later versions.

=== Component versions included in {SMProductName} version 2.2.12
//2.2.12 released with 2.4.5
//Kiali updated to 1.48.11 Nov 8
|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.47.0

|Kiali
|1.48.11
|===
//2.2.12 being released with 2.4.5


== New features {SMProductName} version 2.2.11

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.11 and later versions.

=== Component versions included in {SMProductName} version 2.2.11
//Istio remains the same
//Kiali updated to 1.48.10
//Jaeger updated to 1.47.0
//Envoy remains the same
|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.47.0

|Kiali
|1.48.10
|===
//Components updated for 2.2.11 with 2.4.4 release

== New features {SMProductName} version 2.2.10

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.2.10

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.42.0

|Kiali
|1.48.8
|===

== New features {SMProductName} version 2.2.9

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.2.9

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.42.0

|Kiali
|1.48.7
|===

== New features {SMProductName} version 2.2.8

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.2.8

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.42.0

|Kiali
|1.48.7
|===

== New features {SMProductName} version 2.2.7

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.10 and later versions.

=== Component versions included in {SMProductName} version 2.2.7

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.42.0

|Kiali
|1.48.6
|===

== New features {SMProductName} version 2.2.6

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2.6

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.39

|Kiali
|1.48.5
|===

== New features {SMProductName} version 2.2.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2.5

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.39

|Kiali
|1.48.3
|===

== New features {SMProductName} version 2.2.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2.4

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.36.14

|Kiali
|1.48.3
|===

== New features {SMProductName} version 2.2.3

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2.3

|===
|Component |Version

|Istio
|1.12.9

|Envoy Proxy
|1.20.8

|Jaeger
|1.36

|Kiali
|1.48.3
|===

== New features {SMProductName} version 2.2.2

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2.2

|===
|Component |Version

|Istio
|1.12.7

|Envoy Proxy
|1.20.6

|Jaeger
|1.36

|Kiali
|1.48.2-1
|===

=== Copy route labels

With this enhancement, in addition to copying annotations, you can copy specific labels for an OpenShift route. {SMProductName} copies all labels and annotations present in the Istio Gateway resource (with the exception of annotations starting with kubectl.kubernetes.io) into the managed OpenShift Route resource.

== New features {SMProductName} version 2.2.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2.1

|===
|Component |Version

|Istio
|1.12.7

|Envoy Proxy
|1.20.6

|Jaeger
|1.34.1

|Kiali
|1.48.2-1
|===

== New features {SMProductName} 2.2

This release of {SMProductName} adds new features and enhancements, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.2

|===
|Component |Version

|Istio
|1.12.7

|Envoy Proxy
|1.20.4

|Jaeger
|1.34.1

|Kiali
|1.48.0.16
|===

=== `WasmPlugin` API
This release adds support for the `WasmPlugin` API and deprecates the `ServiceMeshExtension` API.

=== ROSA support
This release introduces service mesh support for Red Hat OpenShift on AWS (ROSA), including multi-cluster federation.

=== `istio-node` DaemonSet renamed
This release, the `istio-node` DaemonSet is renamed to `istio-cni-node` to match the name in upstream Istio.

=== Envoy sidecar networking changes
Istio 1.10 updated Envoy to send traffic to the application container using `eth0` rather than `lo` by default.

=== Service Mesh Control Plane 1.1
This release marks the end of support for {SMProductShortName} Control Planes based on Service Mesh 1.1 for all platforms.

=== Istio 1.12 Support

{SMProductShortName} 2.2 is based on Istio 1.12, which brings in new features and product enhancements. While many Istio 1.12 features are supported, the following unsupported features should be noted:

* AuthPolicy Dry Run is a tech preview feature.
* gRPC Proxyless Service Mesh is a tech preview feature.
* Telemetry API is a tech preview feature.
* Discovery selectors is not a supported feature.
* External control plane is not a supported feature.
* Gateway injection is not a supported feature.

=== Kubernetes Gateway API
:FeatureName: Kubernetes Gateway API
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

Kubernetes Gateway API is a technology preview feature that is disabled by default. If the Kubernetes API deployment controller is disabled, you must manually deploy and link an ingress gateway to the created Gateway object.

If the Kubernetes API deployment controller is enabled, then an ingress gateway automatically deploys when a Gateway object is created.

==== Installing the Gateway API CRDs
The Gateway API CRDs do not come preinstalled by default on OpenShift clusters. Install the CRDs prior to enabling Gateway API support in the SMCP.

[source,terminal]
----
$ kubectl get crd gateways.gateway.networking.k8s.io || { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd?ref=v0.4.0" | kubectl apply -f -; }
----

==== Enabling Kubernetes Gateway API
To enable the feature, set the following environment variables for the `Istiod` container in `ServiceMeshControlPlane`:

[source,yaml]
----
spec:
  runtime:
    components:
      pilot:
        container:
          env:
            PILOT_ENABLE_GATEWAY_API: "true"
            PILOT_ENABLE_GATEWAY_API_STATUS: "true"
            # and optionally, for the deployment controller
            PILOT_ENABLE_GATEWAY_API_DEPLOYMENT_CONTROLLER: "true"
----
Restricting route attachment on Gateway API listeners is possible using the `SameNamespace` or `All` settings. Istio ignores usage of label selectors in `listeners.allowedRoutes.namespaces` and reverts to the default behavior (`SameNamespace`).

==== Manually linking an existing gateway to a Gateway resource
If the Kubernetes API deployment controller is disabled, you must manually deploy and then link an ingress gateway to the created Gateway resource.

[source,yaml]
----
  apiVersion: gateway.networking.k8s.io/v1alpha2
  kind: Gateway
  metadata:
    name: gateway
  spec:
    addresses:
    - value: ingress.istio-gateways.svc.cluster.local
      type: Hostname
----

== New features {SMProductName} 2.1.6

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.1.6

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.5

|Jaeger
|1.36

|Kiali
|1.36.16
|===

== New features {SMProductName} 2.1.5.2

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), contains bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.1.5.2

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.5

|Jaeger
|1.36

|Kiali
|1.24.17
|===

== New features {SMProductName} 2.1.5.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.1.5.1

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.5

|Jaeger
|1.36

|Kiali
|1.36.13
|===

== New features {SMProductName} 2.1.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 and later versions.

=== Component versions included in {SMProductName} version 2.1.5

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.1

|Jaeger
|1.36

|Kiali
|1.36.12-1
|===

== New features {SMProductName} 2.1.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

=== Component versions included in {SMProductName} version 2.1.4

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.1

|Jaeger
|1.30.2

|Kiali
|1.36.12-1
|===

== New features {SMProductName} 2.1.3

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

=== Component versions included in {SMProductName} version 2.1.3

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.1

|Jaeger
|1.30.2

|Kiali
|1.36.10-2
|===

== New features {SMProductName} 2.1.2.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

=== Component versions included in {SMProductName} version 2.1.2.1

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.1

|Jaeger
|1.30.2

|Kiali
|1.36.9
|===

== New features {SMProductName} 2.1.2

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

With this release, the {JaegerName} Operator is now installed to the `openshift-distributed-tracing` namespace by default.  Previously the default installation had been in the `openshift-operator` namespace.

=== Component versions included in {SMProductName} version 2.1.2

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.1

|Jaeger
|1.30.1

|Kiali
|1.36.8
|===

== New features {SMProductName} 2.1.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

This release also adds the ability to disable the automatic creation of network policies.

=== Component versions included in {SMProductName} version 2.1.1

|===
|Component |Version

|Istio
|1.9.9

|Envoy Proxy
|1.17.1

|Jaeger
|1.24.1

|Kiali
|1.36.7
|===

[id="ossm-config-disable-networkpolicy_{context}"]
=== Disabling network policies

{SMProductName} automatically creates and manages a number of `NetworkPolicies` resources in the {SMProductShortName} control plane and application namespaces. This is to ensure that applications and the control plane can communicate with each other.

If you want to disable the automatic creation and management of `NetworkPolicies` resources, for example to enforce company security policies, you can do so.  You can edit the `ServiceMeshControlPlane` to set the `spec.security.manageNetworkPolicy` setting to `false`

[NOTE]
====
When you disable `spec.security.manageNetworkPolicy` {SMProductName} will not create *any* `NetworkPolicy` objects.  The system administrator is responsible for managing the network and fixing any issues this might cause.
====

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Select the project where you installed the {SMProductShortName} control plane, for example `istio-system`, from the Project menu.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane`, for example `basic-install`.

. On the *Create ServiceMeshControlPlane Details* page, click `YAML` to modify your configuration.

. Set the `ServiceMeshControlPlane` field `spec.security.manageNetworkPolicy` to `false`, as shown in this example.
+
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
spec:
  security:
      trust:
      manageNetworkPolicy: false
----
+
. Click *Save*.

== New features and enhancements {SMProductName} 2.1

This release of {SMProductName} adds support for Istio 1.9.8, Envoy Proxy 1.17.1, Jaeger 1.24.1, and Kiali 1.36.5 on {product-title} 4.6 EUS, 4.7, 4.8, 4.9, along with new features and enhancements.

=== Component versions included in {SMProductName} version 2.1

|===
|Component |Version

|Istio
|1.9.6

|Envoy Proxy
|1.17.1

|Jaeger
|1.24.1

|Kiali
|1.36.5
|===

=== Service Mesh Federation

New Custom Resource Definitions (CRDs) have been added to support federating service meshes. Service meshes may be federated both within the same cluster or across different OpenShift clusters. These new resources include:

* `ServiceMeshPeer` - Defines a federation with a separate service mesh, including gateway configuration, root trust certificate configuration, and status fields. In a pair of federated meshes, each mesh will define its own separate `ServiceMeshPeer` resource.

* `ExportedServiceMeshSet` - Defines which services for a given `ServiceMeshPeer` are available for the peer mesh to import.

* `ImportedServiceSet` - Defines which services for a given `ServiceMeshPeer` are imported from the peer mesh. These services must also be made available by the peer’s `ExportedServiceMeshSet` resource.

Service Mesh Federation is not supported between clusters on Red Hat OpenShift Service on AWS (ROSA), Azure Red Hat OpenShift (ARO), or OpenShift Dedicated (OSD).

=== OVN-Kubernetes Container Network Interface (CNI) generally available

The OVN-Kubernetes Container Network Interface (CNI) was previously introduced as a Technology Preview feature in {SMProductName} 2.0.1 and is now generally available in {SMProductName} 2.1 and 2.0.x for use on {product-title} 4.7.32, {product-title} 4.8.12, and {product-title} 4.9.

=== Service Mesh WebAssembly (WASM) Extensions

The `ServiceMeshExtensions` Custom Resource Definition (CRD), first introduced in 2.0 as Technology Preview, is now generally available. You can use CRD to build your own plugins, but Red Hat does not provide support for the plugins you create.

Mixer has been completely removed in Service Mesh 2.1. Upgrading from a Service Mesh 2.0.x release to 2.1 will be blocked if Mixer is enabled. Mixer plugins will need to be ported to WebAssembly Extensions.

=== 3scale WebAssembly Adapter (WASM)

With Mixer now officially removed, OpenShift Service Mesh 2.1 does not support the 3scale mixer adapter. Before upgrading to Service Mesh 2.1, remove the Mixer-based 3scale adapter and any additional Mixer plugins. Then, manually install and configure the new 3scale WebAssembly adapter with Service Mesh 2.1+ using a `ServiceMeshExtension` resource.

3scale 2.11 introduces an updated Service Mesh integration based on  `WebAssembly`.

=== Istio 1.9 Support

{SMProductShortName} 2.1 is based on Istio 1.9, which brings in a large number of new features and product enhancements. While the majority of Istio 1.9 features are supported, the following exceptions should be noted:

* Virtual Machine integration is not yet supported
* Kubernetes Gateway API is not yet supported
* Remote fetch and load of WebAssembly HTTP filters are not yet supported
* Custom CA Integration using the Kubernetes CSR API is not yet supported
* Request Classification for monitoring traffic is a tech preview feature
* Integration with external authorization systems via Authorization policy’s CUSTOM action is a tech preview feature

=== Improved Service Mesh operator performance

The amount of time {SMProductName} uses to prune old resources at the end of every `ServiceMeshControlPlane` reconciliation has been reduced. This results in faster `ServiceMeshControlPlane` deployments, and allows changes applied to existing SMCPs to take effect more quickly.


=== Kiali updates

Kiali 1.36 includes the following features and enhancements:

* {SMProductShortName} troubleshooting functionality
** Control plane and gateway monitoring
** Proxy sync statuses
** Envoy configuration views
** Unified view showing Envoy proxy and application logs interleaved
* Namespace and cluster boxing to support federated service mesh views
* New validations, wizards, and distributed tracing enhancements

== New features {SMProductName} 2.0.11.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 or later.

=== Component versions included in {SMProductName} version 2.0.11.1

|===
|Component |Version

|Istio
|1.6.14

|Envoy Proxy
|1.14.5

|Jaeger
|1.36

|Kiali
|1.24.17
|===

== New features {SMProductName} 2.0.11

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs), bug fixes, and is supported on {product-title} 4.9 or later.

=== Component versions included in {SMProductName} version 2.0.11

|===
|Component |Version

|Istio
|1.6.14

|Envoy Proxy
|1.14.5

|Jaeger
|1.36

|Kiali
|1.24.16-1
|===

== New features {SMProductName} 2.0.10

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

=== Component versions included in {SMProductName} version 2.0.10

|===
|Component |Version

|Istio
|1.6.14

|Envoy Proxy
|1.14.5

|Jaeger
|1.28.0

|Kiali
|1.24.16-1
|===

== New features {SMProductName} 2.0.9

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

=== Component versions included in {SMProductName} version 2.0.9

|===
|Component |Version

|Istio
|1.6.14

|Envoy Proxy
|1.14.5

|Jaeger
|1.24.1

|Kiali
|1.24.11
|===

== New features {SMProductName} 2.0.8

This release of {SMProductName} addresses bug fixes.

== New features {SMProductName} 2.0.7.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs).

=== Change in how {SMProductName} handles URI fragments

{SMProductName} contains a remotely exploitable vulnerability, link:https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-39156[CVE-2021-39156], where an HTTP request with a fragment (a section in the end of a URI that begins with a # character) in the URI path could bypass the Istio URI path-based authorization policies. For instance, an Istio authorization policy denies requests sent to the URI path `/user/profile`. In the vulnerable versions, a request with URI path `/user/profile#section1` bypasses the deny policy and routes to the backend (with the normalized URI `path /user/profile%23section1`), possibly leading to a security incident.

You are impacted by this vulnerability if you use authorization policies with DENY actions and `operation.paths`, or ALLOW actions and `operation.notPaths`.

With the mitigation, the fragment part of the request’s URI is removed before the authorization and routing. This prevents a request with a fragment in its URI from bypassing authorization policies which are based on the URI without the fragment part.

To opt-out from the new behavior in the mitigation, the fragment section in the URI will be kept. You can configure your `ServiceMeshControlPlane` to keep URI fragments.

[WARNING]
====
Disabling the new behavior will normalize your paths as described above and is considered unsafe. Ensure that you have accommodated for this in any security policies before opting to keep URI fragments.
====

.Example `ServiceMeshControlPlane` modification
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  techPreview:
    meshConfig:
      defaultConfig:
        proxyMetadata: HTTP_STRIP_FRAGMENT_FROM_PATH_UNSAFE_IF_DISABLED: "false"
----

=== Required update for authorization policies

Istio generates hostnames for both the hostname itself and all matching ports. For instance, a virtual service or Gateway for a host of "httpbin.foo" generates a config matching "httpbin.foo and httpbin.foo:*". However, exact match authorization policies only match the exact string given for the `hosts` or `notHosts` fields.

Your cluster is impacted if you have `AuthorizationPolicy` resources using exact string comparison for the rule to determine link:https://istio.io/latest/docs/reference/config/security/authorization-policy/#Operation[hosts or notHosts].

You must update your authorization policy link:https://istio.io/latest/docs/reference/config/security/authorization-policy/#Rule[rules] to use prefix match instead of exact match.  For example, replacing `hosts: ["httpbin.com"]` with `hosts: ["httpbin.com:*"]` in the first `AuthorizationPolicy` example.

.First example AuthorizationPolicy using prefix match
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: httpbin
  namespace: foo
spec:
  action: DENY
  rules:
  - from:
    - source:
        namespaces: ["dev"]
    to:
    - operation:
        hosts: [“httpbin.com”,"httpbin.com:*"]
----

.Second example AuthorizationPolicy using prefix match
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: httpbin
  namespace: default
spec:
  action: DENY
  rules:
  - to:
    - operation:
        hosts: ["httpbin.example.com:*"]
----

== New features {SMProductName} 2.0.7

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== {SMProductName} on {product-dedicated} and Microsoft Azure Red Hat OpenShift

{SMProductName} is now supported through {product-dedicated} and Microsoft Azure Red Hat OpenShift.

== New features {SMProductName} 2.0.6

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 2.0.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 2.0.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

[IMPORTANT]
====
There are manual steps that must be completed to address CVE-2021-29492 and CVE-2021-31920.
====

[id="manual-updates-cve-2021-29492_{context}"]
=== Manual updates required by CVE-2021-29492 and CVE-2021-31920

Istio contains a remotely exploitable vulnerability where an HTTP request path with multiple slashes or escaped slash characters (`%2F` or `%5C`) could potentially bypass an Istio authorization policy when path-based authorization rules are used.

For example, assume an Istio cluster administrator defines an authorization DENY policy to reject the request at path `/admin`. A request sent to the URL path `//admin` will NOT be rejected by the authorization policy.

According to https://tools.ietf.org/html/rfc3986#section-6[RFC 3986], the path `//admin` with multiple slashes should technically be treated as a different path from the `/admin`. However, some backend services choose to normalize the URL paths by merging multiple slashes into a single slash. This can result in a bypass of the authorization policy (`//admin` does not match `/admin`), and a user can access the resource at path `/admin` in the backend; this would represent a security incident.

Your cluster is impacted by this vulnerability if you have authorization policies using `ALLOW action + notPaths` field or `DENY action + paths field` patterns. These patterns are vulnerable to unexpected policy bypasses.

Your cluster is NOT impacted by this vulnerability if:

* You don’t have authorization policies.
* Your authorization policies don’t define `paths` or `notPaths` fields.
* Your authorization policies use `ALLOW action + paths` field or `DENY action + notPaths` field patterns. These patterns could only cause unexpected rejection instead of policy bypasses. The upgrade is optional for these cases.

[NOTE]
====
The {SMProductName} configuration location for path normalization is different from the Istio configuration.
====

=== Updating the path normalization configuration

Istio authorization policies can be based on the URL paths in the HTTP request.
https://en.wikipedia.org/wiki/URI_normalization[Path normalization], also known as URI normalization, modifies and standardizes the incoming requests' paths so that the normalized paths can be processed in a standard way.
Syntactically different paths may be equivalent after path normalization.

Istio supports the following normalization schemes on the request paths before evaluating against the authorization policies and routing the requests:

.Normalization schemes
[options="header"]
[cols="a, a, a, a"]
|====
| Option | Description | Example |Notes
|`NONE`
|No normalization is done. Anything received by Envoy will be forwarded exactly as-is to any backend service.
|`../%2Fa../b` is evaluated by the authorization policies and sent to your service.
|This setting is vulnerable to CVE-2021-31920.

|`BASE`
|This is currently the option used in the *default* installation of Istio. This applies the https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto#envoy-v3-api-field-extensions-filters-network-http-connection-manager-v3-httpconnectionmanager-normalize-path[`normalize_path`] option on Envoy proxies, which follows https://tools.ietf.org/html/rfc3986[RFC 3986] with extra normalization to convert backslashes to forward slashes.
|`/a/../b` is normalized to `/b`. `\da` is normalized to `/da`.
|This setting is vulnerable to CVE-2021-31920.

| `MERGE_SLASHES`
| Slashes are merged after the _BASE_ normalization.
| `/a//b` is normalized to `/a/b`.
|Update to this setting to mitigate CVE-2021-31920.

|`DECODE_AND_MERGE_SLASHES`
|The strictest setting when you allow all traffic by default. This setting is recommended, with the caveat that you must thoroughly test your authorization policies routes. https://tools.ietf.org/html/rfc3986#section-2.1[Percent-encoded] slash and backslash characters (`%2F`, `%2f`, `%5C` and `%5c`) are decoded to `/` or `\`, before the `MERGE_SLASHES` normalization.
|`/a%2fb` is normalized to `/a/b`.
|Update to this setting to mitigate CVE-2021-31920. This setting is more secure, but also has the potential to break applications. Test your applications before deploying to production.
|====

The normalization algorithms are conducted in the following order:

. Percent-decode `%2F`, `%2f`, `%5C` and `%5c`.
. The https://tools.ietf.org/html/rfc3986[RFC 3986] and other normalization implemented by the https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto#envoy-v3-api-field-extensions-filters-network-http-connection-manager-v3-httpconnectionmanager-normalize-path[`normalize_path`] option in Envoy.
. Merge slashes.

[WARNING]
====
While these normalization options represent recommendations from HTTP standards and common industry practices, applications may interpret a URL in any way it chooses to. When using denial policies, ensure that you understand how your application behaves.
====

=== Path normalization configuration examples

Ensuring Envoy normalizes request paths to match your backend services' expectations is critical to the security of your system.
The following examples can be used as a reference for you to configure your system.
The normalized URL paths, or the original URL paths if `NONE` is selected, will be:

. Used to check against the authorization policies.
. Forwarded to the backend application.

.Configuration examples
[options="header"]
[cols="a, a"]
|====
|If your application... |Choose...
|Relies on the proxy to do normalization
|`BASE`, `MERGE_SLASHES` or `DECODE_AND_MERGE_SLASHES`

|Normalizes request paths based on https://tools.ietf.org/html/rfc3986[RFC 3986] and does not merge slashes.
|`BASE`

|Normalizes request paths based on https://tools.ietf.org/html/rfc3986[RFC 3986] and merges slashes, but does not decode https://tools.ietf.org/html/rfc3986#section-2.1[percent-encoded] slashes.
|`MERGE_SLASHES`

|Normalizes request paths based on https://tools.ietf.org/html/rfc3986[RFC 3986], decodes https://tools.ietf.org/html/rfc3986#section-2.1[percent-encoded] slashes, and merges slashes.
|`DECODE_AND_MERGE_SLASHES`

|Processes request paths in a way that is incompatible with https://tools.ietf.org/html/rfc3986[RFC 3986].
|`NONE`
|====

=== Configuring your SMCP for path normalization

To configure path normalization for {SMProductName}, specify the following in your `ServiceMeshControlPlane`. Use the configuration examples to help determine the settings for your system.

.SMCP v2 pathNormalization
[source,yaml]
----
spec:
  techPreview:
    global:
      pathNormalization: <option>
----

=== Configuring for case normalization

In some environments, it may be useful to have paths in authorization policies compared in a case insensitive manner.
For example, treating `https://myurl/get` and `https://myurl/GeT` as equivalent.
In those cases, you can use the `EnvoyFilter` shown below.
This filter will change both the path used for comparison and the path presented to the application. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

Save the `EnvoyFilter` to a file and run the following command:

[source,terminal]
----
$ oc create -f <myEnvoyFilterFile>
----

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: ingress-case-insensitive
  namespace: istio-system
spec:
  configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: GATEWAY
      listener:
        filterChain:
          filter:
            name: "envoy.filters.network.http_connection_manager"
            subFilter:
              name: "envoy.filters.http.router"
    patch:
      operation: INSERT_BEFORE
      value:
        name: envoy.lua
        typed_config:
            "@type": "type.googleapis.com/envoy.extensions.filters.http.lua.v3.Lua"
            inlineCode: |
              function envoy_on_request(request_handle)
                local path = request_handle:headers():get(":path")
                request_handle:headers():replace(":path", string.lower(path))
              end

----


== New features {SMProductName} 2.0.3

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

In addition, this release has the following new features:

* Added an option to the `must-gather` data collection tool that gathers information from a specified {SMProductShortName} control plane namespace. For more information, see link:https://issues.redhat.com/browse/OSSM-351[OSSM-351].
* Improved performance for {SMProductShortName} control planes with hundreds of namespaces

== New features {SMProductName} 2.0.2

This release of {SMProductName} adds support for {ibm-z-name} and {ibm-power-name} Systems. It also addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 2.0.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 2.0

This release of {SMProductName} adds support for Istio 1.6.5, Jaeger 1.20.0, Kiali 1.24.2, and the 3scale Istio Adapter 2.0 and {product-title} 4.6.

In addition, this release has the following new features:

** Simplifies installation, upgrades, and management of the {SMProductShortName} control plane.
** Reduces the {SMProductShortName} control plane's resource usage and startup time.
** Improves performance by reducing inter-control plane communication over networking.

* Adds support for Envoy's Secret Discovery Service (SDS). SDS is a more secure and efficient mechanism for delivering secrets to Envoy side car proxies.
** Removes the need to use Kubernetes Secrets, which have well known security risks.
** Improves performance during certificate rotation, as proxies no longer require a restart to recognize new certificates.

* Adds support for Istio's Telemetry v2 architecture, which is built using WebAssembly extensions. This new architecture brings significant performance improvements.

* Updates the ServiceMeshControlPlane resource to v2 with a streamlined configuration to make it easier to manage the {SMProductShortName} Control Plane.

* Introduces WebAssembly extensions as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] feature.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
- v2x\servicemesh-release-notes.adoc
////

[id="ossm-rn-tech-preview_{context}"]
= Technology Preview

Some features in this release are currently in Technology Preview. These experimental features are not intended for production use.

[IMPORTANT]
====
Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/servicemesh-release-notes.adoc
////

[id="ossm-deprecated-features_{context}"]
////
Description - Description of the any features (including technology previews) that have been removed from the product. Write the description from a customer perspective, what UI elements, commands, or options are no longer available.
Consequence or a recommended replacement - Description of what the customer can no longer do, and recommended replacement (if known).
////
= Deprecated and removed features
Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

Removed functionality no longer exists in the product.

== Deprecated and removed features in {SMProductName} 2.4

The v2.1 `ServiceMeshControlPlane` resource is no longer supported. Customers should upgrade their mesh deployments to use a later version of the `ServiceMeshControlPlane` resource.

Support for Istio OpenShift Routing (IOR) is deprecated and will be removed in a future release.

Support for Grafana is deprecated and will be removed in a future release.

Support for the following cipher suites, which were deprecated in {SMProductName} 2.3, has been removed from the default list of ciphers used in TLS negotiations on both the client and server sides. Applications that require access to services requiring one of these cipher suites will fail to connect when a TLS connection is initiated from the proxy.

* ECDHE-ECDSA-AES128-SHA
* ECDHE-RSA-AES128-SHA
* AES128-GCM-SHA256
* AES128-SHA
* ECDHE-ECDSA-AES256-SHA
* ECDHE-RSA-AES256-SHA
* AES256-GCM-SHA384
* AES256-SHA

== Deprecated and removed features in {SMProductName} 2.3

Support for the following cipher suites has been deprecated. In a future release, they will be removed from the default list of ciphers used in TLS negotiations on both the client and server sides.

* ECDHE-ECDSA-AES128-SHA
* ECDHE-RSA-AES128-SHA
* AES128-GCM-SHA256
* AES128-SHA
* ECDHE-ECDSA-AES256-SHA
* ECDHE-RSA-AES256-SHA
* AES256-GCM-SHA384
* AES256-SHA

The `ServiceMeshExtension` API, which was deprecated in {SMProductName} version 2.2, was removed in {SMProductName} version 2.3. If you are using the `ServiceMeshExtension` API, you must migrate to the `WasmPlugin` API to continue using your WebAssembly extensions.

== Deprecated features in {SMProductName} 2.2

The `ServiceMeshExtension` API is deprecated as of release 2.2 and will be removed in a future release.  While `ServiceMeshExtension` API is still supported in release 2.2, customers should start moving to the new `WasmPlugin` API.

== Removed features in {SMProductName} 2.2

This release marks the end of support for {SMProductShortName} control planes based on Service Mesh 1.1 for all platforms.

== Removed features in {SMProductName} 2.1

In Service Mesh 2.1, the Mixer component is removed. Bug fixes and support is provided through the end of the Service Mesh 2.0 life cycle.

Upgrading from a Service Mesh 2.0.x release to 2.1 will not proceed if Mixer plugins are enabled. Mixer plugins must be ported to WebAssembly Extensions.

== Deprecated features in {SMProductName} 2.0

The Mixer component was deprecated in release 2.0 and will be removed in release 2.1. While using Mixer for implementing extensions was still supported in release 2.0, extensions should have been migrated to the new link:https://istio.io/latest/blog/2020/wasm-announce/[WebAssembly] mechanism.

The following resource types are no longer supported in {SMProductName} 2.0:

* `Policy` (authentication.istio.io/v1alpha1) is no longer supported. Depending on the specific configuration in your Policy resource, you may have to configure multiple resources to achieve the same effect.
** Use `RequestAuthentication` (security.istio.io/v1beta1)
** Use `PeerAuthentication` (security.istio.io/v1beta1)
* `ServiceMeshPolicy` (maistra.io/v1) is no longer supported.
** Use `RequestAuthentication` or `PeerAuthentication`, as mentioned above, but place in the {SMProductShortName} control plane namespace.
* `RbacConfig` (rbac.istio.io/v1alpha1) is no longer supported.
** Replaced by `AuthorizationPolicy` (security.istio.io/v1beta1), which encompasses behavior of `RbacConfig`, `ServiceRole`, and `ServiceRoleBinding`.
* `ServiceMeshRbacConfig` (maistra.io/v1) is no longer supported.
** Use `AuthorizationPolicy` as above, but place in {SMProductShortName} control plane namespace.
* `ServiceRole` (rbac.istio.io/v1alpha1) is no longer supported.
* `ServiceRoleBinding` (rbac.istio.io/v1alpha1) is no longer supported.
* In Kiali, the `login` and `LDAP` strategies are deprecated. A future version will introduce authentication using OpenID providers.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/servicemesh-release-notes.adoc
////
:_mod-docs-content-type: REFERENCE
[id="ossm-rn-known-issues_{context}"]
= Known issues

////
*Consequence* - What user action or situation would make this problem appear (Selecting the Foo option with the Bar version 1.3 plugin enabled results in an error message)?  What did the customer experience as a result of the issue? What was the symptom?
*Cause* (if it has been identified) - Why did this happen?
*Workaround* (If there is one)- What can you do to avoid or negate the effects of this issue in the meantime?  Sometimes if there is no workaround it is worthwhile telling readers to contact support for advice. Never promise future fixes.
*Result* - If the workaround does not completely address the problem.
////

These limitations exist in {SMProductName}:

* {SMProductName} does not yet fully support link:https://issues.redhat.com/browse/MAISTRA-1314[IPv6]. As a result, {SMProductName} does not support dual-stack clusters.

* Graph layout - The layout for the Kiali graph can render differently, depending on your application architecture and the data to display (number of graph nodes and their interactions). Because it is difficult if not impossible to create a single layout that renders nicely for every situation, Kiali offers a choice of several different layouts. To choose a different layout, you can choose a different *Layout Schema* from the *Graph Settings* menu.

* The first time you access related services such as {JaegerShortName} and Grafana, from the Kiali console, you must accept the certificate and re-authenticate using your {product-title} login credentials. This happens due to an issue with how the framework displays embedded pages in the console.

* The Bookinfo sample application cannot be installed on {ibm-power-name}, {ibm-z-name}, and {ibm-linuxone-name}.

* WebAssembly extensions are not supported on {ibm-power-name}, {ibm-z-name}, and {ibm-linuxone-name}.

* LuaJIT is not supported on {ibm-power-name}, {ibm-z-name}, and {ibm-linuxone-name}.

* Single stack IPv6 support is not available on {ibm-power-name}, {ibm-z-name}, and {ibm-linuxone-name}.

[id="ossm-rn-known-issues-ossm_{context}"]
== {SMProductShortName} known issues

These are the known issues in {SMProductName}:

* https://issues.redhat.com/browse/OSSM-3890[OSSM-3890] Attempting to use the Gateway API in a multitenant mesh deployment generates an error message similar to the following:
+
[source,text]
----
2023-05-02T15:20:42.541034Z	error	watch error in cluster Kubernetes: failed to list *v1alpha2.TLSRoute: the server could not find the requested resource (get tlsroutes.gateway.networking.k8s.io)
2023-05-02T15:20:42.616450Z	info	kube	controller "gateway.networking.k8s.io/v1alpha2/TCPRoute" is syncing...
----
+
To support Gateway API in a multitenant mesh deployment, all Gateway API Custom Resource Definition (CRD) files must be present in the cluster.
+
In a multitenant mesh deployment, CRD scan is disabled, and Istio has no way to discover which CRDs are present in a cluster. As a result, Istio attempts to watch all supported Gateway API CRDs, but generates errors if some of those CRDs are not present.
+
{SMProductShortName} 2.3.1 and later versions support both `v1alpha2` and `v1beta1` CRDs. Therefore, both CRD versions must be present for a multitenant mesh deployment to support the Gateway API.
+
Workaround: In the following example, the `kubectl get` operation installs the `v1alpha2` and `v1beta1` CRDs. Note the URL contains the additional `experimental` segment and updates any of your existing scripts accordingly:
+
[source,terminal]
----
$ kubectl get crd gateways.gateway.networking.k8s.io ||   { kubectl kustomize "github.com/kubernetes-sigs/gateway-api/config/crd/experimental?ref=v0.5.1" | kubectl apply -f -; }
----

* https://issues.redhat.com/browse/OSSM-2042[OSSM-2042] Deployment of SMCP named `default` fails. If you are creating an SMCP object, and set its version field to v2.3, the name of the object cannot be `default`. If the name is `default`, then the control plane fails to deploy, and OpenShift generates a `Warning` event with the following message:
+
`Error processing component mesh-config: error: [mesh-config/templates/telemetryv2_1.6.yaml: Internal error occurred: failed calling webhook "rev.validation.istio.io": Post "https://istiod-default.istio-system.svc:443/validate?timeout=10s": x509: certificate is valid for istiod.istio-system.svc, istiod-remote.istio-system.svc, istio-pilot.istio-system.svc, not istiod-default.istio-system.svc, mesh-config/templates/enable-mesh-permissive.yaml`

//Keep OSSM-1655 in RN, closed as "explained" error is expected.
* https://issues.redhat.com/browse/OSSM-1655[OSSM-1655] Kiali dashboard shows error after enabling mTLS in `SMCP`.
+
After enabling the `spec.security.controlPlane.mtls` setting in the SMCP, the Kiali console displays the following error message `No subsets defined`.

* https://issues.redhat.com/browse/OSSM-1505[OSSM-1505] This issue only occurs when using the `ServiceMeshExtension` resource on OpenShift Container Platform 4.11. When you use `ServiceMeshExtension` on OpenShift Container Platform 4.11 the resource never becomes ready. If you inspect the issue using `oc describe ServiceMeshExtension` you will see the following error:  `stderr: Error creating mount namespace before pivot: function not implemented`.
+
Workaround: `ServiceMeshExtension` was deprecated in {SMProductShortName} 2.2. Migrate from `ServiceMeshExtension` to the `WasmPlugin` resource.
For more information, see Migrating from `ServiceMeshExtension` to `WasmPlugin` resources.

* https://issues.redhat.com/browse/OSSM-1396[OSSM-1396] If a gateway resource contains the `spec.externalIPs` setting, instead of being recreated when the `ServiceMeshControlPlane` is updated, the gateway is removed and never recreated.

* https://issues.redhat.com/browse/OSSM-1168[OSSM-1168] When service mesh resources are created as a single YAML file, the Envoy proxy sidecar is not reliably injected into pods. When the SMCP, SMMR, and Deployment resources are created individually, the deployment works as expected.
//Keep OSSM-1052 in RN - Closed as documented.

* https://issues.redhat.com/browse/OSSM-1115[OSSM-1115] The `concurrency` field of the `spec.proxy` API did not propagate to the istio-proxy. The `concurrency` field works when set with `ProxyConfig`. The `concurrency` field specifies the number of worker threads to run. If the field is set to `0`, then the number of worker threads available is equal to the number of CPU cores. If the field is not set, then the number of worker threads available defaults to `2`.
+
In the following example, the `concurrency` field is set to `0`.
+
[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: ProxyConfig
metadata:
  name: mesh-wide-concurrency
  namespace: <istiod-namespace>
spec:
  concurrency: 0
----

* https://issues.redhat.com/browse/OSSM-1052[OSSM-1052] When configuring a Service `ExternalIP` for the ingressgateway in the {SMProductShortName} control plane, the service is not created. The schema for the SMCP is missing the parameter for the service.
+
Workaround: Disable the gateway creation in the SMCP spec and manage the gateway deployment entirely manually (including Service, Role and RoleBinding).

//Keep OSSM-882 in RN to document the workaround
* https://issues.redhat.com/browse/OSSM-882[OSSM-882] This applies for {SMProductShortName} 2.1 and earlier. Namespace is in the accessible_namespace list but does not appear in Kiali UI. By default, Kiali will not show any namespaces that start with "kube" because these namespaces are typically internal-use only and not part of a mesh.
+
For example, if you create a namespace called 'akube-a' and add it to the Service Mesh member roll, then the Kiali UI does not display the namespace. For defined exclusion patterns, the software excludes namespaces that start with or contain the pattern.
+
Workaround: Change the Kiali Custom Resource setting so it prefixes the setting with a carat (^). For example:
+
[source,yaml]
----
api:
  namespaces:
    exclude:
    - "^istio-operator"
    - "^kube-.*"
    - "^openshift.*"
    - "^ibm.*"
    - "^kiali-operator"
----

+
* link:https://issues.redhat.com/browse/MAISTRA-2692[MAISTRA-2692] With Mixer removed, custom metrics that have been defined in {SMProductShortName} 2.0.x cannot be used in 2.1. Custom metrics can be configured using `EnvoyFilter`. Red Hat is unable to support `EnvoyFilter` configuration except where explicitly documented. This is due to tight coupling with the underlying Envoy APIs, meaning that backward compatibility cannot be maintained.

* link:https://issues.redhat.com/browse/MAISTRA-2648[MAISTRA-2648] Service mesh extensions are currently not compatible with meshes deployed on {ibm-z-name}.

* link:https://issues.jboss.org/browse/MAISTRA-1959[MAISTRA-1959] _Migration to 2.0_ Prometheus scraping (`spec.addons.prometheus.scrape` set to `true`) does not work when mTLS is enabled. Additionally, Kiali displays extraneous graph data when mTLS is disabled.
+
This problem can be addressed by excluding port 15020 from proxy configuration, for example,
+
[source,yaml]
----
spec:
  proxy:
    networking:
      trafficControl:
        inbound:
          excludedPorts:
          - 15020
----

* link:https://issues.jboss.org/browse/MAISTRA-453[MAISTRA-453] If you create a new project and deploy pods immediately, sidecar injection does not occur. The operator fails to add the `maistra.io/member-of` before the pods are created, therefore the pods must be deleted and recreated for sidecar injection to occur.

* link:https://issues.jboss.org/browse/MAISTRA-158[MAISTRA-158] Applying multiple gateways referencing the same hostname will cause all gateways to stop functioning.

[id="ossm-rn-known-issues-kiali_{context}"]
== Kiali known issues

[NOTE]
====
New issues for Kiali should be created in the link:https://issues.redhat.com/projects/OSSM/[OpenShift Service Mesh] project with the `Component` set to `Kiali`.
====

These are the known issues in Kiali:

//Keep KIALI-2206 in RN as this is for information purposes.
* link:https://issues.jboss.org/browse/KIALI-2206[KIALI-2206] When you are accessing the Kiali console for the first time, and there is no cached browser data for Kiali, the “View in Grafana” link on the Metrics tab of the Kiali Service Details page redirects to the wrong location. The only way you would encounter this issue is if you are accessing Kiali for the first time.
//Keep KIALI-507 in RN as this is for information purposes.
* link:https://github.com/kiali/kiali/issues/507[KIALI-507] Kiali does not support Internet Explorer 11. This is because the underlying frameworks do not support Internet Explorer. To access the Kiali console, use one of the two most recent versions of the Chrome, Edge, Firefox or Safari browser.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/servicemesh-release-notes.adoc
////
:_mod-docs-content-type: REFERENCE
[id="ossm-rn-fixed-issues_{context}"]
= Fixed issues

////
Provide the following info for each issue if possible:
*Consequence* - What user action or situation would make this problem appear (If you have the foo option enabled and did x)? What did the customer experience as a result of the issue? What was the symptom?
*Cause* - Why did this happen?
*Fix* - What did we change to fix the problem?
*Result* - How has the behavior changed as a result? Try to avoid “It is fixed” or “The issue is resolved” or “The error no longer presents”.
////

The following issue has been resolved in the current release:

* https://issues.redhat.com/browse/OSSM-3647[OSSM-3647] Previously, in the {SMProductShortName} control plane (SMCP) v2.2 (Istio 1.12), WasmPlugins were applied only to inbound listeners. Since SMCP v2.3 (Istio 1.14), WasmPlugins have been applied to inbound and outbound listeners by default, which introduced regression for users of the 3scale WasmPlugin. Now, the environment variable `APPLY_WASM_PLUGINS_TO_INBOUND_ONLY` is added, which allows safe migration from SMCP v2.2 to v2.3 and v2.4.
+
The following setting should be added to the SMCP config:
+
[source, yaml]
----
spec:
  runtime:
    components:
      pilot:
        container:
          env:
            APPLY_WASM_PLUGINS_TO_INBOUND_ONLY: "true"

----
+
To ensure safe migration, perform the following steps:
+
--
. Set `APPLY_WASM_PLUGINS_TO_INBOUND_ONLY` in SMCP v2.2.
. Upgrade to 2.4.
. Set `spec.match[].mode: SERVER` in WasmPlugins.
. Remove the previously-added environment variable.
--

The following issues have been resolved in previous releases:

[id="ossm-rn-fixed-issues-ossm_{context}"]
== {SMProductShortName} fixed issues

* https://issues.redhat.com/browse/OSSM-4851[OSSM-4851] Previously, an error occurred in the operator deploying new pods in a namespace scoped inside the mesh when `runAsGroup`, `runAsUser`, or `fsGroup` parameters were `nil`. Now, a yaml validation has been added to avoid the `nil` value.

* https://issues.redhat.com/browse/OSSM-3771[OSSM-3771] Previously, OpenShift routes could not be disabled for additional ingress gateways defined in a Service Mesh Control Plane (SMCP). Now, a `routeConfig` block can be added to each `additionalIngress` gateway so the creation of OpenShift routes can be enabled or disabled for each gateway.

* https://issues.redhat.com/browse/OSSM-4197[OSSM-4197] Previously, if you deployed a v2.2 or v2.1 of the 'ServiceMeshControlPlane' resource, the `/etc/cni/multus/net.d/` directory was not created. As a result, the `istio-cni` pod failed to become ready, and the `istio-cni` pods log contained the following message:
+
[source,terminal]
----
$ error   Installer exits with open /host/etc/cni/multus/net.d/v2-2-istio-cni.kubeconfig.tmp.841118073: no such file or directory
----
+
Now, if you deploy a v2.2 or v2.1 of the 'ServiceMeshControlPlane' resource, the `/etc/cni/multus/net.d/` directory is created, and the `istio-cni` pod becomes ready.

* https://issues.redhat.com/browse/OSSM-3993[OSSM-3993] Previously, Kiali only supported OpenShift OAuth via a proxy on the standard HTTPS port of `443`. Now, Kiali supports OpenShift OAuth over a non-standard HTTPS port. To enable the port, you must set the `spec.server.web_port` field to the proxy's non-standard HTTPS port in the Kiali CR.

* https://issues.redhat.com/browse/OSSM-3936[OSSM-3936] Previously, the values for the `injection_label_rev` and `injection_label_name` attributes were hardcoded. This prevented custom configurations from taking effect in the Kiali Custom Resource Definition (CRD). Now, the attribute values are not hardcoded. You can customize the values for the `injection_label_rev` and `injection_label_name` attributes in the `spec.istio_labels` specification.

* https://issues.redhat.com/browse/OSSM-3644[OSSM-3644] Previously, the federation egress-gateway received the wrong update of network gateway endpoints, causing extra endpoint entries. Now, the federation-egress gateway has been updated on the server side so it receives the correct network gateway endpoints.

* https://issues.redhat.com/browse/OSSM-3595[OSSM-3595] Previously, the `istio-cni` plugin sometimes failed on {op-system-base} because SELinux did not allow the utility `iptables-restore` to open files in the `/tmp` directory. Now, SELinux passes `iptables-restore` via `stdin` input stream instead of via a file.

* https://issues.redhat.com/browse/OSSM-3586[OSSM-3586] Previously, Istio proxies were slow to start when Google Cloud Platform (GCP) metadata servers were not available. When you upgrade to Istio 1.14.6, Istio proxies start as expected on GCP, even if metadata servers are not available.

* https://issues.redhat.com/browse/OSSM-3025[OSSM-3025] Istiod sometimes fails to become ready. Sometimes, when a mesh contained many member namespaces, the Istiod pod did not become ready due to a deadlock within Istiod. The deadlock is now resolved and the pod now starts as expected.

* https://issues.redhat.com/browse/OSSM-2493[OSSM-2493] Default `nodeSelector` and `tolerations` in SMCP not passed to Kiali. The `nodeSelector` and `tolerations` you add to `SMCP.spec.runtime.defaults` are now passed to the Kiali resource.

* https://issues.redhat.com/browse/OSSM-2492[OSSM-2492] Default tolerations in SMCP not passed to Jaeger. The `nodeSelector` and `tolerations` you add to `SMCP.spec.runtime.defaults` are now passed to the Jaeger resource.

* https://issues.redhat.com/browse/OSSM-2374[OSSM-2374] If you deleted one of the `ServiceMeshMember` resources, then the Service Mesh operator deleted the `ServiceMeshMemberRoll`. While this is expected behavior when you delete the last `ServiceMeshMember`, the operator should not delete the `ServiceMeshMemberRoll` if it contains any members in addition to the one that was deleted. This issue is now fixed and the operator only deletes the ServiceMeshMemberRoll when the last `ServiceMeshMember` resource is deleted.

* https://issues.redhat.com/browse/OSSM-2373[OSSM-2373] Error trying to get OAuth metadata when logging in. To fetch the cluster version, the `system:anonymous` account is used. With the cluster's default bundled ClusterRoles and ClusterRoleBinding, the anonymous account can fetch the version correctly. If the `system:anonymous` account loses its privileges to fetch the cluster version, OpenShift authentication becomes unusable.
+
This is fixed by using the Kiali SA to fetch the cluster version. This also allows for improved security on the cluster.

* https://issues.redhat.com/browse/OSSM-2371[OSSM-2371] Despite Kiali being configured as "view-only," a user can change the proxy logging level via the Workload details' Logs tab's kebab menu. This issue has been fixed so the options under "Set Proxy Log Level" are disabled when Kiali is configured as "view-only."

* https://issues.redhat.com/browse/OSSM-2344[OSSM-2344] Restarting Istiod causes Kiali to flood CRI-O with port-forward requests. This issue occurred when Kiali could not connect to Istiod and Kiali simultaneously issued a large number of requests to istiod. Kiali now limits the number of requests it sends to istiod.

* https://issues.redhat.com/browse/OSSM-2335[OSSM-2335] Dragging the mouse pointer over the Traces scatterchart plot sometimes caused the Kiali console to stop responding due to concurrent backend requests.

* https://issues.redhat.com/browse/OSSM-2221[OSSM-2221] Previously, gateway injection in the `ServiceMeshControlPlane` namespace was not possible because the `ignore-namespace` label was applied to the namespace by default.
+
When creating a v2.4 control plane, the namespace no longer has the `ignore-namespace` label applied, and gateway injection is possible.
+
In the following example, the `oc label` command removes the `ignore-namespace` label from a namespace in an existing deployment:
+
[source,terminal]
----
$ oc label namespace <istio_system> maistra.io/ignore-namespace-
----
+
In the example above, <istio_system> represents the name of the `ServiceMeshControlPlane` namespace.

* https://issues.redhat.com/browse/OSSM-2053[OSSM-2053] Using {SMProductName} Operator 2.2 or 2.3, during SMCP reconciliation, the SMMR controller removed the member namespaces from `SMMR.status.configuredMembers`. This caused the services in the member namespaces to become unavailable for a few moments.
+
Using {SMProductName} Operator 2.2 or 2.3, the SMMR controller no longer removes the namespaces from `SMMR.status.configuredMembers`. Instead, the controller adds the namespaces to `SMMR.status.pendingMembers` to indicate that they are not up-to-date. During reconciliation, as each namespace synchronizes with the SMCP, the namespace is automatically removed from `SMMR.status.pendingMembers`.

* https://issues.redhat.com/browse/OSSM-1962[OSSM-1962] Use `EndpointSlices` in federation controller. The federation controller now uses `EndpointSlices`, which improves scalability and performance in large deployments. The PILOT_USE_ENDPOINT_SLICE flag is enabled by default. Disabling the flag prevents use of federation deployments.

* https://issues.redhat.com/browse/OSSM-1668[OSSM-1668] A new field `spec.security.jwksResolverCA` was added to the Version 2.1 `SMCP` but was missing in the 2.2.0 and 2.2.1 releases. When upgrading from an Operator version where this field was present to an Operator version that was missing this field, the `.spec.security.jwksResolverCA` field was not available in the `SMCP`.

* https://issues.redhat.com/browse/OSSM-1325[OSSM-1325] istiod pod crashes and displays the following error message: `fatal error: concurrent map iteration and map write`.

* https://issues.redhat.com/browse/OSSM-1211[OSSM-1211]
Configuring Federated service meshes for failover does not work as expected.
+
The Istiod pilot log displays the following error: `envoy connection [C289] TLS error: 337047686:SSL routines:tls_process_server_certificate:certificate verify failed`

* https://issues.redhat.com/browse/OSSM-1099[OSSM-1099]
The Kiali console displayed the message `Sorry, there was a problem. Try a refresh or navigate to a different page.`

* https://issues.redhat.com/browse/OSSM-1074[OSSM-1074]
Pod annotations defined in SMCP are not injected in the pods.

* https://issues.redhat.com/browse/OSSM-999[OSSM-999]
Kiali retention did not work as expected. Calendar times were greyed out in the dashboard graph.

* link:https://issues.redhat.com/browse/OSSM-797[OSSM-797] Kiali Operator pod generates `CreateContainerConfigError` while installing or updating the operator.

* https://issues.redhat.com/browse/OSSM-722[OSSM-722]
Namespace starting with `kube` is hidden from Kiali.

* link:https://issues.redhat.com/browse/OSSM-569[OSSM-569] There is no CPU memory limit for the Prometheus `istio-proxy` container. The Prometheus `istio-proxy` sidecar now uses the resource limits defined in `spec.proxy.runtime.container`.

* link:https://issues.redhat.com/browse/OSSM-535[OSSM-535] Support validationMessages in SMCP. The `ValidationMessages` field in the Service Mesh Control Plane can now be set to `True`. This writes a log for the status of the resources, which can be helpful when troubleshooting problems.

* link:https://issues.redhat.com/browse/OSSM-449[OSSM-449] VirtualService and Service causes an error "Only unique values for domains are permitted. Duplicate entry of domain."

* link:https://issues.redhat.com/browse/OSSM-419[OSSM-419] Namespaces with similar names will all show in Kiali namespace list, even though namespaces may not be defined in Service Mesh Member Role.

* link:https://issues.redhat.com/browse/OSSM-296[OSSM-296] When adding health configuration to the Kiali custom resource (CR) is it not being replicated to the Kiali configmap.

* link:https://issues.redhat.com/browse/OSSM-291[OSSM-291] In the Kiali console, on the Applications, Services, and Workloads pages, the "Remove Label from Filters" function is not working.

* link:https://issues.redhat.com/browse/OSSM-289[OSSM-289] In the Kiali console, on the Service Details pages for the 'istio-ingressgateway' and 'jaeger-query' services there are no Traces being displayed. The traces exist in Jaeger.

* link:https://issues.redhat.com/browse/OSSM-287[OSSM-287] In the Kiali console there are no traces being displayed on the Graph Service.

* link:https://issues.redhat.com/browse/OSSM-285[OSSM-285] When trying to access the Kiali console, receive the following error message "Error trying to get OAuth Metadata".
+
Workaround: Restart the Kiali pod.

* link:https://issues.redhat.com/browse/MAISTRA-2735[MAISTRA-2735] The resources that the Service Mesh Operator deletes when reconciling the SMCP changed in {SMProductName} version 2.1. Previously, the Operator deleted a resource with the following labels:

** `maistra.io/owner`
** `app.kubernetes.io/version`

+
Now, the Operator ignores resources that does not also include the `app.kubernetes.io/managed-by=maistra-istio-operator` label. If you create your own resources, you should not add the `app.kubernetes.io/managed-by=maistra-istio-operator` label to them.


* link:https://issues.jboss.org/browse/MAISTRA-2687[MAISTRA-2687] {SMProductName} 2.1 federation gateway does not send the full certificate chain when using external certificates. The {SMProductShortName} federation egress gateway only sends the client certificate. Because the federation ingress gateway only knows about the root certificate, it cannot verify the client certificate unless you add the root certificate to the federation import `ConfigMap`.

* link:https://issues.redhat.com/browse/MAISTRA-2635[MAISTRA-2635] Replace deprecated Kubernetes API. To remain compatible with {product-title} 4.8, the `apiextensions.k8s.io/v1beta1` API was deprecated as of {SMProductName} 2.0.8.

* link:https://issues.redhat.com/browse/MAISTRA-2631[MAISTRA-2631] The WASM feature is not working because podman is failing due to nsenter binary not being present. {SMProductName} generates the following error message: `Error: error configuring CNI network plugin exec: "nsenter": executable file not found in $PATH`. The container image now contains nsenter and WASM works as expected.

* link:https://issues.redhat.com/browse/MAISTRA-2534[MAISTRA-2534] When istiod attempted to fetch the JWKS for an issuer specified in a JWT rule, the issuer service responded with a 502.  This prevented the proxy container from becoming ready and caused deployments to hang. The fix for the link:https://github.com/istio/istio/issues/24629[community bug] has been included in the  {SMProductShortName} 2.0.7 release.

* link:https://issues.jboss.org/browse/MAISTRA-2411[MAISTRA-2411] When the Operator creates a new ingress gateway using `spec.gateways.additionaIngress` in the `ServiceMeshControlPlane`, Operator is not creating a `NetworkPolicy` for the additional ingress gateway like it does for the default istio-ingressgateway. This is causing a 503 response from the route of the new gateway.
+
Workaround: Manually create the `NetworkPolicy` in the <istio-system> namespace.

* link:https://issues.redhat.com/browse/MAISTRA-2401[MAISTRA-2401] CVE-2021-3586 servicemesh-operator: NetworkPolicy resources incorrectly specified ports for ingress resources. The NetworkPolicy resources installed for {SMProductName} did not properly specify which ports could be accessed. This allowed access to all ports on these resources from any pod. Network policies applied to the following resources are affected:

** Galley
** Grafana
** Istiod
** Jaeger
** Kiali
** Prometheus
** Sidecar injector

* link:https://issues.redhat.com/browse/MAISTRA-2378[MAISTRA-2378] When the cluster is configured to use OpenShift SDN with `ovs-multitenant` and the mesh contains a large number of namespaces (200+), the {product-title} networking plugin is unable to configure the namespaces quickly. {SMProductShortName} times out causing namespaces to be continuously dropped from the service mesh and then reenlisted.

* link:https://issues.redhat.com/browse/MAISTRA-2370[MAISTRA-2370] Handle tombstones in listerInformer. The updated cache codebase was not handling tombstones when translating the events from the namespace caches to the aggregated cache, leading to a panic in the go routine.

* link:https://issues.redhat.com/browse/MAISTRA-2117[MAISTRA-2117] Add optional `ConfigMap` mount to operator. The CSV now contains an optional `ConfigMap` volume mount, which mounts the `smcp-templates` `ConfigMap` if it exists. If the `smcp-templates` `ConfigMap` does not exist, the mounted directory is empty. When you create the `ConfigMap`, the directory is populated with the entries from the `ConfigMap` and can be referenced in `SMCP.spec.profiles`. No restart of the Service Mesh operator is required.
+
Customers using the 2.0 operator with a modified CSV to mount the smcp-templates ConfigMap can upgrade to {SMProductName} 2.1. After upgrading, you can continue using an existing ConfigMap, and the profiles it contains, without editing the CSV. Customers that previously used ConfigMap with a different name will either have to rename the ConfigMap or update the CSV after upgrading.

* link:https://issues.redhat.com/browse/MAISTRA-2010[MAISTRA-2010] AuthorizationPolicy does not support `request.regex.headers` field. The `validatingwebhook` rejects any AuthorizationPolicy with the field, and even if you disable that, Pilot tries to validate it using the same code, and it does not work.

* link:https://issues.jboss.org/browse/MAISTRA-1979[MAISTRA-1979] _Migration to 2.0_ The conversion webhook drops the following important fields when converting `SMCP.status` from v2 to v1:

** conditions
** components
** observedGeneration
** annotations
+
Upgrading the operator to 2.0 might break client tools that read the SMCP status using the maistra.io/v1 version of the resource.
+
This also causes the READY and STATUS columns to be empty when you run `oc get servicemeshcontrolplanes.v1.maistra.io`.

* link:https://issues.jboss.org/browse/MAISTRA-1947[MAISTRA-1947] _Technology Preview_ Updates to ServiceMeshExtensions are not applied.
+
Workaround: Remove and recreate the `ServiceMeshExtensions`.

* link:https://issues.redhat.com/browse/MAISTRA-1983[MAISTRA-1983] _Migration to 2.0_ Upgrading to 2.0.0 with an existing invalid `ServiceMeshControlPlane` cannot easily be repaired. The invalid items in the `ServiceMeshControlPlane` resource caused an unrecoverable error. The fix makes the errors recoverable. You can delete the invalid resource and replace it with a new one or edit the resource to fix the errors. For more information about editing your resource, see [Configuring the Red Hat OpenShift Service Mesh installation].

* link:https://issues.redhat.com/browse/MAISTRA-1502[MAISTRA-1502] As a result of CVEs fixes in version 1.0.10, the Istio dashboards are not available from the *Home Dashboard* menu in Grafana. To access the Istio dashboards, click the *Dashboard* menu in the navigation panel and select the *Manage* tab.

* link:https://issues.redhat.com/browse/MAISTRA-1399[MAISTRA-1399] {SMProductName} no longer prevents you from installing unsupported CNI protocols. The supported network configurations has not changed.

* link:https://issues.jboss.org/browse/MAISTRA-1089[MAISTRA-1089] _Migration to 2.0_ Gateways created in a non-control plane namespace are automatically deleted. After removing the gateway definition from the SMCP spec, you need to manually delete these resources.

* link:https://issues.jboss.org/browse/MAISTRA-858[MAISTRA-858] The following Envoy log messages describing link:https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated[deprecated options and configurations associated with Istio 1.1.x] are expected:
+
** [2019-06-03 07:03:28.943][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:129] Using deprecated option 'envoy.api.v2.listener.Filter.config'. This configuration will be removed from Envoy soon.
** [2019-08-12 22:12:59.001][13][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon.

* link:https://issues.jboss.org/browse/MAISTRA-806[MAISTRA-806] Evicted Istio Operator Pod causes mesh and CNI not to deploy.
+
Workaround: If the `istio-operator` pod is evicted while deploying the control pane, delete the evicted `istio-operator` pod.

* link:https://issues.jboss.org/browse/MAISTRA-681[MAISTRA-681] When the {SMProductShortName} control plane has many namespaces, it can lead to performance issues.

* link:https://issues.jboss.org/browse/MAISTRA-193[MAISTRA-193] Unexpected console info messages are visible when health checking is enabled for citadel.

* link:https://bugzilla.redhat.com/show_bug.cgi?id=1821432[Bugzilla 1821432] The toggle controls in {product-title} Custom Resource details page does not update the CR correctly. UI Toggle controls in the {SMProductShortName} Control Plane (SMCP) Overview page in the {product-title} web console sometimes updates the wrong field in the resource. To update a SMCP, edit the YAML content directly or update the resource from the command line instead of clicking the toggle controls.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-architecture"]
= Understanding Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-architecture

toc::[]

{SMProductName} provides a platform for behavioral insight and operational control over your networked microservices in a service mesh. With {SMProductName}, you can connect, secure, and monitor microservices in your {product-title} environment.

:leveloffset: +1

////
Module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-understanding-service-mesh_{context}"]
= Understanding service mesh

A _service mesh_ is the network of microservices that make up applications in a distributed microservice architecture and the interactions between those microservices. When a {SMProductShortName} grows in size and complexity, it can become harder to understand and manage.

Based on the open source link:https://istio.io/[Istio] project, {SMProductName} adds a transparent layer on existing distributed applications without requiring any changes to the service code. You add {SMProductName} support to services by deploying a special sidecar proxy to relevant services in the mesh that intercepts all network communication between microservices. You configure and manage the {SMProductShortName} using the {SMProductShortName} control plane features.

{SMProductName} gives you an easy way to create a network of deployed services that provide:

* Discovery
* Load balancing
* Service-to-service authentication
* Failure recovery
* Metrics
* Monitoring

{SMProductName} also provides more complex operational functions including:

* A/B testing
* Canary releases
* Access control
* End-to-end authentication

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// -service_mesh/v2x/ossm-architecture.adoc

[id="ossm-architecture_{context}"]
= Service Mesh architecture

Service mesh technology operates at the network communication level. That is, service mesh components capture or intercept traffic to and from microservices, either modifying requests, redirecting them, or creating new requests to other services.

image::ossm-architecture.png[Service Mesh architecture image]

At a high level, {SMProductName} consists of a data plane and a control plane

The *data plane* is a set of intelligent proxies, running alongside application containers in a pod, that intercept and control all inbound and outbound network communication between microservices in the service mesh.
The data plane is implemented in such a way that it intercepts all inbound (ingress) and outbound (egress) network traffic. The Istio data plane is composed of Envoy containers running along side application containers in a pod. The Envoy container acts as a proxy, controlling all network communication into and out of the pod.

* *Envoy proxies* are the only Istio components that interact with data plane traffic. All incoming (ingress) and outgoing (egress) network traffic between services flows through the proxies. The Envoy proxy also collects all metrics related to services traffic within the mesh. Envoy proxies are deployed as sidecars, running in the same pod as services. Envoy proxies are also used to implement mesh gateways.

** *Sidecar proxies* manage inbound and outbound communication for their workload instance.

** *Gateways* are proxies operating as load balancers receiving incoming or outgoing HTTP/TCP connections. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads. You use a Gateway to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh.

*** *Ingress-gateway* - Also known as an Ingress Controller, the Ingress Gateway is a dedicated Envoy proxy that receives and controls traffic entering the service mesh. An Ingress Gateway allows features such as monitoring and route rules to be applied to traffic entering the cluster.

*** *Egress-gateway* - Also known as an egress controller, the Egress Gateway is a dedicated Envoy proxy that manages traffic leaving the service mesh. An Egress Gateway allows features such as monitoring and route rules to be applied to traffic exiting the mesh.

The *control plane* manages and configures the proxies that make up the data plane. It is the authoritative source for configuration, manages access control and usage policies, and collects metrics from the proxies in the service mesh.

* The Istio control plane is composed of *Istiod* which consolidates several previous control plane components (Citadel, Galley, Pilot) into a single binary. Istiod provides service discovery, configuration, and certificate management. It converts high-level routing rules to Envoy configurations and propagates them to the sidecars at runtime.

** Istiod can act as a Certificate Authority (CA), generating certificates supporting secure mTLS communication in the data plane. You can also use an external CA for this purpose.

** Istiod is responsible for injecting sidecar proxy containers into workloads deployed to an OpenShift cluster.

{SMProductName} uses the *istio-operator* to manage the installation of the control plane. An _Operator_ is a piece of software that enables you to implement and automate common activities in your OpenShift cluster. It acts as a controller, allowing you to set or change the desired state of objects in your cluster, in this case, a {SMProductName} installation.

{SMProductName} also bundles the following Istio add-ons as part of the product:

* *Kiali* - Kiali is the management console for {SMProductName}. It provides dashboards, observability, and robust configuration and validation capabilities. It shows the structure of your service mesh by inferring traffic topology and displays the health of your mesh. Kiali provides detailed metrics, powerful validation, access to Grafana, and strong integration with the {JaegerShortName}.

* *Prometheus* - {SMProductName} uses Prometheus to store telemetry information from services. Kiali depends on Prometheus to obtain metrics, health status, and mesh topology.

* *Jaeger* - {SMProductName} supports the {JaegerShortName}. Jaeger is an open source traceability server that centralizes and displays traces associated with a single request between multiple services. Using the {JaegerShortName} you can monitor and troubleshoot your microservices-based distributed systems.

* *Elasticsearch* - Elasticsearch is an open source, distributed, JSON-based search and analytics engine. The {JaegerShortName} uses Elasticsearch for persistent storage.

* *Grafana* - Grafana provides mesh administrators with advanced query and metrics analysis and dashboards for Istio data. Optionally, Grafana can be used to analyze service mesh metrics.

The following Istio integrations are supported with {SMProductName}:

* *3scale* - Istio provides an optional integration with Red Hat 3scale API Management solutions. For versions prior to 2.1, this integration was achieved via the 3scale Istio adapter. For version 2.1 and later, the 3scale integration is achieved via a WebAssembly module.

:leveloffset: 2

For information about how to install the 3scale adapter, refer to the xref:threescale-adapter[3scale Istio adapter documentation]

== Understanding Kiali

Kiali provides visibility into your service mesh by showing you the microservices in your service mesh, and how they are connected.

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-kiali-overview_{context}"]
= Kiali overview

Kiali provides observability into the {SMProductShortName} running on {product-title}. Kiali helps you define, validate, and observe your Istio service mesh. It helps you to understand the structure of your service mesh by inferring the topology, and also provides information about the health of your service mesh.

Kiali provides an interactive graph view of your namespace in real time that provides visibility into features like circuit breakers, request rates, latency, and even graphs of traffic flows. Kiali offers insights about components at different levels, from Applications to Services and Workloads, and can display the interactions with contextual information and charts on the selected graph node or edge. Kiali also provides the ability to validate your Istio configurations, such as gateways, destination rules, virtual services, mesh policies, and more. Kiali provides detailed metrics, and a basic Grafana integration is available for advanced queries. Distributed tracing is provided by integrating Jaeger into the Kiali console.

Kiali is installed by default as part of the {SMProductName}.

:leveloffset: 2

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-kiali-architecture_{context}"]
= Kiali architecture

Kiali is based on the open source link:https://kiali.io/[Kiali project]. Kiali is composed of two components: the Kiali application and the Kiali console.

* *Kiali application* (back end) – This component runs in the container application platform and communicates with the service mesh components, retrieves and processes data, and exposes this data to the console. The Kiali application does not need storage. When deploying the application to a cluster, configurations are set in ConfigMaps and secrets.

* *Kiali console* (front end) – The Kiali console is a web application. 	The Kiali application serves the Kiali console, which then queries the back end for data to present it to the user.

In addition, Kiali depends on external services and components provided by the container application platform and Istio.

* *Red Hat Service Mesh* (Istio) - Istio is a Kiali requirement. Istio is the component that provides and controls the service mesh. Although Kiali and Istio can be installed separately, Kiali depends on Istio and will not work if it is not present. Kiali needs to retrieve Istio data and configurations, which are exposed through Prometheus and the cluster API.

* *Prometheus* - A dedicated Prometheus instance is included as part of the {SMProductName} installation. When Istio telemetry is enabled, metrics data are stored in Prometheus. Kiali uses this Prometheus data to determine the mesh topology, display metrics, calculate health, show possible problems, and so on. Kiali communicates directly with Prometheus and assumes the data schema used by Istio Telemetry. Prometheus is an Istio dependency and a hard dependency for Kiali, and many of Kiali's features will not work without Prometheus.

* *Cluster API* - Kiali uses the API of the {product-title} (cluster API) to fetch and resolve service mesh configurations. Kiali queries the cluster API to retrieve, for example, definitions for namespaces, services, deployments, pods, and other entities. Kiali also makes queries to resolve relationships between the different cluster entities. The cluster API is also queried to retrieve Istio configurations like virtual services, destination rules, route rules, gateways, quotas, and so on.

* *Jaeger* - Jaeger is optional, but is installed by default as part of the {SMProductName} installation. When you install the {JaegerShortName} as part of the default {SMProductName} installation, the Kiali console includes a tab to display distributed tracing data. Note that tracing data will not be available if you disable Istio's distributed tracing feature. Also note that user must have access to the namespace where the {SMProductShortName} control plane is installed to view tracing data.

* *Grafana* - Grafana is optional, but is installed by default as part of the {SMProductName} installation. When available, the metrics pages of Kiali display links to direct the user to the same metric in Grafana. Note that user must have access to the namespace where the {SMProductShortName} control plane is installed to view links to the Grafana dashboard and view Grafana data.

:leveloffset: 2

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

[id="ossm-kiali-features_{context}"]
= Kiali features
//In the title include nouns or noun phrases that are used in the body text.
//Do not start the title of concept modules with a verb..

The Kiali console is integrated with Red Hat Service Mesh and provides the following capabilities:

* *Health* – Quickly identify issues with applications, services, or workloads.

* *Topology* – Visualize how your applications, services, or workloads communicate via the Kiali graph.

* *Metrics* – Predefined metrics dashboards let you chart service mesh and application performance for Go, Node.js. Quarkus, Spring Boot, Thorntail and Vert.x. You can also create your own custom dashboards.

* *Tracing* – Integration with Jaeger lets you follow the path of a request through various microservices that make up an application.

* *Validations* – Perform advanced validations on the most common Istio objects (Destination Rules, Service Entries, Virtual Services, and so on).

* *Configuration* – Optional ability to create, update and delete Istio routing configuration using wizards or directly in the YAML editor in the Kiali Console.

:leveloffset: 2

== Understanding distributed tracing

Every time a user takes an action in an application, a request is executed by the architecture that may require dozens of different services to participate to produce a response.
The path of this request is a distributed transaction. The {JaegerShortName} lets you perform distributed tracing, which follows the path of a request through various microservices that make up an application.

*Distributed tracing* is a technique that is used to tie the information about different units of work together—usually executed in different processes or hosts—to understand a whole chain of events in a distributed transaction.
Distributed tracing lets developers visualize call flows in large service oriented architectures.
It can be invaluable in understanding serialization, parallelism, and sources of latency.

The {JaegerShortName} records the execution of individual requests across the whole stack of microservices, and presents them as traces. A *trace* is a data/execution path through the system. An end-to-end trace comprises one or more spans.

A *span* represents a logical unit of work that has an operation name, the start time of the operation, and the duration. Spans may be nested and ordered to model causal relationships.

:leveloffset: +2

// Module included in the following assemblies:
//
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-0.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-1.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-2.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-3.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-4.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-5.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-6.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-7.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-8.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-9.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-3-0.adoc
// * distr_tracing_arch/distr-tracing-architecture.adoc
// * service_mesh/v2x/ossm-architecture.adoc
// * serverless/serverless-tracing.adoc

:_mod-docs-content-type: CONCEPT
[id="distr-tracing-product-overview_{context}"]
= Distributed tracing overview

As a service owner, you can use distributed tracing to instrument your services to gather insights into your service architecture.
You can use the {DTProductName} for monitoring, network profiling, and troubleshooting the interaction between components in modern, cloud-native, microservices-based applications.

With the {DTShortName}, you can perform the following functions:

* Monitor distributed transactions

* Optimize performance and latency

* Perform root cause analysis

The {DTShortName} consists of three components:

* *{JaegerName}*, which is based on the open source link:https://www.jaegertracing.io/[Jaeger project].

* *{TempoName}*, which is based on the open source link:https://grafana.com/oss/tempo/[Grafana Tempo project].

* *{OTELNAME}*, which is based on the open source link:https://opentelemetry.io/[OpenTelemetry project].

[IMPORTANT]
====
Jaeger does not use FIPS validated cryptographic modules.
====

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
-service_mesh/v2x/ossm-architecture.adoc
-dist_tracing_arch/distr-tracing-architecture.adoc
////
:_mod-docs-content-type: CONCEPT
[id="distr-tracing-architecture_{context}"]
= {DTProductName} architecture

{DTProductName} is made up of several components that work together to collect, store, and display tracing data.

* *{JaegerName}* - This component is based on the open source link:https://www.jaegertracing.io/[Jaeger project].

** *Client* (Jaeger client, Tracer, Reporter, instrumented application, client libraries)- The {JaegerShortName} clients are language-specific implementations of the OpenTracing API. They can be used to instrument applications for distributed tracing either manually or with a variety of existing open source frameworks, such as Camel (Fuse), Spring Boot (RHOAR), MicroProfile (RHOAR/Thorntail), Wildfly (EAP), and many more, that are already integrated with OpenTracing.

** *Agent* (Jaeger agent, Server Queue, Processor Workers) - The {JaegerShortName} agent is a network daemon that listens for spans sent over User Datagram Protocol (UDP), which it batches and sends to the Collector. The agent is meant to be placed on the same host as the instrumented application. This is typically accomplished by having a sidecar in container environments such as Kubernetes.

** *Jaeger Collector* (Collector, Queue, Workers) - Similar to the Jaeger agent, the Jaeger Collector receives spans and places them in an internal queue for processing. This allows the Jaeger Collector to return immediately to the client/agent instead of waiting for the span to make its way to the storage.

** *Storage* (Data Store) - Collectors require a persistent storage backend. {JaegerName} has a pluggable mechanism for span storage. Note that for this release, the only supported storage is Elasticsearch.

** *Query* (Query Service) - Query is a service that retrieves traces from storage.

** *Ingester* (Ingester Service) - {DTProductName} can use Apache Kafka as a buffer between the Collector and the actual Elasticsearch backing storage. Ingester is a service that reads data from Kafka and writes to the Elasticsearch storage backend.

** *Jaeger Console* – With the {JaegerName} user interface, you can visualize your distributed tracing data. On the Search page, you can find traces and explore details of the spans that make up an individual trace.

* *{TempoName}* - This component is based on the open source link:https://grafana.com/oss/tempo/[Grafana Tempo project].

** *Gateway* – The Gateway handles authentication, authorization, and forwarding requests to the Distributor or Query front-end service.

** *Distributor* – The Distributor accepts spans in multiple formats including Jaeger, OpenTelemetry, and Zipkin. It routes spans to Ingesters by hashing the `+traceID+` and using a distributed consistent hash ring.

** *Ingester* – The Ingester batches a trace into blocks, creates bloom filters and indexes, and then flushes it all to the back end.

** *Query Frontend* – The Query Frontend is responsible for sharding the search space for an incoming query. The search query is then sent to the Queriers. The Query Frontend deployment exposes the Jaeger UI through the Tempo Query sidecar.

** *Querier* - The Querier is responsible for finding the requested trace ID in either the Ingesters or the back-end storage. Depending on parameters, it can query the Ingesters and pull Bloom indexes from the back end to search blocks in object storage.

** *Compactor* – The Compactors stream blocks to and from the back-end storage to reduce the total number of blocks.

* *{OTELName}* - This component is based on the open source link:https://opentelemetry.io/[OpenTelemetry project].

** *OpenTelemetry Collector* - The OpenTelemetry Collector is a vendor-agnostic way to receive, process, and export telemetry data. The OpenTelemetry Collector supports open-source observability data formats, for example, Jaeger and Prometheus, sending to one or more open-source or commercial back-ends. The Collector is the default location instrumentation libraries export their telemetry data.


:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
-service_mesh/v2x/ossm-architecture.adoc
-dist_tracing_arch/distr-tracing-architecture.adoc
////

[id="distr-tracing-features_{context}"]
= {DTProductName} features

{DTProductName} provides the following capabilities:

* Integration with Kiali – When properly configured, you can view {DTShortName} data from the Kiali console.

* High scalability – The {DTShortName} back end is designed to have no single points of failure and to scale with the business needs.

* Distributed Context Propagation – Enables you to connect data from different components together to create a complete end-to-end trace.

* Backwards compatibility with Zipkin – {DTProductName} has APIs that enable it to be used as a drop-in replacement for Zipkin, but Red Hat is not supporting Zipkin compatibility in this release.

:leveloffset: 2

== Next steps

* xref:preparing-ossm-installation[Prepare to install {SMProductName}] in your {product-title} environment.

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-deployment-models"]
= Service mesh deployment models
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-deployment-models

toc::[]

{SMProductName} supports several different deployment models that can be combined in different ways to best suit your business requirements.

In Istio, a tenant is a group of users that share common access and privileges for a set of deployed workloads. You can use tenants to provide a level of isolation between different teams. You can segregate access to different tenants using `NetworkPolicies`, `AuthorizationPolicies`, and `exportTo` annotations on istio.io or service resources.

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-deployment-models.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-deploy-cluster-wide-mesh_{context}"]
= Cluster-Wide (Single Tenant) mesh deployment model

A cluster-wide deployment contains a Service Mesh Control Plane that monitors resources for an entire cluster. Monitoring resources for an entire cluster closely resembles Istio functionality in that the control plane uses a single query across all namespaces to monitor Istio and Kubernetes resources. As a result, cluster-wide deployments decrease the number of requests sent to the API server.

Similar to Istio, a cluster-wide mesh includes namespaces with the `istio-injection=enabled` namespace label by default. You can change this label by modifying the `spec.labelSelectors` field of the `ServiceMeshMemberRoll` resource.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-deploy-mod-v2x.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-deploy-multitenant_{context}"]
= Multitenant deployment model

{SMProductName} installs a `ServiceMeshControlPlane` that is configured for multitenancy by default. {SMProductName} uses a multitenant Operator to manage the {SMProductShortName} control plane lifecycle. Within a mesh, namespaces are used for tenancy.

{SMProductName} uses `ServiceMeshControlPlane` resources to manage mesh installations, whose scope is limited by default to namespace that contains the resource. You use `ServiceMeshMemberRoll` and `ServiceMeshMember` resources to include additional namespaces into the mesh. A namespace can only be included in a single mesh, and multiple meshes can be installed in a single OpenShift cluster.

Typical service mesh deployments use a single {SMProductShortName} control plane to configure communication between services in the mesh. {SMProductName} supports “soft multitenancy”, where there is one control plane and one mesh per tenant, and there can be multiple independent control planes within the cluster. Multitenant deployments specify the projects that can access the {SMProductShortName} and isolate the {SMProductShortName} from other control plane instances.

The cluster administrator gets control and visibility across all the Istio control planes, while the tenant administrator only gets control over their specific {SMProductShortName}, Kiali, and Jaeger instances.

You can grant a team permission to deploy its workloads only to a given namespace or set of namespaces. If granted the `mesh-user` role by the service mesh administrator, users can create a `ServiceMeshMember` resource to add namespaces to the `ServiceMeshMemberRoll`.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-deploy-mod-v2x.adoc


[id="ossm-deploy-multi-mesh_{context}"]
= Multimesh or federated deployment model

_Federation_ is a deployment model that lets you share services and workloads between separate meshes managed in distinct administrative domains.

The Istio multi-cluster model requires a high level of trust between meshes and remote access to all Kubernetes API servers on which the individual meshes reside. {SMProductName} federation takes an opinionated approach to a multi-cluster implementation of Service Mesh that assumes _minimal_ trust between meshes.

A _federated mesh_ is a group of meshes behaving as a single mesh. The services in each mesh can be unique services, for example a mesh adding services by importing them from another mesh, can provide additional workloads for the same services across the meshes, providing high availability, or a combination of both. All meshes that are joined into a federated mesh remain managed individually, and you must explicitly configure which services are exported to and imported from other meshes in the federation. Support functions such as certificate generation, metrics and trace collection remain local in their respective meshes.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-vs-community"]
= Service Mesh and Istio differences
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-vs-istio

toc::[]

{SMProductName} differs from an installation of Istio to provide additional features or to handle differences when deploying on {product-title}.

// The following include statements pull in the module files that comprise the assembly.

:leveloffset: +1

////
Module included in the following assemblies:
-service_mesh/v2x/ossm-vs-community.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-vs-istio_{context}"]
= Differences between Istio and {SMProductName}

The following features are different in {SMProductShortName} and Istio.

[id="ossm-cli-tool_{context}"]
== Command line tool

The command line tool for {SMProductName} is `oc`.  {SMProductName} does not support `istioctl`.


[id="ossm-installation-upgrade_{context}"]
== Installation and upgrades

{SMProductName} does not support Istio installation profiles.

{SMProductName} does not support canary upgrades of the service mesh.


[id="ossm-automatic-injection_{context}"]
== Automatic injection

The upstream Istio community installation automatically injects the sidecar into pods within the projects you have labeled.

{SMProductName} does not automatically inject the sidecar into any pods, but you must opt in to injection using an annotation without labeling projects. This method requires fewer privileges and does not conflict with other {product-title} capabilities such as builder pods. To enable automatic injection, specify the `sidecar.istio.io/inject` label, or annotation, as described in the _Automatic sidecar injection_ section.

.Sidecar injection label and annotation settings
[options="header"]
[cols="a, a, a"]
|===
|
|Upstream Istio
|{SMProductName}

|Namespace Label
|supports "enabled" and "disabled"
|supports "disabled"

|Pod Label
|supports "true" and "false"
|supports "true" and "false"

|Pod Annotation
|supports "false" only
|supports "true" and "false"
|===


[id="ossm-rbac_{context}"]
== Istio Role Based Access Control features

Istio Role Based Access Control (RBAC) provides a mechanism you can use to control access to a service. You can identify subjects by user name or by specifying a set of properties and apply access controls accordingly.

The upstream Istio community installation includes options to perform exact header matches, match wildcards in headers, or check for a header containing a specific prefix or suffix.

{SMProductName} extends the ability to match request headers by using a regular expression. Specify a property key of `request.regex.headers` with a regular expression.

.Upstream Istio community matching request headers example
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: httpbin-usernamepolicy
spec:
  action: ALLOW
  rules:
    - when:
        - key: 'request.regex.headers[username]'
          values:
            - "allowed.*"
  selector:
    matchLabels:
      app: httpbin
----

[id="ossm-openssl_{context}"]
== OpenSSL

{SMProductName} replaces BoringSSL with OpenSSL. OpenSSL is a software library that contains an open source implementation of the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. The {SMProductName} Proxy binary dynamically links the OpenSSL libraries (libssl and libcrypto) from the underlying Red Hat Enterprise Linux operating system.

[id="ossm-external-workloads_{context}"]
== External workloads

{SMProductName} does not support external workloads, such as virtual machines running outside OpenShift on bare metal servers.

[id="ossm-virtual-machine-support_{context}"]
== Virtual Machine Support

You can deploy virtual machines to OpenShift using OpenShift Virtualization. Then, you can apply a mesh policy, such as mTLS or AuthorizationPolicy, to these virtual machines, just like any other pod that is part of a mesh.

[id="ossm-component-modifications_{context}"]
== Component modifications

* A _maistra-version_ label has been added to all resources.
* All Ingress resources have been converted to OpenShift Route resources.
* Grafana, distributed tracing (Jaeger), and Kiali are enabled by default and exposed through OpenShift routes.
* Godebug has been removed from all templates
* The `istio-multi` ServiceAccount and ClusterRoleBinding have been removed, as well as the `istio-reader` ClusterRole.

[id="ossm-envoy-filters_{context}"]
== Envoy filters

{SMProductName} does not support `EnvoyFilter` configuration except where explicitly documented. Due to tight coupling with the underlying Envoy APIs, backward compatibility cannot be maintained. `EnvoyFilter` patches are very sensitive to the format of the Envoy configuration that is generated by Istio. If the configuration generated by Istio changes, it has the potential to break the application of the `EnvoyFilter`.

[id="ossm-envoy-services_{context}"]
== Envoy services

{SMProductName} does not support QUIC-based services.

[id="ossm-cni_{context}"]
== Istio Container Network Interface (CNI) plugin

{SMProductName} includes CNI plugin, which provides you with an alternate way to configure application pod networking. The CNI plugin replaces the `init-container` network configuration eliminating the need to grant service accounts and projects access to security context constraints (SCCs) with elevated privileges.

[id="ossm-global-mtls_{context}"]
== Global mTLS settings
{SMProductName} creates a `PeerAuthentication` resource that enables or disables Mutual TLS authentication (mTLS) within the mesh.

[id="ossm-gateways_{context}"]
== Gateways

{SMProductName} installs ingress and egress gateways by default. You can disable gateway installation in the `ServiceMeshControlPlane` (SMCP) resource by using the following settings:

* `spec.gateways.enabled=false` to disable both ingress and egress gateways.
* `spec.gateways.ingress.enabled=false` to disable ingress gateways.
* `spec.gateways.egress.enabled=false`  to disable egress gateways.

[NOTE]
====
The Operator annotates the default gateways to indicate that they are generated by and managed by the {SMProductName} Operator.
====

[id="ossm-multicluster-configuration_{context}"]
== Multicluster configurations

{SMProductName} support for multicluster configurations is limited to the federation of service meshes across multiple clusters.

[id="ossm-certificate-signing-request_{context}"]
== Custom Certificate Signing Requests (CSR)

You cannot configure {SMProductName} to process CSRs through the Kubernetes certificate authority (CA).

[id="ossm-routes-gateways_{context}"]
== Routes for Istio Gateways

OpenShift routes for Istio Gateways are automatically managed in {SMProductName}. Every time an Istio Gateway is created, updated or deleted inside the service mesh, an OpenShift route is created, updated or deleted.

A {SMProductName} control plane component called Istio OpenShift Routing (IOR) synchronizes the gateway route. For more information, see Automatic route creation.

[id="ossm-catch-all-domains_{context}"]
=== Catch-all domains
Catch-all domains ("\*") are not supported. If one is found in the Gateway definition, {SMProductName} _will_ create the route, but will rely on OpenShift to create a default hostname. This means that the newly created route will __not__ be a catch all ("*") route, instead it will have a hostname in the form `<route-name>[-<project>].<suffix>`. See the {product-title} documentation for more information about how default hostnames work and how a `cluster-admin` can customize it. If you use {product-dedicated}, refer to the {product-dedicated} the `dedicated-admin` role.

[id="ossm-subdomains_{context}"]
=== Subdomains
Subdomains (e.g.: "*.domain.com") are supported. However this ability doesn't come enabled by default in {product-title}. This means that {SMProductName} _will_ create the route with the subdomain, but it will only be in effect if {product-title} is configured to enable it.

[id="ossm-tls_{context}"]
=== Transport layer security
Transport Layer Security (TLS) is supported. This means that, if the Gateway contains a `tls` section, the OpenShift Route will be configured to support TLS.

:leveloffset: 2
[discrete]
[id="additional-resources_ossm-vs-istio-v2x"]
[role="_additional-resources"]
==== Additional resources

* xref:ossm-auto-route_traffic-management[Automatic route creation]

:leveloffset: +1

////
Module included in the following assemblies:
-ossm-vs-community.adoc
////

[id="ossm-multitenant-install_{context}"]
= Multitenant installations

Whereas upstream Istio takes a single tenant approach, {SMProductName} supports multiple independent control planes within the cluster. {SMProductName} uses a multitenant operator to manage the control plane lifecycle.

{SMProductName} installs a multitenant control plane by default. You specify the projects that can access the {SMProductShortName}, and isolate the {SMProductShortName} from other control plane instances.

[id="ossm-mt-vs-clusterwide_{context}"]
== Multitenancy versus cluster-wide installations

The main difference between a multitenant installation and a cluster-wide installation is the scope of privileges used by istod. The components no longer use cluster-scoped Role Based Access Control (RBAC) resource `ClusterRoleBinding`.

Every project in the `ServiceMeshMemberRoll` `members` list will have a `RoleBinding` for each service account associated with the control plane deployment and each control plane deployment will only watch those member projects. Each member project has a `maistra.io/member-of` label added to it, where the `member-of` value is the project containing the control plane installation.

{SMProductName} configures each member project to ensure network access between itself, the control plane, and other member projects. The exact configuration differs depending on how {product-title} software-defined networking (SDN) is configured. See About OpenShift SDN for additional details.

If the {product-title} cluster is configured to use the SDN plugin:

* *`NetworkPolicy`*: {SMProductName} creates a `NetworkPolicy` resource in each member project allowing ingress to all pods from the other members and the control plane. If you remove a member from {SMProductShortName}, this `NetworkPolicy` resource is deleted from the project.
+
[NOTE]
====
This also restricts ingress to only member projects. If you require ingress from non-member projects, you need to create a `NetworkPolicy` to allow that traffic through.
====

* *Multitenant*: {SMProductName} joins the `NetNamespace` for each member project to the `NetNamespace` of the control plane project (the equivalent of running `oc adm pod-network join-projects --to control-plane-project member-project`). If you remove a member from the {SMProductShortName}, its `NetNamespace` is isolated from the control plane (the equivalent of running `oc adm pod-network isolate-projects member-project`).

* *Subnet*: No additional configuration is performed.

[id="ossm-cluster-scoped-resources_{context}"]
== Cluster scoped resources

Upstream Istio has two cluster scoped resources that it relies on. The `MeshPolicy` and the `ClusterRbacConfig`. These are not compatible with a multitenant cluster and have been replaced as described below.

* _ServiceMeshPolicy_ replaces MeshPolicy for configuration of control-plane-wide authentication policies. This must be created in the same project as the control plane.
* _ServicemeshRbacConfig_ replaces ClusterRbacConfig for configuration of control-plane-wide role based access control. This must be created in the same project as the control plane.

:leveloffset: 2

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-vs-community.adoc
-service_mesh/v2x/ossm-vs-community.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-kiali-service-mesh_{context}"]
= Kiali and service mesh

Installing Kiali via the Service Mesh on {product-title} differs from community Kiali installations in multiple ways. These modifications are sometimes necessary to resolve issues, provide additional features, or to handle differences when deploying on {product-title}.

* Kiali has been enabled by default.
* Ingress has been enabled by default.
* Updates have been made to the Kiali ConfigMap.
* Updates have been made to the ClusterRole settings for Kiali.
* Do not edit the ConfigMap, because your changes might be overwritten by the {SMProductShortName} or Kiali Operators. Files that the Kiali Operator manages have a `kiali.io/` label or annotation. Updating the Operator files should be restricted to those users with `cluster-admin` privileges. If you use {product-dedicated}, updating the Operator files should be restricted to those users with `dedicated-admin` privileges.

:leveloffset: 2

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-vs-community.adoc
-service_mesh/v2x/ossm-vs-community.adoc
////

[id="ossm-jaeger-service-mesh_{context}"]
= Distributed tracing and service mesh

Installing the {JaegerShortName} with the Service Mesh on {product-title} differs from community Jaeger installations in multiple ways. These modifications are sometimes necessary to resolve issues, provide additional features, or to handle differences when deploying on {product-title}.

* Distributed tracing has been enabled by default for {SMProductShortName}.
* Ingress has been enabled by default for {SMProductShortName}.
* The name for the Zipkin port name has changed to `jaeger-collector-zipkin` (from `http`)
* Jaeger uses Elasticsearch for storage by default when you select either the `production` or `streaming` deployment option.
* The community version of Istio provides a generic "tracing" route. {SMProductName} uses a "jaeger" route that is installed by the {JaegerName} Operator and is already protected by OAuth.
* {SMProductName} uses a sidecar for the Envoy proxy, and Jaeger also uses a sidecar, for the Jaeger agent.
These two sidecars are configured separately and should not be confused with each other. The proxy sidecar creates spans related to the pod's ingress and egress traffic. The agent sidecar receives the spans emitted by the application and sends them to the Jaeger Collector.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="preparing-ossm-installation"]
= Preparing to install Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: preparing-ossm-installation

toc::[]

Before you can install {SMProductName}, you must subscribe to {product-title} and install {product-title} in a supported configuration.

== Prerequisites

* Maintain an active {product-title} subscription on your Red Hat account. If you do not have a subscription, contact your sales representative for more information.

* Review the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#installation-overview_architecture-installation[{product-title} {product-version} overview].
* Install {product-title} {product-version}. If you are installing {SMProductName} on a link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#supported-installation-methods-for-different-platforms[restricted network], follow the instructions for your chosen {product-title} infrastructure.
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-account[Install {product-title} {product-version} on AWS]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-user-infra[Install {product-title} {product-version} on user-provisioned AWS]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-bare-metal[Install {product-title} {product-version} on bare metal]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-vsphere[Install {product-title} {product-version} on vSphere]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-ibm-z[Install {product-title} {product-version} on {ibm-z-name} and {ibm-linuxone-name}]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-ibm-power[Install {product-title} {product-version} on {ibm-power-name}]

* Install the version of the {product-title} command line utility (the `oc` client tool) that matches your {product-title} version and add it to your path.
** If you are using {product-title} {product-version}, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/cli_tools/#cli-about-cli_cli-developer-commands[About the OpenShift CLI].

For additional information about {SMProductName} lifecycle and supported platforms, refer to the link:https://access.redhat.com/support/policy/updates/openshift#ossm[Support Policy].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/preparing-ossm-install.adoc
// * service_mesh/v2x/servicemesh-release-notes.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-supported-configurations_{context}"]
= Supported configurations

The following configurations are supported for the current release of {SMProductName}.

[id="ossm-supported-platforms_{context}"]
== Supported platforms

The {SMProductName} Operator supports multiple versions of the `ServiceMeshControlPlane` resource. Version {MaistraVersion} {SMProductShortName} control planes are supported on the following platform versions:

* Red Hat {product-title} version 4.10 or later.
* {product-dedicated} version 4.
* Azure Red Hat OpenShift (ARO) version 4.
* Red Hat OpenShift Service on AWS (ROSA).

[id="ossm-unsupported-configurations_{context}"]
== Unsupported configurations

Explicitly unsupported cases include:

* OpenShift Online is not supported for {SMProductName}.
* {SMProductName} does not support the management of microservices outside the cluster where {SMProductShortName} is running.

[id="ossm-supported-configurations-networks_{context}"]
== Supported network configurations

{SMProductName} supports the following network configurations.

* OpenShift-SDN
* OVN-Kubernetes is available on all supported versions of {product-title}.
* Third-Party Container Network Interface (CNI) plugins that have been certified on {product-title} and passed {SMProductShortName} conformance testing. See link:https://access.redhat.com/articles/5436171[Certified OpenShift CNI Plug-ins] for more information.

[id="ossm-supported-configurations-sm_{context}"]
== Supported configurations for {SMProductShortName}

* This release of {SMProductName} is only available on {product-title} x86_64, {ibm-z-name}, and {ibm-power-name}.
** {ibm-z-name} is only supported on {product-title} 4.10 and later.
** {ibm-power-name} is only supported on {product-title} 4.10 and later.
* Configurations where all {SMProductShortName} components are contained within a single {product-title} cluster.
* Configurations that do not integrate external services such as virtual machines.
* {SMProductName} does not support `EnvoyFilter` configuration except where explicitly documented.

[id="ossm-supported-configurations-kiali_{context}"]
== Supported configurations for Kiali

* The Kiali console is only supported on the two most recent releases of the Google Chrome, Microsoft Edge, Mozilla Firefox, or Apple Safari browsers.
* The `openshift` authentication strategy is the only supported authentication configuration when Kiali is deployed with {SMProductName} (OSSM). The `openshift` strategy controls access based on the individual's role-based access control (RBAC) roles of the {product-title}.

[id="ossm-supported-configurations-jaeger_{context}"]
== Supported configurations for Distributed Tracing

* Jaeger agent as a sidecar is the only supported configuration for Jaeger. Jaeger as a daemonset is not supported for multitenant installations or OpenShift Dedicated.

[id="ossm-supported-configurations-webassembly_{context}"]
== Supported WebAssembly module

* 3scale WebAssembly is the only provided WebAssembly module. You can create custom WebAssembly modules.

:leveloffset: 2

== Next steps

* xref:installing-ossm[Install {SMProductName}] in your {product-title} environment.

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="installing-ossm"]
= Installing the Operators
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: installing-ossm

toc::[]

To install {SMProductName}, first install the required Operators on {product-title} and then create a `ServiceMeshControlPlane` resource to deploy the control plane.

[NOTE]
====
This basic installation is configured based on the default OpenShift settings and is not designed for production use.  Use this default installation to verify your installation, and then configure your service mesh for your specific environment.
====

.Prerequisites
* Read the xref:preparing-ossm-installation[Preparing to install {SMProductName}] process.
* An account with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

The following steps show how to install a basic instance of {SMProductName} on {product-title}.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/preparing-ossm-installation.adoc
// * service_mesh/v2x/preparing-ossm-installation.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-installation-activities_{context}"]
= Operator overview

{SMProductName} requires the following Operators:

* *OpenShift Elasticsearch* - (Optional) Provides database storage for tracing and logging with the {JaegerShortName}. It is based on the open source link:https://www.elastic.co/[Elasticsearch] project.
* *{JaegerName}* - Provides distributed tracing to monitor and troubleshoot transactions in complex distributed systems. It is based on the open source link:https://www.jaegertracing.io/[Jaeger] project.
* *Kiali Operator provided by Red Hat* - Provides observability for your service mesh. You can view configurations, monitor traffic, and analyze traces in a single console. It is based on the open source link:https://www.kiali.io/[Kiali] project.
* *{SMProductName}* - Allows you to connect, secure, control, and observe the microservices that comprise your applications. The {SMProductShortName} Operator defines and monitors the `ServiceMeshControlPlane` resources that manage the deployment, updating, and deletion of the {SMProductShortName} components. It is based on the open source link:https://istio.io/[Istio] project.

:leveloffset: 2

[WARNING]
====
Do not install Community versions of the Operators. Community Operators are not supported.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// - service_mesh/v1x/installing-ossm.adoc
// - service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-install-ossm-operator_{context}"]
= Installing the Operators

To install {SMProductName}, install the following Operators in this order. Repeat the procedure for each Operator.

* OpenShift Elasticsearch
* {JaegerName}
* Kiali Operator provided by Red Hat
* {SMProductName}

[NOTE]
====
If you have already installed the OpenShift Elasticsearch Operator as part of OpenShift Logging, you do not need to install the OpenShift Elasticsearch Operator again. The {JaegerName} Operator will create the Elasticsearch instance using the installed OpenShift Elasticsearch Operator.
====

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. In the {product-title} web console, click *Operators* -> *OperatorHub*.

. Type the name of the Operator into the filter box and select the Red Hat version of the Operator. Community versions of the Operators are not supported.

. Click *Install*.

. On the *Install Operator* page for each Operator, accept  the default settings.

. Click *Install*. Wait until the Operator has installed before repeating the steps for the next Operator in the list.
+
* The OpenShift Elasticsearch Operator is installed in the `openshift-operators-redhat` namespace and is available for all namespaces in the cluster.
* The {JaegerName} is installed in the `openshift-distributed-tracing` namespace and is available for all namespaces in the cluster.
* The Kiali Operator provided by Red Hat and the {SMProductName} Operator are installed in the `openshift-operators` namespace and are available for all namespaces in the cluster.

. After all you have installed all four Operators, click *Operators* -> *Installed Operators* to verify that your Operators installed.

:leveloffset: 2


:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-deployment-models.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-operator-infrastructure-node_{context}"]
= Configuring the {SMProductShortName} Operator to run on infrastructure nodes

This task should only be performed if the {SMProductShortName} Operator runs on an infrastructure node.

If the operator will run on a worker node, skip this task.

.Prerequisites

* The {SMProductShortName} Operator must be installed.

* One of the nodes comprising the deployment must be an infrastructure node. For more information, see "Creating infrastructure machine sets."

.Procedure

. List the operators installed in the namespace:
+
[source,terminal]
----
$ oc -n openshift-operators get subscriptions
----

. Edit the {SMProductShortName} Operator `Subscription` resource to specify where the operator should run:
+
[source,terminal]
----
$ oc -n openshift-operators edit subscription <name> <1>
----
<1> `<name>` represents the name of the `Subscription` resource. The default name of the `Subscription` resource is `servicemeshoperator`.

. Add the `nodeSelector` and `tolerations` to `spec.config` in the `Subscription` resource:
+
[source,yaml]
----
spec:
  config:
    nodeSelector: <1>
      node-role.kubernetes.io/infra: ""
    tolerations: <2>
    - effect: NoSchedule
      key: node-role.kubernetes.io/infra
      value: reserved
    - effect: NoExecute
      key: node-role.kubernetes.io/infra
      value: reserved
----
<1> Ensures that the operator pod is only scheduled on an infrastructure node.
<2> Ensures that the pod is accepted by the infrastructure node.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-confirm-operator-infrastructure-node_{context}"]
= Verifying the {SMProductShortName} Operator is running on infrastructure node

.Procedure

* Verify that the node associated with the Operator pod is an infrastructure node:
+
[source,terminal]
----
$ oc -n openshift-operators get po -l name=istio-operator -owide
----

:leveloffset: 2


== Next steps

* The {SMProductName} Operator does not create the {SMProductShortName} custom resource definitions (CRDs) until you deploy a {SMProductShortName} control plane. You can use the `ServiceMeshControlPlane` resource to install and configure the {SMProductShortName} components. For more information, see xref:ossm-create-smcp[Creating the ServiceMeshControlPlane].

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-create-smcp"]
= Creating the ServiceMeshControlPlane
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-create-smcp

toc::[]

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-smcp_{context}"]
= About ServiceMeshControlPlane

The control plane includes Istiod, Ingress and Egress Gateways, and other components, such as Kiali and Jaeger. The control plane must be deployed in a separate namespace than the {SMProductShortName} Operators and the data plane applications and services. You can deploy a basic installation of the `ServiceMeshControlPlane`(SMCP) from the {product-title} web console or the command line using the `oc` client tool.

[NOTE]
====
This basic installation is configured based on the default {product-title} settings and is not designed for production use. Use this default installation to verify your installation, and then configure your `ServiceMeshControlPlane` settings for your environment.
====

[NOTE]
====
Red Hat OpenShift Service on AWS (ROSA) places additional restrictions on where you can create resources, and as a result, the default deployment does not work. See Installing {SMProductShortName} on Red Hat OpenShift Service on AWS for additional requirements before deploying your SMCP in a ROSA environment.
====

[NOTE]
====
The {SMProductShortName} documentation uses `istio-system` as the example project, but you can deploy the service mesh to any project.
====


:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-deploy-operatorhub_{context}"]
= Deploying the {SMProductShortName} control plane from the web console

You can deploy a basic `ServiceMeshControlPlane` by using the web console.  In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

.Prerequisites

* The {SMProductName} Operator must be installed.
* An account with the `cluster-admin` role.

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Create a project named `istio-system`.
+
.. Navigate to *Home* -> *Projects*.
+
.. Click *Create Project*.
+
.. In the *Name* field, enter `istio-system`. The `ServiceMeshControlPlane` resource must be installed in a project that is separate from your microservices and Operators.
+
These steps use `istio-system` as an example, but you can deploy your {SMProductShortName} control plane in any project as long as it is separate from the project that contains your services.
+
.. Click *Create*.

. Navigate to *Operators* -> *Installed Operators*.

. Click the {SMProductName} Operator, then click *Istio Service Mesh Control Plane*.

. On the *Istio Service Mesh Control Plane* tab, click *Create ServiceMeshControlPlane*.

. On the *Create ServiceMeshControlPlane* page, accept the default {SMProductShortName} control plane version to take advantage of the features available in the most current version of the product. The version of the control plane determines the features available regardless of the version of the Operator.
+
.. Click *Create*. The Operator creates pods, services, and {SMProductShortName} control plane components based on your configuration parameters. You can configure `ServiceMeshControlPlane` settings later.
+
. To verify the control plane installed correctly, click the *Istio Service Mesh Control Plane* tab.
+
.. Click the name of the new control plane.
+
.. Click the *Resources* tab to see the {SMProductName} control plane resources the Operator created and configured.

:leveloffset: 2

:leveloffset: +2

// This module is included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-deploy-cli_{context}"]
= Deploying the {SMProductShortName} control plane using the CLI

You can deploy a basic `ServiceMeshControlPlane` from the command line.

.Prerequisites

* The {SMProductName} Operator must be installed.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. Create a project named `istio-system`.
+
[source,terminal]
----
$ oc new-project istio-system
----
+
. Create a `ServiceMeshControlPlane` file named `istio-installation.yaml` using the following example. The version of the {SMProductShortName} control plane determines the features available regardless of the version of the Operator.
+
.Example version {MaistraVersion} istio-installation.yaml
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  version: v{MaistraVersion}
  tracing:
    type: Jaeger
    sampling: 10000
  addons:
    jaeger:
      name: jaeger
      install:
        storage:
          type: Memory
    kiali:
      enabled: true
      name: kiali
    grafana:
      enabled: true
----
+
. Run the following command to deploy the {SMProductShortName} control plane, where `<istio_installation.yaml>` includes the full path to your file.
+
[source,terminal]
----
$ oc create -n istio-system -f <istio_installation.yaml>
----
+
. To watch the progress of the pod deployment, run the following command:
+
[source,terminal]
----
$ oc get pods -n istio-system -w
----
+
You should see output similar to the following:
+
[source,terminal]
----
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-b4d59bd7-mrgbr                 2/2     Running   0          65m
istio-egressgateway-678dc97b4c-wrjkp   1/1     Running   0          108s
istio-ingressgateway-b45c9d54d-4qg6n   1/1     Running   0          108s
istiod-basic-55d78bbbcd-j5556          1/1     Running   0          108s
jaeger-67c75bd6dc-jv6k6                2/2     Running   0          65m
kiali-6476c7656c-x5msp                 1/1     Running   0          43m
prometheus-58954b8d6b-m5std            2/2     Running   0          66m
----

:leveloffset: 2

:leveloffset: +2

////
This module is included in the following assemblies:
* service_mesh/v2x/ossm-create-smcp.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-validate-control-plane-cli_{context}"]
= Validating your SMCP installation with the CLI
You can validate the creation of the `ServiceMeshControlPlane` from the command line.

.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use Red Hat OpenShift Dedicated, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login https://<HOSTNAME>:6443
----
+
. Run the following command to verify the {SMProductShortName} control plane installation, where `istio-system` is the namespace where you installed the {SMProductShortName} control plane.
+
[source,terminal]
----
$ oc get smcp -n istio-system
----
+
The installation has finished successfully when the `STATUS` column is `ComponentsReady`.
+
[source,terminal]
----
NAME    READY   STATUS            PROFILES      VERSION   AGE
basic   10/10   ComponentsReady   ["default"]   2.1.1     66m
----

:leveloffset: 2


:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-control-plane-components-and-infrastructure-nodes_{context}"]
= About control plane components and infrastructure nodes

Infrastructure nodes provide a way to isolate infrastructure workloads for two primary purposes:

* To prevent incurring billing costs against subscription counts
* To separate maintenance and management of infrastructure workloads

You can configure some or all of the {SMProductShortName} control plane components to run on infrastructure nodes.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-deployment-models.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-control-plane-infrastructure-node-console_{context}"]
= Configuring all control plane components to run on infrastructure nodes using the web console

Perform this task if all of the components deployed by the {SMProductShortName} control plane will run on infrastructure nodes. These deployed components include Istiod, Ingress Gateway, and Egress Gateway, and optional applications such as Prometheus, Grafana, and Distributed Tracing.

If the control plane will run on a worker node, skip this task.

.Prerequisites

* You have installed the {SMProductName} Operator.
* You are logged in as a user with the `cluster-admin` role. If you use {product-dedicated}, you are logged in as a user with the `dedicated-admin` role.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.

. Click the {SMProductName} Operator, and then click *Istio Service Mesh Control Plane*.

. Click the name of the control plane resource. For example, `basic`.

. Click *YAML*.

. Add the `nodeSelector` and `tolerations` fields to the `spec.runtime.defaults.pod` specification in the `ServiceMeshControlPlane` resource, as shown in the following example:
+
[source,yaml]
----
spec:
  runtime:
    defaults:
      pod:
        nodeSelector: <1>
          node-role.kubernetes.io/infra: ""
        tolerations: <2>
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          value: reserved
        - effect: NoExecute
          key: node-role.kubernetes.io/infra
          value: reserved
----
<1> Ensures that the `ServiceMeshControlPlane` pod is only scheduled on an infrastructure node.
<2> Ensures that the pod is accepted by the infrastructure node for execution.

. Click *Save*.

. Click *Reload*.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-deployment-models.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-individual-control-plane-infrastructure-node-console_{context}"]
= Configuring individual control plane components to run on infrastructure nodes using the web console

Perform this task if individual components deployed by the {SMProductShortName} control plane will run on infrastructure nodes. These deployed components include Istiod, the Ingress Gateway, and the Egress Gateway.

If the control plane will run on a worker node, skip this task.

.Prerequisites

* You have installed the {SMProductName} Operator.
* You are logged in as a user with the `cluster-admin` role. If you use {product-dedicated}, you are logged in as a user with the `dedicated-admin` role.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.

. Click the {SMProductName} Operator, and then click *Istio Service Mesh Control Plane*.

. Click the name of the control plane resource. For example, `basic`.

. Click *YAML*.

. Add the `nodeSelector` and `tolerations` fields to the `spec.runtime.components.pilot.pod` specification in the `ServiceMeshControlPlane` resource, as shown in the following example:
+
[source,yaml]
----
spec:
  runtime:
    components:
      pilot:
        pod:
          nodeSelector: <1>
            node-role.kubernetes.io/infra: ""
          tolerations: <2>
          - effect: NoSchedule
            key: node-role.kubernetes.io/infra
            value: reserved
          - effect: NoExecute
            key: node-role.kubernetes.io/infra
            value: reserved
----
<1> Ensures that the `Istiod` pod is only scheduled on an infrastructure node.
<2> Ensures that the pod is accepted by the infrastructure node for execution.

. Add the `nodeSelector` and the `tolerations` fields to the `spec.gateways.ingress.runtime.pod` and `spec.gateways.egress.runtime.pod` specifications in the `ServiceMeshControlPlane` resource, as shown in the following example:
+
[source,yaml]
----
spec:
  gateways:
    ingress:
      runtime:
        pod:
          nodeSelector: <1>
            node-role.kubernetes.io/infra: ""
          tolerations: <2>
          - effect: NoSchedule
            key: node-role.kubernetes.io/infra
            value: reserved
          - effect: NoExecute
            key: node-role.kubernetes.io/infra
            value: reserved
    egress:
      runtime:
        pod:
          nodeSelector: <1>
            node-role.kubernetes.io/infra: ""
          tolerations: <2>
          - effect: NoSchedule
            key: node-role.kubernetes.io/infra
            value: reserved
          - effect: NoExecute
            key: node-role.kubernetes.io/infra
            value: reserved
----
<1> Ensures that the gateway pod is only scheduled on an infrastructure node
<2> Ensures that the pod is accepted by the infrastructure node for execution.

. Click *Save*.

. Click *Reload*.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-deployment-models.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-control-plane-infrastructure-node-cli_{context}"]
= Configuring all control plane components to run on infrastructure nodes using the CLI

Perform this task if all of the components deployed by the {SMProductShortName} control plane will run on infrastructure nodes. These deployed components include Istiod, Ingress Gateway, and Egress Gateway, and optional applications such as Prometheus, Grafana, and Distributed Tracing.

If the control plane will run on a worker node, skip this task.

.Prerequisites

* You have installed the {SMProductName} Operator.
* You are logged in as a user with the `cluster-admin` role. If you use {product-dedicated}, you are logged in as a user with the `dedicated-admin` role.

.Procedure

. Open the `ServiceMeshControlPlane` resource as a YAML file:
+
[source,terminal]
----
$ oc -n istio-system edit smcp <name> <1>
----
<1> `<name>` represents the name of the `ServiceMeshControlPlane` resource.

. To run all of the {SMProductShortName} components deployed by the `ServiceMeshControlPlane` on infrastructure nodes, add the `nodeSelector` and `tolerations` fields to the `spec.runtime.defaults.pod` spec in the `ServiceMeshControlPlane` resource:
+
[source,yaml]
----
spec:
  runtime:
    defaults:
      pod:
        nodeSelector: <1>
          node-role.kubernetes.io/infra: ""
        tolerations: <2>
        - effect: NoSchedule
          key: node-role.kubernetes.io/infra
          value: reserved
        - effect: NoExecute
          key: node-role.kubernetes.io/infra
          value: reserved
----
<1> Ensures that the SMCP pods are only scheduled on an infrastructure node.
<2> Ensures that the pods are accepted by the infrastructure node.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-deployment-models.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-individual-control-plane-infrastructure-node-cli_{context}"]
= Configuring individual control plane components to run on infrastructure nodes using the CLI

Perform this task if individual components deployed by the {SMProductShortName} control plane will run on infrastructure nodes. These deployed components include Istiod, the Ingress Gateway, and the Egress Gateway.

If the control plane will run on a worker node, skip this task.

.Prerequisites

* You have installed the {SMProductName} Operator.
* You are logged in as a user with the `cluster-admin` role. If you use {product-dedicated}, you are logged in as a user with the `dedicated-admin` role.

.Procedure

. Open the `ServiceMeshControlPlane` resource as a YAML file.
+
[source,terminal]
----
$ oc -n istio-system edit smcp <name> <1>
----
<1>  `<name>` represents the name of the `ServiceMeshControlPlane` resource.

. To run the Istiod component on an infrastructure node, add the `nodeSelector` and the `tolerations` fields to the `spec.runtime.components.pilot.pod` spec in the `ServiceMeshControlPlane` resource.
+
[source,yaml]
----
spec:
  runtime:
    components:
      pilot:
        pod:
          nodeSelector: <1>
            node-role.kubernetes.io/infra: ""
          tolerations: <2>
          - effect: NoSchedule
            key: node-role.kubernetes.io/infra
            value: reserved
          - effect: NoExecute
            key: node-role.kubernetes.io/infra
            value: reserved
----
<1> Ensures that the `Istiod` pod is only scheduled on an infrastructure node.
<2> Ensures that the pod is accepted by the infrastructure node.

. To run Ingress and Egress Gateways on infrastructure nodes, add the `nodeSelector` and the `tolerations` fields to the `spec.gateways.ingress.runtime.pod` spec and the `spec.gateways.egress.runtime.pod` spec in the `ServiceMeshControlPlane` resource.
+
[source,yaml]
----
spec:
  gateways:
    ingress:
      runtime:
        pod:
          nodeSelector: <1>
            node-role.kubernetes.io/infra: ""
          tolerations: <2>
          - effect: NoSchedule
            key: node-role.kubernetes.io/infra
            value: reserved
          - effect: NoExecute
            key: node-role.kubernetes.io/infra
            value: reserved
    egress:
      runtime:
        pod:
          nodeSelector: <1>
            node-role.kubernetes.io/infra: ""
          tolerations: <2>
          - effect: NoSchedule
            key: node-role.kubernetes.io/infra
            value: reserved
          - effect: NoExecute
            key: node-role.kubernetes.io/infra
            value: reserved
----
<1> Ensures that the gateway pod is only scheduled on an infrastructure node
<2> Ensures that the pod is accepted by the infrastructure node.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-confirm-smcp-infrastructure-node_{context}"]
= Verifying the {SMProductShortName} control plane is running on infrastructure nodes

.Procedure

* Confirm that the nodes associated with Istiod, Ingress Gateway, and Egress Gateway pods are infrastructure nodes:
+
[source,terminal]
----
$ oc -n istio-system get pods -owide
----

:leveloffset: 2


:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-control-plane-and-cluster-wide-deployment_{context}"]
= About control plane and cluster-wide deployments

A cluster-wide deployment contains a {SMProductShortName} Control Plane that monitors resources for an entire cluster. Monitoring resources for an entire cluster closely resembles Istio functionality in that the control plane uses a single query across all namespaces to monitor Istio and Kubernetes resources. As a result, cluster-wide deployments decrease the number of requests sent to the API server.

You can configure the {SMProductShortName} Control Plane for cluster-wide deployments using either the {product-title} web console or the CLI.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-deploy-cluster-wide-control-plane-console_{context}"]
= Configuring the control plane for cluster-wide deployment with the web console

You can configure the `ServiceMeshControlPlane` resource for cluster-wide deployment using the {product-title} web console. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

.Prerequisites

* The {SMProductName} Operator is installed.
* You are logged in using an account with the `cluster-admin` role, or if you use {product-dedicated} with the `dedicated-admin` role.

.Procedure

. Create a project named `istio-system`.
+
.. Navigate to *Home* -> *Projects*.
+
.. Click *Create Project*.
+
.. In the *Name* field, enter `istio-system`. The `ServiceMeshControlPlane` resource must be installed in a project that is separate from your microservices and Operators.
+
These steps use `istio-system` as an example. You can deploy the {SMProductShortName} control plane to any project as long as it is separate from the project that contains your services.
+
.. Click *Create*.

. Navigate to *Operators* -> *Installed Operators*.

. Click the {SMProductName} Operator, then click *Istio Service Mesh Control Plane*.

. On the *Istio Service Mesh Control Plane* tab, click *Create ServiceMeshControlPlane*.

. Click *YAML view*. The version of the {SMProductShortName} control plane determines the features available regardless of the version of the Operator.

. Modify the `spec.mode` field of the YAML file to specify `ClusterWide`.
+
.Example version {MaistraVersion} istio-installation.yaml
+
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  version: v{MaistraVersion}
  mode: ClusterWide
----

. Click *Create*. The Operator creates pods, services, and {SMProductShortName} control plane components based on your configuration parameters. The operator also creates the `ServiceMeshMemberRoll` if it does not exist as part of the default configuration.

. To verify that the control plane installed correctly, click the *Istio Service Mesh Control Plane* tab.
+
.. Click the name of the new `ServiceMeshControlPlane` object.
+
.. Click the *Resources* tab to see the {SMProductName} control plane resources that the Operator created and configured.

:leveloffset: 2

:leveloffset: +2

// This module is included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-deploy-cluster-wide-control-plane-cli_{context}"]
= Configuring the control plane for cluster-wide deployment with the CLI

You can configure the `ServiceMeshControlPlane` resource for cluster-wide deployment using the CLI. In this example, `istio-system` is the name of the Service Mesh control plane namespace.

.Prerequisites

* The {SMProductName} Operator is installed.
* You have access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. Create a project named `istio-system`.
+
[source,terminal]
----
$ oc new-project istio-system
----

. Create a `ServiceMeshControlPlane` file named `istio-installation.yaml` using the following example.
+
.Example version {MaistraVersion} istio-installation.yaml
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  version: v{MaistraVersion}
  mode: ClusterWide
----

. Run the following command to deploy the {SMProductShortName} control plane, where `<istio_installation.yaml>` includes the full path to your file.
+
[source,terminal]
----
$ oc create -n istio-system -f <istio_installation.yaml>
----
+
. To monitor the progress of the pod deployment, run the following command:
+
[source,terminal]
----
$ oc get pods -n istio-system -w
----
+
You should see output similar to the following example:
+
.Example output
[source,terminal]
----
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-b4d59bd7-mrgbr                 2/2     Running   0          65m
istio-egressgateway-678dc97b4c-wrjkp   1/1     Running   0          108s
istio-ingressgateway-b45c9d54d-4qg6n   1/1     Running   0          108s
istiod-basic-55d78bbbcd-j5556          1/1     Running   0          108s
jaeger-67c75bd6dc-jv6k6                2/2     Running   0          65m
kiali-6476c7656c-x5msp                 1/1     Running   0          43m
prometheus-58954b8d6b-m5std            2/2     Running   0          66m
----

:leveloffset: 2

:leveloffset: +2

// This module is included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-smcp.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-customize-smrr-cluster-wide_{context}"]
= Customizing the member roll for a cluster-wide mesh

In cluster-wide mode, when you create the `ServiceMeshControlPlane` resource, the `ServiceMeshMemberRoll` resource is also created. You can modify the `ServiceMeshMemberRoll` resource after it gets created. After you modify the resource, the {SMProductShortName} operator no longer changes it. If you modify the `ServiceMeshMemberRoll` resource by using the {product-title} web console, accept the prompt to overwrite the modifications.

Alternatively, you can create a `ServiceMeshMemberRoll` resource before deploying the `ServiceMeshControlPlane` resource. When you create the `ServiceMeshControlPlane` resource, the {SMProductShortName} Operator will not modify the `ServiceMeshMemberRoll`.

[NOTE]
====
The `ServiceMeshMemberRoll` resource name must be named `default` and must be created in the same project namespace as the `ServiceMeshControlPlane` resource.
====

There are two ways to add a namespace to the mesh. You can either add the namespace by specifying its name in the `spec.members` list, or configure a set of namespace label selectors to include or exclude namespaces based on their labels.

[NOTE]
====
Regardless of how members are specified in the `ServiceMeshMemberRoll` resource, you can also add members to the mesh by creating the `ServiceMeshMember` resource in each namespace.
====

:leveloffset: 2

:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v2x/ossm-create-smcp.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-validate-control-plane-kiali_{context}"]
= Validating your SMCP installation with Kiali

You can use the Kiali console to validate your {SMProductShortName} installation. The Kiali console offers several ways to validate your {SMProductShortName} components are deployed and configured properly.

.Procedure

. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Networking* -> *Routes*.

. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+
. If necessary, use the filter to find the route for the Kiali console. Click the route *Location* to launch the console.

. Click *Log In With OpenShift*.
+
When you first log in to the Kiali Console, you see the *Overview* page which displays all the namespaces in your service mesh that you have permission to view. When there are multiple namespaces shown on the *Overview* page, Kiali shows namespaces with health or validation problems first.
+
.Kiali Overview page
image::ossm-kiali-overview.png[Kiali Overview page showing istio-system]
+
The tile for each namespace displays the number of labels, the *Istio Config* health, the number of and *Applications* health, and *Traffic* for the namespace. If you are validating the console installation and namespaces have not yet been added to the mesh, there might not be any data to display other than `istio-system`.

. Kiali has four dashboards specifically for the namespace where the {SMProductShortName} control plane is installed.  To view these dashboards, click the *Options* menu {kebab} on the tile for the control plane namespace, for example, `istio-system`, and select one of the following options:

** *Istio Mesh Dashboard*
** *Istio Control Plane Dashboard*
** *Istio Performance Dashboard*
** *Istio Wasm Exetension Dashboard*
+
.Grafana Istio Control Plane Dashboard
image::ossm-grafana-control-plane-dashboard.png[Istio Control Plane Dashboard showing data for bookinfo sample project]
+
Kiali also installs two additional Grafana dashboards, available from the Grafana *Home* page:
** *Istio Workload Dashboard*
** *Istio Service Dashboard*
+
. To view the {SMProductShortName} control plane nodes, click the *Graph* page, select the *Namespace* where you installed the `ServiceMeshControlPlane` from the menu, for example `istio-system`.

.. If necessary, click *Display idle nodes*.

.. To learn more about the *Graph* page, click the *Graph tour* link.

.. To view the mesh topology, select one or more additional namespaces from the Service Mesh Member Roll from the *Namespace* menu.

. To view the list of applications in the `istio-system` namespace, click the *Applications* page. Kiali displays the health of the applications.

.. Hover your mouse over the information icon to view any additional information noted in the *Details* column.

. To view the list of workloads in the `istio-system` namespace, click the *Workloads* page. Kiali displays the health of the workloads.

.. Hover your mouse over the information icon to view any additional information noted in the *Details* column.

. To view the list of services in the `istio-system` namespace, click the *Services* page. Kiali displays the health of the services and of the configurations.

.. Hover your mouse over the information icon to view any additional information noted in the *Details* column.

. To view a list of the Istio Configuration objects in the `istio-system` namespace, click the *Istio Config* page. Kiali displays the health of the configuration.

.. If there are configuration errors, click the row and Kiali opens the configuration file with the error highlighted.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2/ossm-create-smcp.adoc
////

:_mod-docs-content-type: REFERENCE
[id="ossm-install-rosa_{context}"]
= Installing on Red Hat OpenShift Service on AWS (ROSA)

Starting with version 2.2, {SMProductName} supports installation on Red Hat OpenShift Service on AWS (ROSA). This section documents the additional requirements when installing Service Mesh on this platform.

[id="ossm-install-rosa-location_{context}"]
== Installation location

You must create a new namespace, for example `istio-system`, when installing {SMProductName} and creating the `ServiceMeshControlPlane`.

[id="ossm-install-rosa-smcp_{context}"]
== Required {SMProductShortName} control plane configuration

The default configuration in the `ServiceMeshControlPlane` file does not work on a ROSA cluster. You must modify the default SMCP and set `spec.security.identity.type=ThirdParty` when installing on Red Hat OpenShift Service on AWS.

.Example `ServiceMeshControlPlane` resource for ROSA
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  version: v{MaistraVersion}
  security:
    identity:
      type: ThirdParty  #required setting for ROSA
  tracing:
    type: Jaeger
    sampling: 10000
  policy:
    type: Istiod
  addons:
    grafana:
      enabled: true
    jaeger:
      install:
        storage:
          type: Memory
    kiali:
      enabled: true
    prometheus:
      enabled: true
  telemetry:
    type: Istiod
----

[id="ossm-install-rosa-kiali-config_{context}"]
== Restrictions on Kiali configuration

Red Hat OpenShift Service on AWS places additional restrictions on where you can create resources and does not let you create the Kiali resource in a Red Hat managed namespace.

This means that the following common settings for `spec.deployment.accessible_namespaces` are not allowed in a ROSA cluster:

* `['**']`   (all namespaces)
* `default`
* `codeready-*`
* `openshift-*`
* `redhat-*`

The validation error message provides a complete list of all the restricted namespaces.

.Example `Kiali` resource for ROSA
[source,yaml]
----
apiVersion: kiali.io/v1alpha1
kind: Kiali
metadata:
  name: kiali
  namespace: istio-system
spec:
  auth:
    strategy: openshift
  deployment:
    accessible_namespaces:   #restricted setting for ROSA
      - istio-system
    image_pull_policy: ''
    ingress_enabled: true
    namespace: istio-system
----

:leveloffset: 2

[role="_additional-resources"]
== Additional resources

{SMProductName} supports multiple independent control planes within the cluster. You can create reusable configurations with `ServiceMeshControlPlane` profiles. For more information, see xref:ossm-control-plane-profiles_ossm-profiles-users[Creating control plane profiles].

== Next steps

* Add a project to the {SMProductShortName} so that applications can be made available. For more information, see xref:ossm-create-mesh[Adding services to a service mesh].

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-create-mesh"]
= Adding services to a service mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-create-mesh

toc::[]

A project contains services; however, the services are only available if you add the project to the service mesh.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-adding-namespace_{context}"]
= About adding projects to a service mesh

After installing the Operators and creating the `ServiceMeshControlPlane` resource, add one or more projects to the service mesh.

[NOTE]
====
In {product-title}, a project is essentially a Kubernetes namespace with additional annotations, such as the range of user IDs that can be used in the project. Typically, the {product-title} web console uses the term project, and the CLI uses the term namespace, but the terms are essentially synonymous.
====

You can add projects to an existing service mesh using either the {product-title} web console or the CLI. There are three methods to add a project to a service mesh:

* Specifying the project name in the `ServiceMeshMemberRoll` resource.

* Configuring label selectors in the `spec.labelSelectors` field of the `ServiceMeshMemberRoll` resource.

* Creating the `ServiceMeshMember` resource in the project.

If you use the first method, then you must create the `ServiceMeshMemberRoll` resource.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-member-roll-create_{context}"]
= Creating the {SMProductName} member roll

The `ServiceMeshMemberRoll` lists the projects that belong to the {SMProductShortName} control plane. Only projects listed in the `ServiceMeshMemberRoll` are affected by the control plane. A project does not belong to a service mesh until you add it to the member roll for a particular control plane deployment.

You must create a `ServiceMeshMemberRoll` resource named `default` in the same project as the `ServiceMeshControlPlane`, for example `istio-system`.

[id="ossm-member-roll-create-console_{context}"]
== Creating the member roll from the web console

You can add one or more projects to the {SMProductShortName} member roll from the web console. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

.Prerequisites
* An installed, verified {SMProductName} Operator.
* List of existing projects to add to the service mesh.

.Procedure

. Log in to the {product-title} web console.

. If you do not already have services for your mesh, or you are starting from scratch, create a project for your applications. It must be different from the project where you installed the {SMProductShortName} control plane.

.. Navigate to *Home* -> *Projects*.

.. Enter a name in the *Name* field.

.. Click *Create*.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose the project where your `ServiceMeshControlPlane` resource is deployed from the list, for example `istio-system`.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Member Roll* tab.

. Click *Create ServiceMeshMemberRoll*

. Click *Members*, then enter the name of your project in the *Value* field. You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.

. Click *Create*.

[id="ossm-member-roll-create-cli_{context}"]
== Creating the member roll from the CLI

You can add a project to the `ServiceMeshMemberRoll` from the command line.

.Prerequisites

* An installed, verified {SMProductName} Operator.
* List of projects to add to the service mesh.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----

. If you do not already have services for your mesh, or you are starting from scratch, create a project for your applications. It must be different from the project where you installed the {SMProductShortName} control plane.
+
[source,terminal]
----
$ oc new-project <your-project>
----

. To add your projects as members, modify the following example YAML. You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
.Example servicemeshmemberroll-default.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system
spec:
  members:
    # a list of projects joined into the service mesh
    - your-project-name
    - another-project-name
----

. Run the following command to upload and create the `ServiceMeshMemberRoll` resource in the `istio-system` namespace.
+
[source,terminal]
----
$ oc create -n istio-system -f servicemeshmemberroll-default.yaml
----

. Run the following command to verify the `ServiceMeshMemberRoll` was created successfully.
+
[source,terminal]
----
$ oc get smmr -n istio-system default
----
+
The installation has finished successfully when the `STATUS` column is `Configured`.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-create-mesh.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-adding-projects-using-smmr_{context}"]
= About adding projects using the ServiceMeshMemberRoll resource

Using the `ServiceMeshMemberRoll` resource is the simplest way to add a project to a service mesh. To add a project, specify the project name in the `spec.members` field of the `ServiceMeshMemberRoll` resource. The `ServiceMeshMemberRoll` resource specifies which projects are controlled by the `ServiceMeshControlPlane` resource.

image::ossm-adding-project-using-smmr.png[Adding project using `ServiceMeshMemberRoll` resource image]

[NOTE]
====
Adding projects using this method requires the user to have the `update servicemeshmemberrolls` and the `update pods` privileges in the project that is being added.
====

* If you already have an application, workload, or service to add to the service mesh, see the following:
** Adding or removing projects from the mesh using the `ServiceMeshMemberRoll` resource with the web console
** Adding or removing projects from the mesh using the `ServiceMeshMemberRoll` resource with the CLI

* Alternatively, to install a sample application called Bookinfo and add it to a `ServiceMeshMemberRoll` resource, see the Bookinfo example application tutorial.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-add-project-member-roll-recourse-console_{context}"]
= Adding or removing projects from the mesh using the ServiceMeshMemberRoll resource with the web console

You can add or remove projects from the mesh using the `ServiceMeshMemberRoll` resource with the {product-title} web console. You can add any number of projects, but a project can only belong to *one* mesh.

The `ServiceMeshMemberRoll` resource is deleted when its corresponding `ServiceMeshControlPlane` resource is deleted.

.Prerequisites

* An installed, verified {SMProductName} Operator.
* An existing `ServiceMeshMemberRoll` resource.
* The name of the project with the `ServiceMeshMemberRoll` resource.
* The names of the projects you want to add or remove from the mesh.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose the project where your `ServiceMeshControlPlane` resource is deployed from the list. For example `istio-system`.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Member Roll* tab.

. Click the `default` link.

. Click the YAML tab.

. Modify the YAML to add projects as members (or delete them to remove existing members). You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.
+
.Example servicemeshmemberroll-default.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system #control plane project
spec:
  members:
    # a list of projects joined into the service mesh
    - your-project-name
    - another-project-name
----

. Click *Save*.

. Click *Reload*.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-add-project-member-roll-resource-cli_{context}"]
= Adding or removing projects from the mesh using ServiceMeshMemberRoll resource with the CLI

You can add one or more projects to the mesh using the `ServiceMeshMemberRoll` resource with the CLI. You can add any number of projects, but a project can only belong to *one* mesh.

The `ServiceMeshMemberRoll` resource is deleted when its corresponding `ServiceMeshControlPlane` resource is deleted.

.Prerequisites

* An installed, verified {SMProductName} Operator.
* An existing `ServiceMeshMemberRoll` resource.
* The name of the project with the `ServiceMeshMemberRoll` resource.
* The names of the projects you want to add or remove from the mesh.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI.

. Edit the `ServiceMeshMemberRoll` resource.
+
[source,terminal]
----
$ oc edit smmr -n <controlplane-namespace>
----

. Modify the YAML to add or remove projects as members. You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.
+
.Example servicemeshmemberroll-default.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system #control plane project
spec:
  members:
    # a list of projects joined into the service mesh
    - your-project-name
    - another-project-name
----

. Save the file and exit the editor.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/ossm-create-mesh.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-adding-projects-using-smm_{context}"]
= About adding projects using the ServiceMeshMember resource

A `ServiceMeshMember` resource provides a way to add a project to a service mesh without modifying the `ServiceMeshMemberRoll` resource. To add a project, create a `ServiceMeshMember` resource in the project that you want to add to the service mesh. When the {SMProductShortName} Operator processes the `ServiceMeshMember` object, the project appears in the `status.members` list of the `ServiceMeshMemberRoll` resource. Then, the services that reside in the project are made available to the mesh.

image::ossm-adding-project-using-smm.png[Adding project using `ServiceMeshMember` resource image]

The mesh administrator must grant each mesh user permission to reference the `ServiceMeshControlPlane` resource in the `ServiceMeshMember` resource. With this permission in place, a mesh user can add a project to a mesh even when that user does not have direct access rights for the service mesh project or the `ServiceMeshMemberRoll` resource. For more information, see Creating the {SMProductName} members.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-mesh.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-adding-project-using-smm-resource-console_{context}"]
= Adding a project to the mesh using the ServiceMeshMember resource with the web console

You can add one or more projects to the mesh using the `ServiceMeshMember` resource with the {product-title} web console.

.Prerequisites
* You have installed the {SMProductName} Operator.
* You know the name of the `ServiceMeshControlPlane` resource and the name of the project that the resource belongs to.
* You know the name of the project you want to add to the mesh.
* A service mesh administrator must explicitly grant access to the service mesh. Administrators can grant users permissions to access the mesh by assigning them the `mesh-user` `Role` using a `RoleBinding` or `ClusterRoleBinding`. For more information, see _Creating the {SMProductName} members_.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose the project that you want to add to the mesh from the drop-down list. For example, `istio-system`.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Member* tab.

. Click *Create ServiceMeshMember*

. Accept the default name for the `ServiceMeshMember`.

. Click to expand *ControlPlaneRef*.

. In the *Namespace* field, select the project that the `ServiceMeshControlPlane` resource belongs to. For example, `istio-system`.

. In the *Name* field, enter the name of the `ServiceMeshControlPlane` resource that this namespace belongs to. For example, `basic`.

. Click *Create*.

. Confirm the `ServiceMeshMember` resource was created, and that the project was added to the mesh. Click the resource name; for example, `default`. View the *Conditions* section shown at the end of the screen. Confirm that the `Status` of the `Reconciled` and `Ready` conditions is `True`. If the `Status` is `False`, see the `Reason` and `Message` columns for more information.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-mesh.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-adding-project-using-smm-resource-cli_{context}"]
= Adding a project to the mesh using the ServiceMeshMember resource with the CLI

You can add one or more projects to the mesh using the `ServiceMeshMember` resource with the CLI.

.Prerequisites
* You have installed the {SMProductName} Operator.
* You know the name of the `ServiceMeshControlPlane` resource and the name of the project it belongs to.
* You know the name of the project you want to add to the mesh.
* A service mesh administrator must explicitly grant access to the service mesh. Administrators can grant users permissions to access the mesh by assigning them the `mesh-user` `Role` using a `RoleBinding` or `ClusterRoleBinding`. For more information, see _Creating the {SMProductName} members_.

.Procedure

. Log in to the {product-title} CLI.

. Create the YAML file for the `ServiceMeshMember` manifest. The manifest adds the `my-application` project to the service mesh that was created by the `ServiceMeshControlPlane` resource deployed in the `istio-system` namespace:
+
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMember
metadata:
  name: default
  namespace: my-application
spec:
  controlPlaneRef:
    namespace: istio-system
    name: basic
----

. Apply the YAML file to create the `ServiceMeshMember` resource:
+
[source,terminal]
----
$ oc apply -f <file-name>
----

. After creating the `ServiceMeshMember` resource, verify that the namespace is part of the mesh. Confirm the that the value `True` appears in the `READY` column when you run the following command:
+
[source,terminal]
----
$ oc get smm default -n my-application
----
+
Alternatively, if you can access the `ServiceMeshMemberRoll` resource, you can also confirm that the `my-application` namespace is displayed in the `status.members` and `status.configuredMembers` fields of the `ServiceMeshMemberRoll` resource.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/create-mesh.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-about-adding-projects-using-label-selectors_{context}"]
= About adding projects using label selectors

For cluster-wide deployments, you can use label selectors to add projects to the mesh. Label selectors specified in the `ServiceMeshMemberRoll` resource enable the {SMProductShortName} Operator to add or remove namespaces to or from the mesh based on namespace labels. Unlike other standard {product-title} resources that you can use to specify a single label selector, you can use the `ServiceMeshMemberRoll` resource to specify multiple label selectors.

image::ossm-adding-project-using-label-selector.png[Adding project using label selector image]

If the labels for a namespace match any of the selectors specified in the `ServiceMeshMemberRoll` resource, then the namespace is included in the mesh.

[NOTE]
====
In {product-title}, a project is essentially a Kubernetes namespace with additional annotations, such as the range of user IDs that can be used in the project. Typically, the {product-title} web console uses the term _project_, and the CLI uses the term _namespace_, but the terms are essentially synonymous.
====

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-create-mesh.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-adding-project-using-label-selectors-console_{context}"]
= Adding a project to the mesh using label selectors with the web console

You can use labels selectors to add a project to the {SMProductShortName} with the {product-title} web console.

.Prerequisites
* You have installed the {SMProductName} Operator.
* The deployment has an existing `ServiceMeshMemberRoll` resource.
* You are logged in as a user with the `cluster-admin` role. If you use {product-dedicated}, you are logged in as a user with the `dedicated-admin` role.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu, and from the drop-down list, select the project where your `ServiceMeshMemberRoll` resource is deployed. For example, *istio-system*.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Member Roll* tab.

. Click *Create ServiceMeshMember Roll*.

. Accept the default name for the `ServiceMeshMemberRoll`.

. In the *Labels* field, enter key-value pairs to define the labels that identify which namespaces to include in the service mesh. If a project namespace has either label specified by the selectors, then the project namespace is included in the service mesh. You do not need to include both labels.
+
For example, entering `mykey=myvalue` includes all namespaces with this label as part of the mesh. When the selector identifies a match, the project namespace is added to the service mesh.
+
Entering `myotherkey=myothervalue` includes all namespaces with this label as part of the mesh. When the selector identifies a match, the project namespace is added to the service mesh.

. Click *Create*.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-adding-project-using-label-selectors-cli_{context}"]
= Adding a project to the mesh using label selectors with the CLI

You can use label selectors to add a project to the {SMProductShortName} with the CLI.

.Prerequisites
* You have installed the {SMProductName} Operator.
* The deployment has an existing `ServiceMeshMemberRoll` resource.
* You are logged in as a user with the `cluster-admin` role. If you use {product-dedicated}, you are logged in as a user with the `dedicated-admin` role.

.Procedure

. Log in to the {product-title} CLI.

. Edit the `ServiceMeshMemberRoll` resource.
+
[source,terminal]
----
$ oc edit smmr default -n istio-system
----
+
You can deploy the {SMProductShortName} control plane to any project provided that it is separate from the project that contains your services.


. Modify the YAML file to include namespace label selectors in the `spec.memberSelectors` field of the `ServiceMeshMemberRoll` resource.
+
[NOTE]
====
Instead of using the `matchLabels` field, you can also use the `matchExpressions` field in the selector.
====
+
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system
spec:
  memberSelectors: <1>
  - matchLabels: <2>
      mykey: myvalue <2>
  - matchLabels: <3>
      myotherkey: myothervalue <3>
----
<1> Contains the label selectors used to identify which project namespaces are included in the service mesh. If a project namespace has either label specified by the selectors, then the project namespace is included in the service mesh. The project namespace does not need both labels to be included.
<2> Specifies all namespaces with the `mykey=myvalue` label. When the selector identifies a match, the project namespace is added to the service mesh.
<3> Specifies all namespaces with the `myotherkey=myothervalue` label. When the selector identifies a match, the project namespace is added to the service mesh.

:leveloffset: 2

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

[id="ossm-tutorial-bookinfo-overview_{context}"]
= Bookinfo example application

The Bookinfo example application allows you to test your {SMProductName} {SMProductVersion} installation on {product-title}.

The Bookinfo application displays information about a book, similar to a single catalog entry of an online book store. The application displays a page that describes the book, book details (ISBN, number of pages, and other information), and book reviews.

The Bookinfo application consists of these microservices:

* The `productpage` microservice calls the `details` and `reviews` microservices to populate the page.
* The `details` microservice contains book information.
* The `reviews` microservice contains book reviews. It also calls the `ratings` microservice.
* The `ratings` microservice contains book ranking information that accompanies a book review.

There are three versions of the reviews microservice:

* Version v1 does not call the `ratings` Service.
* Version v2 calls the `ratings` Service and displays each rating as one to five black stars.
* Version v3 calls the `ratings` Service and displays each rating as one to five red stars.

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-install_{context}"]
= Installing the Bookinfo application

This tutorial walks you through how to create a sample application by creating a project, deploying the Bookinfo application to that project, and viewing the running application in {SMProductShortName}.

.Prerequisites:

* {product-title} 4.1 or higher installed.
* {SMProductName} {SMProductVersion} installed.
* Access to the OpenShift CLI (`oc`).
* An account with the `cluster-admin` role.

[NOTE]
====
The Bookinfo sample application cannot be installed on {ibm-z-name} and {ibm-power-name}.
====

[NOTE]
====
The commands in this section assume the {SMProductShortName} control plane project is `istio-system`.  If you installed the control plane in another namespace, edit each command before you run it.
====

.Procedure

. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Click *Home* -> *Projects*.

. Click *Create Project*.

. Enter `bookinfo` as the *Project Name*, enter a *Display Name*, and enter a *Description*, then click *Create*.
+
** Alternatively, you can run this command from the CLI to create the `bookinfo` project.
+
[source,terminal]
----
$ oc new-project bookinfo
----
+
. Click *Operators* -> *Installed Operators*.

. Click the *Project* menu and use the {SMProductShortName} control plane namespace. In this example, use `istio-system`.

. Click the *{SMProductName}* Operator.

. Click the *Istio Service Mesh Member Roll* tab.

.. If you have already created a Istio Service Mesh Member Roll, click the name, then click the YAML tab to open the YAML editor.

.. If you have not created a `ServiceMeshMemberRoll`, click *Create ServiceMeshMemberRoll*.
+
. Click *Members*, then enter the name of your project in the *Value* field.
+
. Click *Create* to save the updated Service Mesh Member Roll.
+
.. Or, save the following example to a YAML file.
+
.Bookinfo ServiceMeshMemberRoll example servicemeshmemberroll-default.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - bookinfo
----
+
.. Run the following command to upload that file and create the `ServiceMeshMemberRoll` resource in the `istio-system` namespace.   In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc create -n istio-system -f servicemeshmemberroll-default.yaml
----
+
. Run the following command to verify the `ServiceMeshMemberRoll` was created successfully.
+
[source,terminal]
----
$ oc get smmr -n istio-system -o wide
----
+
The installation has finished successfully when the `STATUS` column is `Configured`.
+
[source,terminal]
----
NAME      READY   STATUS       AGE   MEMBERS
default   1/1     Configured   70s   ["bookinfo"]
----
. From the CLI, deploy the Bookinfo application in the _`bookinfo`_ project by applying the `bookinfo.yaml` file:
+
[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/platform/kube/bookinfo.yaml
----
+
You should see output similar to the following:
+
[source,terminal]
----
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
----
+
. Create the ingress gateway by applying the `bookinfo-gateway.yaml` file:
+
[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/bookinfo-gateway.yaml
----
+
You should see output similar to the following:
+
[source,terminal]
----
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
----
+
. Set the value for the `GATEWAY_URL` parameter:
+
[source,terminal]
----
$ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')
----

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-adding-destination-rules_{context}"]
= Adding default destination rules

Before you can use the Bookinfo application, you must first add default destination rules. There are two preconfigured YAML files, depending on whether or not you enabled mutual transport layer security (TLS) authentication.

.Procedure

. To add destination rules, run one of the following commands:
** If you did not enable mutual TLS:
+

[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/destination-rule-all.yaml
----
+
** If you enabled mutual TLS:
+

[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/destination-rule-all-mtls.yaml
----
+
You should see output similar to the following:
+
[source,terminal]
----
destinationrule.networking.istio.io/productpage created
destinationrule.networking.istio.io/reviews created
destinationrule.networking.istio.io/ratings created
destinationrule.networking.istio.io/details created
----

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-verify-install_{context}"]
= Verifying the Bookinfo installation

To confirm that the sample Bookinfo application was successfully deployed, perform the following steps.

.Prerequisites

* {SMProductName} installed.
* Complete the steps for installing the Bookinfo sample app.

.Procedure from CLI

. Log in to the {product-title} CLI.

. Verify that all pods are ready with this command:
+
[source,terminal]
----
$ oc get pods -n bookinfo
----
+
All pods should have a status of `Running`. You should see output similar to the following:
+
[source,terminal]
----
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-55b869668-jh7hb        2/2     Running   0          12m
productpage-v1-6fc77ff794-nsl8r   2/2     Running   0          12m
ratings-v1-7d7d8d8b56-55scn       2/2     Running   0          12m
reviews-v1-868597db96-bdxgq       2/2     Running   0          12m
reviews-v2-5b64f47978-cvssp       2/2     Running   0          12m
reviews-v3-6dfd49b55b-vcwpf       2/2     Running   0          12m
----
+
. Run the following command to retrieve the URL for the product page:
+
[source,terminal]
----
echo "http://$GATEWAY_URL/productpage"
----
. Copy and paste the output in a web browser to verify the Bookinfo product page is deployed.

.Procedure from Kiali web console

. Obtain the address for the Kiali web console.

.. Log in to the {product-title} web console as a user with `cluster-admin` rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

.. Navigate to *Networking* -> *Routes*.

.. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+

.. Click the link in the *Location* column for Kiali.

.. Click *Log In With OpenShift*. The Kiali *Overview* screen presents tiles for each project namespace.

. In Kiali, click *Graph*.

. Select bookinfo from the *Namespace* list, and App graph from the *Graph Type* list.

. Click *Display idle nodes* from the *Display* menu.
+
This displays nodes that are defined but have not received or sent requests. It can confirm that an application is properly defined, but that no request traffic has been reported.
+
image::ossm-kiali-graph-bookinfo.png[Kiali displaying bookinfo application]
+
* Use the *Duration* menu to increase the time period to help ensure older traffic is captured.
+
* Use the *Refresh Rate* menu to refresh traffic more or less often, or not at all.

. Click *Services*, *Workloads* or *Istio Config* to see list views of bookinfo components, and confirm that they are healthy.

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-removing_{context}"]
= Removing the Bookinfo application

Follow these steps to remove the Bookinfo application.

.Prerequisites

* {product-title} 4.1 or higher installed.
* {SMProductName} {SMProductVersion} installed.
* Access to the OpenShift CLI (`oc`).

[id="ossm-delete-bookinfo-project_{context}"]
== Delete the Bookinfo project

.Procedure

. Log in to the {product-title} web console.

. Click to *Home* -> *Projects*.

. Click the `bookinfo` menu {kebab}, and then click *Delete Project*.

. Type `bookinfo` in the confirmation dialog box, and then click *Delete*.
+
** Alternatively, you can run this command using the CLI to create the `bookinfo` project.
+
[source,terminal]
----
$ oc delete project bookinfo
----

[id="ossm-remove-bookinfo-smmr_{context}"]
== Remove the Bookinfo project from the {SMProductShortName} member roll

.Procedure

. Log in to the {product-title} web console.

. Click *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose `istio-system` from the list.

. Click the *Istio Service Mesh Member Roll* link under *Provided APIS* for the *{SMProductName}* Operator.

. Click the `ServiceMeshMemberRoll` menu {kebab} and select *Edit Service Mesh Member Roll*.

. Edit the default Service Mesh Member Roll YAML and remove `bookinfo` from the *members* list.
+
** Alternatively, you can run this command using the CLI to remove the `bookinfo` project from the `ServiceMeshMemberRoll`. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc -n istio-system patch --type='json' smmr default -p '[{"op": "remove", "path": "/spec/members", "value":["'"bookinfo"'"]}]'
----

. Click *Save* to update Service Mesh Member Roll.

:leveloffset: 2

== Next steps

* To continue the installation process, you must xref:deploying-applications-ossm[enable sidecar injection].

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="deploying-applications-ossm"]
= Enabling sidecar injection
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: deploying-applications-ossm

toc::[]

After adding the namespaces that contain your services to your mesh, the next step is to enable automatic sidecar injection in the Deployment resource for your application. You must enable automatic sidecar injection for each deployment.

If you have installed the Bookinfo sample application, the application was deployed and the sidecars were injected as part of the installation procedure. If you are using your own project and service, deploy your applications on {product-title}.

For more information, see the {product-title} documentation, link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#[Understanding deployments].

[NOTE]
====
Traffic started by Init Containers, specialized containers that run before the application containers in a pod, cannot travel outside of the service mesh by default. Any action Init Containers perform that requires establishing a network traffic connection outside of the mesh fails.

For more information about connecting Init Containers to a service, see the Red Hat Knowledgebase solution link:https://access.redhat.com/solutions/6653601[initContainer in CrashLoopBackOff on pod with Service Mesh sidecar injected]
====

== Prerequisites

* xref:ossm-tutorial-bookinfo-overview_ossm-create-mesh[Services deployed to the mesh], for example the Bookinfo sample application.

* A Deployment resource file.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-automatic-sidecar-injection_{context}"]
= Enabling automatic sidecar injection

When deploying an application, you must opt-in to injection by configuring the annotation `sidecar.istio.io/inject` in `spec.template.metadata.annotations` to `true` in the `deployment` object. Opting in ensures that the sidecar injection does not interfere with other {product-title} features such as builder pods used by numerous frameworks within the {product-title} ecosystem.

.Prerequisites

* Identify the namespaces that are part of your service mesh and the deployments that need automatic sidecar injection.

.Procedure

. To find your deployments use the `oc get` command.
+
[source,terminal]
----
$ oc get deployment -n <namespace>
----
+
For example, to view the deployment file for the 'ratings-v1' microservice in the `bookinfo` namespace, use the following command to see the resource in YAML format.
+
[source,terminal]
----
oc get deployment -n bookinfo ratings-v1 -o yaml
----
+
. Open the application's deployment configuration YAML file in an editor.

. Add `spec.template.metadata.annotations.sidecar.istio/inject` to your Deployment YAML and set `sidecar.istio.io/inject` to `true` as shown in the following example.
+
.Example snippet from bookinfo deployment-ratings-v1.yaml
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ratings-v1
  namespace: bookinfo
  labels:
    app: ratings
    version: v1
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: 'true'
----
+
. Save the Deployment configuration file.

. Add the file back to the project that contains your app.
+
[source,terminal]
----
$ oc apply -n <namespace> -f deployment.yaml
----
+
In this example, `bookinfo` is the name of the project that contains the `ratings-v1` app and `deployment-ratings-v1.yaml` is the file you edited.
+
[source,terminal]
----
$ oc apply -n bookinfo -f deployment-ratings-v1.yaml
----
+
. To verify that the resource uploaded successfully, run the following command.
+
[source,terminal]
----
$ oc get deployment -n <namespace> <deploymentName> -o yaml
----
+
For example,
+
[source,terminal]
----
$ oc get deployment -n bookinfo ratings-v1 -o yaml
----

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-validating-sidecar_{context}"]
= Validating sidecar injection

The Kiali console offers several ways to validate whether or not your applications, services, and workloads have a sidecar proxy.

.Missing sidecar badge
image::ossm-node-badge-missing-sidecar.png[Missing Sidecar badge]

The *Graph* page displays a node badge indicating a *Missing Sidecar* on the following graphs:

* App graph
* Versioned app graph
* Workload graph

.Missing sidecar icon
image::ossm-icon-missing-sidecar.png[Missing Sidecar icon]

The *Applications* page displays a *Missing Sidecar* icon in the *Details* column for any applications in a namespace that do not have a sidecar.

The *Workloads* page displays a *Missing Sidecar* icon in the *Details* column for any applications in a namespace that do not have a sidecar.

The *Services* page displays a *Missing Sidecar* icon in the *Details* column for any applications in a namespace that do not have a sidecar. When there are multiple versions of a service, you use the *Service Details* page to view *Missing Sidecar* icons.

The *Workload Details* page has a special unified *Logs* tab that lets you view and correlate application and proxy logs. You can view the Envoy logs as another way to validate sidecar injection for your application workloads.

The *Workload Details* page also has an *Envoy* tab for any workload that is an Envoy proxy or has been injected with an Envoy proxy. This tab displays a built-in Envoy dashboard that includes subtabs for *Clusters*, *Listeners*, *Routes*, *Bootstrap*, *Config*, and *Metrics*.

:leveloffset: 2

For information about enabling Envoy access logs, see the xref:enabling-envoy-access-logs[Troubleshooting] section.

For information about viewing Envoy logs, see xref:ossm-viewing-logs_observability[Viewing logs in the Kiali console]

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-sidecar-injection-env-var_{context}"]
= Setting proxy environment variables through annotations

Configuration for the Envoy sidecar proxies is managed by the `ServiceMeshControlPlane`.

You can set environment variables for the sidecar proxy for applications by adding pod annotations to the deployment in the `injection-template.yaml` file. The environment variables are injected to the sidecar.

.Example injection-template.yaml
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource
spec:
  replicas: 7
  selector:
    matchLabels:
      app: resource
  template:
    metadata:
      annotations:
        sidecar.maistra.io/proxyEnv: "{ \"maistra_test_env\": \"env_value\", \"maistra_test_env_2\": \"env_value_2\" }"
----

[WARNING]
====
You should never include `maistra.io/` labels and annotations when creating your own custom resources.  These labels and annotations indicate that the resources are generated and managed by the Operator. If you are copying content from an Operator-generated resource when creating your own resources, do not include labels or annotations that start with `maistra.io/`.  Resources that include these labels or annotations will be overwritten or deleted by the Operator during the next reconciliation.
====

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-update-app-sidecar_{context}"]
= Updating sidecar proxies

In order to update the configuration for sidecar proxies the application administrator must restart the application pods.

If your deployment uses automatic sidecar injection, you can update the pod template in the deployment by adding or modifying an annotation. Run the following command to redeploy the pods:

[source,terminal]
----
$ oc patch deployment/<deployment> -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date -Iseconds`'"}}}}}'
----

If your deployment does not use automatic sidecar injection, you must manually update the sidecars by modifying the sidecar container image specified in the deployment or pod, and then restart the pods.

:leveloffset: 2

== Next steps

Configure {SMProductName} features for your environment.

* xref:ossm-security[Security]
* xref:ossm-routing-traffic[Traffic management]
* xref:ossm-observability[Metrics, logs, and traces]

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="upgrading-ossm"]
= Upgrading Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-upgrade

toc::[]

To access the most current features of {SMProductName}, upgrade to the current version, {SMProductVersion}.

////
The following include statements pull in the module files that comprise the assembly.
////
:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

[id="ossm-versioning_{context}"]
= Understanding versioning

Red Hat uses semantic versioning for product releases. Semantic Versioning is a 3-component number in the format of X.Y.Z, where:

* X stands for a Major version. Major releases usually denote some sort of breaking change: architectural changes, API changes, schema changes, and similar major updates.

* Y stands for a Minor version. Minor releases contain new features and functionality while maintaining backwards compatibility.

* Z stands for a Patch version (also known as a z-stream release). Patch releases are used to addresses Common Vulnerabilities and Exposures (CVEs) and release bug fixes. New features and functionality are generally not released as part of a Patch release.

== How versioning affects Service Mesh upgrades

Depending on the version of the update you are making, the upgrade process is different.

* *Patch updates* - Patch upgrades are managed by the Operator Lifecycle Manager (OLM); they happen automatically when you update your Operators.

* *Minor upgrades* - Minor upgrades require both updating to the most recent {SMProductName} Operator version and manually modifying the `spec.version` value in your `ServiceMeshControlPlane` resources.

* *Major upgrades* - Major upgrades require both updating to the most recent {SMProductName} Operator version and manually modifying the `spec.version` value in your `ServiceMeshControlPlane` resources. Because major upgrades can contain changes that are not backwards compatible, additional manual changes might be required.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc
// * service_mesh/v2x/ossm-troubleshooting.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-versions_{context}"]
= Understanding Service Mesh versions

In order to understand what version of {SMProductName} you have deployed on your system, you need to understand how each of the component versions is managed.

* *Operator* version - The most current Operator version is {SMProductVersion}. The Operator version number only indicates the version of the currently installed Operator. Because the {SMProductName} Operator supports multiple versions of the {SMProductShortName} control plane, the version of the Operator does not determine the version of your deployed `ServiceMeshControlPlane` resources.
+
[IMPORTANT]
====
Upgrading to the latest Operator version automatically applies patch updates, but does not automatically upgrade your {SMProductShortName} control plane to the latest minor version.
====
+
* *ServiceMeshControlPlane* version - The `ServiceMeshControlPlane` version determines what version of {SMProductName} you are using. The value of the `spec.version` field in the `ServiceMeshControlPlane` resource controls the architecture and configuration settings that are used to install and deploy {SMProductName}. When you create the {SMProductShortName} control plane you can set the version in one of two ways:

** To configure in the Form View, select the version from the *Control Plane Version* menu.

** To configure in the YAML View, set the value for `spec.version` in the YAML file.

Operator Lifecycle Manager (OLM) does not manage {SMProductShortName} control plane upgrades, so the version number for your Operator and `ServiceMeshControlPlane` (SMCP) may not match, unless you have manually upgraded your SMCP.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

[id="ossm-upgrade-considerations_{context}"]
= Upgrade considerations

The `maistra.io/` label or annotation should not be used on a user-created custom resource, because it indicates that the resource was generated by and should be managed by the {SMProductName} Operator.

[WARNING]
====
During the upgrade, the Operator makes changes, including deleting or replacing files, to resources that include the following labels or annotations that indicate that the resource is managed by the Operator.
====

Before upgrading check for user-created custom resources that include the following labels or annotations:

* `maistra.io/` AND the `app.kubernetes.io/managed-by` label set to `maistra-istio-operator` ({SMProductName})

* `kiali.io/` (Kiali)

* `jaegertracing.io/` ({JaegerName})

* `logging.openshift.io/` (Red Hat Elasticsearch)

Before upgrading, check your user-created custom resources for labels or annotations that indicate they are Operator managed. Remove the label or annotation from custom resources that you do not want to be managed by the Operator.

When upgrading to version 2.0, the Operator only deletes resources with these labels in the same namespace as the SMCP.

When upgrading to version 2.1, the Operator deletes resources with these labels in all namespaces.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-upgrade-known-issues_{context}"]
= Known issues that may affect upgrade

Known issues that may affect your upgrade include:

* When upgrading an Operator, custom configurations for Jaeger or Kiali might be reverted. Before upgrading an Operator, note any custom configuration settings for the Jaeger or Kiali objects in the {SMProductShortName} production deployment so that you can recreate them.

* {SMProductName} does not support the use of `EnvoyFilter` configuration except where explicitly documented. This is due to tight coupling with the underlying Envoy APIs, meaning that backward compatibility cannot be maintained. If you are using Envoy Filters, and the configuration generated by Istio has changed due to the lastest version of Envoy introduced by upgrading your `ServiceMeshControlPlane`, that has the potential to break any `EnvoyFilter` you may have implemented.

* https://issues.redhat.com/browse/OSSM-1505[OSSM-1505] `ServiceMeshExtension` does not work with {product-title} version 4.11. Because `ServiceMeshExtension` has been deprecated in {SMProductName} 2.2, this known issue will not be fixed and you must migrate your extensions to `WasmPluging`

* https://issues.redhat.com/browse/OSSM-1396[OSSM-1396] If a gateway resource contains the `spec.externalIPs` setting, rather than being recreated when the `ServiceMeshControlPlane` is updated, the gateway is removed and never recreated.

//Keep OSSM-1052 in RN - Closed as documented.
* https://issues.redhat.com/browse/OSSM-1052[OSSM-1052] When configuring a Service `ExternalIP` for the ingressgateway in the {SMProductShortName} control plane, the service is not created. The schema for the SMCP is missing the parameter for the service.
+
Workaround: Disable the gateway creation in the SMCP spec and manage the gateway deployment entirely manually (including Service, Role and RoleBinding).

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-upgrading-operator_{context}"]
= Upgrading the Operators

In order to keep your {SMProductShortName} patched with the latest security fixes, bug fixes, and software updates, you must keep your Operators updated. You initiate patch updates by upgrading your Operators.

[IMPORTANT]
====
The version of the Operator does *not* determine the version of your service mesh. The version of your deployed {SMProductShortName} control plane determines your version of Service Mesh.
====

Because the {SMProductName} Operator supports multiple versions of the {SMProductShortName} control plane, updating the {SMProductName} Operator does _not_ update the `spec.version` value of your deployed `ServiceMeshControlPlane`. Also note that the `spec.version` value is a two digit number, for example 2.2, and that patch updates, for example 2.2.1, are not reflected in the SMCP version value.

Operator Lifecycle Manager (OLM) controls the installation, upgrade, and role-based access control (RBAC) of Operators in a cluster. The OLM runs by default in {product-title}. OLM queries for available Operators as well as upgrades for installed Operators.

Whether or not you have to take action to upgrade your Operators depends on the settings you selected when installing them. When you installed each of your Operators, you selected an *Update Channel* and an *Approval Strategy*. The combination of these two settings determine when and how your Operators are updated.

.Interaction of Update Channel and Approval Strategy
[options="header"]
[cols="a, a, a"]
|====
| |Versioned channel|"Stable" or "Preview" Channel
|*Automatic*
|Automatically updates the Operator for minor and patch releases for that version only. Will not automatically update to the next major version (that is, from version 2.0 to 3.0). Manual change to Operator subscription required to update to the next major version.
|Automatically updates Operator for all major, minor, and patch releases.

|*Manual*
|Manual updates required for minor and patch releases for the specified version. Manual change to Operator subscription required to update to the next major version.
|Manual updates required for all major, minor, and patch releases.
|====

When you update your {SMProductName} Operator the Operator Lifecycle Manager (OLM) removes the old Operator pod and starts a new pod. Once the new Operator pod starts, the reconciliation process checks the `ServiceMeshControlPlane` (SMCP), and if there are updated container images available for any of the {SMProductShortName} control plane components, it replaces those {SMProductShortName} control plane pods with ones that use the new container images.

When you upgrade the Kiali and {JaegerName} Operators, the OLM reconciliation process scans the cluster and upgrades the managed instances to the version of the new Operator. For example, if you update the {JaegerName} Operator from version 1.30.2 to version 1.34.1, the Operator scans for running instances of {JaegerShortName} and upgrades them to 1.34.1 as well.

To stay on a particular patch version of {SMProductName}, you would need to disable automatic updates and remain on that specific version of the Operator.

:leveloffset: 2

For more information about upgrading Operators, refer to the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#[Operator Lifecycle Manager] documentation.

[id="upgrading-control-plane"]
== Upgrading the control plane

You must manually update the control plane for minor and major releases. The community Istio project recommends canary upgrades, {SMProductName} only supports in-place upgrades. {SMProductName} requires that you upgrade from each minor release to the next minor release in sequence. For example, you must upgrade from version 2.0 to version 2.1, and then upgrade to version 2.2. You cannot update from {SMProductName} 2.0 to 2.2 directly.

When you upgrade the service mesh control plane, all Operator managed resources, for example gateways, are also upgraded.

Although you can deploy multiple versions of the control plane in the same cluster, {SMProductName} does not support canary upgrades of the service mesh. That is, you can have different SCMP resources with different values for `spec.version`, but they cannot be managing the same mesh.

For more information about migrating your extensions, refer to xref:ossm-extensions-migration-overview_ossm-extensions[Migrating from ServiceMeshExtension to WasmPlugin resources].

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-upgrade-23-24-changes_{context}"]
= Upgrade changes from version 2.3 to version 2.4

Upgrading the {SMProductShortName} control plane from version 2.3 to 2.4 introduces the following behavioral changes:

* Support for Istio OpenShift Routing (IOR) has been deprecated. IOR functionality is still enabled, but it will be removed in a future release.

* The following cipher suites are no longer supported, and were removed from the list of ciphers used in client and server side TLS negotiations.

** ECDHE-ECDSA-AES128-SHA
** ECDHE-RSA-AES128-SHA
** AES128-GCM-SHA256
** AES128-SHA
** ECDHE-ECDSA-AES256-SHA
** ECDHE-RSA-AES256-SHA
** AES256-GCM-SHA384
** AES256-SHA
+
Applications that require access to services that use one of these cipher suites will fail to connect when the proxy initiates a TLS connection.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-upgrade-22-23-changes_{context}"]
= Upgrade changes from version 2.2 to version 2.3

Upgrading the {SMProductShortName} control plane from version 2.2 to 2.3 introduces the following behavioral changes:

* This release requires use of the `WasmPlugin` API. Support for the `ServiceMeshExtension` API, which was deprecated in 2.2, has now been removed. If you attempt to upgrade while using the `ServiceMeshExtension` API, then the upgrade fails.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-upgrade-21-22-changes_{context}"]
= Upgrade changes from version 2.1 to version 2.2

Upgrading the {SMProductShortName} control plane from version 2.1 to 2.2 introduces the following behavioral changes:

* The `istio-node` DaemonSet is renamed to `istio-cni-node` to match the name in upstream Istio.

* Istio 1.10 updated Envoy to send traffic to the application container using `eth0` rather than `lo` by default.

* This release adds support for the `WasmPlugin` API and deprecates the `ServiceMeshExtension` API.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-upgrade-20-21-changes_{context}"]
= Upgrade changes from version 2.0 to version 2.1

Upgrading the {SMProductShortName} control plane from version 2.0 to 2.1 introduces the following architectural and behavioral changes.

.Architecture changes

Mixer has been completely removed in {SMProductName} 2.1. Upgrading from a {SMProductName} 2.0.x release to 2.1 will be blocked if Mixer is enabled.

If you see the following message when upgrading from v2.0 to v2.1, update the existing `Mixer` type to `Istiod` type in the existing Control Plane spec before you update the `.spec.version` field:

[source,text]
----
An error occurred
admission webhook smcp.validation.maistra.io denied the request: [support for policy.type "Mixer" and policy.Mixer options have been removed in v2.1, please use another alternative, support for telemetry.type "Mixer" and telemetry.Mixer options have been removed in v2.1, please use another alternative]”
----

For example:

[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
spec:
  policy:
    type: Istiod
  telemetry:
    type: Istiod
  version: v{MaistraVersion}
----


[id="ossm-upgrading-differences-behavior_{context}"]
.Behavioral changes

* `AuthorizationPolicy` updates:
** With the PROXY protocol, if you're using `ipBlocks` and `notIpBlocks` to specify remote IP addresses, update the configuration to use `remoteIpBlocks` and `notRemoteIpBlocks` instead.
** Added support for nested JSON Web Token (JWT) claims.
* `EnvoyFilter` breaking changes>
** Must use `typed_config`
** xDS v2 is no longer supported
** Deprecated filter names
* Older versions of proxies may report 503 status codes when receiving 1xx or 204 status codes from newer proxies.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-upgrading-smcp_{context}"]
= Upgrading the Service Mesh control plane

To upgrade {SMProductName}, you must update the version field of the {SMProductName} `ServiceMeshControlPlane` v2 resource. Then, once it is configured and applied, restart the application pods to update each sidecar proxy and its configuration.

.Prerequisites

* You are running {product-title} 4.9 or later.
* You have the latest {SMProductName} Operator.

.Procedure

. Switch to the project that contains your `ServiceMeshControlPlane` resource. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc project istio-system
----

. Check your v2 `ServiceMeshControlPlane` resource configuration to verify it is valid.
+
.. Run the following command to view your `ServiceMeshControlPlane` resource as a v2 resource.
+
[source,terminal]
----
$ oc get smcp -o yaml
----
+
[TIP]
====
Back up your {SMProductShortName} control plane configuration.
====

. Update the `.spec.version` field and apply the configuration.
+
For example:
+
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
----
+
Alternatively, instead of using the command line, you can use the web console to edit the {SMProductShortName} control plane. In the {product-title} web console, click *Project* and select the project name you just entered.
+
.. Click *Operators* -> *Installed Operators*.
.. Find your `ServiceMeshControlPlane` instance.
.. Select *YAML view* and update text of the YAML file, as shown in the previous example.
.. Click *Save*.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-migrating-to-20_{context}"]
= Migrating {SMProductName} from version 1.1 to version 2.0

Upgrading from version 1.1 to 2.0 requires manual steps that migrate your workloads and application to a new instance of {SMProductName} running the new version.

.Prerequisites

* You must upgrade to {product-title} 4.7. before you upgrade to {SMProductName} 2.0.
* You must have {SMProductName} version 2.0 operator. If you selected the *automatic* upgrade path, the operator automatically downloads the latest information. However, there are steps you must take to use the features in {SMProductName} version 2.0.

[id="ossm-migrating_{context}"]
== Upgrading {SMProductName}

To upgrade {SMProductName}, you must create an instance of {SMProductName} `ServiceMeshControlPlane` v2 resource in a new namespace. Then, once it's configured, move your microservice applications and workloads from your old mesh to the new service mesh.

.Procedure

. Check your v1 `ServiceMeshControlPlane` resource configuration to make sure it is valid.
+
.. Run the following command to view your `ServiceMeshControlPlane` resource as a v2 resource.
+
[source,terminal]
----
$ oc get smcp -o yaml
----
+
.. Check the `spec.techPreview.errored.message` field in the output for information about any invalid fields.
+
.. If there are invalid fields in your v1 resource, the resource is not reconciled and cannot be edited as a v2 resource. All updates to v2 fields will be overridden by the original v1 settings. To fix the invalid fields, you can replace, patch, or edit the v1 version of the resource. You can also delete the resource without fixing it. After the resource has been fixed, it can be reconciled, and you can to modify or view the v2 version of the resource.
+
.. To fix the resource by editing a file, use `oc get` to retrieve the resource, edit the text file locally, and replace the resource with the file you edited.
+
[source,terminal]
----
$ oc get smcp.v1.maistra.io <smcp_name> > smcp-resource.yaml
#Edit the smcp-resource.yaml file.
$ oc replace -f smcp-resource.yaml
----
+
.. To fix the resource using patching, use `oc patch`.
+
[source,terminal]
----
$ oc patch smcp.v1.maistra.io <smcp_name> --type json --patch '[{"op": "replace","path":"/spec/path/to/bad/setting","value":"corrected-value"}]'
----
+
.. To fix the resource by editing with command line tools, use `oc edit`.
+
[source,terminal]
----
$ oc edit smcp.v1.maistra.io <smcp_name>
----
+
. Back up your {SMProductShortName} control plane configuration. Switch to the project that contains your `ServiceMeshControlPlane` resource.  In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc project istio-system
----
+
. Enter the following command to retrieve the current configuration. Your <smcp_name> is specified in the metadata of your `ServiceMeshControlPlane` resource, for example `basic-install` or `full-install`.
+
[source,terminal]
----
$ oc get servicemeshcontrolplanes.v1.maistra.io <smcp_name> -o yaml > <smcp_name>.v1.yaml
----
+
. Convert your `ServiceMeshControlPlane` to a v2 control plane version that contains information about your configuration as a starting point.
+
[source,terminal]
----
$ oc get smcp <smcp_name> -o yaml > <smcp_name>.v2.yaml
----
+
. Create a project. In the {product-title} console Project menu, click `New Project` and enter a name for your project, `istio-system-upgrade`, for example. Or, you can run this command from the CLI.
+
[source,terminal]
----
$ oc new-project istio-system-upgrade
----
+
. Update the `metadata.namespace` field in your v2 `ServiceMeshControlPlane` with your new project name. In this example, use `istio-system-upgrade`.
+
. Update the `version` field from 1.1 to 2.0 or remove it in your v2 `ServiceMeshControlPlane`.
+
. Create a `ServiceMeshControlPlane` in the new namespace. On the command line, run the following command to deploy the control plane with the v2 version of the `ServiceMeshControlPlane` that you retrieved. In this example, replace `<smcp_name.v2> `with the path to your file.
+
[source,terminal]
----
$ oc create -n istio-system-upgrade -f <smcp_name>.v2.yaml
----
+
Alternatively, you can use the console to create the {SMProductShortName} control plane. In the {product-title} web console, click *Project*. Then, select the project name you just entered.
+
.. Click *Operators* -> *Installed Operators*.
.. Click *Create ServiceMeshControlPlane*.
.. Select *YAML view* and paste text of the YAML file you retrieved into the field. Check that the `apiVersion` field is set to `maistra.io/v2` and modify the `metadata.namespace` field to use the new namespace, for example `istio-system-upgrade`.
.. Click *Create*.

[id="ossm-migrating-smcp_{context}"]
== Configuring the 2.0 ServiceMeshControlPlane

The `ServiceMeshControlPlane` resource has been changed for {SMProductName} version 2.0. After you created a v2 version of the `ServiceMeshControlPlane` resource, modify it to take advantage of the new features and to fit your deployment. Consider the following changes to the specification and behavior of {SMProductName} 2.0 as you're modifying your `ServiceMeshControlPlane` resource. You can also refer to the {SMProductName} 2.0 product documentation for new information to features you use. The v2 resource must be used for {SMProductName} 2.0 installations.

[id="ossm-migrating-differences-arch_{context}"]
=== Architecture changes

The architectural units used by previous versions have been replaced by Istiod. In 2.0 the {SMProductShortName} control plane components Mixer, Pilot, Citadel, Galley, and the sidecar injector functionality have been combined into a single component, Istiod.

Although Mixer is no longer supported as a control plane component, Mixer policy and telemetry plugins are now supported through WASM extensions in Istiod. Mixer can be enabled for policy and telemetry if you need to integrate legacy Mixer plugins.

Secret Discovery Service (SDS) is used to distribute certificates and keys to sidecars directly from Istiod. In {SMProductName} version 1.1, secrets were generated by Citadel, which were used by the proxies to retrieve their client certificates and keys.

[id="ossm-migrating-differences-annotation_{context}"]
=== Annotation changes

The following annotations are no longer supported in v2.0. If you are using one of these annotations, you must update your workload before moving it to a v2.0 {SMProductShortName} control plane.

* `sidecar.maistra.io/proxyCPULimit` has been replaced with `sidecar.istio.io/proxyCPULimit`. If you were using `sidecar.maistra.io` annotations on your workloads, you must modify those workloads to use `sidecar.istio.io` equivalents instead.
* `sidecar.maistra.io/proxyMemoryLimit` has been replaced with `sidecar.istio.io/proxyMemoryLimit`
* `sidecar.istio.io/discoveryAddress` is no longer supported. Also, the default discovery address has moved from `pilot.<control_plane_namespace>.svc:15010` (or port 15011, if mtls is enabled) to `istiod-<smcp_name>.<control_plane_namespace>.svc:15012`.
* The health status port is no longer configurable and is hard-coded to 15021.  * If you were defining a custom status port, for example, `status.sidecar.istio.io/port`, you must remove the override before moving the workload to a v2.0 {SMProductShortName} control plane. Readiness checks can still be disabled by setting the status port to `0`.
* Kubernetes Secret resources are no longer used to distribute client certificates for sidecars. Certificates are now distributed through Istiod's SDS service. If you were relying on mounted secrets, they are longer available for workloads in v2.0 {SMProductShortName} control planes.

[id="ossm-migrating-differences-behavior_{context}"]
=== Behavioral changes

Some features in {SMProductName} 2.0 work differently than they did in previous versions.

* The readiness port on gateways has moved from `15020` to `15021`.
* The target host visibility includes VirtualService, as well as ServiceEntry resources. It includes any restrictions applied through Sidecar resources.
* Automatic mutual TLS is enabled by default. Proxy to proxy communication is automatically configured to use mTLS, regardless of global PeerAuthentication policies in place.
* Secure connections are always used when proxies communicate with the {SMProductShortName} control plane regardless of `spec.security.controlPlane.mtls` setting. The `spec.security.controlPlane.mtls` setting is only used when configuring connections for Mixer telemetry or policy.

[id="ossm-migrating-unsupported-resources_{context}"]
=== Migration details for unsupported resources

.Policy (authentication.istio.io/v1alpha1)

Policy resources must be migrated to new resource types for use with v2.0 {SMProductShortName} control planes, PeerAuthentication and RequestAuthentication. Depending on the specific configuration in your Policy resource, you may have to configure multiple resources to achieve the same effect.

.Mutual TLS

Mutual TLS enforcement is accomplished using the `security.istio.io/v1beta1` PeerAuthentication resource. The legacy `spec.peers.mtls.mode` field maps directly to the new resource's `spec.mtls.mode` field. Selection criteria has changed from specifying a service name in `spec.targets[x].name` to a label selector in `spec.selector.matchLabels`. In PeerAuthentication, the labels must match the selector on the services named in the targets list. Any port-specific settings will need to be mapped into `spec.portLevelMtls`.

.Authentication

Additional authentication methods specified in `spec.origins`, must be mapped into a `security.istio.io/v1beta1` RequestAuthentication resource.  `spec.selector.matchLabels` must be configured similarly to the same field on PeerAuthentication. Configuration specific to JWT principals from `spec.origins.jwt` items map to similar fields in `spec.rules` items.

* `spec.origins[x].jwt.triggerRules` specified in the Policy must be mapped into one or more `security.istio.io/v1beta1` AuthorizationPolicy resources. Any `spec.selector.labels` must be configured similarly to the same field on RequestAuthentication.
* `spec.origins[x].jwt.triggerRules.excludedPaths` must be mapped into an AuthorizationPolicy whose spec.action is set to ALLOW, with `spec.rules[x].to.operation.path` entries matching the excluded paths.
* `spec.origins[x].jwt.triggerRules.includedPaths` must be mapped into a separate AuthorizationPolicy whose `spec.action` is set to `ALLOW`, with `spec.rules[x].to.operation.path` entries matching the included paths, and `spec.rules.[x].from.source.requestPrincipals` entries that align with the `specified spec.origins[x].jwt.issuer` in the Policy resource.

.ServiceMeshPolicy (maistra.io/v1)

ServiceMeshPolicy was configured automatically for the {SMProductShortName} control plane through the `spec.istio.global.mtls.enabled` in the v1 resource or `spec.security.dataPlane.mtls` in the v2 resource setting. For v2 control planes, a functionally equivalent PeerAuthentication resource is created during installation. This feature is deprecated in {SMProductName} version 2.0

.RbacConfig, ServiceRole, ServiceRoleBinding (rbac.istio.io/v1alpha1)

These resources were replaced by the `security.istio.io/v1beta1` AuthorizationPolicy resource.

Mimicking RbacConfig behavior requires writing a default AuthorizationPolicy whose settings depend on the spec.mode specified in the RbacConfig.

* When `spec.mode` is set to `OFF`, no resource is required as the default policy is ALLOW, unless an AuthorizationPolicy applies to the request.
* When `spec.mode` is set to ON, set `spec: {}`.  You must create AuthorizationPolicy policies for all services in the mesh.
* `spec.mode` is set to `ON_WITH_INCLUSION`, must create an AuthorizationPolicy with `spec: {}` in each included namespace. Inclusion of individual services is not supported by AuthorizationPolicy. However, as soon as any AuthorizationPolicy is created that applies to the workloads for the service, all other requests not explicitly allowed will be denied.
* When `spec.mode` is set to `ON_WITH_EXCLUSION`, it is not supported by AuthorizationPolicy. A global DENY policy can be created, but an AuthorizationPolicy must be created for every workload in the mesh because there is no allow-all policy that can be applied to either a namespace or a workload.

AuthorizationPolicy includes configuration for both the selector to which the configuration applies, which is similar to the function ServiceRoleBinding provides and the rules which should be applied, which is similar to the function ServiceRole provides.

.ServiceMeshRbacConfig (maistra.io/v1)

This resource is replaced by using a `security.istio.io/v1beta1` AuthorizationPolicy resource with an empty spec.selector in the {SMProductShortName} control plane's namespace.  This policy will be the default authorization policy applied to all workloads in the mesh.  For specific migration details, see RbacConfig above.

[id="ossm-migrating-mixer_{context}"]
=== Mixer plugins

Mixer components are disabled by default in version 2.0. If you rely on Mixer plugins for your workload, you must configure your version 2.0 `ServiceMeshControlPlane` to include the Mixer components.

To enable the Mixer policy components, add the following snippet to your `ServiceMeshControlPlane`.

[source,yaml]
----
spec:
  policy:
    type: Mixer
----

To enable the Mixer telemetry components, add the following snippet to your `ServiceMeshControlPlane`.

[source,yaml]
----
spec:
  telemetry:
    type: Mixer
----

Legacy mixer plugins can also be migrated to WASM and integrated using the new ServiceMeshExtension (maistra.io/v1alpha1) custom resource.

Built-in WASM filters included in the upstream Istio distribution are not available in {SMProductName} 2.0.

[id="ossm-migrating-mtls_{context}"]
=== Mutual TLS changes

When using mTLS with workload specific PeerAuthentication policies, a corresponding DestinationRule is required to allow traffic if the workload policy differs from the namespace/global policy.

Auto mTLS is enabled by default, but can be disabled by setting `spec.security.dataPlane.automtls` to false in the `ServiceMeshControlPlane` resource. When disabling auto mTLS, DestinationRules may be required for proper communication between services. For example, setting PeerAuthentication to `STRICT` for one namespace may prevent services in other namespaces from accessing them, unless a DestinationRule configures TLS mode for the services in the namespace.

For information about mTLS, see xref:ossm-security-mtls_ossm-security[Enabling mutual Transport Layer Security (mTLS)]

==== Other mTLS Examples

To disable mTLS For productpage service in the bookinfo sample application, your Policy resource was configured the following way for {SMProductName} v1.1.

.Example Policy resource
[source,yaml]
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: productpage-mTLS-disable
  namespace: <namespace>
spec:
  targets:
  - name: productpage
----

To disable mTLS For productpage service in the bookinfo sample application, use the following example to configure your PeerAuthentication resource for {SMProductName} v2.0.

.Example PeerAuthentication resource
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: productpage-mTLS-disable
  namespace: <namespace>
spec:
  mtls:
    mode: DISABLE
  selector:
    matchLabels:
      # this should match the selector for the "productpage" service
      app: productpage
----

To enable mTLS With JWT authentication for the `productpage` service in the bookinfo sample application, your Policy resource was configured the following way for {SMProductName} v1.1.

.Example Policy resource
[source,yaml]
----
apiVersion: authentication.istio.io/v1alpha1
kind: Policy
metadata:
  name: productpage-mTLS-with-JWT
  namespace: <namespace>
spec:
  targets:
  - name: productpage
    ports:
    - number: 9000
  peers:
  - mtls:
  origins:
  - jwt:
      issuer: "https://securetoken.google.com"
      audiences:
      - "productpage"
      jwksUri: "https://www.googleapis.com/oauth2/v1/certs"
      jwtHeaders:
      - "x-goog-iap-jwt-assertion"
      triggerRules:
      - excludedPaths:
        - exact: /health_check
  principalBinding: USE_ORIGIN
----

To enable mTLS With JWT authentication for the productpage service in the bookinfo sample application, use the following example to configure your PeerAuthentication resource for {SMProductName} v2.0.

.Example PeerAuthentication resource
[source,yaml]
----
#require mtls for productpage:9000
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: productpage-mTLS-with-JWT
  namespace: <namespace>
spec:
  selector:
    matchLabels:
      # this should match the selector for the "productpage" service
      app: productpage
  portLevelMtls:
    9000:
      mode: STRICT
---
#JWT authentication for productpage
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: productpage-mTLS-with-JWT
  namespace: <namespace>
spec:
  selector:
    matchLabels:
      # this should match the selector for the "productpage" service
      app: productpage
  jwtRules:
  - issuer: "https://securetoken.google.com"
    audiences:
    - "productpage"
    jwksUri: "https://www.googleapis.com/oauth2/v1/certs"
    fromHeaders:
    - name: "x-goog-iap-jwt-assertion"
---
#Require JWT token to access product page service from
#any client to all paths except /health_check
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: productpage-mTLS-with-JWT
  namespace: <namespace>
spec:
  action: ALLOW
  selector:
    matchLabels:
      # this should match the selector for the "productpage" service
      app: productpage
  rules:
  - to: # require JWT token to access all other paths
      - operation:
          notPaths:
          - /health_check
    from:
      - source:
          # if using principalBinding: USE_PEER in the Policy,
          # then use principals, e.g.
          # principals:
          # - “*”
          requestPrincipals:
          - “*”
  - to: # no JWT token required to access health_check
      - operation:
          paths:
          - /health_check
----

[id="ossm-migrating-config-recipes_{context}"]
== Configuration recipes

You can configure the following items with these configuration recipes.

[id="ossm-migrating-config-mtls_{context}"]
=== Mutual TLS in a data plane

Mutual TLS for data plane communication is configured through `spec.security.dataPlane.mtls` in the `ServiceMeshControlPlane` resource, which is `false` by default.

[id="ossm-migrating-config-sign-key_{context}"]
=== Custom signing key

Istiod manages client certificates and private keys used by service proxies. By default, Istiod uses a self-signed certificate for signing, but you can configure a custom certificate and private key. For more information about how to configure signing keys, see xref:ossm-cert-manage_ossm-security[Adding an external certificate authority key and certificate]

[id="ossm-migrating-config-tracing_{context}"]
=== Tracing

Tracing is configured in `spec.tracing`. Currently, the only type of tracer that is supported is `Jaeger`. Sampling is a scaled integer representing 0.01% increments, for example, 1 is 0.01% and 10000 is 100%. The tracing implementation and sampling rate can be specified:

[source,yaml]
----
spec:
  tracing:
    sampling: 100 # 1%
    type: Jaeger
----

Jaeger is configured in the `addons` section of the `ServiceMeshControlPlane` resource.

[source,yaml]
----
spec:
  addons:
    jaeger:
      name: jaeger
      install:
        storage:
          type: Memory # or Elasticsearch for production mode
          memory:
            maxTraces: 100000
          elasticsearch: # the following values only apply if storage:type:=Elasticsearch
            storage: # specific storageclass configuration for the Jaeger Elasticsearch (optional)
              size: "100G"
              storageClassName: "storageclass"
            nodeCount: 3
            redundancyPolicy: SingleRedundancy
  runtime:
    components:
      tracing.jaeger: {} # general Jaeger specific runtime configuration (optional)
      tracing.jaeger.elasticsearch: #runtime configuration for Jaeger Elasticsearch deployment (optional)
        container:
          resources:
            requests:
              memory: "1Gi"
              cpu: "500m"
            limits:
              memory: "1Gi"
----

The Jaeger installation can be customized with the `install` field. Container configuration, such as resource limits is configured in `spec.runtime.components.jaeger` related fields. If a Jaeger resource matching the value of `spec.addons.jaeger.name` exists, the {SMProductShortName} control plane will be configured to use the existing installation. Use an existing Jaeger resource to fully customize your Jaeger installation.

[id="ossm-migrating-config-vis_{context}"]
=== Visualization

Kiali and Grafana are configured under the `addons` section of the `ServiceMeshControlPlane` resource.

[source,yaml]
----
spec:
  addons:
    grafana:
      enabled: true
      install: {} # customize install
    kiali:
      enabled: true
      name: kiali
      install: {} # customize install
----

The Grafana and Kiali installations can be customized through their respective `install` fields. Container customization, such as resource limits, is configured in `spec.runtime.components.kiali` and `spec.runtime.components.grafana`. If an existing Kiali resource matching the value of name exists, the {SMProductShortName} control plane configures the Kiali resource for use with the control plane. Some fields in the Kiali resource are overridden, such as the `accessible_namespaces` list, as well as the endpoints for Grafana, Prometheus, and tracing. Use an existing resource to fully customize your Kiali installation.

=== Resource utilization and scheduling

Resources are configured under `spec.runtime.<component>`. The following component names are supported.

|===
|Component |Description |Versions supported

|security
|Citadel container
|v1.0/1.1

|galley
|Galley container
|v1.0/1.1

|pilot
|Pilot/Istiod container
|v1.0/1.1/2.0

|mixer
|istio-telemetry and istio-policy containers
|v1.0/1.1

|`mixer.policy`
|istio-policy container
|v2.0

|`mixer.telemetry`
|istio-telemetry container
|v2.0

|`global.oauthproxy`
|oauth-proxy container used with various addons
|v1.0/1.1/2.0

|`sidecarInjectorWebhook`
|sidecar injector webhook container
|v1.0/1.1

|`tracing.jaeger`
|general Jaeger container - not all settings may be applied. Complete customization of Jaeger installation is supported by specifying an existing Jaeger resource in the {SMProductShortName} control plane configuration.
|v1.0/1.1/2.0

|`tracing.jaeger.agent`
|settings specific to Jaeger agent
|v1.0/1.1/2.0

|`tracing.jaeger.allInOne`
|settings specific to Jaeger allInOne
|v1.0/1.1/2.0

|`tracing.jaeger.collector`
|settings specific to Jaeger collector
|v1.0/1.1/2.0

|`tracing.jaeger.elasticsearch`
|settings specific to Jaeger elasticsearch deployment
|v1.0/1.1/2.0

|`tracing.jaeger.query`
|settings specific to Jaeger query
|v1.0/1.1/2.0

|prometheus
|prometheus container
|v1.0/1.1/2.0

|kiali
|Kiali container - complete customization of Kiali installation is supported by specifying an existing Kiali resource in the {SMProductShortName} control plane configuration.
|v1.0/1.1/2.0

|grafana
|Grafana container
|v1.0/1.1/2.0

|3scale
|3scale container
|v1.0/1.1/2.0

|`wasmExtensions.cacher`
|WASM extensions cacher container
|v2.0 - tech preview
|===

Some components support resource limiting and scheduling. For more information, see xref:ossm-performance-scalability[Performance and scalability].

[id="ossm-migrrating-apps_{context}"]
== Next steps for migrating your applications and workloads

Move the application workload to the new mesh and remove the old instances to complete your upgrade.

:leveloffset: 2

[id="upgrading-data-plane"]
== Upgrading the data plane

Your data plane will still function after you have upgraded the control plane. But in order to apply updates to the Envoy proxy and any changes to the proxy configuration, you must restart your application pods and workloads.

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-upgrading-apps-workloads_{context}"]
= Updating your applications and workloads

To complete the migration, restart all of the application pods in the mesh to upgrade the Envoy sidecar proxies and their configuration.

To perform a rolling update of a deployment use the following command:

[source,terminal]
----
$ oc rollout restart <deployment>
----

You must perform a rolling update for all applications that make up the mesh.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-profiles-users"]
= Managing users and profiles
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-profiles-users

toc::[]

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

[id="ossm-members_{context}"]
= Creating the {SMProductName} members

`ServiceMeshMember` resources provide a way for {SMProductName} administrators to delegate permissions to add projects to a service mesh, even when the respective users don't have direct access to the service mesh project or member roll. While project administrators are automatically given permission to create the `ServiceMeshMember` resource in their project, they cannot point it to any `ServiceMeshControlPlane` until the service mesh administrator explicitly grants access to the service mesh. Administrators can grant users permissions to access the mesh by granting them the `mesh-user` user role. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

[source,terminal]
----
$ oc policy add-role-to-user -n istio-system --role-namespace istio-system mesh-user <user_name>
----

Administrators can modify the `mesh-user` role binding in the {SMProductShortName} control plane project to specify the users and groups that are granted access. The `ServiceMeshMember` adds the project to the `ServiceMeshMemberRoll` within the {SMProductShortName} control plane project that it references.

[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMember
metadata:
  name: default
spec:
  controlPlaneRef:
    namespace: istio-system
    name: basic
----

The `mesh-users` role binding is created automatically after the administrator creates the `ServiceMeshControlPlane` resource. An administrator can use the following command to add a role to a user.

[source,terminal]
----
$ oc policy add-role-to-user
----

The administrator can also create the `mesh-user` role binding before the administrator creates the `ServiceMeshControlPlane` resource. For example, the administrator can create it in the same `oc apply` operation as the `ServiceMeshControlPlane` resource.

This example adds a role binding for `alice`:

[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  namespace: istio-system
  name: mesh-users
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: mesh-user
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: alice
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-profiles_{context}"]
= Creating {SMProductShortName} control plane profiles

You can create reusable configurations with `ServiceMeshControlPlane` profiles. Individual users can extend the profiles they create with their own configurations. Profiles can also inherit configuration information from other profiles. For example, you can create an accounting control plane for the accounting team and a marketing control plane for the marketing team. If you create a development template and a production template, members of the marketing team and the accounting team can extend the development and production profiles with team-specific customization.

When you configure {SMProductShortName} control plane profiles, which follow the same syntax as the `ServiceMeshControlPlane`, users inherit settings in a hierarchical fashion. The Operator is delivered with a `default` profile with default settings for {SMProductName}.

[id="ossm-create-configmap_{context}"]
== Creating the ConfigMap

To add custom profiles, you must create a `ConfigMap` named `smcp-templates` in the `openshift-operators` project. The Operator container automatically mounts the `ConfigMap`.

.Prerequisites

* An installed, verified {SMProductShortName} Operator.
* An account with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
* Location of the Operator deployment.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a `cluster-admin`. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. From the CLI, run this command to create the ConfigMap named `smcp-templates` in the `openshift-operators` project and replace `<profiles-directory>` with the location of the `ServiceMeshControlPlane` files on your local disk:
+
[source,terminal]
----
$ oc create configmap --from-file=<profiles-directory> smcp-templates -n openshift-operators
----

. You can use the `profiles` parameter in the `ServiceMeshControlPlane` to specify one or more templates.
+
[source,yaml]
----
  apiVersion: maistra.io/v2
  kind: ServiceMeshControlPlane
  metadata:
    name: basic
  spec:
    profiles:
    - default
----

:leveloffset: 2

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-config-network-policy_{context}"]

== Setting the correct network policy

{SMProductShortName} creates network policies in the {SMProductShortName} control plane and member namespaces to allow traffic between them. Before you deploy, consider the following conditions to ensure the services in your service mesh that were previously exposed through an {product-title} route.

* Traffic into the service mesh must always go through the ingress-gateway for Istio to work properly.
* Deploy services external to the service mesh in separate namespaces that are not in any service mesh.
* Non-mesh services that need to be deployed within a service mesh enlisted namespace should label their deployments `maistra.io/expose-route: "true"`, which ensures {product-title} routes to these services still work.

:leveloffset: 2


:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-security"]
= Security
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-security

toc::[]

If your service mesh application is constructed with a complex array of microservices, you can use {SMProductName} to customize the security of the communication between those services. The infrastructure of {product-title} along with the traffic management features of {SMProductShortName} help you manage the complexity of your applications and secure microservices.

.Before you begin

If you have a project, add your project to the xref:ossm-member-roll-create_ossm-create-mesh[`ServiceMeshMemberRoll` resource].

If you don't have a project, install the xref:ossm-tutorial-bookinfo-overview_ossm-create-mesh[Bookinfo sample application] and add it to the `ServiceMeshMemberRoll` resource. The sample application helps illustrate security concepts.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-config.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-security-mtls_{context}"]
= About mutual Transport Layer Security (mTLS)

Mutual Transport Layer Security (mTLS) is a protocol that enables two parties to authenticate each other. It is the default mode of authentication in some protocols (IKE, SSH) and optional in others (TLS). You can use mTLS without changes to the application or service code. The TLS is handled entirely by the service mesh infrastructure and between the two sidecar proxies.

By default, mTLS in {SMProductName} is enabled and set to permissive mode, where the sidecars in {SMProductShortName} accept both plain-text traffic and connections that are encrypted using mTLS. If a service in your mesh configured to use strict mTLS is communicating with a service outside the mesh, communication might break between those services because strict mTLS requires both the client and the server to be able to verify the identify of each other. Use permissive mode while you migrate your workloads to {SMProductShortName}. Then, you can enable strict mTLS across your mesh, namespace, or application.

Enabling mTLS across your mesh at the {SMProductShortName} control plane level secures all the traffic in your service mesh without rewriting your applications and workloads. You can secure namespaces in your mesh at the data plane level in the `ServiceMeshControlPlane` resource. To customize traffic encryption connections, configure namespaces at the application level with `PeerAuthentication` and `DestinationRule` resources.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-security-enabling-strict-mtls_{context}"]
= Enabling strict mTLS across the service mesh

If your workloads do not communicate with outside services, you can quickly enable mTLS across your mesh without communication interruptions. You can enable it by setting `spec.security.dataPlane.mtls` to `true` in the `ServiceMeshControlPlane` resource. The Operator creates the required resources.

[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
spec:
  version: v{MaistraVersion}
  security:
    dataPlane:
      mtls: true
----

You can also enable mTLS by using the {product-title} web console.

.Procedure

. Log in to the web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click *Operators* -> *Installed Operators*.

. Click *Service Mesh Control Plane* under *Provided APIs*.

. Click the name of your `ServiceMeshControlPlane` resource, for example, `basic`.

. On the *Details* page, click the toggle in the *Security* section for *Data Plane Security*.

:leveloffset: 2

:leveloffset: +3

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-security-mtls-sidecars-incoming-services_{context}"]
= Configuring sidecars for incoming connections for specific services

You can also configure mTLS for individual services by creating a policy.

.Procedure

. Create a YAML file using the following example.
+
.PeerAuthentication Policy example policy.yaml
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: PeerAuthentication
metadata:
  name: default
  namespace: <namespace>
spec:
  mtls:
    mode: STRICT
----
+
.. Replace `<namespace>` with the namespace where the service is located.

. Run the following command to create the resource in the namespace where the service is located. It must match the `namespace` field in the Policy resource you just created.
+
[source,terminal]
----
$ oc create -n <namespace> -f <policy.yaml>
----

[NOTE]
====
If you are not using automatic mTLS and you are setting `PeerAuthentication` to STRICT, you must create a `DestinationRule` resource for your service.
====

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-security-mtls-sidecars-outgoing_{context}"]
== Configuring sidecars for outgoing connections

Create a destination rule to configure {SMProductShortName} to use mTLS when sending requests to other services in the mesh.

.Procedure

. Create a YAML file using the following example.
+
.DestinationRule example destination-rule.yaml
[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: default
  namespace: <namespace>
spec:
  host: "*.<namespace>.svc.cluster.local"
  trafficPolicy:
   tls:
    mode: ISTIO_MUTUAL
----
+
.. Replace `<namespace>` with the namespace where the service is located.

. Run the following command to create the resource in the namespace where the service is located. It must match the `namespace` field in the `DestinationRule` resource you just created.
+
[source,terminal]
----
$ oc create -n <namespace> -f <destination-rule.yaml>
----

:leveloffset: 2

:leveloffset: +2

:_mod-docs-content-type: PROCEDURE
[id="ossm-security-min-max-tls_{context}"]
== Setting the minimum and maximum protocol versions

If your environment has specific requirements for encrypted traffic in your service mesh, you can control the cryptographic functions that are allowed by setting the `spec.security.controlPlane.tls.minProtocolVersion` or `spec.security.controlPlane.tls.maxProtocolVersion` in your `ServiceMeshControlPlane` resource. Those values, configured in your {SMProductShortName} control plane resource, define the minimum and maximum TLS version used by mesh components when communicating securely over TLS.

The default is `TLS_AUTO` and does not specify a version of TLS.

.Valid values
|===
|Value|Description

|`TLS_AUTO`
| default

|`TLSv1_0`
|TLS version 1.0

|`TLSv1_1`
|TLS version 1.1

|`TLSv1_2`
|TLS version 1.2

|`TLSv1_3`
|TLS version 1.3
|===

.Procedure

. Log in to the web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click *Operators* -> *Installed Operators*.

. Click *Service Mesh Control Plane* under *Provided APIs*.

. Click the name of your `ServiceMeshControlPlane` resource, for example, `basic`.

. Click the *YAML* tab.

. Insert the following code snippet in the YAML editor. Replace the value in the `minProtocolVersion` with the TLS version value. In this example, the minimum TLS version is set to `TLSv1_2`.
+
.ServiceMeshControlPlane snippet
[source,yaml]
----
kind: ServiceMeshControlPlane
spec:
  security:
    controlPlane:
      tls:
        minProtocolVersion: TLSv1_2
----

. Click *Save*.

. Click *Refresh* to verify that the changes updated correctly.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-validating-sidecar_{context}"]
= Validating encryption with Kiali

The Kiali console offers several ways to validate whether or not your applications, services, and workloads have mTLS encryption enabled.

.Masthead icon mesh-wide mTLS enabled
image::ossm-kiali-masthead-mtls-enabled.png[mTLS enabled]

At the right side of the masthead, Kiali shows a lock icon when the mesh has strictly enabled mTLS for the whole service mesh. It means that all communications in the mesh use mTLS.

.Masthead icon mesh-wide mTLS partially enabled
image::ossm-kiali-masthead-mtls-partial.png[mTLS partially enabled]

Kiali displays a hollow lock icon when either the mesh is configured in `PERMISSIVE` mode or there is a error in the mesh-wide mTLS configuration.

.Security badge
image::ossm-kiali-graph-badge-security.png[Security badge]

The *Graph* page has the option to display a *Security* badge on the graph edges to indicate that mTLS is enabled.  To enable security badges on the graph, from the *Display* menu, under *Show Badges*, select the *Security* checkbox.  When an edge shows a lock icon, it means at least one request with mTLS enabled is present.  In case there are both mTLS and non-mTLS requests, the side-panel will show the percentage of requests that use mTLS.

The *Applications Detail Overview* page displays a *Security* icon on the graph edges where at least one request with mTLS enabled is present.

The *Workloads Detail Overview* page displays a *Security* icon on the graph edges where at least one request with mTLS enabled is present.

The *Services Detail Overview* page displays a *Security* icon on the graph edges where at least one request with mTLS enabled is present.  Also note that Kiali displays a lock icon in the *Network* section next to ports that are configured for mTLS.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
-service_mesh/v2x/ossm-security.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-vs-istio_{context}"]
= Configuring Role Based Access Control (RBAC)

Role-based access control (RBAC) objects determine whether a user or service is allowed to perform a given action within a project. You can define mesh-, namespace-, and workload-wide access control for your workloads in the mesh.

To configure RBAC, create an `AuthorizationPolicy` resource in the namespace for which you are configuring access. If you are configuring mesh-wide access, use the project where you installed the {SMProductShortName} control plane, for example `istio-system`.

For example, with RBAC, you can create policies that:

* Configure intra-project communication.
* Allow or deny full access to all workloads in the default namespace.
* Allow or deny ingress gateway access.
* Require a token for access.

An authorization policy includes a selector, an action, and a list of rules:

* The `selector` field specifies the target of the policy.
* The `action` field specifies whether to allow or deny the request.
* The `rules` field specifies when to trigger the action.
** The `from` field specifies constraints on the request origin.
** The `to` field specifies constraints on request target and parameters.
** The `when` field specifies additional conditions that to apply the rule.

.Procedure

. Create your `AuthorizationPolicy` resource. The following example shows a resource that updates the ingress-policy `AuthorizationPolicy` to deny an IP address from accessing the ingress gateway.
+
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: ingress-policy
  namespace: istio-system
spec:
  selector:
    matchLabels:
      app: istio-ingressgateway
  action: DENY
  rules:
  - from:
    - source:
      ipBlocks: ["1.2.3.4"]
----
+
. Run the following command after you write your resource to create your resource in your namespace. The namespace must match your `metadata.namespace` field in your `AuthorizationPolicy` resource.
+
[source,terminal]
----
$ oc create -n istio-system -f <filename>
----

.Next steps

Consider the following examples for other common configurations.

== Configure intra-project communication

You can use `AuthorizationPolicy` to configure your {SMProductShortName} control plane to allow or deny the traffic communicating with your mesh or services in your mesh.

=== Restrict access to services outside a namespace

You can deny requests from any source that is not in the `bookinfo` namespace with the following `AuthorizationPolicy` resource example.

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
 name: httpbin-deny
 namespace: bookinfo
spec:
 selector:
   matchLabels:
     app: httpbin
     version: v1
 action: DENY
 rules:
 - from:
   - source:
       notNamespaces: ["bookinfo"]
----

=== Creating allow-all and default deny-all authorization policies

The following example shows an allow-all authorization policy that allows full access to all workloads in the `bookinfo` namespace.

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: allow-all
  namespace: bookinfo
spec:
  action: ALLOW
  rules:
  - {}
----

The following example shows a policy that denies any access to all workloads in the `bookinfo` namespace.

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: deny-all
  namespace: bookinfo
spec:
  {}
----

== Allow or deny access to the ingress gateway

You can set an authorization policy to add allow or deny lists based on IP addresses.

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: ingress-policy
  namespace: istio-system
spec:
  selector:
    matchLabels:
      app: istio-ingressgateway
  action: ALLOW
  rules:
  - from:
    - source:
       ipBlocks: ["1.2.3.4", "5.6.7.0/24"]
----

== Restrict access with JSON Web Token

You can restrict what can access your mesh with a JSON Web Token (JWT). After authentication, a user or service can access routes, services that are associated with that token.

Create a `RequestAuthentication` resource, which defines the authentication methods that are supported by a workload. The following example accepts a JWT issued by `http://localhost:8080/auth/realms/master`.

[source,yaml]
----
apiVersion: "security.istio.io/v1beta1"
kind: "RequestAuthentication"
metadata:
  name: "jwt-example"
  namespace: bookinfo
spec:
  selector:
    matchLabels:
      app: httpbin
  jwtRules:
  - issuer: "http://localhost:8080/auth/realms/master"
    jwksUri: "http://keycloak.default.svc:8080/auth/realms/master/protocol/openid-connect/certs"
----

Then, create an `AuthorizationPolicy` resource in the same namespace to work with `RequestAuthentication` resource you created. The following example requires a JWT to be present in the `Authorization` header when sending a request to `httpbin` workloads.

[source,yaml]
----
apiVersion: "security.istio.io/v1beta1"
kind: "AuthorizationPolicy"
metadata:
  name: "frontend-ingress"
  namespace: bookinfo
spec:
  selector:
    matchLabels:
      app: httpbin
  action: DENY
  rules:
  - from:
    - source:
        notRequestPrincipals: ["*"]
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

[id="ossm-security-cipher_{context}"]
= Configuring cipher suites and ECDH curves

Cipher suites and Elliptic-curve Diffie–Hellman (ECDH curves) can help you secure your service mesh. You can define a comma separated list of cipher suites using `spec.security.controlplane.tls.cipherSuites` and ECDH curves using `spec.security.controlplane.tls.ecdhCurves` in your `ServiceMeshControlPlane` resource. If either of these attributes are empty, then the default values are used.

The `cipherSuites` setting is effective if your service mesh uses TLS 1.2 or earlier. It has no effect when negotiating with TLS 1.3.

Set your cipher suites in the comma separated list in order of priority. For example, `ecdhCurves: CurveP256, CurveP384` sets `CurveP256` as a higher priority than `CurveP384`.

[NOTE]
====
You must include either `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256` or  `TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256` when you configure the cipher suite. HTTP/2 support requires at least one of these cipher suites.

====

The supported cipher suites are:

* TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
* TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
* TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
* TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
* TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
* TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA
* TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
* TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA
* TLS_RSA_WITH_AES_128_GCM_SHA256
* TLS_RSA_WITH_AES_256_GCM_SHA384
* TLS_RSA_WITH_AES_128_CBC_SHA256
* TLS_RSA_WITH_AES_128_CBC_SHA
* TLS_RSA_WITH_AES_256_CBC_SHA
* TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA
* TLS_RSA_WITH_3DES_EDE_CBC_SHA

The supported ECDH Curves are:

* CurveP256
* CurveP384
* CurveP521
* X25519

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-configuring-jwks-resolver-ca_{context}"]
= Configuring JSON Web Key Sets resolver certificate authority

You can configure your own JSON Web Key Sets (JWKS) resolver certificate authority (CA) from the `ServiceMeshControlPlane` (SMCP) spec.

.Procedure

. Edit the `ServiceMeshControlPlane` spec file:
+
[source, yaml]
----
$ oc edit smcp <smcp-name>
----

. Enable `mtls` for the data plane by setting the value of the `mtls` field to `true` in the `ServiceMeshControlPlane` spec, as shown in the following example:
+
[source,yaml]
----
spec:
  security:
    dataPlane:
        mtls: true # enable mtls for data plane
    # JWKSResolver extra CA
    # PEM-encoded certificate content to trust an additional CA
    jwksResolverCA: |
        -----BEGIN CERTIFICATE-----
        [...]
        [...]
        -----END CERTIFICATE-----
...
----

. Save the changes. {product-title} automatically applies them.

A `ConfigMap` such as `pilot-jwks-cacerts-<SMCP name>` is created with the CA `.pem data`.

.Example ConfigMap `pilot-jwks-cacerts-<SMCP name>`
[source, yaml]
----
kind: ConfigMap
apiVersion: v1
data:
  extra.pem: |
      -----BEGIN CERTIFICATE-----
      [...]
      [...]
      -----END CERTIFICATE-----
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-cert-manage_{context}"]
= Adding an external certificate authority key and certificate

By default, {SMProductName} generates a self-signed root certificate and key and uses them to sign the workload certificates. You can also use the user-defined certificate and key to sign workload certificates with user-defined root certificate. This task demonstrates an example to plug certificates and key into {SMProductShortName}.

.Prerequisites

* Install {SMProductName} with mutual TLS enabled to configure certificates.
* This example uses the certificates from the link:https://github.com/maistra/istio/tree/maistra-{MaistraVersion}/samples/certs[Maistra repository]. For production, use your own certificates from your certificate authority.
* Deploy the Bookinfo sample application to verify the results with these instructions.
* OpenSSL is required to verify certificates.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-cert-manage-add-cert-key_{context}"]
== Adding an existing certificate and key

To use an existing signing (CA) certificate and key, you must create a chain of trust file that includes the CA certificate, key, and root certificate. You must use the following exact file names for each of the corresponding certificates. The CA certificate is named `ca-cert.pem`, the key is `ca-key.pem`, and the root certificate, which signs `ca-cert.pem`, is named `root-cert.pem`. If your workload uses intermediate certificates, you must specify them in a `cert-chain.pem` file.

. Save the example certificates from the link:https://github.com/maistra/istio/tree/maistra-{MaistraVersion}/samples/certs[Maistra repository] locally and replace `<path>` with the path to your certificates.

. Create a secret named `cacert` that includes the input files `ca-cert.pem`, `ca-key.pem`, `root-cert.pem` and `cert-chain.pem`.
+
[source,terminal]
----
$ oc create secret generic cacerts -n istio-system --from-file=<path>/ca-cert.pem \
    --from-file=<path>/ca-key.pem --from-file=<path>/root-cert.pem \
    --from-file=<path>/cert-chain.pem
----
+
. In the `ServiceMeshControlPlane` resource set `spec.security.dataPlane.mtls true` to `true` and configure the `certificateAuthority` field as shown in the following example. The default `rootCADir` is `/etc/cacerts`. You do not need to set the `privateKey` if the key and certs are mounted in the default location.  {SMProductShortName} reads the certificates and key from the secret-mount files.
+
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
spec:
  security:
    dataPlane:
      mtls: true
    certificateAuthority:
      type: Istiod
      istiod:
        type: PrivateKey
        privateKey:
          rootCADir: /etc/cacerts
----

. After creating/changing/deleting the `cacert` secret, the {SMProductShortName} control plane `istiod` and `gateway` pods must be restarted so the changes go into effect. Use the following command to restart the pods:
+
[source,terminal]
----
$ oc -n istio-system delete pods -l 'app in (istiod,istio-ingressgateway, istio-egressgateway)'
----
+
The Operator will automatically recreate the pods after they have been deleted.

. Restart the bookinfo application pods so that the sidecar proxies pick up the secret changes. Use the following command to restart the pods:
+
[source,terminal]
----
$ oc -n bookinfo delete pods --all
----
+
You should see output similar to the following:
+

[source,terminal]
----
pod "details-v1-6cd699df8c-j54nh" deleted
pod "productpage-v1-5ddcb4b84f-mtmf2" deleted
pod "ratings-v1-bdbcc68bc-kmng4" deleted
pod "reviews-v1-754ddd7b6f-lqhsv" deleted
pod "reviews-v2-675679877f-q67r2" deleted
pod "reviews-v3-79d7549c7-c2gjs" deleted
----

. Verify that the pods were created and are ready with the following command:
+

[source,terminal]
----
$ oc get pods -n bookinfo
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-cert-manage-verify-cert_{context}"]
= Verifying your certificates

Use the Bookinfo sample application to verify that the workload certificates are signed by the certificates that were plugged into the CA. This process requires that you have `openssl` installed on your machine.

. To extract certificates from bookinfo workloads use the following command:
+
[source,terminal]
----
$ sleep 60
$ oc -n bookinfo exec "$(oc -n bookinfo get pod -l app=productpage -o jsonpath={.items..metadata.name})" -c istio-proxy -- openssl s_client -showcerts -connect details:9080 > bookinfo-proxy-cert.txt
$ sed -n '/-----BEGIN CERTIFICATE-----/{:start /-----END CERTIFICATE-----/!{N;b start};/.*/p}' bookinfo-proxy-cert.txt > certs.pem
$ awk 'BEGIN {counter=0;} /BEGIN CERT/{counter++} { print > "proxy-cert-" counter ".pem"}' < certs.pem
----
+
After running the command, you should have three files in your working directory: `proxy-cert-1.pem`, `proxy-cert-2.pem` and `proxy-cert-3.pem`.

. Verify that the root certificate is the same as the one specified by the administrator. Replace `<path>` with the path to your certificates.
+
[source,terminal]
----
$ openssl x509 -in <path>/root-cert.pem -text -noout > /tmp/root-cert.crt.txt
----
+
Run the following syntax at the terminal window.
+
[source,terminal]
----
$ openssl x509 -in ./proxy-cert-3.pem -text -noout > /tmp/pod-root-cert.crt.txt
----
+
Compare the certificates by running the following syntax at the terminal window.
+
[source,terminal]
----
$ diff -s /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt
----
+
You should see the following result:
`Files /tmp/root-cert.crt.txt and /tmp/pod-root-cert.crt.txt are identical`


. Verify that the CA certificate is the same as the one specified by the administrator. Replace `<path>` with the path to your certificates.
+
[source,terminal]
----
$ openssl x509 -in <path>/ca-cert.pem -text -noout > /tmp/ca-cert.crt.txt
----
Run the following syntax at the terminal window.
+
[source,terminal]
----
$ openssl x509 -in ./proxy-cert-2.pem -text -noout > /tmp/pod-cert-chain-ca.crt.txt
----
Compare the certificates by running the following syntax at the terminal window.
+
[source,terminal]
----
$ diff -s /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt
----
You should see the following result:
`Files /tmp/ca-cert.crt.txt and /tmp/pod-cert-chain-ca.crt.txt are identical.`

. Verify the certificate chain from the root certificate to the workload certificate. Replace `<path>` with the path to your certificates.
+
[source,terminal]
----
$ openssl verify -CAfile <(cat <path>/ca-cert.pem <path>/root-cert.pem) ./proxy-cert-1.pem
----
You should see the following result:
`./proxy-cert-1.pem: OK`

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-cert-cleanup_{context}"]
== Removing the certificates

To remove the certificates you added, follow these steps.

. Remove the secret `cacerts`. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc delete secret cacerts -n istio-system
----
+
. Redeploy {SMProductShortName} with a self-signed root certificate in the `ServiceMeshControlPlane` resource.
+
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
spec:
  security:
    dataPlane:
      mtls: true
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-cert-manager-integration-istio_{context}"]
= About integrating Service Mesh with cert-manager and istio-csr

The cert-manager tool is a solution for X.509 certificate management on Kubernetes. It delivers a unified API to integrate applications with private or public key infrastructure (PKI), such as Vault, Google Cloud Certificate Authority Service, Let's Encrypt, and other providers.

The cert-manager tool ensures the certificates are valid and up-to-date by attempting to renew certificates at a configured time before they expire.

For Istio users, cert-manager also provides integration with `istio-csr`, which is a certificate authority (CA) server that handles certificate signing requests (CSR) from Istio proxies. The server then delegates signing to cert-manager, which forwards CSRs to the configured CA server.

[NOTE]
====
Red Hat provides support for integrating with `istio-csr` and cert-manager. Red Hat does not provide direct support for the `istio-csr` or the community cert-manager components. The use of community cert-manager shown here is for demonstration purposes only.
====

.Prerequisites
* One of these versions of cert-manager:
** {cert-manager-operator} 1.10 or later
** community cert-manager Operator 1.11 or later
** cert-manager 1.11 or later

* OpenShift Service Mesh Operator 2.4 or later
* `istio-csr` 0.6.0 or later

[NOTE]
====
To avoid creating config maps in all namespaces when the `istio-csr` server is installed with the `jetstack/cert-manager-istio-csr` Helm chart, use the following setting: `app.controller.configmapNamespaceSelector: "maistra.io/member-of: <istio-namespace>"` in the `istio-csr.yaml` file.
====



:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-security.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-cert-manager-installation_{context}"]
= Installing cert-manager

You can install the `cert-manager` tool to manage the lifecycle of TLS certificates and ensure that they are valid and up-to-date. If you are running Istio in your environment, you can also install the `istio-csr` certificate authority (CA) server, which handles certificate signing requests (CSR) from Istio proxies. The `istio-csr` CA delegates signing to the `cert-manager` tool, which delegates to the configured CA.

.Procedure

. Create the root cluster issuer:
+
[source,terminal]
----
$ oc apply -f cluster-issuer.yaml
----
+
[source,terminal]
----
$ oc apply -n istio-system -f istio-ca.yaml
----
+
.Example `cluster-issuer.yaml`
[source, yaml]
----
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: selfsigned-root-issuer
  namespace: cert-manager
spec:
  selfSigned: {}
---
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: root-ca
  namespace: cert-manager
spec:
  isCA: true
  duration: 21600h # 900d
  secretName: root-ca
  commonName: root-ca.my-company.net
  subject:
    organizations:
    - my-company.net
  issuerRef:
    name: selfsigned-root-issuer
    kind: Issuer
    group: cert-manager.io
---
apiVersion: cert-manager.io/v1
kind: ClusterIssuer
metadata:
  name: root-ca
spec:
  ca:
    secretName: root-ca
----
+
.Example `istio-ca.yaml`
[source, yaml]
----
apiVersion: cert-manager.io/v1
kind: Certificate
metadata:
  name: istio-ca
  namespace: istio-system
spec:
  isCA: true
  duration: 21600h
  secretName: istio-ca
  commonName: istio-ca.my-company.net
  subject:
    organizations:
    - my-company.net
  issuerRef:
    name: root-ca
    kind: ClusterIssuer
    group: cert-manager.io
---
apiVersion: cert-manager.io/v1
kind: Issuer
metadata:
  name: istio-ca
  namespace: istio-system
spec:
  ca:
    secretName: istio-ca
----
+
====
[NOTE]
The namespace of the `selfsigned-root-issuer` issuer and `root-ca` certificate is `cert-manager` because `root-ca` is a cluster issuer, so the cert-manager looks for a referenced secret in its own namespace. Its own namespace is `cert-manager` in the case of the {cert-manager-operator}.
====

. Install `istio-csr`:
+
[source,terminal]
----
$ helm install istio-csr jetstack/cert-manager-istio-csr \
    -n istio-system \
    -f deploy/examples/cert-manager/istio-csr/istio-csr.yaml
----
+
.Example `istio-csr.yaml`
[source, yaml]
----
replicaCount: 2

image:
  repository: quay.io/jetstack/cert-manager-istio-csr
  tag: v0.6.0
  pullSecretName: ""

app:
  certmanager:
    namespace: istio-system
    issuer:
      group: cert-manager.io
      kind: Issuer
      name: istio-ca

  controller:
    configmapNamespaceSelector: "maistra.io/member-of=istio-system"
    leaderElectionNamespace: istio-system

  istio:
    namespace: istio-system
    revisions: ["basic"]

  server:
    maxCertificateDuration: 5m

  tls:
    certificateDNSNames:
    # This DNS name must be set in the SMCP spec.security.certificateAuthority.cert-manager.address
    - cert-manager-istio-csr.istio-system.svc
----

. Deploy SMCP:
+
[source,terminal]
----
$ oc apply -f mesh.yaml -n istio-system
----
+
.Example `mesh.yaml`
[source, yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  addons:
    grafana:
      enabled: false
    kiali:
      enabled: false
    prometheus:
      enabled: false
  proxy:
    accessLogging:
      file:
        name: /dev/stdout
  security:
    certificateAuthority:
      cert-manager:
        address: cert-manager-istio-csr.istio-system.svc:443
      type: cert-manager
    dataPlane:
      mtls: true
    identity:
      type: ThirdParty
  tracing:
    type: None
---
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - httpbin
  - sleep
----

====
[NOTE]
`security.identity.type: ThirdParty` must be set when `security.certificateAuthority.type: cert-manager` is configured.
====

.Verification

Use the sample `httpbin` service and `sleep` app to check mTLS traffic from ingress gateways and verify that the `cert-manager` tool is installed.

. Deploy the HTTP and `sleep` apps:
+
[source,terminal]
----
$ oc new-project <namespace>
----
+
[source,terminal]
----
$ oc apply -f https://raw.githubusercontent.com/maistra/istio/maistra-2.4/samples/httpbin/httpbin.yaml
----
+
[source,terminal]
----
$ oc apply -f https://raw.githubusercontent.com/maistra/istio/maistra-2.4/samples/sleep/sleep.yaml
----

. Verify that `sleep` can access the `httpbin` service:
+
[source,terminal]
----
$ oc exec "$(oc get pod -l app=sleep -n <namespace> \
   -o jsonpath={.items..metadata.name})" -c sleep -n <namespace> -- \
   curl http://httpbin.<namespace>:8000/ip -s -o /dev/null \
   -w "%{http_code}\n"
----
+
.Example output:
[source,terminal]
----
200
----

. Check mTLS traffic from the ingress gateway to the `httpbin` service:
+
[source,terminal]
----
$ oc apply -n <namespace> -f https://raw.githubusercontent.com/maistra/istio/maistra-2.4/samples/httpbin/httpbin-gateway.yaml
----

. Get the `istio-ingressgateway` route:
+
[source,terminal]
----
INGRESS_HOST=$(oc -n istio-system get routes istio-ingressgateway -o jsonpath='{.spec.host}')
----

. Verify mTLS traffic from the ingress gateway to the `httpbin` service:
+
[source,terminal]
----
$ curl -s -I http://$INGRESS_HOST/headers -o /dev/null -w "%{http_code}" -s
----


:leveloffset: 2

[role="_additional-resources"]
[id="additional-resources_cert-manager-operator-red-hat-openshift"]
== Additional resources

For information about how to install the cert-manager Operator for {product-title}, see:
link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#[Installing the cert-manager Operator for Red Hat OpenShift].

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-routing-traffic"]
= Managing traffic in your service mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: traffic-management

toc::[]

Using {SMProductName}, you can control the flow of traffic and API calls between services. Some services in your service mesh might need to communicate within the mesh and others might need to be hidden. You can manage the traffic to hide specific backend services, expose services, create testing or versioning deployments, or add a security layer on a set of services.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-gateways_{context}"]
= Using gateways

You can use a gateway to manage inbound and outbound traffic for your mesh to specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads.

Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, {SMProductName} gateways use the full power and flexibility of traffic routing.

The {SMProductName} gateway resource can use layer 4-6 load balancing properties, such as ports, to expose and configure {SMProductName} TLS settings. Instead of adding application-layer traffic routing (L7) to the same API resource, you can bind a regular {SMProductName} virtual service to the gateway and manage gateway traffic like any other data plane traffic in a service mesh.

Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh. This enables you to limit which services have access to external networks, which adds security control to your service mesh. You can also use a gateway to configure a purely internal proxy.

.Gateway example

A gateway resource describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections. The specification describes a set of ports that should be exposed, the type of protocol to use, SNI configuration for the load balancer, and so on.

The following example shows a sample gateway configuration for external HTTPS ingress traffic:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: ext-host-gwy
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - ext-host.example.com
    tls:
      mode: SIMPLE
      serverCertificate: /tmp/tls.crt
      privateKey: /tmp/tls.key
----

This gateway configuration lets HTTPS traffic from `ext-host.example.com` into the mesh on port 443, but doesn’t specify any routing for the traffic.

To specify routing and for the gateway to work as intended, you must also bind the gateway to a virtual service. You do this using the virtual service's gateways field, as shown in the following example:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: virtual-svc
spec:
  hosts:
  - ext-host.example.com
  gateways:
    - ext-host-gwy
----

You can then configure the virtual service with routing rules for the external traffic.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-automatic-gateway-injection_{context}"]
= Enabling gateway injection

Gateway configurations apply to standalone Envoy proxies running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads. Because gateways are Envoy proxies, you can configure {SMProductShortName} to inject gateways automatically, similar to how you can inject sidecars.

Using automatic injection for gateways, you can deploy and manage gateways independent from the `ServiceMeshControlPlane` resource and manage the gateways with your user applications. Using auto-injection for gateway deployments gives developers full control over the gateway deployment while simplifying operations. When a new upgrade is available, or a configuration has changed, you restart the gateway pods to update them. Doing so makes the experience of operating a gateway deployment the same as operating sidecars.

[NOTE]
====
Injection is disabled by default for the `ServiceMeshControlPlane` namespace, for example the `istio-system` namespace. As a security best practice, deploy gateways in a different namespace from the control plane.
====

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-deploying-automatic-gateway-injection_{context}"]
= Deploying automatic gateway injection

When deploying a gateway, you must opt-in to injection by adding an injection label or annotation to the gateway `deployment` object. The following example deploys a gateway.

.Prerequisites

* The namespace must be a member of the mesh by defining it in the `ServiceMeshMemberRoll` or by creating a `ServiceMeshMember` resource.

.Procedure

. Set a unique label for the Istio ingress gateway. This setting is required to ensure that the gateway can select the workload. This example uses `ingressgateway` as the name of the gateway.
+
[source,yaml]
----
apiVersion: v1
kind: Service
metadata:
  name: istio-ingressgateway
  namespace: istio-ingress
spec:
  type: ClusterIP
  selector:
    istio: ingressgateway
  ports:
  - name: http2
    port: 80
    targetPort: 8080
  - name: https
    port: 443
    targetPort: 8443
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: istio-ingressgateway
  namespace: istio-ingress
spec:
  selector:
    matchLabels:
      istio: ingressgateway
  template:
    metadata:
      annotations:
        inject.istio.io/templates: gateway
      labels:
        istio: ingressgateway
        sidecar.istio.io/inject: "true" <1>
    spec:
      containers:
      - name: istio-proxy
        image: auto <2>
----
<1> Enable gateway injection by setting the `sidecar.istio.io/inject` field to `"true"`.
<2> Set the `image` field to `auto` so that the image automatically updates each time the pod starts.

. Set up roles to allow reading credentials for TLS.
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: istio-ingressgateway-sds
  namespace: istio-ingress
rules:
  - apiGroups: [""]
    resources: ["secrets"]
    verbs: ["get", "watch", "list"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: istio-ingressgateway-sds
  namespace: istio-ingress
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: istio-ingressgateway-sds
subjects:
- kind: ServiceAccount
  name: default
----

. Grant access to the new gateway from outside the cluster, which is required whenever `spec.security.manageNetworkPolicy` is set to `true`.
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: gatewayingress
  namespace: istio-ingress
spec:
  podSelector:
    matchLabels:
      istio: ingressgateway
  ingress:
    - {}
  policyTypes:
  - Ingress
----

. Automatically scale the pod when ingress traffic increases. This example sets the minimum replicas to `2` and the maximum replicas to `5`. It also creates another replica when utilization reaches 80%.
+
[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  labels:
    istio: ingressgateway
    release: istio
  name: ingressgatewayhpa
  namespace: istio-ingress
spec:
  maxReplicas: 5
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 80
        type: Utilization
    type: Resource
  minReplicas: 2
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: istio-ingressgateway
----

. Specify the minimum number of pods that must be running on the node. This example ensures one replica is running if a pod gets restarted on a new node.
+
[source,yaml]
----
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  labels:
    istio: ingressgateway
    release: istio
  name: ingressgatewaypdb
  namespace: istio-ingress
spec:
  minAvailable: 1
  selector:
    matchLabels:
      istio: ingressgateway
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-ingress_{context}"]
= Managing ingress traffic

In {SMProductName}, the Ingress Gateway enables features such as monitoring, security, and route rules to apply to traffic that enters the cluster. Use a {SMProductShortName} gateway to expose a service outside of the service mesh.

[id="ossm-routing-determine-ingress_{context}"]
== Determining the ingress IP and ports

Ingress configuration differs depending on if your environment supports an external load balancer. An external load balancer is set in the ingress IP and ports for the cluster. To determine if your cluster's IP and ports are configured for external load balancers, run the following command. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

[source,terminal]
----
$ oc get svc istio-ingressgateway -n istio-system
----

That command returns the `NAME`, `TYPE`, `CLUSTER-IP`, `EXTERNAL-IP`, `PORT(S)`, and `AGE` of each item in your namespace.

If the `EXTERNAL-IP` value is set, your environment has an external load balancer that you can use for the ingress gateway.

If the `EXTERNAL-IP` value is `<none>`, or perpetually `<pending>`, your environment does not provide an external load balancer for the ingress gateway.
You can access the gateway using the service's link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#[node port].

////
TO DO - remove XREF in this module.
Determine the ingress according to your environment. For an environment with load balancer support, xref:ossm-routing-config-ig-lb_routing-traffic[Determining ingress ports with a load balancer]. For an environment without load balancer support, xref:ossm-routing-config-ig-no-lb_routing-traffic[Determining ingress ports without a load balancer]. After you have determined the ingress ports, see xref:ossm-routing-gateways_routing-traffic[Configuring ingress using a gateway] to complete your configuration.
////

[id="ossm-routing-config-ig-lb_{context}"]
=== Determining ingress ports with a load balancer

Follow these instructions if your environment has an external load balancer.

.Procedure

. Run the following command to set the ingress IP and ports. This command sets a variable in your terminal.
+
[source,terminal]
----
$ export INGRESS_HOST=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
----

. Run the following command to set the ingress port.
+
[source,terminal]
----
$ export INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
----

. Run the following command to set the secure ingress port.
+
[source,terminal]
----
$ export SECURE_INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
----

. Run the following command to set the TCP ingress port.
+
[source,terminal]
----
$ export TCP_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="tcp")].port}')
----

[NOTE]
====
In some environments, the load balancer may be exposed using a hostname instead of an IP address. For that case, the ingress gateway's `EXTERNAL-IP` value is not an IP address. Instead, it's a hostname, and the previous command fails to set the `INGRESS_HOST` environment variable.

In that case, use the following command to correct the `INGRESS_HOST` value:
====

[source,terminal]
----
$ export INGRESS_HOST=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
----

[id="ossm-routing-config-ig-no-lb_{context}"]
=== Determining ingress ports without a load balancer

If your environment does not have an external load balancer, determine the ingress ports and use a node port instead.

.Procedure

. Set the ingress ports.
+
[source,terminal]
----
$ export INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
----

. Run the following command to set the secure ingress port.
+
[source,terminal]
----
$ export SECURE_INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
----

. Run the following command to set the TCP ingress port.
+
[source,terminal]
----
$ export TCP_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="tcp")].nodePort}')
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-ingress-gateway_{context}"]
= Configuring an ingress gateway

An ingress gateway is a load balancer operating at the edge of the mesh that receives incoming HTTP/TCP connections. It configures exposed ports and protocols but does not include any traffic routing configuration. Traffic routing for ingress traffic is instead configured with routing rules, the same way as for internal service requests.

The following steps show how to create a gateway and configure a `VirtualService` to expose a service in the Bookinfo sample application to outside traffic for paths `/productpage` and `/login`.

.Procedure

. Create a gateway to accept traffic.
+
.. Create a YAML file, and copy the following YAML into it.
+
.Gateway example gateway.yaml
[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
----
+
.. Apply the YAML file.
+
[source,terminal]
----
$ oc apply -f gateway.yaml
----

. Create a `VirtualService` object to rewrite the host header.
+
.. Create a YAML file, and copy the following YAML into it.
+
.Virtual service example
[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: bookinfo
spec:
  hosts:
  - "*"
  gateways:
  - bookinfo-gateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /logout
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
----
+
.. Apply the YAML file.
+
[source,terminal]
----
$ oc apply -f vs.yaml
----

. Test that the gateway and VirtualService have been set correctly.
+
.. Set the Gateway URL.
+
[source,terminal]
----
export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')
----
+
.. Set the port number. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
export TARGET_PORT=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.port.targetPort}')
----
+
.. Test a page that has been explicitly exposed.
+
[source,terminal]
----
curl -s -I "$GATEWAY_URL/productpage"
----
+
The expected result is `200`.

:leveloffset: 2

[id="ossm-auto-route_{context}"]
== Understanding automatic routes

OpenShift routes for gateways are automatically managed in {SMProductShortName}. Every time an Istio Gateway is created, updated or deleted inside the service mesh, an OpenShift route is created, updated or deleted.

[id="ossm-auto-route-subdomains_{context}"]
=== Routes with subdomains

{SMProductName} creates the route with the subdomain, but {product-title} must be configured to enable it. Subdomains, for example `*.domain.com`, are supported, but not by default. Configure an {product-title} wildcard policy before configuring a wildcard host gateway.

For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#using-wildcard-routes_configuring-ingress[Using wildcard routes].

:leveloffset: +2

// Module is included in the following assemblies:
// * service_mesh/v2x/ossm-traffic-manage.adoc
//

:_mod-docs-content-type: PROCEDURE
[id="ossm-auto-route-create-subdomains_{context}"]
= Creating subdomain routes

The following example creates a gateway in the Bookinfo sample application, which creates subdomain routes.

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: gateway1
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - www.bookinfo.com
    - bookinfo.example.com
----

The `Gateway` resource creates the following OpenShift routes. You can check that the routes are created by using the following command. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

[source,terminal]
----
$ oc -n istio-system get routes
----

.Expected output
[source,terminal]
----
NAME           HOST/PORT             PATH  SERVICES               PORT  TERMINATION   WILDCARD
gateway1-lvlfn bookinfo.example.com        istio-ingressgateway   <all>               None
gateway1-scqhv www.bookinfo.com            istio-ingressgateway   <all>               None
----

If you delete the gateway, {SMProductName} deletes the routes. However, routes you have manually created are never modified by {SMProductName}.

:leveloffset: 2

:leveloffset: +2

// Module is included in the following assemblies:
// * service_mesh/v2x/ossm-traffic-manage.adoc
//

[id="ossm-auto-route-annotations_{context}"]
= Route labels and annotations

Sometimes specific labels or annotations are needed in an OpenShift route.
For example, some advanced features in OpenShift routes are managed using special annotations. See "Route-specific annotations" in the following "Additional resources" section.

For this and other use cases, {SMProductName} will copy all labels and annotations present in the Istio gateway resource (with the exception of annotations starting with `kubectl.kubernetes.io`) into the managed OpenShift route resource.

If you need specific labels or annotations in the OpenShift routes created by {SMProductShortName}, create them in the Istio gateway resource and they will be copied into the OpenShift route resources managed by the {SMProductShortName}.

:leveloffset: 2

.Additional resources
* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#nw-route-specific-annotations_route-configuration[Route-specific annotations].

:leveloffset: +2

// Module is included in the following assemblies:
// * service_mesh/v2x/ossm-traffic-manage.adoc
//
:_mod-docs-content-type: REFERENCE
[id="ossm-auto-route-enable_{context}"]
= Disabling automatic route creation

By default, the `ServiceMeshControlPlane` resource automatically synchronizes the Istio gateway resources with OpenShift routes. Disabling the automatic route creation allows you more flexibility to control routes if you have a special case or prefer to control routes manually.

[id="disabling-automatic-route-creation-specific-cases_{context}"]
== Disabling automatic route creation for specific cases

If you want to disable the automatic management of OpenShift routes for a specific Istio gateway, you must add the annotation `maistra.io/manageRoute: false` to the gateway metadata definition. {SMProductName} will ignore Istio gateways with this annotation, while keeping the automatic management of the other Istio gateways.

[id="disabling-automatic-route-creation-all-cases_{context}"]
== Disabling automatic route creation for all cases

You can disable the automatic management of OpenShift routes for all gateways in your mesh.

Disable integration between Istio gateways and OpenShift routes by setting the `ServiceMeshControlPlane` field `gateways.openshiftRoute.enabled` to `false`. For example, see the following resource snippet.

[source,yaml]
----
apiVersion: maistra.io/v1alpha1
kind: ServiceMeshControlPlane
metadata:
  namespace: istio-system
spec:
  gateways:
    openshiftRoute:
      enabled: false
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-routing-service-entries_{context}"]
= Understanding service entries

A service entry adds an entry to the service registry that {SMProductName} maintains internally. After you add the service entry, the Envoy proxies send traffic to the service as if it is a service in your mesh. Service entries allow you to do the following:

* Manage traffic for services that run outside of the service mesh.
* Redirect and forward traffic for external destinations (such as, APIs consumed from the web) or traffic to services in legacy infrastructure.
* Define retry, timeout, and fault injection policies for external destinations.
* Run a mesh service in a Virtual Machine (VM) by adding VMs to your mesh.

[NOTE]
====
Add services from a different cluster to the mesh to configure a multicluster {SMProductName} mesh on Kubernetes.
====

.Service entry examples
The following example is a mesh-external service entry that adds the `ext-resource` external dependency to the {SMProductName} service registry:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: svc-entry
spec:
  hosts:
  - ext-svc.example.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
----

Specify the external resource using the `hosts` field. You can qualify it fully or use a wildcard prefixed domain name.

You can configure virtual services and destination rules to control traffic to a service entry in the same way you configure traffic for any other service in the mesh. For example, the following destination rule configures the traffic route to use mutual TLS to secure the connection to the `ext-svc.example.com` external service that is configured using the service entry:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: ext-res-dr
spec:
  host: ext-svc.example.com
  trafficPolicy:
    tls:
      mode: MUTUAL
      clientCertificate: /etc/certs/myclientcert.pem
      privateKey: /etc/certs/client_private_key.pem
      caCertificates: /etc/certs/rootcacerts.pem
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE

[id="ossm-routing-virtual-services_{context}"]
= Using VirtualServices

You can route requests dynamically to multiple versions of a microservice through {SMProductName} with a virtual service. With virtual services, you can:

* Address multiple application services through a single virtual service. If your mesh uses Kubernetes, for example, you can configure a virtual service to handle all services in a specific namespace. A virtual service enables you to turn a monolithic application into a service consisting of distinct microservices with a seamless consumer experience.
* Configure traffic rules in combination with gateways to control ingress and egress traffic.

[id="ossm-routing-vs_{context}"]
== Configuring VirtualServices

Requests are routed to services within a service mesh with virtual services. Each virtual service consists of a set of routing rules that are evaluated in order. {SMProductName} matches each given request to the virtual service to a specific real destination within the mesh.

Without virtual services, {SMProductName} distributes traffic using least requests load balancing between all service instances. With a virtual service, you can specify traffic behavior for one or more hostnames. Routing rules in the virtual service tell {SMProductName} how to send the traffic for the virtual service to appropriate destinations. Route destinations can be versions of the same service or entirely different services.

.Procedure

. Create a YAML file using the following example to route requests to different versions of the Bookinfo sample application service depending on which user connects to the application.
+
.Example VirtualService.yaml
[source,YAML]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  - route:
    - destination:
        host: reviews
        subset: v3
----

. Run the following command to apply `VirtualService.yaml`, where `VirtualService.yaml` is the path to the file.
+
[source,terminal]
----
$ oc apply -f <VirtualService.yaml>
----

== VirtualService configuration reference

//Need a sentence or two here

[options="header"]
[cols="l, a"]
|===
|Parameter |Description
|spec:
  hosts:
|The `hosts` field lists the virtual service's destination address to which the routing rules apply. This is the address(es) that are used to send requests to the service. The virtual service hostname can be an IP address, a DNS name, or a short name that resolves to a fully qualified domain name.

|spec:
  http:
  - match:
|The `http` section contains the virtual service's routing rules which describe match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent to the destination as specified in the hosts field. A routing rule consists of the destination where you want the traffic to go and any specified match conditions.
The first routing rule in the example has a condition that begins with the match field. In this example, this routing applies to all requests from the user `jason`. Add the `headers`, `end-user`, and `exact` fields to select the appropriate requests.

|spec:
  http:
  - match:
    - destination:
|The `destination` field in the route section specifies the actual destination for traffic that matches this condition. Unlike the virtual service's host, the destination's host must be a real destination that exists in the {SMProductName} service registry. This can be a mesh service with proxies or a non-mesh service added using a service entry. In this example, the hostname is a Kubernetes service name:
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-routing-destination-rules_{context}"]
= Understanding destination rules

Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic's real destination. Virtual services route traffic to a destination. Destination rules configure what happens to traffic at that destination.

By default, {SMProductName} uses a least requests load balancing policy, where the service instance in the pool with the least number of active connections receives the request. {SMProductName} also supports the following models, which you can specify in destination rules for requests to a particular service or service subset.

* Random: Requests are forwarded at random to instances in the pool.
* Weighted: Requests are forwarded to instances in the pool according to a specific percentage.
* Least requests: Requests are forwarded to instances with the least number of requests.

.Destination rule example

The following example destination rule configures three different subsets for the `my-svc` destination service, with different load balancing policies:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: my-destination-rule
spec:
  host: my-svc
  trafficPolicy:
    loadBalancer:
      simple: RANDOM
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
  - name: v3
    labels:
      version: v3
----

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
-service_mesh/v2x/ossm-traffic-manage.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-understanding-networkpolicy_{context}"]
= Understanding network policies

{SMProductName} automatically creates and manages a number of `NetworkPolicies` resources in the {SMProductShortName} control plane and application namespaces. This is to ensure that applications and the control plane can communicate with each other.

For example, if you have configured your {product-title} cluster to use the SDN plugin, {SMProductName} creates a `NetworkPolicy` resource in each member project. This enables ingress to all pods in the mesh from the other mesh members and the control plane. This also restricts ingress to only member projects. If you require ingress from non-member projects, you need to create a `NetworkPolicy` to allow that traffic through. If you remove a namespace from {SMProductShortName}, this `NetworkPolicy` resource is deleted from the project.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
-service_mesh/v2x/ossm-traffic-manage.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-config-disable-networkpolicy_{context}"]
= Disabling automatic NetworkPolicy creation

If you want to disable the automatic creation and management of `NetworkPolicy` resources, for example to enforce company security policies, or to allow direct access to pods in the mesh, you can do so. You can edit the `ServiceMeshControlPlane` and set `spec.security.manageNetworkPolicy` to `false`.

[NOTE]
====
When you disable `spec.security.manageNetworkPolicy` {SMProductName} will not create *any* `NetworkPolicy` objects. The system administrator is responsible for managing the network and fixing any issues this might cause.
====

.Prerequisites

* {SMProductName} Operator version 2.1.1 or higher installed.
* `ServiceMeshControlPlane` resource updated to version 2.1 or higher.

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Select the project where you installed the {SMProductShortName} control plane, for example `istio-system`, from the *Project* menu.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane`, for example `basic-install`.

. On the *Create ServiceMeshControlPlane Details* page, click `YAML` to modify your configuration.

. Set the `ServiceMeshControlPlane` field `spec.security.manageNetworkPolicy` to `false`, as shown in this example.
+
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
spec:
  security:
      manageNetworkPolicy: false
----
+
. Click *Save*.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc
:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-sidecar_{context}"]
= Configuring sidecars for traffic management

By default, {SMProductName} configures every Envoy proxy to accept traffic on all the ports of its associated workload, and to reach every workload in the mesh when forwarding traffic. You can use a sidecar configuration to do the following:

* Fine-tune the set of ports and protocols that an Envoy proxy accepts.
* Limit the set of services that the Envoy proxy can reach.

[NOTE]
====
To optimize performance of your service mesh, consider limiting Envoy proxy configurations.
====

In the Bookinfo sample application, configure a Sidecar so all services can reach other services running in the same namespace and control plane. This Sidecar configuration is required for using {SMProductName} policy and telemetry features.

.Procedure

. Create a YAML file using the following example to specify that you want a sidecar configuration to apply to all workloads in a particular namespace. Otherwise, choose specific workloads using a `workloadSelector`.
+
.Example sidecar.yaml
[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Sidecar
metadata:
  name: default
  namespace: bookinfo
spec:
  egress:
  - hosts:
    - "./*"
    - "istio-system/*"
----

. Run the following command to apply `sidecar.yaml`, where `sidecar.yaml` is the path to the file.
+
[source,terminal]
----
$ oc apply -f sidecar.yaml
----

. Run the following command to verify that the sidecar was created successfully.
+
[source,terminal]
----
$ oc get sidecar
----

:leveloffset: 2

== Routing Tutorial

This guide references the Bookinfo sample application to provide examples of routing in an example application. Install the xref:ossm-tutorial-bookinfo-overview_ossm-create-mesh[Bookinfo application] to learn how these routing examples work.

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

[id="ossm-routing-bookinfo_{context}"]
= Bookinfo routing tutorial

The {SMProductShortName} Bookinfo sample application consists of four separate microservices, each with multiple versions. After installing the Bookinfo sample application, three different versions of the `reviews` microservice run concurrently.

When you access the Bookinfo app `/product` page in a browser and refresh several times, sometimes the book review output contains star ratings and other times it does not. Without an explicit default service version to route to, {SMProductShortName} routes requests to all available versions one after the other.

This tutorial helps you apply rules that route all traffic to `v1` (version 1) of the microservices. Later, you can apply a rule to route traffic based on the value of an HTTP request header.

.Prerequisites:

* Deploy the Bookinfo sample application to work with the following examples.

:leveloffset: 2

:leveloffset: +2

:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-bookinfo-applying_{context}"]
= Applying a virtual service

In the following procedure, the virtual service routes all traffic to `v1` of each micro-service by applying virtual services that set the default version for the micro-services.

.Procedure

. Apply the virtual services.
+
[source,bash,subs="attributes"]
----
$ oc apply -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/virtual-service-all-v1.yaml
----

. To verify that you applied the virtual services, display the defined routes with the following command:
+
[source,terminal]
----
$ oc get virtualservices -o yaml
----
+
That command returns a resource of `kind: VirtualService` in YAML format.

You have configured {SMProductShortName} to route to the `v1` version of the Bookinfo microservices including the `reviews` service version 1.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc
:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-bookinfo-test_{context}"]
= Testing the new route configuration

Test the new configuration by refreshing the `/productpage` of the Bookinfo application.

.Procedure

. Set the value for the `GATEWAY_URL` parameter. You can use this variable to find the URL for your Bookinfo product page later. In this example, istio-system is the name of the control plane project.
+
[source,terminal]
----
export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')
----

. Run the following command to retrieve the URL for the product page.
+
[source,terminal]
----
echo "http://$GATEWAY_URL/productpage"
----

. Open the Bookinfo site in your browser.

The reviews part of the page displays with no rating stars, no matter how many times you refresh. This is because you configured {SMProductShortName} to route all traffic for the reviews service to the version `reviews:v1` and this version of the service does not access the star ratings service.

Your service mesh now routes traffic to one version of a service.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc
:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-bookinfo-route_{context}"]
= Route based on user identity

Change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named `jason` will be routed to the service `reviews:v2`.

{SMProductShortName} does not have any special, built-in understanding of user identity. This example is enabled by the fact that the `productpage` service adds a custom `end-user` header to all outbound HTTP requests to the reviews service.

.Procedure

. Run the following command to enable user-based routing in the Bookinfo sample application.
+
[source,bash,subs="attributes"]
----
$ oc apply -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
----

. Run the following command to confirm the rule is created. This command returns all resources of `kind: VirtualService` in YAML format.
+
[source,terminal]
----
$ oc get virtualservice reviews -o yaml
----

. On the `/productpage` of the Bookinfo app, log in as user `jason` with no password.
+
. Refresh the browser. The star ratings appear next to each review.

. Log in as another user (pick any name you want). Refresh the browser. Now the stars are gone. Traffic is now routed to `reviews:v1` for all users except Jason.

You have successfully configured the Bookinfo sample application to route traffic based on user identity.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-observability"]
= Metrics, logs, and traces
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: observability

toc::[]

Once you have added your application to the mesh, you can observe the data flow through your application. If you do not have your own application installed, you can see how observability works in {SMProductName} by installing the xref:ossm-tutorial-bookinfo-overview_ossm-create-mesh[Bookinfo sample application].

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-observability-addresses_{context}"]
= Discovering console addresses

{SMProductName} provides the following consoles to view your service mesh data:

* *Kiali console* - Kiali is the management console for {SMProductName}.
* *Jaeger console* - Jaeger is the management console for {DTProductName}.
* *Grafana console* - Grafana provides mesh administrators with advanced query and metrics analysis and dashboards for Istio data. Optionally, Grafana can be used to analyze service mesh metrics.
* *Prometheus console* - {SMProductName} uses Prometheus to store telemetry information from services.

When you install the {SMProductShortName} control plane, it automatically generates routes for each of the installed components. Once you have the route address, you can access the Kiali, Jaeger, Prometheus, or Grafana console to view and manage your service mesh data.

.Prerequisite

* The component must be enabled and installed.  For example, if you did not install distributed tracing, you will not be able to access the Jaeger console.

.Procedure from OpenShift console

. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Networking* -> *Routes*.

. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+
. If necessary, use the filter to find the component console whose route you want to access.  Click the route *Location* to launch the console.

. Click *Log In With OpenShift*.

.Procedure from the CLI
. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. Switch to the {SMProductShortName} control plane project. In this example, `istio-system` is the {SMProductShortName} control plane project.  Run the following command:
+
[source,terminal]
----
$ oc project istio-system
----
+
. To get the routes for the various {SMProductName} consoles, run the folowing command:
+
[source,terminal]
----
$ oc get routes
----
+
This command returns the URLs for the Kiali, Jaeger, Prometheus, and Grafana web consoles, and any other routes in your service mesh. You should see output similar to the following:
+

[source,terminal]
----
NAME                    HOST/PORT                         SERVICES              PORT    TERMINATION
bookinfo-gateway        bookinfo-gateway-yourcompany.com  istio-ingressgateway          http2
grafana                 grafana-yourcompany.com           grafana               <all>   reencrypt/Redirect
istio-ingressgateway    istio-ingress-yourcompany.com     istio-ingressgateway  8080
jaeger                  jaeger-yourcompany.com            jaeger-query          <all>   reencrypt
kiali                   kiali-yourcompany.com             kiali                 20001   reencrypt/Redirect
prometheus              prometheus-yourcompany.com        prometheus            <all>   reencrypt/Redirect
----

. Copy the URL for the console you want to access from the `HOST/PORT` column into a browser to open the console.

. Click *Log In With OpenShift*.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
* service_mesh/v2x/ossm-troubleshooting-istio.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-accessing-kiali-console_{context}"]
= Accessing the Kiali console

You can view your application's topology, health, and metrics in the Kiali console. If your service is experiencing problems, the Kiali console lets you view the data flow through your service. You can view insights about the mesh components at different levels, including abstract applications, services, and workloads. Kiali also provides an interactive graph view of your namespace in real time.

To access the Kiali console you must have {SMProductName} installed, Kiali installed and configured.

The installation process creates a route to access the Kiali console.

If you know the URL for the Kiali console, you can access it directly.  If you do not know the URL, use the following directions.

.Procedure for administrators

. Log in to the {product-title} web console with an administrator role.

. Click *Home* -> *Projects*.

. On the *Projects* page, if necessary, use the filter to find the name of your project.

. Click the name of your project, for example, `bookinfo`.

. On the *Project details* page, in the *Launcher* section, click the *Kiali* link.

. Log in to the Kiali console with the same user name and password that you use to access the {product-title} console.
+
When you first log in to the Kiali Console, you see the *Overview* page which displays all the namespaces in your service mesh that you have permission to view.
+
If you are validating the console installation and namespaces have not yet been added to the mesh, there might not be any data to display other than `istio-system`.

.Procedure for developers

. Log in to the {product-title} web console with a developer role.

. Click *Project*.

. On the *Project Details* page, if necessary, use the filter to find the name of your project.

. Click the name of your project, for example, `bookinfo`.

. On the *Project* page, in the *Launcher* section, click the *Kiali* link.

. Click *Log In With OpenShift*.

:leveloffset: 2

:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v1x/ossm-observability.adoc
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-observability-visual_{context}"]
= Viewing service mesh data in the Kiali console

The Kiali Graph offers a powerful visualization of your mesh traffic. The topology combines real-time request traffic with your Istio configuration information to present immediate insight into the behavior of your service mesh, letting you quickly pinpoint issues. Multiple Graph Types let you visualize traffic as a high-level service topology, a low-level workload topology, or as an application-level topology.

There are several graphs to choose from:

* The *App graph* shows an aggregate workload for all applications that are labeled the same.

* The *Service graph* shows a node for each service in your mesh but excludes all applications and workloads from the graph. It provides a high level view and aggregates all traffic for defined services.

* The *Versioned App graph* shows a node for each version of an application. All versions of an application are grouped together.

* The *Workload graph* shows a node for each workload in your service mesh. This graph does not require you to use the application and version labels. If your application does not use version labels, use this the graph.

Graph nodes are decorated with a variety of information, pointing out various route routing options like virtual services and service entries, as well as special configuration like fault-injection and circuit breakers. It can identify mTLS issues, latency issues, error traffic and more. The Graph is highly configurable, can show traffic animation, and has powerful Find and Hide abilities.

Click the *Legend* button to view information about the shapes, colors, arrows, and badges displayed in the graph.

To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel.

[id="ossm-observability-topology_{context}"]
== Changing graph layouts in Kiali

The layout for the Kiali graph can render differently depending on your application architecture and the data to display. For example, the number of graph nodes and their interactions can determine how the Kiali graph is rendered. Because it is not possible to create a single layout that renders nicely for every situation, Kiali offers a choice of several different layouts.

.Prerequisites

*  If you do not have your own application installed, install the Bookinfo sample application.  Then generate traffic for the Bookinfo application by entering the following command several times.
+
[source,terminal]
----
$ curl "http://$GATEWAY_URL/productpage"
----
+
This command simulates a user visiting the `productpage` microservice of the application.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.

. In Kiali console, click *Graph* to view a namespace graph.

. From the *Namespace* menu, select your application namespace, for example, `bookinfo`.

. To choose a different graph layout, do either or both of the following:

* Select different graph data groupings from the menu at the top of the graph.

** App graph
** Service graph
** Versioned App graph (default)
** Workload graph

* Select a different graph layout from the Legend at the bottom of the graph.
** Layout default dagre
** Layout 1 cose-bilkent
** Layout 2 cola

:leveloffset: 2

:leveloffset: +2

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-viewing-logs_{context}"]
= Viewing logs in the Kiali console

You can view logs for your workloads in the Kiali console.  The *Workload Detail* page includes a *Logs* tab which displays a unified logs view that displays both application and proxy logs. You can select how often you want the log display in Kiali to be refreshed.

To change the logging level on the logs displayed in Kiali, you change the logging configuration for the workload or the proxy.

.Prerequisites

* Service Mesh installed and configured.
* Kiali installed and configured.
* The address for the Kiali console.
* Application or Bookinfo sample application added to the mesh.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.
+
The Kiali Overview page displays namespaces that have been added to the mesh that you have permissions to view.
+
. Click *Workloads*.

. On the *Workloads* page, select the project from the *Namespace* menu.

. If necessary, use the filter to find the workload whose logs you want to view.  Click the workload *Name*.  For example, click *ratings-v1*.

. On the *Workload Details* page, click the *Logs* tab to view the logs for the workload.

[TIP]
====
If you do not see any log entries, you may need to adjust either the Time Range or the Refresh interval.
====

:leveloffset: 2

:leveloffset: +2

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-viewing-metrics_{context}"]
= Viewing metrics in the Kiali console

You can view inbound and outbound metrics for your applications, workloads, and services in the Kiali console.  The Detail pages include the following tabs:

* inbound Application metrics
* outbound Application metrics
* inbound Workload metrics
* outbound Workload metrics
* inbound Service metrics

These tabs display predefined metrics dashboards, tailored to the relevant application, workload or service level. The application and workload detail views show request and response metrics such as volume, duration, size, or TCP traffic. The service detail view shows request and response metrics for inbound traffic only.

Kiali lets you customize the charts by choosing the charted dimensions. Kiali can also present metrics reported by either source or destination proxy metrics. And for troubleshooting, Kiali can overlay trace spans on the metrics.

.Prerequisites

* Service Mesh installed and configured.
* Kiali installed and configured.
* The address for the Kiali console.
* (Optional) Distributed tracing installed and configured.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.
+
The Kiali Overview page displays namespaces that have been added to the mesh that you have permissions to view.
+
. Click either *Applications*, *Workloads*, or *Services*.

. On the *Applications*, *Workloads*, or *Services* page, select the project from the *Namespace* menu.

. If necessary, use the filter to find the application, workload, or service whose logs you want to view.  Click the *Name*.

. On the *Application Detail*, *Workload Details*, or *Service Details* page, click either the *Inbound Metrics* or *Outbound Metrics* tab to view the metrics.

:leveloffset: 2

:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v1x/ossm-config.adoc
* service_mesh/v2x/ossm-observability.adoc
////

[id="ossm-overview-distr-tracing_{context}"]
= Distributed tracing

Distributed tracing is the process of tracking the performance of individual services in an application by tracing the path of the service calls in the application. Each time a user takes action in an application, a request is executed that might require many services to interact to produce a response. The path of this request is called a distributed transaction.

{SMProductName} uses {DTProductName} to allow developers to view call flows in a microservice application.

:leveloffset: 2

:leveloffset: +2

////
This module is included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-external-jaeger_{context}"]
= Connecting an existing distributed tracing instance

If you already have an existing {JaegerName} instance in {product-title}, you can configure your `ServiceMeshControlPlane` resource to use that instance for {DTShortName}.

.Prerequisites

* {DTProductName} instance installed and configured.

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane` resource, for example `basic`.

. Add the name of your {JaegerShortName} instance to the `ServiceMeshControlPlane`.
+
.. Click the *YAML* tab.
+
.. Add the name of your {JaegerShortName} instance to `spec.addons.jaeger.name` in your `ServiceMeshControlPlane` resource. In the following example, `distr-tracing-production` is the name of the {JaegerShortName} instance.
+
.Example distributed tracing configuration
[source,yaml]
----
spec:
  addons:
    jaeger:
      name: distr-tracing-production
----
+
.. Click *Save*.

. Click *Reload* to verify the `ServiceMeshControlPlane` resource was configured correctly.

:leveloffset: 2

:leveloffset: +2

////
This module is included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-config-sampling_{context}"]
= Adjusting the sampling rate

A trace is an execution path between services in the service mesh. A trace is comprised of one or more spans. A span is a logical unit of work that has a name, start time, and duration. The sampling rate determines how often a trace is persisted.

The Envoy proxy sampling rate is set to sample 100% of traces in your service mesh by default. A high sampling rate consumes cluster resources and performance but is useful when debugging issues. Before you deploy {SMProductName} in production, set the value to a smaller proportion of traces. For example, set `spec.tracing.sampling` to `100` to sample 1% of traces.

Configure the Envoy proxy sampling rate as a scaled integer representing 0.01% increments.

In a basic installation, `spec.tracing.sampling` is set to `10000`, which samples 100% of traces. For example:

* Setting the value to 10 samples 0.1% of traces.
* Setting the value to 500 samples 5% of traces.

[NOTE]
====
The Envoy proxy sampling rate applies for applications that are available to a Service Mesh, and use the Envoy proxy. This sampling rate determines how much data the Envoy proxy collects and tracks.

The Jaeger remote sampling rate applies to applications that are external to the Service Mesh, and do not use the Envoy proxy, such as a database. This sampling rate determines how much data the distributed tracing system collects and stores.
For more information, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/distributed_tracing/#distr-tracing-config-sampling_deploying-distributed-tracing-platform[Distributed tracing configuration options].
====

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Click the *Project* menu and select the project where you installed the control plane, for example *istio-system*.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane` resource, for example `basic`.

. To adjust the sampling rate, set a different value for `spec.tracing.sampling`.
+
.. Click the *YAML* tab.
+
.. Set the value for `spec.tracing.sampling` in your `ServiceMeshControlPlane` resource. In the following example, set it to `100`.
+
.Jaeger sampling example
[source,yaml]
----
spec:
  tracing:
    sampling: 100
----
+
.. Click *Save*.

. Click *Reload* to verify the `ServiceMeshControlPlane` resource was configured correctly.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
* service_mesh/v2x/ossm-troubleshooting-istio.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-accessing-jaeger-console_{context}"]
= Accessing the Jaeger console

To access the Jaeger console you must have {SMProductName} installed, {JaegerName} installed and configured.

The installation process creates a route to access the Jaeger console.

If you know the URL for the Jaeger console, you can access it directly.  If you do not know the URL, use the following directions.

.Procedure from OpenShift console
. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Networking* -> *Routes*.

. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+
. If necessary, use the filter to find the `jaeger` route.  Click the route *Location* to launch the console.

. Click *Log In With OpenShift*.


.Procedure from Kiali console

. Launch the Kiali console.

. Click *Distributed Tracing* in the left navigation pane.

. Click *Log In With OpenShift*.


.Procedure from the CLI

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. To query for details of the route using the command line, enter the following command. In this example, `istio-system` is the {SMProductShortName} control plane namespace.
+
[source,terminal]
----
$ export JAEGER_URL=$(oc get route -n istio-system jaeger -o jsonpath='{.spec.host}')
----
+
. Launch a browser and navigate to ``\https://<JAEGER_URL>``, where `<JAEGER_URL>` is the route that you discovered in the previous step.

. Log in using the same user name and password that you use to access the {Product-title} console.

. If you have added services to the service mesh and have generated traces, you can use the filters and *Find Traces* button to search your trace data.
+
If you are validating the console installation, there is no trace data to display.

:leveloffset: 2

For more information about configuring Jaeger, see the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/distributed_tracing/#distr-tracing-deploy-default_deploying-distributed-tracing-platform[distributed tracing documentation].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-access-grafana_{context}"]
= Accessing the Grafana console

Grafana is an analytics tool you can use to view, query, and analyze your service mesh metrics. In this example, `istio-system` is the {SMProductShortName} control plane namespace. To access Grafana, do the following:

.Procedure

. Log in to the {product-title} web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click *Routes*.

. Click the link in the *Location* column for the *Grafana* row.

. Log in to the Grafana console with your {product-title} credentials.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-access-prometheus_{context}"]
= Accessing the Prometheus console

Prometheus is a monitoring and alerting tool that you can use to collect multi-dimensional data about your microservices. In this example, `istio-system` is the {SMProductShortName} control plane namespace.

.Procedure

. Log in to the {product-title} web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click *Routes*.

. Click the link in the *Location* column for the *Prometheus* row.

. Log in to the Prometheus console with your {product-title} credentials.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-integrating-with-user-workload-monitoring_{context}"]
= Integrating with user-workload monitoring

By default, {SMProductName} (OSSM) installs the Service Mesh control plane (SMCP) with a dedicated instance of Prometheus for collecting metrics from a mesh. However, production systems need more advanced monitoring systems, like {product-title} monitoring for user-defined projects.

The following steps show how to integrate Service Mesh with user-workload monitoring.

.Prerequisites

* User-workload monitoring is enabled.
* {SMProductName} Operator 2.4 is installed.
* Kiali Operator 1.65 is installed.

.Procedure

. Create a token to Thanos for Kiali by running the following commands:
+
.. Set the `SECRET` environment variable by running the following command:
+
[source,terminal]
----
$ SECRET=`oc get secret -n openshift-user-workload-monitoring |
 grep  prometheus-user-workload-token | head -n 1 | awk '{print $1 }'`
----
+
.. Set the `TOKEN` environment variable by running the following command:
+
[source,terminal]
----
$ TOKEN=`oc get secret $SECRET -n openshift-user-workload-monitoring -o jsonpath='{.data.token}' | base64 -d`
----
+
.. Create a token to Thanos for Kiali by running the following command:
+
[source,terminal]
----
$ oc create secret generic thanos-querier-web-token -n istio-system --from-literal=token=$TOKEN
----

. Configure Kiali for user-workload monitoring:
+
[source,yaml]
----
apiVersion: kiali.io/v1alpha1
kind: Kiali
metadata:
  name: kiali-user-workload-monitoring
  namespace: istio-system
spec:
  external_services:
    istio:
      url_service_version: 'http://istiod-basic.istio-system:15014/version'
      config_map_name: istio-basic # <1>
    prometheus:
      auth:
        token: secret:thanos-querier-web-token:token
        type: bearer
        use_kiali_token: false
      query_scope:
        mesh_id: "basic-istio-system"
      thanos_proxy:
        enabled: true
      url: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091
  version: v1.65
----
<1> Set the `ServiceMeshControlPlane` name prefixed with `istio-`.

. Configure the SMCP for external Prometheus:
+
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  addons:
    prometheus:
      enabled: false # <1>
    grafana:
      enabled: false # <2>
    kiali:
      name: kiali-user-workload-monitoring
  meshConfig:
    extensionProviders:
    - name: prometheus
      prometheus: {}
----
<1> Disable the default Prometheus instance provided by OSSM.
<2> Disable Grafana. It is not supported with an external Prometheus instance.

. Apply a custom network policy to allow ingress traffic from the monitoring namespace:
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: user-workload-access
  namespace: bookinfo # <1>
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
----
<1> The custom network policy must be applied to all namespaces.

. Apply a `Telemetry` object to enable traffic metrics in Istio proxies:
+
[source,yaml]
----
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: enable-prometheus-metrics
  namespace: istio-system # <1>
spec:
  selector: # <2>
    matchLabels:
      app: bookinfo
  metrics:
  - providers:
    - name: prometheus
----
<1> A `Telemetry` object created in the control plane namespace applies to all workloads in a mesh. To apply telemetry to only one namespace, create the object in the target namespace.
<2> Optional: Setting the `selector.matchLabels` spec applies the `Telemetry` object to specific workloads in the target namespace.

. Apply a `ServiceMonitor` object to monitor the Istio control plane:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: istiod-monitor
  namespace: istio-system # <1>
spec:
  targetLabels:
  - app
  selector:
    matchLabels:
      istio: pilot
  endpoints:
  - port: http-monitoring
    interval: 30s
    relabelings:
    - action: replace
      replacement: "basic-istio-system" # <2>
      targetLabel: mesh_id
----
<1> Create  this `ServiceMonitor` object in the Istio control plane namespace because it monitors the Istiod service. In this example, the namespace is `istio-system`.
<2> The string `"basic-istio-system"` is a combination of the SMCP name and its namespace, but any label can be used as long as it is unique for every mesh using user workload monitoring in the cluster. The `spec.prometheus.query_scope` of the Kiali resource configured in Step 2 needs to match this value.
+
[NOTE]
====
If there is only one mesh using user-workload monitoring, then both the `mesh_id` relabeling and the `spec.prometheus.query_scope` field in the Kiali resource are optional (but the `query_scope` field given here should be removed if the `mesh_id` label is removed).

If there might be multiple meshes using user-workload monitoring, then both the `mesh_id` relabelings and the `spec.prometheus.query_scope` field in the Kiali resource are required so that Kiali only sees metrics from its associated mesh. If Kiali is not being deployed, applying the `mesh_id` relabeling is still recommended so that metrics from different meshes can be distinguished from one another.
====

. Apply a `PodMonitor` object to collect metrics from Istio proxies:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-proxies-monitor
  namespace: istio-system # <1>
spec:
  selector:
    matchExpressions:
    - key: istio-prometheus-ignore
      operator: DoesNotExist
  podMetricsEndpoints:
  - path: /stats/prometheus
    interval: 30s
    relabelings:
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_container_name]
      regex: "istio-proxy"
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_scrape]
    - action: replace
      regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
      replacement: '[$2]:$1'
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: replace
      regex: (\d+);((([0-9]+?)(\.|$)){4})
      replacement: $2:$1
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: labeldrop
      regex: "__meta_kubernetes_pod_label_(.+)"
    - sourceLabels: [__meta_kubernetes_namespace]
      action: replace
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod_name
    - action: replace
      replacement: "basic-istio-system" # <2>
      targetLabel: mesh_id
----
<1> Since {product-title} monitoring ignores the `namespaceSelector` spec in `ServiceMonitor` and `PodMonitor` objects, you must apply the `PodMonitor` object in all mesh namespaces, including the control plane namespace.
<2> The string `"basic-istio-system"` is a combination of the SMCP name and its namespace, but any label can be used as long as it is unique for every mesh using user workload monitoring in the cluster. The `spec.prometheus.query_scope` of the Kiali resource configured in Step 2 needs to match this value.
+
[NOTE]
====
If there is only one mesh using user-workload monitoring, then both the `mesh_id` relabeling and the `spec.prometheus.query_scope` field in the Kiali resource are optional (but the `query_scope` field given here should be removed if the `mesh_id` label is removed).

If there might be multiple meshes using user-workload monitoring, then both the `mesh_id` relabelings and the `spec.prometheus.query_scope` field in the Kiali resource are required so that Kiali only sees metrics from its associated mesh. If Kiali is not being deployed, applying the `mesh_id` relabeling is still recommended so that metrics from different meshes can be distinguished from one another.
====

. Open the {product-title} web console, and check that metrics are visible.

:leveloffset: 2

[role="_additional-resources"]
[id="additional-resources_user-workload-monitoring"]
== Additional resources

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#[Enabling monitoring for user-defined projects]

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-performance-scalability"]
= Performance and scalability
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: performance-scalability

toc::[]

The default `ServiceMeshControlPlane` settings are not intended for production use; they are designed to install successfully on a default {product-title} installation, which is a resource-limited environment. After you have verified a successful SMCP installation, you should modify the settings defined within the SMCP to suit your environment.

// The following include statements pull in the module files that comprise the assembly.

:leveloffset: +1

////
This module included in the following assemblies:
- /v2x/ossm-performance-scalability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-recommended-resources_{context}"]
= Setting limits on compute resources

By default, `spec.proxy` has the settings `cpu: 10m` and  `memory: 128M`. If you are using Pilot, `spec.runtime.components.pilot` has the same default values.

The settings in the following example are based on 1,000 services and 1,000 requests per second. You can change the values for `cpu` and `memory` in the `ServiceMeshControlPlane`.

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane`, for example `basic`.

. Add the name of your standalone Jaeger instance to the `ServiceMeshControlPlane`.
+
.. Click the *YAML* tab.
+
.. Set the values for `spec.proxy.runtime.container.resources.requests.cpu` and `spec.proxy.runtime.container.resources.requests.memory` in your `ServiceMeshControlPlane` resource.
+
.Example version {MaistraVersion} ServiceMeshControlPlane
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  version: v{MaistraVersion}
  proxy:
    runtime:
      container:
        resources:
          requests:
            cpu: 600m
            memory: 50Mi
          limits: {}

  runtime:
    components:
      pilot:
        container:
          resources:
            requests:
              cpu: 1000m
              memory: 1.6Gi
            limits: {}
----
+
.. Click *Save*.

. Click *Reload* to verify the `ServiceMeshControlPlane` resource was configured correctly.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
- /v2x/ossm-performance-scalability.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-load-test-results_{context}"]
= Load test results

The upstream Istio community load tests mesh consists of *1000* services and *2000* sidecars with 70,000 mesh-wide requests per second.
Running the tests using Istio 1.12.3, generated the following results:

* The Envoy proxy uses *0.35 vCPU* and *40 MB memory* per 1000 requests per second going through the proxy.
* Istiod uses *1 vCPU* and *1.5 GB* of memory.
* The Envoy proxy adds *2.65 ms* to the 90th percentile latency.
* The legacy `istio-telemetry` service (disabled by default in Service Mesh 2.0) uses *0.6 vCPU* per 1000 mesh-wide requests per second for deployments that use Mixer.
// TODO The Envoy numbers goes down in 1.9, check for the latest data with next version of Istio.
The data plane components, the Envoy proxies, handle data flowing through the system. The {SMProductShortName} control plane component, Istiod, configures the data plane. The data plane and control plane have distinct performance concerns.

== {SMProductShortName} Control plane performance

Istiod configures sidecar proxies based on user authored configuration files and the current state of the system.
In a Kubernetes environment, Custom Resource Definitions (CRDs) and deployments constitute the configuration and state of the system.
The Istio configuration objects like gateways and virtual services, provide the user-authored configuration.
To produce the configuration for the proxies, Istiod processes the combined configuration and system state from the Kubernetes environment and the user-authored configuration.

The {SMProductShortName} control plane supports thousands of services, spread across thousands of pods with a similar number of user authored virtual services and other configuration objects.
Istiod's CPU and memory requirements scale with the number of configurations and possible system states.
The CPU consumption scales with the following factors:

* The rate of deployment changes.
* The rate of configuration changes.
* The number of proxies connecting to Istiod.

However this part is inherently horizontally scalable.

//Do we support namespace isolation?  When namespace isolation is enabled, a single Istiod instance can support 1000 services, 2000 sidecars with 1 vCPU and 1.5 GB of memory.
//You can increase the number of Istiod instances to reduce the amount of time it takes for the configuration to reach all proxies.

== Data plane performance

Data plane performance depends on many factors, for example:

* Number of client connections
* Target request rate
* Request size and response size
* Number of proxy worker threads
* Protocol
* CPU cores
* Number and types of proxy filters, specifically telemetry v2 related filters.

The latency, throughput, and the proxies' CPU and memory consumption are measured as a function of these factors.

=== CPU and memory consumption

Since the sidecar proxy performs additional work on the data path, it consumes CPU and memory. As of Istio 1.12.3, a proxy consumes about 0.5 vCPU per 1000 requests per second.
//TODO As of Istio 1.7, a proxy consumes about 0.5 vCPU per 1000 requests per second.

The memory consumption of the proxy depends on the total configuration state the proxy holds.
A large number of listeners, clusters, and routes can increase memory usage.
//Istio 1.1 introduced namespace isolation to limit the scope of the configuration sent to a proxy. In a large namespace, the proxy consumes approximately 50 MB of memory.

Since the proxy normally doesn't buffer the data passing through, request rate doesn't affect the memory consumption.

=== Additional latency

Since Istio injects a sidecar proxy on the data path, latency is an important consideration. Istio adds an authentication filter, a telemetry filter, and a metadata exchange filter to the proxy.
Every additional filter adds to the path length inside the proxy and affects latency.

The Envoy proxy collects raw telemetry data after a response is sent to the client.
The time spent collecting raw telemetry for a request does not contribute to the total time taken to complete that request.
However, since the worker is busy handling the request, the worker won't start handling the next request immediately.
This process adds to the queue wait time of the next request and affects average and tail latencies.
The actual tail latency depends on the traffic pattern.

Inside the mesh, a request traverses the client-side proxy and then the server-side proxy. In the default configuration of Istio 1.12.3 (that is, Istio with telemetry v2), the two proxies add about 1.7 ms and 2.7 ms to the 90th and 99th percentile latency, respectively, over the baseline data plane latency.

:leveloffset: 2


:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-production"]
= Configuring Service Mesh for production
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-architecture
toc::[]

When you are ready to move from a basic installation to production, you must configure your control plane, tracing, and security certificates to meet production requirements.

.Prerequisites

* Install and configure {SMProductName}.
* Test your configuration in a staging environment.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-deploy-production.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-smcp-prod_{context}"]
= Configuring your ServiceMeshControlPlane resource for production

If you have installed a basic `ServiceMeshControlPlane` resource to test {SMProductShortName}, you must configure it to production specification before you use {SMProductName} in production.

You cannot change the `metadata.name` field of an existing `ServiceMeshControlPlane` resource. For production deployments, you must customize the default template.

.Procedure

. Configure the {JaegerShortName} for production.
+
.. Edit the `ServiceMeshControlPlane` resource to use the `production` deployment strategy, by setting `spec.addons.jaeger.install.storage.type` to `Elasticsearch` and specify additional configuration options under `install`. You can create and configure your Jaeger instance and set `spec.addons.jaeger.name` to the name of the Jaeger instance.
+
.Default Jaeger parameters including Elasticsearch
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 100
    type: Jaeger
  addons:
    jaeger:
      name: MyJaeger
      install:
        storage:
          type: Elasticsearch
        ingress:
          enabled: true
  runtime:
    components:
      tracing.jaeger.elasticsearch: # only supports resources and image name
        container:
          resources: {}
----

.. Configure the sampling rate for production. For more information, see the Performance and scalability section.

. Ensure your security certificates are production ready by installing security certificates from an external certificate authority. For more information, see the Security section.

. Verify the results. Enter the following command to verify that the `ServiceMeshControlPlane` resource updated properly. In this example, `basic` is the name of the `ServiceMeshControlPlane` resource.
+
[source,terminal]
----
$ oc get smcp basic -o yaml
----

:leveloffset: 2

[id="additional-resources_ossm-production"]
[role="_additional-resources"]
== Additional resources

* For more information about tuning {SMProductShortName} for performance, see xref:ossm-performance-scalability[Performance and scalability].

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-federation"]
= Connecting service meshes
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: federation

toc::[]

_Federation_ is a deployment model that lets you share services and workloads between separate meshes managed in distinct administrative domains.

// The following include statements pull in the module files that comprise the assembly.

:leveloffset: +1

////
This module included in the following assemblies:
- ossm-federation.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-federation-overview_{context}"]
= Federation overview

Federation is a set of features that let you connect services between separate meshes, allowing the use of {SMProductShortName} features such as authentication, authorization, and traffic management across multiple, distinct administrative domains.

Implementing a federated mesh lets you run, manage, and observe a single service mesh running across multiple OpenShift clusters. {SMProductName} federation takes an opinionated approach to a multi-cluster implementation of Service Mesh that assumes _minimal_ trust between meshes.

Service Mesh federation assumes that each mesh is managed individually and retains its own administrator. The default behavior is that no communication is permitted and no information is shared between meshes. The sharing of information between meshes is on an explicit opt-in basis. Nothing is shared in a federated mesh unless it has been configured for sharing. Support functions such as certificate generation, metrics and trace collection remain local in their respective meshes.

You configure the `ServiceMeshControlPlane` on each service mesh to create ingress and egress gateways specifically for the federation, and to specify the trust domain for the mesh.

Federation also involves the creation of additional federation files. The following resources are used to configure the federation between two or more meshes.

* A *ServiceMeshPeer* resource declares the federation between a pair of service meshes.

* An *ExportedServiceSet* resource declares that one or more services from the mesh are available for use by a peer mesh.

* An *ImportedServiceSet* resource declares which services exported by a peer mesh will be imported into the mesh.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-features_{context}"]
= Federation features

[role="_abstract"]
Features of the {SMProductName} federated approach to joining meshes include the following:

* Supports common root certificates for each mesh.
* Supports different root certificates for each mesh.
//* Supports rotating any mesh’s intermediate certificate while preserving the federation connection.
* Mesh administrators must manually configure certificate chains, service discovery endpoints, trust domains, etc for meshes outside of the Federated mesh.
* Only export/import the services that you want to share between meshes.
** Defaults to not sharing information about deployed workloads with other meshes in the federation. A service can be *exported* to make it visible to other meshes and allow requests from workloads outside of its own mesh.
** A service that has been exported can be *imported* to another mesh, enabling workloads on that mesh to send requests to the imported service.
* Encrypts communication between meshes at all times.
//* Supports configuring failover from a service that is locally deployed to a service that is deployed in another mesh in the federation.
* Supports configuring load balancing across workloads deployed locally and workloads that are deployed in another mesh in the federation.

When a mesh is joined to another mesh it can do the following:

* Provide trust details about itself to the federated mesh.
* Discover trust details about the federated mesh.
* Provide information to the federated mesh about its own exported services.
* Discover information about services exported by the federated mesh.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-security_{context}"]
= Federation security

Red Hat OpenShift Service Mesh federation takes an opinionated approach to a multi-cluster implementation of Service Mesh that assumes minimal trust between meshes. Data security is built in as part of the federation features.

* Each mesh is considered to be a unique tenant, with a unique administration.
* You create a unique trust domain for each mesh in the federation.
* Traffic between the federated meshes is automatically encrypted using mutual Transport Layer Security (mTLS).
* The Kiali graph only displays your mesh and services that you have imported. You cannot see the other mesh or services that have not been imported into your mesh.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-limitations_{context}"]
= Federation limitations

The {SMProductName} federated approach to joining meshes has the following limitations:

* Federation of meshes is not supported on OpenShift Dedicated.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-prerequisites_{context}"]
= Federation prerequisites

The {SMProductName} federated approach to joining meshes has the following prerequisites:

* Two or more {product-title} 4.6 or above clusters.
* Federation was introduced in {SMProductName} 2.1 or later. You must have the {SMProductName} 2.1 or later Operator installed on each mesh that you want to federate.
* You must have a version 2.1 or later `ServiceMeshControlPlane` deployed on each mesh that you want to federate.
* You must configure the load balancers supporting the services associated with the federation gateways to support raw TLS traffic. Federation traffic consists of HTTPS for discovery and raw encrypted TCP for service traffic.
* Services that you want to expose to another mesh should be deployed before you can export and import them. However, this is not a strict requirement. You can specify service names that do not yet exist for export/import. When you deploy the services named in the `ExportedServiceSet` and `ImportedServiceSet` they will be automatically made available for export/import.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-planning_{context}"]
= Planning your mesh federation

Before you start configuring your mesh federation, you should take some time to plan your implementation.

* How many meshes do you plan to join in a federation? You probably want to start with a limited number of meshes, perhaps two or three.
* What naming convention do you plan to use for each mesh? Having a pre-defined naming convention will help with configuration and troubleshooting. The examples in this documentation use different colors for each mesh. You should decide on a naming convention that will help you determine who owns and manages each mesh, as well as the following federation resources:
** Cluster names
** Cluster network names
** Mesh names and namespaces
** Federation ingress gateways
** Federation egress gateways
** Security trust domains
+
[NOTE]
====
Each mesh in the federation must have its own unique trust domain.
====
+
* Which services from each mesh do you plan to export to the federated mesh? Each service can be exported individually, or you can specify labels or use wildcards.
** Do you want to use aliases for the service namespaces?
** Do you want to use aliases for the exported services?
* Which exported services does each mesh plan to import? Each mesh only imports the services that it needs.
** Do you want to use aliases for the imported services?

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-across-clusters_{context}"]
= Mesh federation across clusters

To connect one instance of the OpenShift Service Mesh with one running in a different cluster, the procedure is not much different as when connecting two meshes deployed in the same cluster. However, the ingress gateway of one mesh must be reachable from the other mesh. One way of ensuring this is to configure the gateway service as a `LoadBalancer` service if the cluster supports this type of service.

The service must be exposed through a load balancer that operates at Layer4 of the OSI model.

== Exposing the federation ingress on clusters running on bare metal
If the cluster runs on bare metal and fully supports `LoadBalancer` services, the IP address found in the `.status.loadBalancer.ingress.ip` field of the ingress gateway `Service` object should be specified as one of the entries in the `.spec.remote.addresses` field of the `ServiceMeshPeer` object.

If the cluster does not support `LoadBalancer` services, using a `NodePort` service could be an option if the nodes are accessible from the cluster running the other mesh. In the `ServiceMeshPeer` object, specify the IP addresses of the nodes in the `.spec.remote.addresses` field and the service's node ports in the `.spec.remote.discoveryPort` and `.spec.remote.servicePort` fields.

== Exposing the federation ingress on clusters running on {ibm-power-title} and {ibm-z-title}
If the cluster runs on {ibm-power-name} or {ibm-z-name} infrastructure and fully supports `LoadBalancer` services, the IP address found in the `.status.loadBalancer.ingress.ip` field of the ingress gateway `Service` object should be specified as one of the entries in the `.spec.remote.addresses` field of the `ServiceMeshPeer` object.

If the cluster does not support `LoadBalancer` services, using a `NodePort` service could be an option if the nodes are accessible from the cluster running the other mesh. In the `ServiceMeshPeer` object, specify the IP addresses of the nodes in the `.spec.remote.addresses` field and the service's node ports in the `.spec.remote.discoveryPort` and `.spec.remote.servicePort` fields.

== Exposing the federation ingress on Amazon Web Services (AWS)
By default, LoadBalancer services in clusters running on AWS do not support L4 load balancing. In order for {SMProductName} federation to operate correctly, the following annotation must be added to the ingress gateway service:

service.beta.kubernetes.io/aws-load-balancer-type: nlb

The Fully Qualified Domain Name found in the `.status.loadBalancer.ingress.hostname` field of the ingress gateway `Service` object should be specified as one of the entries in the `.spec.remote.addresses` field of the `ServiceMeshPeer` object.

== Exposing the federation ingress on Azure
On Microsoft Azure, merely setting the service type to `LoadBalancer` suffices for mesh federation to operate correctly.

The IP address found in the `.status.loadBalancer.ingress.ip` field of the ingress gateway `Service` object should be specified as one of the entries in the `.spec.remote.addresses` field of the `ServiceMeshPeer` object.

== Exposing the federation ingress on Google Cloud Platform (GCP)
On Google Cloud Platform, merely setting the service type to `LoadBalancer` suffices for mesh federation to operate correctly.

The IP address found in the `.status.loadBalancer.ingress.ip` field of the ingress gateway `Service` object should be specified as one of the entries in the `.spec.remote.addresses` field of the `ServiceMeshPeer` object.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="con-my-concept-module-a_{context}"]
= Federation implementation checklist

Federating services meshes involves the following activities:

* [ ] Configure networking between the clusters that you are going to federate.

** [ ] Configure the load balancers supporting the services associated with the federation gateways to support raw TLS traffic.

* [ ] Installing the {SMProductName} version 2.1 or later Operator in each of your clusters.

* [ ] Deploying a version 2.1 or later `ServiceMeshControlPlane` to each of your clusters.

* [ ] Configuring the SMCP for federation for each mesh that you want to federate:

** [ ] Create a federation egress gateway for each mesh you are going to federate with.
** [ ] Create a federation ingress gateway for each mesh you are going to federate with.
** [ ] Configure a unique trust domain.

* [ ] Federate two or more meshes by creating a `ServiceMeshPeer` resource for each mesh pair.

* [ ] Export services by creating an `ExportedServiceSet` resource to make services available from one mesh to a peer mesh.

* [ ] Import services by creating an `ImportedServiceSet` resource to import services shared by a mesh peer.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-federation-config-smcp_{context}"]
= Configuring a {SMProductShortName} control plane for federation

Before a mesh can be federated, you must configure the `ServiceMeshControlPlane` for mesh federation. Because all meshes that are members of the federation are equal, and each mesh is managed independently, you must configure the SMCP for _each_ mesh that will participate in the federation.

In the following example, the administrator for the `red-mesh` is configuring the SMCP for federation with both the `green-mesh` and the `blue-mesh`.

.Sample SMCP for red-mesh
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: red-mesh
  namespace: red-mesh-system
spec:
  version: v{MaistraVersion}
  runtime:
    defaults:
      container:
        imagePullPolicy: Always
  gateways:
    additionalEgress:
      egress-green-mesh:
        enabled: true
        requestedNetworkView:
        - green-network
        service:
          metadata:
            labels:
              federation.maistra.io/egress-for: egress-green-mesh
          ports:
          - port: 15443
            name: tls
          - port: 8188
            name: http-discovery  #note HTTP here
      egress-blue-mesh:
        enabled: true
        requestedNetworkView:
        - blue-network
        service:
          metadata:
            labels:
              federation.maistra.io/egress-for: egress-blue-mesh
          ports:
          - port: 15443
            name: tls
          - port: 8188
            name: http-discovery  #note HTTP here
    additionalIngress:
      ingress-green-mesh:
        enabled: true
        service:
          type: LoadBalancer
          metadata:
            labels:
              federation.maistra.io/ingress-for: ingress-green-mesh
          ports:
          - port: 15443
            name: tls
          - port: 8188
            name: https-discovery  #note HTTPS here
      ingress-blue-mesh:
        enabled: true
        service:
          type: LoadBalancer
          metadata:
            labels:
              federation.maistra.io/ingress-for: ingress-blue-mesh
          ports:
          - port: 15443
            name: tls
          - port: 8188
            name: https-discovery  #note HTTPS here
  security:
    trust:
      domain: red-mesh.local
----


.ServiceMeshControlPlane federation configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|spec:
  cluster:
    name:
|Name of the cluster. You are not required to specify a cluster name, but it is helpful for troubleshooting.
|String
|N/A

|spec:
  cluster:
    network:
|Name of the cluster network. You are not required to specify a name for the network, but it is helpful for configuration and troubleshooting.
|String
|N/A
|===

== Understanding federation gateways

You use a *gateway* to manage inbound and outbound traffic for your mesh, letting you specify which traffic you want to enter or leave the mesh.

You use ingress and egress gateways to manage traffic entering and leaving the service mesh (North-South traffic). When you create a federated mesh, you create additional ingress/egress gateways, to facilitate service discovery between federated meshes, communication between federated meshes, and to manage traffic flow between service meshes (East-West traffic).

To avoid naming conflicts between meshes, you must create separate egress and ingress gateways for each mesh. For example, `red-mesh` would have separate egress gateways for traffic going to `green-mesh` and `blue-mesh`.

.Federation gateway parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|spec:
  gateways:
    additionalEgress:
      <egressName>:
|Define an additional egress gateway for _each_ mesh peer in the federation.
|
|

|spec:
  gateways:
    additionalEgress:
      <egressName>:
        enabled:
|This parameter enables or disables the federation egress.
|`true`/`false`
|`true`

|spec:
  gateways:
    additionalEgress:
      <egressName>:
        requestedNetworkView:
|Networks associated with exported services.
|Set to the value of `spec.cluster.network` in the SMCP for the mesh, otherwise use <ServiceMeshPeer-name>-network. For example, if the `ServiceMeshPeer` resource for that mesh is named `west`, then the network would be named `west-network`.
|
|

|spec:
  gateways:
    additionalEgress:
      <egressName>:
        service:
          metadata:
            labels:
              federation.maistra.io/egress-for:
|Specify a unique label for the gateway to prevent federated traffic from flowing through the cluster's default system gateways.
|
|

|spec:
  gateways:
    additionalEgress:
      <egressName>:
        service:
          ports:
|Used to specify the `port:` and `name:` used for TLS and service discovery. Federation traffic consists of raw encrypted TCP for service traffic.
|Port `15443` is required for sending TLS service requests to other meshes in the federation. Port `8188` is required for sending service discovery requests to other meshes in the federation.
|

|spec:
  gateways:
    additionalIngress:
|Define an additional ingress gateway gateway for _each_ mesh peer in the federation.
|
|

|spec:
  gateways:
    additionalIgress:
      <ingressName>:
        enabled:
|This parameter enables or disables the federation ingress.
|`true`/`false`
|`true`


|spec:
  gateways:
    additionalIngress:
      <ingressName>:
        service:
          type:
|The ingress gateway service must be exposed through a load balancer that operates at Layer 4 of the OSI model and is publicly available.
|`LoadBalancer`
|

|spec:
  gateways:
    additionalIngress:
      <ingressName>:
        service:
          type:
|If the cluster does not support `LoadBalancer` services, the ingress gateway service can be exposed through a `NodePort` service.
|`NodePort`
|

|spec:
  gateways:
    additionalIngress:
      <ingressName>:
        service:
          metadata:
            labels:
              federation.maistra.io/ingress-for:
|Specify a unique label for the gateway to prevent federated traffic from flowing through the cluster's default system gateways.
|
|

|spec:
  gateways:
    additionalIngress:
      <ingressName>:
        service:
          ports:
|Used to specify the `port:` and `name:` used for TLS and service discovery. Federation traffic consists of raw encrypted TCP for service traffic. Federation traffic consists of HTTPS for discovery.
|Port `15443` is required for receiving TLS service requests to other meshes in the federation. Port `8188` is required for receiving service discovery requests to other meshes in the federation.
|

|spec:
  gateways:
    additionalIngress:
      <ingressName>:
        service:
          ports:
            nodePort:
|Used to specify the `nodePort:` if the cluster does not support `LoadBalancer` services.
|If specified, is required in addition to `port:` and `name:` for both TLS and service discovery. `nodePort:` must be in the range  `30000`-`32767`.
|
|===

In the following example, the administrator is configuring the SMCP for federation with  the `green-mesh` using a `NodePort` service.

.Sample SMCP for NodePort
[source,yaml]
----
  gateways:
     additionalIngress:
      ingress-green-mesh:
        enabled: true
        service:
          type: NodePort
          metadata:
            labels:
              federation.maistra.io/ingress-for: ingress-green-mesh
          ports:
          - port: 15443
            nodePort: 30510
            name: tls
          - port: 8188
            nodePort: 32359
            name: https-discovery
----

== Understanding federation trust domain parameters

Each mesh in the federation must have its own unique trust domain. This value is used when configuring mesh federation in the `ServiceMeshPeer` resource.

[source,yaml]
----
kind: ServiceMeshControlPlane
metadata:
  name: red-mesh
  namespace: red-mesh-system
spec:
  security:
    trust:
      domain: red-mesh.local
----

.Federation security parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|spec:
  security:
    trust:
      domain:
|Used to specify a unique name for the trust domain for the mesh. Domains must be unique for every mesh in the federation.
|`<mesh-name>.local`
|N/A
|===

////
TODO
.Sample SMCP green mesh
[%collapsible]
====
[source,yaml]
----
apiVersion:
kind:
metadata:
spec:
----
====


.Sample SMCP blue mesh
[%collapsible]
====
[source,yaml]
----
apiVersion:
kind:
metadata:
spec:
----
====
////

.Procedure from the Console

Follow this procedure to edit the `ServiceMeshControlPlane` with the {product-title} web console. This example uses the `red-mesh` as an example.

. Log in to the {product-title} web console as a user with the cluster-admin role.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane. For example, `red-mesh-system`.

. Click the {SMProductName} Operator.

. On the *Istio Service Mesh Control Plane* tab, click the name of your `ServiceMeshControlPlane`, for example `red-mesh`.

. On the *Create ServiceMeshControlPlane Details* page, click `YAML` to modify your configuration.

. Modify your `ServiceMeshControlPlane` to add federation ingress and egress gateways and to specify the trust domain.

. Click *Save*.


.Procedure from the CLI

Follow this procedure to create or edit the `ServiceMeshControlPlane` with the command line. This example uses the `red-mesh` as an example.

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. Enter the following command. Then, enter your username and password when prompted.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the {SMProductShortName} control plane, for example red-mesh-system.
+
[source,terminal]
----
$ oc project red-mesh-system
----
+
. Edit the `ServiceMeshControlPlane` file to add federation ingress and egress gateways and to specify the trust domain.

. Run the following command to edit the {SMProductShortName} control plane where `red-mesh-system` is the system namespace and `red-mesh` is the name of the `ServiceMeshControlPlane` object:
+
[source,terminal]
----
$ oc edit -n red-mesh-system smcp red-mesh
----
+
. Enter the following command, where `red-mesh-system` is the system namespace, to see the status of the {SMProductShortName} control plane installation.
+
[source,terminal]
----
$ oc get smcp -n red-mesh-system
----
+
The installation has finished successfully when the READY column indicates that all components are ready.
+
----
NAME       READY   STATUS            PROFILES      VERSION   AGE
red-mesh   10/10   ComponentsReady   ["default"]   2.1.0     4m25s
----

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-federation-joining_{context}"]
= Joining a federated mesh

You declare the federation between two meshes by creating a `ServiceMeshPeer` resource. The `ServiceMeshPeer` resource defines the federation between two meshes, and you use it to configure discovery for the peer mesh, access to the peer mesh, and certificates used to validate the other mesh’s clients.

image::ossm-federated-mesh.png[Service Mesh federated mesh peers illustration]

Meshes are federated on a one-to-one basis, so each pair of peers requires a pair of `ServiceMeshPeer` resources specifying the federation connection to the other service mesh. For example, federating two meshes named `red` and `green` would require two `ServiceMeshPeer` files.

. On red-mesh-system, create a `ServiceMeshPeer` for the green mesh.
. On green-mesh-system, create a `ServiceMeshPeer` for the red mesh.

Federating three meshes named `red`, `blue`, and `green` would require six `ServiceMeshPeer` files.

. On red-mesh-system, create a `ServiceMeshPeer` for the green mesh.
. On red-mesh-system, create a `ServiceMeshPeer` for the blue mesh.
. On green-mesh-system, create a `ServiceMeshPeer` for the red mesh.
. On green-mesh-system, create a `ServiceMeshPeer` for the blue mesh.
. On blue-mesh-system, create a `ServiceMeshPeer` for the red mesh.
. On blue-mesh-system, create a `ServiceMeshPeer` for the green mesh.

Configuration in the `ServiceMeshPeer` resource includes the following:

* The address of the other mesh’s ingress gateway, which is used for discovery and service requests.
* The names of the local ingress and egress gateways that is used for interactions with the specified peer mesh.
* The client ID used by the other mesh when sending requests to this mesh.
* The trust domain used by the other mesh.
* The name of a `ConfigMap` containing a root certificate that is used to validate client certificates in the trust domain used by the other mesh.

In the following example, the administrator for the `red-mesh` is configuring federation with the `green-mesh`.

.Example ServiceMeshPeer resource for red-mesh
[source,yaml]
----
kind: ServiceMeshPeer
apiVersion: federation.maistra.io/v1
metadata:
  name: green-mesh
  namespace: red-mesh-system
spec:
  remote:
    addresses:
    - ingress-red-mesh.green-mesh-system.apps.domain.com
  gateways:
    ingress:
      name: ingress-green-mesh
    egress:
      name: egress-green-mesh
  security:
    trustDomain: green-mesh.local
    clientID: green-mesh.local/ns/green-mesh-system/sa/egress-red-mesh-service-account
    certificateChain:
      kind: ConfigMap
      name: green-mesh-ca-root-cert
----

.ServiceMeshPeer configuration parameters
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Values
|metadata:
  name:
|Name of the peer mesh that this resource is configuring federation with.
|String

|metadata:
  namespace:
|System namespace for this mesh, that is, where the {SMProductShortName} control plane is installed.
|String

|spec:
  remote:
    addresses:
|List of public addresses of the peer meshes' ingress gateways that are servicing requests from this mesh.
|

|spec:
  remote:
    discoveryPort:
|The port on which the addresses are handling discovery requests.
|Defaults to 8188

|spec:
  remote:
    servicePort:
|The port on which the addresses are handling service requests.
|Defaults to 15443

|spec:
  gateways:
    ingress:
      name:
|Name of the ingress on this mesh that is servicing requests received from the peer mesh. For example, `ingress-green-mesh`.
|

|spec:
  gateways:
    egress:
      name:
|Name of the egress on this mesh that is servicing requests sent to the peer mesh. For example, `egress-green-mesh`.
|

|spec:
  security:
    trustDomain:
|The trust domain used by the peer mesh.
|<peerMeshName>.local

|spec:
  security:
    clientID:
|The client ID used by the peer mesh when calling into this mesh.
|<peerMeshTrustDomain>/ns/<peerMeshSystem>/sa/<peerMeshEgressGatewayName>-service-account

|spec:
  security:
    certificateChain:
      kind: ConfigMap
      name:
|The kind (for example, ConfigMap) and name of a resource containing the root certificate used to validate the client and server certificate(s) presented to this mesh by the peer mesh.
The key of the config map entry containing the certificate should be `root-cert.pem`.
|kind: ConfigMap
name: <peerMesh>-ca-root-cert
|===

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-federation-create-peer_{context}"]
= Creating a ServiceMeshPeer resource

.Prerequisites

* Two or more {product-title} 4.6 or above clusters.
* The clusters must already be networked.
* The load balancers supporting the services associated with the federation gateways must be configured to support raw TLS traffic.
* Each cluster must have a version 2.1 or later `ServiceMeshControlPlane` configured to support federation deployed.
* An account with the `cluster-admin` role.

////
.Procedure from the Console
This is conjecture about what the flow might look like…

Follow this procedure to create a `ServiceMeshPeer` resource from the console. This example shows the `red-mesh` creating a peer resource for the `green-mesh`.

. Log in to the {product-title} web console as a user with the cluster-admin role.
. Navigate to *Operators* → *Installed Operators*.
. Click the *Project* menu and select the project where you installed the control plane for the mesh that is creating the `ServiceMeshPeer` resource. For example, `red-mesh-system`.
. Click the {SMProductName} Operator, then click *Istio Service Mesh ServiceMeshPeer*.
. On the *Istio Service Mesh ServiceMeshPeer* tab, click *Create ServiceMeshPeer*.
. On the *Create ServiceMeshPeer* page, click *YAML* to modify your configuration.
. Modify the default configuration with values for the mesh federation between the peers.
. Click *Create*. The Operator creates the mesh peer based on your configuration parameters.
. To verify the `ServiceMeshPeer` resource was created, click the *Istio Service Mesh ServiceMeshPeer* tab.
.. Click the name of the new `ServiceMeshPeer`, for example, `green-mesh`.
.. Click the *Resources* tab to see the `ServiceMeshPeer` resource the Operator created and configured.
////

.Procedure from the CLI

Follow this procedure to create a `ServiceMeshPeer` resource from the command line. This example shows the `red-mesh` creating a peer resource for the `green-mesh`.

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. Enter the following command. Then, enter your username and password when prompted.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> <API token> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the control plane, for example, `red-mesh-system`.
+
[source,terminal]
----
$ oc project red-mesh-system
----
+
. Create a `ServiceMeshPeer` file based the following example for the two meshes that you want to federate.
+
.Example ServiceMeshPeer resource for red-mesh to green-mesh
[source,yaml]
----
kind: ServiceMeshPeer
apiVersion: federation.maistra.io/v1
metadata:
  name: green-mesh
  namespace: red-mesh-system
spec:
  remote:
    addresses:
    - ingress-red-mesh.green-mesh-system.apps.domain.com
  gateways:
    ingress:
      name: ingress-green-mesh
    egress:
      name: egress-green-mesh
  security:
    trustDomain: green-mesh.local
    clientID: green-mesh.local/ns/green-mesh-system/sa/egress-red-mesh-service-account
    certificateChain:
      kind: ConfigMap
      name: green-mesh-ca-root-cert
----
+
. Run the following command to deploy the resource, where `red-mesh-system` is the system namespace and `servicemeshpeer.yaml` includes a full path to the file you edited:
+
[source,terminal]
----
$ oc create -n red-mesh-system -f servicemeshpeer.yaml
----
+
. To confirm that connection between the red mesh and green mesh is established, inspect the status of the green-mesh `ServiceMeshPeer` in the red-mesh-system namespace:
+
[source,terminal]
----
$ oc -n red-mesh-system get servicemeshpeer green-mesh -o yaml
----
+
.Example ServiceMeshPeer connection between red-mesh and green-mesh
[source,yaml]
----
status:
  discoveryStatus:
    active:
    - pod: istiod-red-mesh-b65457658-9wq5j
      remotes:
      - connected: true
        lastConnected: "2021-10-05T13:02:25Z"
        lastFullSync: "2021-10-05T13:02:25Z"
        source: 10.128.2.149
      watch:
        connected: true
        lastConnected: "2021-10-05T13:02:55Z"
        lastDisconnectStatus: 503 Service Unavailable
        lastFullSync: "2021-10-05T13:05:43Z"
----
The `status.discoveryStatus.active.remotes` field shows that istiod in the peer mesh (in this example, the green mesh) is connected to istiod in the current mesh (in this example, the red mesh).
+
The `status.discoveryStatus.active.watch` field shows that istiod in the current mesh is connected to istiod in the peer mesh.
+
If you check the `servicemeshpeer` named `red-mesh` in `green-mesh-system`, you'll find information about the same two connections from the perspective of the green mesh.
+
When the connection between two meshes is not established, the `ServiceMeshPeer` status indicates this in the `status.discoveryStatus.inactive` field.
+
For more information on why a connection attempt failed, inspect the Istiod log, the access log of the egress gateway handling egress traffic for the peer, and the ingress gateway handling ingress traffic for the current mesh in the peer mesh.
+
For example, if the red mesh can't connect to the green mesh, check the following logs:

* istiod-red-mesh in red-mesh-system
* egress-green-mesh in red-mesh-system
* ingress-red-mesh in green-mesh-system

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-config-export_{context}"]
= Exporting a service from a federated mesh

Exporting services allows a mesh to share one or more of its services with another member of the federated mesh.

image::ossm-federation-export-service.png[Service Mesh federation exporting service illustration]

You use an `ExportedServiceSet` resource to declare the services from one mesh that you are making available to another peer in the federated mesh. You must explicitly declare each service to be shared with a peer.

* You can select services by namespace or name.
* You can use wildcards to select services; for example, to export all the services in a namespace.
* You can export services using an alias. For example, you can export the `foo/bar` service as `custom-ns/bar`.
// Need non foo/bar example above
* You can only export services that are visible to the mesh’s system namespace. For example, a service in another namespace with a `networking.istio.io/exportTo` label set to ‘.’ would not be a candidate for export.
* For exported services, their target services will only see traffic from the ingress gateway, not the original requestor (that is, they won’t see the client ID of either the other mesh’s egress gateway or the workload originating the request)

The following example is for services that `red-mesh` is exporting to `green-mesh`.

.Example ExportedServiceSet resource
[source,yaml]
----
kind: ExportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: green-mesh
  namespace: red-mesh-system
spec:
  exportRules:
  # export ratings.mesh-x-bookinfo as ratings.bookinfo
  - type: NameSelector
    nameSelector:
      namespace: red-mesh-bookinfo
      name: red-ratings
      alias:
        namespace: bookinfo
        name: ratings
  # export any service in red-mesh-bookinfo namespace with label export-service=true
  - type: LabelSelector
    labelSelector:
      namespace: red-mesh-bookinfo
      selector:
        matchLabels:
          export-service: "true"
      aliases: # export all matching services as if they were in the bookinfo namespace
      - namespace: "*"
        name: "*"
        alias:
          namespace: bookinfo
----

.ExportedServiceSet parameters
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Values
|metadata:
  name:
|Name of the ServiceMeshPeer you are exposing this service to.
|Must match the `name` value for the mesh in the `ServiceMeshPeer` resource.

|metadata:
  namespace:
|Name of the project/namespace containing this resource (should be the system namespace for the mesh) .
|

|spec:
  exportRules:
  - type:
|Type of rule that will govern the export for this service. The first matching rule found for the service will be used for the export.
|`NameSelector`, `LabelSelector`

|spec:
  exportRules:
  - type: NameSelector
    nameSelector:
      namespace:
      name:
|To create a `NameSelector` rule, specify the `namespace` of the service and the `name` of the service as defined in the `Service` resource.
|

|spec:
  exportRules:
  - type: NameSelector
    nameSelector:
      alias:
        namespace:
        name:
|To create a `NameSelector` rule that uses an alias for the service, after specifying the `namespace` and `name` for the service, then specify the alias for the `namespace` and the alias to be used for `name` of the service.
|

|spec:
  exportRules:
  - type: LabelSelector
    labelSelector:
      namespace: <exportingMesh>
      selector:
        matchLabels:
          <labelKey>: <labelValue>
|To create a `LabelSelector` rule, specify the `namespace` of the service and specify the `label` defined in the `Service` resource. In the example above, the label is `export-service`.
|

|spec:
  exportRules:
  - type: LabelSelector
    labelSelector:
      namespace: <exportingMesh>
      selector:
        matchLabels:
          <labelKey>: <labelValue>
      aliases:
      - namespace:
        name:
        alias:
          namespace:
          name:
|To create a `LabelSelector` rule that uses aliases for the services, after specifying the `selector`, specify the aliases to be used for `name` or `namespace` of the service. In the example above, the namespace alias is `bookinfo` for all matching services.
|
|===



.Export services with the name "ratings" from all namespaces in the red-mesh to blue-mesh.
[source,yaml]
----
kind: ExportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: blue-mesh
  namespace: red-mesh-system
spec:
  exportRules:
  - type: NameSelector
    nameSelector:
      namespace: "*"
      name: ratings
----

.Export all services from the west-data-center namespace to green-mesh
[source,yaml]
----
kind: ExportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: green-mesh
  namespace: red-mesh-system
spec:
  exportRules:
  - type: NameSelector
    nameSelector:
      namespace: west-data-center
      name: "*"
----

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-federation-create-export_{context}"]
= Creating an ExportedServiceSet

You create an `ExportedServiceSet` resource to explicitly declare the services that you want to be available to a mesh peer.

Services are exported as `<export-name>.<export-namespace>.svc.<ServiceMeshPeer.name>-exports.local` and will automatically route to the target service.  This is the name by which the exported service is known in the exporting mesh. When the ingress gateway receives a request destined for this name, it will be routed to the actual service being exported. For example, if a service named `ratings.red-mesh-bookinfo` is exported to `green-mesh` as `ratings.bookinfo`, the service will be exported under the name `ratings.bookinfo.svc.green-mesh-exports.local`, and traffic received by the ingress gateway for that hostname will be routed to the `ratings.red-mesh-bookinfo` service.

.Prerequisites

* The cluster and `ServiceMeshControlPlane` have been configured for mesh federation.
* An account with the `cluster-admin` role.

[NOTE]
====
You can configure services for export even if they don't exist yet. When a service that matches the value specified in the ExportedServiceSet is deployed, it will be automatically exported.
====

////
.Procedure from the Console
This is conjecture about what the flow might look like.

Follow this procedure to create an `ExportedServiceSet` with the web console. This example shows the red-mesh exporting the ratings service from the bookinfo application to the green-mesh.

. Log in to the {product-title} web console as a user with the cluster-admin role.
. Navigate to *Operators* → *Installed Operators*.
. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane for the mesh that will export services. For example, `red-mesh-system`.
. Click the {SMProductName} Operator, then click *Istio Service Mesh ExportedServiceSet*.
. On the *Istio Service Mesh ExportedServiceSet* tab, click *Create ExportedServiceSet*.
. On the *Create ExportedServiceSet* page, click *YAML* to modify your configuration.
. Modify the default configuration with values for your export.
. Click *Create*. The Operator creates the export based on your configuration parameters.
. To verify the `ExportedServiceSet` resource was created, click the *Istio Service Mesh ExportedServiceSet* tab.
.. Click the name of the new `ExportedServiceSet`; for example, `export-to-green-mesh`.
.. Click the *Resources* tab to see the `ExportedServiceSet` resource the Operator created and configured.
////

.Procedure from the CLI

Follow this procedure to create an `ExportedServiceSet` from the command line.

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. Enter the following command. Then, enter your username and password when prompted.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> <API token> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the {SMProductShortName} control plane; for example, `red-mesh-system`.
+
[source,terminal]
----
$ oc project red-mesh-system
----
+
. Create an `ExportedServiceSet` file based on the following example where `red-mesh` is exporting services to `green-mesh`.
+
.Example ExportedServiceSet resource from red-mesh to green-mesh
[source,yaml]
----
apiVersion: federation.maistra.io/v1
kind: ExportedServiceSet
metadata:
  name: green-mesh
  namespace: red-mesh-system
spec:
  exportRules:
  - type: NameSelector
    nameSelector:
      namespace: red-mesh-bookinfo
      name: ratings
      alias:
        namespace: bookinfo
        name: red-ratings
  - type: NameSelector
    nameSelector:
      namespace: red-mesh-bookinfo
      name: reviews
----
+
. Run the following command to upload and create the `ExportedServiceSet` resource in the red-mesh-system namespace.
+
[source,terminal]
----
$ oc create -n <ControlPlaneNamespace> -f <ExportedServiceSet.yaml>
----
+
For example:
+
[source,terminal]
----
$ oc create -n red-mesh-system -f export-to-green-mesh.yaml
----
+
. Create additional `ExportedServiceSets` as needed for each mesh peer in your federated mesh.
//TODO - Add sample output after the validation
. To validate the services you've exported from `red-mesh` to share with `green-mesh`, run the following command:
+
[source,terminal]
----
$ oc get exportedserviceset <PeerMeshExportedTo> -o yaml
----
+
For example:
+
[source,terminal]
----
$ oc get exportedserviceset green-mesh -o yaml
----
+
. Run the following command to validate the services the red-mesh exports to share with green-mesh:
+
[source,terminal]
----
$ oc get exportedserviceset <PeerMeshExportedTo> -o yaml
----
+
For example:
+
[source,terminal]
----
$ oc -n red-mesh-system get exportedserviceset green-mesh -o yaml
----
+
.Example validating the services exported from the red mesh that are shared with the green mesh.
[source,yaml]
----
  status:
    exportedServices:
    - exportedName: red-ratings.bookinfo.svc.green-mesh-exports.local
      localService:
        hostname: ratings.red-mesh-bookinfo.svc.cluster.local
        name: ratings
        namespace: red-mesh-bookinfo
    - exportedName: reviews.red-mesh-bookinfo.svc.green-mesh-exports.local
      localService:
        hostname: reviews.red-mesh-bookinfo.svc.cluster.local
        name: reviews
        namespace: red-mesh-bookinfo
----
The `status.exportedServices` array lists the services that are currently exported (these services matched the export rules in the `ExportedServiceSet object`). Each entry in the array indicates the name of the exported service and details about the local service that is exported.
+
If a service that you expected to be exported is missing, confirm the Service object exists, its name or labels match the `exportRules` defined in the `ExportedServiceSet` object, and that the Service object's namespace is configured as a member of the service mesh using the `ServiceMeshMemberRoll` or `ServiceMeshMember` object.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-config-import_{context}"]
= Importing a service into a federated mesh

Importing services lets you explicitly specify which services exported from another mesh should be accessible within your service mesh.

image::ossm-federation-import-service.png[Service Mesh federation importing service illustration]

You use an `ImportedServiceSet` resource to select services for import. Only services exported by a mesh peer and explicitly imported are available to the mesh. Services that you do not explicitly import are not made available within the mesh.

* You can select services by namespace or name.
* You can use wildcards to select services, for example, to import all the services that were exported to the namespace.
* You can select services for export using a label selector, which may be global to the mesh, or scoped to a specific member namespace.
* You can import services using an alias. For example, you can import the `custom-ns/bar` service as `other-mesh/bar`.
// Need non foo/bar example above
* You can specify a custom domain suffix, which will be appended to the `name.namespace` of an imported service for its fully qualified domain name; for example, `bar.other-mesh.imported.local`.

The following example is for the `green-mesh` importing a service that was exported by `red-mesh`.

.Example ImportedServiceSet
[source,yaml]
----
kind: ImportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: red-mesh #name of mesh that exported the service
  namespace: green-mesh-system #mesh namespace that service is being imported into
spec:
  importRules: # first matching rule is used
  # import ratings.bookinfo as ratings.bookinfo
  - type: NameSelector
    importAsLocal: false
    nameSelector:
      namespace: bookinfo
      name: ratings
      alias:
        # service will be imported as ratings.bookinfo.svc.red-mesh-imports.local
        namespace: bookinfo
        name: ratings
----

.ImportedServiceSet parameters
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Values
|metadata:
  name:
|Name of the ServiceMeshPeer that exported the service to the federated mesh.
|

|metadata:
  namespace:
|Name of the namespace containing the ServiceMeshPeer resource (the mesh system namespace).
|

|spec:
  importRules:
  - type:
|Type of rule that will govern the import for the service. The first matching rule found for the service will be used for the import.
|`NameSelector`

|spec:
  importRules:
  - type: NameSelector
    nameSelector:
      namespace:
      name:
|To create a `NameSelector` rule, specify the `namespace` and the `name` of the exported service.
|

|spec:
  importRules:
  - type: NameSelector
    importAsLocal:
|Set to `true` to aggregate remote endpoint with local services. When `true`, services will be imported as `<name>.<namespace>.svc.cluster.local`
|`true`/`false`

|spec:
  importRules:
  - type: NameSelector
    nameSelector:
      namespace:
      name:
      alias:
        namespace:
        name:
|To create a `NameSelector` rule that uses an alias for the service, after specifying the `namespace` and `name` for the service, then specify the alias for the `namespace` and the alias to be used for `name` of the service.
|
|===




.Import the "bookinfo/ratings" service from the red-mesh into blue-mesh
[source,yaml]
----
kind: ImportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: red-mesh
  namespace: blue-mesh-system
spec:
  importRules:
  - type: NameSelector
    importAsLocal: false
    nameSelector:
      namespace: bookinfo
      name: ratings
----

.Import all services from the red-mesh's west-data-center namespace into the green-mesh. These services will be accessible as <name>.west-data-center.svc.red-mesh-imports.local
[source,yaml]
----
kind: ImportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: red-mesh
  namespace: green-mesh-system
spec:
  importRules:
  - type: NameSelector
    importAsLocal: false
    nameSelector:
      namespace: west-data-center
      name: "*"
----

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-federation-create-import_{context}"]
= Creating an ImportedServiceSet

You create an `ImportedServiceSet` resource to explicitly declare the services that you want to import into your mesh.

Services are imported with the name `<exported-name>.<exported-namespace>.svc.<ServiceMeshPeer.name>.remote` which is a "hidden" service, visible only within the egress gateway namespace and is associated with the exported service's hostname. The service will be available locally as `<export-name>.<export-namespace>.<domainSuffix>`, where `domainSuffix` is `svc.<ServiceMeshPeer.name>-imports.local` by default, unless `importAsLocal` is set to `true`, in which case `domainSuffix` is `svc.cluster.local`.  If `importAsLocal` is set to `false`, the domain suffix in the import rule will be applied.  You can treat the local import just like any other service in the mesh. It automatically routes through the egress gateway, where it is redirected to the exported service's remote name.

.Prerequisites

* The cluster and `ServiceMeshControlPlane` have been configured for mesh federation.
* An account with the `cluster-admin` role.

[NOTE]
====
You can configure services for import even if they haven't been exported yet. When a service that matches the value specified in the ImportedServiceSet is deployed and exported, it will be automatically imported.
====

////
.Procedure from the Console
This is conjecture about what the flow might look like.

Follow this procedure to create an `ImportedServiceSet` with the web console. This example shows the green-mesh importing the ratings service that was exported by the red-mesh.

. Log in to the {product-title} web console as a user with the cluster-admin role.
. Navigate to *Operators* → *Installed Operators*.
. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane for the mesh you want to import services into. For example, `green-mesh-system`.
. Click the {SMProductName} Operator, then click *Istio Service Mesh ImportedServiceSet*.
. On the *Istio Service Mesh ImportedServiceSet* tab, click *Create ImportedServiceSet*.
. On the *Create ImportedServiceSet* page, click *YAML* to modify your configuration.
. Modify the default configuration with values for your import.
. Click *Create*. The Operator creates the import the based on your configuration parameters.
. To verify the `ImportedServiceSet` resource was created, click the *Istio Service Mesh ImportedServiceSet* tab.
.. Click the name of the new `ImportedServiceSet`; for example, `import-from-red-mesh`.
.. Click the *Resources* tab to see the `ImportedServiceSet` resource the Operator created and configured.
////


.Procedure from the CLI

Follow this procedure to create an `ImportedServiceSet` from the command line.

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. Enter the following command. Then, enter your username and password when prompted.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> <API token> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the {SMProductShortName} control plane; for example, `green-mesh-system`.
+
[source,terminal]
----
$ oc project green-mesh-system
----
+
. Create an `ImportedServiceSet` file based on the following example where `green-mesh` is importing services previously exported by `red-mesh`.
+
.Example ImportedServiceSet resource from red-mesh to green-mesh
[source,yaml]
----
kind: ImportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: red-mesh
  namespace: green-mesh-system
spec:
  importRules:
  - type: NameSelector
    importAsLocal: false
    nameSelector:
      namespace: bookinfo
      name: red-ratings
      alias:
        namespace: bookinfo
        name: ratings
----
+
. Run the following command to upload and create the `ImportedServiceSet` resource in the green-mesh-system namespace.
+
[source,terminal]
----
$ oc create -n <ControlPlaneNamespace> -f <ImportedServiceSet.yaml>
----
+
For example:
+
[source,terminal]
----
$ oc create -n green-mesh-system -f import-from-red-mesh.yaml
----
+
. Create additional `ImportedServiceSet` resources as needed for each mesh peer in your federated mesh.
//TODO - Add sample output after the validation
. To validate the services you've imported into `green-mesh`, run the following command:
+
[source,terminal]
----
$ oc get importedserviceset <PeerMeshImportedInto> -o yaml
----
+
For example:
+
[source,terminal]
----
$ oc get importedserviceset green-mesh -o yaml
----
+
. Run the following command to validate the services imported into a mesh.
+
[source,terminal]
----
$ oc get importedserviceset <PeerMeshImportedInto> -o yaml
----
+
.Example validating that the services exported from the red mesh have been imported into the green mesh using the status section of the `importedserviceset/red-mesh' object in the 'green-mesh-system` namespace:
+
[source,terminal]
----
$ oc -n green-mesh-system get importedserviceset/red-mesh -o yaml
----
+
[source,yaml]
----
status:
  importedServices:
  - exportedName: red-ratings.bookinfo.svc.green-mesh-exports.local
    localService:
      hostname: ratings.bookinfo.svc.red-mesh-imports.local
      name: ratings
      namespace: bookinfo
  - exportedName: reviews.red-mesh-bookinfo.svc.green-mesh-exports.local
    localService:
      hostname: ""
      name: ""
      namespace: ""
----
+
In the preceding example only the ratings service is imported, as indicated by the populated fields under `localService`. The reviews service is available for import, but isn't currently imported because it does not match any `importRules` in the `ImportedServiceSet` object.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-federation-config-failover-overview_{context}"]
= Configuring a federated mesh for failover

Failover is the ability to switch automatically and seamlessly to a reliable backup system, for example another server. In the case of a federated mesh, you can configure a service in one mesh to failover to a service in another mesh.

You configure Federation for failover by setting the `importAsLocal` and `locality` settings in an `ImportedServiceSet` resource and then configuring a `DestinationRule` that configures failover for the service to the locality specified in the `ImportedServiceSet`.

.Prerequisites

* Two or more {product-title} 4.6 or above clusters already networked and federated.
* `ExportedServiceSet` resources already created for each mesh peer in the federated mesh.
* `ImportedServiceSet` resources already created for each mesh peer in the federated mesh.
* An account with the `cluster-admin` role.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-federation-config-importedserviceset-failover_{context}"]
= Configuring an ImportedServiceSet for failover

Locality-weighted load balancing allows administrators to control the distribution of traffic to endpoints based on the localities of where the traffic originates and where it will terminate. These localities are specified using arbitrary labels that designate a hierarchy of localities in {region}/{zone}/{sub-zone} form.

In the examples in this section, the `green-mesh` is located in the `us-east` region, and the `red-mesh` is located in the `us-west` region.

.Example `ImportedServiceSet` resource from red-mesh to green-mesh
[source,yaml]
----
kind: ImportedServiceSet
apiVersion: federation.maistra.io/v1
metadata:
  name: red-mesh #name of mesh that exported the service
  namespace: green-mesh-system #mesh namespace that service is being imported into
spec:
  importRules: # first matching rule is used
  # import ratings.bookinfo as ratings.bookinfo
  - type: NameSelector
    importAsLocal: true
    nameSelector:
      namespace: bookinfo
      name: ratings
      alias:
        # service will be imported as ratings.bookinfo.svc.red-mesh-imports.local
        namespace: bookinfo
        name: ratings
  #Locality within which imported services should be associated.
  locality:
    region: us-west
----

.`ImportedServiceLocality` fields table
|===
| Name | Description | Type

|region:
|Region within which imported services are located.
|string

|subzone:
|Subzone within which imported services are located.  I Subzone is specified, Zone must also be specified.
|string

|zone:
|Zone within which imported services are located.  If Zone is specified, Region must also be specified.
|string
|===


.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role, enter the following command:
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> <API token> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the {SMProductShortName} control plane, enter the following command:
+
[source,terminal]
----
$ oc project <smcp-system>
----
+
For example, `green-mesh-system`.
+
[source,terminal]
----
$ oc project green-mesh-system
----
+
.  Edit the `ImportedServiceSet` file, where `<ImportedServiceSet.yaml>` includes a full path to the file you want to edit, enter the following command:
+
[source,terminal]
----
$ oc edit -n <smcp-system> -f <ImportedServiceSet.yaml>
----
+
For example, if you want to modify the file that imports from the red-mesh-system to the green-mesh-system as shown in the previous `ImportedServiceSet` example.
+
[source,terminal]
----
$ oc edit -n green-mesh-system -f import-from-red-mesh.yaml
----
. Modify the file:
.. Set `spec.importRules.importAsLocal` to `true`.
.. Set `spec.locality` to a `region`, `zone`, or `subzone`.
.. Save your changes.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-federation-config-destinationrule-failover_{context}"]
= Configuring a DestinationRule for failover

Create a `DestinationRule` resource that configures the following:

* Outlier detection for the service. This is required in order for failover to function properly. In particular, it configures the sidecar proxies to know when endpoints for a service are unhealthy, eventually triggering a failover to the next locality.

* Failover policy between regions. This ensures that failover beyond a region boundary will behave predictably.

.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. Enter the following command. Then, enter your username and password when prompted.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> <API token> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the {SMProductShortName} control plane.
+
[source,terminal]
----
$ oc project <smcp-system>
----
+
For example, `green-mesh-system`.
+
[source,terminal]
----
$ oc project green-mesh-system
----
+
. Create a `DestinationRule` file based on the following example where if green-mesh is unavailable, the traffic should be routed from the green-mesh in the `us-east` region to the red-mesh in `us-west`.
+
.Example `DestinationRule`
[source,yaml]
----
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: default-failover
  namespace: bookinfo
spec:
  host: "ratings.bookinfo.svc.cluster.local"
  trafficPolicy:
    loadBalancer:
      localityLbSetting:
        enabled: true
        failover:
          - from: us-east
            to: us-west
    outlierDetection:
      consecutive5xxErrors: 3
      interval: 10s
      baseEjectionTime: 1m
----
+
. Deploy the `DestinationRule`, where `<DestinationRule>` includes the full path to your file, enter the following command:
+
[source,terminal]
----
$ oc create -n <application namespace> -f <DestinationRule.yaml>
----
+
For example:
+
[source,terminal]
----
$ oc create -n bookinfo -f green-mesh-us-west-DestinationRule.yaml
----

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-remove-service_{context}"]
= Removing a service from the federated mesh

If you need to remove a service from the federated mesh, for example if it has become obsolete or has been replaced by a different service, you can do so.

== To remove a service from a single mesh

Remove the entry for the service from the `ImportedServiceSet` resource for the mesh peer that no longer should access the service.

== To remove a service from the entire federated mesh

Remove the entry for the service from the `ExportedServiceSet` resource for the mesh that owns the service.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
* service_mesh/v2x/ossm-federation.adoc
////

[id="ossm-federation-remove-mesh_{context}"]
= Removing a mesh from the federated mesh

If you need to remove a mesh from the federation, you can do so.

. Edit the removed mesh's `ServiceMeshControlPlane` resource to remove all federation ingress gateways for peer meshes.

. For each mesh peer that the removed mesh has been federated with:

.. Remove the `ServiceMeshPeer` resource that links the two meshes.

.. Edit the peer mesh's `ServiceMeshControlPlane` resource to remove the egress gateway that serves the removed mesh.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-extensions"]
= Extensions
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-extensions

toc::[]

You can use WebAssembly extensions to add new features directly into the {SMProductName} proxies. This lets you move even more common functionality out of your applications, and implement them in a single language that compiles to WebAssembly bytecode.

[NOTE]
====
WebAssembly extensions are not supported on {ibm-z-name} and {ibm-power-name}.
====


:leveloffset: +1

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-extensions-overview_{context}"]
= WebAssembly modules overview

WebAssembly modules can be run on many platforms, including proxies, and have broad language support, fast execution, and a sandboxed-by-default security model.

{SMProductName} extensions are link:https://www.envoyproxy.io/docs/envoy/v1.20.0/intro/arch_overview/http/http_filters#arch-overview-http-filters[Envoy HTTP Filters], giving them a wide range of capabilities:

* Manipulating the body and headers of requests and responses.
* Out-of-band HTTP requests to services not in the request path, such as authentication or policy checking.
* Side-channel data storage and queues for filters to communicate with each other.

[NOTE]
====
When creating new WebAssembly extensions, use the `WasmPlugin` API. The `ServiceMeshExtension` API was deprecated in {SMProductName} version 2.2 and was removed in {SMProductName} version 2.3.
====

There are two parts to writing a {SMProductName} extension:

. You must write your extension using an SDK that exposes the link:https://github.com/proxy-wasm/spec[proxy-wasm API] and compile it to a WebAssembly module.
. You must then package the module into a container.

.Supported languages

You can use any language that compiles to WebAssembly bytecode to write a {SMProductName} extension, but the following languages have existing SDKs that expose the proxy-wasm API so that it can be consumed directly.

.Supported languages
|===
| Language | Maintainer | Repository

| AssemblyScript
| solo.io
| link:https://github.com/solo-io/proxy-runtime[solo-io/proxy-runtime]

| C++
| proxy-wasm team (Istio Community)
| link:https://github.com/proxy-wasm/proxy-wasm-cpp-sdk[proxy-wasm/proxy-wasm-cpp-sdk]

| Go
| tetrate.io
| link:https://github.com/tetratelabs/proxy-wasm-go-sdk[tetratelabs/proxy-wasm-go-sdk]

| Rust
| proxy-wasm team (Istio Community)
| link:https://github.com/proxy-wasm/proxy-wasm-rust-sdk[proxy-wasm/proxy-wasm-rust-sdk]
|===

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: REFERENCE
[id="ossm-extensions-wasmplugin-format_{context}"]
= `WasmPlugin` container format

Istio supports Open Container Initiative (OCI) images in its Wasm Plugin mechanism. You can distribute your Wasm Plugins as a container image, and you can use the `spec.url` field to refer to a container registry location.  For example, `quay.io/my-username/my-plugin:latest`.

Because each execution environment (runtime) for a WASM module can have runtime-specific configuration parameters, a WASM image can be composed of two layers:

* *plugin.wasm* (Required) - Content layer. This layer consists of a `.wasm` binary containing the bytecode of your WebAssembly module, to be loaded by the runtime.  You must name this file `plugin.wasm`.

* *runtime-config.json* (Optional) - Configuration layer. This layer consists of a JSON-formatted string that describes metadata about the module for the target runtime. The config layer might also contain additional data, depending on the target runtime. For example, the config for a WASM Envoy Filter contains root_ids available on the filter.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: REFERENCE
[id="ossm-wasm-ref-wasmplugin_{context}"]
= WasmPlugin API reference

The WasmPlugins API provides a mechanism to extend the functionality provided by the Istio proxy through WebAssembly filters.

You can deploy multiple WasmPlugins. The `phase` and `priority` settings determine the order of execution (as part of Envoy's filter chain), allowing the configuration of complex interactions between user-supplied WasmPlugins and Istio’s internal filters.

In the following example, an authentication filter implements an OpenID flow and populates the Authorization header with a JSON Web Token (JWT). Istio authentication consumes this token and deploys it to the ingress gateway. The WasmPlugin file lives in the proxy sidecar filesystem. Note the field `url`.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: openid-connect
  namespace: istio-ingress
spec:
  selector:
    matchLabels:
      istio: ingressgateway
  url: file:///opt/filters/openid.wasm
  sha256: 1ef0c9a92b0420cf25f7fe5d481b231464bc88f486ca3b9c83ed5cc21d2f6210
  phase: AUTHN
  pluginConfig:
    openid_server: authn
    openid_realm: ingress
----

Below is the same example, but this time an Open Container Initiative (OCI) image is used instead of a file in the filesystem. Note the fields `url`, `imagePullPolicy`, and `imagePullSecret`.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: openid-connect
  namespace: istio-system
spec:
  selector:
    matchLabels:
      istio: ingressgateway
  url: oci://private-registry:5000/openid-connect/openid:latest
  imagePullPolicy: IfNotPresent
  imagePullSecret: private-registry-pull-secret
  phase: AUTHN
  pluginConfig:
    openid_server: authn
    openid_realm: ingress
----

.WasmPlugin Field Reference
[options="header"]
[cols="a, a, a, a"]
|===
| Field | Type | Description | Required

|spec.selector
|WorkloadSelector
|Criteria used to select the specific set of pods/VMs on which this plugin configuration should be applied. If omitted, this configuration will be applied to all workload instances in the same namespace. If the `WasmPlugin` field is present in the config root namespace, it will be applied to all applicable workloads in any namespace.
|No

|spec.url
|string
|URL of a Wasm module or OCI container. If no scheme is present, defaults to `oci://`, referencing an OCI image. Other valid schemes are `file://` for referencing .wasm module files present locally within the proxy container, and `http[s]://` for .wasm module files hosted remotely.
|No

|spec.sha256
|string
|SHA256 checksum that will be used to verify the Wasm module or OCI container. If the `url` field already references a SHA256 (using the `@sha256:` notation), it must match the value of this field. If an OCI image is referenced by tag and this field is set, its checksum will be verified against the contents of this field after pulling.
|No

|spec.imagePullPolicy
|PullPolicy
|The pull behavior to be applied when fetching an OCI image. Only relevant when images are referenced by tag instead of SHA. Defaults to the value `IfNotPresent`, except when an OCI image is referenced in the `url` field and the `latest` tag is used, in which case the value `Always` is the default, mirroring K8s behavior. Setting is ignored if the `url` field is referencing a Wasm module directly using `file://` or `http[s]://`.
|No

|spec.imagePullSecret
|string
|Credentials to use for OCI image pulling. The name of a secret in the same namespace as the `WasmPlugin` object that contains a pull secret for authenticating against the registry when pulling the image.
|No

|spec.phase
|PluginPhase
|Determines where in the filter chain this `WasmPlugin` object is injected.
|No

|spec.priority
|`int64`
|Determines the ordering of `WasmPlugins` objects that have the same `phase` value. When multiple `WasmPlugins` objects are applied to the same workload in the same phase, they will be applied by priority and in descending order. If the `priority` field is not set, or two `WasmPlugins` objects with the same value, the ordering will be determined from the name and namespace of the `WasmPlugins` objects. Defaults to the value `0`.
|No

|spec.pluginName
|string
|The plugin name used in the Envoy configuration. Some Wasm modules might require this value to select the Wasm plugin to execute.
|No

|spec.pluginConfig
|Struct
|The configuration that will be passed on to the plugin.
|No

|spec.pluginConfig.verificationKey
|string
|The public key used to verify signatures of signed OCI images or Wasm modules. Must be supplied in PEM format.
|No
|===

The `WorkloadSelector` object specifies the criteria used to determine if a filter can be applied to a proxy. The matching criteria includes the metadata associated with a proxy, workload instance information such as labels attached to the pod/VM, or any other information that the proxy provides to Istio during the initial handshake. If multiple conditions are specified, all conditions need to match in order for the workload instance to be selected. Currently, only label based selection mechanism is supported.

.WorkloadSelector
[options="header"]
[cols="a, a, a, a"]
|===
| Field | Type | Description | Required
|matchLabels
|map<string, string>
|One or more labels that indicate a specific set of pods/VMs on which a policy should be applied. The scope of label search is restricted to the configuration namespace in which the resource is present.
|Yes
|===

The `PullPolicy` object specifies the pull behavior to be applied when fetching an OCI image.

.PullPolicy
[options="header"]
[cols="a, a"]
|===
| Value | Description
|<empty>
|Defaults to the value `IfNotPresent`, except for OCI images with tag latest, for which the default will be the value `Always`.

|IfNotPresent
|If an existing version of the image has been pulled before, that will be used. If no version of the image is present locally, we will pull the latest version.

|Always
|Always pull the latest version of an image when applying this plugin.
|===

`Struct` represents a structured data value, consisting of fields which map to dynamically typed values. In some languages, Struct might be supported by a native representation. For example, in scripting languages like JavaScript a struct is represented as an object.

.Struct
[options="header"]
[cols="a, a, a"]
|===
| Field | Type | Description
|fields
|map<string, Value>
|Map of dynamically typed values.
|===

`PluginPhase` specifies the phase in the filter chain where the plugin will be injected.

.PluginPhase
[options="header"]
[cols="a, a"]
|===
| Field | Description
|<empty>
|Control plane decides where to insert the plugin. This will generally be at the end of the filter chain, right before the Router. Do not specify PluginPhase if the plugin is independent of others.

|AUTHN
|Insert plugin before Istio authentication filters.

|AUTHZ
|Insert plugin before Istio authorization filters and after Istio authentication filters.

|STATS
|Insert plugin before Istio stats filters and after Istio authorization filters.
|===

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-wasmplugin-deploy_{context}"]
= Deploying `WasmPlugin` resources

You can enable {SMProductName} extensions using the `WasmPlugin` resource. In this example, `istio-system` is the name of the {SMProductShortName} control plane project. The following example creates an `openid-connect` filter that performs an OpenID Connect flow to authenticate the user.

.Procedure

. Create the following example resource:
+
.Example plugin.yaml
[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: openid-connect
  namespace: istio-system
spec:
  selector:
    matchLabels:
      istio: ingressgateway
  url: oci://private-registry:5000/openid-connect/openid:latest
  imagePullPolicy: IfNotPresent
  imagePullSecret: private-registry-pull-secret
  phase: AUTHN
  pluginConfig:
    openid_server: authn
    openid_realm: ingress
----

. Apply your `plugin.yaml` file with the following command:
+
[source,terminal]
----
$ oc apply -f plugin.yaml
----

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: REFERENCE
[id="ossm-extensions-smextension-format_{context}"]
= `ServiceMeshExtension` container format

You must have a `.wasm` file containing the bytecode of your WebAssembly module, and a `manifest.yaml` file in the root of the container filesystem to make your container image a valid extension image.

[NOTE]
====
When creating new WebAssembly extensions, use the `WasmPlugin` API. The `ServiceMeshExtension` API was deprecated in {SMProductName} version 2.2 and was removed in {SMProductName} version 2.3.
====

.manifest.yaml
[source,yaml]
----
schemaVersion: 1

name: <your-extension>
description: <description>
version: 1.0.0
phase: PreAuthZ
priority: 100
module: extension.wasm
----

.Field Reference for manifest.yml
[options="header"]
[cols="a, a, a"]
|===
| Field | Description |Required

|schemaVersion
|Used for versioning of the manifest schema. Currently the only possible value is `1`.
|This is a required field.

|name
|The name of your extension.
|This field is just metadata and currently unused.

|description
|The description of your extension.
|This field is just metadata and currently unused.

|version
|The version of your extension.
|This field is just metadata and currently unused.

|phase
|The default execution phase of your extension.
|This is a required field.

|priority
|The default priority of your extension.
|This is a required field.

|module
|The relative path from the container filesystem's root to your WebAssembly module.
|This is a required field.
|===

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: REFERENCE
[id="ossm-wasm-ref-smextension_{context}"]
= ServiceMeshExtension reference

The ServiceMeshExtension API provides a mechanism to extend the functionality provided by the Istio proxy through WebAssembly filters. There are two parts to writing a WebAssembly extension:

. Write your extension using an SDK that exposes the proxy-wasm API and compile it to a WebAssembly module.
. Package it into a container.

[NOTE]
====
When creating new WebAssembly extensions, use the `WasmPlugin` API. The `ServiceMeshExtension` API, which was deprecated in {SMProductName} version 2.2, was removed in {SMProductName} version 2.3.
====

.ServiceMeshExtension Field Reference
[options="header"]
[cols="a, a"]
|===
| Field | Description

|metadata.namespace
|The `metadata.namespace` field of a `ServiceMeshExtension` source has a special semantic: if it equals the Control Plane Namespace, the extension will be applied to all workloads in the Service Mesh that match its `workloadSelector` value. When deployed to any other Mesh Namespace, it will only be applied to workloads in that same Namespace.

|spec.workloadSelector
|The `spec.workloadSelector` field has the same semantic as the `spec.selector` field of the link:https://istio.io/v1.6/docs/reference/config/networking/gateway/#Gateway[Istio Gateway resource]. It will match a workload based on its Pod labels. If no `workloadSelector` value is specified, the extension will be applied to all workloads in the namespace.

|spec.config
|This is a structured field that will be handed over to the extension, with the semantics dependent on the extension you are deploying.

|spec.image
|A container image URI pointing to the image that holds the extension.

|spec.phase
|The phase determines where in the filter chain the extension is injected, in relation to existing Istio functionality like Authentication, Authorization and metrics generation. Valid values are: PreAuthN, PostAuthN, PreAuthZ, PostAuthZ, PreStats, PostStats. This field defaults to the value set in the `manifest.yaml` file of the extension, but can be overwritten by the user.

|spec.priority
|If multiple extensions with the same `spec.phase` value are applied to the same workload instance, the `spec.priority` value determines the ordering of execution. Extensions with higher priority will be executed first. This allows for inter-dependent extensions. This field defaults to the value set in the `manifest.yaml` file of the extension, but can be overwritten by the user.
|===

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-smextensions-deploy_{context}"]
= Deploying `ServiceMeshExtension` resources

You can enable {SMProductName} extensions using the `ServiceMeshExtension` resource. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

[NOTE]
====
When creating new WebAssembly extensions, use the `WasmPlugin` API. The `ServiceMeshExtension` API was deprecated in {SMProductName} version 2.2 and removed in {SMProductName} version 2.3.
====

For a complete example that was built using the Rust SDK, take a look at the link:https://github.com/maistra/header-append-filter[header-append-filter]. It is a simple filter that appends one or more headers to the HTTP responses, with their names and values taken out from the `config` field of the extension. See a sample configuration in the snippet below.

.Procedure

. Create the following example resource:
+
.Example ServiceMeshExtension resource extension.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshExtension
metadata:
  name: header-append
  namespace: istio-system
spec:
  workloadSelector:
    labels:
      app: httpbin
  config:
    first-header: some-value
    another-header: another-value
  image: quay.io/maistra-dev/header-append-filter:2.1
  phase: PostAuthZ
  priority: 100
----

. Apply your `extension.yaml` file with the following command:
+
[source,terminal]
----
$ oc apply -f <extension>.yaml
----

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-extensions-migration-overview_{context}"]
= Migrating from `ServiceMeshExtension` to `WasmPlugin` resources

The `ServiceMeshExtension` API, which was deprecated in {SMProductName} version 2.2, was removed in {SMProductName} version 2.3. If you are using the `ServiceMeshExtension` API, you must migrate to the `WasmPlugin` API to continue using your WebAssembly extensions.

The APIs are very similar. The migration consists of two steps:

. Renaming your plugin file and updating the module packaging.

. Creating a `WasmPlugin` resource that references the updated container image.

[id="ossm-extensions-migration-api-changes_{context}"]
== API changes

The new `WasmPlugin` API is similar to the `ServiceMeshExtension`, but with a few differences, especially in the field names:


.Field changes between `ServiceMeshExtensions` and `WasmPlugin`
[options="header"]
[cols="a, a"]
|===
|ServiceMeshExtension |WasmPlugin
|`spec.config`
|`spec.pluginConfig`

|`spec.workloadSelector`
|`spec.selector`

|`spec.image`
|`spec.url`

//Question about the case here, is WasmPlugin app caps?
|`spec.phase` valid values: PreAuthN, PostAuthN, PreAuthZ, PostAuthZ, PreStats, PostStats
|`spec.phase` valid values: <empty>, AUTHN, AUTHZ, STATS
|===

The following is an example of how a `ServiceMeshExtension` resource could be converted into a `WasmPlugin` resource.

.ServiceMeshExtension resource
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshExtension
metadata:
  name: header-append
  namespace: istio-system
spec:
  workloadSelector:
    labels:
      app: httpbin
  config:
    first-header: some-value
    another-header: another-value
  image: quay.io/maistra-dev/header-append-filter:2.2
  phase: PostAuthZ
  priority: 100
----

.New WasmPlugin resource equivalent to the ServiceMeshExtension above
[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: header-append
  namespace: istio-system
spec:
  selector:
    matchLabels:
      app: httpbin
  url: oci://quay.io/maistra-dev/header-append-filter:2.2
  phase: STATS
  pluginConfig:
    first-header: some-value
    another-header: another-value
----

[id="ossm-extensions-migration-format-changes_{context}"]
== Container image format changes

The new `WasmPlugin` container image format is similar to the `ServiceMeshExtensions`, with the following differences:

* The `ServiceMeshExtension` container format required a metadata file named `manifest.yaml` in the root directory of the container filesystem. The `WasmPlugin` container format does not require a `manifest.yaml` file.

* The `.wasm` file (the actual plugin) that previously could have any filename now must be named `plugin.wasm` and must be located in the root directory of the container filesystem.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
*service_mesh_/v2x/ossm-extensions.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-extensions-migrating-to-wasmplugin_{context}"]
= Migrating to `WasmPlugin` resources

To upgrade your WebAssembly extensions from the `ServiceMeshExtension` API to the `WasmPlugin` API, you rename your plugin file.

.Prerequisites

* `ServiceMeshControlPlane` is upgraded to version 2.2 or later.

.Procedure

. Update your container image. If the plugin is already in `/plugin.wasm` inside the container, skip to the next step.  If not:

.. Ensure the plugin file is named `plugin.wasm`. You must name the extension file `plugin.wasm`.

.. Ensure the plugin file is located in the root (/) directory. You must store extension files in the root of the container filesystem..

.. Rebuild your container image and push it to a container registry.

. Remove the `ServiceMeshExtension` resource and create a `WasmPlugin` resource that refers to the new container image you built.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-threescale-webassembly-module"]
= Using the 3scale WebAssembly module
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-threescale-webassembly-module

toc::[]

[NOTE]
====
The `threescale-wasm-auth` module runs on integrations of 3scale API Management 2.11 or later with {SMProductName} 2.1.0 or later.
====

The `threescale-wasm-auth` module is a link:https://webassembly.org[WebAssembly] module that uses a set of interfaces, known as an application binary interfaces (_ABI_). This is defined by the link:https://github.com/proxy-wasm/spec[_Proxy-WASM_] specification to drive any piece of software that implements the ABI so it can authorize HTTP requests against 3scale.

As an ABI specification, Proxy-WASM defines the interaction between a piece of software named _host_ and another named _module_, _program_, or _extension_. The host exposes a set of services used by the module to perform a task, and in this case, to process proxy requests.

The host environment is composed of a WebAssembly virtual machine interacting with a piece of software, in this case, an HTTP proxy.

The module itself runs in isolation to the outside world except for the instructions it runs on the virtual machine and the ABI specified by Proxy-WASM. This is a safe way to provide extension points to software: the extension can only interact in well-defined ways with the virtual machine and the host. The interaction provides a computing model and a connection to the outside world the proxy is meant to have.

[id="compatibility_ossm-threescale-webassembly-module"]
== Compatibility

The `threescale-wasm-auth` module is designed to be fully compatible with all implementations of the _Proxy-WASM ABI_ specification. At this point, however, it has only been thoroughly tested to work with the link:https://www.envoyproxy.io[Envoy] reverse proxy.

[id="usage-as-a-stand-alone-module_ossm-threescale-webassembly-module"]
== Usage as a stand-alone module

Because of its self-contained design, it is possible to configure this module to work with Proxy-WASM proxies independently of {SMProductShortName}, as well as 3scale Istio adapter deployments.

[id="prerequisites_ossm-threescale-webassembly-module"]
== Prerequisites

* The module works with all supported 3scale releases, except when configuring a service to use link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#configuring-oidc-identity-provider[OpenID connect (OIDC)], which requires 3scale 2.11 or later.

:leveloffset: +1

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-configuring-the-threescale-wasm-auth-module_{context}"]
= Configuring the threescale-wasm-auth module

Cluster administrators on {product-title} can configure the `threescale-wasm-auth` module to authorize HTTP requests to 3scale API Management through an application binary interface (ABI). The ABI defines the interaction between host and the module, exposing the hosts services, and allows you to use the module to process proxy requests.

[id="the-wasmplugin-api-extension_{context}"]
== The WasmPlugin API extension

{SMProductShortName} provides a custom resource definition to specify and apply Proxy-WASM extensions to sidecar proxies, known as xref:ossm-extensions-wasmplugin-format_ossm-extensions[`WasmPlugin`]. {SMProductShortName} applies this custom resource to the set of workloads that require HTTP API management with 3scale.

See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#crd-extending-api-with-crds[custom resource definition] for more information.

[NOTE]
====
Configuring the WebAssembly extension is currently a manual process. Support for fetching the configuration for services from the 3scale system will be available in a future release.
====

.Prerequisites

* Identify a Kubernetes workload and namespace on your {SMProductShortName} deployment that you will apply this module.
* You must have a 3scale tenant account. See link:https://www.3scale.net/signup[SaaS] or link:https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.11/html-single/installing_3scale/index#install-threescale-on-openshift-guide[3scale 2.11 On-Premises] with a matching service and relevant applications and metrics defined.
* If you apply the module to the `<product_page>` microservice in the `bookinfo` namespace, see the xref:ossm-tutorial-bookinfo-overview_deploying-applications-ossm-v1x[Bookinfo sample application].
** The following example is the YAML format for the custom resource for `threescale-wasm-auth` module.
This example refers to the upstream Maistra version of {SMProductShortName}, `WasmPlugin` API. You must declare the namespace where the `threescale-wasm-auth` module is deployed, alongside a `selector` to identify the set of applications the module will apply to:
+
[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
  namespace: <bookinfo> <1>
spec:
  selector: <2>
    labels:
      app: <product_page>
  pluginConfig: <yaml_configuration>
  url: oci://registry.redhat.io/3scale-amp2/3scale-auth-wasm-rhel8:0.0.3
  phase: AUTHZ
  priority: 100
----
<1> The `namespace`.
<2> The `selector`.
* The `spec.pluginConfig` field depends on the module configuration and it is not populated in the previous example. Instead, the example uses the `<yaml_configuration>` placeholder value. You can use the format of this custom resource example.
** The `spec.pluginConfig` field varies depending on the application. All other fields persist across multiple instances of this custom resource. As examples:
+
--
*** `url`:  Only changes when newer versions of the module are deployed.
*** `phase`:  Remains the same, since this module needs to be invoked after the proxy has done any local authorization, such as validating OpenID Connect (OIDC) tokens.
--
* After you have the module configuration in `spec.pluginConfig` and the rest of the custom resource, apply it with the `oc apply` command:
+
[source,terminal]
----
$ oc apply -f threescale-wasm-auth-bookinfo.yaml
----

:leveloffset: 2

[role="_additional-resources"]
.Additional resources
* xref:ossm-extensions-migration-overview_ossm-extensions[Migrating from `ServiceMeshExtension` to `WasmPlugin` resources]
* link:https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources[Custom Resources]

:leveloffset: +1

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-applying-external-service-entry-objects_{context}"]
= Applying 3scale external ServiceEntry objects

To have the `threescale-wasm-auth` module authorize requests against 3scale, the module must have access to 3scale services. You can do this within {SMProductName} by applying an external `ServiceEntry` object and a corresponding `DestinationRule` object for TLS configuration to use the HTTPS protocol.

The custom resources (CRs) set up the service entries and destination rules for secure access from within {SMProductShortName} to 3scale Hosted (SaaS) for the backend and system components of the Service Management API and the Account Management API. The Service Management API receives queries for the authorization status of each request. The Account Management API provides API management configuration settings for your services.

.Procedure

. Apply the following external `ServiceEntry` CR and related `DestinationRule` CR for 3scale Hosted *backend* to your cluster:
.. Add the `ServiceEntry` CR to a file called `service-entry-threescale-saas-backend.yml`:
+
.ServiceEntry CR
[source,terminal]
----
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: service-entry-threescale-saas-backend
spec:
  hosts:
  - su1.3scale.net
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
----
.. Add the `DestinationRule` CR to a file called `destination-rule-threescale-saas-backend.yml`:
+
.DestinationRule CR
[source,terminal]
----
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: destination-rule-threescale-saas-backend
spec:
  host: su1.3scale.net
  trafficPolicy:
    tls:
      mode: SIMPLE
      sni: su1.3scale.net
----

.. Apply and save the external `ServiceEntry` CR for the 3scale Hosted backend to your cluster, by running the following command:
+
[source,terminal]
----
$ oc apply -f service-entry-threescale-saas-backend.yml
----

.. Apply and save the external `DestinationRule` CR for the 3scale Hosted backend to your cluster, by running the following command:
+
[source,terminal]
----
$ oc apply -f destination-rule-threescale-saas-backend.yml
----

. Apply the following external `ServiceEntry` CR and related `DestinationRule` CR for 3scale Hosted *system* to your cluster:
.. Add the `ServiceEntry` CR to a file called `service-entry-threescale-saas-system.yml`:
+
.ServiceEntry CR
[source,terminal]
----
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: service-entry-threescale-saas-system
spec:
  hosts:
  - multitenant.3scale.net
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
----
.. Add the `DestinationRule` CR to a file called `destination-rule-threescale-saas-system.yml`:
+
.DestinationRule CR
[source,terminal]
----
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: destination-rule-threescale-saas-system
spec:
  host: multitenant.3scale.net
  trafficPolicy:
    tls:
      mode: SIMPLE
      sni: multitenant.3scale.net
----

.. Apply and save the external `ServiceEntry` CR for the 3scale Hosted system to your cluster, by running the following command:
+
[source,terminal]
----
$ oc apply -f service-entry-threescale-saas-system.yml
----

.. Apply and save the external `DestinationRule` CR for the 3scale Hosted system to your cluster, by running the following command:
+
[source,terminal]
----
$ oc apply -f <destination-rule-threescale-saas-system.yml>
----

Alternatively, you can deploy an in-mesh 3scale service. To deploy an in-mesh 3scale service, change the location of the services in the CR by deploying 3scale and linking to the deployment.

[role="_additional-resources"]
.Additional resources
* xref:ossm-routing-service-entries_traffic-management[Service entry and destination rule documentation]

:leveloffset: 2

:leveloffset: +1

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-configuration_{context}"]
= The 3scale WebAssembly module configuration

The `WasmPlugin` custom resource spec provides the configuration that the `Proxy-WASM` module reads from.

The spec is embedded in the host and read by the `Proxy-WASM` module. Typically, the configurations are in the JSON file format for the modules to parse, however the `WasmPlugin` resource can interpret the spec value as YAML and convert it to JSON for consumption by the module.

If you use the `Proxy-WASM` module in stand-alone mode, you must write the configuration using the JSON format. Using the JSON format means using escaping and quoting where needed within the `host` configuration files, for example `Envoy`. When you use the WebAssembly module with the `WasmPlugin` resource, the configuration is in the YAML format. In this case, an invalid configuration forces the module to show diagnostics based on its JSON representation to a sidecar's logging stream.

[IMPORTANT]
====
The `EnvoyFilter` custom resource is not a supported API, although it can be used in some 3scale Istio adapter or {SMProductShortName} releases. Using the `EnvoyFilter` custom resource is not recommended. Use the `WasmPlugin` API instead of the `EnvoyFilter` custom resource.
If you must use the `EnvoyFilter` custom resource, you must specify the spec in JSON format.
====

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="threescale-configuring-the-threescale-webassembly-module_{context}"]
= Configuring the 3scale WebAssembly module

The architecture of the 3scale WebAssembly module configuration depends on the 3scale account and authorization service, and the list of services to handle.

.Prerequisites

The prerequisites are a set of minimum mandatory fields in all cases:

* For the 3scale account and authorization service: the `backend-listener` URL.
* For the list of services to handle: the service IDs and at least one credential look up method and where to find it.
* You will find examples for dealing with `userkey`, `appid` with `appkey`, and OpenID Connect (OIDC) patterns.
* The WebAssembly module uses the settings you specified in the static configuration. For example, if you add a mapping rule configuration to the module, it will always apply, even when the 3scale Admin Portal has no such mapping rule. The rest of the `WasmPlugin` resource exists around the `spec.pluginConfig` YAML entry.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-api-object_{context}"]
= The 3scale WebAssembly module api object

The `api` top-level string from the 3scale WebAssembly module defines which version of the configuration the module will use.

[NOTE]
====
A non-existent or unsupported version of the `api` object renders the 3scale WebAssembly module inoperable.
====

.The `api` top-level string example
[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
  namespace: <bookinfo>
spec:
  pluginConfig:
    api: v1
...
----

The `api` entry defines the rest of the values for the configuration. The only accepted value is `v1`. New settings that break compatibility with the current configuration or need more logic that modules using `v1` cannot handle, will require different values.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-system-object_{context}"]
= The 3scale WebAssembly module system object

The `system` top-level object specifies how to access the 3scale Account Management API for a specific account. The `upstream` field is the most important part of the object. The `system` object is optional, but recommended unless you are providing a fully static configuration for the 3scale WebAssembly module, which is an option if you do not want to provide connectivity to the _system_ component of 3scale.

When you provide static configuration objects in addition to the `system` object, the static ones always take precedence.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  pluginConfig:
    system:
      name: <saas_porta>
      upstream: <object>
      token: <my_account_token>
      ttl: 300
  ...
----

.`system` object fields
|===
|Name |Description |Required

a|`name`
|An identifier for the 3scale service, currently not referenced elsewhere.
|Optional

a|`upstream`
a|The details about a network host to be contacted. `upstream` refers to the 3scale Account Management API host known as system.
|Yes

a|`token`
|A 3scale personal access token with read permissions.
|Yes

a|`ttl`
|The minimum amount of seconds to consider a configuration retrieved from this host as valid before trying to fetch new changes. The default is 600 seconds (10 minutes). *Note:* there is no maximum amount, but the module will generally fetch any configuration within a reasonable amount of time after this TTL elapses.
|Optional
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-upstream-object_{context}"]
= The 3scale WebAssembly module upstream object

The `upstream` object describes an external host to which the proxy can perform calls.

[source,yaml]
----
apiVersion: maistra.io/v1
upstream:
  name: outbound|443||multitenant.3scale.net
  url: "https://myaccount-admin.3scale.net/"
  timeout: 5000
...
----

.`upstream` object fields
|===
|Name |Description |Required

a|`name`
a|`name` is not a free-form identifier. It is the identifier for the external host as defined by the proxy configuration. In the case of stand-alone `Envoy` configurations, it maps to the name of a link:https://www.envoyproxy.io/docs/envoy/v1.19.0/api-v3/config/cluster/v3/cluster.proto#config-cluster-v3-cluster[Cluster], also known as `upstream` in other proxies. *Note:* the value of this field, because the {SMProductShortName} and 3scale Istio adapter control plane configure the name according to a format using a vertical bar (\|) as the separator of multiple fields. For the purposes of this integration, always use the format: `outbound\|<port>\|\|<hostname>`.
|Yes

a|`url`
|The complete URL to access the described service. Unless implied by the scheme, you must include the TCP port.
|Yes

a|`Timeout`
|Timeout in milliseconds so that connections to this service that take more than the amount of time to respond will be considered errors. Default is 1000 seconds.
|Optional
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-backend-object_{context}"]
= The 3scale WebAssembly module backend object

The `backend` top-level object specifies how to access the 3scale Service Management API for authorizing and reporting HTTP requests. This service is provided by the _Backend_ component of 3scale.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  pluginConfig:
    ...
    backend:
      name: backend
      upstream: <object>
    ...
----

.`backend` object fields
|===
|Name |Description |Required

a|`name`
|An identifier for the 3scale backend, currently not referenced elsewhere.
|Optional

a|`upstream`
|The details about a network host to be contacted. This must refer to the 3scale Account Management API host, known, system.
|Yes. The most important and required field.
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-services-object_{context}"]
= The 3scale WebAssembly module services object

The `services` top-level object specifies which service identifiers are handled by this particular instance of the `module`.

Since accounts have multiple services, you must specify which ones are handled. The rest of the configuration revolves around how to configure services.

The `services` field is required. It is an array that must contain at least one service to be useful.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  pluginConfig:
    ...
    services:
    - id: "2555417834789"
      token: service_token
      authorities:
        - "*.app"
        - 0.0.0.0
        - "0.0.0.0:8443"
      credentials: <object>
      mapping_rules: <object>
    ...
----

Each element in the `services` array represents a 3scale service.

.`services` object fields
|===
|Name |Description |Required

a|`ID`
|An identifier for this 3scale service, currently not referenced elsewhere.
|Yes

a|`token`
a|This `token` can be found in the proxy configuration for your service in System or you can retrieve the it from System with following `curl` command:

`curl \https://<system_host>/admin/api/services/<service_id>/proxy/configs/production/latest.json?access_token=<access_token>" \| jq '.proxy_config.content.backend_authentication_value`
|Optional

a|`authorities`
|An array of strings, each one representing the _Authority_ of a _URL_ to match. These strings accept glob patterns supporting the asterisk (_*_), plus sign (_+_), and question mark (_?_) matchers.
|Yes

a|`credentials`
|An object defining which kind of credentials to look for and where.
|Yes

a|`mapping_rules`
|An array of objects representing mapping rules and 3scale methods to hit.
|Optional
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-credentials-object_{context}"]
= The 3scale WebAssembly module credentials object

The `credentials` object is a component of the `service` object. `credentials` specifies which kind of credentials to be looked up and the steps to perform this action.

All fields are optional, but you must specify at least one, `user_key` or `app_id`. The order in which you specify each credential is irrelevant because it is pre-established by the module. Only specify one instance of each credential.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  pluginConfig:
    ...
    services:
    - credentials:
        user_key: <array_of_lookup_queries>
        app_id: <array_of_lookup_queries>
        app_key: <array_of_lookup_queries>
    ...
----

.`credentials` object fields
|===
|Name |Description |Required

a|`user_key`
|This is an array of lookup queries that defines a 3scale user key. A user key is commonly known as an API key.
|Optional

a|`app_id`
a|This is an array of lookup queries that define a 3scale application identifier. Application identifiers are provided by 3scale or by using an identity provider like link:https://access.redhat.com/products/red-hat-single-sign-on[Red Hat Single Sign-On (RH-SS0)], or OpenID Connect (OIDC). The resolution of the lookup queries specified here, whenever it is successful and resolves to two values, it sets up the `app_id` and the `app_key`.
|Optional

a|`app_key`
a|This is an array of lookup queries that define a 3scale application key. Application keys without a resolved `app_id` are useless, so only specify this field when `app_id` has been specified.
|Optional
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-lookup-queries_{context}"]
= The 3scale WebAssembly module lookup queries

The `lookup query` object is part of any of the fields in the `credentials` object. It specifies how a given credential field should be found and processed. When evaluated, a successful resolution means that one or more values were found. A failed resolution means that no values were found.

Arrays of `lookup queries` describe a short-circuit or relationship: a successful resolution of one of the queries stops the evaluation of any remaining queries and assigns the value or values to the specified credential-type. Each query in the array is independent of each other.

A `lookup query` is made up of a single field, a source object, which can be one of a number of source types. See the following example:

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  pluginConfig:
    ...
    services:
    - credentials:
        user_key:
          - <source_type>: <object>
          - <source_type>: <object>
          ...
        app_id:
          - <source_type>: <object>
          ...
        app_key:
          - <source_type>: <object>
          ...
    ...
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-source-object_{context}"]
= The 3scale WebAssembly module source object

A `source` object exists as part of an array of sources within any of the `credentials` object fields. The object field name, referred to as a `source`-type is any one of the following:

* `header`: The lookup query receives HTTP request headers as input.
* `query_string`: The `lookup query` receives the URL query string parameters as input.
* `filter`: The `lookup query` receives filter metadata as input.

All `source`-type objects have at least the following two fields:

.`source`-type object fields
|===
|Name |Description |Required

a|`keys`
a|An array of strings, each one a `key`, referring to entries found in the input data.
|Yes

a|`ops`
a|An array of `operations` that perform a `key` entry match. The array is a pipeline where operations receive inputs and generate outputs on the next operation. An `operation` failing to provide an output resolves the `lookup query` as failed. The pipeline order of the operations determines the evaluation order.
|Optional
|===

The `filter` field name has a required `path` entry to show the path in the metadata you use to look up data.

When a `key` matches the input data, the rest of the keys are not evaluated and the source resolution algorithm jumps to executing the `operations` (`ops`) specified, if any. If no `ops` are specified, the result value of the matching `key`, if any, is returned.

`Operations` provide a way to specify certain conditions and transformations for inputs you have after the first phase looks up a `key`. Use `operations` when you need to transform, decode, and assert properties, however they do not provide a mature language to deal with all needs and lack _Turing-completeness_.

A stack stored the outputs of `operations`. When evaluated, the `lookup query` finishes by assigning the value or values at the bottom of the stack, depending on how many values the credential consumes.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-operations-object_{context}"]
= The 3scale WebAssembly module operations object

Each element in the `ops` array belonging to a specific `source type` is an `operation` object that either applies transformations to values or performs tests. The field name to use for such an object is the name of the `operation` itself, and any values are the parameters to the `operation`, which could be structure objects, for example, maps with fields and values, lists, or strings.

Most `operations` attend to one or more inputs, and produce one or more outputs. When they consume inputs or produce outputs, they work with a stack of values: each value consumed by the operations is popped from the stack of values and initially populated with any `source` matches. The values outputted by them are pushed to the stack. Other `operations` do not consume or produce outputs other than asserting certain properties, but they inspect a stack of values.

[NOTE]
====
When resolution finishes, the values picked up by the next step, such as assigning the values to be an `app_id`, `app_key`, or `user_key`, are taken from the bottom values of the stack.
====

There are a few different `operations` categories:

* `decode`: These transform an input value by decoding it to get a different format.
* `string`: These take a string value as input and perform transformations and checks on it.
* `stack`: These take a set of values in the input and perform multiple stack transformations and selection of specific positions in the stack.
* `check`: These assert properties about sets of operations in a side-effect free way.
* `control`: These perform operations that allow for modifying the evaluation flow.
* `format`: These parse the format-specific structure of input values and look up values in it.

All operations are specified by the name identifiers as strings.

[role="_additional-resources"]
.Additional resources
* Available link:https://github.com/3scale/threescale-wasm-auth/blob/main/docs/operations.md[operations]

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-mapping-rules-object_{context}"]
= The 3scale WebAssembly module mapping_rules object

The `mapping_rules` object is part of the `service` object. It specifies a set of REST path patterns and related 3scale metrics and count increments to use when the patterns match.

You need the value if no dynamic configuration is provided in the `system` top-level object. If the object is provided in addition to the `system` top-level entry, then the `mapping_rules` object is evaluated first.

`mapping_rules` is an array object. Each element of that array is a `mapping_rule` object. The evaluated matching mapping rules on an incoming request provide the set of 3scale `methods` for authorization and reporting to the _APIManager_. When multiple matching rules refer to the same `methods`, there is a summation of `deltas` when calling into 3scale. For example, if two rules increase the _Hits_ method twice with `deltas` of 1 and 3, a single method entry for Hits reporting to 3scale has a `delta` of 4.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-mapping-rule-object_{context}"]
= The 3scale WebAssembly module mapping_rule object

The `mapping_rule` object is part of an array in the `mapping_rules` object.

The `mapping_rule` object fields specify the following information:

* The _HTTP request method_ to match.
* A pattern to match the path against.
* The 3scale methods to report along with the amount to report. The order in which you specify the fields determines the evaluation order.

.`mapping_rule` object fields
|===
|Name |Description |Required

a|`method`
|Specifies a string representing an HTTP request method, also known as verb. Values accepted match the any one of the accepted HTTP method names, case-insensitive. A special value of any matches any method.
|Yes

a|`pattern`
a|The pattern to match the HTTP request's URI path component. This pattern follows the same syntax as documented by 3scale. It allows wildcards (use of the asterisk (*) character) using any sequence of characters between braces such as `{this}`.
|Yes

a|`usages`
a|A list of `usage` objects. When the rule matches, all methods with their `deltas` are added to the list of methods sent to 3scale for authorization and reporting.

Embed the `usages` object with the following required fields:

* `name`: The `method` system name to report.
* `delta`: For how much to increase that `method` by.
|Yes

a|`last`
|Whether the successful matching of this rule should stop the evaluation of more mapping rules.
a|Optional Boolean. The default is `false`
|===


The following example is independent of existing hierarchies between methods in 3scale. That is, anything run on the 3scale side will not affect this. For example, the _Hits_ metric might be a parent of them all, so it stores 4 hits due to the sum of all reported methods in the authorized request and calls the 3scale `Authrep` API endpoint.

The example below uses a `GET` request to a path, `/products/1/sold`, that matches all the rules.

.`mapping_rules` `GET` request example
[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  pluginConfig:
    ...
    mapping_rules:
      - method: GET
        pattern: /
        usages:
          - name: hits
            delta: 1
      - method: GET
        pattern: /products/
        usages:
          - name: products
            delta: 1
      - method: ANY
        pattern: /products/{id}/sold
        usages:
          - name: sales
            delta: 1
          - name: products
            delta: 1
    ...
----

All `usages` get added to the request the module performs to 3scale with usage data as follows:

* Hits: 1
* products: 2
* sales: 1

:leveloffset: 2

:leveloffset: +1

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-examples-for-credentials-use-cases_{context}"]
= The 3scale WebAssembly module examples for credentials use cases

You will spend most of your time applying configuration steps to obtain credentials in the requests to your services.

The following are `credentials` examples, which you can modify to adapt to specific use cases.

You can combine them all, although when you specify multiple source objects with their own `lookup queries`, they are evaluated in order until one of them successfully resolves.

[id="api-key-in-query-string-parameters_{context}"]
== API key (user_key) in query string parameters
The following example looks up a `user_key` in a query string parameter or header of the same name:

[source,yaml]
----
credentials:
  user_key:
    - query_string:
        keys:
          - user_key
    - header:
        keys:
          - user_key
----

[id="application-id-and-key_{context}"]
== Application ID and key
The following example looks up `app_key` and `app_id` credentials in a query or headers.

[source,yaml]
----
credentials:
  app_id:
    - header:
        keys:
          - app_id
    - query_string:
        keys:
          - app_id
  app_key:
    - header:
        keys:
          - app_key
    - query_string:
        keys:
          - app_key
----

[id="authorization-header_{context}"]
== Authorization header
A request includes an `app_id` and `app_key` in an `authorization` header. If there is at least one or two values outputted at the end, then you can assign the `app_key`.

The resolution here assigns the `app_key` if there is one or two outputted at the end.

The `authorization` header specifies a value with the type of authorization and its value is encoded as `Base64`. This means you can split the value by a space character, take the second output and then split it again using a colon (:) as the separator. For example, if you use this format `app_id:app_key`, the header looks like the following example for `credential`:

----
aladdin:opensesame:  Authorization: Basic YWxhZGRpbjpvcGVuc2VzYW1l
----

You must use lower case header field names as shown in the following example:

[source,yaml]
----
credentials:
  app_id:
    - header:
        keys:
          - authorization
        ops:
          - split:
              separator: " "
              max: 2
          - length:
              min: 2
          - drop:
              head: 1
          - base64_urlsafe
          - split:
              max: 2
  app_key:
    - header:
        keys:
          - app_key
----

The previous example use case looks at the headers for an `authorization`:

. It takes its string value and split it by a space, checking that it generates at least two values of a `credential`-type and the `credential` itself, then dropping the `credential`-type.
. It then decodes the second value containing the data it needs, and splits it by using a colon (:) character to have an operations stack including first the `app_id`, then the `app_key`, if it exists.
.. If `app_key` does not exist in the authorization header then its specific sources are checked, for example, the header with the key `app_key` in this case.
. To add extra conditions to `credentials`, allow `Basic` authorizations, where `app_id` is either `aladdin` or `admin`, or any `app_id` being at least 8 characters in length.
. `app_key` must contain a value and have a minimum of 64 characters as shown in the following example:
+
[source,yaml]
----
credentials:
  app_id:
    - header:
        keys:
          - authorization
        ops:
          - split:
              separator: " "
              max: 2
          - length:
              min: 2
          - reverse
          - glob:
            - Basic
          - drop:
              tail: 1
          - base64_urlsafe
          - split:
              max: 2
          - test:
              if:
                length:
                  min: 2
              then:
                - strlen:
                    max: 63
                - or:
                    - strlen:
                        min: 1
                    - drop:
                        tail: 1
          - assert:
            - and:
              - reverse
              - or:
                - strlen:
                    min: 8
                - glob:
                  - aladdin
                  - admin
----
+
. After picking up the `authorization` header value, you get a `Basic` `credential`-type by reversing the stack so that the type is placed on top.
. Run a glob match on it. When it validates, and the credential is decoded and split, you get the `app_id` at the bottom of the stack, and potentially the `app_key` at the top.
. Run a `test:` if there are two values in the stack, meaning an `app_key` was acquired.
.. Ensure the string length is between 1 and 63, including `app_id` and `app_key`. If the key's length is zero, drop it and continue as if no key exists. If there was only an `app_id` and no `app_key`, the missing else branch indicates a successful test and evaluation continues.

The last operation, `assert`, indicates that no side-effects make it into the stack. You can then modify the stack:

. Reverse the stack to have the `app_id` at the top.
.. Whether or not an `app_key` is present, reversing the stack ensures `app_id` is at the top.
. Use `and` to preserve the contents of the stack across tests.
+
Then use one of the following possibilities:
+
* Make sure `app_id` has a string length of at least 8.
* Make sure `app_id` matches either `aladdin` or `admin`.

[id="openid-connect-use-case_{context}"]
== OpenID Connect (OIDC) use case
For {SMProductShortName} and the 3scale Istio adapter, you must deploy a `RequestAuthentication` as shown in the following example, filling in your own workload data and `jwtRules`:

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-example
  namespace: bookinfo
spec:
  selector:
    matchLabels:
      app: productpage
  jwtRules:
  - issuer: >-
      http://keycloak-keycloak.34.242.107.254.nip.io/auth/realms/3scale-keycloak
    jwksUri: >-
      http://keycloak-keycloak.34.242.107.254.nip.io/auth/realms/3scale-keycloak/protocol/openid-connect/certs
----

When you apply the `RequestAuthentication`, it configures `Envoy` with a link:https://www.envoyproxy.io/docs/envoy/v1.19.0/api-v3/extensions/filters/http/jwt_authn/v3/config.proto.html[native plugin] to validate `JWT` tokens. The proxy validates everything before running the module so any requests that fail do not make it to the 3scale WebAssembly module.

When a `JWT` token is validated, the proxy stores its contents in an internal metadata object, with an entry whose key depends on the specific configuration of the plugin. This use case gives you the ability to look up structure objects with a single entry containing an unknown key name.

The 3scale `app_id` for OIDC matches the OAuth `client_id`. This is found in the `azp` or `aud` fields of `JWT` tokens.

To get `app_id` field from Envoy's native `JWT` authentication filter, see the following example:

[source,yaml]
----
credentials:
  app_id:
    - filter:
        path:
          - envoy.filters.http.jwt_authn
          - "0"
        keys:
          - azp
          - aud
        ops:
          - take:
              head: 1
----

The example instructs the module to use the `filter` source type to look up filter metadata for an object from the `Envoy`-specific `JWT` authentication native plugin. This plugin includes the `JWT` token as part of a structure object with a single entry and a preconfigured name. Use `0` to specify that you will only access the single entry.

The resulting value is a structure for which you will resolve two fields:

* `azp`: The value where `app_id` is found.
* `aud`: The value where this information can also be found.

The operation ensures only one value is held for assignment.

[id="picking-up-the-jwt-token-from-a-header_{context}"]
== Picking up the JWT token from a header
Some setups might have validation processes for `JWT` tokens where the validated token would reach this module via a header in JSON format.

To get the `app_id`, see the following example:

[source,yaml]
----
credentials:
  app_id:
    - header:
        keys:
          - x-jwt-payload
        ops:
          - base64_urlsafe
          - json:
            - keys:
              - azp
              - aud
          - take:
              head: 1
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assembly:
//
// service_mesh/v2x/ossm-threescale-webassembly-module.adoc

[id="ossm-threescale-webassembly-module-minimal-working-configuration_{context}"]
= 3scale WebAssembly module minimal working configuration

The following is an example of a 3scale WebAssembly module minimal working configuration. You can copy and paste this and edit it to work with your own configuration.

[source,yaml]
----
apiVersion: extensions.istio.io/v1alpha1
kind: WasmPlugin
metadata:
  name: <threescale_wasm_plugin_name>
spec:
  url: oci://registry.redhat.io/3scale-amp2/3scale-auth-wasm-rhel8:0.0.3
  imagePullSecret: <optional_pull_secret_resource>
  phase: AUTHZ
  priority: 100
  selector:
    labels:
      app: <product_page>
  pluginConfig:
    api: v1
    system:
      name: <system_name>
      upstream:
        name: outbound|443||multitenant.3scale.net
        url: https://istiodevel-admin.3scale.net/
        timeout: 5000
      token: <token>
    backend:
      name: <backend_name>
      upstream:
        name: outbound|443||su1.3scale.net
        url: https://su1.3scale.net/
        timeout: 5000
      extensions:
      - no_body
    services:
    - id: '2555417834780'
      authorities:
      - "*"
      credentials:
        user_key:
          - query_string:
              keys:
                - <user_key>
          - header:
              keys:
                - <user_key>
        app_id:
          - query_string:
              keys:
                - <app_id>
          - header:
              keys:
                - <app_id>
        app_key:
          - query_string:
              keys:
                - <app_key>
          - header:
              keys:
                - <app_key>
----

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="threescale-adapter"]
= Using the 3scale Istio adapter
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: threescale-adapter

toc::[]

The 3scale Istio Adapter is an optional adapter that allows you to label a service running within the {SMProductName} and integrate that service with the 3scale API Management solution.
It is not required for {SMProductName}.

[IMPORTANT]
====
You can only use the 3scale Istio adapter with {SMProductName} versions 2.0 and below. The Mixer component was deprecated in release 2.0 and removed in release 2.1. For {SMProductName} versions 2.1.0 and later you should use the xref:ossm-threescale-webassembly-module[3scale WebAssembly module].

If you want to enable 3scale backend cache with the 3scale Istio adapter, you must also enable Mixer policy and Mixer telemetry. See xref:ossm-create-smcp[Deploying the Red Hat OpenShift Service Mesh control plane].
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-integrate_{context}"]
= Integrate the 3scale adapter with {SMProductName}

You can use these examples to configure requests to your services using the 3scale Istio Adapter.


.Prerequisites:

* {SMProductName} version 2.x
* A working 3scale account (link:https://www.3scale.net/signup/[SaaS] or link:https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.9/html/installing_3scale/install-threescale-on-openshift-guide[3scale 2.9 On-Premises])
* Enabling backend cache requires 3scale 2.9 or greater
* {SMProductName} prerequisites
* Ensure Mixer policy enforcement is enabled. Update Mixer policy enforcement section provides instructions to check the current Mixer policy enforcement status and enable policy enforcement.
* Mixer policy and telemetry must be enabled if you are using a mixer plugin.
** You will need to properly configure the Service Mesh Control Plane (SMCP) when upgrading.

[NOTE]
====
To configure the 3scale Istio Adapter, refer to {SMProductName} custom resources for instructions on adding adapter parameters to the custom resource file.
====


[NOTE]
====
Pay particular attention to the `kind: handler` resource. You must update this with your 3scale account credentials. You can optionally add a `service_id` to a handler, but this is kept for backwards compatibility only, since it would render the handler only useful for one service in your 3scale account. If you add `service_id` to a handler, enabling 3scale for other services requires you to create more handlers with different `service_ids`.
====

Use a single handler per 3scale account by following the steps below:

.Procedure

. Create a handler for your 3scale account and specify your account credentials. Omit any service identifier.
+
[source,yaml]
----
  apiVersion: "config.istio.io/v1alpha2"
  kind: handler
  metadata:
   name: threescale
  spec:
   adapter: threescale
   params:
     system_url: "https://<organization>-admin.3scale.net/"
     access_token: "<ACCESS_TOKEN>"
   connection:
     address: "threescale-istio-adapter:3333"
----
+
Optionally, you can provide a `backend_url` field within the _params_ section to override the URL provided by the 3scale configuration. This may be useful if the adapter runs on the same cluster as the 3scale on-premise instance, and you wish to leverage the internal cluster DNS.
+
. Edit or patch the Deployment resource of any services belonging to your 3scale account as follows:
.. Add the `"service-mesh.3scale.net/service-id"` label with a value corresponding to a valid `service_id`.
.. Add the `"service-mesh.3scale.net/credentials"` label with its value being the _name of the handler resource_ from step 1.
. Do step 2 to link it to your 3scale account credentials and to its service identifier, whenever you intend to add more services.
. Modify the rule configuration with your 3scale configuration to dispatch the rule to the threescale handler.
+
.Rule configuration example
[source,yaml]
----
  apiVersion: "config.istio.io/v1alpha2"
  kind: rule
  metadata:
    name: threescale
  spec:
    match: destination.labels["service-mesh.3scale.net"] == "true"
    actions:
      - handler: threescale.handler
        instances:
          - threescale-authorization.instance
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-cr_{context}"]
= Generating 3scale custom resources

The adapter includes a tool that allows you to generate the `handler`, `instance`, and `rule` custom resources.

.Usage
|===
|Option |Description |Required | Default value

|`-h, --help`
|Produces help output for available options
|No
|

|`--name`
|Unique name for this URL, token pair
|Yes
|

|`-n, --namespace`
|Namespace to generate templates
|No
|istio-system

|`-t, --token`
|3scale access token
|Yes
|

|`-u, --url`
|3scale Admin Portal URL
|Yes
|

|`--backend-url`
|3scale backend URL. If set, it overrides the value that is read from system configuration
|No
|

|`-s, --service`
|3scale API/Service ID
|No
|

|`--auth`
|3scale authentication pattern to specify (1=API Key, 2=App Id/App Key, 3=OIDC)
|No
|Hybrid

|`-o, --output`
|File to save produced manifests to
|No
|Standard output

|`--version`
|Outputs the CLI version and exits immediately
|No
|
|===

:leveloffset: 2

:leveloffset: +3

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-templates_{context}"]
= Generate templates from URL examples

[NOTE]
====
* Run the following commands via `oc exec` from the 3scale adapter container image in xref:ossm-threescale-manifests_{context}[Generating manifests from a deployed adapter].
* Use the `3scale-config-gen` command to help avoid YAML syntax and indentation errors.
* You can omit the `--service` if you use the annotations.
* This command must be invoked from within the container image via `oc exec`.
====

.Procedure

* Use the `3scale-config-gen` command to autogenerate templates files allowing the token, URL pair to be shared by multiple services as a single handler:
+
----
$ 3scale-config-gen --name=admin-credentials --url="https://<organization>-admin.3scale.net:443" --token="[redacted]"
----
+
* The following example generates the templates with the service ID embedded in the handler:
+
----
$ 3scale-config-gen --url="https://<organization>-admin.3scale.net" --name="my-unique-id" --service="123456789" --token="[redacted]"
----

[role="_additional-resources"]
.Additional resources
* link:https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html-single/admin_portal_guide/index#tokens[Tokens].

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-manifests_{context}"]
= Generating manifests from a deployed adapter

[NOTE]
====
* `NAME` is an identifier you use to identify with the service you are managing with 3scale.
* The `CREDENTIALS_NAME` reference is an identifier that corresponds to the `match` section in the rule configuration. This is automatically set to the `NAME` identifier if you are using the CLI tool.
* Its value does not need to be anything specific: the label value should just match the contents of the rule. See link:https://github.com/3scale/3scale-istio-adapter/blob/v2.X/README.md#routing-service-traffic-through-the-adapter[Routing service traffic through the adapter] for more information.
====

. Run this command to generate manifests from a deployed adapter in the `istio-system` namespace:
+
----
$ export NS="istio-system" URL="https://replaceme-admin.3scale.net:443" NAME="name" TOKEN="token"
oc exec -n ${NS} $(oc get po -n ${NS} -o jsonpath='{.items[?(@.metadata.labels.app=="3scale-istio-adapter")].metadata.name}') \
-it -- ./3scale-config-gen \
--url ${URL} --name ${NAME} --token ${TOKEN} -n ${NS}
----

. This will produce sample output to the terminal. Edit these samples if required and create the objects using the `oc create` command.

. When the request reaches the adapter, the adapter needs to know how the service maps to an API on 3scale. You can provide this information in two ways:

.. Label the workload (recommended)
.. Hard code the handler as `service_id`


. Update the workload with the required annotations:
+
[NOTE]
====
You only need to update the service ID provided in this example if it is not already embedded in the handler. *The setting in the handler takes precedence*.
====
+
----
$ export CREDENTIALS_NAME="replace-me"
export SERVICE_ID="replace-me"
export DEPLOYMENT="replace-me"
patch="$(oc get deployment "${DEPLOYMENT}"
patch="$(oc get deployment "${DEPLOYMENT}" --template='{"spec":{"template":{"metadata":{"labels":{ {{ range $k,$v := .spec.template.metadata.labels }}"{{ $k }}":"{{ $v }}",{{ end }}"service-mesh.3scale.net/service-id":"'"${SERVICE_ID}"'","service-mesh.3scale.net/credentials":"'"${CREDENTIALS_NAME}"'"}}}}}' )"
oc patch deployment "${DEPLOYMENT}" --patch ''"${patch}"''

----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-routing_{context}"]
= Routing service traffic through the adapter
Follow these steps to drive traffic for your service through the 3scale adapter.

.Prerequisites

* Credentials and service ID from your 3scale administrator.

.Procedure

. Match the rule `destination.labels["service-mesh.3scale.net/credentials"] == "threescale"` that you previously created in the configuration, in the `kind: rule` resource.

. Add the above label to `PodTemplateSpec` on the Deployment of the target workload to integrate a service. the value, `threescale`, refers to the name of the generated handler. This handler stores the access token required to call 3scale.

. Add the `destination.labels["service-mesh.3scale.net/service-id"] == "replace-me"` label to the workload to pass the service ID to the adapter via the instance at request time.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-integration-settings_{context}"]
= Configure the integration settings in 3scale

Follow this procedure to configure the 3scale integration settings.

[NOTE]
====
For 3scale SaaS customers, {SMProductName} is enabled as part of the Early Access program.
====

.Procedure

. Navigate to *[your_API_name]* -> *Integration*

. Click *Settings*.

. Select the *Istio* option under _Deployment_.
+
* The *API Key (user_key)* option under _Authentication_ is selected by default.

. Click *Update Product* to save your selection.

. Click *Configuration*.

. Click *Update Configuration*.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-caching_{context}"]
= Caching behavior
Responses from 3scale System APIs are cached by default within the adapter. Entries will be purged from the cache when they become older than the `cacheTTLSeconds` value. Also by default, automatic refreshing of cached entries will be attempted seconds before they expire, based on the `cacheRefreshSeconds` value. You can disable automatic refreshing by setting this value higher than the `cacheTTLSeconds` value.

Caching can be disabled entirely by setting `cacheEntriesMax` to a non-positive value.

By using the refreshing process, cached values whose hosts become unreachable will be retried before eventually being purged when past their expiry.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-authentication_{context}"]
= Authenticating requests
This release supports the following authentication methods:

* *Standard API Keys*: single randomized strings or hashes acting as an identifier and a secret token.
* *Application identifier and key pairs*: immutable identifier and mutable secret key strings.
* *OpenID authentication method*: client ID string parsed from the JSON Web Token.

[id="ossm-threescale-authentication-patterns_{context}"]
== Applying authentication patterns
Modify the `instance` custom resource, as illustrated in the following authentication method examples, to configure authentication behavior. You can accept the authentication credentials from:

* Request headers
* Request parameters
* Both request headers and query parameters

[NOTE]
====
When specifying values from headers, they must be lower case. For example, if you want to send a header as `User-Key`, this must be referenced in the configuration as `request.headers["user-key"]`.
====

[id="ossm-threescale-apikey-authentication_{context}"]
=== API key authentication method
{SMProductShortName} looks for the API key in query parameters and request headers as specified in the `user` option in the `subject` custom resource parameter. It checks the values in the order given in the custom resource file. You can restrict the search for the API key to either query parameters or request headers by omitting the unwanted option.

In this example, {SMProductShortName} looks for the API key in the `user_key` query parameter. If the API key is not in the query parameter, {SMProductShortName} then checks the `user-key` header.

.API key authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
  namespace: istio-system
spec:
  template: authorization
  params:
    subject:
      user: request.query_params["user_key"] | request.headers["user-key"] | ""
    action:
      path: request.url_path
      method: request.method | "get"
----

If you want the adapter to examine a different query parameter or request header, change the name as appropriate. For example, to check for the API key in a query parameter named “key”, change `request.query_params["user_key"]` to `request.query_params["key"]`.

[id="ossm-threescale-appidapikey-authentication_{context}"]
=== Application ID and application key pair authentication method
{SMProductShortName} looks for the application ID and application key in query parameters and request headers, as specified in the `properties` option in the `subject` custom resource parameter. The application key is optional. It checks the values in the order given in the custom resource file. You can restrict the search for the credentials to either query parameters or request headers by not including the unwanted option.

In this example, {SMProductShortName} looks for the application ID and application key in the query parameters first, moving on to the request headers if needed.

.Application ID and application key pair authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
  namespace: istio-system
spec:
  template: authorization
  params:
    subject:
        app_id: request.query_params["app_id"] | request.headers["app-id"] | ""
        app_key: request.query_params["app_key"] | request.headers["app-key"] | ""
    action:
      path: request.url_path
      method: request.method | "get"
----

If you want the adapter to examine a different query parameter or request header, change the name as appropriate. For example, to check for the application ID in a query parameter named `identification`, change `request.query_params["app_id"]` to `request.query_params["identification"]`.

[id="ossm-threescale-openid-authentication_{context}"]
=== OpenID authentication method
To use the _OpenID Connect (OIDC) authentication method_, use the `properties` value on the `subject` field to set `client_id`, and optionally `app_key`.

You can manipulate this object using the methods described previously. In the example configuration shown below, the client identifier (application ID) is parsed from the JSON Web Token (JWT) under the label _azp_. You can modify this as needed.

.OpenID authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
spec:
  template: threescale-authorization
  params:
    subject:
      properties:
        app_key: request.query_params["app_key"] | request.headers["app-key"] | ""
        client_id: request.auth.claims["azp"] | ""
      action:
        path: request.url_path
        method: request.method | "get"
        service: destination.labels["service-mesh.3scale.net/service-id"] | ""
----

For this integration to work correctly, OIDC must still be done in 3scale for the client to be created in the identity provider (IdP). You should create a link:https://istio.io/latest/docs/tasks/security/authorization/authz-jwt/[Request authorization] for the service you want to protect in the same namespace as that service. The JWT is passed in the `Authorization` header of the request.

In the sample `RequestAuthentication` defined below, replace `issuer`, `jwksUri`, and `selector` as appropriate.

.OpenID Policy example

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-example
  namespace: bookinfo
spec:
  selector:
    matchLabels:
      app: productpage
  jwtRules:
  - issuer: >-
      http://keycloak-keycloak.34.242.107.254.nip.io/auth/realms/3scale-keycloak
    jwksUri: >-
      http://keycloak-keycloak.34.242.107.254.nip.io/auth/realms/3scale-keycloak/protocol/openid-connect/certs
----

[id="ossm-threescale-hybrid-authentication_{context}"]
=== Hybrid authentication method
You can choose to not enforce a particular authentication method and accept any valid credentials for either method. If both an API key and an application ID/application key pair are provided, {SMProductShortName} uses the API key.

In this example, {SMProductShortName} checks for an API key in the query parameters, then the request headers. If there is no API key, it then checks for an application ID and key in the query parameters, then the request headers.

.Hybrid authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
spec:
  template: authorization
  params:
    subject:
      user: request.query_params["user_key"] | request.headers["user-key"] |
      properties:
        app_id: request.query_params["app_id"] | request.headers["app-id"] | ""
        app_key: request.query_params["app_key"] | request.headers["app-key"] | ""
        client_id: request.auth.claims["azp"] | ""
    action:
      path: request.url_path
      method: request.method | "get"
      service: destination.labels["service-mesh.3scale.net/service-id"] | ""
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-metrics_{context}"]
= 3scale Adapter metrics
The adapter, by default reports various Prometheus metrics that are exposed on port `8080` at the `/metrics` endpoint. These metrics provide insight into how the interactions between the adapter and 3scale are performing. The service is labeled to be automatically discovered and scraped by Prometheus.

[NOTE]
====
There are incompatible changes in the 3scale Istio Adapter metrics since the previous releases in Service Mesh 1.x.
====

In Prometheus, metrics have been renamed with one addition for the backend cache, so that the following metrics exist as of Service Mesh 2.0:

.Prometheus metrics
|===
|Metric |Type |Description

|`threescale_latency`
|Histogram
|Request latency between adapter and 3scale.

|`threescale_http_total`
|Counter
|HTTP Status response codes for requests to 3scale backend.

|`threescale_system_cache_hits`
|Counter
|Total number of requests to the 3scale system fetched from the configuration cache.

|`threescale_backend_cache_hits`
|Counter
|Total number of requests to 3scale backend fetched from the backend cache.
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="threescale-backend-cache_{context}"]
= 3scale backend cache

The 3scale backend cache provides an authorization and reporting cache for clients of the 3scale Service Management API. This cache is embedded in the adapter to enable lower latencies in responses in certain situations assuming the administrator is willing to accept the trade-offs.

[NOTE]
====
3scale backend cache is disabled by default. 3scale backend cache functionality trades inaccuracy in rate limiting and potential loss of hits since the last flush was performed for low latency and higher consumption of resources in the processor and memory.
====

== Advantages of enabling backend cache

The following are advantages to enabling the backend cache:

* Enable the backend cache when you find latencies are high while accessing services managed by the 3scale Istio Adapter.
* Enabling the backend cache will stop the adapter from continually checking with the 3scale API manager for request authorizations, which will lower the latency.
** This creates an in-memory cache of 3scale authorizations for the 3scale Istio Adapter to store and reuse before attempting to contact the 3scale API manager for authorizations. Authorizations will then take much less time to be granted or denied.
* Backend caching is useful in cases when you are hosting the 3scale API manager in another geographical location from the service mesh running the 3scale Istio Adapter.
** This is generally the case with the 3scale Hosted (SaaS) platform, but also if a user hosts their 3scale API manager in another cluster located in a different geographical location, in a different availability zone, or in any case where the network overhead to reach the 3scale API manager is noticeable.


== Trade-offs for having lower latencies

The following are trade-offs for having lower latencies:

* Each 3scale adapter's authorization state updates every time a flush happens.
** This means two or more instances of the adapter will introduce more inaccuracy between flushing periods.
** There is a greater chance of too many requests being granted that exceed limits and introduce erratic behavior, which leads to some requests going through and some not, depending on which adapter processes each request.
* An adapter cache that cannot flush its data and update its authorization information risks shut down or crashing without reporting its information to the API manager.
* A fail open or fail closed policy will be applied when an adapter cache cannot determine whether a request must be granted or denied, possibly due to network connectivity issues in contacting the API manager.
* When cache misses occur, typically right after booting the adapter or after a long period of no connectivity, latencies will grow in order to query the API manager.
* An adapter cache must do much more work on computing authorizations than it would without an enabled cache, which will tax processor resources.
* Memory requirements will grow proportionally to the combination of the amount of limits, applications, and services managed by the cache.

== Backend cache configuration settings

The following points explain the backend cache configuration settings:

* Find the settings to configure the backend cache in the 3scale configuration options.
* The last 3 settings control enabling of backend cache:
** `PARAM_USE_CACHE_BACKEND` - set to true to enable backend cache.
** `PARAM_BACKEND_CACHE_FLUSH_INTERVAL_SECONDS` - sets time in seconds between consecutive attempts to flush cache data to the API manager.
** `PARAM_BACKEND_CACHE_POLICY_FAIL_CLOSED` - set whether or not to allow/open or deny/close requests to the services when there is not enough cached data and the 3scale API manager cannot be reached.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="threescale-istio-adapter-apicast_{context}"]
= 3scale Istio Adapter APIcast emulation

The 3scale Istio Adapter performs as APIcast would when the following conditions occur:

* When a request cannot match any mapping rule defined, the returned HTTP code is 404 Not Found. This was previously 403 Forbidden.
* When a request is denied because it goes over limits, the returned HTTP code is 429 Too Many Requests. This was previously 403 Forbidden.
* When generating default templates via the CLI, it will use underscores rather than dashes for the headers, for example: `user_key` rather than `user-key`.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-istio-adapter-verification_{context}"]
= 3scale Istio adapter verification

You might want to check whether the 3scale Istio adapter is working as expected. If your adapter is not working, use the following steps to help troubleshoot the problem.

.Procedure

. Ensure the _3scale-adapter_ pod is running in the {SMProductShortName} control plane namespace:
+
[source,terminal]
----
$ oc get pods -n <istio-system>
----
. Check that the _3scale-adapter_ pod has printed out information about itself booting up, such as its version:
+
[source,terminal]
----
$ oc logs <istio-system>
----
. When performing requests to the services protected by the 3scale adapter integration, always try requests that lack the right credentials and ensure they fail. Check the 3scale adapter logs to gather additional information.

:leveloffset: 2

[role="_additional-resources"]
.Additional resources
* link:https://docs.openshift.com/container-platform/4.7/support/troubleshooting/investigating-pod-issues.html#inspecting-pod-and-container-logs_investigating-pod-issues[Inspecting pod and container logs].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-istio-adapter-troubleshooting-checklist_{context}"]
= 3scale Istio adapter troubleshooting checklist

As the administrator installing the 3scale Istio adapter, there are a number of scenarios that might be causing your integration to not function properly. Use the following list to troubleshoot your installation:

* Incorrect YAML indentation.
* Missing YAML sections.
* Forgot to apply the changes in the YAML to the cluster.
* Forgot to label the service workloads with the `service-mesh.3scale.net/credentials` key.
* Forgot to label the service workloads with `service-mesh.3scale.net/service-id` when using handlers that do not contain a `service_id` so they are reusable per account.
* The _Rule_ custom resource points to the wrong handler or instance custom resources, or the references lack the corresponding namespace suffix.
* The _Rule_ custom resource `match` section cannot possibly match the service you are configuring, or it points to a destination workload that is not currently running or does not exist.
* Wrong access token or URL for the 3scale Admin Portal in the handler.
* The _Instance_ custom resource's `params/subject/properties` section fails to list the right parameters for `app_id`, `app_key`, or `client_id`, either because they specify the wrong location such as the query parameters, headers, and authorization claims, or the parameter names do not match the requests used for testing.
* Failing to use the configuration generator without realizing that it actually lives in the adapter container image and needs `oc exec` to invoke it.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-troubleshooting"]
= Troubleshooting your service mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: troubleshooting-ossm

toc::[]

This section describes how to identify and resolve common problems in {SMProductName}. Use the following sections to help troubleshoot and debug problems when deploying {SMProductName} on {product-title}.

// The following include statements pull in the module files that comprise the assembly.

:leveloffset: +1

// Module included in the following assemblies:
// * service_mesh/v2x/upgrading-ossm.adoc
// * service_mesh/v2x/ossm-troubleshooting.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-versions_{context}"]
= Understanding Service Mesh versions

In order to understand what version of {SMProductName} you have deployed on your system, you need to understand how each of the component versions is managed.

* *Operator* version - The most current Operator version is {SMProductVersion}. The Operator version number only indicates the version of the currently installed Operator. Because the {SMProductName} Operator supports multiple versions of the {SMProductShortName} control plane, the version of the Operator does not determine the version of your deployed `ServiceMeshControlPlane` resources.
+
[IMPORTANT]
====
Upgrading to the latest Operator version automatically applies patch updates, but does not automatically upgrade your {SMProductShortName} control plane to the latest minor version.
====
+
* *ServiceMeshControlPlane* version - The `ServiceMeshControlPlane` version determines what version of {SMProductName} you are using. The value of the `spec.version` field in the `ServiceMeshControlPlane` resource controls the architecture and configuration settings that are used to install and deploy {SMProductName}. When you create the {SMProductShortName} control plane you can set the version in one of two ways:

** To configure in the Form View, select the version from the *Control Plane Version* menu.

** To configure in the YAML View, set the value for `spec.version` in the YAML file.

Operator Lifecycle Manager (OLM) does not manage {SMProductShortName} control plane upgrades, so the version number for your Operator and `ServiceMeshControlPlane` (SMCP) may not match, unless you have manually upgraded your SMCP.

:leveloffset: 2

== Troubleshooting Operator installation

In addition to the information in this section, be sure to review the following topics:

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#[What are Operators?]

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#[Operator Lifecycle Management concepts].

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#[OpenShift Operator troubleshooting section].

* link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#[OpenShift installation troubleshooting section].

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/-ossm-troubleshooting-istio.adoc

[id="ossm-validating-operators_{context}"]
= Validating Operator installation

//The Operator installation steps include verifying the Operator status in the OpenShift console.

When you install the {SMProductName} Operators, OpenShift automatically creates the following objects as part of a successful Operator installation:

* config maps
* custom resource definitions
* deployments
* pods
* replica sets
* roles
* role bindings
* secrets
* service accounts
* services

.From the {product-title} console

You can verify that the Operator pods are available and running by using the {product-title} console.

. Navigate to *Workloads* -> *Pods*.
. Select the `openshift-operators` namespace.
. Verify that the following pods exist and have a status of `running`:
** `istio-operator`
** `jaeger-operator`
** `kiali-operator`
. Select the `openshift-operators-redhat` namespace.
. Verify that the `elasticsearch-operator` pod exists and has a status of `running`.

.From the command line

. Verify the Operator pods are available and running in the `openshift-operators` namespace with the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-operators
----
+
.Example output
[source,terminal]
----
NAME                               READY   STATUS    RESTARTS   AGE
istio-operator-bb49787db-zgr87     1/1     Running   0          15s
jaeger-operator-7d5c4f57d8-9xphf   1/1     Running   0          2m42s
kiali-operator-f9c8d84f4-7xh2v     1/1     Running   0          64s
----
+
. Verify the Elasticsearch operator with the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-operators-redhat
----
+
.Example output
[source,terminal]
----
NAME                                      READY   STATUS    RESTARTS   AGE
elasticsearch-operator-d4f59b968-796vq     1/1     Running   0          15s
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/-ossm-troubleshooting-istio.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-troubleshooting-operators_{context}"]
= Troubleshooting service mesh Operators

If you experience Operator issues:

* Verify your Operator subscription status.
* Verify that you did not install a community version of the Operator, instead of the supported Red Hat version.
* Verify that you have the `cluster-admin` role to install {SMProductName}.
* Check for any errors in the Operator pod logs if the issue is related to installation of Operators.

[NOTE]
====
You can install Operators only through the OpenShift console, the OperatorHub is not accessible from the command line.
====

== Viewing Operator pod logs

You can view Operator logs by using the `oc logs` command. Red Hat may request logs to help resolve support cases.

.Procedure

* To view Operator pod logs, enter the command:
+
[source,terminal]
----
$ oc logs -n openshift-operators <podName>
----
+
For example,
+
[source,terminal]
----
$ oc logs -n openshift-operators istio-operator-bb49787db-zgr87
----

//If your pod fails to start, you may need to use the `--previous` option to see the logs of the last attempt.

:leveloffset: 2

== Troubleshooting the control plane

The Service Mesh _control plane_ is composed of Istiod, which consolidates several previous control plane components (Citadel, Galley, Pilot) into a single binary. Deploying the `ServiceMeshControlPlane` also creates the other components that make up {SMProductName} as described in the xref:ossm-architecture_ossm-architecture[architecture] topic.

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/-ossm-troubleshooting-istio.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-validating-smcp_{context}"]
= Validating the Service Mesh control plane installation

When you create the {SMProductShortName} control plane, the {SMProductShortName} Operator uses the parameters that you have specified in the `ServiceMeshControlPlane` resource file to do the following:

* Creates the Istio components and deploys the following pods:
** `istiod`
** `istio-ingressgateway`
** `istio-egressgateway`
** `grafana`
** `prometheus`
* Calls the Kiali Operator to create Kaili deployment based on configuration in either the SMCP or the Kiali custom resource.
+
[NOTE]
====
You view the Kiali components under the Kiali Operator, not the {SMProductShortName} Operator.
====
+
* Calls the {JaegerName} Operator to create {JaegerShortName} components based on configuration in either the SMCP or the Jaeger custom resource.
+
[NOTE]
====
You view the Jaeger components under the {JaegerName} Operator and the Elasticsearch components under the Red Hat Elasticsearch Operator, not the {SMProductShortName} Operator.
====
+
.From the {product-title} console

You can verify the {SMProductShortName} control plane installation in the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.
. Select the `<istio-system>` namespace.
. Select the {SMProductName} Operator.
.. Click the *Istio Service Mesh Control Plane* tab.
.. Click the name of your control plane, for example `basic`.
.. To view the resources created by the deployment, click the *Resources* tab. You can use the filter to narrow your view, for example, to check that all the *Pods* have a status of `running`.
.. If the SMCP status indicates any problems, check the `status:` output in the YAML file for more information.
.. Navigate back to *Operators* -> *Installed Operators*.

. Select the OpenShift Elasticsearch Operator.
.. Click the *Elasticsearch* tab.
.. Click the name of the deployment, for example `elasticsearch`.
.. To view the resources created by the deployment, click the *Resources* tab. .
.. If the `Status` column any problems, check the `status:` output on the *YAML* tab for more information.
.. Navigate back to *Operators* -> *Installed Operators*.

. Select the {JaegerName} Operator.
.. Click the *Jaeger* tab.
.. Click the name of your deployment, for example `jaeger`.
.. To view the resources created by the deployment, click the *Resources* tab.
.. If the `Status` column indicates any problems, check the `status:` output on the *YAML* tab for more information.
.. Navigate to *Operators* -> *Installed Operators*.

. Select the Kiali Operator.
.. Click the *Istio Service Mesh Control Plane* tab.
.. Click the name of your deployment, for example `kiali`.
.. To view the resources created by the deployment, click the *Resources* tab.
.. If the `Status` column any problems, check the `status:` output on the *YAML* tab for more information.

.From the command line

. Run the following command to see if the {SMProductShortName} control plane pods are available and running, where `istio-system` is the namespace where you installed the SMCP.
+
[source,terminal]
----
$ oc get pods -n istio-system
----
+
.Example output
[source,terminal]
----
NAME                                   READY   STATUS    RESTARTS   AGE
grafana-6776785cfc-6fz7t               2/2     Running   0          102s
istio-egressgateway-5f49dd99-l9ppq     1/1     Running   0          103s
istio-ingressgateway-6dc885c48-jjd8r   1/1     Running   0          103s
istiod-basic-6c9cc55998-wg4zq          1/1     Running   0          2m14s
jaeger-6865d5d8bf-zrfss                2/2     Running   0          100s
kiali-579799fbb7-8mwc8                 1/1     Running   0          46s
prometheus-5c579dfb-6qhjk              2/2     Running   0          115s
----
+
. Check the status of the {SMProductShortName} control plane deployment by using the following command. Replace `istio-system` with the namespace where you deployed the SMCP.
+
[source,terminal]
----
$ oc get smcp -n <istio-system>
----
+
The installation has finished successfully when the STATUS column is `ComponentsReady`.
+
.Example output
[source,terminal]
----
NAME    READY   STATUS            PROFILES      VERSION   AGE
basic   10/10   ComponentsReady   ["default"]   2.1.3     4m2s
----

+
If you have modified and redeployed your {SMProductShortName} control plane, the status should read `UpdateSuccessful`.
+
.Example output
[source,terminal]
----
NAME            READY     STATUS             TEMPLATE   VERSION   AGE
basic-install   10/10     UpdateSuccessful   default     v1.1     3d16h
----
+
. If the SMCP status indicates anything other than `ComponentsReady` check the `status:` output in the SCMP resource for more information.
+
[source,terminal]
----
$ oc describe smcp <smcp-name> -n <controlplane-namespace>
----
+
.Example output
+
[source,terminal]
----
$ oc describe smcp basic -n istio-system
----
+
. Check the status of the Jaeger deployment with the following command, where `istio-system` is the namespace where you deployed the SMCP.
+
[source,terminal]
----
$ oc get jaeger -n <istio-system>
----
+
.Example output
[source,terminal]
----
NAME     STATUS    VERSION   STRATEGY   STORAGE   AGE
jaeger   Running   1.30.0    allinone   memory    15m
----
+
. Check the status of the Kiali deployment with the following command, where `istio-system` is the namespace where you deployed the SMCP.
+
[source,terminal]
----
$ oc get kiali -n <istio-system>
----
+
.Example output
[source,terminal]
----
NAME    AGE
kiali   15m
----

:leveloffset: 2

:leveloffset: +3

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
* service_mesh/v2x/ossm-troubleshooting-istio.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-accessing-kiali-console_{context}"]
= Accessing the Kiali console

You can view your application's topology, health, and metrics in the Kiali console. If your service is experiencing problems, the Kiali console lets you view the data flow through your service. You can view insights about the mesh components at different levels, including abstract applications, services, and workloads. Kiali also provides an interactive graph view of your namespace in real time.

To access the Kiali console you must have {SMProductName} installed, Kiali installed and configured.

The installation process creates a route to access the Kiali console.

If you know the URL for the Kiali console, you can access it directly.  If you do not know the URL, use the following directions.

.Procedure for administrators

. Log in to the {product-title} web console with an administrator role.

. Click *Home* -> *Projects*.

. On the *Projects* page, if necessary, use the filter to find the name of your project.

. Click the name of your project, for example, `bookinfo`.

. On the *Project details* page, in the *Launcher* section, click the *Kiali* link.

. Log in to the Kiali console with the same user name and password that you use to access the {product-title} console.
+
When you first log in to the Kiali Console, you see the *Overview* page which displays all the namespaces in your service mesh that you have permission to view.
+
If you are validating the console installation and namespaces have not yet been added to the mesh, there might not be any data to display other than `istio-system`.

.Procedure for developers

. Log in to the {product-title} web console with a developer role.

. Click *Project*.

. On the *Project Details* page, if necessary, use the filter to find the name of your project.

. Click the name of your project, for example, `bookinfo`.

. On the *Project* page, in the *Launcher* section, click the *Kiali* link.

. Click *Log In With OpenShift*.

:leveloffset: 2

:leveloffset: +3

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
* service_mesh/v2x/ossm-troubleshooting-istio.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-accessing-jaeger-console_{context}"]
= Accessing the Jaeger console

To access the Jaeger console you must have {SMProductName} installed, {JaegerName} installed and configured.

The installation process creates a route to access the Jaeger console.

If you know the URL for the Jaeger console, you can access it directly.  If you do not know the URL, use the following directions.

.Procedure from OpenShift console
. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Networking* -> *Routes*.

. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+
. If necessary, use the filter to find the `jaeger` route.  Click the route *Location* to launch the console.

. Click *Log In With OpenShift*.


.Procedure from Kiali console

. Launch the Kiali console.

. Click *Distributed Tracing* in the left navigation pane.

. Click *Log In With OpenShift*.


.Procedure from the CLI

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. To query for details of the route using the command line, enter the following command. In this example, `istio-system` is the {SMProductShortName} control plane namespace.
+
[source,terminal]
----
$ export JAEGER_URL=$(oc get route -n istio-system jaeger -o jsonpath='{.spec.host}')
----
+
. Launch a browser and navigate to ``\https://<JAEGER_URL>``, where `<JAEGER_URL>` is the route that you discovered in the previous step.

. Log in using the same user name and password that you use to access the {Product-title} console.

. If you have added services to the service mesh and have generated traces, you can use the filters and *Find Traces* button to search your trace data.
+
If you are validating the console installation, there is no trace data to display.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/-ossm-troubleshooting-istio.adoc

[id="ossm-troubleshooting-smcp_{context}"]
= Troubleshooting the Service Mesh control plane

If you are experiencing issues while deploying the Service Mesh control plane,

* Ensure that the `ServiceMeshControlPlane` resource is installed in a project that is separate from your services and Operators. This documentation uses the `istio-system` project as an example, but you can deploy your control plane in any project as long as it is separate from the project that contains your Operators and services.

* Ensure that the `ServiceMeshControlPlane` and `Jaeger` custom resources are deployed in the same project. For example, use the `istio-system` project for both.

//* If you selected to install the Elasticsearch Operator in a specific namespace in the cluster instead of selecting *All namespaces in on the cluster (default)*, then OpenShift could not automatically copy the Operator to the istio-system namespace and the {JaegerName} Operator could not call the Elasticsearch Operator during the installation?

//The steps for deploying the service mesh control plane (SMCP) include verifying the deployment in the OpenShift console.

:leveloffset: 2

== Troubleshooting the data plane

The _data plane_ is a set of intelligent proxies that intercept and control all inbound and outbound network communications between services in the service mesh.

{SMProductName} relies on a proxy sidecar within the application’s pod to provide service mesh capabilities to the application.

:leveloffset: +2

// Module included in the following assemblies:
// * service_mesh/v2x/-ossm-troubleshooting-istio.adoc

[id="ossm-troubleshooting-injection_{context}"]
= Troubleshooting sidecar injection

{SMProductName} does not automatically inject proxy sidecars to pods. You must opt in to sidecar injection.

== Troubleshooting Istio sidecar injection

Check to see if automatic injection is enabled in the Deployment for your application. If automatic injection for the Envoy proxy is enabled, there should be a `sidecar.istio.io/inject:"true"` annotation in the `Deployment` resource under `spec.template.metadata.annotations`.

== Troubleshooting Jaeger agent sidecar injection

Check to see if automatic injection is enabled in the Deployment for your application. If automatic injection for the Jaeger agent is enabled, there should be a `sidecar.jaegertracing.io/inject:"true"` annotation in the `Deployment` resource.

:leveloffset: 2

For more information about sidecar injection, see xref:ossm-automatic-sidecar-injection_deploying-applications-ossm[Enabling automatic injection]

:leveloffset: 2

// Module included in the following assemblies:
// * service_mesh/v2x/-ossm-troubleshooting-istio.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-troubleshooting-proxy_{context}"]
= Troubleshooting Envoy proxy

The Envoy proxy intercepts all inbound and outbound traffic for all services in the service mesh. Envoy also collects and reports telemetry on the service mesh. Envoy is deployed as a sidecar to the relevant service in the same pod.

== Enabling Envoy access logs

Envoy access logs are useful in diagnosing traffic failures and flows, and help with end-to-end traffic flow analysis.

To enable access logging for all istio-proxy containers, edit the `ServiceMeshControlPlane` (SMCP) object to add a file name for the logging output.

.Procedure

. Log in to the OpenShift Container Platform CLI as a user with the cluster-admin role. Enter the following command. Then, enter your username and password when prompted.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the {SMProductShortName} control plane, for example `istio-system`.
+
[source,terminal]
----
$ oc project istio-system
----
+
. Edit the `ServiceMeshControlPlane` file.
+
[source,terminal]
----
$ oc edit smcp <smcp_name>
----
+
. As show in the following example, use `name` to specify the file name for the proxy log. If you do not specify a value for `name`, no log entries will be written.
+
[source,yaml]
----
spec:
  proxy:
    accessLogging:
      file:
        name: /dev/stdout     #file name
----

:leveloffset: 2

For more information about troubleshooting pod issues, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#[Investigating pod issues]

:leveloffset: +1

// Module included in the following assemblies:
//
// * security/compliance_operator/co-scans/compliance-operator-troubleshooting.adoc
// * support/getting-support.adoc
// * distr_tracing/distributed-tracing-release-notes.adoc
// * service_mesh/v2x/ossm-support.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * osd_architecture/osd-support.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-0.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-1.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-2.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-3.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-4.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-5.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-6.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-7.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-8.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-9.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-3-0.adoc
// * microshift_support/microshift-getting-support.adoc

[id="support_{context}"]
= Getting support

If you experience difficulty with a procedure described in this documentation, or with {product-title} in general, visit the link:http://access.redhat.com[Red Hat Customer Portal].

From the Customer Portal, you can:

* Search or browse through the Red Hat Knowledgebase of articles and solutions relating to Red Hat products.
* Submit a support case to Red Hat Support.
* Access other product documentation.

To identify issues with your cluster, you can use Insights in {cluster-manager-url}. Insights provides details about issues and, if available, information on how to solve a problem.

// TODO: verify that these settings apply for Service Mesh and OpenShift virtualization, etc.
If you have a suggestion for improving this documentation or have found an
error, submit a link:https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&summary=Documentation_issue&issuetype=1&components=12367614&priority=10200&versions=12385624[Jira issue] for the most relevant documentation component. Please provide specific details, such as the section name and {product-title} version.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * serverless/serverless-support.adoc
// * support/getting-support.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc
// * osd_architecture/osd-support.adoc
// * microshift_support/microshift-getting-support.adoc

:_mod-docs-content-type: CONCEPT
[id="support-knowledgebase-about_{context}"]
= About the Red Hat Knowledgebase

The link:https://access.redhat.com/knowledgebase[Red Hat Knowledgebase] provides rich content aimed at helping you make the most of Red Hat's products and technologies. The Red Hat Knowledgebase consists of articles, product documentation, and videos outlining best practices on installing, configuring, and using Red Hat products. In addition, you can search for solutions to known issues, each providing concise root cause descriptions and remedial steps.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * serverless/serverless-support.adoc
// * support/getting-support.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc
// * osd_architecture/osd-support.adoc
// * microshift_support/microshift-getting-support.adoc

:_mod-docs-content-type: PROCEDURE
[id="support-knowledgebase-search_{context}"]
= Searching the Red Hat Knowledgebase

In the event of an {product-title} issue, you can perform an initial search to determine if a solution already exists within the Red Hat Knowledgebase.

.Prerequisites

* You have a Red Hat Customer Portal account.

.Procedure

. Log in to the link:http://access.redhat.com[Red Hat Customer Portal].

. Click *Search*.

. In the search field, input keywords and strings relating to the problem, including:
+
* {product-title} components (such as *etcd*)
* Related procedure (such as *installation*)
* Warnings, error messages, and other outputs related to explicit failures

. Click the *Enter* key.

. Optional: Select the *{product-title}* product filter.

. Optional: Select the *Documentation* content type filter.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * service_mesh/v2x/servicemesh-release-notes.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc


:_mod-docs-content-type: CONCEPT
[id="ossm-about-collecting-ossm-data_{context}"]
= About collecting service mesh data

You can use the `oc adm must-gather` CLI command to collect information about your cluster, including features and objects associated with {SMProductName}.

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.

* The {product-title} CLI (`oc`) installed.

.Procedure

. To collect {SMProductName} data with `must-gather`, you must specify the {SMProductName} image.
+
[source,terminal]
----
$ oc adm must-gather --image=registry.redhat.io/openshift-service-mesh/istio-must-gather-rhel8:2.4
----
+
. To collect {SMProductName} data for a specific {SMProductShortName} control plane namespace with `must-gather`, you must specify the {SMProductName} image and namespace. In this example, after `gather,` replace `<namespace>` with your {SMProductShortName} control plane namespace, such as `istio-system`.
+
[source,terminal]
----
$ oc adm must-gather --image=registry.redhat.io/openshift-service-mesh/istio-must-gather-rhel8:2.4 gather <namespace>
----
+
This creates a local directory that contains the following items:

* The Istio Operator namespace and its child objects
* All control plane namespaces and their children objects
* All namespaces and their children objects that belong to any service mesh
* All Istio custom resource definitions (CRD)
* All Istio CRD objects, such as VirtualServices, in a given namespace
* All Istio webhooks

:leveloffset: 2

For prompt support, supply diagnostic information for both {product-title} and {SMProductName}.

:leveloffset: +2

// Module included in the following assemblies:
//
// * serverless/serverless-support.adoc
// * support/getting-support.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc
// * osd_architecture/osd-support.adoc

:_mod-docs-content-type: PROCEDURE
[id="support-submitting-a-case_{context}"]
= Submitting a support case

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the OpenShift CLI (`oc`).
* You have a Red Hat Customer Portal account.
* You have a Red Hat Standard or Premium subscription.

.Procedure

. Log in to link:https://access.redhat.com/support/cases/#/case/list[the *Customer Support* page] of the Red Hat Customer Portal.

. Click *Get support*.

. On the *Cases* tab of the *Customer Support* page:

.. Optional: Change the pre-filled account and owner details if needed.

.. Select the appropriate category for your issue, such as *Bug or Defect*, and click *Continue*.

. Enter the following information:

.. In the *Summary* field, enter a concise but descriptive problem summary and further details about the symptoms being experienced, as well as your expectations.

.. Select *{product-title}* from the *Product* drop-down menu.

.. Select *{product-version}* from the *Version* drop-down.

. Review the list of suggested Red Hat Knowledgebase solutions for a potential match against the problem that is being reported. If the suggested articles do not address the issue, click *Continue*.

. Review the updated list of suggested Red Hat Knowledgebase solutions for a potential match against the problem that is being reported. The list is refined as you provide more information during the case creation process. If the suggested articles do not address the issue, click *Continue*.

. Ensure that the account information presented is as expected, and if not, amend accordingly.

. Check that the autofilled {product-title} Cluster ID is correct. If it is not, manually obtain your cluster ID.
+
* To manually obtain your cluster ID using the {product-title} web console:
.. Navigate to *Home* -> *Overview*.
.. Find the value in the *Cluster ID* field of the *Details* section.
+
* Alternatively, it is possible to open a new support case through the {product-title} web console and have your cluster ID autofilled.
.. From the toolbar, navigate to *(?) Help* -> *Open Support Case*.
.. The *Cluster ID* value is autofilled.
+
* To obtain your cluster ID using the OpenShift CLI (`oc`), run the following command:
+
[source,terminal]
----
$ oc get clusterversion -o jsonpath='{.items[].spec.clusterID}{"\n"}'
----

. Complete the following questions where prompted and then click *Continue*:
+
* What are you experiencing? What are you expecting to happen?
* Define the value or impact to you or the business.
* Where are you experiencing this behavior? What environment?
* When does this behavior occur? Frequency? Repeatedly? At certain times?

. Upload relevant diagnostic data files and click *Continue*.
It is recommended to include data gathered using the `oc adm must-gather` command as a starting point, plus any issue specific data that is not collected by that command.

. Input relevant case management details and click *Continue*.

. Preview the case details and click *Submit*.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-reference"]
= Service Mesh control plane configuration reference
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-reference

toc::[]

You can customize your {SMProductName} by modifying the default `ServiceMeshControlPlane` (SMCP) resource or by creating a completely custom SMCP resource. This reference section documents the configuration options available for the SMCP resource.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-cr-example_{context}"]
= {SMProductShortName} Control plane parameters

The following table lists the top-level parameters for the `ServiceMeshControlPlane` resource.

.`ServiceMeshControlPlane` resource parameters
|===
|Name |Description |Type

|`apiVersion`
|APIVersion defines the versioned schema of this representation of an object. Servers convert recognized schemas to the latest internal value, and may reject unrecognized values. The value for the `ServiceMeshControlPlane` version 2.0 is `maistra.io/v2`.
|The value for `ServiceMeshControlPlane` version 2.0 is `maistra.io/v2`.

|`kind`
|Kind is a string value that represents the REST resource this object represents.
|`ServiceMeshControlPlane` is the only valid value for a ServiceMeshControlPlane.

|`metadata`
|Metadata about this `ServiceMeshControlPlane` instance. You can provide a name for your {SMProductShortName} control plane installation to keep track of your work, for example, `basic`.
|string

|`spec`
|The specification of the desired state of this `ServiceMeshControlPlane`. This includes the configuration options for all components that comprise the {SMProductShortName} control plane.
|For more information, see Table 2.

|`status`
|The current status of this `ServiceMeshControlPlane` and the components that comprise the {SMProductShortName} control plane.
|For more information, see Table 3.
|===

The following table lists the specifications for the `ServiceMeshControlPlane` resource. Changing these parameters configures {SMProductName} components.

.`ServiceMeshControlPlane` resource spec
|===
|Name |Description |Configurable parameters

|`addons`
| The `addons` parameter configures additional features beyond core {SMProductShortName} control plane components, such as visualization, or metric storage.
|`3scale`, `grafana`, `jaeger`, `kiali`, and `prometheus`.

|`cluster`
|The `cluster` parameter sets the general configuration of the cluster (cluster name, network name, multi-cluster, mesh expansion, etc.)
|`meshExpansion`, `multiCluster`, `name`, and `network`

|`gateways`
| You use the `gateways` parameter to configure ingress and egress gateways for the mesh.
|`enabled`, `additionalEgress`, `additionalIngress`, `egress`, `ingress`, and  `openshiftRoute`

|`general`
|The `general` parameter represents general {SMProductShortName} control plane configuration that does not fit anywhere else.
|`logging` and `validationMessages`

|`policy`
|You use the `policy` parameter to configure policy checking for the {SMProductShortName} control plane. Policy checking can be enabled by setting `spec.policy.enabled` to `true`.
|`mixer` `remote`, or `type`. `type` can be set to `Istiod`, `Mixer` or `None`.

|`profiles`
|You select the `ServiceMeshControlPlane` profile to use for default values using the `profiles` parameter.
|`default`

|`proxy`
| You use the `proxy` parameter to configure the default behavior for sidecars.
|`accessLogging`, `adminPort`, `concurrency`, and `envoyMetricsService`

|`runtime`
| You use the `runtime` parameter to configure the {SMProductShortName} control plane components.
|`components`, and `defaults`

|`security`
| The `security` parameter allows you to configure aspects of security for the {SMProductShortName} control plane.
|`certificateAuthority`, `controlPlane`, `identity`, `dataPlane` and `trust`

|`techPreview`
|The `techPreview` parameter enables early access to features that are in technology preview.
|N/A

|`telemetry`
|If `spec.mixer.telemetry.enabled` is set to `true`, telemetry is enabled.
|`mixer`, `remote`, and `type`. `type` can be set to `Istiod`, `Mixer` or `None`.

|`tracing`
|You use the `tracing` parameter to enables distributed tracing for the mesh.
|`sampling`, `type`. `type` can be set to `Jaeger` or `None`.

|`version`
|You use the `version` parameter to specify what Maistra version of the {SMProductShortName} control plane to install. When creating a `ServiceMeshControlPlane` with an empty version, the admission webhook sets the version to the current version. New `ServiceMeshControlPlanes` with an empty version are set to `v2.0`. Existing `ServiceMeshControlPlanes` with an empty version keep their setting.
|string
|===

`ControlPlaneStatus` represents the current state of your service mesh.

.`ServiceMeshControlPlane` resource `ControlPlaneStatus`
|===
|Name |Description |Type

|`annotations`
|The `annotations` parameter stores additional, usually redundant status information, such as the number of components deployed by the `ServiceMeshControlPlane`. These statuses are used by the command line tool, `oc`, which does not yet allow counting objects in JSONPath expressions.
|Not configurable

|`conditions`
|Represents the latest available observations of the object's current state. `Reconciled` indicates whether the operator has finished reconciling the actual state of deployed components with the configuration in the `ServiceMeshControlPlane` resource. `Installed` indicates whether the {SMProductShortName} control plane has been installed. `Ready` indicates whether all {SMProductShortName} control plane components are ready.
|string

|`components`
|Shows the status of each deployed {SMProductShortName} control plane component.
|string

|`appliedSpec`
|The resulting specification of the configuration options after all profiles have been applied.
|`ControlPlaneSpec`

|`appliedValues`
|The resulting values.yaml used to generate the charts.
|`ControlPlaneSpec`

|`chartVersion`
|The version of the charts that were last processed for this resource.
|string

|`observedGeneration`
|The generation observed by the controller during the most recent reconciliation. The information in the status pertains to this particular generation of the object. The `status.conditions` are not up-to-date if the `status.observedGeneration` field doesn't match `metadata.generation`.
|integer

|`operatorVersion`
|The version of the operator that last processed this resource.
|string

|`readiness`
|The readiness status of components & owned resources.
|string
|===


This example `ServiceMeshControlPlane` definition contains all of the supported parameters.

.Example `ServiceMeshControlPlane` resource
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  proxy:
    runtime:
      container:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 128Mi
  tracing:
    type: Jaeger
  gateways:
    ingress: # istio-ingressgateway
      service:
        type: ClusterIP
        ports:
        - name: status-port
          port: 15020
        - name: http2
          port: 80
          targetPort: 8080
        - name: https
          port: 443
          targetPort: 8443
      meshExpansionPorts: []
    egress: # istio-egressgateway
      service:
        type: ClusterIP
        ports:
        - name: status-port
          port: 15020
        - name: http2
          port: 80
          targetPort: 8080
        - name: https
          port: 443
          targetPort: 8443
    additionalIngress:
      some-other-ingress-gateway: {}
    additionalEgress:
      some-other-egress-gateway: {}

  policy:
    type: Mixer
    mixer: # only applies if policy.type: Mixer
      enableChecks: true
      failOpen: false

  telemetry:
    type: Istiod # or Mixer
    mixer: # only applies if telemetry.type: Mixer, for v1 telemetry
      sessionAffinity: false
      batching:
        maxEntries: 100
        maxTime: 1s
      adapters:
        kubernetesenv: true
        stdio:
          enabled: true
          outputAsJSON: true
  addons:
    grafana:
      enabled: true
      install:
        config:
          env: {}
          envSecrets: {}
        persistence:
          enabled: true
          storageClassName: ""
          accessMode: ReadWriteOnce
          capacity:
            requests:
              storage: 5Gi
        service:
          ingress:
            contextPath: /grafana
            tls:
              termination: reencrypt
    kiali:
      name: kiali
      enabled: true
      install: # install kiali CR if not present
        dashboard:
          viewOnly: false
          enableGrafana: true
          enableTracing: true
          enablePrometheus: true
      service:
        ingress:
          contextPath: /kiali
    jaeger:
      name: jaeger
      install:
        storage:
          type: Elasticsearch # or Memory
          memory:
            maxTraces: 100000
          elasticsearch:
            nodeCount: 3
            storage: {}
            redundancyPolicy: SingleRedundancy
            indexCleaner: {}
        ingress: {} # jaeger ingress configuration
  runtime:
    components:
      pilot:
        deployment:
          replicas: 2
        pod:
          affinity: {}
        container:
          resources:
            requests:
              cpu: 100m
              memory: 128Mi
            limits:
              cpu: 500m
              memory: 128Mi
      grafana:
        deployment: {}
        pod: {}
      kiali:
        deployment: {}
        pod: {}
----

:leveloffset: 2

== spec parameters

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-reference-smcp.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-cr-general_{context}"]
= general parameters

Here is an example that illustrates the `spec.general` parameters for the `ServiceMeshControlPlane` object and a description of the available parameters with appropriate values.

.Example general parameters
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  general:
    logging:
      componentLevels: {}
          # misc: error
      logAsJSON: false
    validationMessages: true
----


.Istio general parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value

|logging:
|Use to configure logging for the {SMProductShortName} control plane components.
|
|N/A

|logging:
 componentLevels:
|Use to specify the component logging level.
|Possible values: `trace`, `debug`, `info`, `warning`, `error`, `fatal`, `panic`.
|N/A

|logging:
 logAsJSON:
|Use to enable or disable JSON logging.
|`true`/`false`
|N/A

|validationMessages:
|Use to enable or disable validation messages to the status fields of istio.io resources. This can be useful for detecting configuration errors in resources.
|`true`/`false`
|N/A
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-reference-smcp.adoc

[id="ossm-cr-profiles_{context}"]
= profiles parameters

You can create reusable configurations with `ServiceMeshControlPlane` object profiles. If you do not configure the `profile` setting, {SMProductName} uses the default profile.

Here is an example that illustrates the `spec.profiles` parameter for the `ServiceMeshControlPlane` object:

.Example profiles parameters
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  profiles:
  - YourProfileName
----

:leveloffset: 2

For information about creating profiles, see the xref:ossm-control-plane-profiles_ossm-profiles-users[Creating control plane profiles].

For more detailed examples of security configuration, see xref:ossm-security-mtls_ossm-security[Mutual Transport Layer Security (mTLS)].

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-reference-smcp.adoc

[id="ossm-cr-techPreview_{context}"]
= techPreview parameters

The `spec.techPreview` parameter enables early access to features that are in Technology Preview.

[IMPORTANT]
====
Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-reference-smcp.adoc

[id="ossm-cr-tracing_{context}"]
= tracing parameters

The following example illustrates the `spec.tracing` parameters for the `ServiceMeshControlPlane` object, and a description of the available parameters with appropriate values.

.Example tracing parameters
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 100
    type: Jaeger
----

.Istio tracing parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value

|tracing:
 sampling:

|The sampling rate determines how often the Envoy proxy generates a trace. You use the sampling rate to control what percentage of requests get reported to your tracing system.
|Integer values between 0 and 10000 representing increments of 0.01% (0 to 100%). For example, setting the value to `10` samples 0.1% of requests, setting the value to `100` will sample 1% of requests setting the value to `500` samples 5% of requests, and a setting of `10000` samples 100% of requests.
|`10000` (100% of traces)

|tracing:
 type:
|Currently the only tracing type that is supported is `Jaeger`. Jaeger is enabled by default. To disable tracing, set the `type` parameter to `None`.
|`None`, `Jaeger`
|`Jaeger`
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-reference-smcp.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-cr-version_{context}"]
= version parameter

The {SMProductName} Operator supports installation of different versions of the `ServiceMeshControlPlane`. You use the version parameter to specify what version of the {SMProductShortName} control plane to install. If you do not specify a version parameter when creating your SMCP, the Operator sets the value to the latest version: ({MaistraVersion}). Existing `ServiceMeshControlPlane` objects keep their version setting during upgrades of the Operator.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-threescale_{context}"]

= 3scale configuration

The following table explains the parameters for the 3scale Istio Adapter in the `ServiceMeshControlPlane` resource.

.Example 3scale parameters
[source,yaml]
----
spec:
  addons:
    3Scale:
      enabled: false
      PARAM_THREESCALE_LISTEN_ADDR: 3333
      PARAM_THREESCALE_LOG_LEVEL: info
      PARAM_THREESCALE_LOG_JSON: true
      PARAM_THREESCALE_LOG_GRPC: false
      PARAM_THREESCALE_REPORT_METRICS: true
      PARAM_THREESCALE_METRICS_PORT: 8080
      PARAM_THREESCALE_CACHE_TTL_SECONDS: 300
      PARAM_THREESCALE_CACHE_REFRESH_SECONDS: 180
      PARAM_THREESCALE_CACHE_ENTRIES_MAX: 1000
      PARAM_THREESCALE_CACHE_REFRESH_RETRIES: 1
      PARAM_THREESCALE_ALLOW_INSECURE_CONN: false
      PARAM_THREESCALE_CLIENT_TIMEOUT_SECONDS: 10
      PARAM_THREESCALE_GRPC_CONN_MAX_SECONDS: 60
      PARAM_USE_CACHED_BACKEND: false
      PARAM_BACKEND_CACHE_FLUSH_INTERVAL_SECONDS: 15
      PARAM_BACKEND_CACHE_POLICY_FAIL_CLOSED: true
----

.3scale parameters
|===
|Parameter |Description |Values |Default value

|`enabled`
|Whether to use the 3scale adapter
|`true`/`false`
|`false`

|`PARAM_THREESCALE_LISTEN_ADDR`
|Sets the listen address for the gRPC server
|Valid port number
|`3333`

|`PARAM_THREESCALE_LOG_LEVEL`
|Sets the minimum log output level.
|`debug`, `info`, `warn`, `error`, or `none`
|`info`

|`PARAM_THREESCALE_LOG_JSON`
|Controls whether the log is formatted as JSON
|`true`/`false`
|`true`

|`PARAM_THREESCALE_LOG_GRPC`
|Controls whether the log contains gRPC info
|`true`/`false`
|`true`

|`PARAM_THREESCALE_REPORT_METRICS`
|Controls whether 3scale system and backend metrics are collected and reported to Prometheus
|`true`/`false`
|`true`

|`PARAM_THREESCALE_METRICS_PORT`
|Sets the port that the 3scale `/metrics` endpoint can be scrapped from
|Valid port number
|`8080`

|`PARAM_THREESCALE_CACHE_TTL_SECONDS`
|Time period, in seconds, to wait before purging expired items from the cache
|Time period in seconds
|`300`

|`PARAM_THREESCALE_CACHE_REFRESH_SECONDS`
|Time period before expiry when cache elements are attempted to be refreshed
|Time period in seconds
|`180`

|`PARAM_THREESCALE_CACHE_ENTRIES_MAX`
|Max number of items that can be stored in the cache at any time. Set to `0` to disable caching
|Valid number
|`1000`

|`PARAM_THREESCALE_CACHE_REFRESH_RETRIES`
|The number of times unreachable hosts are retried during a cache update loop
|Valid number
|`1`

|`PARAM_THREESCALE_ALLOW_INSECURE_CONN`
|Allow to skip certificate verification when calling `3scale` APIs. Enabling this is not recommended.
|`true`/`false`
|`false`

|`PARAM_THREESCALE_CLIENT_TIMEOUT_SECONDS`
|Sets the number of seconds to wait before terminating requests to 3scale System and Backend
|Time period in seconds
|`10`

|`PARAM_THREESCALE_GRPC_CONN_MAX_SECONDS`
|Sets the maximum amount of seconds (+/-10% jitter) a connection may exist before it is closed
|Time period in seconds
|60


|`PARAM_USE_CACHE_BACKEND`
|If true, attempt to create an in-memory apisonator cache for authorization requests
|`true`/`false`
|`false`

|`PARAM_BACKEND_CACHE_FLUSH_INTERVAL_SECONDS`
|If the backend cache is enabled, this sets the interval in seconds for flushing the cache against 3scale
|Time period in seconds
|15

|`PARAM_BACKEND_CACHE_POLICY_FAIL_CLOSED`
|Whenever the backend cache cannot retrieve authorization data, whether to deny (closed) or allow (open) requests
|`true`/`false`
|`true`
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-reference-smcp.adoc

:_mod-docs-content-type: REFERENCE
[id="ossm-cr-status_{context}"]
= status parameter

The `status` parameter describes the current state of your service mesh. This information is generated by the Operator and is read-only.

.Istio status parameters
|===
|Name |Description |Type

|`observedGeneration`
|The generation observed by the controller during the most recent reconciliation. The information in the status pertains to this particular generation of the object. The `status.conditions` are not up-to-date if the `status.observedGeneration` field doesn't match `metadata.generation`.
|integer

|`annotations`
|The `annotations` parameter stores additional, usually redundant status information, such as the number of components deployed by the `ServiceMeshControlPlane` object. These statuses are used by the command line tool, `oc`, which does not yet allow counting objects in JSONPath expressions.
|Not configurable

|`readiness`
|The readiness status of components and owned resources.
|string

|`operatorVersion`
|The version of the Operator that last processed this resource.
|string

|`components`
|Shows the status of each deployed {SMProductShortName} control plane component.
|string

|`appliedSpec`
|The resulting specification of the configuration options after all profiles have been applied.
|`ControlPlaneSpec`

|`conditions`
|Represents the latest available observations of the object's current state. `Reconciled` indicates that the Operator has finished reconciling the actual state of deployed components with the configuration in the `ServiceMeshControlPlane` resource. `Installed` indicates that the {SMProductShortName} control plane has been installed. `Ready` indicates that all {SMProductShortName} control plane components are ready.
|string

|`chartVersion`
|The version of the charts that were last processed for this resource.
|string

|`appliedValues`
|The resulting `values.yaml` file that was used to generate the charts.
|`ControlPlaneSpec`
|===

:leveloffset: 2

[id="additional-resources_ossm-reference"]
[role="_additional-resources"]
== Additional resources

* For more information about how to configure the features in the `ServiceMeshControlPlane` resource, see the following links:

** xref:ossm-security-mtls_ossm-security[Security]

** xref:ossm-routing-bookinfo_traffic-management[Traffic management]

** xref:ossm-observability[Metrics and traces]

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="kiali-config-ref"]
= Kiali configuration reference
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: kiali-config-ref

toc::[]

When the {SMProductShortName} Operator creates the `ServiceMeshControlPlane` it also processes the Kiali resource. The Kiali Operator then uses this object when creating Kiali instances.

// The following include statements pull in the module files for the assembly.
:leveloffset: +1

// Module included in the following assemblies:
//* service_mesh/v2x/ossm-reference-kiali.adoc
:_mod-docs-content-type: REFERENCE
[id="ossm-smcp-kiali_{context}"]
= Specifying Kiali configuration in the SMCP

You can configure Kiali under the `addons` section of the `ServiceMeshControlPlane` resource. Kiali is enabled by default. To disable Kiali, set `spec.addons.kiali.enabled` to `false`.

You can specify your Kiali configuration in either of two ways:

* Specify the Kiali configuration in the `ServiceMeshControlPlane` resource under `spec.addons.kiali.install`. This approach has some limitations, because the complete list of Kiali configurations is not available in the SMCP.

* Configure and deploy a Kiali instance and specify the name of the Kiali resource as the value for `spec.addons.kiali.name` in the `ServiceMeshControlPlane` resource. You must create the CR in the same namespace as the {SMProductShortName} control plane, for example, `istio-system`. If a Kiali resource matching the value of `name` exists, the control plane will configure that Kiali resource for use with the control plane. This approach lets you fully customize your Kiali configuration in the Kiali resource. Note that with this approach, various fields in the Kiali resource are overwritten by the {SMProductShortName} Operator, specifically, the `accessible_namespaces` list, as well as the endpoints for Grafana, Prometheus, and tracing.

.Example SMCP parameters for Kiali
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  addons:
    kiali:
      name: kiali
      enabled: true
      install:
        dashboard:
          viewOnly: false
          enableGrafana: true
          enableTracing: true
          enablePrometheus: true
        service:
          ingress:
            contextPath: /kiali
----

.`ServiceMeshControlPlane` Kiali parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|spec:
  addons:
    kiali:
      name:
|Name of Kiali custom resource. If a Kiali CR matching the value of `name` exists, the {SMProductShortname} Operator will use that CR for the installation. If no Kiali CR exists, the Operator will create one using this `name` and the configuration options specified in the SMCP.
|string
|`kiali`

|kiali:
  enabled:
|This parameter enables or disables Kiali. Kiali is enabled by default.
|`true`/`false`
|`true`

|kiali:
  install:
|Install a Kiali resource if the named Kiali resource is not present. The `install` section is ignored if `addons.kiali.enabled` is set to `false`.
|
|

|kiali:
  install:
    dashboard:
|Configuration parameters for the dashboards shipped with Kiali.
|
|

|kiali:
  install:
    dashboard:
      viewOnly:
|This parameter enables or disables view-only mode for the Kiali console. When view-only mode is enabled, users cannot use the Kiali console to make changes to the {SMProductShortname}.
|`true`/`false`
|`false`

|kiali:
  install:
    dashboard:
      enableGrafana:
|Grafana endpoint configured based on `spec.addons.grafana` configuration.
|`true`/`false`
|`true`

|kiali:
  install:
    dashboard:
      enablePrometheus:
|Prometheus endpoint configured based on `spec.addons.prometheus` configuration.
|`true`/`false`
|`true`

|kiali:
  install:
    dashboard:
      enableTracing:
|Tracing endpoint configured based on Jaeger custom resource configuration.
|`true`/`false`
|`true`

|kiali:
  install:
    service:
|Configuration parameters for the Kubernetes service associated with the Kiali installation.
|
|

|kiali:
  install:
    service:
      metadata:
|Use to specify additional metadata to apply to resources.
|N/A
|N/A

|kiali:
  install:
    service:
      metadata:
        annotations:
|Use to specify additional annotations to apply to the component's service.
|string
|N/A

|kiali:
  install:
    service:
      metadata:
        labels:
|Use to specify additional labels to apply to the component's service.
|string
|N/A

|kiali:
  install:
    service:
      ingress:
|Use to specify details for accessing the component’s service through an OpenShift Route.
|N/A
|N/A

|kiali:
  install:
    service:
      ingress:
        metadata:
          annotations:
|Use to specify additional annotations to apply to the component's service ingress.
|string
|N/A

|kiali:
  install:
    service:
      ingress:
        metadata:
          labels:
|Use to specify additional labels to apply to the component's service ingress.
|string
|N/A

|kiali:
  install:
    service:
      ingress:
        enabled:
|Use to customize an OpenShift Route for the service associated with a component.
|`true`/`false`
|`true`

|kiali:
  install:
    service:
      ingress:
        contextPath:
|Use to specify the context path to the service.
|string
|N/A

|install:
  service:
    ingress:
      hosts:
|Use to specify a single hostname per OpenShift route. An empty hostname implies a default hostname for the Route.
|string
|N/A

|install:
  service:
    ingress:
      tls:
|Use to configure the TLS for the OpenShift route.
|
|N/A

|kiali:
  install:
    service:
      nodePort:
|Use to specify the `nodePort` for the component's service `Values.<component>.service.nodePort.port`
|integer
|N/A
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc
:_mod-docs-content-type: CONCEPT
[id="ossm-specifying-external-kiali_{context}"]
= Specifying Kiali configuration in a Kiali custom resource

You can fully customize your Kiali deployment by configuring Kiali in the Kiali custom resource (CR) rather than in the `ServiceMeshControlPlane` (SMCP) resource. This configuration is sometimes called an "external Kiali" since the configuration is specified outside of the SMCP.

[NOTE]
====
You must deploy the `ServiceMeshControlPlane` and Kiali custom resources in the same namespace. For example, `istio-system`.
====

You can configure and deploy a Kiali instance and then specify the `name` of the Kiali resource as the value for `spec.addons.kiali.name` in the SMCP resource. If a Kiali CR matching the value of `name` exists, the {SMProductShortName} control plane will use the existing installation. This approach lets you fully customize your Kiali configuration.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="jaeger-config-ref"]
= Jaeger configuration reference
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: jaeger-config-reference

toc::[]

When the {SMProductShortName} Operator deploys the `ServiceMeshControlPlane` resource, it can also create the resources for distributed tracing. {SMProductShortName} uses Jaeger for distributed tracing.

[IMPORTANT]
====
Jaeger does not use FIPS validated cryptographic modules.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc


[id="ossm-enabling-tracing_{context}"]
= Enabling and disabling tracing

You enable distributed tracing by specifying a tracing type and a sampling rate in the `ServiceMeshControlPlane` resource.

.Default `all-in-one` Jaeger parameters
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 100
    type: Jaeger
----

Currently, the only tracing type that is supported is `Jaeger`.

Jaeger is enabled by default. To disable tracing, set `type` to `None`.

The sampling rate determines how often the Envoy proxy generates a trace. You can use the sampling rate option to control what percentage of requests get reported to your tracing system. You can configure this setting based upon your traffic in the mesh and the amount of tracing data you want to collect. You configure `sampling` as a scaled integer representing 0.01% increments. For example, setting the value to `10` samples 0.1% of traces, setting the value to `500` samples 5% of traces, and a setting of `10000` samples 100% of traces.

[NOTE]
====
The SMCP sampling configuration option controls the Envoy sampling rate. You configure the Jaeger trace sampling rate in the Jaeger custom resource.
====

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-specifying-jaeger-configuration_{context}"]
= Specifying Jaeger configuration in the SMCP

You configure Jaeger under the `addons` section of the `ServiceMeshControlPlane` resource. However, there are some limitations to what you can configure in the SMCP.

When the SMCP passes configuration information to the {JaegerName} Operator, it triggers one of three deployment strategies: `allInOne`, `production`, or `streaming`.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-custom-resources.adoc

[id="ossm-deploying-jaeger_{context}"]
= Deploying the distributed tracing platform

The {JaegerShortName} has predefined deployment strategies. You specify a deployment strategy in the Jaeger custom resource (CR) file. When you create an instance of the {JaegerShortName}, the {JaegerName} Operator uses this configuration file to create the objects necessary for the deployment.

The {JaegerName} Operator currently supports the following deployment strategies:

* *allInOne* (default) - This strategy is intended for development, testing, and demo purposes and it is not for production use. The main back-end components, Agent, Collector, and Query service, are all packaged into a single executable, which is configured (by default) to use in-memory storage. You can configure this deployment strategy in the SMCP.
+
[NOTE]
====
In-memory storage is not persistent, which means that if the Jaeger instance shuts down, restarts, or is replaced, your trace data will be lost. And in-memory storage cannot be scaled, since each pod has its own memory. For persistent storage, you must use the `production` or `streaming` strategies, which use Elasticsearch as the default storage.
====

* *production* - The production strategy is intended for production environments, where long term storage of trace data is important, and a more scalable and highly available architecture is required. Each back-end component is therefore deployed separately. The Agent can be injected as a sidecar on the instrumented application. The Query and Collector services are configured with a supported storage type, which is currently Elasticsearch. Multiple instances of each of these components can be provisioned as required for performance and resilience purposes. You can configure this deployment strategy in the SMCP, but in order to be fully customized, you must specify your configuration in the Jaeger CR and link that to the SMCP.

* *streaming* - The streaming strategy is designed to augment the production strategy by providing a streaming capability that sits between the Collector and the Elasticsearch back-end storage. This provides the benefit of reducing the pressure on the back-end storage, under high load situations, and enables other trace post-processing capabilities to tap into the real-time span data directly from the streaming platform (https://access.redhat.com/documentation/en-us/red_hat_amq/7.6/html/using_amq_streams_on_openshift/index[AMQ Streams]/ https://kafka.apache.org/documentation/[Kafka]). You cannot configure this deployment strategy in the SMCP; you must configure a Jaeger CR and link that to the SMCP.

[NOTE]
====
The streaming strategy requires an additional Red Hat subscription for AMQ Streams.
====

[id="ossm-deploying-jaeger-default_{context}"]
== Default {JaegerShortName} deployment

If you do not specify Jaeger configuration options, the `ServiceMeshControlPlane` resource will use the `allInOne` Jaeger deployment strategy by default. When using the default `allInOne` deployment strategy, set `spec.addons.jaeger.install.storage.type` to `Memory`. You can accept the defaults or specify additional configuration options under `install`.

.Control plane default Jaeger parameters (Memory)
[source,yaml, subs="attributes,verbatim"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 10000
    type: Jaeger
  addons:
    jaeger:
      name: jaeger
      install:
        storage:
          type: Memory
----

[id="ossm-deploying-jaeger-production-min_{context}"]
== Production {JaegerShortName} deployment (minimal)

To use the default settings for the `production` deployment strategy, set `spec.addons.jaeger.install.storage.type` to `Elasticsearch` and specify additional configuration options under `install`. Note that the SMCP only supports configuring Elasticsearch resources and image name.

.Control plane default Jaeger parameters (Elasticsearch)
[source,yaml, subs="attributes"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 10000
    type: Jaeger
  addons:
    jaeger:
      name: jaeger  #name of Jaeger CR
      install:
        storage:
          type: Elasticsearch
        ingress:
          enabled: true
  runtime:
    components:
      tracing.jaeger.elasticsearch: # only supports resources and image name
        container:
          resources: {}
----


[id="ossm-deploying-jaeger-production_{context}"]
== Production {JaegerShortName} deployment (fully customized)

The SMCP supports only minimal Elasticsearch parameters. To fully customize your production environment and access all of the Elasticsearch configuration parameters, use the Jaeger custom resource (CR) to configure Jaeger.

Create and configure your Jaeger instance and set `spec.addons.jaeger.name` to the name of the Jaeger instance, in this example: `MyJaegerInstance`.

.Control plane with linked Jaeger production CR
[source,yaml, subs="attributes"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 1000
    type: Jaeger
  addons:
    jaeger:
      name: MyJaegerInstance #name of Jaeger CR
      install:
        storage:
          type: Elasticsearch
        ingress:
          enabled: true
----

[id="ossm-deploying-jaeger-streaming_{context}"]
== Streaming Jaeger deployment

To use the `streaming` deployment strategy, you create and configure your Jaeger instance first, then set `spec.addons.jaeger.name` to the name of the Jaeger instance, in this example: `MyJaegerInstance`.

.Control plane with linked Jaeger streaming CR
[source,yaml, subs="attributes"]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
spec:
  version: v{MaistraVersion}
  tracing:
    sampling: 1000
    type: Jaeger
  addons:
    jaeger:
      name: MyJaegerInstance  #name of Jaeger CR
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-specifying-external-jaeger_{context}"]
= Specifying Jaeger configuration in a Jaeger custom resource

You can fully customize your Jaeger deployment by configuring Jaeger in the Jaeger custom resource (CR) rather than in the `ServiceMeshControlPlane` (SMCP) resource. This configuration is sometimes referred to as an "external Jaeger" since the configuration is specified outside of the SMCP.

[NOTE]
====
You must deploy the SMCP and Jaeger CR in the same namespace. For example, `istio-system`.
====

You can configure and deploy a standalone Jaeger instance and then specify the `name` of the Jaeger resource as the value for `spec.addons.jaeger.name` in the SMCP resource. If a Jaeger CR matching the value of `name` exists, the {SMProductShortName} control plane will use the existing installation. This approach lets you fully customize your Jaeger configuration.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: CONCEPT
[id="distr-tracing-deployment-best-practices_{context}"]
= Deployment best practices

* {DTProductName} instance names must be unique. If you want to have multiple {JaegerName} instances and are using sidecar injected agents, then the {JaegerName} instances should have unique names, and the injection annotation should explicitly specify the {JaegerName} instance name the tracing data should be reported to.

* If you have a multitenant implementation and tenants are separated by namespaces, deploy a {JaegerName} instance to each tenant namespace.

:leveloffset: 2

For information about configuring persistent storage, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#[Understanding persistent storage] and the appropriate configuration topic for your chosen storage option.

:leveloffset: +2

////
This module included in the following assemblies:
service_mesh/v2x/ossm-reference-jaeger.adoc
////
:_mod-docs-content-type: CONCEPT
[id="distr-tracing-config-security-ossm_{context}"]
= Configuring distributed tracing security for service mesh

The {JaegerShortName} uses OAuth for default authentication. However {SMProductName} uses a secret called `htpasswd` to facilitate communication between dependent services such as Grafana, Kiali, and the {JaegerShortName}. When you configure your {JaegerShortName} in the `ServiceMeshControlPlane` the {SMProductShortName} automatically configures security settings to use `htpasswd`.

If you are specifying your {JaegerShortName} configuration in a Jaeger custom resource, you must manually configure the `htpasswd` settings and ensure the `htpasswd` secret is mounted into your Jaeger instance so that Kiali can communicate with it.

:leveloffset: 2

:leveloffset: +3

////
This module included in the following assemblies:
service_mesh/v2x/ossm-reference-jaeger.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="distr-tracing-config-security-ossm-web_{context}"]
= Configuring distributed tracing security for service mesh from the web console

You can modify the Jaeger resource to configure {JaegerShortName} security for use with {SMproductShortName} in the web console.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
* The {SMProductName} Operator must be installed.
* The `ServiceMeshControlPlane` deployed to the cluster.
* You have access to the {product-title} web console.

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role.

. Navigate to Operators → Installed Operators.

. Click the *Project* menu and select the project where your `ServiceMeshControlPlane` resource is deployed from the list, for example `istio-system`.

. Click the *{JaegerName} Operator*.

. On the *Operator Details* page, click the *Jaeger* tab.

. Click the name of your Jaeger instance.

. On the Jaeger details page, click the *YAML* tab to modify your configuration.

. Edit the `Jaeger` custom resource file to add the `htpasswd` configuration as shown in the following example.

* `spec.ingress.openshift.htpasswdFile`
* `spec.volumes`
* `spec.volumeMounts`
+
.Example Jaeger resource showing `htpasswd` configuration
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
spec:
  ingress:
    enabled: true
    openshift:
      htpasswdFile: /etc/proxy/htpasswd/auth
      sar: '{"namespace": "istio-system", "resource": "pods", "verb": "get"}'
    options: {}
    resources: {}
    security: oauth-proxy
  volumes:
    - name: secret-htpasswd
      secret:
        secretName: htpasswd
    - configMap:
        defaultMode: 420
        items:
          - key: ca-bundle.crt
            path: tls-ca-bundle.pem
        name: trusted-ca-bundle
        optional: true
      name: trusted-ca-bundle
  volumeMounts:
    - mountPath: /etc/proxy/htpasswd
      name: secret-htpasswd
    - mountPath: /etc/pki/ca-trust/extracted/pem/
      name: trusted-ca-bundle
      readOnly: true
----
+
. Click *Save*.

:leveloffset: 2

:leveloffset: +3

////
This module included in the following assemblies:
service_mesh/v2x/ossm-reference-jaeger.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="distr-tracing-config-security-ossm-cli_{context}"]
= Configuring distributed tracing security for service mesh from the command line

You can modify the Jaeger resource to configure {JaegerShortName} security for use with {SMproductShortName} from the command line by running the {oc-first}.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
* The {SMProductName} Operator must be installed.
* The `ServiceMeshControlPlane` deployed to the cluster.
* You have access to the {oc-first} that matches your {product-title} version.

.Procedure

. Log in to the {oc-first} as a user with the `cluster-admin` role by running the following command. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login https://<HOSTNAME>:6443
----
+
. Change to the project where you installed the control plane, for example `istio-system`, by entering the following command:
+
[source,terminal]
----
$ oc project istio-system
----
+
. Run the following command to edit the Jaeger custom resource file, where `jaeger.yaml` is the name of your Jaeger custom resource.
+
[source,terminal]
----
$ oc edit -n tracing-system -f jaeger.yaml
----
+
. Edit the `Jaeger` custom resource file to add the `htpasswd` configuration as shown in the following example.

* `spec.ingress.openshift.htpasswdFile`
* `spec.volumes`
* `spec.volumeMounts`
+
.Example Jaeger resource showing `htpasswd` configuration
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
spec:
  ingress:
    enabled: true
    openshift:
      htpasswdFile: /etc/proxy/htpasswd/auth
      sar: '{"namespace": "istio-system", "resource": "pods", "verb": "get"}'
    options: {}
    resources: {}
    security: oauth-proxy
  volumes:
    - name: secret-htpasswd
      secret:
        secretName: htpasswd
    - configMap:
        defaultMode: 420
        items:
          - key: ca-bundle.crt
            path: tls-ca-bundle.pem
        name: trusted-ca-bundle
        optional: true
      name: trusted-ca-bundle
  volumeMounts:
    - mountPath: /etc/proxy/htpasswd
      name: secret-htpasswd
    - mountPath: /etc/pki/ca-trust/extracted/pem/
      name: trusted-ca-bundle
      readOnly: true
----
+
. Run the following command to apply your changes, where <jaeger.yaml> is the name of your Jaeger custom resource.
+
[source,terminal]
----
$ oc apply -n tracing-system -f <jaeger.yaml>
----
+
. Run the following command to watch the progress of the pod deployment:
+
[source,terminal]
----
$ oc get pods -n tracing-system -w
----

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: REFERENCE
[id="distr-tracing-config-default_{context}"]
= Distributed tracing default configuration options

The Jaeger custom resource (CR) defines the architecture and settings to be used when creating the {JaegerShortName} resources. You can modify these parameters to customize your {JaegerShortName} implementation to your business needs.

.Generic YAML example of the Jaeger CR
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: name
spec:
  strategy: <deployment_strategy>
  allInOne:
    options: {}
    resources: {}
  agent:
    options: {}
    resources: {}
  collector:
    options: {}
    resources: {}
  sampling:
    options: {}
  storage:
    type:
    options: {}
  query:
    options: {}
    resources: {}
  ingester:
    options: {}
    resources: {}
  options: {}
----

.Jaeger parameters
[options="header"]
|===
|Parameter |Description |Values |Default value

|`apiVersion:`
|API version to use when creating the object.
|`jaegertracing.io/v1`
|`jaegertracing.io/v1`

|`kind:`
|Defines the kind of Kubernetes object to create.
|`jaeger`
|

|`metadata:`
|Data that helps uniquely identify the object, including a `name` string, `UID`, and optional `namespace`.
|
|{product-title} automatically generates the `UID` and completes the `namespace` with the name of the project where the object is created.

|`name:`
|Name for the object.
|The name of your {JaegerShortName} instance.
|`jaeger-all-in-one-inmemory`

|`spec:`
|Specification for the object to be created.
|Contains all of the configuration parameters for your {JaegerShortName} instance. When a common definition for all Jaeger components is required, it is defined under the `spec` node. When the definition relates to an individual component, it is placed under the `spec/<component>` node.
|N/A

|`strategy:`
|Jaeger deployment strategy
|`allInOne`, `production`, or `streaming`
|`allInOne`

|`allInOne:`
|Because the `allInOne` image deploys the Agent, Collector, Query, Ingester, and Jaeger UI in a single pod, configuration for this deployment must nest component configuration under the `allInOne` parameter.
|
|

|`agent:`
|Configuration options that define the Agent.
|
|

|`collector:`
|Configuration options that define the Jaeger Collector.
|
|

|`sampling:`
|Configuration options that define the sampling strategies for tracing.
|
|

|`storage:`
|Configuration options that define the storage. All storage-related options must be placed under `storage`, rather than under the `allInOne` or other component options.
|
|

|`query:`
|Configuration options that define the Query service.
|
|

|`ingester:`
|Configuration options that define the Ingester service.
|
|

|===

The following example YAML is the minimum required to create a {JaegerName} deployment using the default settings.

.Example minimum required dist-tracing-all-in-one.yaml
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger-all-in-one-inmemory
----

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: REFERENCE
[id="distr-tracing-config-jaeger-collector_{context}"]
= Jaeger Collector configuration options

The Jaeger Collector is the component responsible for receiving the spans that were captured by the tracer and writing them to persistent Elasticsearch storage when using the `production` strategy, or to AMQ Streams when using the `streaming` strategy.

The Collectors are stateless and thus many instances of Jaeger Collector can be run in parallel. Collectors require almost no configuration, except for the location of the Elasticsearch cluster.

.Parameters used by the Operator to define the Jaeger Collector
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Values
|collector:
  replicas:
|Specifies the number of Collector replicas to create.
|Integer, for example, `5`
|===


.Configuration parameters passed to the Collector
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Values
|spec:
 collector:
  options: {}
|Configuration options that define the Jaeger Collector.
|

|options:
  collector:
    num-workers:
|The number of workers pulling from the queue.
|Integer, for example, `50`

|options:
  collector:
    queue-size:
|The size of the Collector queue.
|Integer, for example, `2000`

|options:
  kafka:
    producer:
      topic: jaeger-spans
|The `topic` parameter identifies the Kafka configuration used by the Collector to produce the messages, and the Ingester to consume the messages.
|Label for the producer.

|options:
  kafka:
    producer:
      brokers: my-cluster-kafka-brokers.kafka:9092
|Identifies the Kafka configuration used by the Collector to produce the messages. If brokers are not specified, and you have AMQ Streams 1.4.0+ installed, the {JaegerName} Operator will self-provision Kafka.
|

|options:
  log-level:
|Logging level for the Collector.
|Possible values: `debug`, `info`, `warn`, `error`, `fatal`, `panic`.

|options:
  otlp:
    enabled: true
    grpc:
      host-port: 4317
      max-connection-age: 0s
      max-connection-age-grace: 0s
      max-message-size: 4194304
      tls:
        enabled: false
        cert: /path/to/cert.crt
        cipher-suites: "TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256"
        client-ca: /path/to/cert.ca
        reload-interval: 0s
        min-version: 1.2
        max-version: 1.3
|To accept OTLP/gRPC, explicitly enable the `otlp`. All the other options are optional.

|options:
  otlp:
    enabled: true
    http:
      cors:
        allowed-headers: [<header-name>[, <header-name>]*]
        allowed-origins: *
      host-port: 4318
      max-connection-age: 0s
      max-connection-age-grace: 0s
      max-message-size: 4194304
      read-timeout: 0s
      read-header-timeout: 2s
      idle-timeout: 0s
      tls:
        enabled: false
        cert: /path/to/cert.crt
        cipher-suites: "TLS_AES_256_GCM_SHA384,TLS_CHACHA20_POLY1305_SHA256"
        client-ca: /path/to/cert.ca
        reload-interval: 0s
        min-version: 1.2
        max-version: 1.3
|To accept OTLP/HTTP, explicitly enable the `otlp`. All the other options are optional.
|===

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: REFERENCE
[id="distr-tracing-config-sampling_{context}"]
= Distributed tracing sampling configuration options

The {JaegerName} Operator can be used to define sampling strategies that will be supplied to tracers that have been configured to use a remote sampler.

While all traces are generated, only a few are sampled. Sampling a trace marks the trace for further processing and storage.

[NOTE]
====
This is not relevant if a trace was started by the Envoy proxy, as the sampling decision is made there. The Jaeger sampling decision is only relevant when the trace is started by an application using the client.
====

When a service receives a request that contains no trace context, the client starts a new trace, assigns it a random trace ID, and makes a sampling decision based on the currently installed sampling strategy. The sampling decision propagates to all subsequent requests in the trace so that other services are not making the sampling decision again.

{JaegerShortName} libraries support the following samplers:

* *Probabilistic* - The sampler makes a random sampling decision with the probability of sampling equal to the value of the `sampling.param` property. For example, using `sampling.param=0.1` samples approximately 1 in 10 traces.

* *Rate Limiting* - The sampler uses a leaky bucket rate limiter to ensure that traces are sampled with a certain constant rate. For example, using `sampling.param=2.0` samples requests with the rate of 2 traces per second.

.Jaeger sampling options
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|spec:
 sampling:
  options: {}
    default_strategy:
    service_strategy:
|Configuration options that define the sampling strategies for tracing.
|
|If you do not provide configuration, the Collectors will return the default probabilistic sampling policy with 0.001 (0.1%) probability for all services.

|default_strategy:
  type:
service_strategy:
  type:
|Sampling strategy to use. See descriptions above.
|Valid values are `probabilistic`, and `ratelimiting`.
|`probabilistic`

|default_strategy:
  param:
service_strategy:
  param:
|Parameters for the selected sampling strategy.
|Decimal and integer values (0, .1, 1, 10)
|1
|===

This example defines a default sampling strategy that is probabilistic, with a 50% chance of the trace instances being sampled.

.Probabilistic sampling example
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: with-sampling
spec:
  sampling:
    options:
      default_strategy:
        type: probabilistic
        param: 0.5
      service_strategies:
        - service: alpha
          type: probabilistic
          param: 0.8
          operation_strategies:
            - operation: op1
              type: probabilistic
              param: 0.2
            - operation: op2
              type: probabilistic
              param: 0.4
        - service: beta
          type: ratelimiting
          param: 5
----

If there are no user-supplied configurations, the {JaegerShortName} uses the following settings:

.Default sampling
[source,yaml]
----
spec:
  sampling:
    options:
      default_strategy:
        type: probabilistic
        param: 1
----

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: REFERENCE
[id="distr-tracing-config-storage_{context}"]
= Distributed tracing storage configuration options

You configure storage for the Collector, Ingester, and Query services under `spec.storage`. Multiple instances of each of these components can be provisioned as required for performance and resilience purposes.

.General storage parameters used by the {JaegerName} Operator to define distributed tracing storage

[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|spec:
  storage:
    type:
|Type of storage to use for the deployment.
|`memory` or `elasticsearch`.
Memory storage is only appropriate for development, testing, demonstrations, and proof of concept environments as the data does not persist if the pod is shut down. For production environments {JaegerShortName} supports Elasticsearch for persistent storage.
|`memory`

|storage:
  secretname:
|Name of the secret, for example `tracing-secret`.
|
|N/A

|storage:
  options: {}
|Configuration options that define the storage.
|
|
|===

.Elasticsearch index cleaner parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|storage:
  esIndexCleaner:
    enabled:
|When using Elasticsearch storage, by default a job is created to clean old traces from the index. This parameter enables or disables the index cleaner job.
|`true`/ `false`
|`true`

|storage:
  esIndexCleaner:
    numberOfDays:
|Number of days to wait before deleting an index.
|Integer value
|`7`

|storage:
  esIndexCleaner:
    schedule:
|Defines the schedule for how often to clean the Elasticsearch index.
|Cron expression
|"55 23 * * *"
|===

[id="distributed-tracing-config-auto-provisioning-es_{context}"]
== Auto-provisioning an Elasticsearch instance

When you deploy a Jaeger custom resource, the {JaegerName} Operator uses the OpenShift Elasticsearch Operator to create an Elasticsearch cluster based on the configuration provided in the `storage` section of the custom resource file. The {JaegerName} Operator will provision Elasticsearch if the following configurations are set:

* `spec.storage:type` is set to `elasticsearch`
* `spec.storage.elasticsearch.doNotProvision` set to `false`
* `spec.storage.options.es.server-urls` is not defined, that is, there is no connection to an Elasticsearch instance that was not provisioned by the Red Hat Elasticsearch Operator.

When provisioning Elasticsearch, the {JaegerName} Operator sets the Elasticsearch custom resource `name` to the value of `spec.storage.elasticsearch.name` from the Jaeger custom resource.  If you do not specify a value for `spec.storage.elasticsearch.name`, the Operator uses `elasticsearch`.

.Restrictions

* You can have only one {JaegerShortName} with self-provisioned Elasticsearch instance per namespace. The Elasticsearch cluster is meant to be dedicated for a single {JaegerShortName} instance.
* There can be only one Elasticsearch per namespace.

[NOTE]
====
If you already have installed Elasticsearch as part of OpenShift Logging, the {JaegerName} Operator can use the installed OpenShift Elasticsearch Operator to provision storage.
====

The following configuration parameters are for a _self-provisioned_ Elasticsearch instance, that is an instance created by the {JaegerName} Operator using the OpenShift Elasticsearch Operator. You specify configuration options for self-provisioned Elasticsearch under `spec:storage:elasticsearch` in your configuration file.

.Elasticsearch resource configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|elasticsearch:
  properties:
    doNotProvision:
|Use to specify whether or not an Elasticsearch instance should be provisioned by the {JaegerName} Operator.
|`true`/`false`
|`true`

|elasticsearch:
  properties:
    name:
|Name of the Elasticsearch instance. The {JaegerName} Operator uses the Elasticsearch instance specified in this parameter to connect to Elasticsearch.
|string
|`elasticsearch`

|elasticsearch:
  nodeCount:
|Number of Elasticsearch nodes. For high availability use at least 3 nodes. Do not use 2 nodes as “split brain” problem can happen.
|Integer value. For example, Proof of concept = 1,
Minimum deployment =3
|3

|elasticsearch:
  resources:
    requests:
      cpu:
|Number of central processing units for requests, based on your environment's configuration.
|Specified in cores or millicores, for example, 200m, 0.5, 1. For example, Proof of concept = 500m,
Minimum deployment =1
|1

|elasticsearch:
  resources:
    requests:
      memory:
|Available memory for requests, based on your environment's configuration.
|Specified in bytes, for example, 200Ki, 50Mi, 5Gi. For example, Proof of concept = 1Gi,
Minimum deployment = 16Gi*
|16Gi

|elasticsearch:
  resources:
    limits:
      cpu:
|Limit on number of central processing units, based on your environment's configuration.
|Specified in cores or millicores, for example, 200m, 0.5, 1. For example, Proof of concept = 500m,
Minimum deployment =1
|

|elasticsearch:
  resources:
    limits:
      memory:
|Available memory limit based on your environment's configuration.
|Specified in bytes, for example, 200Ki, 50Mi, 5Gi. For example, Proof of concept = 1Gi,
Minimum deployment = 16Gi*
|

|elasticsearch:
  redundancyPolicy:
|Data replication policy defines how Elasticsearch shards are replicated across data nodes in the cluster. If not specified, the {JaegerName} Operator automatically determines the most appropriate replication based on number of nodes.
|`ZeroRedundancy`(no replica shards), `SingleRedundancy`(one replica shard), `MultipleRedundancy`(each index is spread over half of the Data nodes), `FullRedundancy` (each index is fully replicated on every Data node in the cluster).
|

|elasticsearch:
  useCertManagement:
|Use to specify whether or not {JaegerShortName} should use the certificate management feature of the Red Hat Elasticsearch Operator.  This feature was added to {logging-title} 5.2 in {product-title} 4.7 and is the preferred setting for new Jaeger deployments.
|`true`/`false`
|`true`

|===

Each Elasticsearch node can operate with a lower memory setting though this is NOT recommended for production deployments. For production use, you must have no less than 16 Gi allocated to each pod by default, but preferably allocate as much as you can, up to 64 Gi per pod.

.Production storage example
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-prod
spec:
  strategy: production
  storage:
    type: elasticsearch
    elasticsearch:
      nodeCount: 3
      resources:
        requests:
          cpu: 1
          memory: 16Gi
        limits:
          memory: 16Gi
----

.Storage example with persistent storage:
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-prod
spec:
  strategy: production
  storage:
    type: elasticsearch
    elasticsearch:
      nodeCount: 1
      storage: # <1>
        storageClassName: gp2
        size: 5Gi
      resources:
        requests:
          cpu: 200m
          memory: 4Gi
        limits:
          memory: 4Gi
      redundancyPolicy: ZeroRedundancy
----

<1> Persistent storage configuration. In this case AWS `gp2` with `5Gi` size. When no value is specified, {JaegerShortName} uses `emptyDir`. The OpenShift Elasticsearch Operator provisions `PersistentVolumeClaim` and `PersistentVolume` which are not removed with {JaegerShortName} instance. You can mount the same volumes if you create a {JaegerShortName} instance with the same name and namespace.


[id="distributed-tracing-config-external-es_{context}"]
== Connecting to an existing Elasticsearch instance

You can use an existing Elasticsearch cluster for storage with {DTShortName}. An existing Elasticsearch cluster, also known as an _external_ Elasticsearch instance, is an instance that was not installed by the {JaegerName} Operator or by the Red Hat Elasticsearch Operator.

When you deploy a Jaeger custom resource, the {JaegerName} Operator will not provision Elasticsearch if the following configurations are set:

* `spec.storage.elasticsearch.doNotProvision` set to `true`
* `spec.storage.options.es.server-urls` has a value
* `spec.storage.elasticsearch.name` has a value, or if the Elasticsearch instance name is `elasticsearch`.

The {JaegerName} Operator uses the Elasticsearch instance specified in `spec.storage.elasticsearch.name` to connect to Elasticsearch.

.Restrictions

* You cannot share or reuse a {product-title} logging Elasticsearch instance with {JaegerShortName}. The Elasticsearch cluster is meant to be dedicated for a single {JaegerShortName} instance.

[NOTE]
====
Red Hat does not provide support for your external Elasticsearch instance. You can review the tested integrations matrix on the link:https://access.redhat.com/articles/5381021[Customer Portal].
====

The following configuration parameters are for an already existing Elasticsearch instance, also known as an _external_ Elasticsearch instance. In this case, you specify configuration options for Elasticsearch under `spec:storage:options:es` in your custom resource file.

.General ES configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|es:
  server-urls:
|URL of the Elasticsearch instance.
|The fully-qualified domain name of the Elasticsearch server.
|`http://elasticsearch.<namespace>.svc:9200`

|es:
  max-doc-count:
|The maximum document count to return from an Elasticsearch query. This will also apply to aggregations. If you set both `es.max-doc-count` and `es.max-num-spans`, Elasticsearch will use the smaller value of the two.
|
|10000

|es:
  max-num-spans:
|[*Deprecated* - Will be removed in a future release, use `es.max-doc-count` instead.] The maximum number of spans to fetch at a time, per query, in Elasticsearch. If you set both `es.max-num-spans` and `es.max-doc-count`, Elasticsearch will use the smaller value of the two.
|
|10000

|es:
  max-span-age:
|The maximum lookback for spans in Elasticsearch.
|
|72h0m0s

|es:
  sniffer:
|The sniffer configuration for Elasticsearch. The client uses the sniffing process to find all nodes automatically. Disabled by default.
|`true`/ `false`
|`false`

|es:
  sniffer-tls-enabled:
|Option to enable TLS when sniffing an Elasticsearch Cluster. The client uses the sniffing process to find all nodes automatically. Disabled by default
|`true`/ `false`
|`false`

|es:
  timeout:
|Timeout used for queries. When set to zero there is no timeout.
|
|0s

|es:
  username:
|The username required by Elasticsearch. The basic authentication also loads CA if it is specified. See also `es.password`.
|
|

|es:
  password:
|The password required by Elasticsearch. See also, `es.username`.
|
|

|es:
  version:
|The major Elasticsearch version. If not specified, the value will be auto-detected from Elasticsearch.
|
|0
|===

.ES data replication parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|es:
  num-replicas:
|The number of replicas per index in Elasticsearch.
|
|1

|es:
  num-shards:
|The number of shards per index in Elasticsearch.
|
|5
|===

.ES index configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|es:
  create-index-templates:
|Automatically create index templates at application startup when set to `true`. When templates are installed manually, set to `false`.
|`true`/ `false`
|`true`

|es:
  index-prefix:
|Optional prefix for {JaegerShortName} indices. For example, setting this to "production" creates indices named "production-tracing-*".
|
|
|===

.ES bulk processor configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|es:
  bulk:
    actions:
|The number of requests that can be added to the queue before the bulk processor decides to commit updates to disk.
|
|1000

//What is the default here? The original text said "Set to zero to disable. By default, this is disabled."
|es:
  bulk:
    flush-interval:
|A `time.Duration` after which bulk requests are committed, regardless of other thresholds. To disable the bulk processor flush interval, set this to zero.
|
|200ms

|es:
  bulk:
    size:
|The number of bytes that the bulk requests can take up before the bulk processor decides to commit updates to disk.
|
|5000000

|es:
  bulk:
    workers:
|The number of workers that are able to receive and commit bulk requests to Elasticsearch.
|
|1
|===

.ES TLS configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|es:
  tls:
    ca:
|Path to a TLS Certification Authority (CA) file used to verify the remote servers.
|
|Will use the system truststore by default.

|es:
  tls:
    cert:
|Path to a TLS Certificate file, used to identify this process to the remote servers.
|
|

|es:
  tls:
    enabled:
|Enable transport layer security (TLS) when talking to the remote servers. Disabled by default.
|`true`/ `false`
|`false`

|es:
  tls:
    key:
|Path to a TLS Private Key file, used to identify this process to the remote servers.
|
|

|es:
  tls:
    server-name:
|Override the expected TLS server name in the certificate of the remote servers.
|
|
//Clarification of "if specified" for `token-file` and `username`, does that mean if this is set? Or that it only loads the CA if one is specified (that is, if es.tls.ca has a value?)
|es:
  token-file:
|Path to a file containing the bearer token. This flag also loads the Certification Authority (CA) file if it is specified.
|
|
|===

.ES archive configuration parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value
|es-archive:
  bulk:
    actions:
|The number of requests that can be added to the queue before the bulk processor decides to commit updates to disk.
|
|0

//What is the default here? The original text said "Set to zero to disable. By default, this is disabled."
|es-archive:
  bulk:
    flush-interval:
|A `time.Duration` after which bulk requests are committed, regardless of other thresholds. To disable the bulk processor flush interval, set this to zero.
|
|0s

|es-archive:
  bulk:
    size:
|The number of bytes that the bulk requests can take up before the bulk processor decides to commit updates to disk.
|
|0

|es-archive:
  bulk:
    workers:
|The number of workers that are able to receive and commit bulk requests to Elasticsearch.
|
|0

|es-archive:
  create-index-templates:
|Automatically create index templates at application startup when set to `true`. When templates are installed manually, set to `false`.
|`true`/ `false`
|`false`

|es-archive:
  enabled:
|Enable extra storage.
|`true`/ `false`
|`false`

|es-archive:
  index-prefix:
|Optional prefix for {JaegerShortName} indices. For example, setting this to "production" creates indices named "production-tracing-*".
|
|

|es-archive:
  max-doc-count:
|The maximum document count to return from an Elasticsearch query. This will also apply to aggregations.
|
|0

|es-archive:
  max-num-spans:
|[*Deprecated* - Will be removed in a future release, use `es-archive.max-doc-count` instead.] The maximum number of spans to fetch at a time, per query, in Elasticsearch.
|
|0

|es-archive:
  max-span-age:
|The maximum lookback for spans in Elasticsearch.
|
|0s

|es-archive:
  num-replicas:
|The number of replicas per index in Elasticsearch.
|
|0

|es-archive:
  num-shards:
|The number of shards per index in Elasticsearch.
|
|0

|es-archive:
  password:
|The password required by Elasticsearch. See also, `es.username`.
|
|

|es-archive:
  server-urls:
|The comma-separated list of Elasticsearch servers. Must be specified as fully qualified URLs, for example, `\http://localhost:9200`.
|
|

|es-archive:
  sniffer:
|The sniffer configuration for Elasticsearch. The client uses the sniffing process to find all nodes automatically. Disabled by default.
|`true`/ `false`
|`false`

|es-archive:
  sniffer-tls-enabled:
|Option to enable TLS when sniffing an Elasticsearch Cluster. The client uses the sniffing process to find all nodes automatically. Disabled by default.
|`true`/ `false`
|`false`

|es-archive:
  timeout:
|Timeout used for queries. When set to zero there is no timeout.
|
|0s

|es-archive:
  tls:
    ca:
|Path to a TLS Certification Authority (CA) file used to verify the remote servers.
|
|Will use the system truststore by default.

|es-archive:
  tls:
    cert:
|Path to a TLS Certificate file, used to identify this process to the remote servers.
|
|

|es-archive:
  tls:
    enabled:
|Enable transport layer security (TLS) when talking to the remote servers. Disabled by default.
|`true`/ `false`
|`false`

|es-archive:
  tls:
    key:
|Path to a TLS Private Key file, used to identify this process to the remote servers.
|
|

|es-archive:
  tls:
    server-name:
|Override the expected TLS server name in the certificate of the remote servers.
|
|

//Clarification of "if specified" for next two rows, does that mean if this is set? Or that it only loads the CA if one is specified (that is, if es-archive.tls.ca has a value?)
|es-archive:
  token-file:
|Path to a file containing the bearer token. This flag also loads the Certification Authority (CA) file if it is specified.
|
|

|es-archive:
  username:
|The username required by Elasticsearch. The basic authentication also loads CA if it is specified. See also `es-archive.password`.
|
|

|es-archive:
  version:
|The major Elasticsearch version. If not specified, the value will be auto-detected from Elasticsearch.
|
|0
|===


.Storage example with volume mounts
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-prod
spec:
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: https://quickstart-es-http.default.svc:9200
        index-prefix: my-prefix
        tls:
          ca: /es/certificates/ca.crt
    secretName: tracing-secret
  volumeMounts:
    - name: certificates
      mountPath: /es/certificates/
      readOnly: true
  volumes:
    - name: certificates
      secret:
        secretName: quickstart-es-http-certs-public
----

The following example shows a Jaeger CR using an external Elasticsearch cluster with TLS CA certificate mounted from a volume and user/password stored in a secret.

.External Elasticsearch example
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-prod
spec:
  strategy: production
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: https://quickstart-es-http.default.svc:9200 # <1>
        index-prefix: my-prefix
        tls: # <2>
          ca: /es/certificates/ca.crt
    secretName: tracing-secret # <3>
  volumeMounts: # <4>
    - name: certificates
      mountPath: /es/certificates/
      readOnly: true
  volumes:
    - name: certificates
      secret:
        secretName: quickstart-es-http-certs-public
----
<1> URL to Elasticsearch service running in default namespace.
<2> TLS configuration. In this case only CA certificate, but it can also contain es.tls.key and es.tls.cert when using mutual TLS.
<3> Secret which defines environment variables ES_PASSWORD and ES_USERNAME. Created by kubectl create secret generic tracing-secret --from-literal=ES_PASSWORD=changeme --from-literal=ES_USERNAME=elastic
<4> Volume mounts and volumes which are mounted into all storage components.

[id="distr-tracing-manage-es-certificates_{context}"]
= Managing certificates with Elasticsearch

You can create and manage certificates using the Red Hat Elasticsearch Operator. Managing certificates using the Red Hat Elasticsearch Operator also lets you use a single Elasticsearch cluster with multiple Jaeger Collectors.

:FeatureName: Managing certificates with Elasticsearch
:leveloffset: +1

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 4

Starting with version 2.4, the {JaegerName} Operator delegates certificate creation to the Red Hat Elasticsearch Operator by using the following annotations in the Elasticsearch custom resource:

* `logging.openshift.io/elasticsearch-cert-management: "true"`
* `logging.openshift.io/elasticsearch-cert.jaeger-<shared-es-node-name>: "user.jaeger"`
* `logging.openshift.io/elasticsearch-cert.curator-<shared-es-node-name>: "system.logging.curator"`

Where the `<shared-es-node-name>` is the name of the Elasticsearch node. For example, if you create an Elasticsearch node named `custom-es`, your custom resource might look like the following example.

.Example Elasticsearch CR showing annotations
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: Elasticsearch
metadata:
  annotations:
    logging.openshift.io/elasticsearch-cert-management: "true"
    logging.openshift.io/elasticsearch-cert.jaeger-custom-es: "user.jaeger"
    logging.openshift.io/elasticsearch-cert.curator-custom-es: "system.logging.curator"
  name: custom-es
spec:
  managementState: Managed
  nodeSpec:
    resources:
      limits:
        memory: 16Gi
      requests:
        cpu: 1
        memory: 16Gi
  nodes:
    - nodeCount: 3
      proxyResources: {}
      resources: {}
      roles:
        - master
        - client
        - data
      storage: {}
  redundancyPolicy: ZeroRedundancy
----

.Prerequisites

* {product-title} 4.7
* {logging-title} 5.2
* The Elasticsearch node and the Jaeger instances must be deployed in the same namespace.  For example, `tracing-system`.

You enable certificate management by setting `spec.storage.elasticsearch.useCertManagement` to `true` in the Jaeger custom resource.

.Example showing `useCertManagement`
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: jaeger-prod
spec:
  strategy: production
  storage:
    type: elasticsearch
    elasticsearch:
      name: custom-es
      doNotProvision: true
      useCertManagement: true
----

The {JaegerName} Operator sets the Elasticsearch custom resource `name` to the value of `spec.storage.elasticsearch.name` from the Jaeger custom resource when provisioning Elasticsearch.

The certificates are provisioned by the Red Hat Elasticsearch Operator and the {JaegerName} Operator injects the certificates.

:leveloffset: 2

For more information about configuring Elasticsearch with {product-title}, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#logging-config-es-store[Configuring the Elasticsearch log store] or link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/distributed_tracing/#[Configuring and deploying distributed tracing].

//TO DO For information about connecting to an external Elasticsearch instance, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/distributed_tracing/#jaeger-config-external-es_jaeger-deploying[Connecting to an existing Elasticsearch instance].

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: REFERENCE
[id="distr-tracing-config-query_{context}"]
= Query configuration options

Query is a service that retrieves traces from storage and hosts the user interface to display them.

.Parameters used by the {JaegerName} Operator to define Query
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value

|spec:
  query:
    replicas:
|Specifies the number of Query replicas to create.
|Integer, for example, `2`
|
|===


.Configuration parameters passed to Query
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value

|spec:
  query:
    options: {}
|Configuration options that define the Query service.
|
|

|options:
  log-level:
|Logging level for Query.
|Possible values: `debug`, `info`, `warn`, `error`, `fatal`, `panic`.
|

|options:
  query:
    base-path:
|The base path for all jaeger-query HTTP routes can be set to a non-root value, for example, `/jaeger` would cause all UI URLs to start with `/jaeger`. This can be useful when running jaeger-query behind a reverse proxy.
|/<path>
|
|===

.Sample Query configuration
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: "Jaeger"
metadata:
  name: "my-jaeger"
spec:
  strategy: allInOne
  allInOne:
    options:
      log-level: debug
      query:
        base-path: /jaeger
----

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-configuring.adoc
////
:_mod-docs-content-type: REFERENCE
[id="distr-tracing-config-ingester_{context}"]
= Ingester configuration options

Ingester is a service that reads from a Kafka topic and writes to the Elasticsearch storage backend. If you are using the `allInOne` or `production` deployment strategies, you do not need to configure the Ingester service.

.Jaeger parameters passed to the Ingester
[options="header"]
[cols="l, a, a"]
|===
|Parameter |Description |Values
|spec:
  ingester:
    options: {}
|Configuration options that define the Ingester service.
|

|options:
  deadlockInterval:
|Specifies the interval, in seconds or minutes, that the Ingester must wait for a message before terminating.
The deadlock interval is disabled by default (set to `0`), to avoid terminating the Ingester when no messages arrive during system initialization.
|Minutes and seconds, for example, `1m0s`. Default value is `0`.

|options:
  kafka:
    consumer:
      topic:
|The `topic` parameter identifies the Kafka configuration used by the collector to produce the messages, and the Ingester to consume the messages.
|Label for the consumer. For example, `jaeger-spans`.

|options:
  kafka:
    consumer:
      brokers:
|Identifies the Kafka configuration used by the Ingester to consume the messages.
|Label for the broker, for example, `my-cluster-kafka-brokers.kafka:9092`.

|options:
  log-level:
|Logging level for the Ingester.
|Possible values: `debug`, `info`, `warn`, `error`, `fatal`, `dpanic`, `panic`.
|===

.Streaming Collector and Ingester example
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: Jaeger
metadata:
  name: simple-streaming
spec:
  strategy: streaming
  collector:
    options:
      kafka:
        producer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
  ingester:
    options:
      kafka:
        consumer:
          topic: jaeger-spans
          brokers: my-cluster-kafka-brokers.kafka:9092
      ingester:
        deadlockInterval: 5
  storage:
    type: elasticsearch
    options:
      es:
        server-urls: http://elasticsearch:9200
----

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="removing-ossm"]
= Uninstalling Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: removing-ossm

toc::[]

To uninstall {SMProductName} from an existing {product-title} instance and remove its resources, you must delete the control plane, delete the Operators, and run commands to manually remove some resources.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-remove_{context}"]
= Removing the {SMProductName} control plane

To uninstall {SMProductShortName} from an existing {product-title} instance, first you delete the {SMProductShortName} control plane and the Operators. Then, you run commands to remove residual resources.

[id="ossm-control-plane-remove-operatorhub_{context}"]
== Removing the {SMProductShortName} control plane using the web console

You can remove the {SMProductName} control plane by using the web console.

.Procedure

. Log in to the {product-title} web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Navigate to *Operators* -> *Installed Operators*.

. Click *Service Mesh Control Plane* under *Provided APIs*.

. Click the `ServiceMeshControlPlane` menu {kebab}.

. Click *Delete Service Mesh Control Plane*.

. Click *Delete* on the confirmation dialog window to remove the `ServiceMeshControlPlane`.

[id="ossm-control-plane-remove-cli_{context}"]
== Removing the {SMProductShortName} control plane using the CLI

You can remove the {SMProductName} control plane by using the CLI.  In this example, `istio-system` is the name of the control plane project.

.Procedure

. Log in to the {product-title} CLI.

. Run the following command to delete the `ServiceMeshMemberRoll` resource.
+
[source,terminal]
----
$ oc delete smmr -n istio-system default
----

. Run this command to retrieve the name of the installed `ServiceMeshControlPlane`:
+
[source,terminal]
----
$ oc get smcp -n istio-system
----

. Replace `<name_of_custom_resource>` with the output from the previous command, and run this command to remove the custom resource:
+
[source,terminal]
----
$ oc delete smcp -n istio-system <name_of_custom_resource>
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-operatorhub-remove-operators_{context}"]
= Removing the installed Operators

You must remove the Operators to successfully remove {SMProductName}. After you remove the {SMProductName} Operator, you must remove the Kiali Operator, the {JaegerName} Operator, and the OpenShift Elasticsearch Operator.

[id="ossm-remove-operator-servicemesh_{context}"]
== Removing the Operators

Follow this procedure to remove the Operators that make up {SMProductName}. Repeat the steps for each of the following Operators.

* {SMProductName}
* Kiali
* {JaegerName}
* OpenShift Elasticsearch

.Procedure

. Log in to the {product-title} web console.

. From the *Operators* → *Installed Operators* page, scroll or type a keyword into the *Filter by name* to find each Operator. Then, click the Operator name.

. On the *Operator Details* page, select *Uninstall Operator* from the *Actions* menu. Follow the prompts to uninstall each Operator.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/installing-ossm.adoc


:_mod-docs-content-type: PROCEDURE
[id="ossm-remove-cleanup_{context}"]
= Clean up Operator resources

You can manually remove resources left behind after removing the {SMProductName} Operator using the {product-title} web console.

.Prerequisites

* An account with cluster administration access. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a cluster administrator.

. Run the following commands to clean up resources after uninstalling the Operators. If you intend to keep using {JaegerShortName} as a stand-alone service without service mesh, do not delete the Jaeger resources.
+
[NOTE]
====
The OpenShift Elasticsearch Operator is installed in `openshift-operators-redhat` by default. The other Operators are installed in the `openshift-operators` namespace by default. If you installed the Operators in another namespace, replace `openshift-operators` with the name of the project where the {SMProductName} Operator was installed.
====
+
[source,terminal]
----
$ oc delete validatingwebhookconfiguration/openshift-operators.servicemesh-resources.maistra.io
----
+
[source,terminal]
----
$ oc delete mutatingwebhookconfiguration/openshift-operators.servicemesh-resources.maistra.io
----
+
[source,terminal]
----
$ oc delete svc maistra-admission-controller -n openshift-operators
----
+
[source,terminal]
----
$ oc -n openshift-operators delete ds -lmaistra-version
----
+
[source,terminal]
----
$ oc delete clusterrole/istio-admin clusterrole/istio-cni clusterrolebinding/istio-cni clusterrole/ossm-cni clusterrolebinding/ossm-cni
----
+
[source,terminal]
----
$ oc delete clusterrole istio-view istio-edit
----
+
[source,terminal]
----
$ oc delete clusterrole jaegers.jaegertracing.io-v1-admin jaegers.jaegertracing.io-v1-crdview jaegers.jaegertracing.io-v1-edit jaegers.jaegertracing.io-v1-view
----
+
[source,terminal]
----
$ oc get crds -o name | grep '.*\.istio\.io' | xargs -r -n 1 oc delete
----
+
[source,terminal]
----
$ oc get crds -o name | grep '.*\.maistra\.io' | xargs -r -n 1 oc delete
----
+
[source,terminal]
----
$ oc get crds -o name | grep '.*\.kiali\.io' | xargs -r -n 1 oc delete
----
+
[source,terminal]
----
$ oc delete crds jaegers.jaegertracing.io
----
+
[source,terminal]
----
$ oc delete cm -n openshift-operators maistra-operator-cabundle
----
+
[source,terminal]
----
$ oc delete cm -n openshift-operators -lmaistra-version
----
+
[source,terminal]
----
$ oc delete sa -n openshift-operators -lmaistra-version
----

:leveloffset: 2

:leveloffset!:

== Service Mesh 1.x
:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="service-mesh-release-notes-v1x"]
= Service Mesh Release Notes
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-release-notes-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

== Making open source more inclusive

Red Hat is committed to replacing problematic language in our code, documentation, and web properties. We are beginning with these four terms: master, slave, blacklist, and whitelist. Because of the enormity of this endeavor, these changes will be implemented gradually over several upcoming releases. For more details, see link:https://www.redhat.com/en/blog/making-open-source-more-inclusive-eradicating-problematic-language[our CTO Chris Wright's message].


// The following include statements pull in the module files that comprise 1.x release notes.

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-about.adoc
////

[id="ossm-servicemesh-overview_{context}"]
= Introduction to {SMProductName}

{SMProductName} addresses a variety of problems in a microservice architecture by creating a centralized point of control in an application. It adds a transparent layer on existing distributed applications without requiring any changes to the application code.

Microservice architectures split the work of enterprise applications into modular services, which can make scaling and maintenance easier. However, as an enterprise application built on a microservice architecture grows in size and complexity, it becomes difficult to understand and manage. {SMProductShortName} can address those architecture problems by capturing or intercepting traffic between services and can modify, redirect, or create new requests to other services.

{SMProductShortName}, which is based on the open source link:https://istio.io/[Istio project], provides an easy way to create a network of deployed services that provides discovery, load balancing, service-to-service authentication, failure recovery, metrics, and monitoring. A service mesh also provides more complex operational functionality, including A/B testing, canary releases, access control, and end-to-end authentication.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * security/compliance_operator/co-scans/compliance-operator-troubleshooting.adoc
// * support/getting-support.adoc
// * distr_tracing/distributed-tracing-release-notes.adoc
// * service_mesh/v2x/ossm-support.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * osd_architecture/osd-support.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-0.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-1.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-2.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-3.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-4.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-5.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-6.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-7.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-8.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-9.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-3-0.adoc
// * microshift_support/microshift-getting-support.adoc

[id="support_{context}"]
= Getting support

If you experience difficulty with a procedure described in this documentation, or with {product-title} in general, visit the link:http://access.redhat.com[Red Hat Customer Portal].

From the Customer Portal, you can:

* Search or browse through the Red Hat Knowledgebase of articles and solutions relating to Red Hat products.
* Submit a support case to Red Hat Support.
* Access other product documentation.

To identify issues with your cluster, you can use Insights in {cluster-manager-url}. Insights provides details about issues and, if available, information on how to solve a problem.

// TODO: verify that these settings apply for Service Mesh and OpenShift virtualization, etc.
If you have a suggestion for improving this documentation or have found an
error, submit a link:https://issues.redhat.com/secure/CreateIssueDetails!init.jspa?pid=12332330&summary=Documentation_issue&issuetype=1&components=12367614&priority=10200&versions=12385624[Jira issue] for the most relevant documentation component. Please provide specific details, such as the section name and {product-title} version.

:leveloffset: 2

When opening a support case, it is helpful to provide debugging
information about your cluster to Red Hat Support.

The `must-gather` tool enables you to collect diagnostic information about your
{product-title} cluster, including virtual machines and other data related to
{SMProductName}.

For prompt support, supply diagnostic information for both {product-title}
and {SMProductName}.

:leveloffset: +2

// Module included in the following assemblies:
//
// * sandboxed_containers/troubleshooting-sandboxed-containers.adoc
// * virt/support/virt-collecting-virt-data.adoc
// * support/gathering-cluster-data.adoc
// * service_mesh/v2x/ossm-support.adoc
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * serverless/serverless-support.adoc

:_mod-docs-content-type: CONCEPT
[id="about-must-gather_{context}"]
= About the must-gather tool

The `oc adm must-gather` CLI command collects the information from your cluster that is most likely needed for debugging issues, including:

* Resource definitions
* Service logs

By default, the `oc adm must-gather` command uses the default plugin image and writes into `./must-gather.local`.

Alternatively, you can collect specific information by running the command with the appropriate arguments as described in the following sections:

* To collect data related to one or more specific features, use the `--image` argument with an image, as listed in a following section.
+
For example:
+
[source,terminal,subs="attributes+"]
----
$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v{HCOVersion}
----

* To collect the audit logs, use the `-- /usr/bin/gather_audit_logs` argument, as described in a following section.
+
For example:
+
[source,terminal]
----
$ oc adm must-gather -- /usr/bin/gather_audit_logs
----
+
[NOTE]
====
Audit logs are not collected as part of the default set of information to reduce the size of the files.
====

When you run `oc adm must-gather`, a new pod with a random name is created in a new project on the cluster. The data is collected on that pod and saved in a new directory that starts with `must-gather.local` in the current working directory.


For example:

[source,terminal]
----
NAMESPACE                      NAME                 READY   STATUS      RESTARTS      AGE
...
openshift-must-gather-5drcj    must-gather-bklx4    2/2     Running     0             72s
openshift-must-gather-5drcj    must-gather-s8sdh    2/2     Running     0             72s
...
----
// todo: table or ref module listing available images?
Optionally, you can run the `oc adm must-gather` command in a specific namespace by using the `--run-namespace` option.

For example:

[source,terminal,subs="attributes+"]
----
$ oc adm must-gather --run-namespace <namespace> \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v{HCOVersion}
----

:leveloffset: 2

=== Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.

* The {product-title} CLI (`oc`) installed.

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * service_mesh/v2x/servicemesh-release-notes.adoc
// * service_mesh/v2x/ossm-troubleshooting-istio.adoc


:_mod-docs-content-type: CONCEPT
[id="ossm-about-collecting-ossm-data_{context}"]
= About collecting service mesh data

You can use the `oc adm must-gather` CLI command to collect information about your cluster, including features and objects associated with {SMProductName}.

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.

* The {product-title} CLI (`oc`) installed.

.Procedure

. To collect {SMProductName} data with `must-gather`, you must specify the {SMProductName} image.
+
[source,terminal]
----
$ oc adm must-gather --image=registry.redhat.io/openshift-service-mesh/istio-must-gather-rhel8:2.4
----
+
. To collect {SMProductName} data for a specific {SMProductShortName} control plane namespace with `must-gather`, you must specify the {SMProductName} image and namespace. In this example, after `gather,` replace `<namespace>` with your {SMProductShortName} control plane namespace, such as `istio-system`.
+
[source,terminal]
----
$ oc adm must-gather --image=registry.redhat.io/openshift-service-mesh/istio-must-gather-rhel8:2.4 gather <namespace>
----
+
This creates a local directory that contains the following items:

* The Istio Operator namespace and its child objects
* All control plane namespaces and their children objects
* All namespaces and their children objects that belong to any service mesh
* All Istio custom resource definitions (CRD)
* All Istio CRD objects, such as VirtualServices, in a given namespace
* All Istio webhooks

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/preparing-ossm-install.adoc
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * post_installation_configuration/network-configuration.adoc

[id="ossm-supported-configurations-v1x_{context}"]
= {SMProductName} supported configurations

The following are the only supported configurations for the {SMProductName}:

* {product-title} version 4.6 or later.

[NOTE]
====
OpenShift Online and {product-dedicated} are not supported for {SMProductName}.
====

* The deployment must be contained within a single {product-title} cluster that is not federated.
* This release of {SMProductName} is only available on {product-title} x86_64.
* This release only supports configurations where all {SMProductShortName} components are contained in the {product-title} cluster in which it operates. It does not support management of microservices that reside outside of the cluster, or in a multi-cluster scenario.
* This release only supports configurations that do not integrate external services such as virtual machines.

For additional information about {SMProductName} lifecycle and supported configurations, refer to the link:https://access.redhat.com/support/policy/updates/openshift#ossm[Support Policy].

[id="ossm-supported-configurations-kiali_{context}"]
== Supported configurations for Kiali on {SMProductName}

* The Kiali observability console is only supported on the two most recent releases of the Chrome, Edge, Firefox, or Safari browsers.

[id="ossm-supported-configurations-adapters_{context}"]
== Supported Mixer adapters

* This release only supports the following Mixer adapter:
** 3scale Istio Adapter

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v1x/servicemesh-release-notes.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-rn-new-features-1x_{context}"]
= New Features

////
*Feature* – Describe the new functionality available to the customer. For enhancements, try to describe as specifically as possible where the customer will see changes.
*Reason* – If known, include why has the enhancement been implemented (use case, performance, technology, etc.). For example, showcases integration of X with Y, demonstrates Z API feature, includes latest framework bug fixes. There may not have been a 'problem' previously, but system behavior may have changed.
*Result* – If changed, describe the current user experience
////
{SMProductName} provides a number of key capabilities uniformly across a network of services:

* *Traffic Management* - Control the flow of traffic and API calls between services, make calls more reliable, and make the network more robust in the face of adverse conditions.
* *Service Identity and Security* - Provide services in the mesh with a verifiable identity and provide the ability to protect service traffic as it flows over networks of varying degrees of trustworthiness.
* *Policy Enforcement* - Apply organizational policy to the interaction between services, ensure access policies are enforced and resources are fairly distributed among consumers. Policy changes are made by configuring the mesh, not by changing application code.
* *Telemetry* - Gain understanding of the dependencies between services and the nature and flow of traffic between them, providing the ability to quickly identify issues.

== New features {SMProductName} 1.1.18.2

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs).

=== Component versions included in {SMProductName} version 1.1.18.2

|===
|Component |Version

|Istio
|1.4.10

|Jaeger
|1.30.2

|Kiali
|1.12.21.1

|3scale Istio Adapter
|1.0.0
|===

== New features {SMProductName} 1.1.18.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs).

=== Component versions included in {SMProductName} version 1.1.18.1

|===
|Component |Version

|Istio
|1.4.10

|Jaeger
|1.30.2

|Kiali
|1.12.20.1

|3scale Istio Adapter
|1.0.0
|===

== New features {SMProductName} 1.1.18

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs).

=== Component versions included in {SMProductName} version 1.1.18

|===
|Component |Version

|Istio
|1.4.10

|Jaeger
|1.24.0

|Kiali
|1.12.18

|3scale Istio Adapter
|1.0.0
|===

== New features {SMProductName} 1.1.17.1

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs).

=== Change in how {SMProductName} handles URI fragments

{SMProductName} contains a remotely exploitable vulnerability, link:https://cve.mitre.org/cgi-bin/cvename.cgi?name=CVE-2021-39156[CVE-2021-39156], where an HTTP request with a fragment (a section in the end of a URI that begins with a # character) in the URI path could bypass the Istio URI path-based authorization policies. For instance, an Istio authorization policy denies requests sent to the URI path `/user/profile`. In the vulnerable versions, a request with URI path `/user/profile#section1` bypasses the deny policy and routes to the backend (with the normalized URI `path /user/profile%23section1`), possibly leading to a security incident.

You are impacted by this vulnerability if you use authorization policies with DENY actions and `operation.paths`, or ALLOW actions and `operation.notPaths`.

With the mitigation, the fragment part of the request’s URI is removed before the authorization and routing. This prevents a request with a fragment in its URI from bypassing authorization policies which are based on the URI without the fragment part.

=== Required update for authorization policies

Istio generates hostnames for both the hostname itself and all matching ports. For instance, a virtual service or Gateway for a host of "httpbin.foo" generates a config matching "httpbin.foo and httpbin.foo:*". However, exact match authorization policies only match the exact string given for the `hosts` or `notHosts` fields.

Your cluster is impacted if you have `AuthorizationPolicy` resources using exact string comparison for the rule to determine link:https://istio.io/latest/docs/reference/config/security/authorization-policy/#Operation[hosts or notHosts].

You must update your authorization policy link:https://istio.io/latest/docs/reference/config/security/authorization-policy/#Rule[rules] to use prefix match instead of exact match.  For example, replacing `hosts: ["httpbin.com"]` with `hosts: ["httpbin.com:*"]` in the first `AuthorizationPolicy` example.

.First example AuthorizationPolicy using prefix match
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: httpbin
  namespace: foo
spec:
  action: DENY
  rules:
  - from:
    - source:
        namespaces: ["dev"]
    to:
    - operation:
        hosts: [“httpbin.com”,"httpbin.com:*"]
----

.Second example AuthorizationPolicy using prefix match
[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: AuthorizationPolicy
metadata:
  name: httpbin
  namespace: default
spec:
  action: DENY
  rules:
  - to:
    - operation:
        hosts: ["httpbin.example.com:*"]
----

== New features {SMProductName} 1.1.17

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.16

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.15

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.14

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

[IMPORTANT]
====
There are manual steps that must be completed to address CVE-2021-29492 and CVE-2021-31920.
====

[id="manual-updates-cve-2021-29492_{context}"]
=== Manual updates required by CVE-2021-29492 and CVE-2021-31920

Istio contains a remotely exploitable vulnerability where an HTTP request path with multiple slashes or escaped slash characters (``%2F` or ``%5C`) could potentially bypass an Istio authorization policy when path-based authorization rules are used.

For example, assume an Istio cluster administrator defines an authorization DENY policy to reject the request at path `/admin`. A request sent to the URL path `//admin` will NOT be rejected by the authorization policy.

According to https://tools.ietf.org/html/rfc3986#section-6[RFC 3986], the path `//admin` with multiple slashes should technically be treated as a different path from the `/admin`. However, some backend services choose to normalize the URL paths by merging multiple slashes into a single slash. This can result in a bypass of the authorization policy (`//admin` does not match `/admin`), and a user can access the resource at path `/admin` in the backend; this would represent a security incident.

Your cluster is impacted by this vulnerability if you have authorization policies using `ALLOW action + notPaths` field or `DENY action + paths field` patterns. These patterns are vulnerable to unexpected policy bypasses.

Your cluster is NOT impacted by this vulnerability if:

* You don’t have authorization policies.
* Your authorization policies don’t define `paths` or `notPaths` fields.
* Your authorization policies use `ALLOW action + paths` field or `DENY action + notPaths` field patterns. These patterns could only cause unexpected rejection instead of policy bypasses. The upgrade is optional for these cases.

[NOTE]
====
The {SMProductName} configuration location for path normalization is different from the Istio configuration.
====

=== Updating the path normalization configuration

Istio authorization policies can be based on the URL paths in the HTTP request.
https://en.wikipedia.org/wiki/URI_normalization[Path normalization], also known as URI normalization, modifies and standardizes the incoming requests' paths so that the normalized paths can be processed in a standard way.
Syntactically different paths may be equivalent after path normalization.

Istio supports the following normalization schemes on the request paths before evaluating against the authorization policies and routing the requests:

.Normalization schemes
[options="header"]
[cols="a, a, a, a"]
|====
| Option | Description | Example |Notes
|`NONE`
|No normalization is done. Anything received by Envoy will be forwarded exactly as-is to any backend service.
|`../%2Fa../b` is evaluated by the authorization policies and sent to your service.
|This setting is vulnerable to CVE-2021-31920.

|`BASE`
|This is currently the option used in the *default* installation of Istio. This applies the https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto#envoy-v3-api-field-extensions-filters-network-http-connection-manager-v3-httpconnectionmanager-normalize-path[`normalize_path`] option on Envoy proxies, which follows https://tools.ietf.org/html/rfc3986[RFC 3986] with extra normalization to convert backslashes to forward slashes.
|`/a/../b` is normalized to `/b`. `\da` is normalized to `/da`.
|This setting is vulnerable to CVE-2021-31920.

| `MERGE_SLASHES`
| Slashes are merged after the _BASE_ normalization.
| `/a//b` is normalized to `/a/b`.
|Update to this setting to mitigate CVE-2021-31920.

|`DECODE_AND_MERGE_SLASHES`
|The strictest setting when you allow all traffic by default. This setting is recommended, with the caveat that you must thoroughly test your authorization policies routes. https://tools.ietf.org/html/rfc3986#section-2.1[Percent-encoded] slash and backslash characters (`%2F`, `%2f`, `%5C` and `%5c`) are decoded to `/` or `\`, before the `MERGE_SLASHES` normalization.
|`/a%2fb` is normalized to `/a/b`.
|Update to this setting to mitigate CVE-2021-31920.  This setting is more secure, but also has the potential to break applications.  Test your applications before deploying to production.
|====

The normalization algorithms are conducted in the following order:

. Percent-decode `%2F`, `%2f`, `%5C` and `%5c`.
. The https://tools.ietf.org/html/rfc3986[RFC 3986] and other normalization implemented by the https://www.envoyproxy.io/docs/envoy/latest/api-v3/extensions/filters/network/http_connection_manager/v3/http_connection_manager.proto#envoy-v3-api-field-extensions-filters-network-http-connection-manager-v3-httpconnectionmanager-normalize-path[`normalize_path`] option in Envoy.
. Merge slashes.

[WARNING]
====
While these normalization options represent recommendations from HTTP standards and common industry practices, applications may interpret a URL in any way it chooses to. When using denial policies, ensure that you understand how your application behaves.
====

=== Path normalization configuration examples

Ensuring Envoy normalizes request paths to match your backend services' expectations is critical to the security of your system.
The following examples can be used as a reference for you to configure your system.
The normalized URL paths, or the original URL paths if `NONE` is selected, will be:

. Used to check against the authorization policies.
. Forwarded to the backend application.

.Configuration examples
[options="header"]
[cols="a, a"]
|====
|If your application... |Choose...
|Relies on the proxy to do normalization
|`BASE`, `MERGE_SLASHES` or `DECODE_AND_MERGE_SLASHES`

|Normalizes request paths based on https://tools.ietf.org/html/rfc3986[RFC 3986] and does not merge slashes.
|`BASE`

|Normalizes request paths based on https://tools.ietf.org/html/rfc3986[RFC 3986] and merges slashes, but does not decode https://tools.ietf.org/html/rfc3986#section-2.1[percent-encoded] slashes.
|`MERGE_SLASHES`

|Normalizes request paths based on https://tools.ietf.org/html/rfc3986[RFC 3986], decodes https://tools.ietf.org/html/rfc3986#section-2.1[percent-encoded] slashes, and merges slashes.
|`DECODE_AND_MERGE_SLASHES`

|Processes request paths in a way that is incompatible with https://tools.ietf.org/html/rfc3986[RFC 3986].
|`NONE`
|====

=== Configuring your SMCP for path normalization

To configure path normalization for {SMProductName}, specify the following in your `ServiceMeshControlPlane`.  Use the configuration examples to help determine the settings for your system.

.SMCP v1 pathNormalization
[source,yaml]
----
spec:
  global:
    pathNormalization: <option>
----


== New features {SMProductName} 1.1.13

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.12

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.11

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.10

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.9

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.8

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.7

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.6

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.5

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

This release also added support for configuring cipher suites.

== New features {SMProductName} 1.1.4

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

[NOTE]
====
There are manual steps that must be completed to address CVE-2020-8663.
====

[id="manual-updates-cve-2020-8663_{context}"]
=== Manual updates required by CVE-2020-8663

The fix for link:https://bugzilla.redhat.com/show_bug.cgi?id=1844254[CVE-2020-8663]`: envoy: Resource exhaustion when accepting too many connections` added a configurable limit on downstream connections. The configuration option for this limit must be configured to mitigate this vulnerability.

[IMPORTANT]
====
These manual steps are required to mitigate this CVE whether you are using the 1.1 version or the 1.0 version of {SMProductName}.
====

This new configuration option is called `overload.global_downstream_max_connections`, and it is configurable as a proxy `runtime` setting.  Perform the following steps to configure limits at the Ingress Gateway.

.Procedure

. Create a file named `bootstrap-override.json` with the following text to force the proxy to override the bootstrap template and load runtime configuration from disk:
+
  {
    "runtime": {
      "symlink_root": "/var/lib/istio/envoy/runtime"
    }
  }
+
. Create a secret from the `bootstrap-override.json` file, replacing <SMCPnamespace> with the namespace where you created the service mesh control plane (SMCP):
+
[source,terminal]
----
$  oc create secret generic -n <SMCPnamespace> gateway-bootstrap --from-file=bootstrap-override.json
----
+
. Update the SMCP configuration to activate the override.

+
.Updated SMCP configuration example #1
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    gateways:
      istio-ingressgateway:
        env:
          ISTIO_BOOTSTRAP_OVERRIDE: /var/lib/istio/envoy/custom-bootstrap/bootstrap-override.json
        secretVolumes:
        - mountPath: /var/lib/istio/envoy/custom-bootstrap
          name: custom-bootstrap
          secretName: gateway-bootstrap
----
+

. To set the new configuration option, create a secret that has the desired value for the `overload.global_downstream_max_connections` setting.  The following example uses a value of `10000`:
+
[source,terminal]
----
$  oc create secret generic -n <SMCPnamespace> gateway-settings --from-literal=overload.global_downstream_max_connections=10000
----
+

. Update the SMCP again to mount the secret in the location where Envoy is looking for runtime configuration:

.Updated SMCP configuration example #2
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  template: default
#Change the version to "v1.0" if you are on the 1.0 stream.
  version: v1.1
  istio:
    gateways:
      istio-ingressgateway:
        env:
          ISTIO_BOOTSTRAP_OVERRIDE: /var/lib/istio/envoy/custom-bootstrap/bootstrap-override.json
        secretVolumes:
        - mountPath: /var/lib/istio/envoy/custom-bootstrap
          name: custom-bootstrap
          secretName: gateway-bootstrap
        # below is the new secret mount
        - mountPath: /var/lib/istio/envoy/runtime
          name: gateway-settings
          secretName: gateway-settings

----

[id="upgrading_es5_es6_{context}"]
=== Upgrading from Elasticsearch 5 to Elasticsearch 6

When updating from Elasticsearch 5 to Elasticsearch 6, you must delete your Jaeger instance, then recreate the Jaeger instance because of an issue with certificates. Re-creating the Jaeger instance triggers creating a new set of certificates.   If you are using persistent storage the same volumes can be mounted for the new Jaeger instance as long as the Jaeger name and namespace for the new Jaeger instance are the same as the deleted Jaeger instance.

.Procedure if Jaeger is installed as part of Red Hat Service Mesh

. Determine the name of your Jaeger custom resource file:
+
[source,terminal]
----
$ oc get jaeger -n istio-system
----
+
You should see something like the following:
+
[source,terminal]
----
NAME     AGE
jaeger   3d21h
----
+
. Copy the generated custom resource file into a temporary directory:
+
[source,terminal]
----
$ oc get jaeger jaeger -oyaml -n istio-system > /tmp/jaeger-cr.yaml
----
+
. Delete the Jaeger instance:
+
[source,terminal]
----
$ oc delete jaeger jaeger -n istio-system
----
+
. Recreate the Jaeger instance from your copy of the custom resource file:
+
[source,terminal]
----
$ oc create -f /tmp/jaeger-cr.yaml -n istio-system
----
+
. Delete the copy of the generated custom resource file:
+
[source,terminal]
----
$ rm /tmp/jaeger-cr.yaml
----


.Procedure if Jaeger not installed as part of Red Hat Service Mesh

Before you begin, create a copy of your Jaeger custom resource file.

. Delete the Jaeger instance by deleting the custom resource file:
+
[source,terminal]
----
$ oc delete -f <jaeger-cr-file>
----
+
For example:
+
[source,terminal]
----
$ oc delete -f jaeger-prod-elasticsearch.yaml
----
+
. Recreate your Jaeger instance from the backup copy of your custom resource file:
+
[source,terminal]
----
$ oc create -f <jaeger-cr-file>
----
+
. Validate that your Pods have restarted:
+
[source,terminal]
----
$ oc get pods -n jaeger-system -w
----
+




== New features {SMProductName} 1.1.3

This release of {SMProductName} addresses Common Vulnerabilities and Exposures (CVEs) and bug fixes.

== New features {SMProductName} 1.1.2

This release of {SMProductName} addresses a security vulnerability.

== New features {SMProductName} 1.1.1

This release of {SMProductName} adds support for a disconnected installation.

== New features {SMProductName} 1.1.0

This release of {SMProductName} adds support for Istio 1.4.6 and Jaeger 1.17.1.

[id="ossm-manual-updates-1.0-1.1_{context}"]
=== Manual updates from 1.0 to 1.1

If you are updating from {SMProductName} 1.0 to 1.1, you must update the `ServiceMeshControlPlane` resource to update the control plane components to the new version.

. In the web console, click the {SMProductName} Operator.

. Click the *Project* menu and choose the project where your `ServiceMeshControlPlane` is deployed from the list, for example `istio-system`.

. Click the name of your control plane, for example `basic-install`.

. Click YAML and add a version field to the `spec:` of your `ServiceMeshControlPlane` resource. For example, to update to {SMProductName} 1.1.0, add `version: v1.1`.

----
spec:
  version: v1.1
  ...
----

The version field specifies the version of {SMProductShortName} to install and defaults to the latest available version.

[NOTE]
====
Note that support for {SMProductName} v1.0 ended in October, 2020.  You must upgrade to either v1.1 or v2.0.
====

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v1x/servicemesh-release-notes.adoc
////

[id="ossm-deprecated-features-1x_{context}"]
////
Description - Description of the any features (including technology previews) that have been removed from the product.  Write the description from a customer perspective, what UI elements, commands, or options are no longer available.
Consequence or a recommended replacement - Description of what the customer can no longer do, and recommended replacement (if known).
////
= Deprecated features
Some features available in previous releases have been deprecated or removed.

Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

== Deprecated features {SMProductName} 1.1.5

The following custom resources were deprecated in release 1.1.5 and were removed in release 1.1.12

* `Policy` - The `Policy` resource is deprecated and will be replaced by the `PeerAuthentication` resource in a future release.
* `MeshPolicy` - The `MeshPolicy` resource is deprecated and will be replaced by the `PeerAuthentication` resource in a future release.
* `v1alpha1` RBAC API -The v1alpha1 RBAC policy is deprecated by the v1beta1 `AuthorizationPolicy`. RBAC (Role Based Access Control) defines `ServiceRole` and `ServiceRoleBinding` objects.
** `ServiceRole`
** `ServiceRoleBinding`
* `RbacConfig` - `RbacConfig` implements the Custom Resource Definition for controlling Istio RBAC behavior.
** `ClusterRbacConfig`(versions prior to {SMProductName} 1.0)
** `ServiceMeshRbacConfig` ({SMProductName} version 1.0 and later)
* In Kiali, the `login` and `LDAP` strategies are deprecated. A future version will introduce authentication using OpenID providers.

The following components are also deprecated in this release and will be replaced by the *Istiod* component in a future release.

* *Mixer* - access control and usage policies
* *Pilot* - service discovery and proxy configuration
* *Citadel* - certificate generation
* *Galley* - configuration validation and distribution

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v1x/servicemesh-release-notes.adoc
////

[id="ossm-rn-known-issues-1x_{context}"]
= Known issues

////
*Consequence* - What user action or situation would make this problem appear (Selecting the Foo option with the Bar version 1.3 plugin enabled results in an error message)?  What did the customer experience as a result of the issue? What was the symptom?
*Cause* (if it has been identified) - Why did this happen?
*Workaround* (If there is one)- What can you do to avoid or negate the effects of this issue in the meantime?  Sometimes if there is no workaround it is worthwhile telling readers to contact support for advice. Never promise future fixes.
*Result* - If the workaround does not completely address the problem.
////

These limitations exist in {SMProductName}:

* link:https://github.com/istio/old_issues_repo/issues/115[{SMProductName} does not support IPv6], as it is not supported by the upstream Istio project, nor fully supported by {product-title}.

* Graph layout - The layout for the Kiali graph can render differently, depending on your application architecture and the data to display (number of graph nodes and their interactions). Because it is difficult if not impossible to create a single layout that renders nicely for every situation, Kiali offers a choice of several different layouts. To choose a different layout, you can choose a different *Layout Schema* from the *Graph Settings* menu.

* The first time you access related services such as Jaeger and Grafana, from the Kiali console, you must accept the certificate and re-authenticate using your {product-title} login credentials. This happens due to an issue with how the framework displays embedded pages in the console.

[id="ossm-rn-known-issues-ossm_{context}"]
== {SMProductShortName} known issues

These are the known issues in {SMProductName}:

* link:https://access.redhat.com/solutions/4970771[Jaeger/Kiali Operator upgrade blocked with operator pending] When upgrading the Jaeger or Kiali Operators with Service Mesh 1.0.x installed, the operator status shows as Pending.
+
Workaround: See the linked Knowledge Base article for more information.

* link:https://github.com/istio/istio/issues/14743[Istio-14743] Due to limitations in the version of Istio that this release of {SMProductName} is based on, there are several applications that are currently incompatible with {SMProductShortName}. See the linked community issue for details.

* link:https://issues.jboss.org/browse/MAISTRA-858[MAISTRA-858] The following Envoy log messages describing link:https://www.envoyproxy.io/docs/envoy/latest/intro/deprecated[deprecated options and configurations associated with Istio 1.1.x] are expected:
+
** [2019-06-03 07:03:28.943][19][warning][misc] [external/envoy/source/common/protobuf/utility.cc:129] Using deprecated option 'envoy.api.v2.listener.Filter.config'. This configuration will be removed from Envoy soon.
** [2019-08-12 22:12:59.001][13][warning][misc] [external/envoy/source/common/protobuf/utility.cc:174] Using deprecated option 'envoy.api.v2.Listener.use_original_dst' from file lds.proto. This configuration will be removed from Envoy soon.

* link:https://issues.jboss.org/browse/MAISTRA-806[MAISTRA-806] Evicted Istio Operator Pod causes mesh and CNI not to deploy.
+
Workaround: If the `istio-operator` pod is evicted while deploying the control pane, delete the evicted `istio-operator` pod.
+
* link:https://issues.jboss.org/browse/MAISTRA-681[MAISTRA-681] When the control plane has many namespaces, it can lead to performance issues.

* link:https://issues.jboss.org/browse/MAISTRA-465[MAISTRA-465] The Maistra Operator fails to create a service for operator metrics.

* link:https://issues.jboss.org/browse/MAISTRA-453[MAISTRA-453] If you create a new project and deploy pods immediately, sidecar injection does not occur. The operator fails to add the `maistra.io/member-of` before the pods are created, therefore the pods must be deleted and recreated for sidecar injection to occur.

* link:https://issues.jboss.org/browse/MAISTRA-158[MAISTRA-158] Applying multiple gateways referencing the same hostname will cause all gateways to stop functioning.



[id="ossm-rn-known-issues-kiali_{context}"]
== Kiali known issues

[NOTE]
====
New issues for Kiali should be created in the link:https://issues.redhat.com/projects/OSSM/[OpenShift Service Mesh]  project with the `Component` set to `Kiali`.
====

These are the known issues in Kiali:

* link:https://issues.jboss.org/browse/KIALI-2206[KIALI-2206] When you are accessing the Kiali console for the first time, and there is no cached browser data for Kiali, the “View in Grafana” link on the Metrics tab of the Kiali Service Details page redirects to the wrong location. The only way you would encounter this issue is if you are accessing Kiali for the first time.

* link:https://github.com/kiali/kiali/issues/507[KIALI-507] Kiali does not support Internet Explorer 11. This is because the underlying frameworks do not support Internet Explorer. To access the Kiali console, use one of the two most recent versions of the Chrome, Edge, Firefox or Safari browser.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v1x/servicemesh-release-notes.adoc
////

[id="ossm-rn-fixed-issues-1x_{context}"]
= Fixed issues

////
Provide the following info for each issue if possible:
*Consequence* - What user action or situation would make this problem appear (If you have the foo option enabled and did x)? What did the customer experience as a result of the issue? What was the symptom?
*Cause* - Why did this happen?
*Fix* - What did we change to fix the problem?
*Result* - How has the behavior changed as a result? Try to avoid “It is fixed” or “The issue is resolved” or “The error no longer presents”.
////

The following issues been resolved in the current release:

[id="ossm-rn-fixed-issues-ossm_{context}"]
== {SMProductShortName} fixed issues

* link:https://issues.redhat.com/browse/MAISTRA-2371[MAISTRA-2371] Handle tombstones in listerInformer. The updated cache codebase was not handling tombstones when translating the events from the namespace caches to the aggregated cache, leading to a panic in the go routine.

* link:https://issues.redhat.com/browse/OSSM-542[OSSM-542] Galley is not using the new certificate after rotation.

* link:https://issues.jboss.org/browse/OSSM-99[OSSM-99] Workloads generated from direct pod without labels may crash Kiali.

* link:https://issues.jboss.org/browse/OSSM-93[OSSM-93] IstioConfigList can't filter by two or more names.

* link:https://issues.jboss.org/browse/OSSM-92[OSSM-92] Cancelling unsaved changes on the VS/DR YAML edit page does not cancel the changes.

* link:https://issues.jboss.org/browse/OSSM-90[OSSM-90] Traces not available on the service details page.

[id="ossm-rn-fixed-issues-maistra_{context}"]
* link:https://issues.redhat.com/projects/MAISTRA/issues/MAISTRA-1649[MAISTRA-1649] Headless services conflict when in different namespaces. When deploying headless services within different namespaces the endpoint configuration is merged and results in invalid Envoy configurations being pushed to the sidecars.

* link:https://issues.redhat.com/browse/MAISTRA-1541[MAISTRA-1541] Panic in kubernetesenv when the controller is not set on owner reference. If a pod has an ownerReference which does not specify the controller, this will cause a panic within the `kubernetesenv cache.go` code.

* link:https://issues.redhat.com/browse/MAISTRA-1352[MAISTRA-1352] Cert-manager Custom Resource Definitions (CRD) from the control plane installation have been removed for this release and future releases. If you have already installed {SMProductName}, the CRDs must be removed manually if cert-manager is not being used.

* link:https://issues.jboss.org/browse/MAISTRA-1001[MAISTRA-1001] Closing HTTP/2 connections could lead to segmentation faults in `istio-proxy`.

* link:https://issues.jboss.org/browse/MAISTRA-932[MAISTRA-932] Added the `requires` metadata to add dependency relationship between Jaeger Operator and OpenShift Elasticsearch Operator. Ensures that when the Jaeger Operator is installed, it automatically deploys the OpenShift Elasticsearch Operator if it is not available.

* link:https://issues.jboss.org/browse/MAISTRA-862[MAISTRA-862] Galley dropped watches and stopped providing configuration to other components after many namespace deletions and re-creations.

* link:https://issues.jboss.org/browse/MAISTRA-833[MAISTRA-833] Pilot stopped delivering configuration after many namespace deletions and re-creations.

* link:https://issues.jboss.org/browse/MAISTRA-684[MAISTRA-684] The default Jaeger version in the `istio-operator` is 1.12.0, which does not match Jaeger version 1.13.1 that shipped in {SMProductName} 0.12.TechPreview.

* link:https://issues.jboss.org/browse/MAISTRA-622[MAISTRA-622] In Maistra 0.12.0/TP12, permissive mode does not work. The user has the option to use Plain text mode or Mutual TLS mode, but not permissive.

* link:https://issues.jboss.org/browse/MAISTRA-572[MAISTRA-572] Jaeger cannot be used with Kiali. In this release Jaeger is configured to use the OAuth proxy, but is also only configured to work through a browser and does not allow service access. Kiali cannot properly communicate with the Jaeger endpoint and it considers Jaeger to be disabled. See also link:https://issues.jboss.org/browse/TRACING-591[TRACING-591].

* link:https://issues.jboss.org/browse/MAISTRA-357[MAISTRA-357] In OpenShift 4 Beta on AWS, it is not possible, by default, to access a TCP or HTTPS service through the ingress gateway on a port other than port 80. The AWS load balancer has a health check that verifies if port 80 on the service endpoint is active. Without a service running on port 80, the load balancer health check fails.

* link:https://issues.jboss.org/browse/MAISTRA-348[MAISTRA-348] OpenShift 4 Beta on AWS does not support ingress gateway traffic on ports other than 80 or 443.  If you configure your ingress gateway to handle TCP traffic with a port number other than 80 or 443, you have to use the service hostname provided by the AWS load balancer rather than the OpenShift router as a workaround.

* link:https://issues.jboss.org/browse/MAISTRA-193[MAISTRA-193] Unexpected console info messages are visible when health checking is enabled for citadel.

* link:https://bugzilla.redhat.com/show_bug.cgi?id=1821432[Bug 1821432] Toggle controls in {product-title} Control Resource details page do not update the CR correctly. UI Toggle controls in the Service Mesh Control Plane (SMCP) Overview page in the {product-title} web console sometimes update the wrong field in the resource. To update a ServiceMeshControlPlane resource, edit the YAML content directly or update the resource from the command line instead of clicking the toggle controls.

[id="ossm-rn-fixed-issues-kiali_{context}"]
== Kiali fixed issues

* link:https://issues.jboss.org/browse/KIALI-3239[KIALI-3239] If a Kiali Operator pod has failed with a status of “Evicted” it blocks the Kiali operator from deploying. The workaround is to delete the Evicted pod and redeploy the Kiali operator.

* link:https://issues.jboss.org/browse/KIALI-3118[KIALI-3118] After changes to the ServiceMeshMemberRoll, for example adding or removing projects, the Kiali pod restarts and then displays errors on the Graph page while the Kiali pod is restarting.

* link:https://issues.jboss.org/browse/KIALI-3096[KIALI-3096] Runtime metrics fail in {SMProductShortName}. There is an OAuth filter between the {SMProductShortName} and Prometheus, requiring a bearer token to be passed to Prometheus before access is granted. Kiali has been updated to use this token when communicating to the Prometheus server, but the application metrics are currently failing with 403 errors.

* link:https://issues.jboss.org/browse/KIALI-3070[KIALI-3070] This bug only affects custom dashboards, not the default dashboards. When you select labels in metrics settings and refresh the page, your selections are retained in the menu but your selections are not displayed on the charts.

* link:https://github.com/kiali/kiali/issues/1603[KIALI-2686] When the control plane has many namespaces, it can lead to performance issues.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-architecture-v1x"]
= Understanding Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-architecture-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

{SMProductName} provides a platform for behavioral insight and operational control over your networked microservices in a service mesh. With {SMProductName}, you can connect, secure, and monitor microservices in your {product-title} environment.

:leveloffset: +1

////
Module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-understanding-service-mesh_{context}"]
= Understanding service mesh

A _service mesh_ is the network of microservices that make up applications in a distributed microservice architecture and the interactions between those microservices. When a {SMProductShortName} grows in size and complexity, it can become harder to understand and manage.

Based on the open source link:https://istio.io/[Istio] project, {SMProductName} adds a transparent layer on existing distributed applications without requiring any changes to the service code. You add {SMProductName} support to services by deploying a special sidecar proxy to relevant services in the mesh that intercepts all network communication between microservices. You configure and manage the {SMProductShortName} using the {SMProductShortName} control plane features.

{SMProductName} gives you an easy way to create a network of deployed services that provide:

* Discovery
* Load balancing
* Service-to-service authentication
* Failure recovery
* Metrics
* Monitoring

{SMProductName} also provides more complex operational functions including:

* A/B testing
* Canary releases
* Access control
* End-to-end authentication

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// -service_mesh/v1x/ossm-architecture.adoc

[id="ossm-architecture-1x_{context}"]
= {SMProductName} Architecture

{SMProductName} is logically split into a data plane and a control plane:

The *data plane* is a set of intelligent proxies deployed as sidecars. These proxies intercept and control all inbound and outbound network communication between microservices in the service mesh. Sidecar proxies also communicate with Mixer, the general-purpose policy and telemetry hub.

* *Envoy proxy* intercepts all inbound and outbound traffic for all services in the service mesh. Envoy is deployed as a sidecar to the relevant service in the same pod.

The *control plane* manages and configures proxies to route traffic, and configures Mixers to enforce policies and collect telemetry.

* *Mixer* enforces access control and usage policies (such as authorization, rate limits, quotas, authentication, and request tracing) and collects telemetry data from the Envoy proxy and other services.
* *Pilot* configures the proxies at runtime. Pilot provides service discovery for the Envoy sidecars, traffic management capabilities for intelligent routing (for example, A/B tests or canary deployments), and resiliency (timeouts, retries, and circuit breakers).
* *Citadel* issues and rotates certificates. Citadel provides strong service-to-service and end-user authentication with built-in identity and credential management. You can use Citadel to upgrade unencrypted traffic in the service mesh. Operators can enforce policies based on service identity rather than on network controls using Citadel.
* *Galley* ingests the service mesh configuration, then validates, processes, and distributes the configuration. Galley protects the other service mesh components from obtaining user configuration details from {product-title}.

{SMProductName} also uses the *istio-operator* to manage the installation of the control plane. An _Operator_ is a piece of software that enables you to implement and automate common activities in your {product-title} cluster. It acts as a controller, allowing you to set or change the desired state of objects in your cluster.

:leveloffset: 2

== Understanding Kiali

Kiali provides visibility into your service mesh by showing you the microservices in your service mesh, and how they are connected.

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-kiali-overview_{context}"]
= Kiali overview

Kiali provides observability into the {SMProductShortName} running on {product-title}. Kiali helps you define, validate, and observe your Istio service mesh. It helps you to understand the structure of your service mesh by inferring the topology, and also provides information about the health of your service mesh.

Kiali provides an interactive graph view of your namespace in real time that provides visibility into features like circuit breakers, request rates, latency, and even graphs of traffic flows. Kiali offers insights about components at different levels, from Applications to Services and Workloads, and can display the interactions with contextual information and charts on the selected graph node or edge. Kiali also provides the ability to validate your Istio configurations, such as gateways, destination rules, virtual services, mesh policies, and more. Kiali provides detailed metrics, and a basic Grafana integration is available for advanced queries. Distributed tracing is provided by integrating Jaeger into the Kiali console.

Kiali is installed by default as part of the {SMProductName}.

:leveloffset: 2

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-kiali-architecture_{context}"]
= Kiali architecture

Kiali is based on the open source link:https://kiali.io/[Kiali project]. Kiali is composed of two components: the Kiali application and the Kiali console.

* *Kiali application* (back end) – This component runs in the container application platform and communicates with the service mesh components, retrieves and processes data, and exposes this data to the console. The Kiali application does not need storage. When deploying the application to a cluster, configurations are set in ConfigMaps and secrets.

* *Kiali console* (front end) – The Kiali console is a web application. 	The Kiali application serves the Kiali console, which then queries the back end for data to present it to the user.

In addition, Kiali depends on external services and components provided by the container application platform and Istio.

* *Red Hat Service Mesh* (Istio) - Istio is a Kiali requirement. Istio is the component that provides and controls the service mesh. Although Kiali and Istio can be installed separately, Kiali depends on Istio and will not work if it is not present. Kiali needs to retrieve Istio data and configurations, which are exposed through Prometheus and the cluster API.

* *Prometheus* - A dedicated Prometheus instance is included as part of the {SMProductName} installation. When Istio telemetry is enabled, metrics data are stored in Prometheus. Kiali uses this Prometheus data to determine the mesh topology, display metrics, calculate health, show possible problems, and so on. Kiali communicates directly with Prometheus and assumes the data schema used by Istio Telemetry. Prometheus is an Istio dependency and a hard dependency for Kiali, and many of Kiali's features will not work without Prometheus.

* *Cluster API* - Kiali uses the API of the {product-title} (cluster API) to fetch and resolve service mesh configurations. Kiali queries the cluster API to retrieve, for example, definitions for namespaces, services, deployments, pods, and other entities. Kiali also makes queries to resolve relationships between the different cluster entities. The cluster API is also queried to retrieve Istio configurations like virtual services, destination rules, route rules, gateways, quotas, and so on.

* *Jaeger* - Jaeger is optional, but is installed by default as part of the {SMProductName} installation. When you install the {JaegerShortName} as part of the default {SMProductName} installation, the Kiali console includes a tab to display distributed tracing data. Note that tracing data will not be available if you disable Istio's distributed tracing feature. Also note that user must have access to the namespace where the {SMProductShortName} control plane is installed to view tracing data.

* *Grafana* - Grafana is optional, but is installed by default as part of the {SMProductName} installation. When available, the metrics pages of Kiali display links to direct the user to the same metric in Grafana. Note that user must have access to the namespace where the {SMProductShortName} control plane is installed to view links to the Grafana dashboard and view Grafana data.

:leveloffset: 2

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
////

[id="ossm-kiali-features_{context}"]
= Kiali features
//In the title include nouns or noun phrases that are used in the body text.
//Do not start the title of concept modules with a verb..

The Kiali console is integrated with Red Hat Service Mesh and provides the following capabilities:

* *Health* – Quickly identify issues with applications, services, or workloads.

* *Topology* – Visualize how your applications, services, or workloads communicate via the Kiali graph.

* *Metrics* – Predefined metrics dashboards let you chart service mesh and application performance for Go, Node.js. Quarkus, Spring Boot, Thorntail and Vert.x. You can also create your own custom dashboards.

* *Tracing* – Integration with Jaeger lets you follow the path of a request through various microservices that make up an application.

* *Validations* – Perform advanced validations on the most common Istio objects (Destination Rules, Service Entries, Virtual Services, and so on).

* *Configuration* – Optional ability to create, update and delete Istio routing configuration using wizards or directly in the YAML editor in the Kiali Console.

:leveloffset: 2

== Understanding Jaeger

Every time a user takes an action in an application, a request is executed by the architecture that may require dozens of different services to participate to produce a response.
The path of this request is a distributed transaction.
Jaeger lets you perform distributed tracing, which follows the path of a request through various microservices that make up an application.

*Distributed tracing* is a technique that is used to tie the information about different units of work together—usually executed in different processes or hosts—to understand a whole chain of events in a distributed transaction.
Distributed tracing lets developers visualize call flows in large service oriented architectures.
It can be invaluable in understanding serialization, parallelism, and sources of latency.

Jaeger records the execution of individual requests across the whole stack of microservices, and presents them as traces. A *trace* is a data/execution path through the system. An end-to-end trace is comprised of one or more spans.

A *span* represents a logical unit of work in Jaeger that has an operation name, the start time of the operation, and the duration. Spans may be nested and ordered to model causal relationships.

:leveloffset: +2

// Module included in the following assemblies:
//
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-0.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-1.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-2.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-3.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-4.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-5.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-6.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-7.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-8.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-2-9.adoc
// * distr_tracing/distr_tracing_rn/distr-tracing-rn-3-0.adoc
// * distr_tracing_arch/distr-tracing-architecture.adoc
// * service_mesh/v2x/ossm-architecture.adoc
// * serverless/serverless-tracing.adoc

:_mod-docs-content-type: CONCEPT
[id="distr-tracing-product-overview_{context}"]
= Distributed tracing overview

As a service owner, you can use distributed tracing to instrument your services to gather insights into your service architecture.
You can use the {DTProductName} for monitoring, network profiling, and troubleshooting the interaction between components in modern, cloud-native, microservices-based applications.

With the {DTShortName}, you can perform the following functions:

* Monitor distributed transactions

* Optimize performance and latency

* Perform root cause analysis

The {DTShortName} consists of three components:

* *{JaegerName}*, which is based on the open source link:https://www.jaegertracing.io/[Jaeger project].

* *{TempoName}*, which is based on the open source link:https://grafana.com/oss/tempo/[Grafana Tempo project].

* *{OTELNAME}*, which is based on the open source link:https://opentelemetry.io/[OpenTelemetry project].

[IMPORTANT]
====
Jaeger does not use FIPS validated cryptographic modules.
====

:leveloffset: 2

:leveloffset: +2

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-architecture.adoc
-service_mesh/v2x/ossm-architecture.adoc
-rhbjaeger-architecture.adoc
////
:_mod-docs-content-type: CONCEPT
[id="jaeger-architecture_{context}"]
= Distributed tracing architecture

The {JaegerShortName} is based on the open source link:https://www.jaegertracing.io/[Jaeger project]. The {JaegerShortName} is made up of several components that work together to collect, store, and display tracing data.

* *Jaeger Client* (Tracer, Reporter, instrumented application, client libraries)- Jaeger clients are language specific implementations of the OpenTracing API. They can be used to instrument applications for distributed tracing either manually or with a variety of existing open source frameworks, such as Camel (Fuse), Spring Boot (RHOAR), MicroProfile (RHOAR/Thorntail), Wildfly (EAP), and many more, that are already integrated with OpenTracing.

* *Jaeger Agent* (Server Queue, Processor Workers) - The Jaeger agent is a network daemon that listens for spans sent over User Datagram Protocol (UDP), which it batches and sends to the collector. The agent is meant to be placed on the same host as the instrumented application. This is typically accomplished by having a sidecar in container environments like Kubernetes.

* *Jaeger Collector* (Queue, Workers) - Similar to the Agent, the Collector is able to receive spans and place them in an internal queue for processing. This allows the collector to return immediately to the client/agent instead of waiting for the span to make its way to the storage.

* *Storage* (Data Store) - Collectors require a persistent storage backend. Jaeger has a pluggable mechanism for span storage. Note that for this release, the only supported storage is Elasticsearch.

* *Query* (Query Service) - Query is a service that retrieves traces from storage.

* *Ingester* (Ingester Service) - Jaeger can use Apache Kafka as a buffer between the collector and the actual backing storage (Elasticsearch). Ingester is a service that reads data from Kafka and writes to another storage backend (Elasticsearch).

* *Jaeger Console* – Jaeger provides a user interface that lets you visualize your distributed tracing data. On the Search page, you can find traces and explore details of the spans that make up an individual trace.

:leveloffset: 2

:leveloffset: +2

////
This module included in the following assemblies:
-service_mesh/v2x/ossm-architecture.adoc
-dist_tracing_arch/distr-tracing-architecture.adoc
////

[id="distr-tracing-features_{context}"]
= {DTProductName} features

{DTProductName} provides the following capabilities:

* Integration with Kiali – When properly configured, you can view {DTShortName} data from the Kiali console.

* High scalability – The {DTShortName} back end is designed to have no single points of failure and to scale with the business needs.

* Distributed Context Propagation – Enables you to connect data from different components together to create a complete end-to-end trace.

* Backwards compatibility with Zipkin – {DTProductName} has APIs that enable it to be used as a drop-in replacement for Zipkin, but Red Hat is not supporting Zipkin compatibility in this release.

:leveloffset: 2


== Next steps

* xref:preparing-ossm-installation-v1x[Prepare to install {SMProductName}] in your {product-title} environment.

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-vs-community-v1x"]
= Service Mesh and Istio differences
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-vs-istio-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

An installation of {SMProductName} differs from upstream Istio community installations in multiple ways. The modifications to {SMProductName} are sometimes necessary to resolve issues, provide additional features, or to handle differences when deploying on {product-title}.

The current release of {SMProductName} differs from the current upstream Istio community release in the following ways:

// The following include statements pull in the module files that comprise the assembly.

:leveloffset: +1

////
Module included in the following assemblies:
-ossm-vs-community.adoc
////

[id="ossm-multitenant-install_{context}"]
= Multitenant installations

Whereas upstream Istio takes a single tenant approach, {SMProductName} supports multiple independent control planes within the cluster. {SMProductName} uses a multitenant operator to manage the control plane lifecycle.

{SMProductName} installs a multitenant control plane by default. You specify the projects that can access the {SMProductShortName}, and isolate the {SMProductShortName} from other control plane instances.

[id="ossm-mt-vs-clusterwide_{context}"]
== Multitenancy versus cluster-wide installations

The main difference between a multitenant installation and a cluster-wide installation is the scope of privileges used by istod. The components no longer use cluster-scoped Role Based Access Control (RBAC) resource `ClusterRoleBinding`.

Every project in the `ServiceMeshMemberRoll` `members` list will have a `RoleBinding` for each service account associated with the control plane deployment and each control plane deployment will only watch those member projects. Each member project has a `maistra.io/member-of` label added to it, where the `member-of` value is the project containing the control plane installation.

{SMProductName} configures each member project to ensure network access between itself, the control plane, and other member projects. The exact configuration differs depending on how {product-title} software-defined networking (SDN) is configured. See About OpenShift SDN for additional details.

If the {product-title} cluster is configured to use the SDN plugin:

* *`NetworkPolicy`*: {SMProductName} creates a `NetworkPolicy` resource in each member project allowing ingress to all pods from the other members and the control plane. If you remove a member from {SMProductShortName}, this `NetworkPolicy` resource is deleted from the project.
+
[NOTE]
====
This also restricts ingress to only member projects. If you require ingress from non-member projects, you need to create a `NetworkPolicy` to allow that traffic through.
====

* *Multitenant*: {SMProductName} joins the `NetNamespace` for each member project to the `NetNamespace` of the control plane project (the equivalent of running `oc adm pod-network join-projects --to control-plane-project member-project`). If you remove a member from the {SMProductShortName}, its `NetNamespace` is isolated from the control plane (the equivalent of running `oc adm pod-network isolate-projects member-project`).

* *Subnet*: No additional configuration is performed.

[id="ossm-cluster-scoped-resources_{context}"]
== Cluster scoped resources

Upstream Istio has two cluster scoped resources that it relies on. The `MeshPolicy` and the `ClusterRbacConfig`. These are not compatible with a multitenant cluster and have been replaced as described below.

* _ServiceMeshPolicy_ replaces MeshPolicy for configuration of control-plane-wide authentication policies. This must be created in the same project as the control plane.
* _ServicemeshRbacConfig_ replaces ClusterRbacConfig for configuration of control-plane-wide role based access control. This must be created in the same project as the control plane.

:leveloffset: 2

:leveloffset: +1

////
Module included in the following assemblies:
-service_mesh/v1x/ossm-vs-community.adoc
////

[id="ossm-vs-istio_{context}"]
= Differences between Istio and {SMProductName}

An installation of {SMProductName} differs from an installation of Istio in multiple ways. The modifications to {SMProductName} are sometimes necessary to resolve issues, provide additional features, or to handle differences when deploying on {product-title}.

[id="ossm-cli-tool_{context}"]
== Command line tool

The command line tool for {SMProductName} is oc.  {SMProductName}  does not support istioctl.

[id="ossm-automatic-injection_{context}"]
== Automatic injection

The upstream Istio community installation automatically injects the sidecar into pods within the projects you have labeled.

{SMProductName} does not automatically inject the sidecar to any pods, but requires you to opt in to injection using an annotation without labeling projects. This method requires fewer privileges and does not conflict with other OpenShift capabilities such as builder pods. To enable automatic injection you specify the `sidecar.istio.io/inject` annotation as described in the Automatic sidecar injection section.

[id="ossm-rbac_{context}"]
== Istio Role Based Access Control features

Istio Role Based Access Control (RBAC) provides a mechanism you can use to control access to a service. You can identify subjects by user name or by specifying a set of properties and apply access controls accordingly.

The upstream Istio community installation includes options to perform exact header matches, match wildcards in headers, or check for a header containing a specific prefix or suffix.

{SMProductName} extends the ability to match request headers by using a regular expression. Specify a property key of `request.regex.headers` with a regular expression.

.Upstream Istio community matching request headers example
[source,yaml]
----
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: httpbin-client-binding
  namespace: httpbin
spec:
  subjects:
  - user: "cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"
    properties:
      request.headers[<header>]: "value"
----

.{SMProductName} matching request headers by using regular expressions
[source,yaml]
----
apiVersion: "rbac.istio.io/v1alpha1"
kind: ServiceRoleBinding
metadata:
  name: httpbin-client-binding
  namespace: httpbin
spec:
  subjects:
  - user: "cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"
    properties:
      request.regex.headers[<header>]: "<regular expression>"
----


[id="ossm-openssl_{context}"]
== OpenSSL

{SMProductName} replaces BoringSSL with OpenSSL. OpenSSL is a software library that contains an open source implementation of the Secure Sockets Layer (SSL) and Transport Layer Security (TLS) protocols. The {SMProductName} Proxy binary dynamically links the OpenSSL libraries (libssl and libcrypto) from the underlying Red Hat Enterprise Linux operating system.

[id="ossm-component-modifications_{context}"]
== Component modifications

* A _maistra-version_ label has been added to all resources.
* All Ingress resources have been converted to OpenShift Route resources.
* Grafana, Tracing (Jaeger), and Kiali are enabled by default and exposed through OpenShift routes.
* Godebug has been removed from all templates
* The `istio-multi` ServiceAccount and ClusterRoleBinding have been removed, as well as the `istio-reader` ClusterRole.

[id="ossm-envoy-sds-ca_{context}"]
== Envoy, Secret Discovery Service, and certificates

* {SMProductName} does not support QUIC-based services.
* Deployment of TLS certificates using the Secret Discovery Service (SDS) functionality of Istio is not currently supported in {SMProductName}. The Istio implementation depends on a nodeagent container that uses hostPath mounts.

[id="ossm-cni_{context}"]
== Istio Container Network Interface (CNI) plugin

{SMProductName} includes CNI plugin, which provides you with an alternate way to configure application pod networking. The CNI plugin replaces the `init-container` network configuration eliminating the need to grant service accounts and projects access to Security Context Constraints (SCCs) with elevated privileges.

[id="ossm-routes-gateways_{context}"]
== Routes for Istio Gateways

OpenShift routes for Istio Gateways are automatically managed in {SMProductName}. Every time an Istio Gateway is created, updated or deleted inside the service mesh, an OpenShift route is created, updated or deleted.

A {SMProductName} control plane component called Istio OpenShift Routing (IOR) synchronizes the gateway route.  For more information, see Automatic route creation.

[id="ossm-catch-all-domains_{context}"]
=== Catch-all domains
Catch-all domains ("\*") are not supported. If one is found in the Gateway definition, {SMProductName} _will_ create the route, but will rely on OpenShift to create a default hostname. This means that the newly created route will __not__ be a catch all ("*") route, instead it will have a hostname in the form `<route-name>[-<project>].<suffix>`. See the OpenShift documentation for more information about how default hostnames work and how a cluster administrator can customize it.

[id="ossm-subdomains_{context}"]
=== Subdomains
Subdomains (e.g.: "*.domain.com") are supported. However this ability doesn't come enabled by default in {product-title}. This means that {SMProductName} _will_ create the route with the subdomain, but it will only be in effect if {product-title} is configured to enable it.

[id="ossm-tls_{context}"]
=== Transport layer security
Transport Layer Security (TLS) is supported. This means that, if the Gateway contains a `tls` section, the OpenShift Route will be configured to support TLS.

:leveloffset: 2
[discrete]
[id="additional-resources_ossm-vs-istio-v1x"]
[role="_additional-resources"]
==== Additional resources

* xref:ossm-auto-route-1x_routing-traffic-v1x[Automatic route creation]

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-vs-community.adoc
-service_mesh/v2x/ossm-vs-community.adoc
////
:_mod-docs-content-type: CONCEPT
[id="ossm-kiali-service-mesh_{context}"]
= Kiali and service mesh

Installing Kiali via the Service Mesh on {product-title} differs from community Kiali installations in multiple ways. These modifications are sometimes necessary to resolve issues, provide additional features, or to handle differences when deploying on {product-title}.

* Kiali has been enabled by default.
* Ingress has been enabled by default.
* Updates have been made to the Kiali ConfigMap.
* Updates have been made to the ClusterRole settings for Kiali.
* Do not edit the ConfigMap, because your changes might be overwritten by the {SMProductShortName} or Kiali Operators. Files that the Kiali Operator manages have a `kiali.io/` label or annotation. Updating the Operator files should be restricted to those users with `cluster-admin` privileges. If you use {product-dedicated}, updating the Operator files should be restricted to those users with `dedicated-admin` privileges.

:leveloffset: 2

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
-service_mesh/v1x/ossm-vs-community.adoc
-service_mesh/v2x/ossm-vs-community.adoc
////

[id="ossm-jaeger-service-mesh_{context}"]
= Distributed tracing and service mesh

Installing the {JaegerShortName} with the Service Mesh on {product-title} differs from community Jaeger installations in multiple ways. These modifications are sometimes necessary to resolve issues, provide additional features, or to handle differences when deploying on {product-title}.

* Distributed tracing has been enabled by default for {SMProductShortName}.
* Ingress has been enabled by default for {SMProductShortName}.
* The name for the Zipkin port name has changed to `jaeger-collector-zipkin` (from `http`)
* Jaeger uses Elasticsearch for storage by default when you select either the `production` or `streaming` deployment option.
* The community version of Istio provides a generic "tracing" route. {SMProductName} uses a "jaeger" route that is installed by the {JaegerName} Operator and is already protected by OAuth.
* {SMProductName} uses a sidecar for the Envoy proxy, and Jaeger also uses a sidecar, for the Jaeger agent.
These two sidecars are configured separately and should not be confused with each other. The proxy sidecar creates spans related to the pod's ingress and egress traffic. The agent sidecar receives the spans emitted by the application and sends them to the Jaeger Collector.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="preparing-ossm-installation-v1x"]
= Preparing to install Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: preparing-ossm-installation-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

Before you can install {SMProductName}, review the installation activities, ensure that you meet the prerequisites:

== Prerequisites

* Possess an active {product-title} subscription on your Red Hat account. If you do not have a subscription, contact your sales representative for more information.

* Review the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#installation-overview_architecture-installation[{product-title} {product-version} overview].
* Install {product-title} {product-version}.
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-account[Install {product-title} {product-version} on AWS]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-user-infra[Install {product-title} {product-version} on user-provisioned AWS]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-bare-metal[Install {product-title} {product-version} on bare metal]
** link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-vsphere[Install {product-title} {product-version} on vSphere]
+
[NOTE]
====
If you are installing {SMProductName} on a link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#supported-installation-methods-for-different-platforms[restricted network], follow the instructions for your chosen {product-title} infrastructure.
====
+

* Install the version of the {product-title} command line utility (the `oc` client tool) that matches your {product-title} version and add it to your path.

** If you are using {product-title} {product-version}, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/cli_tools/#cli-about-cli_cli-developer-commands[About the OpenShift CLI].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/preparing-ossm-install.adoc
// * service_mesh/v1x/servicemesh-release-notes.adoc
// * post_installation_configuration/network-configuration.adoc

[id="ossm-supported-configurations-v1x_{context}"]
= {SMProductName} supported configurations

The following are the only supported configurations for the {SMProductName}:

* {product-title} version 4.6 or later.

[NOTE]
====
OpenShift Online and {product-dedicated} are not supported for {SMProductName}.
====

* The deployment must be contained within a single {product-title} cluster that is not federated.
* This release of {SMProductName} is only available on {product-title} x86_64.
* This release only supports configurations where all {SMProductShortName} components are contained in the {product-title} cluster in which it operates. It does not support management of microservices that reside outside of the cluster, or in a multi-cluster scenario.
* This release only supports configurations that do not integrate external services such as virtual machines.

For additional information about {SMProductName} lifecycle and supported configurations, refer to the link:https://access.redhat.com/support/policy/updates/openshift#ossm[Support Policy].

[id="ossm-supported-configurations-kiali_{context}"]
== Supported configurations for Kiali on {SMProductName}

* The Kiali observability console is only supported on the two most recent releases of the Chrome, Edge, Firefox, or Safari browsers.

[id="ossm-supported-configurations-adapters_{context}"]
== Supported Mixer adapters

* This release only supports the following Mixer adapter:
** 3scale Istio Adapter

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/preparing-ossm-installation.adoc
// * service_mesh/v2x/preparing-ossm-installation.adoc
// * post_installation_configuration/network-configuration.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-installation-activities_{context}"]
= Operator overview

{SMProductName} requires the following Operators:

* *OpenShift Elasticsearch* - (Optional) Provides database storage for tracing and logging with the {JaegerShortName}. It is based on the open source link:https://www.elastic.co/[Elasticsearch] project.
* *{JaegerName}* - Provides distributed tracing to monitor and troubleshoot transactions in complex distributed systems. It is based on the open source link:https://www.jaegertracing.io/[Jaeger] project.
* *Kiali Operator provided by Red Hat* - Provides observability for your service mesh. You can view configurations, monitor traffic, and analyze traces in a single console. It is based on the open source link:https://www.kiali.io/[Kiali] project.
* *{SMProductName}* - Allows you to connect, secure, control, and observe the microservices that comprise your applications. The {SMProductShortName} Operator defines and monitors the `ServiceMeshControlPlane` resources that manage the deployment, updating, and deletion of the {SMProductShortName} components. It is based on the open source link:https://istio.io/[Istio] project.

:leveloffset: 2

[WARNING]
====
See link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#logging-config-es-store[Configuring the Elasticsearch log store] for details on configuring the default Jaeger parameters for Elasticsearch in a production environment.
====

== Next steps

* xref:installing-ossm-v1x[Install {SMProductName}] in your {product-title} environment.

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="installing-ossm-v1x"]
= Installing Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: installing-ossm-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

Installing the {SMProductShortName} involves installing the OpenShift Elasticsearch, Jaeger, Kiali and {SMProductShortName} Operators, creating and managing a `ServiceMeshControlPlane` resource to deploy the control plane, and creating a `ServiceMeshMemberRoll` resource to specify the namespaces associated with the {SMProductShortName}.

[NOTE]
====
Mixer's policy enforcement is disabled by default. You must enable it to run policy tasks. See xref:ossm-mixer-policy-1x_deploying-applications-ossm-v1x[Update Mixer policy enforcement] for instructions on enabling Mixer policy enforcement.
====

[NOTE]
====
Multi-tenant control plane installations are the default configuration.
====

[NOTE]
====
The {SMProductShortName} documentation uses `istio-system` as the example project, but you can deploy the service mesh to any project.
====

== Prerequisites
* Follow the xref:preparing-ossm-installation-v1x[Preparing to install {SMProductName}] process.
* An account with the `cluster-admin` role.

The {SMProductShortName} installation process uses the link:https://operatorhub.io/[OperatorHub] to install the `ServiceMeshControlPlane` custom resource definition within the `openshift-operators` project. The {SMProductName} defines and monitors the `ServiceMeshControlPlane` related to the deployment, update, and deletion of the control plane.

Starting with {SMProductName} {SMProductVersion1x}, you must install the OpenShift Elasticsearch Operator, the Jaeger Operator, and the Kiali Operator before the {SMProductName} Operator can install the control plane.

:leveloffset: +1

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-installing.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="distr-tracing-operator-install-elasticsearch_{context}"]
= Installing the OpenShift Elasticsearch Operator

The default {JaegerName} deployment uses in-memory storage because it is designed to be installed quickly for those evaluating {DTProductName}, giving demonstrations, or using {JaegerName} in a test environment. If you plan to use {JaegerName} in production, you must install and configure a persistent storage option, in this case, Elasticsearch.

.Prerequisites
* You have access to the {product-title} web console.
* You have access to the cluster as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

[WARNING]
====
Do not install Community versions of the Operators. Community Operators are not supported.
====

[NOTE]
====
If you have already installed the OpenShift Elasticsearch Operator as part of OpenShift Logging, you do not need to install the OpenShift Elasticsearch Operator again. The {JaegerName} Operator creates the Elasticsearch instance using the installed OpenShift Elasticsearch Operator.
====

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Operators* -> *OperatorHub*.

. Type *Elasticsearch* into the filter box to locate the OpenShift Elasticsearch Operator.

. Click the *OpenShift Elasticsearch Operator* provided by Red Hat to display information about the Operator.

. Click *Install*.

. On the *Install Operator* page, select the *stable* Update Channel. This automatically updates your Operator as new versions are released.

. Accept the default *All namespaces on the cluster (default)*. This installs the Operator in the default `openshift-operators-redhat` project and makes the Operator available to all projects in the cluster.
+
[NOTE]
====
The Elasticsearch installation requires the *openshift-operators-redhat* namespace for the OpenShift Elasticsearch Operator. The other {DTProductName} Operators are installed in the `openshift-operators` namespace.
====
+

. Accept the default *Automatic* approval strategy. By accepting the default, when a new version of this Operator is available, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without human intervention. If you select *Manual* updates, when a newer version of an Operator is available, OLM creates an update request. As a cluster administrator, you must then manually approve that update request to have the Operator updated to the new version.
+
[NOTE]
====
The *Manual* approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.
====

. Click *Install*.

. On the *Installed Operators* page, select the `openshift-operators-redhat` project. Wait until you see that the OpenShift Elasticsearch Operator shows a status of "InstallSucceeded" before continuing.

:leveloffset: 2

:leveloffset: +1

////
This module included in the following assemblies:
- distr_tracing_jaeger/distr-tracing-jaeger-installing.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="distr-tracing-jaeger-operator-install_{context}"]
= Installing the {JaegerName} Operator

To install {JaegerName}, you use the link:https://operatorhub.io/[OperatorHub] to install the {JaegerName} Operator.

By default, the Operator is installed in the `openshift-operators` project.

.Prerequisites
* You have access to the {product-title} web console.
* You have access to the cluster as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
* If you require persistent storage, you must also install the OpenShift Elasticsearch Operator before installing the {JaegerName} Operator.

[WARNING]
====
Do not install Community versions of the Operators. Community Operators are not supported.
====

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Operators* -> *OperatorHub*.

. Type *distributed tracing platform* into the filter to locate the {JaegerName} Operator.

. Click the *{JaegerName} Operator* provided by Red Hat to display information about the Operator.

. Click *Install*.

. On the *Install Operator* page, select the *stable* Update Channel. This automatically updates your Operator as new versions are released.
//If you select a maintenance channel, for example, *Stable*, you will receive bug fixes and security patches for the length of the support cycle for that version.

. Accept the default *All namespaces on the cluster (default)*. This installs the Operator in the default `openshift-operators` project and makes the Operator available to all projects in the cluster.

* Accept the default *Automatic* approval strategy. By accepting the default, when a new version of this Operator is available, Operator Lifecycle Manager (OLM) automatically upgrades the running instance of your Operator without human intervention. If you select *Manual* updates, when a newer version of an Operator is available, OLM creates an update request. As a cluster administrator, you must then manually approve that update request to have the Operator updated to the new version.
+
[NOTE]
====
The *Manual* approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.
====

. Click *Install*.

. Navigate to *Operators* -> *Installed Operators*.

. On the *Installed Operators* page, select the `openshift-operators` project. Wait until you see that the {JaegerName} Operator shows a status of "Succeeded" before continuing.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-install-kiali_{context}"]
= Installing the Kiali Operator

You must install the Kiali Operator for the {SMProductName} Operator to install the {SMProductShortName} control plane.

[WARNING]
====
Do not install Community versions of the Operators. Community Operators are not supported.
====


.Prerequisites

* Access to the {product-title} web console.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *OperatorHub*.

. Type *Kiali* into the filter box to find the Kiali Operator.

. Click the *Kiali Operator* provided by Red Hat to display information about the Operator.

. Click *Install*.

. On the *Operator Installation* page, select the *stable* Update Channel.

. Select *All namespaces on the cluster (default)*. This installs the Operator in the default `openshift-operators` project and makes the Operator available to all projects in the cluster.

. Select the *Automatic* Approval Strategy.
+
[NOTE]
====
The Manual approval strategy requires a user with appropriate credentials to approve the Operator install and subscription process.
====

. Click *Install*.

. The *Installed Operators* page displays the Kiali Operator's installation progress.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// - service_mesh/v1x/installing-ossm.adoc
// - service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-install-ossm-operator_{context}"]
= Installing the Operators

To install {SMProductName}, install the following Operators in this order. Repeat the procedure for each Operator.

* OpenShift Elasticsearch
* {JaegerName}
* Kiali Operator provided by Red Hat
* {SMProductName}

[NOTE]
====
If you have already installed the OpenShift Elasticsearch Operator as part of OpenShift Logging, you do not need to install the OpenShift Elasticsearch Operator again. The {JaegerName} Operator will create the Elasticsearch instance using the installed OpenShift Elasticsearch Operator.
====

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. In the {product-title} web console, click *Operators* -> *OperatorHub*.

. Type the name of the Operator into the filter box and select the Red Hat version of the Operator. Community versions of the Operators are not supported.

. Click *Install*.

. On the *Install Operator* page for each Operator, accept  the default settings.

. Click *Install*. Wait until the Operator has installed before repeating the steps for the next Operator in the list.
+
* The OpenShift Elasticsearch Operator is installed in the `openshift-operators-redhat` namespace and is available for all namespaces in the cluster.
* The {JaegerName} is installed in the `openshift-distributed-tracing` namespace and is available for all namespaces in the cluster.
* The Kiali Operator provided by Red Hat and the {SMProductName} Operator are installed in the `openshift-operators` namespace and are available for all namespaces in the cluster.

. After all you have installed all four Operators, click *Operators* -> *Installed Operators* to verify that your Operators installed.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-deploy-1x_{context}"]
= Deploying the {SMProductName} control plane

////
TODO - Flesh out how multitenancy affects this, link to control plate template topic.
////

The `ServiceMeshControlPlane` resource defines the configuration to be used during installation. You can deploy the default configuration provided by Red Hat or customize the `ServiceMeshControlPlane` file to fit your business needs.

You can deploy the {SMProductShortName} control plane by using the {product-title} web console or from the command line using the `oc` client tool.

[id="ossm-control-plane-deploy-operatorhub_{context}"]
== Deploying the control plane from the web console

Follow this procedure to deploy the {SMProductName} control plane by using the web console.  In this example, `istio-system` is the name of the control plane project.

.Prerequisites

* The {SMProductName} Operator must be installed.
* Review the instructions for how to customize the {SMProductName} installation.
* An account with the `cluster-admin` role.

.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role.

. Create a project named `istio-system`.

.. Navigate to *Home* -> *Projects*.

.. Click *Create Project*.

.. Enter `istio-system` in the *Name* field.

.. Click *Create*.

. Navigate to *Operators* -> *Installed Operators*.

. If necessary, select `istio-system` from the Project menu.  You may have to wait a few moments for the Operators to be copied to the new project.

. Click the {SMProductName} Operator.  Under *Provided APIs*, the Operator provides links to create two resource types:
** A `ServiceMeshControlPlane` resource
** A `ServiceMeshMemberRoll` resource

. Under *Istio Service Mesh Control Plane* click *Create ServiceMeshControlPlane*.

. On the *Create Service Mesh Control Plane* page, modify the YAML for the default `ServiceMeshControlPlane` template as needed.
+
[NOTE]
====
For additional information about customizing the control plane, see customizing the {SMProductName} installation. For production, you _must_ change the default Jaeger template.
====

. Click *Create* to create the control plane.  The Operator creates pods, services, and {SMProductShortName} control plane components based on your configuration parameters.

. Click the *Istio Service Mesh Control Plane* tab.

. Click the name of the new control plane.

. Click the *Resources* tab to see the {SMProductName} control plane resources the Operator created and configured.


[id="ossm-control-plane-deploy-cli_{context}"]
== Deploying the control plane from the CLI

Follow this procedure to deploy the {SMProductName} control plane the command line.

.Prerequisites

* The {SMProductName} Operator must be installed.
* Review the instructions for how to customize the {SMProductName} installation.
* An account with the `cluster-admin` role.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a user with the `cluster-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----

. Create a project named `istio-system`.
+
[source,terminal]
----
$ oc new-project istio-system
----

. Create a `ServiceMeshControlPlane` file named `istio-installation.yaml` using the example found in "Customize the {SMProductName} installation". You can customize the values as needed to match your use case.  For production deployments you _must_ change the default Jaeger template.

. Run the following command to deploy the control plane:
+
[source,terminal]
----
$ oc create -n istio-system -f istio-installation.yaml
----
+
. Execute the following command to see the status of the control plane installation.
+
[source,terminal]
----
$ oc get smcp -n istio-system
----
+
The installation has finished successfully when the STATUS column is `ComponentsReady`.
+
----
NAME            READY   STATUS            PROFILES      VERSION   AGE
basic-install   11/11   ComponentsReady   ["default"]   v1.1.18   4m25s
----
+
. Run the following command to watch the progress of the Pods during the installation process:
+
----
$ oc get pods -n istio-system -w
----
+
You should see output similar to the following:
+
.Example output
[source,terminal]
----
NAME                                     READY   STATUS             RESTARTS   AGE
grafana-7bf5764d9d-2b2f6                 2/2     Running            0          28h
istio-citadel-576b9c5bbd-z84z4           1/1     Running            0          28h
istio-egressgateway-5476bc4656-r4zdv     1/1     Running            0          28h
istio-galley-7d57b47bb7-lqdxv            1/1     Running            0          28h
istio-ingressgateway-dbb8f7f46-ct6n5     1/1     Running            0          28h
istio-pilot-546bf69578-ccg5x             2/2     Running            0          28h
istio-policy-77fd498655-7pvjw            2/2     Running            0          28h
istio-sidecar-injector-df45bd899-ctxdt   1/1     Running            0          28h
istio-telemetry-66f697d6d5-cj28l         2/2     Running            0          28h
jaeger-896945cbc-7lqrr                   2/2     Running            0          11h
kiali-78d9c5b87c-snjzh                   1/1     Running            0          22h
prometheus-6dff867c97-gr2n5              2/2     Running            0          28h
----

:leveloffset: 2

For a multitenant installation, {SMProductName} supports multiple independent control planes within the cluster.  You can create reusable configurations with `ServiceMeshControlPlane` templates.  For more information, see xref:ossm-control-plane-templates-1x_deploying-applications-ossm-v1x[Creating control plane templates].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-member-roll-create_{context}"]
= Creating the {SMProductName} member roll

The `ServiceMeshMemberRoll` lists the projects that belong to the {SMProductShortName} control plane. Only projects listed in the `ServiceMeshMemberRoll` are affected by the control plane. A project does not belong to a service mesh until you add it to the member roll for a particular control plane deployment.

You must create a `ServiceMeshMemberRoll` resource named `default` in the same project as the `ServiceMeshControlPlane`, for example `istio-system`.

[id="ossm-member-roll-create-console_{context}"]
== Creating the member roll from the web console

You can add one or more projects to the {SMProductShortName} member roll from the web console. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

.Prerequisites
* An installed, verified {SMProductName} Operator.
* List of existing projects to add to the service mesh.

.Procedure

. Log in to the {product-title} web console.

. If you do not already have services for your mesh, or you are starting from scratch, create a project for your applications. It must be different from the project where you installed the {SMProductShortName} control plane.

.. Navigate to *Home* -> *Projects*.

.. Enter a name in the *Name* field.

.. Click *Create*.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose the project where your `ServiceMeshControlPlane` resource is deployed from the list, for example `istio-system`.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Member Roll* tab.

. Click *Create ServiceMeshMemberRoll*

. Click *Members*, then enter the name of your project in the *Value* field. You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.

. Click *Create*.

[id="ossm-member-roll-create-cli_{context}"]
== Creating the member roll from the CLI

You can add a project to the `ServiceMeshMemberRoll` from the command line.

.Prerequisites

* An installed, verified {SMProductName} Operator.
* List of projects to add to the service mesh.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----

. If you do not already have services for your mesh, or you are starting from scratch, create a project for your applications. It must be different from the project where you installed the {SMProductShortName} control plane.
+
[source,terminal]
----
$ oc new-project <your-project>
----

. To add your projects as members, modify the following example YAML. You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
.Example servicemeshmemberroll-default.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system
spec:
  members:
    # a list of projects joined into the service mesh
    - your-project-name
    - another-project-name
----

. Run the following command to upload and create the `ServiceMeshMemberRoll` resource in the `istio-system` namespace.
+
[source,terminal]
----
$ oc create -n istio-system -f servicemeshmemberroll-default.yaml
----

. Run the following command to verify the `ServiceMeshMemberRoll` was created successfully.
+
[source,terminal]
----
$ oc get smmr -n istio-system default
----
+
The installation has finished successfully when the `STATUS` column is `Configured`.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-member-roll-modify_{context}"]
= Adding or removing projects from the service mesh

You can add or remove projects from an existing {SMProductShortName} `ServiceMeshMemberRoll` resource using the web console.

* You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.

* The `ServiceMeshMemberRoll` resource is deleted when its corresponding `ServiceMeshControlPlane` resource is deleted.

[id="ossm-member-roll-modify-console_{context}"]
== Adding or removing projects from the member roll using the web console

.Prerequisites
* An installed, verified {SMProductName} Operator.
* An existing `ServiceMeshMemberRoll` resource.
* Name of the project with the `ServiceMeshMemberRoll` resource.
* Names of the projects you want to add or remove from the mesh.

.Procedure

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose the project where your `ServiceMeshControlPlane` resource is deployed from the list, for example `istio-system`.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Member Roll* tab.

. Click the `default` link.

. Click the YAML tab.

. Modify the YAML to add or remove projects as members.  You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.

. Click *Save*.

. Click *Reload*.

[id="ossm-member-roll-modify-cli_{context}"]
== Adding or removing projects from the member roll using the CLI

You can modify an existing {SMProductShortName} member roll using the command line.

.Prerequisites

* An installed, verified {SMProductName} Operator.
* An existing `ServiceMeshMemberRoll` resource.
* Name of the project with the `ServiceMeshMemberRoll` resource.
* Names of the projects you want to add or remove from the mesh.
* Access to the OpenShift CLI (`oc`).


.Procedure

. Log in to the {product-title} CLI.

. Edit the `ServiceMeshMemberRoll` resource.
+
[source,terminal]
----
$ oc edit smmr -n <controlplane-namespace>
----
+

. Modify the YAML to add or remove projects as members.  You can add any number of projects, but a project can only belong to *one* `ServiceMeshMemberRoll` resource.

+
.Example servicemeshmemberroll-default.yaml

[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
  namespace: istio-system #control plane project
spec:
  members:
    # a list of projects joined into the service mesh
    - your-project-name
    - another-project-name
----

:leveloffset: 2

== Manual updates

If you choose to update manually, the Operator Lifecycle Manager (OLM) controls the installation, upgrade, and role-based access control (RBAC) of Operators in a cluster. OLM runs by default in {product-title}.
OLM uses CatalogSources, which use the Operator Registry API, to query for available Operators as well as upgrades for installed Operators.

* For more information about how {product-title} handled upgrades, refer to the link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-overview_olm-understanding-olm[Operator Lifecycle Manager] documentation.

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-update-app-sidecar_{context}"]
= Updating sidecar proxies

In order to update the configuration for sidecar proxies the application administrator must restart the application pods.

If your deployment uses automatic sidecar injection, you can update the pod template in the deployment by adding or modifying an annotation. Run the following command to redeploy the pods:

[source,terminal]
----
$ oc patch deployment/<deployment> -p '{"spec":{"template":{"metadata":{"annotations":{"kubectl.kubernetes.io/restartedAt": "'`date -Iseconds`'"}}}}}'
----

If your deployment does not use automatic sidecar injection, you must manually update the sidecars by modifying the sidecar container image specified in the deployment or pod, and then restart the pods.

:leveloffset: 2

== Next steps

* xref:deploying-applications-ossm-v1x[Prepare to deploy applications] on {SMProductName}.

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-security-v1x"]
= Customizing security in a Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-security-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

If your service mesh application is constructed with a complex array of microservices, you can use {SMProductName} to customize the security of the communication between those services. The infrastructure of {product-title} along with the traffic management features of {SMProductShortName} can help you manage the complexity of your applications and provide service and identity security for microservices.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-security.adoc

[id="ossm-security-mtls_{context}"]
= Enabling mutual Transport Layer Security (mTLS)

Mutual Transport Layer Security (mTLS) is a protocol where two parties authenticate each other. It is the default mode of authentication in some protocols (IKE, SSH) and optional in others (TLS).

mTLS can be used without changes to the application or service code. The TLS is handled entirely by the service mesh infrastructure and between the two sidecar proxies.

By default, {SMProductName} is set to permissive mode, where the sidecars in {SMProductShortName} accept both plain-text traffic and connections that are encrypted using mTLS. If a service in your mesh is communicating with a service outside the mesh, strict mTLS could break communication between those services. Use permissive mode while you migrate your workloads to {SMProductShortName}.

[id="ossm-security-enabling-strict-mtls_{context}"]
== Enabling strict mTLS across the mesh

If your workloads do not communicate with services outside your mesh and communication will not be interrupted by only accepting encrypted connections, you can enable mTLS across your mesh quickly. Set `spec.istio.global.mtls.enabled` to `true` in your `ServiceMeshControlPlane` resource. The operator creates the required resources.

[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    global:
      mtls:
        enabled: true
----

[id="ossm-security-mtls-sidecars-incoming-services_{context}"]
=== Configuring sidecars for incoming connections for specific services

You can also configure mTLS for individual services or namespaces by creating a policy.

[source,yaml]
----
apiVersion: "authentication.istio.io/v1alpha1"
kind: "Policy"
metadata:
  name: default
  namespace: <NAMESPACE>
spec:
  peers:
    - mtls: {}
----

[id="ossm-security-mtls-sidecars-outgoing_{context}"]
== Configuring sidecars for outgoing connections

Create a destination rule to configure {SMProductShortName} to use mTLS when sending requests to other services in the mesh.

[source,yaml]
----
apiVersion: "networking.istio.io/v1alpha3"
kind: "DestinationRule"
metadata:
  name: "default"
  namespace: <CONTROL_PLANE_NAMESPACE>>
spec:
  host: "*.local"
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
----

[id="ossm-security-min-max-tls_{context}"]
== Setting the minimum and maximum protocol versions

If your environment has specific requirements for encrypted traffic in your service mesh, you can control the cryptographic functions that are allowed by setting the `spec.security.controlPlane.tls.minProtocolVersion` or `spec.security.controlPlane.tls.maxProtocolVersion` in your `ServiceMeshControlPlane` resource. Those values, configured in your control plane resource, define the minimum and maximum TLS version used by mesh components when communicating securely over TLS.

[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    global:
      tls:
        minProtocolVersion: TLSv1_2
        maxProtocolVersion: TLSv1_3
----

The default is `TLS_AUTO` and does not specify a version of TLS.

.Valid values
|===
|Value|Description

|`TLS_AUTO`
| default

|`TLSv1_0`
|TLS version 1.0

|`TLSv1_1`
|TLS version 1.1

|`TLSv1_2`
|TLS version 1.2

|`TLSv1_3`
|TLS version 1.3
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-security.adoc

[id="ossm-security-cipher-1x_{context}"]
= Configuring cipher suites and ECDH curves

Cipher suites and Elliptic-curve Diffie–Hellman (ECDH curves) can help you secure your service mesh. You can define a comma separated list of cipher suites using `spec.istio.global.tls.cipherSuites` and ECDH curves using `spec.istio.global.tls.ecdhCurves` in your `ServiceMeshControlPlane` resource. If either of these attributes are empty, then the default values are used.

The `cipherSuites` setting is effective if your service mesh uses TLS 1.2 or earlier. It has no effect when negotiating with TLS 1.3.

Set your cipher suites in the comma separated list in order of priority. For example, `ecdhCurves: CurveP256, CurveP384` sets `CurveP256` as a higher priority than `CurveP384`.

[NOTE]
====
You must include either `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256` or  `TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256` when you configure the cipher suite. HTTP/2 support requires at least one of these cipher suites.

====

The supported cipher suites are:

* TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256
* TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256
* TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256
* TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256
* TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384
* TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA
* TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA256
* TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA
* TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA
* TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA
* TLS_RSA_WITH_AES_128_GCM_SHA256
* TLS_RSA_WITH_AES_256_GCM_SHA384
* TLS_RSA_WITH_AES_128_CBC_SHA256
* TLS_RSA_WITH_AES_128_CBC_SHA
* TLS_RSA_WITH_AES_256_CBC_SHA
* TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA
* TLS_RSA_WITH_3DES_EDE_CBC_SHA

The supported ECDH Curves are:

* CurveP256
* CurveP384
* CurveP521
* X25519

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-security.adoc


[id="ossm-cert-manage-1x_{context}"]
= Adding an external certificate authority key and certificate

By default, {SMProductName} generates self-signed root certificate and key, and uses them to sign the workload certificates. You can also use the user-defined certificate and key to sign workload certificates, with user-defined root certificate. This task demonstrates an example to plug certificates and key into {SMProductShortName}.

.Prerequisites

* You must have installed {SMProductName} with mutual TLS enabled to configure certificates.
* This example uses the certificates from the link:https://github.com/maistra/istio/tree/maistra-2.0/samples/certs[Maistra repository]. For production, use your own certificates from your certificate authority.
* You must deploy the Bookinfo sample application to verify the results with these instructions.

[id="ossm-cert-manage-add-cert-key-1x_{context}"]
== Adding an existing certificate and key

To use an existing signing (CA) certificate and key, you must create a chain of trust file that includes the CA certificate, key, and root certificate. You must use the following exact file names for each of the corresponding certificates. The CA certificate is called `ca-cert.pem`, the key is `ca-key.pem`, and the root certificate, which signs `ca-cert.pem`, is called `root-cert.pem`. If your workload uses intermediate certificates, you must specify them in a `cert-chain.pem` file.

Add the certificates to {SMProductShortName} by following these steps. Save the example certificates from the link:https://github.com/maistra/istio/tree/maistra-1.1/samples/certs[Maistra repo] locally and replace `<path>` with the path to your certificates.

. Create a secret `cacert` that includes the input files `ca-cert.pem`, `ca-key.pem`, `root-cert.pem` and `cert-chain.pem`.
+
[source,terminal]
----
$ oc create secret generic cacerts -n istio-system --from-file=<path>/ca-cert.pem \
    --from-file=<path>/ca-key.pem --from-file=<path>/root-cert.pem \
    --from-file=<path>/cert-chain.pem
----
+
. In the `ServiceMeshControlPlane` resource set `global.mtls.enabled` to `true` and `security.selfSigned` set to `false`. {SMProductShortName} reads the certificates and key from the secret-mount files.
+
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    global:
      mtls:
        enabled: true
    security:
      selfSigned: false
----
+
. To make sure the workloads add the new certificates promptly, delete the secrets generated by {SMProductShortName}, named `istio.*`. In this example, `istio.default`. {SMProductShortName} issues new certificates for the workloads.
+
[source,terminal]
----
$ oc delete secret istio.default
----

[id="ossm-cert-manage-verify-cert-1x_{context}"]
== Verifying your certificates

Use the Bookinfo sample application to verify your certificates are mounted correctly. First, retrieve the mounted certificates. Then, verify the certificates mounted on the pod.

. Store the pod name in the variable `RATINGSPOD`.
+
[source,terminal]
----
$ RATINGSPOD=`oc get pods -l app=ratings -o jsonpath='{.items[0].metadata.name}'`
----
+
. Run the following commands to retrieve the certificates mounted on the proxy.
+
[source,terminal]
----
$ oc exec -it $RATINGSPOD -c istio-proxy -- /bin/cat /etc/certs/root-cert.pem > /tmp/pod-root-cert.pem
----
+
The file `/tmp/pod-root-cert.pem` contains the root certificate propagated to the pod.
+
[source,terminal]
----
$ oc exec -it $RATINGSPOD -c istio-proxy -- /bin/cat /etc/certs/cert-chain.pem > /tmp/pod-cert-chain.pem
----
+
The file `/tmp/pod-cert-chain.pem` contains the workload certificate and the CA certificate propagated to the pod.
+
. Verify the root certificate is the same as the one specified by the Operator. Replace `<path>` with the path to your certificates.
+
[source,terminal]
----
$ openssl x509 -in <path>/root-cert.pem -text -noout > /tmp/root-cert.crt.txt
----
+
[source,terminal]
----
$ openssl x509 -in /tmp/pod-root-cert.pem -text -noout > /tmp/pod-root-cert.crt.txt
----
+
[source,terminal]
----
$ diff /tmp/root-cert.crt.txt /tmp/pod-root-cert.crt.txt
----
+
Expect the output to be empty.
+
. Verify the CA certificate is the same as the one specified by Operator. Replace `<path>` with the path to your certificates.
+
[source,terminal]
----
$ sed '0,/^-----END CERTIFICATE-----/d' /tmp/pod-cert-chain.pem > /tmp/pod-cert-chain-ca.pem
----
+
[source,terminal]
----
$ openssl x509 -in <path>/ca-cert.pem -text -noout > /tmp/ca-cert.crt.txt
----
+
[source,terminal]
----
$ openssl x509 -in /tmp/pod-cert-chain-ca.pem -text -noout > /tmp/pod-cert-chain-ca.crt.txt
----
+
[source,terminal]
----
$ diff /tmp/ca-cert.crt.txt /tmp/pod-cert-chain-ca.crt.txt
----
+
Expect the output to be empty.
+
. Verify the certificate chain from the root certificate to the workload certificate. Replace `<path>` with the path to your certificates.
+
[source,terminal]
----
$ head -n 21 /tmp/pod-cert-chain.pem > /tmp/pod-cert-chain-workload.pem
----
+
[source,terminal]
----
$ openssl verify -CAfile <(cat <path>/ca-cert.pem <path>/root-cert.pem) /tmp/pod-cert-chain-workload.pem
----
+
.Example output
[source,terminal]
----
/tmp/pod-cert-chain-workload.pem: OK
----

[id="ossm-cert-cleanup-1x_{context}"]
== Removing the certificates

To remove the certificates you added, follow these steps.

. Remove the secret `cacerts`.
+
[source,terminal]
----
$ oc delete secret cacerts -n istio-system
----
+
. Redeploy {SMProductShortName} with a self-signed root certificate in the `ServiceMeshControlPlane` resource.
+
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    global:
      mtls:
        enabled: true
    security:
      selfSigned: true
----

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-routing-traffic-v1x"]
= Traffic management
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: routing-traffic-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

You can control the flow of traffic and API calls between services in {SMProductName}. For example, some services in your service mesh may need to communicate within the mesh and others may need to be hidden. Manage the traffic to hide specific backend services, expose services, create testing or versioning deployments, or add a security layer on a set of services.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-gateways_{context}"]
= Using gateways

You can use a gateway to manage inbound and outbound traffic for your mesh to specify which traffic you want to enter or leave the mesh. Gateway configurations are applied to standalone Envoy proxies that are running at the edge of the mesh, rather than sidecar Envoy proxies running alongside your service workloads.

Unlike other mechanisms for controlling traffic entering your systems, such as the Kubernetes Ingress APIs, {SMProductName} gateways use the full power and flexibility of traffic routing.

The {SMProductName} gateway resource can use layer 4-6 load balancing properties, such as ports, to expose and configure {SMProductName} TLS settings. Instead of adding application-layer traffic routing (L7) to the same API resource, you can bind a regular {SMProductName} virtual service to the gateway and manage gateway traffic like any other data plane traffic in a service mesh.

Gateways are primarily used to manage ingress traffic, but you can also configure egress gateways. An egress gateway lets you configure a dedicated exit node for the traffic leaving the mesh. This enables you to limit which services have access to external networks, which adds security control to your service mesh. You can also use a gateway to configure a purely internal proxy.

.Gateway example

A gateway resource describes a load balancer operating at the edge of the mesh receiving incoming or outgoing HTTP/TCP connections. The specification describes a set of ports that should be exposed, the type of protocol to use, SNI configuration for the load balancer, and so on.

The following example shows a sample gateway configuration for external HTTPS ingress traffic:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: ext-host-gwy
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 443
      name: https
      protocol: HTTPS
    hosts:
    - ext-host.example.com
    tls:
      mode: SIMPLE
      serverCertificate: /tmp/tls.crt
      privateKey: /tmp/tls.key
----

This gateway configuration lets HTTPS traffic from `ext-host.example.com` into the mesh on port 443, but doesn’t specify any routing for the traffic.

To specify routing and for the gateway to work as intended, you must also bind the gateway to a virtual service. You do this using the virtual service's gateways field, as shown in the following example:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: virtual-svc
spec:
  hosts:
  - ext-host.example.com
  gateways:
    - ext-host-gwy
----

You can then configure the virtual service with routing rules for the external traffic.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-ingress-gateway_{context}"]
= Configuring an ingress gateway

An ingress gateway is a load balancer operating at the edge of the mesh that receives incoming HTTP/TCP connections. It configures exposed ports and protocols but does not include any traffic routing configuration. Traffic routing for ingress traffic is instead configured with routing rules, the same way as for internal service requests.

The following steps show how to create a gateway and configure a `VirtualService` to expose a service in the Bookinfo sample application to outside traffic for paths `/productpage` and `/login`.

.Procedure

. Create a gateway to accept traffic.
+
.. Create a YAML file, and copy the following YAML into it.
+
.Gateway example gateway.yaml
[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: bookinfo-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"
----
+
.. Apply the YAML file.
+
[source,terminal]
----
$ oc apply -f gateway.yaml
----

. Create a `VirtualService` object to rewrite the host header.
+
.. Create a YAML file, and copy the following YAML into it.
+
.Virtual service example
[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: bookinfo
spec:
  hosts:
  - "*"
  gateways:
  - bookinfo-gateway
  http:
  - match:
    - uri:
        exact: /productpage
    - uri:
        prefix: /static
    - uri:
        exact: /login
    - uri:
        exact: /logout
    - uri:
        prefix: /api/v1/products
    route:
    - destination:
        host: productpage
        port:
          number: 9080
----
+
.. Apply the YAML file.
+
[source,terminal]
----
$ oc apply -f vs.yaml
----

. Test that the gateway and VirtualService have been set correctly.
+
.. Set the Gateway URL.
+
[source,terminal]
----
export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')
----
+
.. Set the port number. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
export TARGET_PORT=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.port.targetPort}')
----
+
.. Test a page that has been explicitly exposed.
+
[source,terminal]
----
curl -s -I "$GATEWAY_URL/productpage"
----
+
The expected result is `200`.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-ingress_{context}"]
= Managing ingress traffic

In {SMProductName}, the Ingress Gateway enables features such as monitoring, security, and route rules to apply to traffic that enters the cluster. Use a {SMProductShortName} gateway to expose a service outside of the service mesh.

[id="ossm-routing-determine-ingress_{context}"]
== Determining the ingress IP and ports

Ingress configuration differs depending on if your environment supports an external load balancer. An external load balancer is set in the ingress IP and ports for the cluster. To determine if your cluster's IP and ports are configured for external load balancers, run the following command. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.

[source,terminal]
----
$ oc get svc istio-ingressgateway -n istio-system
----

That command returns the `NAME`, `TYPE`, `CLUSTER-IP`, `EXTERNAL-IP`, `PORT(S)`, and `AGE` of each item in your namespace.

If the `EXTERNAL-IP` value is set, your environment has an external load balancer that you can use for the ingress gateway.

If the `EXTERNAL-IP` value is `<none>`, or perpetually `<pending>`, your environment does not provide an external load balancer for the ingress gateway.
You can access the gateway using the service's link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#[node port].

////
TO DO - remove XREF in this module.
Determine the ingress according to your environment. For an environment with load balancer support, xref:ossm-routing-config-ig-lb_routing-traffic[Determining ingress ports with a load balancer]. For an environment without load balancer support, xref:ossm-routing-config-ig-no-lb_routing-traffic[Determining ingress ports without a load balancer]. After you have determined the ingress ports, see xref:ossm-routing-gateways_routing-traffic[Configuring ingress using a gateway] to complete your configuration.
////

[id="ossm-routing-config-ig-lb_{context}"]
=== Determining ingress ports with a load balancer

Follow these instructions if your environment has an external load balancer.

.Procedure

. Run the following command to set the ingress IP and ports. This command sets a variable in your terminal.
+
[source,terminal]
----
$ export INGRESS_HOST=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
----

. Run the following command to set the ingress port.
+
[source,terminal]
----
$ export INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
----

. Run the following command to set the secure ingress port.
+
[source,terminal]
----
$ export SECURE_INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
----

. Run the following command to set the TCP ingress port.
+
[source,terminal]
----
$ export TCP_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="tcp")].port}')
----

[NOTE]
====
In some environments, the load balancer may be exposed using a hostname instead of an IP address. For that case, the ingress gateway's `EXTERNAL-IP` value is not an IP address. Instead, it's a hostname, and the previous command fails to set the `INGRESS_HOST` environment variable.

In that case, use the following command to correct the `INGRESS_HOST` value:
====

[source,terminal]
----
$ export INGRESS_HOST=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
----

[id="ossm-routing-config-ig-no-lb_{context}"]
=== Determining ingress ports without a load balancer

If your environment does not have an external load balancer, determine the ingress ports and use a node port instead.

.Procedure

. Set the ingress ports.
+
[source,terminal]
----
$ export INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
----

. Run the following command to set the secure ingress port.
+
[source,terminal]
----
$ export SECURE_INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].nodePort}')
----

. Run the following command to set the TCP ingress port.
+
[source,terminal]
----
$ export TCP_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="tcp")].nodePort}')
----

:leveloffset: 2

:leveloffset: +1

////
This TASK module included in the following assemblies:
// * service_mesh/v1x/ossm-traffic-manage.adoc
////

[id="ossm-auto-route-1x_{context}"]
= Automatic route creation

OpenShift routes for Istio Gateways are automatically managed in {SMProductName}. Every time an Istio Gateway is created, updated or deleted inside the service mesh, an OpenShift route is created, updated or deleted.

[id="ossm-auto-route-enable_{context}"]
== Enabling Automatic Route Creation
A {SMProductName} control plane component called Istio OpenShift Routing (IOR) synchronizes the gateway route. Enable IOR as part of the control plane deployment.

If the Gateway contains a TLS section, the OpenShift Route will be configured to support TLS.

. In the `ServiceMeshControlPlane` resource, add the `ior_enabled` parameter and set it to `true`. For example, see the following resource snippet:

[source,yaml]
----
spec:
  istio:
    gateways:
     istio-egressgateway:
       autoscaleEnabled: false
       autoscaleMin: 1
       autoscaleMax: 5
     istio-ingressgateway:
       autoscaleEnabled: false
       autoscaleMin: 1
       autoscaleMax: 5
       ior_enabled: true
----


[id="ossm-auto-route-subdomains_{context}"]
== Subdomains

{SMProductName} creates the route with the subdomain, but {product-title} must be configured to enable it. Subdomains, for example `*.domain.com`, are supported but not by default. Configure an {product-title} wildcard policy before configuring a wildcard host Gateway. For more information, see the "Links" section.

If the following gateway is created:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: gateway1
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - www.bookinfo.com
    - bookinfo.example.com
----

Then, the following OpenShift Routes are created automatically. You can check that the routes are created with the following command.

[source,terminal]
----
$ oc -n <control_plane_namespace> get routes
----

.Expected output
[source,terminal]
----
NAME           HOST/PORT             PATH  SERVICES               PORT  TERMINATION   WILDCARD
gateway1-lvlfn bookinfo.example.com        istio-ingressgateway   <all>               None
gateway1-scqhv www.bookinfo.com            istio-ingressgateway   <all>               None
----

If the gateway is deleted, {SMProductName} deletes the routes. However, routes created manually are never modified by {SMProductName}.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-routing-service-entries_{context}"]
= Understanding service entries

A service entry adds an entry to the service registry that {SMProductName} maintains internally. After you add the service entry, the Envoy proxies send traffic to the service as if it is a service in your mesh. Service entries allow you to do the following:

* Manage traffic for services that run outside of the service mesh.
* Redirect and forward traffic for external destinations (such as, APIs consumed from the web) or traffic to services in legacy infrastructure.
* Define retry, timeout, and fault injection policies for external destinations.
* Run a mesh service in a Virtual Machine (VM) by adding VMs to your mesh.

[NOTE]
====
Add services from a different cluster to the mesh to configure a multicluster {SMProductName} mesh on Kubernetes.
====

.Service entry examples
The following example is a mesh-external service entry that adds the `ext-resource` external dependency to the {SMProductName} service registry:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: svc-entry
spec:
  hosts:
  - ext-svc.example.com
  ports:
  - number: 443
    name: https
    protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
----

Specify the external resource using the `hosts` field. You can qualify it fully or use a wildcard prefixed domain name.

You can configure virtual services and destination rules to control traffic to a service entry in the same way you configure traffic for any other service in the mesh. For example, the following destination rule configures the traffic route to use mutual TLS to secure the connection to the `ext-svc.example.com` external service that is configured using the service entry:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: ext-res-dr
spec:
  host: ext-svc.example.com
  trafficPolicy:
    tls:
      mode: MUTUAL
      clientCertificate: /etc/certs/myclientcert.pem
      privateKey: /etc/certs/client_private_key.pem
      caCertificates: /etc/certs/rootcacerts.pem
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: PROCEDURE

[id="ossm-routing-virtual-services_{context}"]
= Using VirtualServices

You can route requests dynamically to multiple versions of a microservice through {SMProductName} with a virtual service. With virtual services, you can:

* Address multiple application services through a single virtual service. If your mesh uses Kubernetes, for example, you can configure a virtual service to handle all services in a specific namespace. A virtual service enables you to turn a monolithic application into a service consisting of distinct microservices with a seamless consumer experience.
* Configure traffic rules in combination with gateways to control ingress and egress traffic.

[id="ossm-routing-vs_{context}"]
== Configuring VirtualServices

Requests are routed to services within a service mesh with virtual services. Each virtual service consists of a set of routing rules that are evaluated in order. {SMProductName} matches each given request to the virtual service to a specific real destination within the mesh.

Without virtual services, {SMProductName} distributes traffic using least requests load balancing between all service instances. With a virtual service, you can specify traffic behavior for one or more hostnames. Routing rules in the virtual service tell {SMProductName} how to send the traffic for the virtual service to appropriate destinations. Route destinations can be versions of the same service or entirely different services.

.Procedure

. Create a YAML file using the following example to route requests to different versions of the Bookinfo sample application service depending on which user connects to the application.
+
.Example VirtualService.yaml
[source,YAML]
----
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: reviews
spec:
  hosts:
  - reviews
  http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  - route:
    - destination:
        host: reviews
        subset: v3
----

. Run the following command to apply `VirtualService.yaml`, where `VirtualService.yaml` is the path to the file.
+
[source,terminal]
----
$ oc apply -f <VirtualService.yaml>
----

== VirtualService configuration reference

//Need a sentence or two here

[options="header"]
[cols="l, a"]
|===
|Parameter |Description
|spec:
  hosts:
|The `hosts` field lists the virtual service's destination address to which the routing rules apply. This is the address(es) that are used to send requests to the service. The virtual service hostname can be an IP address, a DNS name, or a short name that resolves to a fully qualified domain name.

|spec:
  http:
  - match:
|The `http` section contains the virtual service's routing rules which describe match conditions and actions for routing HTTP/1.1, HTTP2, and gRPC traffic sent to the destination as specified in the hosts field. A routing rule consists of the destination where you want the traffic to go and any specified match conditions.
The first routing rule in the example has a condition that begins with the match field. In this example, this routing applies to all requests from the user `jason`. Add the `headers`, `end-user`, and `exact` fields to select the appropriate requests.

|spec:
  http:
  - match:
    - destination:
|The `destination` field in the route section specifies the actual destination for traffic that matches this condition. Unlike the virtual service's host, the destination's host must be a real destination that exists in the {SMProductName} service registry. This can be a mesh service with proxies or a non-mesh service added using a service entry. In this example, the hostname is a Kubernetes service name:
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-routing-destination-rules_{context}"]
= Understanding destination rules

Destination rules are applied after virtual service routing rules are evaluated, so they apply to the traffic's real destination. Virtual services route traffic to a destination. Destination rules configure what happens to traffic at that destination.

By default, {SMProductName} uses a least requests load balancing policy, where the service instance in the pool with the least number of active connections receives the request. {SMProductName} also supports the following models, which you can specify in destination rules for requests to a particular service or service subset.

* Random: Requests are forwarded at random to instances in the pool.
* Weighted: Requests are forwarded to instances in the pool according to a specific percentage.
* Least requests: Requests are forwarded to instances with the least number of requests.

.Destination rule example

The following example destination rule configures three different subsets for the `my-svc` destination service, with different load balancing policies:

[source,yaml]
----
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: my-destination-rule
spec:
  host: my-svc
  trafficPolicy:
    loadBalancer:
      simple: RANDOM
  subsets:
  - name: v1
    labels:
      version: v1
  - name: v2
    labels:
      version: v2
    trafficPolicy:
      loadBalancer:
        simple: ROUND_ROBIN
  - name: v3
    labels:
      version: v3
----

:leveloffset: 2

This guide references the Bookinfo sample application to provide examples of routing in an example application. Install the xref:ossm-tutorial-bookinfo-overview_deploying-applications-ossm-v1x[Bookinfo application] to learn how these routing examples work.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc

[id="ossm-routing-bookinfo_{context}"]
= Bookinfo routing tutorial

The {SMProductShortName} Bookinfo sample application consists of four separate microservices, each with multiple versions. After installing the Bookinfo sample application, three different versions of the `reviews` microservice run concurrently.

When you access the Bookinfo app `/product` page in a browser and refresh several times, sometimes the book review output contains star ratings and other times it does not. Without an explicit default service version to route to, {SMProductShortName} routes requests to all available versions one after the other.

This tutorial helps you apply rules that route all traffic to `v1` (version 1) of the microservices. Later, you can apply a rule to route traffic based on the value of an HTTP request header.

.Prerequisites:

* Deploy the Bookinfo sample application to work with the following examples.

:leveloffset: 2

:leveloffset: +2

:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-bookinfo-applying_{context}"]
= Applying a virtual service

In the following procedure, the virtual service routes all traffic to `v1` of each micro-service by applying virtual services that set the default version for the micro-services.

.Procedure

. Apply the virtual services.
+
[source,bash,subs="attributes"]
----
$ oc apply -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/virtual-service-all-v1.yaml
----

. To verify that you applied the virtual services, display the defined routes with the following command:
+
[source,terminal]
----
$ oc get virtualservices -o yaml
----
+
That command returns a resource of `kind: VirtualService` in YAML format.

You have configured {SMProductShortName} to route to the `v1` version of the Bookinfo microservices including the `reviews` service version 1.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc
:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-bookinfo-test_{context}"]
= Testing the new route configuration

Test the new configuration by refreshing the `/productpage` of the Bookinfo application.

.Procedure

. Set the value for the `GATEWAY_URL` parameter. You can use this variable to find the URL for your Bookinfo product page later. In this example, istio-system is the name of the control plane project.
+
[source,terminal]
----
export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')
----

. Run the following command to retrieve the URL for the product page.
+
[source,terminal]
----
echo "http://$GATEWAY_URL/productpage"
----

. Open the Bookinfo site in your browser.

The reviews part of the page displays with no rating stars, no matter how many times you refresh. This is because you configured {SMProductShortName} to route all traffic for the reviews service to the version `reviews:v1` and this version of the service does not access the star ratings service.

Your service mesh now routes traffic to one version of a service.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-traffic-manage.adoc
// * service_mesh/v2x/ossm-traffic-manage.adoc
:_mod-docs-content-type: PROCEDURE
[id="ossm-routing-bookinfo-route_{context}"]
= Route based on user identity

Change the route configuration so that all traffic from a specific user is routed to a specific service version. In this case, all traffic from a user named `jason` will be routed to the service `reviews:v2`.

{SMProductShortName} does not have any special, built-in understanding of user identity. This example is enabled by the fact that the `productpage` service adds a custom `end-user` header to all outbound HTTP requests to the reviews service.

.Procedure

. Run the following command to enable user-based routing in the Bookinfo sample application.
+
[source,bash,subs="attributes"]
----
$ oc apply -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
----

. Run the following command to confirm the rule is created. This command returns all resources of `kind: VirtualService` in YAML format.
+
[source,terminal]
----
$ oc get virtualservice reviews -o yaml
----

. On the `/productpage` of the Bookinfo app, log in as user `jason` with no password.
+
. Refresh the browser. The star ratings appear next to each review.

. Log in as another user (pick any name you want). Refresh the browser. Now the stars are gone. Traffic is now routed to `reviews:v1` for all users except Jason.

You have successfully configured the Bookinfo sample application to route traffic based on user identity.

:leveloffset: 2

[role="_additional-resources-traffic-management"]
== Additional resources
For more information about configuring an {product-title} wildcard policy, see link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#using-wildcard-routes_configuring-ingress[Using wildcard routes].

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="deploying-applications-ossm-v1x"]
= Deploying applications on Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: deploying-applications-ossm-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

When you deploy an application into the {SMProductShortName}, there are several differences between the behavior of applications in the upstream community version of Istio and the behavior of applications within a {SMProductName} installation.

== Prerequisites

* Review xref:ossm-vs-community-v1x[Comparing {SMProductName} and upstream Istio community installations]

* Review xref:installing-ossm-v1x[Installing {SMProductName}]

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-templates-1x_{context}"]
= Creating control plane templates

You can create reusable configurations with `ServiceMeshControlPlane` templates. Individual users can extend the templates they create with their own configurations. Templates can also inherit configuration information from other templates. For example, you can create an accounting control plane for the accounting team and a marketing control plane for the marketing team. If you create a development template and a production template, members of the marketing team and the accounting team can extend the development and production templates with team specific customization.

When you configure control plane templates, which follow the same syntax as the `ServiceMeshControlPlane`, users inherit settings in a hierarchical fashion. The Operator is delivered with a `default` template with default settings for {SMProductName}. To add custom templates you must create a ConfigMap named `smcp-templates` in the `openshift-operators` project and mount the ConfigMap in the Operator container at `/usr/local/share/istio-operator/templates`.

[id="ossm-create-configmap_{context}"]
== Creating the ConfigMap
////
TODO
Write a  ConfigMap overview/definition
////

Follow this procedure to create the ConfigMap.

.Prerequisites

* An installed, verified {SMProductShortName} Operator.
* An account with the `cluster-admin` role.
* Location of the Operator deployment.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a cluster administrator.

. From the CLI, run this command to create the ConfigMap named `smcp-templates` in the `openshift-operators` project and replace `<templates-directory>` with the location of the `ServiceMeshControlPlane` files on your local disk:
+
[source,terminal]
----
$ oc create configmap --from-file=<templates-directory> smcp-templates -n openshift-operators
----

. Locate the Operator ClusterServiceVersion name.
+
[source,terminal]
----
$ oc get clusterserviceversion -n openshift-operators | grep 'Service Mesh'
----
+
.Example output
[source,terminal]
----
maistra.v1.0.0            Red Hat OpenShift Service Mesh   1.0.0                Succeeded
----

. Edit the Operator cluster service version to instruct the Operator to use the `smcp-templates` ConfigMap.
+
[source,terminal]
----
$ oc edit clusterserviceversion -n openshift-operators maistra.v1.0.0
----

. Add a volume mount and volume to the Operator deployment.
+
[source,yaml]
----
deployments:
  - name: istio-operator
    spec:
      template:
        spec:
          containers:
            volumeMounts:
              - name: discovery-cache
                mountPath: /home/istio-operator/.kube/cache/discovery
              - name: smcp-templates
                mountPath: /usr/local/share/istio-operator/templates/
          volumes:
            - name: discovery-cache
              emptyDir:
                medium: Memory
            - name: smcp-templates
              configMap:
                name: smcp-templates
...
----
. Save your changes and exit the editor.

. You can now use the `template` parameter in the `ServiceMeshControlPlane` to specify a template.
+
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  name: minimal-install
spec:
  template: default
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-automatic-sidecar-injection_{context}"]
= Enabling automatic sidecar injection

When deploying an application, you must opt-in to injection by configuring the annotation `sidecar.istio.io/inject` in `spec.template.metadata.annotations` to `true` in the `deployment` object. Opting in ensures that the sidecar injection does not interfere with other {product-title} features such as builder pods used by numerous frameworks within the {product-title} ecosystem.

.Prerequisites

* Identify the namespaces that are part of your service mesh and the deployments that need automatic sidecar injection.

.Procedure

. To find your deployments use the `oc get` command.
+
[source,terminal]
----
$ oc get deployment -n <namespace>
----
+
For example, to view the deployment file for the 'ratings-v1' microservice in the `bookinfo` namespace, use the following command to see the resource in YAML format.
+
[source,terminal]
----
oc get deployment -n bookinfo ratings-v1 -o yaml
----
+
. Open the application's deployment configuration YAML file in an editor.

. Add `spec.template.metadata.annotations.sidecar.istio/inject` to your Deployment YAML and set `sidecar.istio.io/inject` to `true` as shown in the following example.
+
.Example snippet from bookinfo deployment-ratings-v1.yaml
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ratings-v1
  namespace: bookinfo
  labels:
    app: ratings
    version: v1
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/inject: 'true'
----
+
. Save the Deployment configuration file.

. Add the file back to the project that contains your app.
+
[source,terminal]
----
$ oc apply -n <namespace> -f deployment.yaml
----
+
In this example, `bookinfo` is the name of the project that contains the `ratings-v1` app and `deployment-ratings-v1.yaml` is the file you edited.
+
[source,terminal]
----
$ oc apply -n bookinfo -f deployment-ratings-v1.yaml
----
+
. To verify that the resource uploaded successfully, run the following command.
+
[source,terminal]
----
$ oc get deployment -n <namespace> <deploymentName> -o yaml
----
+
For example,
+
[source,terminal]
----
$ oc get deployment -n bookinfo ratings-v1 -o yaml
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
// * service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: CONCEPT
[id="ossm-sidecar-injection-env-var_{context}"]
= Setting proxy environment variables through annotations

Configuration for the Envoy sidecar proxies is managed by the `ServiceMeshControlPlane`.

You can set environment variables for the sidecar proxy for applications by adding pod annotations to the deployment in the `injection-template.yaml` file. The environment variables are injected to the sidecar.

.Example injection-template.yaml
[source,yaml]
----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: resource
spec:
  replicas: 7
  selector:
    matchLabels:
      app: resource
  template:
    metadata:
      annotations:
        sidecar.maistra.io/proxyEnv: "{ \"maistra_test_env\": \"env_value\", \"maistra_test_env_2\": \"env_value_2\" }"
----

[WARNING]
====
You should never include `maistra.io/` labels and annotations when creating your own custom resources.  These labels and annotations indicate that the resources are generated and managed by the Operator. If you are copying content from an Operator-generated resource when creating your own resources, do not include labels or annotations that start with `maistra.io/`.  Resources that include these labels or annotations will be overwritten or deleted by the Operator during the next reconciliation.
====

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-mixer-policy-1x_{context}"]
= Updating Mixer policy enforcement

In previous versions of {SMProductName}, Mixer's policy enforcement was enabled by default. Mixer policy enforcement is now disabled by default. You must enable it before running policy tasks.

.Prerequisites
* Access to the OpenShift CLI (`oc`).

[NOTE]
====
The examples use `<istio-system>` as the control plane namespace. Replace this value with the namespace where you deployed the Service Mesh Control Plane (SMCP).
====

.Procedure

. Log in to the {product-title} CLI.

. Run this command to check the current Mixer policy enforcement status:
+
[source,terminal]
----
$ oc get cm -n <istio-system> istio -o jsonpath='{.data.mesh}' | grep disablePolicyChecks
----

. If `disablePolicyChecks: true`, edit the {SMProductShortName} ConfigMap:
+
[source,terminal]
----
$ oc edit cm -n <istio-system> istio
----

. Locate `disablePolicyChecks: true` within the ConfigMap and change the value to `false`.

. Save the configuration and exit the editor.

. Re-check the Mixer policy enforcement status to ensure it is set to `false`.

:leveloffset: 2

:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: CONCEPT
[id="ossm-config-network-policy_{context}"]

== Setting the correct network policy

{SMProductShortName} creates network policies in the {SMProductShortName} control plane and member namespaces to allow traffic between them. Before you deploy, consider the following conditions to ensure the services in your service mesh that were previously exposed through an {product-title} route.

* Traffic into the service mesh must always go through the ingress-gateway for Istio to work properly.
* Deploy services external to the service mesh in separate namespaces that are not in any service mesh.
* Non-mesh services that need to be deployed within a service mesh enlisted namespace should label their deployments `maistra.io/expose-route: "true"`, which ensures {product-title} routes to these services still work.

:leveloffset: 2


:leveloffset: +1

////
This CONCEPT module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

[id="ossm-tutorial-bookinfo-overview_{context}"]
= Bookinfo example application

The Bookinfo example application allows you to test your {SMProductName} {SMProductVersion} installation on {product-title}.

The Bookinfo application displays information about a book, similar to a single catalog entry of an online book store. The application displays a page that describes the book, book details (ISBN, number of pages, and other information), and book reviews.

The Bookinfo application consists of these microservices:

* The `productpage` microservice calls the `details` and `reviews` microservices to populate the page.
* The `details` microservice contains book information.
* The `reviews` microservice contains book reviews. It also calls the `ratings` microservice.
* The `ratings` microservice contains book ranking information that accompanies a book review.

There are three versions of the reviews microservice:

* Version v1 does not call the `ratings` Service.
* Version v2 calls the `ratings` Service and displays each rating as one to five black stars.
* Version v3 calls the `ratings` Service and displays each rating as one to five red stars.

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-install_{context}"]
= Installing the Bookinfo application

This tutorial walks you through how to create a sample application by creating a project, deploying the Bookinfo application to that project, and viewing the running application in {SMProductShortName}.

.Prerequisites:

* {product-title} 4.1 or higher installed.
* {SMProductName} {SMProductVersion} installed.
* Access to the OpenShift CLI (`oc`).
* An account with the `cluster-admin` role.

[NOTE]
====
The Bookinfo sample application cannot be installed on {ibm-z-name} and {ibm-power-name}.
====

[NOTE]
====
The commands in this section assume the {SMProductShortName} control plane project is `istio-system`.  If you installed the control plane in another namespace, edit each command before you run it.
====

.Procedure

. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Click *Home* -> *Projects*.

. Click *Create Project*.

. Enter `bookinfo` as the *Project Name*, enter a *Display Name*, and enter a *Description*, then click *Create*.
+
** Alternatively, you can run this command from the CLI to create the `bookinfo` project.
+
[source,terminal]
----
$ oc new-project bookinfo
----
+
. Click *Operators* -> *Installed Operators*.

. Click the *Project* menu and use the {SMProductShortName} control plane namespace. In this example, use `istio-system`.

. Click the *{SMProductName}* Operator.

. Click the *Istio Service Mesh Member Roll* tab.

.. If you have already created a Istio Service Mesh Member Roll, click the name, then click the YAML tab to open the YAML editor.

.. If you have not created a `ServiceMeshMemberRoll`, click *Create ServiceMeshMemberRoll*.
+
. Click *Members*, then enter the name of your project in the *Value* field.
+
. Click *Create* to save the updated Service Mesh Member Roll.
+
.. Or, save the following example to a YAML file.
+
.Bookinfo ServiceMeshMemberRoll example servicemeshmemberroll-default.yaml
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshMemberRoll
metadata:
  name: default
spec:
  members:
  - bookinfo
----
+
.. Run the following command to upload that file and create the `ServiceMeshMemberRoll` resource in the `istio-system` namespace.   In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc create -n istio-system -f servicemeshmemberroll-default.yaml
----
+
. Run the following command to verify the `ServiceMeshMemberRoll` was created successfully.
+
[source,terminal]
----
$ oc get smmr -n istio-system -o wide
----
+
The installation has finished successfully when the `STATUS` column is `Configured`.
+
[source,terminal]
----
NAME      READY   STATUS       AGE   MEMBERS
default   1/1     Configured   70s   ["bookinfo"]
----
. From the CLI, deploy the Bookinfo application in the _`bookinfo`_ project by applying the `bookinfo.yaml` file:
+
[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/platform/kube/bookinfo.yaml
----
+
You should see output similar to the following:
+
[source,terminal]
----
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
----
+
. Create the ingress gateway by applying the `bookinfo-gateway.yaml` file:
+
[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/bookinfo-gateway.yaml
----
+
You should see output similar to the following:
+
[source,terminal]
----
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
----
+
. Set the value for the `GATEWAY_URL` parameter:
+
[source,terminal]
----
$ export GATEWAY_URL=$(oc -n istio-system get route istio-ingressgateway -o jsonpath='{.spec.host}')
----

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-adding-destination-rules_{context}"]
= Adding default destination rules

Before you can use the Bookinfo application, you must first add default destination rules. There are two preconfigured YAML files, depending on whether or not you enabled mutual transport layer security (TLS) authentication.

.Procedure

. To add destination rules, run one of the following commands:
** If you did not enable mutual TLS:
+

[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/destination-rule-all.yaml
----
+
** If you enabled mutual TLS:
+

[source,bash,subs="attributes"]
----
$ oc apply -n bookinfo -f https://raw.githubusercontent.com/Maistra/istio/maistra-{MaistraVersion}/samples/bookinfo/networking/destination-rule-all-mtls.yaml
----
+
You should see output similar to the following:
+
[source,terminal]
----
destinationrule.networking.istio.io/productpage created
destinationrule.networking.istio.io/reviews created
destinationrule.networking.istio.io/ratings created
destinationrule.networking.istio.io/details created
----

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-verify-install_{context}"]
= Verifying the Bookinfo installation

To confirm that the sample Bookinfo application was successfully deployed, perform the following steps.

.Prerequisites

* {SMProductName} installed.
* Complete the steps for installing the Bookinfo sample app.

.Procedure from CLI

. Log in to the {product-title} CLI.

. Verify that all pods are ready with this command:
+
[source,terminal]
----
$ oc get pods -n bookinfo
----
+
All pods should have a status of `Running`. You should see output similar to the following:
+
[source,terminal]
----
NAME                              READY   STATUS    RESTARTS   AGE
details-v1-55b869668-jh7hb        2/2     Running   0          12m
productpage-v1-6fc77ff794-nsl8r   2/2     Running   0          12m
ratings-v1-7d7d8d8b56-55scn       2/2     Running   0          12m
reviews-v1-868597db96-bdxgq       2/2     Running   0          12m
reviews-v2-5b64f47978-cvssp       2/2     Running   0          12m
reviews-v3-6dfd49b55b-vcwpf       2/2     Running   0          12m
----
+
. Run the following command to retrieve the URL for the product page:
+
[source,terminal]
----
echo "http://$GATEWAY_URL/productpage"
----
. Copy and paste the output in a web browser to verify the Bookinfo product page is deployed.

.Procedure from Kiali web console

. Obtain the address for the Kiali web console.

.. Log in to the {product-title} web console as a user with `cluster-admin` rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

.. Navigate to *Networking* -> *Routes*.

.. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+

.. Click the link in the *Location* column for Kiali.

.. Click *Log In With OpenShift*. The Kiali *Overview* screen presents tiles for each project namespace.

. In Kiali, click *Graph*.

. Select bookinfo from the *Namespace* list, and App graph from the *Graph Type* list.

. Click *Display idle nodes* from the *Display* menu.
+
This displays nodes that are defined but have not received or sent requests. It can confirm that an application is properly defined, but that no request traffic has been reported.
+
image::ossm-kiali-graph-bookinfo.png[Kiali displaying bookinfo application]
+
* Use the *Duration* menu to increase the time period to help ensure older traffic is captured.
+
* Use the *Refresh Rate* menu to refresh traffic more or less often, or not at all.

. Click *Services*, *Workloads* or *Istio Config* to see list views of bookinfo components, and confirm that they are healthy.

:leveloffset: 2

:leveloffset: +2

////
This PROCEDURE module included in the following assemblies:
* service_mesh/v1x/prepare-to-deploy-applications-ossm.adoc
* service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-tutorial-bookinfo-removing_{context}"]
= Removing the Bookinfo application

Follow these steps to remove the Bookinfo application.

.Prerequisites

* {product-title} 4.1 or higher installed.
* {SMProductName} {SMProductVersion} installed.
* Access to the OpenShift CLI (`oc`).

[id="ossm-delete-bookinfo-project_{context}"]
== Delete the Bookinfo project

.Procedure

. Log in to the {product-title} web console.

. Click to *Home* -> *Projects*.

. Click the `bookinfo` menu {kebab}, and then click *Delete Project*.

. Type `bookinfo` in the confirmation dialog box, and then click *Delete*.
+
** Alternatively, you can run this command using the CLI to create the `bookinfo` project.
+
[source,terminal]
----
$ oc delete project bookinfo
----

[id="ossm-remove-bookinfo-smmr_{context}"]
== Remove the Bookinfo project from the {SMProductShortName} member roll

.Procedure

. Log in to the {product-title} web console.

. Click *Operators* -> *Installed Operators*.

. Click the *Project* menu and choose `istio-system` from the list.

. Click the *Istio Service Mesh Member Roll* link under *Provided APIS* for the *{SMProductName}* Operator.

. Click the `ServiceMeshMemberRoll` menu {kebab} and select *Edit Service Mesh Member Roll*.

. Edit the default Service Mesh Member Roll YAML and remove `bookinfo` from the *members* list.
+
** Alternatively, you can run this command using the CLI to remove the `bookinfo` project from the `ServiceMeshMemberRoll`. In this example, `istio-system` is the name of the {SMProductShortName} control plane project.
+
[source,terminal]
----
$ oc -n istio-system patch --type='json' smmr default -p '[{"op": "remove", "path": "/spec/members", "value":["'"bookinfo"'"]}]'
----

. Click *Save* to update Service Mesh Member Roll.

:leveloffset: 2


:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v1x/ossm-observability.adoc
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="generating-sample-traces-analyzing-trace-data_{context}"]
= Generating example traces and analyzing trace data

Jaeger is an open source distributed tracing system. With Jaeger, you can perform a trace that follows the path of a request through various microservices which make up an application. Jaeger is installed by default as part of the {SMProductShortName}.

This tutorial uses {SMProductShortName} and the Bookinfo sample application to demonstrate how you can use Jaeger to perform distributed tracing.

.Prerequisites:

* {product-title} 4.1 or higher installed.
* {SMProductName} {SMProductVersion} installed.
* Jaeger enabled during the installation.
* Bookinfo example application installed.

.Procedure

. After installing the Bookinfo sample application, send traffic to the mesh. Enter the following command several times.
+
[source,terminal]
----
$ curl "http://$GATEWAY_URL/productpage"
----
+
This command simulates a user visiting the `productpage` microservice of the application.

. In the {product-title} console, navigate to *Networking* -> *Routes* and search for the Jaeger route, which is the URL listed under *Location*.
* Alternatively, use the CLI to query for details of the route. In this example, `istio-system` is the {SMProductShortName} control plane namespace:
+
[source,terminal]
----
$ export JAEGER_URL=$(oc get route -n istio-system jaeger -o jsonpath='{.spec.host}')
----
+
.. Enter the following command to reveal the URL for the Jaeger console. Paste the result in a browser and navigate to that URL.
+
[source,terminal]
----
echo $JAEGER_URL
----

. Log in using the same user name and password as you use to access the {product-title} console.

. In the left pane of the Jaeger dashboard, from the *Service* menu, select *productpage.bookinfo* and click *Find Traces* at the bottom of the pane. A list of traces is displayed.

. Click one of the traces in the list to open a detailed view of that trace.  If you click the first one in the list, which is the most recent trace, you see the details that correspond to the latest refresh of the `/productpage`.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-observability-v1x"]
= Data visualization and observability
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: observability-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

You can view your application's topology, health and metrics in the Kiali console. If your service is having issues, the Kiali console offers ways to visualize the data flow through your service. You can view insights about the mesh components at different levels, including abstract applications, services, and workloads. It also provides an interactive graph view of your namespace in real time.

.Before you begin

You can observe the data flow through your application if you have an application installed. If you don't have your own application installed, you can see how observability works in {SMProductName} by installing the xref:ossm-tutorial-bookinfo-overview_deploying-applications-ossm-v1x[Bookinfo sample application].

:leveloffset: +1

// Module included in the following assemblies:
//
//* service_mesh/v1x/ossm-observability.adoc
//* service_mesh/v2x/ossm-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-observability-access-console_{context}"]
= Viewing service mesh data

The Kiali operator works with the telemetry data gathered in {SMProductName} to provide graphs and real-time network diagrams of the applications, services, and workloads in your namespace.

To access the Kiali console you must have {SMProductName} installed and projects configured for the service mesh.

.Procedure

. Use the perspective switcher to switch to the *Administrator* perspective.

. Click *Home* -> *Projects*.

. Click the name of your project. For example, click `bookinfo`.

. In the *Launcher* section, click *Kiali*.

. Log in to the Kiali console with the same user name and password that you use to access the {product-title} console.

When you first log in to the Kiali Console, you see the *Overview* page which displays all the namespaces in your service mesh that you have permission to view.

If you are validating the console installation, there might not be any data to display.

:leveloffset: 2

:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v1x/ossm-observability.adoc
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-observability-visual_{context}"]
= Viewing service mesh data in the Kiali console

The Kiali Graph offers a powerful visualization of your mesh traffic. The topology combines real-time request traffic with your Istio configuration information to present immediate insight into the behavior of your service mesh, letting you quickly pinpoint issues. Multiple Graph Types let you visualize traffic as a high-level service topology, a low-level workload topology, or as an application-level topology.

There are several graphs to choose from:

* The *App graph* shows an aggregate workload for all applications that are labeled the same.

* The *Service graph* shows a node for each service in your mesh but excludes all applications and workloads from the graph. It provides a high level view and aggregates all traffic for defined services.

* The *Versioned App graph* shows a node for each version of an application. All versions of an application are grouped together.

* The *Workload graph* shows a node for each workload in your service mesh. This graph does not require you to use the application and version labels. If your application does not use version labels, use this the graph.

Graph nodes are decorated with a variety of information, pointing out various route routing options like virtual services and service entries, as well as special configuration like fault-injection and circuit breakers. It can identify mTLS issues, latency issues, error traffic and more. The Graph is highly configurable, can show traffic animation, and has powerful Find and Hide abilities.

Click the *Legend* button to view information about the shapes, colors, arrows, and badges displayed in the graph.

To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel.

[id="ossm-observability-topology_{context}"]
== Changing graph layouts in Kiali

The layout for the Kiali graph can render differently depending on your application architecture and the data to display. For example, the number of graph nodes and their interactions can determine how the Kiali graph is rendered. Because it is not possible to create a single layout that renders nicely for every situation, Kiali offers a choice of several different layouts.

.Prerequisites

*  If you do not have your own application installed, install the Bookinfo sample application.  Then generate traffic for the Bookinfo application by entering the following command several times.
+
[source,terminal]
----
$ curl "http://$GATEWAY_URL/productpage"
----
+
This command simulates a user visiting the `productpage` microservice of the application.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.

. In Kiali console, click *Graph* to view a namespace graph.

. From the *Namespace* menu, select your application namespace, for example, `bookinfo`.

. To choose a different graph layout, do either or both of the following:

* Select different graph data groupings from the menu at the top of the graph.

** App graph
** Service graph
** Versioned App graph (default)
** Workload graph

* Select a different graph layout from the Legend at the bottom of the graph.
** Layout default dagre
** Layout 1 cose-bilkent
** Layout 2 cola

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="ossm-custom-resources-v1x"]
= Custom resources
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: ossm-controler-items-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

You can customize your {SMProductName} by modifying the default {SMProductShortName} custom resource or by creating a new custom resource.

== Prerequisites
* An account with the `cluster-admin` role.
* Completed the xref:preparing-ossm-installation-v1x[Preparing to install {SMProductName}] process.
* Have installed the operators.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc

[id="ossm-cr-example-1x_{context}"]
= {SMProductName} custom resources

[NOTE]
====
The `istio-system` project is used as an example throughout the {SMProductShortName} documentation, but you can use other projects as necessary.
====

A _custom resource_ allows you to extend the API in an {SMProductName} project or cluster. When you deploy {SMProductShortName} it creates a default `ServiceMeshControlPlane` that you can modify to change the project parameters.

The {SMProductShortName} operator extends the API by adding the `ServiceMeshControlPlane` resource type, which enables you to create `ServiceMeshControlPlane` objects within projects. By creating a `ServiceMeshControlPlane` object, you instruct the Operator to install a {SMProductShortName} control plane into the project, configured with the parameters you set in the `ServiceMeshControlPlane` object.

This example `ServiceMeshControlPlane` definition contains all of the supported parameters and deploys {SMProductName} {SMProductVersion1x} images based on Red Hat Enterprise Linux (RHEL).

[IMPORTANT]
====
The 3scale Istio Adapter is deployed and configured in the custom resource file. It also requires a working 3scale account (link:https://www.3scale.net/signup/[SaaS] or link:https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.4/html/infrastructure/onpremises-installation[On-Premises]).
====

.Example istio-installation.yaml

[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  name: basic-install
spec:

  istio:
    global:
      proxy:
        resources:
          requests:
            cpu: 100m
            memory: 128Mi
          limits:
            cpu: 500m
            memory: 128Mi

    gateways:
      istio-egressgateway:
        autoscaleEnabled: false
      istio-ingressgateway:
        autoscaleEnabled: false
        ior_enabled: false

    mixer:
      policy:
        autoscaleEnabled: false

      telemetry:
        autoscaleEnabled: false
        resources:
          requests:
            cpu: 100m
            memory: 1G
          limits:
            cpu: 500m
            memory: 4G

    pilot:
      autoscaleEnabled: false
      traceSampling: 100

    kiali:
      enabled: true

    grafana:
      enabled: true

    tracing:
      enabled: true
      jaeger:
        template: all-in-one
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-parameters_{context}"]
= ServiceMeshControlPlane parameters

The following examples illustrate use of the `ServiceMeshControlPlane` parameters and the tables provide additional information about supported parameters.

[IMPORTANT]
====
The resources you configure for {SMProductName} with these parameters, including CPUs, memory, and the number of pods, are based on the configuration of your {product-title} cluster. Configure these parameters based on the available resources in your current cluster configuration.
====

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-istio-global_{context}"]
= Istio global example

Here is an example that illustrates the Istio global parameters for the `ServiceMeshControlPlane` and a description of the available parameters with appropriate values.

[NOTE]
====
In order for the 3scale Istio Adapter to work, `disablePolicyChecks` must be `false`.
====

.Example global parameters
[source,yaml]
----
  istio:
    global:
      tag: 1.1.0
      hub: registry.redhat.io/openshift-service-mesh/
      proxy:
        resources:
          requests:
            cpu: 10m
            memory: 128Mi
          limits:
      mtls:
        enabled: false
      disablePolicyChecks: true
      policyCheckFailOpen: false
      imagePullSecrets:
        - MyPullSecret
----

.Global parameters
|===
|Parameter |Description |Values |Default value

|`disablePolicyChecks`
|This parameter enables/disables policy checks.
|`true`/`false`
|`true`

|`policyCheckFailOpen`
|This parameter indicates whether traffic is allowed to pass through to the Envoy sidecar when the Mixer policy service cannot be reached.
|`true`/`false`
|`false`

|`tag`
|The tag that the Operator uses to pull the Istio images.
|A valid container image tag.
|`1.1.0`

|`hub`
|The hub that the Operator uses to pull Istio images.
|A valid image repository.
|`maistra/` or `registry.redhat.io/openshift-service-mesh/`

|`mtls`
|This parameter controls whether to enable/disable Mutual Transport Layer Security (mTLS) between services by default.
|`true`/`false`
|`false`

|`imagePullSecrets`
|If access to the registry providing the Istio images is secure, list an link:https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod[imagePullSecret] here.
|redhat-registry-pullsecret OR quay-pullsecret
|None
|===

These parameters are specific to the proxy subset of global parameters.

.Proxy parameters
|===
|Type |Parameter |Description |Values |Default value

|`requests`
|`cpu`
|The amount of CPU resources requested for Envoy proxy.
|CPU resources, specified in cores or millicores (for example, 200m, 0.5, 1) based on your environment's configuration.
|`10m`

|
|`memory`
|The amount of memory requested for Envoy proxy
|Available memory in bytes(for example, 200Ki, 50Mi, 5Gi) based on your environment's configuration.
|`128Mi`

|`limits`
|`cpu`
|The maximum amount of CPU resources requested for Envoy proxy.
|CPU resources, specified in cores or millicores (for example, 200m, 0.5, 1) based on your environment's configuration.
|`2000m`

|
|`memory`
|The maximum amount of memory Envoy proxy is permitted to use.
|Available memory in bytes (for example, 200Ki, 50Mi, 5Gi) based on your environment's configuration.
|`1024Mi`
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-gateway_{context}"]
= Istio gateway configuration

Here is an example that illustrates the Istio gateway parameters for the `ServiceMeshControlPlane` and a description of the available parameters with appropriate values.

.Example gateway parameters
[source,yaml]
----
  gateways:
    egress:
      enabled: true
      runtime:
        deployment:
          autoScaling:
            enabled: true
            maxReplicas: 5
            minReplicas: 1
    enabled: true
    ingress:
      enabled: true
      runtime:
        deployment:
          autoScaling:
            enabled: true
            maxReplicas: 5
            minReplicas: 1
----


.Istio Gateway parameters
|===
|Parameter |Description |Values |Default value

|`gateways.egress.runtime.deployment.autoScaling.enabled`
|This parameter enables/disables autoscaling.
|`true`/`false`
|`true`

|`gateways.egress.runtime.deployment.autoScaling.minReplicas`
|The minimum number of pods to deploy for the egress gateway based on the `autoscaleEnabled` setting.
|A valid number of allocatable pods based on your environment's configuration.
|`1`

|`gateways.egress.runtime.deployment.autoScaling.maxReplicas`
|The maximum number of pods to deploy for the egress gateway based on the `autoscaleEnabled` setting.
|A valid number of allocatable pods based on your environment's configuration.
|`5`

|`gateways.ingress.runtime.deployment.autoScaling.enabled`
|This parameter enables/disables autoscaling.
|`true`/`false`
|`true`

|`gateways.ingress.runtime.deployment.autoScaling.minReplicas`
|The minimum number of pods to deploy for the ingress gateway based on the `autoscaleEnabled` setting.
|A valid number of allocatable pods based on your environment's configuration.
|`1`

|`gateways.ingress.runtime.deployment.autoScaling.maxReplicas`
|The maximum number of pods to deploy for the ingress gateway based on the `autoscaleEnabled` setting.
|A valid number of allocatable pods based on your environment's configuration.
|`5`
|===

:leveloffset: 2

Cluster administrators can refer to link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#using-wildcard-routes_configuring-ingress[Using wildcard routes] for instructions on how to enable subdomains.

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-mixer_{context}"]
= Istio Mixer configuration

Here is an example that illustrates the Mixer parameters for the `ServiceMeshControlPlane` and a description of the available parameters with appropriate values.

.Example mixer parameters
[source,yaml]
----
mixer:
  enabled: true
  policy:
    autoscaleEnabled: false
  telemetry:
    autoscaleEnabled: false
    resources:
    requests:
      cpu: 10m
      memory: 128Mi
      limits:
----


.Istio Mixer policy parameters
|===
|Parameter |Description |Values |Default value

|`enabled`
|This parameter enables/disables Mixer.
|`true`/`false`
|`true`

|`autoscaleEnabled`
|This parameter enables/disables autoscaling. Disable this for small environments.
|`true`/`false`
|`true`

|`autoscaleMin`
|The minimum number of pods to deploy based on the `autoscaleEnabled` setting.
|A valid number of allocatable pods based on your environment's configuration.
|`1`

|`autoscaleMax`
|The maximum number of pods to deploy based on the `autoscaleEnabled` setting.
|A valid number of allocatable pods based on your environment's configuration.
|`5`
|===


.Istio Mixer telemetry parameters
|===
|Type |Parameter |Description |Values |Default

|`requests`
|`cpu`
|The percentage of CPU resources requested for Mixer telemetry.
|CPU resources in millicores based on your environment's configuration.
|`10m`

|
|`memory`
|The amount of memory requested for Mixer telemetry.
|Available memory in bytes (for example, 200Ki, 50Mi, 5Gi) based on your environment's configuration.
|`128Mi`

|`limits`
|`cpu`
|The maximum percentage of CPU resources Mixer telemetry is permitted to use.
|CPU resources in millicores based on your environment's configuration.
|`4800m`

|
|`memory`
|The maximum amount of memory Mixer telemetry is permitted to use.
|Available memory in bytes (for example, 200Ki, 50Mi, 5Gi) based on your environment's configuration.
|`4G`
|===

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-pilot_{context}"]
= Istio Pilot configuration

You can configure Pilot to schedule or set limits on resource allocation.
The following example illustrates the Pilot parameters for the `ServiceMeshControlPlane` and a description of the available parameters with appropriate values.

.Example pilot parameters
[source,yaml]
----
spec:
  runtime:
    components:
      pilot:
        deployment:
          autoScaling:
            enabled: true
            minReplicas: 1
            maxReplicas: 5
            targetCPUUtilizationPercentage: 85
        pod:
          tolerations:
          - key: node.kubernetes.io/unreachable
            operator: Exists
            effect: NoExecute
            tolerationSeconds: 60
          affinity:
            podAntiAffinity:
              requiredDuringScheduling:
              - key: istio
                topologyKey: kubernetes.io/hostname
                operator: In
                values:
                - pilot
        container:
          resources:
            limits:
              cpu: 100m
              memory: 128M
----

.Istio Pilot parameters
|===
|Parameter |Description |Values |Default value

|`cpu`
|The percentage of CPU resources requested for Pilot.
|CPU resources in millicores based on your environment's configuration.
|`10m`

|`memory`
|The amount of memory requested for Pilot.
|Available memory in bytes (for example, 200Ki, 50Mi, 5Gi) based on your environment's configuration.
|`128Mi`

|`autoscaleEnabled`
|This parameter enables/disables autoscaling. Disable this for small environments.
|`true`/`false`
|`true`


|`traceSampling`
|This value controls how often random sampling occurs. *Note:* Increase for development or testing.
|A valid percentage.
|`1.0`
|===

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc
// * service_mesh/v2x/customizing-installation-ossm.adoc
:_mod-docs-content-type: REFERENCE
[id="configuring-kiali_{context}"]
= Configuring Kiali

When the {SMProductShortName} Operator creates the `ServiceMeshControlPlane` it also processes the Kiali resource. The Kiali Operator then uses this object when creating Kiali instances.

The default Kiali parameters specified in the `ServiceMeshControlPlane` are as follows:

.Example Kiali parameters
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
    kiali:
      enabled: true
      dashboard:
        viewOnlyMode: false
      ingress:
        enabled: true
----

.Kiali parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value

|enabled
|This parameter enables/disables Kiali. Kiali is enabled by default.
|`true`/`false`
|`true`

|dashboard
   viewOnlyMode
|This parameter enables/disables view-only mode for the Kiali console.  When view-only mode is enabled, users cannot use the console to make changes to the {SMProductShortName}.
|`true`/`false`
|`false`

|ingress
   enabled
|This parameter enables/disables ingress for Kiali.
|`true`/`false`
|`true`
|===

[id="configuring-kiali-grafana_{context}"]
== Configuring Kiali for Grafana

When you install Kiali and Grafana as part of {SMProductName} the Operator configures the following by default:

* Grafana is enabled as an external service for Kiali
* Grafana authorization for the Kiali console
* Grafana URL for the Kiali console

Kiali can automatically detect the Grafana URL. However if you have a custom Grafana installation that is not easily auto-detectable by Kiali, you must update the URL value in the `ServiceMeshControlPlane` resource.

.Additional Grafana parameters
[source,yaml]
----
spec:
  kiali:
    enabled: true
    dashboard:
      viewOnlyMode: false
      grafanaURL:  "https://grafana-istio-system.127.0.0.1.nip.io"
    ingress:
      enabled: true
----

[id="configuring-kiali-jaeger_{context}"]
== Configuring Kiali for Jaeger

When you install Kiali and Jaeger as part of {SMProductName} the Operator configures the following by default:

* Jaeger is enabled as an external service for Kiali
* Jaeger authorization for the Kiali console
* Jaeger URL for the Kiali console

Kiali can automatically detect the Jaeger URL. However if you have a custom Jaeger installation that is not easily auto-detectable by Kiali, you must update the URL value in the `ServiceMeshControlPlane` resource.

.Additional Jaeger parameters
[source,yaml]
----
spec:
  kiali:
    enabled: true
    dashboard:
      viewOnlyMode: false
      jaegerURL: "http://jaeger-query-istio-system.127.0.0.1.nip.io"
    ingress:
      enabled: true
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-custom-resources.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-configuring-jaeger_{context}"]
= Configuring Jaeger

When the {SMProductShortName} Operator creates the `ServiceMeshControlPlane` resource it can also create the resources for distributed tracing. {SMProductShortName} uses Jaeger for distributed tracing.

You can specify your Jaeger configuration in either of two ways:

* Configure Jaeger in the `ServiceMeshControlPlane` resource. There are some limitations with this approach.

* Configure Jaeger in a custom `Jaeger` resource and then reference that Jaeger instance in the  `ServiceMeshControlPlane` resource. If a Jaeger resource matching the value of `name` exists, the control plane will use the existing installation. This approach lets you fully customize your Jaeger configuration.

The default Jaeger parameters specified in the `ServiceMeshControlPlane` are as follows:

.Default `all-in-one` Jaeger parameters
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  version: v1.1
  istio:
    tracing:
      enabled: true
      jaeger:
        template: all-in-one
----

.Jaeger parameters
[options="header"]
[cols="l, a, a, a"]
|===
|Parameter |Description |Values |Default value

|tracing:
   enabled:
|This parameter enables/disables installing and deploying tracing by the Service Mesh Operator. Installing Jaeger is enabled by default.  To use an existing Jaeger deployment, set this value to `false`.
|`true`/`false`
|`true`

|jaeger:
   template:
|This parameter specifies which Jaeger deployment strategy to use.
|* `all-in-one`- For development, testing, demonstrations, and proof of concept.
* `production-elasticsearch` - For production use.
|`all-in-one`
|===

[NOTE]
====
The default template in the `ServiceMeshControlPlane` resource is the `all-in-one` deployment strategy which uses in-memory storage. For production, the only supported storage option is Elasticsearch, therefore you must configure the `ServiceMeshControlPlane` to request the `production-elasticsearch` template when you deploy {SMProductShortName} within a production environment.
====


[id="ossm-configuring-jaeger-elasticsearch_{context}"]
== Configuring Elasticsearch

The default Jaeger deployment strategy uses the `all-in-one` template so that the installation can be completed using minimal resources.  However, because the `all-in-one` template uses in-memory storage, it is only recommended for development, demo, or testing purposes and should NOT be used for production environments.

If you are deploying {SMProductShortName} and Jaeger in a production environment you must change the template to the `production-elasticsearch` template, which uses Elasticsearch for Jaeger's storage needs.

Elasticsearch is a memory intensive application. The initial set of nodes specified in the default {product-title} installation may not be large enough to support the Elasticsearch cluster.  You should modify the default Elasticsearch configuration to match your use case and the resources you have requested for your {product-title} installation. You can adjust both the CPU and memory limits for each component by modifying the resources block with valid CPU and memory values. Additional nodes must be added to the  cluster if you want to run with the recommended amount (or more) of memory. Ensure that you do not exceed the resources requested for your {product-title} installation.

.Default "production" Jaeger parameters with Elasticsearch
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    tracing:
    enabled: true
    ingress:
      enabled: true
    jaeger:
      template: production-elasticsearch
      elasticsearch:
        nodeCount: 3
        redundancyPolicy:
        resources:
          requests:
            cpu: "1"
            memory: "16Gi"
          limits:
            cpu: "1"
            memory: "16Gi"
----

.Elasticsearch parameters
[options="header"]
[cols="l, a, a, a, a"]
|===
|Parameter |Description |Values |Default Value |Examples

|tracing:
  enabled:
|This parameter enables/disables tracing in {SMProductShortName}. Jaeger is installed by default.
|`true`/`false`
|`true`
|

|ingress:
  enabled:
|This parameter enables/disables ingress for Jaeger.
|`true`/`false`
|`true`
|

|jaeger:
   template:
|This parameter specifies which Jaeger deployment strategy to use.
|`all-in-one`/`production-elasticsearch`
|`all-in-one`
|

|elasticsearch:
  nodeCount:
|Number of Elasticsearch nodes to create.
|Integer value.
|1
|Proof of concept = 1,
Minimum deployment =3

|requests:
  cpu:
|Number of central processing units for requests, based on your environment's configuration.
|Specified in cores or millicores (for example, 200m, 0.5, 1).
|1Gi
|Proof of concept = 500m,
Minimum deployment =1

|requests:
  memory:
|Available memory for requests, based on your environment's configuration.
|Specified in bytes (for example, 200Ki, 50Mi, 5Gi).
|500m
|Proof of concept = 1Gi,
Minimum deployment = 16Gi*

|limits:
  cpu:
|Limit on number of central processing units, based on your environment's configuration.
|Specified in cores or millicores (for example, 200m, 0.5, 1).
|
|Proof of concept = 500m,
Minimum deployment =1

|limits:
  memory:
|Available memory limit based on your environment's configuration.
|Specified in bytes (for example, 200Ki, 50Mi, 5Gi).
|
|Proof of concept = 1Gi,
Minimum deployment = 16Gi*

|
4+|{asterisk} Each Elasticsearch node can operate with a lower memory setting though this is *not* recommended for production deployments. For production use, you should have no less than 16Gi allocated to each pod by default, but preferably allocate as much as you can, up to 64Gi per pod.
|===


.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role.

. Navigate to *Operators* -> *Installed Operators*.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Control Plane* tab.

. Click the name of your control plane file, for example, `basic-install`.

. Click the *YAML* tab.

. Edit the Jaeger parameters, replacing the default `all-in-one` template with parameters for the `production-elasticsearch` template, modified for your use case.  Ensure that the indentation is correct.

. Click *Save*.

. Click *Reload*.
{product-title} redeploys Jaeger and creates the Elasticsearch resources based on the specified parameters.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-custom-resources.adoc

[id="ossm-configuring-jaeger-existing-v1x_{context}"]
= Connecting to an existing Jaeger instance

In order for the SMCP to connect to an existing Jaeger instance, the following must be true:

* The Jaeger instance is deployed in the same namespace as the control plane, for example, into the `istio-system` namespace.

* To enable secure communication between services, you should enable the oauth-proxy, which secures communication to your Jaeger instance, and make sure the secret is mounted into your Jaeger instance so Kiali can communicate with it.

* To use a custom or already existing Jaeger instance, set `spec.istio.tracing.enabled` to "false" to disable the deployment of a Jaeger instance.

* Supply the correct jaeger-collector endpoint to Mixer by setting `spec.istio.global.tracer.zipkin.address` to the hostname and port of your jaeger-collector service. The hostname of the service is usually `<jaeger-instance-name>-collector.<namespace>.svc.cluster.local`.

* Supply the correct jaeger-query endpoint to Kiali for gathering traces by setting `spec.istio.kiali.jaegerInClusterURL` to the hostname of your jaeger-query service - the port is normally not required, as it uses 443 by default. The hostname of the service is usually  `<jaeger-instance-name>-query.<namespace>.svc.cluster.local`.

* Supply the dashboard URL of your Jaeger instance to Kiali to enable accessing Jaeger through the Kiali console. You can retrieve the URL from the OpenShift route that is created by the Jaeger Operator. If your Jaeger resource is called `external-jaeger` and resides in the `istio-system` project, you can retrieve the route using the following command:
+
[source,terminal]
----
$ oc get route -n istio-system external-jaeger
----
+
.Example output
[source,terminal]
----
NAME                   HOST/PORT                                     PATH   SERVICES               [...]
external-jaeger        external-jaeger-istio-system.apps.test        external-jaeger-query  [...]
----
+
The value under `HOST/PORT` is the externally accessible URL of the Jaeger dashboard.


.Example Jaeger resource
[source,yaml]
----
apiVersion: jaegertracing.io/v1
kind: "Jaeger"
metadata:
  name: "external-jaeger"
  # Deploy to the Control Plane Namespace
  namespace: istio-system
spec:
  # Set Up Authentication
  ingress:
    enabled: true
    security: oauth-proxy
    openshift:
      # This limits user access to the Jaeger instance to users who have access
      # to the control plane namespace. Make sure to set the correct namespace here
      sar: '{"namespace": "istio-system", "resource": "pods", "verb": "get"}'
      htpasswdFile: /etc/proxy/htpasswd/auth

  volumeMounts:
  - name: secret-htpasswd
    mountPath: /etc/proxy/htpasswd
  volumes:
  - name: secret-htpasswd
    secret:
      secretName: htpasswd

----

The following `ServiceMeshControlPlane` example assumes that you have deployed Jaeger using the Jaeger Operator and the example Jaeger resource.

.Example `ServiceMeshControlPlane` with external Jaeger
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
metadata:
  name: external-jaeger
  namespace: istio-system
spec:
  version: v1.1
  istio:
    tracing:
      # Disable Jaeger deployment by service mesh operator
      enabled: false
    global:
      tracer:
        zipkin:
          # Set Endpoint for Trace Collection
          address: external-jaeger-collector.istio-system.svc.cluster.local:9411
    kiali:
      # Set Jaeger dashboard URL
      dashboard:
        jaegerURL: https://external-jaeger-istio-system.apps.test
      # Set Endpoint for Trace Querying
      jaegerInClusterURL: external-jaeger-query.istio-system.svc.cluster.local
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-custom-resources.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-jaeger-config-elasticsearch-v1x_{context}"]
= Configuring Elasticsearch

The default Jaeger deployment strategy uses the `all-in-one` template so that the installation can be completed using minimal resources.  However, because the `all-in-one` template uses in-memory storage, it is only recommended for development, demo, or testing purposes and should NOT be used for production environments.

If you are deploying {SMProductShortName} and Jaeger in a production environment you must change the template to the `production-elasticsearch` template, which uses Elasticsearch for Jaeger's storage needs.

Elasticsearch is a memory intensive application. The initial set of nodes specified in the default {product-title} installation may not be large enough to support the Elasticsearch cluster.  You should modify the default Elasticsearch configuration to match your use case and the resources you have requested for your {product-title} installation. You can adjust both the CPU and memory limits for each component by modifying the resources block with valid CPU and memory values. Additional nodes must be added to the  cluster if you want to run with the recommended amount (or more) of memory. Ensure that you do not exceed the resources requested for your {product-title} installation.

.Default "production" Jaeger parameters with Elasticsearch
[source,yaml]
----
apiVersion: maistra.io/v1
kind: ServiceMeshControlPlane
spec:
  istio:
    tracing:
    enabled: true
    ingress:
      enabled: true
    jaeger:
      template: production-elasticsearch
      elasticsearch:
        nodeCount: 3
        redundancyPolicy:
        resources:
          requests:
            cpu: "1"
            memory: "16Gi"
          limits:
            cpu: "1"
            memory: "16Gi"
----

.Elasticsearch parameters
[options="header"]
[cols="l, a, a, a, a"]
|===
|Parameter |Description |Values |Default Value |Examples

|tracing:
  enabled:
|This parameter enables/disables tracing in {SMProductShortName}. Jaeger is installed by default.
|`true`/`false`
|`true`
|

|ingress:
  enabled:
|This parameter enables/disables ingress for Jaeger.
|`true`/`false`
|`true`
|

|jaeger:
   template:
|This parameter specifies which Jaeger deployment strategy to use.
|`all-in-one`/`production-elasticsearch`
|`all-in-one`
|

|elasticsearch:
  nodeCount:
|Number of Elasticsearch nodes to create.
|Integer value.
|1
|Proof of concept = 1,
Minimum deployment =3

|requests:
  cpu:
|Number of central processing units for requests, based on your environment's configuration.
|Specified in cores or millicores (for example, 200m, 0.5, 1).
|1Gi
|Proof of concept = 500m,
Minimum deployment =1

|requests:
  memory:
|Available memory for requests, based on your environment's configuration.
|Specified in bytes (for example, 200Ki, 50Mi, 5Gi).
|500m
|Proof of concept = 1Gi,
Minimum deployment = 16Gi*

|limits:
  cpu:
|Limit on number of central processing units, based on your environment's configuration.
|Specified in cores or millicores (for example, 200m, 0.5, 1).
|
|Proof of concept = 500m,
Minimum deployment =1

|limits:
  memory:
|Available memory limit based on your environment's configuration.
|Specified in bytes (for example, 200Ki, 50Mi, 5Gi).
|
|Proof of concept = 1Gi,
Minimum deployment = 16Gi*

|
4+|{asterisk} Each Elasticsearch node can operate with a lower memory setting though this is *not* recommended for production deployments. For production use, you should have no less than 16Gi allocated to each pod by default, but preferably allocate as much as you can, up to 64Gi per pod.
|===


.Procedure

. Log in to the {product-title} web console as a user with the `cluster-admin` role.

. Navigate to *Operators* -> *Installed Operators*.

. Click the {SMProductName} Operator.

. Click the *Istio Service Mesh Control Plane* tab.

. Click the name of your control plane file, for example, `basic-install`.

. Click the *YAML* tab.

. Edit the Jaeger parameters, replacing the default `all-in-one` template with parameters for the `production-elasticsearch` template, modified for your use case.  Ensure that the indentation is correct.

. Click *Save*.

. Click *Reload*.
{product-title} redeploys Jaeger and creates the Elasticsearch resources based on the specified parameters.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/ossm-custom-resources.adoc

[id="ossm-jaeger-config-es-cleaner-v1x_{context}"]
= Configuring the Elasticsearch index cleaner job

When the {SMProductShortName} Operator creates the `ServiceMeshControlPlane` it also creates the custom resource (CR) for Jaeger. The {JaegerName} Operator then uses this CR when creating Jaeger instances.

When using Elasticsearch storage, by default a job is created to clean old traces from it. To configure the options for this job, you edit the Jaeger custom resource (CR), to customize it for your use case. The relevant options are listed below.

[source,yaml]
----
  apiVersion: jaegertracing.io/v1
  kind: Jaeger
  spec:
    strategy: production
    storage:
      type: elasticsearch
      esIndexCleaner:
        enabled: false
        numberOfDays: 7
        schedule: "55 23 * * *"
----

.Elasticsearch index cleaner parameters
|===
|Parameter |Values |Description

|enabled:
|true/ false
|Enable or disable the index cleaner job.

|numberOfDays:
|integer value
|Number of days to wait before deleting an index.

|schedule:
|"55 23 * * *"
|Cron expression for the job to run
|===

:leveloffset: 2

For more information about configuring Elasticsearch with {product-title}, see  link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#logging-config-es-store[Configuring the Elasticsearch log store].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/customizing-installation-ossm.adoc
// * service_mesh/v2x/customizing-installation-ossm.adoc

[id="ossm-cr-threescale_{context}"]

= 3scale configuration

The following table explains the parameters for the 3scale Istio Adapter in the `ServiceMeshControlPlane` resource.

.Example 3scale parameters
[source,yaml]
----
spec:
  addons:
    3Scale:
      enabled: false
      PARAM_THREESCALE_LISTEN_ADDR: 3333
      PARAM_THREESCALE_LOG_LEVEL: info
      PARAM_THREESCALE_LOG_JSON: true
      PARAM_THREESCALE_LOG_GRPC: false
      PARAM_THREESCALE_REPORT_METRICS: true
      PARAM_THREESCALE_METRICS_PORT: 8080
      PARAM_THREESCALE_CACHE_TTL_SECONDS: 300
      PARAM_THREESCALE_CACHE_REFRESH_SECONDS: 180
      PARAM_THREESCALE_CACHE_ENTRIES_MAX: 1000
      PARAM_THREESCALE_CACHE_REFRESH_RETRIES: 1
      PARAM_THREESCALE_ALLOW_INSECURE_CONN: false
      PARAM_THREESCALE_CLIENT_TIMEOUT_SECONDS: 10
      PARAM_THREESCALE_GRPC_CONN_MAX_SECONDS: 60
      PARAM_USE_CACHED_BACKEND: false
      PARAM_BACKEND_CACHE_FLUSH_INTERVAL_SECONDS: 15
      PARAM_BACKEND_CACHE_POLICY_FAIL_CLOSED: true
----

.3scale parameters
|===
|Parameter |Description |Values |Default value

|`enabled`
|Whether to use the 3scale adapter
|`true`/`false`
|`false`

|`PARAM_THREESCALE_LISTEN_ADDR`
|Sets the listen address for the gRPC server
|Valid port number
|`3333`

|`PARAM_THREESCALE_LOG_LEVEL`
|Sets the minimum log output level.
|`debug`, `info`, `warn`, `error`, or `none`
|`info`

|`PARAM_THREESCALE_LOG_JSON`
|Controls whether the log is formatted as JSON
|`true`/`false`
|`true`

|`PARAM_THREESCALE_LOG_GRPC`
|Controls whether the log contains gRPC info
|`true`/`false`
|`true`

|`PARAM_THREESCALE_REPORT_METRICS`
|Controls whether 3scale system and backend metrics are collected and reported to Prometheus
|`true`/`false`
|`true`

|`PARAM_THREESCALE_METRICS_PORT`
|Sets the port that the 3scale `/metrics` endpoint can be scrapped from
|Valid port number
|`8080`

|`PARAM_THREESCALE_CACHE_TTL_SECONDS`
|Time period, in seconds, to wait before purging expired items from the cache
|Time period in seconds
|`300`

|`PARAM_THREESCALE_CACHE_REFRESH_SECONDS`
|Time period before expiry when cache elements are attempted to be refreshed
|Time period in seconds
|`180`

|`PARAM_THREESCALE_CACHE_ENTRIES_MAX`
|Max number of items that can be stored in the cache at any time. Set to `0` to disable caching
|Valid number
|`1000`

|`PARAM_THREESCALE_CACHE_REFRESH_RETRIES`
|The number of times unreachable hosts are retried during a cache update loop
|Valid number
|`1`

|`PARAM_THREESCALE_ALLOW_INSECURE_CONN`
|Allow to skip certificate verification when calling `3scale` APIs. Enabling this is not recommended.
|`true`/`false`
|`false`

|`PARAM_THREESCALE_CLIENT_TIMEOUT_SECONDS`
|Sets the number of seconds to wait before terminating requests to 3scale System and Backend
|Time period in seconds
|`10`

|`PARAM_THREESCALE_GRPC_CONN_MAX_SECONDS`
|Sets the maximum amount of seconds (+/-10% jitter) a connection may exist before it is closed
|Time period in seconds
|60


|`PARAM_USE_CACHE_BACKEND`
|If true, attempt to create an in-memory apisonator cache for authorization requests
|`true`/`false`
|`false`

|`PARAM_BACKEND_CACHE_FLUSH_INTERVAL_SECONDS`
|If the backend cache is enabled, this sets the interval in seconds for flushing the cache against 3scale
|Time period in seconds
|15

|`PARAM_BACKEND_CACHE_POLICY_FAIL_CLOSED`
|Whenever the backend cache cannot retrieve authorization data, whether to deny (closed) or allow (open) requests
|`true`/`false`
|`true`
|===

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="threescale-adapter-v1x"]
= Using the 3scale Istio adapter
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: threescale-adapter-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

The 3scale Istio Adapter is an optional adapter that allows you to label a service running within the {SMProductName} and integrate that service with the 3scale API Management solution.
It is not required for {SMProductName}.


:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-integrate-1x_{context}"]
= Integrate the 3scale adapter with {SMProductName}

You can use these examples to configure requests to your services using the 3scale Istio Adapter.


.Prerequisites:

* {SMProductName} version 1.x
* A working 3scale account (link:https://www.3scale.net/signup/[SaaS] or link:https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.5/html/installing_3scale/onpremises-installation[3scale 2.5 On-Premises])
* Enabling backend cache requires 3scale 2.9 or greater
* {SMProductName} prerequisites

[NOTE]
====
To configure the 3scale Istio Adapter, refer to {SMProductName} custom resources for instructions on adding adapter parameters to the custom resource file.
====


[NOTE]
====
Pay particular attention to the `kind: handler` resource. You must update this with your 3scale account credentials. You can optionally add a `service_id` to a handler, but this is kept for backwards compatibility only, since it would render the handler only useful for one service in your 3scale account. If you add `service_id` to a handler, enabling 3scale for other services requires you to create more handlers with different `service_ids`.
====

Use a single handler per 3scale account by following the steps below:

.Procedure

. Create a handler for your 3scale account and specify your account credentials. Omit any service identifier.
+
[source,yaml]
----
  apiVersion: "config.istio.io/v1alpha2"
  kind: handler
  metadata:
   name: threescale
  spec:
   adapter: threescale
   params:
     system_url: "https://<organization>-admin.3scale.net/"
     access_token: "<ACCESS_TOKEN>"
   connection:
     address: "threescale-istio-adapter:3333"
----
+
Optionally, you can provide a `backend_url` field within the _params_ section to override the URL provided by the 3scale configuration. This may be useful if the adapter runs on the same cluster as the 3scale on-premise instance, and you wish to leverage the internal cluster DNS.
+
. Edit or patch the Deployment resource of any services belonging to your 3scale account as follows:
.. Add the `"service-mesh.3scale.net/service-id"` label with a value corresponding to a valid `service_id`.
.. Add the `"service-mesh.3scale.net/credentials"` label with its value being the _name of the handler resource_ from step 1.
. Do step 2 to link it to your 3scale account credentials and to its service identifier, whenever you intend to add more services.
. Modify the rule configuration with your 3scale configuration to dispatch the rule to the threescale handler.
+
.Rule configuration example
[source,yaml]
----
  apiVersion: "config.istio.io/v1alpha2"
  kind: rule
  metadata:
    name: threescale
  spec:
    match: destination.labels["service-mesh.3scale.net"] == "true"
    actions:
      - handler: threescale.handler
        instances:
          - threescale-authorization.instance
----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-cr_{context}"]
= Generating 3scale custom resources

The adapter includes a tool that allows you to generate the `handler`, `instance`, and `rule` custom resources.

.Usage
|===
|Option |Description |Required | Default value

|`-h, --help`
|Produces help output for available options
|No
|

|`--name`
|Unique name for this URL, token pair
|Yes
|

|`-n, --namespace`
|Namespace to generate templates
|No
|istio-system

|`-t, --token`
|3scale access token
|Yes
|

|`-u, --url`
|3scale Admin Portal URL
|Yes
|

|`--backend-url`
|3scale backend URL. If set, it overrides the value that is read from system configuration
|No
|

|`-s, --service`
|3scale API/Service ID
|No
|

|`--auth`
|3scale authentication pattern to specify (1=API Key, 2=App Id/App Key, 3=OIDC)
|No
|Hybrid

|`-o, --output`
|File to save produced manifests to
|No
|Standard output

|`--version`
|Outputs the CLI version and exits immediately
|No
|
|===

:leveloffset: 2

:leveloffset: +3

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-templates_{context}"]
= Generate templates from URL examples

[NOTE]
====
* Run the following commands via `oc exec` from the 3scale adapter container image in xref:ossm-threescale-manifests_{context}[Generating manifests from a deployed adapter].
* Use the `3scale-config-gen` command to help avoid YAML syntax and indentation errors.
* You can omit the `--service` if you use the annotations.
* This command must be invoked from within the container image via `oc exec`.
====

.Procedure

* Use the `3scale-config-gen` command to autogenerate templates files allowing the token, URL pair to be shared by multiple services as a single handler:
+
----
$ 3scale-config-gen --name=admin-credentials --url="https://<organization>-admin.3scale.net:443" --token="[redacted]"
----
+
* The following example generates the templates with the service ID embedded in the handler:
+
----
$ 3scale-config-gen --url="https://<organization>-admin.3scale.net" --name="my-unique-id" --service="123456789" --token="[redacted]"
----

[role="_additional-resources"]
.Additional resources
* link:https://access.redhat.com/documentation/en-us/red_hat_3scale_api_management/2.10/html-single/admin_portal_guide/index#tokens[Tokens].

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-manifests_{context}"]
= Generating manifests from a deployed adapter

[NOTE]
====
* `NAME` is an identifier you use to identify with the service you are managing with 3scale.
* The `CREDENTIALS_NAME` reference is an identifier that corresponds to the `match` section in the rule configuration. This is automatically set to the `NAME` identifier if you are using the CLI tool.
* Its value does not need to be anything specific: the label value should just match the contents of the rule. See link:https://github.com/3scale/3scale-istio-adapter/blob/v2.X/README.md#routing-service-traffic-through-the-adapter[Routing service traffic through the adapter] for more information.
====

. Run this command to generate manifests from a deployed adapter in the `istio-system` namespace:
+
----
$ export NS="istio-system" URL="https://replaceme-admin.3scale.net:443" NAME="name" TOKEN="token"
oc exec -n ${NS} $(oc get po -n ${NS} -o jsonpath='{.items[?(@.metadata.labels.app=="3scale-istio-adapter")].metadata.name}') \
-it -- ./3scale-config-gen \
--url ${URL} --name ${NAME} --token ${TOKEN} -n ${NS}
----

. This will produce sample output to the terminal. Edit these samples if required and create the objects using the `oc create` command.

. When the request reaches the adapter, the adapter needs to know how the service maps to an API on 3scale. You can provide this information in two ways:

.. Label the workload (recommended)
.. Hard code the handler as `service_id`


. Update the workload with the required annotations:
+
[NOTE]
====
You only need to update the service ID provided in this example if it is not already embedded in the handler. *The setting in the handler takes precedence*.
====
+
----
$ export CREDENTIALS_NAME="replace-me"
export SERVICE_ID="replace-me"
export DEPLOYMENT="replace-me"
patch="$(oc get deployment "${DEPLOYMENT}"
patch="$(oc get deployment "${DEPLOYMENT}" --template='{"spec":{"template":{"metadata":{"labels":{ {{ range $k,$v := .spec.template.metadata.labels }}"{{ $k }}":"{{ $v }}",{{ end }}"service-mesh.3scale.net/service-id":"'"${SERVICE_ID}"'","service-mesh.3scale.net/credentials":"'"${CREDENTIALS_NAME}"'"}}}}}' )"
oc patch deployment "${DEPLOYMENT}" --patch ''"${patch}"''

----

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-routing_{context}"]
= Routing service traffic through the adapter
Follow these steps to drive traffic for your service through the 3scale adapter.

.Prerequisites

* Credentials and service ID from your 3scale administrator.

.Procedure

. Match the rule `destination.labels["service-mesh.3scale.net/credentials"] == "threescale"` that you previously created in the configuration, in the `kind: rule` resource.

. Add the above label to `PodTemplateSpec` on the Deployment of the target workload to integrate a service. the value, `threescale`, refers to the name of the generated handler. This handler stores the access token required to call 3scale.

. Add the `destination.labels["service-mesh.3scale.net/service-id"] == "replace-me"` label to the workload to pass the service ID to the adapter via the instance at request time.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-integration-settings_{context}"]
= Configure the integration settings in 3scale

Follow this procedure to configure the 3scale integration settings.

[NOTE]
====
For 3scale SaaS customers, {SMProductName} is enabled as part of the Early Access program.
====

.Procedure

. Navigate to *[your_API_name]* -> *Integration*

. Click *Settings*.

. Select the *Istio* option under _Deployment_.
+
* The *API Key (user_key)* option under _Authentication_ is selected by default.

. Click *Update Product* to save your selection.

. Click *Configuration*.

. Click *Update Configuration*.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-caching_{context}"]
= Caching behavior
Responses from 3scale System APIs are cached by default within the adapter. Entries will be purged from the cache when they become older than the `cacheTTLSeconds` value. Also by default, automatic refreshing of cached entries will be attempted seconds before they expire, based on the `cacheRefreshSeconds` value. You can disable automatic refreshing by setting this value higher than the `cacheTTLSeconds` value.

Caching can be disabled entirely by setting `cacheEntriesMax` to a non-positive value.

By using the refreshing process, cached values whose hosts become unreachable will be retried before eventually being purged when past their expiry.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-authentication_{context}"]
= Authenticating requests
This release supports the following authentication methods:

* *Standard API Keys*: single randomized strings or hashes acting as an identifier and a secret token.
* *Application identifier and key pairs*: immutable identifier and mutable secret key strings.
* *OpenID authentication method*: client ID string parsed from the JSON Web Token.

[id="ossm-threescale-authentication-patterns_{context}"]
== Applying authentication patterns
Modify the `instance` custom resource, as illustrated in the following authentication method examples, to configure authentication behavior. You can accept the authentication credentials from:

* Request headers
* Request parameters
* Both request headers and query parameters

[NOTE]
====
When specifying values from headers, they must be lower case. For example, if you want to send a header as `User-Key`, this must be referenced in the configuration as `request.headers["user-key"]`.
====

[id="ossm-threescale-apikey-authentication_{context}"]
=== API key authentication method
{SMProductShortName} looks for the API key in query parameters and request headers as specified in the `user` option in the `subject` custom resource parameter. It checks the values in the order given in the custom resource file. You can restrict the search for the API key to either query parameters or request headers by omitting the unwanted option.

In this example, {SMProductShortName} looks for the API key in the `user_key` query parameter. If the API key is not in the query parameter, {SMProductShortName} then checks the `user-key` header.

.API key authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
  namespace: istio-system
spec:
  template: authorization
  params:
    subject:
      user: request.query_params["user_key"] | request.headers["user-key"] | ""
    action:
      path: request.url_path
      method: request.method | "get"
----

If you want the adapter to examine a different query parameter or request header, change the name as appropriate. For example, to check for the API key in a query parameter named “key”, change `request.query_params["user_key"]` to `request.query_params["key"]`.

[id="ossm-threescale-appidapikey-authentication_{context}"]
=== Application ID and application key pair authentication method
{SMProductShortName} looks for the application ID and application key in query parameters and request headers, as specified in the `properties` option in the `subject` custom resource parameter. The application key is optional. It checks the values in the order given in the custom resource file. You can restrict the search for the credentials to either query parameters or request headers by not including the unwanted option.

In this example, {SMProductShortName} looks for the application ID and application key in the query parameters first, moving on to the request headers if needed.

.Application ID and application key pair authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
  namespace: istio-system
spec:
  template: authorization
  params:
    subject:
        app_id: request.query_params["app_id"] | request.headers["app-id"] | ""
        app_key: request.query_params["app_key"] | request.headers["app-key"] | ""
    action:
      path: request.url_path
      method: request.method | "get"
----

If you want the adapter to examine a different query parameter or request header, change the name as appropriate. For example, to check for the application ID in a query parameter named `identification`, change `request.query_params["app_id"]` to `request.query_params["identification"]`.

[id="ossm-threescale-openid-authentication_{context}"]
=== OpenID authentication method
To use the _OpenID Connect (OIDC) authentication method_, use the `properties` value on the `subject` field to set `client_id`, and optionally `app_key`.

You can manipulate this object using the methods described previously. In the example configuration shown below, the client identifier (application ID) is parsed from the JSON Web Token (JWT) under the label _azp_. You can modify this as needed.

.OpenID authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
spec:
  template: threescale-authorization
  params:
    subject:
      properties:
        app_key: request.query_params["app_key"] | request.headers["app-key"] | ""
        client_id: request.auth.claims["azp"] | ""
      action:
        path: request.url_path
        method: request.method | "get"
        service: destination.labels["service-mesh.3scale.net/service-id"] | ""
----

For this integration to work correctly, OIDC must still be done in 3scale for the client to be created in the identity provider (IdP). You should create a link:https://istio.io/latest/docs/tasks/security/authorization/authz-jwt/[Request authorization] for the service you want to protect in the same namespace as that service. The JWT is passed in the `Authorization` header of the request.

In the sample `RequestAuthentication` defined below, replace `issuer`, `jwksUri`, and `selector` as appropriate.

.OpenID Policy example

[source,yaml]
----
apiVersion: security.istio.io/v1beta1
kind: RequestAuthentication
metadata:
  name: jwt-example
  namespace: bookinfo
spec:
  selector:
    matchLabels:
      app: productpage
  jwtRules:
  - issuer: >-
      http://keycloak-keycloak.34.242.107.254.nip.io/auth/realms/3scale-keycloak
    jwksUri: >-
      http://keycloak-keycloak.34.242.107.254.nip.io/auth/realms/3scale-keycloak/protocol/openid-connect/certs
----

[id="ossm-threescale-hybrid-authentication_{context}"]
=== Hybrid authentication method
You can choose to not enforce a particular authentication method and accept any valid credentials for either method. If both an API key and an application ID/application key pair are provided, {SMProductShortName} uses the API key.

In this example, {SMProductShortName} checks for an API key in the query parameters, then the request headers. If there is no API key, it then checks for an application ID and key in the query parameters, then the request headers.

.Hybrid authentication method example

[source,yaml]
----
apiVersion: "config.istio.io/v1alpha2"
kind: instance
metadata:
  name: threescale-authorization
spec:
  template: authorization
  params:
    subject:
      user: request.query_params["user_key"] | request.headers["user-key"] |
      properties:
        app_id: request.query_params["app_id"] | request.headers["app-id"] | ""
        app_key: request.query_params["app_key"] | request.headers["app-key"] | ""
        client_id: request.auth.claims["azp"] | ""
    action:
      path: request.url_path
      method: request.method | "get"
      service: destination.labels["service-mesh.3scale.net/service-id"] | ""
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-metrics-1x_{context}"]
= 3scale Adapter metrics
The adapter, by default reports various Prometheus metrics that are exposed on port `8080` at the `/metrics` endpoint. These metrics provide insight into how the interactions between the adapter and 3scale are performing. The service is labeled to be automatically discovered and scraped by Prometheus.

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-threescale-istio-adapter-verification_{context}"]
= 3scale Istio adapter verification

You might want to check whether the 3scale Istio adapter is working as expected. If your adapter is not working, use the following steps to help troubleshoot the problem.

.Procedure

. Ensure the _3scale-adapter_ pod is running in the {SMProductShortName} control plane namespace:
+
[source,terminal]
----
$ oc get pods -n <istio-system>
----
. Check that the _3scale-adapter_ pod has printed out information about itself booting up, such as its version:
+
[source,terminal]
----
$ oc logs <istio-system>
----
. When performing requests to the services protected by the 3scale adapter integration, always try requests that lack the right credentials and ensure they fail. Check the 3scale adapter logs to gather additional information.

:leveloffset: 2

[role="_additional-resources"]
.Additional resources
* link:https://docs.openshift.com/container-platform/4.7/support/troubleshooting/investigating-pod-issues.html#inspecting-pod-and-container-logs_investigating-pod-issues[Inspecting pod and container logs].

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/threescale_adapter/threescale-adapter.adoc
// * service_mesh/v2x/threescale_adapter/threescale-adapter.adoc

[id="ossm-threescale-istio-adapter-troubleshooting-checklist_{context}"]
= 3scale Istio adapter troubleshooting checklist

As the administrator installing the 3scale Istio adapter, there are a number of scenarios that might be causing your integration to not function properly. Use the following list to troubleshoot your installation:

* Incorrect YAML indentation.
* Missing YAML sections.
* Forgot to apply the changes in the YAML to the cluster.
* Forgot to label the service workloads with the `service-mesh.3scale.net/credentials` key.
* Forgot to label the service workloads with `service-mesh.3scale.net/service-id` when using handlers that do not contain a `service_id` so they are reusable per account.
* The _Rule_ custom resource points to the wrong handler or instance custom resources, or the references lack the corresponding namespace suffix.
* The _Rule_ custom resource `match` section cannot possibly match the service you are configuring, or it points to a destination workload that is not currently running or does not exist.
* Wrong access token or URL for the 3scale Admin Portal in the handler.
* The _Instance_ custom resource's `params/subject/properties` section fails to list the right parameters for `app_id`, `app_key`, or `client_id`, either because they specify the wrong location such as the query parameters, headers, and authorization claims, or the parameter names do not match the requests used for testing.
* Failing to use the configuration generator without realizing that it actually lives in the adapter container image and needs `oc exec` to invoke it.

:leveloffset: 2

:leveloffset!:

:leveloffset: +2

:_mod-docs-content-type: ASSEMBLY
[id="removing-ossm-v1x"]
= Removing Service Mesh
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 9.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
// OADP attributes
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oadp-short: OADP
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.9
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.5
:sandboxed-containers-version-z: 1.5.0
:sandboxed-containers-legacy-version: 1.4.1
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
:descheduler-operator: Kube Descheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.12
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.2
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.13
:pipelines-version-number: 1.13
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.15
:HCOVersion: 4.15.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 3.0
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.51.0
:OTELName: Red Hat build of OpenTelemetry
:OTELShortName: Red Hat build of OpenTelemetry
:OTELOperator: Red Hat build of OpenTelemetry Operator
:OTELVersion: 0.89.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.3.0
//telco
//logging
:logging: logging
:logging-uc: Logging
:for: for Red Hat OpenShift
:clo: Red Hat OpenShift Logging Operator
:loki-op: Loki Operator
:es-op: OpenShift Elasticsearch Operator
:log-plug: logging Console plugin
//power monitoring
:PM-title-c: Power monitoring for Red Hat OpenShift
:PM-title: power monitoring for Red Hat OpenShift
:PM-shortname: power monitoring
:PM-shortname-c: Power monitoring
:PM-operator: Power monitoring Operator
:PM-kepler: Kepler
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.5
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service
:sts-full: Security Token Service (STS)
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services
:aws-full: Amazon Web Services (AWS)
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM general
:ibm-name: IBM(R)
:ibm-title: IBM
// IBM Cloud
:ibm-cloud-name: IBM Cloud(R)
:ibm-cloud-title: IBM Cloud
// IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm: IBM Cloud(R) Bare Metal (Classic)
:ibm-cloud-bm-title: IBM Cloud Bare Metal (Classic)
// IBM Power
:ibm-power-name: IBM Power(R)
:ibm-power-title: IBM Power
:ibm-power-server-name: IBM Power(R) Virtual Server
:ibm-power-server-title: IBM Power Virtual Server
// IBM zSystems
:ibm-z-name: IBM Z(R)
:ibm-z-title: IBM Z
:ibm-linuxone-name: IBM(R) LinuxONE
:ibm-linuxone-title: IBM LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure (OCI)
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// Cluster Observability Operator
:coo-first: Cluster Observability Operator (COO)
:coo-full: Cluster Observability Operator
:coo-short: COO
//ODF
:odf-first: Red Hat OpenShift Data Foundation (ODF)
:odf-full: Red Hat OpenShift Data Foundation
:odf-short: ODF
:rh-dev-hub: Red Hat Developer Hub
:context: removing-ossm-v1x

toc::[]

// Text snippet included in all Service Mesh v1 assemblies.
// NOTE: The OpenShift docs standards state that snippets should NOT contain xrefs.   https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#writing-text-snippets
//Because this snippet contains two xrefs it should ONLY be used in the v1 assemblies and never in a module.

:_mod-docs-content-type: SNIPPET

[WARNING]
====
*You are viewing documentation for a {SMProductName} release that is no longer supported.*

Service Mesh version 1.0 and 1.1 control planes are no longer supported. For information about upgrading your service mesh control plane, see xref:ossm-versions_ossm-upgrade[Upgrading Service Mesh].

For information about the support status of a particular {SMProductName} release, see the https://access.redhat.com/support/policy/updates/openshift#ossm[Product lifecycle page].
====

To remove {SMProductName} from an existing {product-title} instance, remove the control plane before removing the operators.

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-control-plane-remove_{context}"]
= Removing the {SMProductName} control plane

To uninstall {SMProductShortName} from an existing {product-title} instance, first you delete the {SMProductShortName} control plane and the Operators. Then, you run commands to remove residual resources.

[id="ossm-control-plane-remove-operatorhub_{context}"]
== Removing the {SMProductShortName} control plane using the web console

You can remove the {SMProductName} control plane by using the web console.

.Procedure

. Log in to the {product-title} web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Navigate to *Operators* -> *Installed Operators*.

. Click *Service Mesh Control Plane* under *Provided APIs*.

. Click the `ServiceMeshControlPlane` menu {kebab}.

. Click *Delete Service Mesh Control Plane*.

. Click *Delete* on the confirmation dialog window to remove the `ServiceMeshControlPlane`.

[id="ossm-control-plane-remove-cli_{context}"]
== Removing the {SMProductShortName} control plane using the CLI

You can remove the {SMProductName} control plane by using the CLI.  In this example, `istio-system` is the name of the control plane project.

.Procedure

. Log in to the {product-title} CLI.

. Run the following command to delete the `ServiceMeshMemberRoll` resource.
+
[source,terminal]
----
$ oc delete smmr -n istio-system default
----

. Run this command to retrieve the name of the installed `ServiceMeshControlPlane`:
+
[source,terminal]
----
$ oc get smcp -n istio-system
----

. Replace `<name_of_custom_resource>` with the output from the previous command, and run this command to remove the custom resource:
+
[source,terminal]
----
$ oc delete smcp -n istio-system <name_of_custom_resource>
----

:leveloffset: 2

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc
// * service_mesh/v2x/installing-ossm.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-operatorhub-remove-operators_{context}"]
= Removing the installed Operators

You must remove the Operators to successfully remove {SMProductName}. After you remove the {SMProductName} Operator, you must remove the Kiali Operator, the {JaegerName} Operator, and the OpenShift Elasticsearch Operator.

[id="ossm-remove-operator-servicemesh_{context}"]
== Removing the Operators

Follow this procedure to remove the Operators that make up {SMProductName}. Repeat the steps for each of the following Operators.

* {SMProductName}
* Kiali
* {JaegerName}
* OpenShift Elasticsearch

.Procedure

. Log in to the {product-title} web console.

. From the *Operators* → *Installed Operators* page, scroll or type a keyword into the *Filter by name* to find each Operator. Then, click the Operator name.

. On the *Operator Details* page, select *Uninstall Operator* from the *Actions* menu. Follow the prompts to uninstall each Operator.

:leveloffset: 2

:leveloffset: +2

// Module included in the following assemblies:
//
// * service_mesh/v1x/installing-ossm.adoc


:_mod-docs-content-type: PROCEDURE
[id="ossm-remove-cleanup-1x_{context}"]
= Clean up Operator resources

Follow this procedure to manually remove resources left behind after removing the {SMProductName} Operator using the {product-title} web console.

.Prerequisites

* An account with cluster administration access.
* Access to the OpenShift CLI (`oc`).

.Procedure

. Log in to the {product-title} CLI as a cluster administrator.

. Run the following commands to clean up resources after uninstalling the Operators. If you intend to keep using Jaeger as a stand alone service without service mesh, do not delete the Jaeger resources.
+
[NOTE]
====
The Operators are installed in the `openshift-operators` namespace by default.  If you installed the Operators in another namespace, replace `openshift-operators` with the name of the project where the {SMProductName} Operator was installed.
====
+
[source,terminal]
----
$ oc delete validatingwebhookconfiguration/openshift-operators.servicemesh-resources.maistra.io
----
+
[source,terminal]
----
$ oc delete mutatingwebhookconfiguration/openshift-operators.servicemesh-resources.maistra.io
----
+
[source,terminal]
----
$ oc delete -n openshift-operators daemonset/istio-node
----
+
[source,terminal]
----
$ oc delete clusterrole/istio-admin clusterrole/istio-cni clusterrolebinding/istio-cni
----
// needs a slash?  What is the format here?
+
[source,terminal]
----
$ oc delete clusterrole istio-view istio-edit
----
+
[source,terminal]
----
$ oc delete clusterrole jaegers.jaegertracing.io-v1-admin jaegers.jaegertracing.io-v1-crdview jaegers.jaegertracing.io-v1-edit jaegers.jaegertracing.io-v1-view
----
+
[source,terminal]
----
$ oc get crds -o name | grep '.*\.istio\.io' | xargs -r -n 1 oc delete
----
+
[source,terminal]
----
$ oc get crds -o name | grep '.*\.maistra\.io' | xargs -r -n 1 oc delete
----
+
[source,terminal]
----
$ oc get crds -o name | grep '.*\.kiali\.io' | xargs -r -n 1 oc delete
----
+
[source,terminal]
----
$ oc delete crds jaegers.jaegertracing.io
----
+
[source,terminal]
----
$ oc delete svc admission-controller -n <operator-project>
----
+
[source,terminal]
----
$ oc delete project <istio-system-project>
----

:leveloffset: 2

:leveloffset!:

//# includes=v2x/ossm-about,v2x/_attributes/common-attributes,v2x/modules/ossm-servicemesh-overview,v2x/modules/ossm-core-features,v2x/servicemesh-release-notes,v2x/modules/making-open-source-more-inclusive,v2x/modules/ossm-rn-new-features,v2x/modules/snippets/technology-preview,v2x/modules/ossm-rn-technology-preview,v2x/modules/ossm-rn-deprecated-features,v2x/modules/ossm-rn-known-issues,v2x/modules/ossm-rn-fixed-issues,v2x/ossm-architecture,v2x/modules/ossm-understanding-service-mesh,v2x/modules/ossm-architecture,v2x/modules/ossm-kiali-overview,v2x/modules/ossm-kiali-architecture,v2x/modules/ossm-kiali-features,v2x/modules/distr-tracing-product-overview,v2x/modules/distr-tracing-architecture,v2x/modules/distr-tracing-features,v2x/ossm-deployment-models,v2x/modules/ossm-deploy-cluster-wide-mesh,v2x/modules/ossm-deploy-multitenant,v2x/modules/ossm-deploy-multi-mesh,v2x/ossm-vs-community,v2x/modules/ossm-vs-istio,v2x/modules/ossm-multitenant,v2x/modules/ossm-kiali-service-mesh,v2x/modules/ossm-jaeger-service-mesh,v2x/preparing-ossm-installation,v2x/modules/ossm-supported-configurations,v2x/installing-ossm,v2x/modules/ossm-installation-activities,v2x/modules/ossm-install-ossm-operator,v2x/modules/ossm-config-operator-infrastructure-node,v2x/modules/ossm-confirm-operator-infrastructure-node,v2x/ossm-create-smcp,v2x/modules/ossm-about-smcp,v2x/modules/ossm-control-plane-web,v2x/modules/ossm-control-plane-cli,v2x/modules/ossm-validate-smcp-cli,v2x/modules/ossm-about-control-plane-components-and-infrastructure-nodes,v2x/modules/ossm-config-control-plane-infrastructure-node-console,v2x/modules/ossm-config-individual-control-plane-infrastructure-node-console,v2x/modules/ossm-config-control-plane-infrastructure-node-cli,v2x/modules/ossm-config-individual-control-plane-infrastructure-node-cli,v2x/modules/ossm-confirm-smcp-infrastructure-node,v2x/modules/ossm-about-control-plane-and-cluster-wide-deployment,v2x/modules/ossm-deploy-cluster-wide-control-plane-console,v2x/modules/ossm-deploy-cluster-wide-control-plane-cli,v2x/modules/ossm-customize-smmr-cluster-wide,v2x/modules/ossm-validate-smcp-kiali,v2x/modules/ossm-install-rosa,v2x/ossm-create-mesh,v2x/modules/ossm-about-adding-namespace,v2x/modules/ossm-member-roll-create,v2x/modules/ossm-about-adding-projects-using-smmr,v2x/modules/ossm-add-project-member-roll-resource-console,v2x/modules/ossm-add-project-member-roll-resource-cli,v2x/modules/ossm-about-adding-projects-using-smm,v2x/modules/ossm-adding-project-using-smm-resource-console,v2x/modules/ossm-adding-project-using-smm-resource-cli,v2x/modules/ossm-about-adding-projects-using-label-selectors,v2x/modules/ossm-add-project-using-label-selectors-console,v2x/modules/ossm-add-project-using-label-selectors-cli,v2x/modules/ossm-tutorial-bookinfo-overview,v2x/modules/ossm-tutorial-bookinfo-install,v2x/modules/ossm-tutorial-bookinfo-adding-destination-rules,v2x/modules/ossm-tutorial-bookinfo-verify-install,v2x/modules/ossm-tutorial-bookinfo-removing,v2x/prepare-to-deploy-applications-ossm,v2x/modules/ossm-automatic-sidecar-injection,v2x/modules/ossm-sidecar-validate-kiali,v2x/modules/ossm-sidecar-injection-env-var,v2x/modules/ossm-update-app-sidecar,v2x/upgrading-ossm,v2x/modules/ossm-understanding-versioning,v2x/modules/ossm-understanding-versions,v2x/modules/ossm-upgrade-considerations,v2x/modules/ossm-upgrade-known-issues,v2x/modules/ossm-upgrading-operator,v2x/modules/ossm-upgrade-23-24-changes,v2x/modules/ossm-upgrade-22-23-changes,v2x/modules/ossm-upgrade-21-22-changes,v2x/modules/ossm-upgrade-20-21-changes,v2x/modules/ossm-upgrading-smcp,v2x/modules/ossm-migrating-to-20,v2x/modules/ossm-upgrade-apps-workloads,v2x/ossm-profiles-users,v2x/modules/ossm-members,v2x/modules/ossm-control-plane-profiles,v2x/modules/ossm-config-network-policy,v2x/ossm-security,v2x/modules/ossm-security-mtls,v2x/modules/ossm-config-sec-mtls-mesh,v2x/modules/ossm-config-sidecar-mtls,v2x/modules/ossm-config-sidecar-out-mtls,v2x/modules/ossm-config-mtls-min-max,v2x/modules/ossm-validate-encryption-kiali,v2x/modules/ossm-security-auth-policy,v2x/modules/ossm-security-cipher,v2x/modules/ossm-configuring-jwks-resolver-ca,v2x/modules/ossm-security-cert-manage,v2x/modules/ossm-cert-manage-add-cert-key,v2x/modules/ossm-cert-manage-verify-cert,v2x/modules/ossm-cert-cleanup,v2x/modules/ossm-cert-manager-integration-istio,v2x/modules/ossm-cert-manager-installation,v2x/ossm-traffic-manage,v2x/modules/ossm-gateways,v2x/modules/ossm-automatic-gateway-injection,v2x/modules/ossm-deploying-automatic-gateway-injection,v2x/modules/ossm-routing-ingress,v2x/modules/ossm-routing-gateways,v2x/modules/ossm-auto-route,v2x/modules/ossm-auto-route-annotations,v2x/modules/ossm-auto-route-enable,v2x/modules/ossm-routing-service-entries,v2x/modules/ossm-routing-virtual-service,v2x/modules/ossm-routing-destination-rules,v2x/modules/ossm-networkpolicy-overview,v2x/modules/ossm-config-disable-networkpolicy,v2x/modules/ossm-routing-sidecar,v2x/modules/ossm-routing-bookinfo-example,v2x/modules/ossm-routing-bookinfo-applying,v2x/modules/ossm-routing-bookinfo-test,v2x/modules/ossm-routing-bookinfo-route,v2x/ossm-observability,v2x/modules/ossm-observability-addresses,v2x/modules/ossm-kiali-accessing-console,v2x/modules/ossm-observability-visual,v2x/modules/ossm-kiali-viewing-logs,v2x/modules/ossm-kiali-viewing-metrics,v2x/modules/ossm-distr-tracing,v2x/modules/ossm-config-external-jaeger,v2x/modules/ossm-config-sampling,v2x/modules/ossm-jaeger-accessing-console,v2x/modules/ossm-access-grafana,v2x/modules/ossm-access-prometheus,v2x/modules/ossm-integrating-with-user-workload-monitoring,v2x/ossm-performance-scalability,v2x/modules/ossm-recommended-resources,v2x/modules/ossm-load-test-results,v2x/ossm-deploy-production,v2x/modules/ossm-smcp-prod,v2x/ossm-federation,v2x/modules/ossm-federation-overview,v2x/modules/ossm-federation-features,v2x/modules/ossm-federation-security,v2x/modules/ossm-federation-limitations,v2x/modules/ossm-federation-prerequisites,v2x/modules/ossm-federation-planning,v2x/modules/ossm-federation-across-cluster,v2x/modules/ossm-federation-checklist,v2x/modules/ossm-federation-config-smcp,v2x/modules/ossm-federation-config-meshPeer,v2x/modules/ossm-federation-create-meshPeer,v2x/modules/ossm-federation-config-export,v2x/modules/ossm-federation-create-export,v2x/modules/ossm-federation-config-import,v2x/modules/ossm-federation-create-import,v2x/modules/ossm-federation-config-failover-overview,v2x/modules/ossm-federation-config-importedserviceset-failover,v2x/modules/ossm-federation-config-destinationrule-failover,v2x/modules/ossm-federation-remove-service,v2x/modules/ossm-federation-remove-mesh,v2x/ossm-extensions,v2x/modules/ossm-extensions-overview,v2x/modules/ossm-extensions-wasmplugin-format,v2x/modules/ossm-extensions-ref-wasmplugin,v2x/modules/ossm-extensions-wasmplugin-deploy,v2x/modules/ossm-extensions-smextension-format,v2x/modules/ossm-extensions-ref-smextension,v2x/modules/ossm-extensions-smextension-deploy,v2x/modules/ossm-extensions-migration-overview,v2x/modules/ossm-extensions-migrating-to-wasmplugin,v2x/ossm-threescale-webassembly-module,v2x/modules/ossm-configuring-the-threescale-wasm-auth-module,v2x/modules/ossm-threescale-applying-external-service-entry-objects,v2x/modules/ossm-threescale-webassembly-module-configuration,v2x/modules/ossm-threescale-configuring-the-threescale-webassembly-module,v2x/modules/ossm-threescale-webassembly-module-api-object,v2x/modules/ossm-threescale-webassembly-module-system-object,v2x/modules/ossm-threescale-webassembly-module-upstream-object,v2x/modules/ossm-threescale-webassembly-module-backend-object,v2x/modules/ossm-threescale-webassembly-module-services-object,v2x/modules/ossm-threescale-webassembly-module-credentials-object,v2x/modules/ossm-threescale-webassembly-module-lookup-queries,v2x/modules/ossm-threescale-webassembly-module-source-object,v2x/modules/ossm-threescale-webassembly-module-operations-object,v2x/modules/ossm-threescale-webassembly-module-mapping-rules-object,v2x/modules/ossm-threescale-webassembly-module-mapping-rule-object,v2x/modules/ossm-threescale-webassembly-module-examples-for-credentials-use-cases,v2x/modules/ossm-threescale-webassembly-module-minimal-working-configuration,v2x/threescale-adapter,v2x/modules/ossm-threescale-integrate,v2x/modules/ossm-threescale-cr,v2x/modules/ossm-threescale-templates,v2x/modules/ossm-threescale-manifests,v2x/modules/ossm-threescale-routing,v2x/modules/ossm-threescale-integration-settings,v2x/modules/ossm-threescale-caching,v2x/modules/ossm-threescale-authentication,v2x/modules/ossm-threescale-metrics,v2x/modules/threescale-backend-cache,v2x/modules/threescale-istio-adapter-apicast,v2x/modules/ossm-threescale-istio-adapter-verification,v2x/modules/ossm-threescale-istio-adapter-troubleshooting-checklist,v2x/ossm-troubleshooting-istio,v2x/modules/ossm-validating-operators,v2x/modules/ossm-troubleshooting-operators,v2x/modules/ossm-validating-smcp,v2x/modules/ossm-troubleshooting-smcp,v2x/modules/ossm-troubleshooting-injection,v2x/modules/ossm-troubleshooting-proxy,v2x/modules/support,v2x/modules/support-knowledgebase-about,v2x/modules/support-knowledgebase-search,v2x/modules/ossm-about-collecting-ossm-data,v2x/modules/support-submitting-a-case,v2x/ossm-reference-smcp,v2x/modules/ossm-cr-example,v2x/modules/ossm-cr-general,v2x/modules/ossm-cr-profiles,v2x/modules/ossm-cr-techPreview,v2x/modules/ossm-cr-tracing,v2x/modules/ossm-cr-version,v2x/modules/ossm-cr-threescale,v2x/modules/ossm-cr-status,v2x/ossm-reference-kiali,v2x/modules/ossm-config-smcp-kiali,v2x/modules/ossm-configuring-external-kiali,v2x/ossm-reference-jaeger,v2x/modules/ossm-enabling-jaeger,v2x/modules/ossm-config-smcp-jaeger,v2x/modules/ossm-deploying-jaeger,v2x/modules/ossm-configuring-external-jaeger,v2x/modules/distr-tracing-deployment-best-practices,v2x/modules/distr-tracing-config-security-ossm,v2x/modules/distr-tracing-config-security-ossm-web,v2x/modules/distr-tracing-config-security-ossm-cli,v2x/modules/distr-tracing-config-default,v2x/modules/distr-tracing-config-jaeger-collector,v2x/modules/distr-tracing-config-sampling,v2x/modules/distr-tracing-config-storage,v2x/modules/distr-tracing-config-query,v2x/modules/distr-tracing-config-ingester,v2x/removing-ossm,v2x/modules/ossm-control-plane-remove,v2x/modules/ossm-remove-operators,v2x/modules/ossm-remove-cleanup,v1x/servicemesh-release-notes,v1x/_attributes/common-attributes,v1x/snippets/ossm-out-of-support,v1x/modules/ossm-servicemesh-overview,v1x/modules/support,v1x/modules/about-must-gather,v1x/modules/ossm-about-collecting-ossm-data,v1x/modules/ossm-supported-configurations-v1x,v1x/modules/ossm-rn-new-features-1x,v1x/modules/ossm-rn-deprecated-features-1x,v1x/modules/ossm-rn-known-issues-1x,v1x/modules/ossm-rn-fixed-issues-1x,v1x/ossm-architecture,v1x/modules/ossm-understanding-service-mesh,v1x/modules/ossm-architecture-1x,v1x/modules/ossm-kiali-overview,v1x/modules/ossm-kiali-architecture,v1x/modules/ossm-kiali-features,v1x/modules/distr-tracing-product-overview,v1x/modules/jaeger-architecture,v1x/modules/distr-tracing-features,v1x/ossm-vs-community,v1x/modules/ossm-multitenant,v1x/modules/ossm-vs-istio-1x,v1x/modules/ossm-kiali-service-mesh,v1x/modules/ossm-jaeger-service-mesh,v1x/preparing-ossm-installation,v1x/modules/ossm-installation-activities,v1x/installing-ossm,v1x/modules/distr-tracing-install-elasticsearch,v1x/modules/distr-tracing-install-jaeger-operator,v1x/modules/ossm-install-kiali,v1x/modules/ossm-install-ossm-operator,v1x/modules/ossm-control-plane-deploy-1x,v1x/modules/ossm-member-roll-create,v1x/modules/ossm-member-roll-modify,v1x/modules/ossm-update-app-sidecar,v1x/ossm-security,v1x/modules/ossm-security-mtls-1x,v1x/modules/ossm-security-cipher-1x,v1x/modules/ossm-security-cert-manage-1x,v1x/ossm-traffic-manage,v1x/modules/ossm-gateways,v1x/modules/ossm-routing-gateways,v1x/modules/ossm-routing-ingress,v1x/modules/ossm-auto-route-1x,v1x/modules/ossm-routing-service-entries,v1x/modules/ossm-routing-virtual-service,v1x/modules/ossm-routing-destination-rules,v1x/modules/ossm-routing-bookinfo-example,v1x/modules/ossm-routing-bookinfo-applying,v1x/modules/ossm-routing-bookinfo-test,v1x/modules/ossm-routing-bookinfo-route,v1x/prepare-to-deploy-applications-ossm,v1x/modules/ossm-control-plane-templates-1x,v1x/modules/ossm-automatic-sidecar-injection,v1x/modules/ossm-sidecar-injection-env-var,v1x/modules/ossm-mixer-policy-1x,v1x/modules/ossm-config-network-policy,v1x/modules/ossm-tutorial-bookinfo-overview,v1x/modules/ossm-tutorial-bookinfo-install,v1x/modules/ossm-tutorial-bookinfo-adding-destination-rules,v1x/modules/ossm-tutorial-bookinfo-verify-install,v1x/modules/ossm-tutorial-bookinfo-removing,v1x/modules/ossm-tutorial-jaeger-generating-traces,v1x/ossm-observability,v1x/modules/ossm-observability-access,v1x/modules/ossm-observability-visual,v1x/ossm-custom-resources,v1x/modules/ossm-cr-example-1x,v1x/modules/ossm-cr-parameters,v1x/modules/ossm-cr-istio-global,v1x/modules/ossm-cr-gateway,v1x/modules/ossm-cr-mixer,v1x/modules/ossm-cr-pilot,v1x/modules/ossm-configuring-kiali-v1x,v1x/modules/ossm-configuring-jaeger-v1x,v1x/modules/ossm-configuring-jaeger-existing-v1x,v1x/modules/ossm-jaeger-config-elasticsearch-v1x,v1x/modules/ossm-jaeger-config-es-cleaner-v1x,v1x/modules/ossm-cr-threescale,v1x/threescale-adapter,v1x/modules/ossm-threescale-integrate-1x,v1x/modules/ossm-threescale-cr,v1x/modules/ossm-threescale-templates,v1x/modules/ossm-threescale-manifests,v1x/modules/ossm-threescale-routing,v1x/modules/ossm-threescale-integration-settings,v1x/modules/ossm-threescale-caching,v1x/modules/ossm-threescale-authentication,v1x/modules/ossm-threescale-metrics-1x,v1x/modules/ossm-threescale-istio-adapter-verification,v1x/modules/ossm-threescale-istio-adapter-troubleshooting-checklist,v1x/removing-ossm,v1x/modules/ossm-control-plane-remove,v1x/modules/ossm-remove-operators,v1x/modules/ossm-remove-cleanup-1x

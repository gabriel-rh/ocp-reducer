<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" version="5.0">
<info>
<title>Virtualization</title>
<date>2024-02-23</date>
<title>Virtualization</title>
<productname>OpenShift Container Platform</productname>
<productnumber>4.14</productnumber>
<subtitle>Enter a short description here.</subtitle>
<abstract>
    <para>A short overview and summary of the book's subject and purpose, traditionally no more than one paragraph long.</para>
</abstract>
<authorgroup>
    <orgname>Red Hat OpenShift Documentation Team</orgname>
</authorgroup>
<xi:include href="Common_Content/Legal_Notice.xml" xmlns:xi="http://www.w3.org/2001/XInclude" />
</info>
<chapter xml:id="_about">
<title>About</title>
<section xml:id="about-virt">
<title>About OpenShift Virtualization</title>

<simpara>Learn about OpenShift Virtualization&#8217;s capabilities and support scope.</simpara>
<section xml:id="virt-what-you-can-do-with-virt_about-virt">
<title>What you can do with OpenShift Virtualization</title>
<simpara>OpenShift Virtualization is an add-on to OpenShift Container Platform that allows you to run and manage virtual machine workloads alongside container workloads.</simpara>
<simpara>OpenShift Virtualization adds new objects into your OpenShift Container Platform cluster by using Kubernetes custom resources to enable virtualization tasks. These tasks include:</simpara>
<itemizedlist>
<listitem>
<simpara>Creating and managing Linux and Windows virtual machines (VMs)</simpara>
</listitem>
<listitem>
<simpara>Running pod and VM workloads alongside each other in a cluster</simpara>
</listitem>
<listitem>
<simpara>Connecting to virtual machines through a variety of consoles and CLI tools</simpara>
</listitem>
<listitem>
<simpara>Importing and cloning existing virtual machines</simpara>
</listitem>
<listitem>
<simpara>Managing network interface controllers and storage disks attached to virtual machines</simpara>
</listitem>
<listitem>
<simpara>Live migrating virtual machines between nodes</simpara>
</listitem>
</itemizedlist>
<simpara>An enhanced web console provides a graphical portal to manage these virtualized resources alongside the OpenShift Container Platform cluster containers and infrastructure.</simpara>
<simpara>OpenShift Virtualization is designed and tested to work well with Red Hat OpenShift Data Foundation features.</simpara>
<important>
<simpara>When you deploy OpenShift Virtualization with OpenShift Data Foundation, you must create a dedicated storage class for Windows virtual machine disks. See <link xlink:href="https://access.redhat.com/articles/6978371">Optimizing ODF PersistentVolumes for Windows VMs</link> for details.</simpara>
</important>
<simpara>You can use OpenShift Virtualization with <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#about-ovn-kubernetes">OVN-Kubernetes</link>, <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#about-openshift-sdn">OpenShift SDN</link>, or one of the other certified network plugins listed in <link xlink:href="https://access.redhat.com/articles/5436171">Certified OpenShift CNI Plug-ins</link>.</simpara>
<simpara>You can check your OpenShift Virtualization cluster for compliance issues by installing the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#understanding-compliance">Compliance Operator</link> and running a scan with the <literal>ocp4-moderate</literal> and <literal>ocp4-moderate-node</literal> <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/security_and_compliance/#compliance-operator-supported-profiles">profiles</link>. The Compliance Operator uses OpenSCAP, a <link xlink:href="https://www.nist.gov/">NIST-certified tool</link>, to scan and enforce security policies.</simpara>
<section xml:id="virt-supported-cluster-version_about-virt">
<title>OpenShift Virtualization supported cluster version</title>
<simpara>OpenShift Virtualization 4.15 is supported for use on OpenShift Container Platform 4.14 clusters. To use the latest z-stream release of OpenShift Virtualization, you must first upgrade to the latest version of OpenShift Container Platform.</simpara>
</section>
</section>
<section xml:id="virt-about-storage-volumes-for-vm-disks_about-virt">
<title>About volume and access modes for virtual machine disks</title>
<simpara>If you use the storage API with known storage providers, the volume and access modes are selected automatically. However, if you use a storage class that does not have a storage profile, you must configure the volume and access mode.</simpara>
<simpara>For best results, use the <literal>ReadWriteMany</literal> (RWX) access mode and the <literal>Block</literal> volume mode. This is important for the following reasons:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>ReadWriteMany</literal> (RWX) access mode is required for live migration.</simpara>
</listitem>
<listitem>
<simpara>The <literal>Block</literal> volume mode performs significantly better than the <literal>Filesystem</literal> volume mode. This is because the <literal>Filesystem</literal> volume mode uses more storage layers, including a file system layer and a disk image file. These layers are not necessary for VM disk storage.</simpara>
<simpara>For example, if you use Red Hat OpenShift Data Foundation, Ceph RBD volumes are preferable to CephFS volumes.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>You cannot live migrate virtual machines with the following configurations:</simpara>
<itemizedlist>
<listitem>
<simpara>Storage volume with <literal>ReadWriteOnce</literal> (RWO) access mode</simpara>
</listitem>
<listitem>
<simpara>Passthrough features such as GPUs</simpara>
</listitem>
</itemizedlist>
<simpara>Do not set the <literal>evictionStrategy</literal> field to <literal>LiveMigrate</literal> for these virtual machines.</simpara>
</important>
</section>
<section xml:id="virt-sno-differences_about-virt">
<title>Single-node OpenShift differences</title>
<simpara>You can install OpenShift Virtualization on single-node OpenShift.</simpara>
<simpara>However, you should be aware that Single-node OpenShift does not support the following features:</simpara>
<itemizedlist>
<listitem>
<simpara>High availability</simpara>
</listitem>
<listitem>
<simpara>Pod disruption</simpara>
</listitem>
<listitem>
<simpara>Live migration</simpara>
</listitem>
<listitem>
<simpara>Virtual machines or templates that have an eviction strategy configured</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional-resources_about-virt" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#openshift-storage-common-terms_storage-overview">Glossary of common terms for OpenShift Container Platform storage</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#install-sno-about-installing-on-a-single-node_install-sno-preparing">About single-node OpenShift</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://cloud.redhat.com/blog/using-the-openshift-assisted-installer-service-to-deploy-an-openshift-cluster-on-metal-and-vsphere">Assisted installer</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#priority-preemption-other_nodes-pods-priority">Pod disruption budgets</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-about-live-migration">About live migration</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="eviction-strategies">Eviction strategies</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/articles/6994974">Tuning &amp; Scaling Guide</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-security-policies">
<title>Security policies</title>

<simpara>Learn about OpenShift Virtualization security and authorization.</simpara>
<itemizedlist>
<title>Key points</title>
<listitem>
<simpara>OpenShift Virtualization adheres to the <literal>restricted</literal> <link xlink:href="https://kubernetes.io/docs/concepts/security/pod-security-standards/#restricted">Kubernetes pod security standards</link> profile, which aims to enforce the current best practices for pod security.</simpara>
</listitem>
<listitem>
<simpara>Virtual machine (VM) workloads run as unprivileged pods.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#security-context-constraints-about_configuring-internal-oauth">Security context constraints</link> (SCCs) are defined for the <literal>kubevirt-controller</literal> service account.</simpara>
</listitem>
<listitem>
<simpara>TLS certificates for OpenShift Virtualization components are renewed and rotated automatically.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-about-workload-security_virt-security-policies">
<title>About workload security</title>
<simpara>By default, virtual machine (VM) workloads do not run with root privileges in OpenShift Virtualization, and there are no supported OpenShift Virtualization features that require root privileges.</simpara>
<simpara>For each VM, a <literal>virt-launcher</literal> pod runs an instance of <literal>libvirt</literal> in <emphasis>session mode</emphasis> to manage the VM process. In session mode, the <literal>libvirt</literal> daemon runs as a non-root user account and only permits connections from clients that are running under the same user identifier (UID). Therefore, VMs run as unprivileged pods, adhering to the security principle of least privilege.</simpara>
</section>
<section xml:id="virt-automatic-certificates-renewal_virt-security-policies">
<title>TLS certificates</title>
<simpara>TLS certificates for OpenShift Virtualization components are renewed and rotated automatically. You are not required to refresh them manually.</simpara>
<formalpara>
<title>Automatic renewal schedules</title>
<para>TLS certificates are automatically deleted and replaced according to the following schedule:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara>KubeVirt certificates are renewed daily.</simpara>
</listitem>
<listitem>
<simpara>Containerized Data Importer controller (CDI)
certificates are renewed every 15 days.</simpara>
</listitem>
<listitem>
<simpara>MAC pool certificates are renewed every year.</simpara>
</listitem>
</itemizedlist>
<simpara>Automatic TLS certificate rotation does not disrupt any operations. For example, the following operations continue to function without any disruption:</simpara>
<itemizedlist>
<listitem>
<simpara>Migrations</simpara>
</listitem>
<listitem>
<simpara>Image uploads</simpara>
</listitem>
<listitem>
<simpara>VNC and console connections</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="authorization_virt-security-policies">
<title>Authorization</title>
<simpara>OpenShift Virtualization uses <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#using-rbac">role-based access control</link> (RBAC) to define permissions for human users and service accounts. The permissions defined for service accounts control the actions that OpenShift Virtualization components can perform.</simpara>
<simpara>You can also use RBAC roles to manage user access to virtualization features. For example, an administrator can create an RBAC role that provides the permissions required to launch a virtual machine. The administrator can then restrict access by binding the role to specific users.</simpara>
<section xml:id="default-cluster-roles-for-virt_virt-security-policies">
<title>Default cluster roles for OpenShift Virtualization</title>
<simpara>By using cluster role aggregation, OpenShift Virtualization extends the default OpenShift Container Platform cluster roles to include permissions for accessing virtualization objects.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>OpenShift Virtualization cluster roles</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="16.6666*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="66.6668*"/>
<thead>
<row>
<entry align="left" valign="top">Default cluster role</entry>
<entry align="left" valign="top">OpenShift Virtualization cluster role</entry>
<entry align="left" valign="top">OpenShift Virtualization cluster role description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>view</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>kubevirt.io:view</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can view all OpenShift Virtualization resources in the cluster but cannot create, delete, modify, or access them. For example, the user can see that a virtual machine (VM) is running but cannot shut it down or gain access to its console.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>edit</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>kubevirt.io:edit</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that can modify all OpenShift Virtualization resources in the cluster. For example, the user can create VMs, access VM consoles, and delete VMs.</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>admin</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>kubevirt.io:admin</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A user that has full permissions to all OpenShift Virtualization resources, including the ability to delete collections of resources. The user can also view and modify the OpenShift Virtualization runtime configuration, which is located in the <literal>HyperConverged</literal> custom resource in the <literal>openshift-cnv</literal> namespace.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-storage-rbac-roles_virt-security-policies">
<title>RBAC roles for storage features in OpenShift Virtualization</title>
<simpara>The following permissions are granted to the Containerized Data Importer (CDI), including the <literal>cdi-operator</literal> and <literal>cdi-controller</literal> service accounts.</simpara>
<section xml:id="cluster-wide-rbac-roles-cdi">
<title>Cluster-wide RBAC roles</title>
<table frame="all" rowsep="1" colsep="1">
<title>Aggregated cluster roles for the <literal>cdi.kubevirt.io</literal> API group</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="50*"/>
<colspec colname="col_3" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">CDI cluster role</entry>
<entry align="left" valign="top">Resources</entry>
<entry align="left" valign="top">Verbs</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle" morerows="1"><simpara><literal>cdi.kubevirt.io:admin</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>datavolumes</literal>, <literal>uploadtokenrequests</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal> (all)</simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>datavolumes/source</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle" morerows="1"><simpara><literal>cdi.kubevirt.io:edit</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>datavolumes</literal>, <literal>uploadtokenrequests</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>datavolumes/source</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle" morerows="1"><simpara><literal>cdi.kubevirt.io:view</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>cdiconfigs</literal>, <literal>dataimportcrons</literal>, <literal>datasources</literal>, <literal>datavolumes</literal>, <literal>objecttransfers</literal>, <literal>storageprofiles</literal>, <literal>volumeimportsources</literal>, <literal>volumeuploadsources</literal>, <literal>volumeclonesources</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>datavolumes/source</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>cdi.kubevirt.io:config-reader</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>cdiconfigs</literal>, <literal>storageprofiles</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster-wide roles for the <literal>cdi-operator</literal> service account</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">API group</entry>
<entry align="left" valign="top">Resources</entry>
<entry align="left" valign="top">Verbs</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>rbac.authorization.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>clusterrolebindings</literal>, <literal>clusterroles</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>security.openshift.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>securitycontextconstraints</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>update</literal>, <literal>create</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>apiextensions.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>customresourcedefinitions</literal>, <literal>customresourcedefinitions/status</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>cdi.kubevirt.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>upload.cdi.kubevirt.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>admissionregistration.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>validatingwebhookconfigurations</literal>, <literal>mutatingwebhookconfigurations</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>admissionregistration.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>validatingwebhookconfigurations</literal></simpara><simpara>Allow list: <literal>cdi-api-dataimportcron-validate, cdi-api-populator-validate, cdi-api-datavolume-validate, cdi-api-validate, objecttransfer-api-validate</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>admissionregistration.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>mutatingwebhookconfigurations</literal></simpara><simpara>Allow list: <literal>cdi-api-datavolume-mutate</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>apiregistration.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>apiservices</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster-wide roles for the <literal>cdi-controller</literal> service account</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">API group</entry>
<entry align="left" valign="top">Resources</entry>
<entry align="left" valign="top">Verbs</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>events</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal>, <literal>patch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>persistentvolumeclaims</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal>, <literal>deletecollection</literal>, <literal>patch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>persistentvolumes</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>update</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>persistentvolumeclaims/finalizers</literal>, <literal>pods/finalizers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>update</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>pods</literal>, <literal>services</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>configmaps</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>create</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>storage.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>storageclasses</literal>, <literal>csidrivers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>config.openshift.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>proxies</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>cdi.kubevirt.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>*</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>snapshot.storage.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>volumesnapshots</literal>, <literal>volumesnapshotclasses</literal>, <literal>volumesnapshotcontents</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>snapshot.storage.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>volumesnapshots</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>update</literal>, <literal>deletecollection</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>apiextensions.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>customresourcedefinitions</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>scheduling.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>priorityclasses</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>image.openshift.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>imagestreams</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>secrets</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>kubevirt.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>virtualmachines/finalizers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>update</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="namespaced-rbac-roles-cdi">
<title>Namespaced RBAC roles</title>
<table frame="all" rowsep="1" colsep="1">
<title>Namespaced roles for the <literal>cdi-operator</literal> service account</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">API group</entry>
<entry align="left" valign="top">Resources</entry>
<entry align="left" valign="top">Verbs</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>rbac.authorization.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>rolebindings</literal>, <literal>roles</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>serviceaccounts</literal>, <literal>configmaps</literal>, <literal>events</literal>, <literal>secrets</literal>, <literal>services</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>patch</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>apps</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>deployments</literal>, <literal>deployments/finalizers</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>route.openshift.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>routes</literal>, <literal>routes/custom-host</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>config.openshift.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>proxies</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>monitoring.coreos.com</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>servicemonitors</literal>, <literal>prometheusrules</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>delete</literal>, <literal>update</literal>, <literal>patch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>coordination.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>leases</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>create</literal>, <literal>update</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>Namespaced roles for the <literal>cdi-controller</literal> service account</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">API group</entry>
<entry align="left" valign="top">Resources</entry>
<entry align="left" valign="top">Verbs</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>configmaps</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>""</literal> (core)</simpara></entry>
<entry align="left" valign="middle"><simpara><literal>secrets</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>batch</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>cronjobs</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal>, <literal>create</literal>, <literal>update</literal>, <literal>delete</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>batch</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>jobs</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>create</literal>, <literal>delete</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>coordination.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>leases</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>create</literal>, <literal>update</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>networking.k8s.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>ingresses</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="middle"><simpara><literal>route.openshift.io</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>routes</literal></simpara></entry>
<entry align="left" valign="middle"><simpara><literal>get</literal>, <literal>list</literal>, <literal>watch</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="virt-additional-scc-for-kubevirt-controller_virt-security-policies">
<title>Additional SCCs and permissions for the kubevirt-controller service account</title>
<simpara>Security context constraints (SCCs) control permissions for pods. These permissions include actions that a pod, a collection of containers, can perform and what resources it can access. You can use SCCs to define a set of conditions that a pod must run with to be accepted into the system.</simpara>
<simpara>The <literal>virt-controller</literal> is a cluster controller that creates the <literal>virt-launcher</literal> pods for virtual machines in the cluster. These pods are granted permissions by the <literal>kubevirt-controller</literal> service account.</simpara>
<simpara>The <literal>kubevirt-controller</literal> service account is granted additional SCCs and Linux capabilities so that it can create <literal>virt-launcher</literal> pods with the appropriate permissions. These extended permissions allow virtual machines to use OpenShift Virtualization features that are beyond the scope of typical pods.</simpara>
<simpara>The <literal>kubevirt-controller</literal> service account is granted the following SCCs:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>scc.AllowHostDirVolumePlugin = true</literal><?asciidoc-br?>
This allows virtual machines to use the hostpath volume plugin.</simpara>
</listitem>
<listitem>
<simpara><literal>scc.AllowPrivilegedContainer = false</literal><?asciidoc-br?>
This ensures the virt-launcher pod is not run as a privileged container.</simpara>
</listitem>
<listitem>
<simpara><literal>scc.AllowedCapabilities = []corev1.Capability{"SYS_NICE", "NET_BIND_SERVICE"}</literal></simpara>
<itemizedlist>
<listitem>
<simpara><literal>SYS_NICE</literal> allows setting the CPU affinity.</simpara>
</listitem>
<listitem>
<simpara><literal>NET_BIND_SERVICE</literal> allows DHCP and Slirp operations.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<formalpara>
<title>Viewing the SCC and RBAC definitions for the kubevirt-controller</title>
<para>You can view the <literal>SecurityContextConstraints</literal> definition for the <literal>kubevirt-controller</literal> by using the <literal>oc</literal> tool:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get scc kubevirt-controller -o yaml</programlisting>
<simpara>You can view the RBAC definition for the <literal>kubevirt-controller</literal> clusterrole by using the <literal>oc</literal> tool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterrole kubevirt-controller -o yaml</programlisting>
</section>
</section>
<section xml:id="additional-resources_virt-security-policies" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#security-context-constraints-about_configuring-internal-oauth">Managing security context constraints</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#using-rbac">Using RBAC to define and apply permissions</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#creating-cluster-role_using-rbac">Creating a cluster role</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#cluster-role-binding-commands_using-rbac">Cluster role binding commands</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-enabling-user-permissions-to-clone-datavolumes">Enabling user permissions to clone data volumes across namespaces</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-architecture">
<title>OpenShift Virtualization Architecture</title>

<simpara>The Operator Lifecycle Manager (OLM) deploys operator pods for each component of OpenShift Virtualization:</simpara>
<itemizedlist>
<listitem>
<simpara>Compute: <literal>virt-operator</literal></simpara>
</listitem>
<listitem>
<simpara>Storage: <literal>cdi-operator</literal></simpara>
</listitem>
<listitem>
<simpara>Network: <literal>cluster-network-addons-operator</literal></simpara>
</listitem>
<listitem>
<simpara>Scaling: <literal>ssp-operator</literal></simpara>
</listitem>
<listitem>
<simpara>Templating: <literal>tekton-tasks-operator</literal></simpara>
</listitem>
</itemizedlist>
<simpara>OLM also deploys the <literal>hyperconverged-cluster-operator</literal> pod, which is responsible for the deployment, configuration, and life cycle of other components, and several helper pods: <literal>hco-webhook</literal>, and <literal>hyperconverged-cluster-cli-download</literal>.</simpara>
<simpara>After all operator pods are successfully deployed, you should create the <literal>HyperConverged</literal> custom resource (CR). The configurations set in the <literal>HyperConverged</literal> CR serve as the single source of truth and the entrypoint for OpenShift Virtualization, and guide the behavior of the CRs.</simpara>
<simpara>The <literal>HyperConverged</literal> CR creates corresponding CRs for the operators of all other components within its reconciliation loop. Each operator then creates resources such as daemon sets, config maps, and additional components for the OpenShift Virtualization control plane. For example, when the HyperConverged Operator (HCO) creates the <literal>KubeVirt</literal> CR, the OpenShift Virtualization Operator reconciles it and creates additional resources such as <literal>virt-controller</literal>, <literal>virt-handler</literal>, and <literal>virt-api</literal>.</simpara>
<simpara>The OLM deploys the Hostpath Provisioner (HPP) Operator, but it is not functional until you create a <literal>hostpath-provisioner</literal> CR.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_main.png"/>
</imageobject>
<textobject><phrase>Deployments</phrase></textobject>
</mediaobject>
</informalfigure>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-virtctl-commands_virt-using-the-cli-tools">Virtctl client commands</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-about-hco-operator_virt-architecture">
<title>About the HyperConverged Operator (HCO)</title>
<simpara>The HCO, <literal>hco-operator</literal>, provides a single entry point for deploying and managing OpenShift Virtualization and several helper operators with opinionated defaults. It also creates custom resources (CRs) for those operators.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_hco-operator.png"/>
</imageobject>
<textobject><phrase>hco-operator components</phrase></textobject>
</mediaobject>
</informalfigure>
<table frame="all" rowsep="1" colsep="1">
<title>HyperConverged Operator components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Component</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Description</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/hco-webhook</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Validates the <literal>HyperConverged</literal> custom resource contents.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/hyperconverged-cluster-cli-download</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides the <literal>virtctl</literal> tool binaries to the cluster so that you can download them directly from the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>KubeVirt/kubevirt-kubevirt-hyperconverged</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Contains all operators, CRs, and objects needed by OpenShift Virtualization.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>SSP/ssp-kubevirt-hyperconverged</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A Scheduling, Scale, and Performance (SSP) CR. This is automatically created by the HCO.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>CDI/cdi-kubevirt-hyperconverged</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A Containerized Data Importer (CDI) CR. This is automatically created by the HCO.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>NetworkAddonsConfig/cluster</literal></simpara></entry>
<entry align="left" valign="top"><simpara>A CR that instructs and is managed by the <literal>cluster-network-addons-operator</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-about-cdi-operator_virt-architecture">
<title>About the Containerized Data Importer (CDI) Operator</title>
<simpara>The CDI Operator, <literal>cdi-operator</literal>, manages CDI and its related resources, which imports a virtual machine (VM) image into a persistent volume claim (PVC) by using a data volume.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_cdi-operator.png"/>
</imageobject>
<textobject><phrase>cdi-operator components</phrase></textobject>
</mediaobject>
</informalfigure>
<table frame="all" rowsep="1" colsep="1">
<title>CDI Operator components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Component</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Description</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/cdi-apiserver</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Manages the authorization to upload VM disks into PVCs by issuing secure upload tokens.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/cdi-uploadproxy</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Directs external disk upload traffic to the appropriate upload server pod so that it can be written to the correct PVC. Requires a valid upload token.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pod/cdi-importer</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Helper pod that imports a virtual machine image into a PVC when creating a data volume.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-about-cluster-network-addons-operator_virt-architecture">
<title>About the Cluster Network Addons Operator</title>
<simpara>The Cluster Network Addons Operator, <literal>cluster-network-addons-operator</literal>, deploys networking components on a cluster and manages the related resources for extended network functionality.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_cluster-network-addons-operator.png"/>
</imageobject>
<textobject><phrase>cluster-network-addons-operator components</phrase></textobject>
</mediaobject>
</informalfigure>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster Network Addons Operator components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Component</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Description</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/kubemacpool-cert-manager</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Manages TLS certificates of Kubemacpoolâ€™s webhooks.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/kubemacpool-mac-controller-manager</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides a MAC address pooling service for virtual machine (VM) network interface cards (NICs).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>daemonset/bridge-marker</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Marks network bridges available on nodes as node resources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>daemonset/kube-cni-linux-bridge-plugin</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Installs Container Network Interface (CNI) plugins on cluster nodes, enabling the attachment of VMs to Linux bridges through network attachment definitions.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-about-hpp-operator_virt-architecture">
<title>About the Hostpath Provisioner (HPP) Operator</title>
<simpara>The HPP Operator, <literal>hostpath-provisioner-operator</literal>, deploys and manages the multi-node HPP and related resources.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_hpp-operator.png"/>
</imageobject>
<textobject><phrase>hpp-operator components</phrase></textobject>
</mediaobject>
</informalfigure>
<table frame="all" rowsep="1" colsep="1">
<title>HPP Operator components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Component</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Description</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/hpp-pool-hpp-csi-pvc-block-&lt;worker_node_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides a worker for each node where the HPP is designated to run. The pods mount the specified backing storage on the node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>daemonset/hostpath-provisioner-csi</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Implements the Container Storage Interface (CSI) driver interface of the HPP.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>daemonset/hostpath-provisioner</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Implements the legacy driver interface of the HPP.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-about-ssp-operator_virt-architecture">
<title>About the Scheduling, Scale, and Performance (SSP) Operator</title>
<simpara>The SSP Operator, <literal>ssp-operator</literal>, deploys the common templates, the related default boot sources, the pipeline tasks, and the template validator.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_ssp-operator.png"/>
</imageobject>
<textobject><phrase>ssp-operator components</phrase></textobject>
</mediaobject>
</informalfigure>
<table frame="all" rowsep="1" colsep="1">
<title>SSP Operator components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Component</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Description</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/create-vm-from-template</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Creates a VM from a template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/copy-template</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Copies a VM template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/modify-vm-template</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Creates or removes a VM template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/modify-data-object</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Creates or removes data volumes or data sources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/cleanup-vm</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Runs a script or a command on a VM, then stops or deletes the VM afterward.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/disk-virt-customize</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Runs a <literal>customize</literal> script on a target persistent volume claim (PVC) using <literal>virt-customize</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/disk-virt-sysprep</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Runs a <literal>sysprep</literal> script on a target PVC by using <literal>virt-sysprep</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/wait-for-vmi-status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Waits for a specific virtual machine instance (VMI) status, then fails or succeeds according to that status.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/create-vm-from-manifest</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Creates a VM from a manifest.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-about-virt-operator_virt-architecture">
<title>About the OpenShift Virtualization Operator</title>
<simpara>The OpenShift Virtualization Operator, <literal>virt-operator</literal> deploys, upgrades, and manages OpenShift Virtualization without disrupting current virtual machine (VM) workloads.</simpara>
<informalfigure>
<mediaobject>
<imageobject>
<imagedata fileref="images/cnv_components_virt-operator.png"/>
</imageobject>
<textobject><phrase>virt-operator components</phrase></textobject>
</mediaobject>
</informalfigure>
<table frame="all" rowsep="1" colsep="1">
<title>virt-operator components</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top"><emphasis role="strong">Component</emphasis></entry>
<entry align="left" valign="top"><emphasis role="strong">Description</emphasis></entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/virt-api</literal></simpara></entry>
<entry align="left" valign="top"><simpara>HTTP API server that serves as the entry point for all virtualization-related flows.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>deployment/virt-controller</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Observes the creation of a new VM instance object and creates a corresponding pod. When the pod is scheduled on a node, <literal>virt-controller</literal> updates the VM with the node name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>daemonset/virt-handler</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Monitors any changes to a VM and instructs <literal>virt-launcher</literal> to perform the required operations. This component is node-specific.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>pod/virt-launcher</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Contains the VM that was created by the user as implemented by <literal>libvirt</literal> and <literal>qemu</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
</chapter>
<chapter xml:id="_release-notes">
<title>Release notes</title>
<section xml:id="virt-release-notes-placeholder">
<title>OpenShift Virtualization release notes placeholder</title>

<simpara>Do not add or edit release notes here. Edit release notes directly in the branch
that they are relevant for.</simpara>
<simpara>This file is here to allow builds to work.</simpara>
</section>
</chapter>
<chapter xml:id="_getting-started">
<title>Getting started</title>
<section xml:id="virt-getting-started">
<title>Getting started with OpenShift Virtualization</title>

<simpara>You can explore the features and functionalities of OpenShift Virtualization by installing and configuring a basic environment.</simpara>
<note>
<simpara>Cluster configuration procedures require <literal>cluster-admin</literal> privileges.</simpara>
</note>
<section xml:id="planning-and-installing-virt_virt-getting-started">
<title>Planning and installing OpenShift Virtualization</title>
<simpara>Plan and install OpenShift Virtualization on an OpenShift Container Platform cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#virt-planning-bare-metal-cluster-for-ocp-virt_preparing-to-install-on-bare-metal">Plan your bare metal cluster for OpenShift Virtualization</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="preparing-cluster-for-virt">Prepare your cluster for OpenShift Virtualization</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-installing-virt-operator_installing-virt">Install the OpenShift Virtualization Operator</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="installing-virtctl_virt-using-the-cli-tools">Install the <literal>virtctl</literal> command line interface (CLI) tool</link>.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="additional-resources_planning-and-installing" role="_additional-resources" renderas="sect4">Planning and installation resources</bridgehead>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-about-storage-volumes-for-vm-disks_preparing-cluster-for-virt">About storage volumes for virtual machine disks</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-csi">Using a CSI-enabled storage provider</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-configuring-local-storage-with-hpp">Configuring local storage for virtual machines</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#installing-the-kubernetes-nmstate-operator-cli">Installing the Kubernetes NMState Operator</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-specifying-nodes-for-vms">Specifying nodes for virtual machines</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-virtctl-commands_virt-using-the-cli-tools"><literal>Virtctl</literal> commands</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-and-managing-vms_virt-getting-started">
<title>Creating and managing virtual machines</title>
<simpara>Create a virtual machine (VM):</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-creating-vms-from-rh-images-overview">Create a VM from a Red Hat image</link>.</simpara>
<simpara>You can create a VM by using a Red Hat template or an instance type.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-creating-vms-from-custom-images-overview">Create a VM from a custom image</link>.</simpara>
<simpara>You can create a VM by importing a custom image from a container registry or a web page, by uploading an image from your local machine, or by cloning a persistent volume claim (PVC).</simpara>
</listitem>
</itemizedlist>
<simpara>Connect a VM to a secondary network:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-connecting-vm-to-linux-bridge">Linux bridge network</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-connecting-vm-to-ovn-secondary-network">Open Virtual Network (OVN)-Kubernetes secondary network</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-connecting-vm-to-sriov">Single Root I/O Virtualization (SR-IOV) network</link>.</simpara>
<note>
<simpara>VMs are connected to the pod network by default.</simpara>
</note>
</listitem>
</itemizedlist>
<simpara>Connect to a VM:</simpara>
<itemizedlist>
<listitem>
<simpara>Connect to the <link linkend="serial-console_virt-accessing-vm-consoles">serial console</link> or <link linkend="vnc-console_virt-accessing-vm-consoles">VNC console</link> of a VM.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-accessing-vm-ssh">Connect to a VM by using SSH</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="desktop-viewer_virt-accessing-vm-consoles">Connect to the desktop viewer for Windows VMs</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>Manage a VM:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-controlling-vm-states">Manage a VM by using the web console</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-virtctl-commands_virt-using-the-cli-tools">Manage a VM by using the <literal>virtctl</literal> CLI tool</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-accessing-exported-vm-manifests_virt-exporting-vms">Export a VM</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="next-steps_virt-getting-started">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-post-install-config">Review postinstallation configuration options</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-storage-config-overview">Configure storage options and automatic boot source updates</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-monitoring-overview">Learn about monitoring and health checks</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-about-live-migration">Learn about live migration</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-backup-restore-overview">Back up and restore VMs</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/articles/6994974">Tune and scale your cluster</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-using-the-cli-tools">
<title>Using the virtctl and libguestfs CLI tools</title>

<simpara>You can manage OpenShift Virtualization resources by using the <literal>virtctl</literal> command line tool.</simpara>
<simpara>You can access and modify virtual machine (VM) disk images by using the <link xlink:href="https://libguestfs.org"><literal>libguestfs</literal></link> command line tool. You deploy <literal>libguestfs</literal> by using the <literal>virtctl libguestfs</literal> command.</simpara>
<section xml:id="installing-virtctl_virt-using-the-cli-tools">
<title>Installing virtctl</title>
<simpara>To install <literal>virtctl</literal> on Red Hat Enterprise Linux (RHEL) 9 or later, Linux, Windows, and MacOS operating systems, you can download and install the <literal>virtctl</literal> binary file.</simpara>
<simpara>To install <literal>virtctl</literal> on RHEL 8, you can enable the OpenShift Virtualization repository and then install the <literal>kubevirt-virtctl</literal> RPM package.</simpara>
<section xml:id="virt-installing-virtctl-binary_virt-using-the-cli-tools">
<title>Installing the virtctl binary on RHEL 9 or later, Linux, Windows, or macOS</title>
<simpara>You can download the <literal>virtctl</literal> binary by using the OpenShift Container Platform web console and then install it on Red Hat Enterprise Linux (RHEL) 9 or later, Linux, Windows, or macOS.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Virtualization &#8594; Overview</emphasis> page in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Download virtctl</emphasis> link to download the <literal>virtctl</literal> binary for your operating system.</simpara>
</listitem>
<listitem>
<simpara>Install <literal>virtctl</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>For RHEL and other Linux operating systems:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Decompress the archive file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tar -xvf &lt;virtctl-version-distribution.arch&gt;.tar.gz</programlisting>
</listitem>
<listitem>
<simpara>Run the following command to make the <literal>virtctl</literal> binary executable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ chmod +x &lt;path/virtctl-file-name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Move the <literal>virtctl</literal> binary to a directory in your <literal>PATH</literal> environment variable.</simpara>
<simpara>You can check your path by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo $PATH</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>KUBECONFIG</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KUBECONFIG=/home/&lt;user&gt;/clusters/current/auth/kubeconfig</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For Windows:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Decompress the archive file.</simpara>
</listitem>
<listitem>
<simpara>Navigate the extracted folder hierarchy and double-click the <literal>virtctl</literal> executable file to install the client.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>virtctl</literal> binary to a directory in your <literal>PATH</literal> environment variable.</simpara>
<simpara>You can check your path by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">C:\&gt; path</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>For macOS:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Decompress the archive file.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>virtctl</literal> binary to a directory in your <literal>PATH</literal> environment variable.</simpara>
<simpara>You can check your path by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">echo $PATH</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-installing-virtctl-rhel8-rpm_virt-using-the-cli-tools">
<title>Installing the virtctl RPM package on RHEL 8</title>
<simpara>You can install the <literal>virtctl</literal> RPM package on Red Hat Enterprise Linux (RHEL) 8 by enabling the OpenShift Virtualization repository and then installing the <literal>kubevirt-virtctl</literal> RPM package.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Each host in your cluster must be registered with Red Hat Subscription Manager (RHSM) and have an active OpenShift Container Platform subscription.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable the OpenShift Virtualization repository by using the <literal>subscription-manager</literal> CLI tool to run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># subscription-manager repos --enable cnv-4.15-for-rhel-8-x86_64-rpms</programlisting>
</listitem>
<listitem>
<simpara>Install the <literal>kubevirt-virtctl</literal> RPM package by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum install kubevirt-virtctl</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-virtctl-commands_virt-using-the-cli-tools">
<title>virtctl commands</title>
<simpara>The <literal>virtctl</literal> client is a command-line utility for managing OpenShift Virtualization resources.</simpara>
<note>
<simpara>The virtual machine (VM) commands also apply to virtual machine instances (VMIs) unless otherwise specified.</simpara>
</note>
<section xml:id="virtctl-information-commands_virt-using-the-cli-tools">
<title>virtctl information commands</title>
<simpara>You use <literal>virtctl</literal> information commands to view information about the <literal>virtctl</literal> client.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Information commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl version</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View the <literal>virtctl</literal> client and server versions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View a list of <literal>virtctl</literal> commands.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl &lt;command&gt; -h|--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View a list of options for a specific command.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl options</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View a list of global command options for any <literal>virtctl</literal> command.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="vm-information-commands_virt-using-the-cli-tools">
<title>VM information commands</title>
<simpara>You can use <literal>virtctl</literal> to view information about virtual machines (VMs) and virtual machine instances (VMIs).</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>VM information commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl fslist &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View the file systems available on a guest machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl guestosinfo &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View information about the operating systems on a guest machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl userlist &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>View the logged-in users on a guest machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="vm-management-commands_virt-using-the-cli-tools">
<title>VM management commands</title>
<simpara>You use <literal>virtctl</literal> virtual machine (VM) management commands to manage and migrate virtual machines (VMs) and virtual machine instances (VMIs).</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>VM management commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl create -name &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a <literal>VirtualMachine</literal> manifest.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl start &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Start a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl start --paused &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Start a VM in a paused state. This option enables you to interrupt the boot process from the VNC console.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl stop &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Stop a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl stop &lt;vm_name&gt; --grace-period 0 --force</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Force stop a VM. This option might cause data inconsistency or data loss.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl pause vm &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Pause a VM. The machine state is kept in memory.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl unpause vm &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Unpause a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl migrate &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Migrate a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl migrate-cancel &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Cancel a VM migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl restart &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Restart a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl create instancetype --cpu &lt;cpu_value&gt; --memory &lt;memory_value&gt; --name &lt;instancetype_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create an <literal>InstanceType</literal> manifest for a <literal>ClusterInstanceType</literal>, or a namespaced <literal>InstanceType</literal>, to streamline the creation of your <literal>InstanceType</literal> specifications.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl create preference --name &lt;preference_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a <literal>Preference</literal> manifest for a <literal>ClusterPreference</literal>, or a namespaced <literal>Preference</literal>, to streamline the creation of your <literal>Preference</literal> specifications.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="vm-connection-commands_virt-using-the-cli-tools">
<title>VM connection commands</title>
<simpara>You use <literal>virtctl</literal> connection commands to expose ports and connect to virtual machines (VMs) and virtual machine instances (VMIs).</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>VM connection commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl console &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connect to the serial console of a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl expose vm &lt;vm_name&gt; --name &lt;service_name&gt; --type &lt;ClusterIP|NodePort|LoadBalancer&gt; --port &lt;port&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a service that forwards a designated port of a VM and expose the service on the specified port of the node.</simpara>
<simpara>Example: <literal>virtctl expose vm rhel9_vm --name rhel9-ssh --type NodePort --port 22</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl scp -i &lt;ssh_key&gt; &lt;file_name&gt; &lt;user_name&gt;@&lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Copy a file from your machine to a VM. This command uses the private key of an SSH key pair. The VM must be configured with the public key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl scp -i &lt;ssh_key&gt; &lt;user_name@&lt;vm_name&gt;:&lt;file_name&gt; .</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Copy a file from a VM to your machine. This command uses the private key of an SSH key pair. The VM must be configured with the public key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl ssh -i &lt;ssh_key&gt; &lt;user_name&gt;@&lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Open an SSH connection with a VM. This command uses the private key of an SSH key pair. The VM must be configured with the public key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vnc &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Connect to the VNC console of a VM.</simpara>
<simpara>You must have <literal>virt-viewer</literal> installed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vnc --proxy-only=true &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Display the port number and connect manually to a VM by using any viewer through the VNC connection.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vnc --port=&lt;port-number&gt; &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Specify a port number to run the proxy on the specified port, if that port is available.</simpara>
<simpara>If a port number is not specified, the proxy runs on a random port.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="vm-volume-export-commands_virt-using-the-cli-tools">
<title>VM export commands</title>
<simpara>Use <literal>virtctl vmexport</literal> commands to create, download, or delete a volume exported from a VM, VM snapshot, or persistent volume claim (PVC). Certain manifests also contain a header secret, which grants access to the endpoint to import a disk image in a format that OpenShift Virtualization can use.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>VM export commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport create &lt;vmexport_name&gt; --vm|snapshot|pvc=&lt;object_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a <literal>VirtualMachineExport</literal> custom resource (CR) to export a volume from a VM, VM snapshot, or PVC.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--vm</literal>: Exports the PVCs of a VM.</simpara>
</listitem>
<listitem>
<simpara><literal>--snapshot</literal>: Exports the PVCs contained in a <literal>VirtualMachineSnapshot</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>--pvc</literal>: Exports a PVC.</simpara>
</listitem>
<listitem>
<simpara>Optional: <literal>--ttl=1h</literal> specifies the time to live. The default duration is 2 hours.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport delete &lt;vmexport_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Delete a <literal>VirtualMachineExport</literal> CR manually.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download &lt;vmexport_name&gt; --output=&lt;output_file&gt; --volume=&lt;volume_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Download the volume defined in a <literal>VirtualMachineExport</literal> CR.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--output</literal> specifies the file format. Example: <literal>disk.img.gz</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>--volume</literal> specifies the volume to download. This flag is optional if only one volume is available.</simpara>
</listitem>
</itemizedlist>
<simpara>Optional:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--keep-vme</literal> retains the <literal>VirtualMachineExport</literal> CR after download. The default behavior is to delete the <literal>VirtualMachineExport</literal> CR after download.</simpara>
</listitem>
<listitem>
<simpara><literal>--insecure</literal> enables an insecure HTTP connection.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download &lt;vmexport_name&gt; --&lt;vm|snapshot|pvc&gt;=&lt;object_name&gt; --output=&lt;output_file&gt; --volume=&lt;volume_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a <literal>VirtualMachineExport</literal> CR and then download the volume defined in the CR.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download export --manifest</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Retrieve the manifest for an existing export. The manifest does not include the header secret.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download export --manifest --vm=example</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a VM export for a VM example, and retrieve the manifest. The manifest does not include the header secret.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download export --manifest --snap=example</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a VM export for a VM snapshot example, and retrieve the manifest. The manifest does not include the header secret.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download export --manifest --include-secret</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Retrieve the manifest for an existing export. The manifest includes the header secret.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download export --manifest --manifest-output-format=json</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Retrieve the manifest for an existing export in json format. The manifest does not include the header secret.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl vmexport download export --manifest --include-secret --output=manifest.yaml</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Retrieve the manifest for an existing export. The manifest includes the header secret and writes it to the file specified.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="vm-memory-dump-commands_virt-using-the-cli-tools">
<title>VM memory dump commands</title>
<simpara>You can use the <literal>virtctl memory-dump</literal> command to output a VM memory dump on a PVC. You can specify an existing PVC or use the <literal>--create-claim</literal> flag to create a new PVC.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The PVC volume mode must be <literal>FileSystem</literal>.</simpara>
</listitem>
<listitem>
<simpara>The PVC must be large enough to contain the memory dump.</simpara>
<simpara>The formula for calculating the PVC size is <literal>(VMMemorySize + 100Mi) * FileSystemOverhead</literal>, where <literal>100Mi</literal> is the memory dump overhead.</simpara>
</listitem>
<listitem>
<simpara>You must enable the hot plug feature gate in the <literal>HyperConverged</literal> custom resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op": "add", "path": "/spec/featureGates", \
  "value": "HotplugVolumes"}]'</programlisting>
</listitem>
</itemizedlist>
<formalpara>
<title>Downloading the memory dump</title>
<para>You must use the <literal>virtctl vmexport download</literal> command to download the memory dump:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl vmexport download &lt;vmexport_name&gt; --vm|pvc=&lt;object_name&gt; \
  --volume=&lt;volume_name&gt; --output=&lt;output_file&gt;</programlisting>
<table frame="all" rowsep="1" colsep="1">
<title>VM memory dump commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl memory-dump get &lt;vm_name&gt; --claim-name=&lt;pvc_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Save the memory dump of a VM on a PVC. The memory dump status is displayed in the <literal>status</literal> section of the <literal>VirtualMachine</literal> resource.</simpara>
<simpara>Optional:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--create-claim</literal> creates a new PVC with the appropriate size. This flag has the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--storage-class=&lt;storage_class&gt;</literal>: Specify a storage class for the PVC.</simpara>
</listitem>
<listitem>
<simpara><literal>--access-mode=&lt;access_mode&gt;</literal>: Specify <literal>ReadWriteOnce</literal> or <literal>ReadWriteMany</literal>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl memory-dump get &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Rerun the <literal>virtctl memory-dump</literal> command with the same PVC.</simpara>
<simpara>This command overwrites the previous memory dump.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl memory-dump remove &lt;vm_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Remove a memory dump.</simpara>
<simpara>You must remove a memory dump manually if you want to change the target PVC.</simpara>
<simpara>This command removes the association between the VM and the PVC, so that the memory dump is not displayed in the <literal>status</literal> section of the <literal>VirtualMachine</literal> resource. The PVC is not affected.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="hot-plug-and-hot-unplug-commands_virt-using-the-cli-tools">
<title>Hot plug and hot unplug commands</title>
<simpara>You use <literal>virtctl</literal> to add or remove resources from running virtual machines (VMs) and virtual machine instances (VMIs).</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Hot plug and hot unplug commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl addvolume &lt;vm_name&gt; --volume-name=&lt;datavolume_or_PVC&gt; [--persist] [--serial=&lt;label&gt;]</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Hot plug a data volume or persistent volume claim (PVC).</simpara>
<simpara>Optional:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>--persist</literal> mounts the virtual disk permanently on a VM. <emphasis role="strong">This flag does not apply to VMIs.</emphasis></simpara>
</listitem>
<listitem>
<simpara><literal>--serial=&lt;label&gt;</literal> adds a label to the VM. If you do not specify a label, the default label is the data volume or PVC name.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl removevolume &lt;vm_name&gt; --volume-name=&lt;virtual_disk&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Hot unplug a virtual disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl addinterface &lt;vm_name&gt; --network-attachment-definition-name &lt;net_attach_def_name&gt; --name &lt;interface_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Hot plug a Linux bridge network interface.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl removeinterface &lt;vm_name&gt; --name &lt;interface_name&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Hot unplug a Linux bridge network interface.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="image-upload-commands_virt-using-the-cli-tools">
<title>Image upload commands</title>
<simpara>You use the <literal>virtctl image-upload</literal> commands to upload a VM image to a data volume.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Image upload commands</title>
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="141.6665*"/>
<colspec colname="col_2" colwidth="283.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl image-upload dv &lt;datavolume_name&gt; --image-path=&lt;/path/to/image&gt; --no-create</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Upload a VM image to a data volume that already exists.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virtctl image-upload dv &lt;datavolume_name&gt; --size=&lt;datavolume_size&gt; --image-path=&lt;/path/to/image&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Upload a VM image to a new data volume of a specified requested size.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
</section>
<section xml:id="virt-deploying-libguestfs-with-virtctl_virt-using-the-cli-tools">
<title>Deploying libguestfs by using virtctl</title>
<simpara>You can use the <literal>virtctl guestfs</literal> command to deploy an interactive container with <literal>libguestfs-tools</literal> and a persistent volume claim (PVC) attached to it.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To deploy a container with <literal>libguestfs-tools</literal>, mount the PVC, and attach a shell to it, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl guestfs -n &lt;namespace&gt; &lt;pvc_name&gt; <co xml:id="CO1-1"/></programlisting>
<calloutlist>
<callout arearefs="CO1-1">
<para>The PVC name is a required argument. If you do not include it, an error message appears.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="virt-about-libguestfs-tools-virtctl-guestfs_virt-using-the-cli-tools">
<title>Libguestfs and virtctl guestfs commands</title>
<simpara><literal>Libguestfs</literal> tools help you access and modify virtual machine (VM) disk images. You can use <literal>libguestfs</literal> tools to view and edit files in a guest, clone and build virtual machines, and format and resize disks.</simpara>
<simpara>You can also use the <literal>virtctl guestfs</literal> command and its sub-commands to modify, inspect, and debug VM disks on a PVC. To see a complete list of possible sub-commands, enter <literal>virt-</literal> on the command line and press the Tab key. For example:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="178.5*"/>
<colspec colname="col_2" colwidth="246.4999*"/>
<thead>
<row>
<entry align="left" valign="top">Command</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>virt-edit -a /dev/vda /etc/motd</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Edit a file interactively in your terminal.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-customize -a /dev/vda --ssh-inject root:string:&lt;public key example&gt;</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Inject an ssh key into the guest and create a login.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-df -a /dev/vda -h</literal></simpara></entry>
<entry align="left" valign="top"><simpara>See how much disk space is used by a VM.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-customize -a /dev/vda --run-command 'rpm -qa &gt; /rpm-list'</literal></simpara></entry>
<entry align="left" valign="top"><simpara>See the full list of all RPMs installed on a guest by creating an output file containing the full list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-cat -a /dev/vda /rpm-list</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Display the output file list of all RPMs created using the <literal>virt-customize -a /dev/vda --run-command 'rpm -qa &gt; /rpm-list'</literal> command in your terminal.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-sysprep -a /dev/vda</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Seal a virtual machine disk image to be used as a template.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>By default, <literal>virtctl guestfs</literal> creates a session with everything needed to manage a VM disk. However, the command also supports several flag options if you want to customize the behavior:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<?dbhtml table-width="100%"?>
<?dbfo table-width="100%"?>
<?dblatex table-width="100%"?>
<tgroup cols="2">
<colspec colname="col_1" colwidth="178.5*"/>
<colspec colname="col_2" colwidth="246.4999*"/>
<thead>
<row>
<entry align="left" valign="top">Flag Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>--h</literal> or <literal>--help</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Provides help for <literal>guestfs</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>-n &lt;namespace&gt;</literal> option with a <literal>&lt;pvc_name&gt;</literal> argument</simpara></entry>
<entry align="left" valign="top"><simpara>To use a PVC from a specific namespace.</simpara><simpara>If you do not use the <literal>-n &lt;namespace&gt;</literal> option, your current project is used. To change projects, use <literal>oc project &lt;namespace&gt;</literal>.</simpara><simpara>If you do not include a <literal>&lt;pvc_name&gt;</literal> argument, an error message appears.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--image string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Lists the <literal>libguestfs-tools</literal> container image.</simpara><simpara>You can configure the container to use a custom image by using the <literal>--image</literal> option.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--kvm</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Indicates that <literal>kvm</literal> is used by the <literal>libguestfs-tools</literal> container.</simpara><simpara>By default, <literal>virtctl guestfs</literal> sets up <literal>kvm</literal> for the interactive container, which greatly speeds up the <literal>libguest-tools</literal> execution because it uses QEMU.</simpara><simpara>If a cluster does not have any <literal>kvm</literal> supporting nodes, you must disable <literal>kvm</literal> by setting the option <literal>--kvm=false</literal>.</simpara><simpara>If not set, the <literal>libguestfs-tools</literal> pod remains pending because it cannot be scheduled on any node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>--pull-policy string</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Shows the pull policy for the <literal>libguestfs</literal> image.</simpara><simpara>You can also overwrite the image&#8217;s pull policy by setting the <literal>pull-policy</literal> option.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>The command also checks if a PVC is in use by another pod, in which case an error message appears. However, once the <literal>libguestfs-tools</literal> process starts, the setup cannot avoid a new pod using the same PVC. You must verify that there are no active <literal>virtctl guestfs</literal> pods before starting the VM that accesses the same PVC.</simpara>
<note>
<simpara>The <literal>virtctl guestfs</literal> command accepts only a single PVC attached to the interactive pod.</simpara>
</note>
</section>
</section>
</section>
<section xml:id="virt-web-console-overview">
<title>Web console overview</title>

<simpara>The <emphasis role="strong">Virtualization</emphasis> section of the OpenShift Container Platform web console contains the following pages for managing and monitoring your OpenShift Virtualization environment.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title><emphasis role="strong">Virtualization</emphasis> pages</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Page</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-page_virt-web-console-overview"><emphasis role="strong">Overview</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Manage and monitor the OpenShift Virtualization environment.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="catalog-page_virt-web-console-overview"><emphasis role="strong">Catalog</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create virtual machines from a catalog of templates.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachines-page_virt-web-console-overview"><emphasis role="strong">VirtualMachines</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create and manage virtual machines.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="templates-page_virt-web-console-overview"><emphasis role="strong">Templates</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create and manage templates.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="instancetypes-page_virt-web-console-overview"><emphasis role="strong">InstanceTypes</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create and manage virtual machine instance types.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="instancetypes-page_virt-web-console-overview"><emphasis role="strong">Preferences</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create and manage virtual machine preferences.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="bootablevolumes-page_virt-web-console-overview"><emphasis role="strong">Bootable volumes</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create and manage DataSources for bootable volumes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="migrationpolicies-page_virt-web-console-overview"><emphasis role="strong">MigrationPolicies</emphasis> page</link></simpara></entry>
<entry align="left" valign="top"><simpara>Create and manage migration policies for workloads.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<table frame="all" rowsep="1" colsep="1">
<title>Key</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Icon</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-pencil.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon pencil</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara>Edit icon</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-link.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon link</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara>Link icon</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="overview-page_virt-web-console-overview">
<title>Overview page</title>
<simpara>The <emphasis role="strong">Overview</emphasis> page displays resources, metrics, migration progress, and cluster-level settings.</simpara>
<example>
<title><emphasis role="strong">Overview</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download virtctl</emphasis> <inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-link.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon link</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara>Download the <literal>virtctl</literal> command line tool to manage resources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-overview_virt-web-console-overview"><emphasis role="strong">Overview</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Resources, usage, alerts, and status.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-top-consumers_virt-web-console-overview"><emphasis role="strong">Top consumers</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Top consumers of CPU, memory, and storage resources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-migrations_virt-web-console-overview"><emphasis role="strong">Migrations</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Status of live migrations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-settings_virt-web-console-overview"><emphasis role="strong">Settings</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>The <emphasis role="strong">Settings</emphasis> tab contains the <emphasis role="strong">Cluster</emphasis> tab, <emphasis role="strong">User</emphasis> tab, and <emphasis role="strong">Preview features</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Settings</emphasis> &#8594; <link linkend="overview-settings-cluster_virt-web-console-overview"><emphasis role="strong">Cluster</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization version, update status, live migration, templates project, preview features, and load balancer service settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Settings</emphasis> &#8594; <link linkend="overview-settings-user_virt-web-console-overview"><emphasis role="strong">User</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Public SSH keys, user permissions, and welcome information settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Settings</emphasis> &#8594; <emphasis role="strong">Preview features</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Enable select <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">preview features</link> in the UI. Features in this tab change frequently.</simpara><simpara>Preview features are disabled by default and must not be enabled in production environments.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="overview-overview_virt-web-console-overview">
<title>Overview tab</title>
<simpara>The <emphasis role="strong">Overview</emphasis> tab displays resources, usage, alerts, and status.</simpara>
<example>
<title><emphasis role="strong">Overview</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Getting started resources</emphasis> card</simpara></entry>
<entry align="left" valign="top"><itemizedlist>
<listitem>
<simpara><emphasis role="strong">Quick Starts</emphasis> tile: Learn how to create, import, and run virtual machines with step-by-step instructions and tasks.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Feature highlights</emphasis> tile: Read the latest information about key virtualization features.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Related operators</emphasis> tile: Install Operators such as the Kubernetes NMState Operator or the OpenShift Data Foundation Operator.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Memory</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>Memory usage, with a chart showing the last 7 days' trend.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Storage</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>Storage usage, with a chart showing the last 7 days' trend.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachines</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>Number of virtual machines, with a chart showing the last 7 days' trend.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">vCPU usage</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>vCPU usage, with a chart showing the last 7 days' trend.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachine statuses</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>Number of virtual machines, grouped by status.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Alerts</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization alerts, grouped by severity.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachines per resource</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Number of virtual machines created from templates and instance types.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="overview-top-consumers_virt-web-console-overview">
<title>Top consumers tab</title>
<simpara>The <emphasis role="strong">Top consumers</emphasis> tab displays the top consumers of CPU, memory, and storage.</simpara>
<example>
<title><emphasis role="strong">Top consumers</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">View virtualization dashboard</emphasis> <inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-link.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon link</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara>Link to <emphasis role="strong">Observe &#8594; Dashboards</emphasis>, which displays the top consumers for OpenShift Virtualization.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Time period</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select a time period to filter the results.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Top consumers</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select the number of top consumers to filter the results.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">CPU</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machines with the highest CPU usage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Memory</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machines with the highest memory usage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Memory swap traffic</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machines with the highest memory swap traffic.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">vCPU wait</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machines with the highest vCPU wait periods.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Storage throughput</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machines with the highest storage throughput usage.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Storage IOPS</emphasis> chart</simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machines with the highest storage input/output operations per second usage.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="overview-migrations_virt-web-console-overview">
<title>Migrations tab</title>
<simpara>The <emphasis role="strong">Migrations</emphasis> tab displays the status of virtual machine migrations.</simpara>
<example>
<title><emphasis role="strong">Migrations</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Time period</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select a time period to filter virtual machine migrations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachineInstanceMigrations information</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of virtual machine migrations.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="overview-settings_virt-web-console-overview">
<title>Settings tab</title>
<simpara>The <emphasis role="strong">Settings</emphasis> tab displays cluster-wide settings.</simpara>
<example>
<title>Tabs on the <emphasis role="strong">Settings</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Tab</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-settings-cluster_virt-web-console-overview"><emphasis role="strong">Cluster</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization version and update status, live migration, templates project, and load balancer service settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="overview-settings-user_virt-web-console-overview"><emphasis role="strong">User</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Public SSH key management, user permissions, and welcome information settings.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="overview-settings-cluster_virt-web-console-overview">
<title>Cluster tab</title>
<simpara>The <emphasis role="strong">Cluster</emphasis> tab displays the OpenShift Virtualization version and update status. You configure live migration and other settings on the <emphasis role="strong">Cluster</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Cluster</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Installed version</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization version.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Update status</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization update status.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Channel</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization update channel.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Live Migration</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to configure live migration settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Live Migration</emphasis> &#8594; <emphasis role="strong">Max. migrations per cluster</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Select the maximum number of live migrations per cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Live Migration</emphasis> &#8594; <emphasis role="strong">Max. migrations per node</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Select the maximum number of live migrations per node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Live Migration</emphasis> &#8594; <emphasis role="strong">Live migration network</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select a dedicated secondary network for live migration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">SSH Configuration</emphasis> &#8594; <emphasis role="strong">SSH over LoadBalancer service</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Enable the creation of LoadBalancer services for SSH connections to VMs.</simpara><simpara>You must configure a load balancer.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Automatic subscription of new RHEL VirtualMachines</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to enable automatic subscription for Red Hat Enterprise Linux (RHEL) virtual machines.</simpara><simpara>To enable this feature, you need cluster administrator permissions, an organization ID, and an activation key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">LoadBalancer</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to enable the creation of load balancer services for SSH access to virtual machines.</simpara><simpara>The cluster must have a load balancer configured.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Template project</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to select a project for Red Hat templates. The default project is <literal>openshift</literal>.</simpara><simpara>To store Red Hat templates in multiple projects, <link linkend="templates-page_virt-web-console-overview">clone the template</link> and then select a project for the cloned template.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="overview-settings-user_virt-web-console-overview">
<title>User tab</title>
<simpara>You view user permissions and manage public SSH keys and welcome information on the <emphasis role="strong">User</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">User</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage SSH keys</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to add public SSH keys to a project.</simpara><simpara>The keys are added automatically to all virtual machines that you subsequently create in the selected project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Permissions</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to view cluster-wide user permissions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Welcome information</emphasis> section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand this section to show or hide the <emphasis role="strong">Welcome information</emphasis> dialog.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="overview-settings-preview_virt-web-console-overview">
<title>Preview features tab</title>
<simpara>Enable select <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">preview features</link> in the UI. Features in this tab change frequently.</simpara>
</section>
</section>
</section>
<section xml:id="catalog-page_virt-web-console-overview">
<title>Catalog page</title>
<simpara>You create a virtual machine from a template or instance type on the <emphasis role="strong">Catalog</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">Catalog</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><link linkend="catalog-template_virt-web-console-overview"><emphasis role="strong">Template catalog</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a catalog of templates for creating a virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="catalog-instancetypes_virt-web-console-overview"><emphasis role="strong">InstanceTypes</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Displays bootable volumes and instance types for creating a virtual machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="catalog-template_virt-web-console-overview">
<title>Template catalog tab</title>
<simpara>You select a template on the <emphasis role="strong">Template catalog</emphasis> tab to create a virtual machine.</simpara>
<example>
<title><emphasis role="strong">Template catalog</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Template project</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select the project in which Red Hat templates are located.</simpara>
<simpara>By default, Red Hat templates are stored in the <literal>openshift</literal> project. You can edit the template project on the <link linkend="overview-settings-cluster_virt-web-console-overview"><emphasis role="strong">Overview</emphasis> page &#8594; <emphasis role="strong">Settings</emphasis> tab &#8594; <emphasis role="strong">Cluster</emphasis> tab</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">All items</emphasis>|<emphasis role="strong">Default templates</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click <emphasis role="strong">All items</emphasis> to display all available templates.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Boot source available</emphasis> checkbox</simpara></entry>
<entry align="left" valign="top"><simpara>Select the checkbox to display templates with an available boot source.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Operating system</emphasis> checkboxes</simpara></entry>
<entry align="left" valign="top"><simpara>Select checkboxes to display templates with selected operating systems.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Workload</emphasis> checkboxes</simpara></entry>
<entry align="left" valign="top"><simpara>Select checkboxes to display templates with selected workloads.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search templates by keyword.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Template tiles</simpara></entry>
<entry align="left" valign="top"><simpara>Click a template tile to view template details and to create a virtual machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="catalog-instancetypes_virt-web-console-overview">
<title>InstanceTypes tab</title>
<simpara>You create a virtual machine from an instance type on the <emphasis role="strong">InstanceTypes</emphasis> tab.</simpara>
<important>
<simpara>Creating a virtual machine from an instance type is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<informalexample>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Volumes project</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Project in which bootable volumes are stored. The default is <literal>openshift-virtualization-os-images</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add volume</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Click to upload a new volume or to use an existing persistent volume claim.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter boot sources by operating system or resource.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search boot sources by name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Volume table</simpara></entry>
<entry align="left" valign="top"><simpara>Select a bootable volume for your virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Red Hat provided</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Select an instance type provided by Red Hat.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">User provided</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Select an instance type that you created on the <emphasis role="strong">InstanceType</emphasis> page.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachine details</emphasis> pane</simpara></entry>
<entry align="left" valign="top"><simpara>Displays the virtual machine settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Optional: Enter the virtual machine name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">SSH key name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a public SSH key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Start this VirtualMachine after creation</emphasis> checkbox</simpara></entry>
<entry align="left" valign="top"><simpara>Clear this checkbox to prevent the virtual machine from starting automatically.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Create VirtualMachine</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Creates a virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML &amp; CLI</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Displays the YAML configuration file and the <literal>virtctl create</literal> command to create the virtual machine from the command line.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</informalexample>
</section>
</section>
<section xml:id="virtualmachines-page_virt-web-console-overview">
<title>VirtualMachines page</title>
<simpara>You create and manage virtual machines on the <emphasis role="strong">VirtualMachines</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">VirtualMachines</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Create</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Create a virtual machine from a template, volume, or YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter virtual machines by status, template, operating system, or node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for virtual machines by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table. The <emphasis role="strong">Namespace</emphasis> column is only displayed when <emphasis role="strong">All Projects</emphasis> is selected from the <emphasis role="strong">Projects</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Virtual machines table</simpara></entry>
<entry align="left" valign="top"><simpara>List of virtual machines.</simpara><simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a virtual machine to select <emphasis role="strong">Stop</emphasis>, <emphasis role="strong">Restart</emphasis>, <emphasis role="strong">Pause</emphasis>, <emphasis role="strong">Clone</emphasis>, <emphasis role="strong">Migrate</emphasis>, <emphasis role="strong">Copy SSH command</emphasis>, <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, or <emphasis role="strong">Delete</emphasis>. If you select <emphasis role="strong">Stop</emphasis>, <emphasis role="strong">Force stop</emphasis> replaces <emphasis role="strong">Stop</emphasis> in the action menu. Use <emphasis role="strong">Force stop</emphasis> to initiate an immediate shutdown if the operating system becomes unresponsive.</simpara><simpara>Click a virtual machine to navigate to the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="virtualmachine-details-page_virt-web-console-overview">
<title>VirtualMachine details page</title>
<simpara>You configure a virtual machine on the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">VirtualMachine details</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actions</emphasis> menu</simpara></entry>
<entry align="left" valign="top"><simpara>Click the <emphasis role="strong">Actions</emphasis> menu to select <emphasis role="strong">Stop</emphasis>, <emphasis role="strong">Restart</emphasis>, <emphasis role="strong">Pause</emphasis>, <emphasis role="strong">Clone</emphasis>, <emphasis role="strong">Migrate</emphasis>, <emphasis role="strong">Copy SSH command</emphasis>, <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, or <emphasis role="strong">Delete</emphasis>. If you select <emphasis role="strong">Stop</emphasis>, <emphasis role="strong">Force stop</emphasis> replaces <emphasis role="strong">Stop</emphasis> in the action menu. Use <emphasis role="strong">Force stop</emphasis> to initiate an immediate shutdown if the operating system becomes unresponsive.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-overview_virt-web-console-overview"><emphasis role="strong">Overview</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Resource usage, alerts, disks, and devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-details_virt-web-console-overview"><emphasis role="strong">Details</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine details and configurations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-metrics_virt-web-console-overview"><emphasis role="strong">Metrics</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Memory, CPU, storage, network, and migration metrics.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-yaml_virt-web-console-overview"><emphasis role="strong">YAML</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-configuration_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Contains the <emphasis role="strong">Disks</emphasis>, <emphasis role="strong">Network interfaces</emphasis>, <emphasis role="strong">Scheduling</emphasis>, <emphasis role="strong">Environment</emphasis>, and <emphasis role="strong">Scripts</emphasis> tabs.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-disks_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Disks</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Disks.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-network-interfaces_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Network interfaces</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Network interfaces.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-scheduling_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Scheduling</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Scheduling a virtual machine to run on specific nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-environment_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Environment</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Config map, secret, and service account management.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-scripts_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Scripts</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Cloud-init settings, public SSH key and dynamic key injection for Linux virtual machines, Sysprep settings for Windows virtual machines.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-events_virt-web-console-overview"><emphasis role="strong">Events</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine event stream.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-console_virt-web-console-overview"><emphasis role="strong">Console</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Console session management.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-snapshots_virt-web-console-overview"><emphasis role="strong">Snapshots</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Snapshot management.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-diagnostics_virt-web-console-overview"><emphasis role="strong">Diagnostics</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Status conditions and volume snapshot status.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="virtualmachine-details-overview_virt-web-console-overview">
<title>Overview tab</title>
<simpara>The <emphasis role="strong">Overview</emphasis> tab displays resource usage, alerts, and configuration information.</simpara>
<example>
<title><emphasis role="strong">Overview</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Details</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>General virtual machine information.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Utilization</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">CPU</emphasis>, <emphasis role="strong">Memory</emphasis>, <emphasis role="strong">Storage</emphasis>, and <emphasis role="strong">Network transfer</emphasis> charts. By default, <emphasis role="strong">Network transfer</emphasis> displays the sum of all networks. To view the breakdown for a specific network, click <emphasis role="strong">Breakdown by network</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Hardware devices</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>GPU and host devices.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Alerts</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization alerts, grouped by severity.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Snapshots</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Take snapshot</emphasis> <inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-link.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon link</phrase></textobject>
</inlinemediaobject> and snapshots table.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Network interfaces</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>Network interfaces table.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Disks</emphasis> tile</simpara></entry>
<entry align="left" valign="top"><simpara>Disks table.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-details_virt-web-console-overview">
<title>Details tab</title>
<simpara>You view information about the virtual machine and edit labels, annotations, and other metadata and on the <emphasis role="strong">Details</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Details</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Set to <emphasis role="strong">ON</emphasis> to view your live changes in the YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Namespace</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine namespace or project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the labels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Annotations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the annotations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Description</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to enter a description.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Operating system</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Operating system name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">CPU|Memory</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the CPU|Memory request. Restart the virtual machine to apply the change.</simpara>
<simpara>The number of CPUs is calculated by using the following formula: <literal>sockets * threads * cores</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Machine type</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Machine type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Boot mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the boot mode. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Start in pause mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to enable this setting. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Template</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Name of the template used to create the virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Created at</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine creation date.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Owner</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine owner.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Status</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine status.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Pod</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><literal>virt-launcher</literal> pod name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachineInstance</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Virtual machine instance name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Boot order</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to select a boot source. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">IP address</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>IP address of the virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Hostname</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Hostname of the virtual machine. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Time zone</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Time zone of the virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Node</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Node on which the virtual machine is running.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Workload profile</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the workload profile.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">SSH access</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>These settings apply to Linux.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">SSH using virtctl</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the copy icon to copy the <literal>virtctl ssh</literal> command to the clipboard. This feature is disabled if the virtual machine does not have a public SSH key.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">SSH service type</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">SSH over LoadBalancer</emphasis>.</simpara>
<simpara>After you create a service, the SSH command is displayed. Click the copy icon to copy the command to the clipboard.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">GPU devices</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a GPU device. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Host devices</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a host device. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Headless mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to set headless mode to <emphasis role="strong">ON</emphasis> and to disable VNC console. Restart the virtual machine to apply the change.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Services</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a list of services if QEMU guest agent is installed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Active users</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Displays a list of active users if QEMU guest agent is installed.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-metrics_virt-web-console-overview">
<title>Metrics tab</title>
<simpara>The <emphasis role="strong">Metrics</emphasis> tab displays memory, CPU, network, storage, and migration usage charts, as well as live migration progress.</simpara>
<example>
<title><emphasis role="strong">Metrics</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Time range</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select a time range to filter the results.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Virtualization dashboard</emphasis> <inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-link.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon link</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara>Link to the <emphasis role="strong">Workloads</emphasis> tab of the current project.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Utilization</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Memory</emphasis> and <emphasis role="strong">CPU</emphasis> charts.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Storage</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Storage total read/write</emphasis> and <emphasis role="strong">Storage IOPS total read/write</emphasis> charts.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Network</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Network in</emphasis>, <emphasis role="strong">Network out</emphasis>, <emphasis role="strong">Network bandwidth</emphasis>, and <emphasis role="strong">Network interface</emphasis> charts. Select <emphasis role="strong">All networks</emphasis> or a specific network from the <emphasis role="strong">Network interface</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Migration</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">Migration</emphasis> and <emphasis role="strong">KV data transfer rate</emphasis> charts.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">LiveMigration progress</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara><emphasis role="strong">LiveMigration</emphasis> completion status.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-yaml_virt-web-console-overview">
<title>YAML tab</title>
<simpara>You configure the virtual machine by editing the YAML file on the <emphasis role="strong">YAML</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">YAML</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Save</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Save changes to the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Reload</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Discard your changes and reload the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cancel</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Exit the <emphasis role="strong">YAML</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Download the YAML file to your local machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-configuration_virt-web-console-overview">
<title>Configuration tab</title>
<simpara>You configure scheduling, network interfaces, disks, and other options on the <emphasis role="strong">Configuration</emphasis> tab.</simpara>
<example>
<title>Tabs on the <emphasis role="strong">Configuration</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Set to <emphasis role="strong">ON</emphasis> to view your live changes in the YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-disks_virt-web-console-overview"><emphasis role="strong">Disks</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Disks.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-network-interfaces_virt-web-console-overview"><emphasis role="strong">Network interfaces</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Network interfaces.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-scheduling_virt-web-console-overview"><emphasis role="strong">Scheduling</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Scheduling and resource requirements.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-environment_virt-web-console-overview"><emphasis role="strong">Environment</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Config maps, secrets, and service accounts.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="virtualmachine-details-scripts_virt-web-console-overview"><emphasis role="strong">Scripts</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Cloud-init settings, public SSH key for Linux virtual machines, Sysprep answer file for Windows virtual machines.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="virtualmachine-details-disks_virt-web-console-overview">
<title>Disks tab</title>
<simpara>You manage disks on the <emphasis role="strong">Disks</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Disks</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add disk</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Add a disk to the virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter by disk type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for a disk by name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Mount Windows drivers disk</emphasis> checkbox</simpara></entry>
<entry align="left" valign="top"><simpara>Select to mount a <literal>virtio-win</literal> container disk as a CD-ROM to install VirtIO drivers.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Disks</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of virtual machine disks.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a disk to select <emphasis role="strong">Edit</emphasis> or <emphasis role="strong">Detach</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">File systems</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of virtual machine file systems.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-network-interfaces_virt-web-console-overview">
<title>Network interfaces tab</title>
<simpara>You manage network interfaces on the <emphasis role="strong">Network interfaces</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Network interfaces</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add network interface</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Add a network interface to the virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter by interface type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for a network interface by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Network interface</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of network interfaces.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a network interface to select <emphasis role="strong">Edit</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-scheduling_virt-web-console-overview">
<title>Scheduling tab</title>
<simpara>You configure virtual machines to run on specific nodes on the <emphasis role="strong">Scheduling</emphasis> tab.</simpara>
<simpara>Restart the virtual machine to apply changes.</simpara>
<example>
<title><emphasis role="strong">Scheduling</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Node selector</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a label to specify qualifying nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Tolerations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a toleration to specify qualifying nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Affinity rules</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add an affinity rule.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descheduler</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Enable or disable the descheduler. The descheduler evicts a running pod so that the pod can be rescheduled onto a more suitable node.</simpara>
<simpara>This field is disabled if the virtual machine cannot be live migrated.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Dedicated resources</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to select <emphasis role="strong">Schedule this workload with dedicated resources (guaranteed policy)</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Eviction strategy</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to select <emphasis role="strong">LiveMigrate</emphasis> as the virtual machine eviction strategy.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-environment_virt-web-console-overview">
<title>Environment tab</title>
<simpara>You manage config maps, secrets, and service accounts on the <emphasis role="strong">Environment</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Environment</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add Config Map, Secret or Service Account</emphasis> <inlinemediaobject>
<imageobject>
<imagedata fileref="images/icon-link.png" contentwidth="20"/>
</imageobject>
<textobject><phrase>icon link</phrase></textobject>
</inlinemediaobject></simpara></entry>
<entry align="left" valign="top"><simpara>Click the link and select a config map, secret, or service account from the resource list.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-scripts_virt-web-console-overview">
<title>Scripts tab</title>
<simpara>You manage cloud-init settings, add SSH keys, or configure Sysprep for Windows virtual machines on the <emphasis role="strong">Scripts</emphasis> tab.</simpara>
<simpara>Restart the virtual machine to apply changes.</simpara>
<example>
<title><emphasis role="strong">Scripts</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cloud-init</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the cloud-init settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Public SSH key</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a public SSH key to a Linux virtual machine.</simpara>
<simpara>The key is added as a cloud-init data source at first boot.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Dynamic SSH key injection</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Set <emphasis role="strong">Dynamic SSH key injection</emphasis> to on to enable dynamic public SSH key injection. Then, you can add or revoke the key at runtime.</simpara>
<simpara>Dynamic SSH key injection is only supported by Red Hat Enterprise Linux (RHEL) 9. If you manually disable this setting, the virtual machine inherits the SSH key settings of the image from which it was created.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Sysprep</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to upload an <literal>Autounattend.xml</literal> or <literal>Unattend.xml</literal> answer file to automate Windows virtual machine setup.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
<section xml:id="virtualmachine-details-events_virt-web-console-overview">
<title>Events tab</title>
<simpara>The <emphasis role="strong">Events</emphasis> tab displays a list of virtual machine events.</simpara>
</section>
<section xml:id="virtualmachine-details-console_virt-web-console-overview">
<title>Console tab</title>
<simpara>You can open a console session to the virtual machine on the <emphasis role="strong">Console</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Console</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Guest login credentials section</simpara></entry>
<entry align="left" valign="top"><simpara>Expand <emphasis role="strong">Guest login credentials</emphasis> to view the credentials created with <literal>cloud-init</literal>. Click the copy icon to copy the credentials to the clipboard.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Console</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">VNC console</emphasis> or <emphasis role="strong">Serial console</emphasis>.</simpara>
<simpara>The <emphasis role="strong">Desktop viewer</emphasis> option is displayed for Windows virtual machines. You must install an RDP client on a machine on the same network.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Send key</emphasis> list</simpara></entry>
<entry align="left" valign="top"><simpara>Select a key-stroke combination to send to the console.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Disconnect</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Disconnect the console connection.</simpara>
<simpara>You must manually disconnect the console connection if you open a new console session. Otherwise, the first console session continues to run in the background.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Paste</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Paste a string from your clipboard to the VNC console.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-snapshots_virt-web-console-overview">
<title>Snapshots tab</title>
<simpara>You create snapshots and restore virtual machines from snapshots on the <emphasis role="strong">Snapshots</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Snapshots</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Take snapshot</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Create a snapshot.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter snapshots by status.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for snapshots by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Snapshot</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of snapshots</simpara>
<simpara>Click the snapshot name to edit the labels or annotations.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a snapshot to select <emphasis role="strong">Restore</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="virtualmachine-details-diagnostics_virt-web-console-overview">
<title>Diagnostics tab</title>
<simpara>You view the status conditions and volume snapshot status on the <emphasis role="strong">Diagnostics</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Diagnostics</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Status conditions</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>Display a list of conditions that are reported for the virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter status conditions by category and condition.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search status conditions by reason.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Volume snapshot status</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of volumes, their snapshot enablement status, and reason.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
</section>
<section xml:id="templates-page_virt-web-console-overview">
<title>Templates page</title>
<simpara>You create, edit, and clone virtual machine templates on the <emphasis role="strong">VirtualMachine Templates</emphasis> page.</simpara>
<note>
<simpara>You cannot edit a Red Hat template. However, you can clone a Red Hat template and edit it to create a custom template.</simpara>
</note>
<example>
<title><emphasis role="strong">VirtualMachine Templates</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Create Template</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Create a template by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter templates by type, boot source, template provider, or operating system.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for templates by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table. The <emphasis role="strong">Namespace</emphasis> column is only displayed when <emphasis role="strong">All Projects</emphasis> is selected from the <emphasis role="strong">Projects</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Virtual machine templates table</simpara></entry>
<entry align="left" valign="top"><simpara>List of virtual machine templates.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a template to select <emphasis role="strong">Edit</emphasis>, <emphasis role="strong">Clone</emphasis>, <emphasis role="strong">Edit boot source</emphasis>, <emphasis role="strong">Edit boot source reference</emphasis>, <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, or <emphasis role="strong">Delete</emphasis>. You cannot edit a Red Hat provided template. You can clone the Red Hat template and then edit the custom template.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="template-details-page_virt-web-console-overview">
<title>Template details page</title>
<simpara>You view template settings and edit custom templates on the <emphasis role="strong">Template details</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">Template details</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Set to <emphasis role="strong">ON</emphasis> to view your live changes in the YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actions</emphasis> menu</simpara></entry>
<entry align="left" valign="top"><simpara>Click the <emphasis role="strong">Actions</emphasis> menu to select <emphasis role="strong">Edit</emphasis>, <emphasis role="strong">Clone</emphasis>, <emphasis role="strong">Edit boot source</emphasis>, <emphasis role="strong">Edit boot source reference</emphasis>, <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-details_virt-web-console-overview"><emphasis role="strong">Details</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Template settings and configurations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-yaml_virt-web-console-overview"><emphasis role="strong">YAML</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-scheduling_virt-web-console-overview"><emphasis role="strong">Scheduling</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Scheduling configurations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-network-interfaces_virt-web-console-overview"><emphasis role="strong">Network interfaces</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Network interface management.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-disks_virt-web-console-overview"><emphasis role="strong">Disks</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Disk management.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-scripts_virt-web-console-overview"><emphasis role="strong">Scripts</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Cloud-init, SSH key, and Sysprep management.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><link linkend="template-details-parameters_virt-web-console-overview"><emphasis role="strong">Parameters</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Name and cloud user password management.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="template-details-details_virt-web-console-overview">
<title>Details tab</title>
<simpara>You configure a custom template on the <emphasis role="strong">Details</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Details</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Namespace</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template namespace.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the labels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Annotations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the annotations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Display name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the display name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Description</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to enter a description.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Operating system</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Operating system name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">CPU|Memory</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the CPU|Memory request.</simpara>
<simpara>The number of CPUs is calculated by using the following formula: <literal>sockets * threads * cores</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Machine type</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template machine type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Boot mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the boot mode.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Base template</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Name of the base template used to create this template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Created at</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template creation date.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Owner</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template owner.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Boot order</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template boot order.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Boot source</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Boot source availability.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Provider</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template provider.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Support</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Template support level.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">GPU devices</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a GPU device.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Host devices</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a host device.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Headless mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to set headless mode to <emphasis role="strong">ON</emphasis> and to disable VNC console.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="template-details-yaml_virt-web-console-overview">
<title>YAML tab</title>
<simpara>You configure a custom template by editing the YAML file on the <emphasis role="strong">YAML</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">YAML</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Save</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Save changes to the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Reload</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Discard your changes and reload the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cancel</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Exit the <emphasis role="strong">YAML</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Download the YAML file to your local machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="template-details-scheduling_virt-web-console-overview">
<title>Scheduling tab</title>
<simpara>You configure scheduling on the <emphasis role="strong">Scheduling</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Scheduling</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Node selector</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a label to specify qualifying nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Tolerations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add a toleration to specify qualifying nodes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Affinity rules</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to add an affinity rule.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Descheduler</emphasis> switch</simpara></entry>
<entry align="left" valign="top"><simpara>Enable or disable the descheduler. The descheduler evicts a running pod so that the pod can be rescheduled onto a more suitable node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Dedicated resources</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to select <emphasis role="strong">Schedule this workload with dedicated resources (guaranteed policy)</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Eviction strategy</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to select <emphasis role="strong">LiveMigrate</emphasis> as the virtual machine eviction strategy.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="template-details-network-interfaces_virt-web-console-overview">
<title>Network interfaces tab</title>
<simpara>You manage network interfaces on the <emphasis role="strong">Network interfaces</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Network interfaces</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add network interface</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Add a network interface to the template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter by interface type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for a network interface by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Network interface table</simpara></entry>
<entry align="left" valign="top"><simpara>List of network interfaces.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a network interface to select <emphasis role="strong">Edit</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="template-details-disks_virt-web-console-overview">
<title>Disks tab</title>
<simpara>You manage disks on the <emphasis role="strong">Disks</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Disks</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Setting</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add disk</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Add a disk to the template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter by disk type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for a disk by name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Disks table</simpara></entry>
<entry align="left" valign="top"><simpara>List of template disks.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a disk to select <emphasis role="strong">Edit</emphasis> or <emphasis role="strong">Detach</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="template-details-scripts_virt-web-console-overview">
<title>Scripts tab</title>
<simpara>You manage the cloud-init settings, SSH keys, and Sysprep answer files on the <emphasis role="strong">Scripts</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Scripts</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cloud-init</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the cloud-init settings.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Public SSH key</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to create a new secret or to attach an existing secret to a Linux virtual machine.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Sysprep</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to upload an <literal>Autounattend.xml</literal> or <literal>Unattend.xml</literal> answer file to automate Windows virtual machine setup.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="template-details-parameters_virt-web-console-overview">
<title>Parameters tab</title>
<simpara>You edit selected template settings on the <emphasis role="strong">Parameters</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Parameters</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">NAME</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Set the name parameters for a virtual machine created from this template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">CLOUD_USER_PASSWORD</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Set the cloud user password parameters for a virtual machine created from this template.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
</section>
<section xml:id="instancetypes-page_virt-web-console-overview">
<title>InstanceTypes page</title>
<simpara>You view and manage virtual machine instance types on the <emphasis role="strong">InstanceTypes</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">VirtualMachineClusterInstancetypes</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Create</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Create an instance type by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for an instance type by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table. The <emphasis role="strong">Namespace</emphasis> column is only displayed when <emphasis role="strong">All Projects</emphasis> is selected from the <emphasis role="strong">Projects</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Instance types table</simpara></entry>
<entry align="left" valign="top"><simpara>List of instance.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside an instance type to select <emphasis role="strong">Clone</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<simpara>Click an instance type to view the <emphasis role="strong">VirtualMachineClusterInstancetypes details</emphasis> page.</simpara>
<section xml:id="instancetypes-details-page_virt-web-console-overview">
<title>VirtualMachineClusterInstancetypes details page</title>
<simpara>You configure an instance type on the <emphasis role="strong">VirtualMachineClusterInstancetypes details</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">VirtualMachineClusterInstancetypes details</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Details</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure an instance type by editing a form.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure an instance type by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actions</emphasis> menu</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, <emphasis role="strong">Edit VirtualMachineClusterInstancetype</emphasis>, or <emphasis role="strong">Delete VirtualMachineClusterInstancetype</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="instancetypes-details-details_virt-web-console-overview">
<title>Details tab</title>
<simpara>You configure an instance type by editing a form on the <emphasis role="strong">Details</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Details</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>VirtualMachineClusterInstancetype name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the labels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Annotations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the annotations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Created at</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Instance type creation date.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Owner</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Instance type owner.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="instancetypes-details-yaml_virt-web-console-overview">
<title>YAML tab</title>
<simpara>You configure an instance type by editing the YAML file on the <emphasis role="strong">YAML</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">YAML</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Save</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Save changes to the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Reload</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Discard your changes and reload the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cancel</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Exit the <emphasis role="strong">YAML</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Download the YAML file to your local machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
</section>
<section xml:id="preferences-page_virt-web-console-overview">
<title>Preferences page</title>
<simpara>You view and manage virtual machine preferences on the <emphasis role="strong">Preferences</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">VirtualMachineClusterPreferences</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Create</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Create a preference by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for a preference by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table. The <emphasis role="strong">Namespace</emphasis> column is only displayed when <emphasis role="strong">All Projects</emphasis> is selected from the <emphasis role="strong">Projects</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Preferences table</simpara></entry>
<entry align="left" valign="top"><simpara>List of preferences.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a preference to select <emphasis role="strong">Clone</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<simpara>Click a preference to view the <emphasis role="strong">VirtualMachineClusterPreference details</emphasis> page.</simpara>
<section xml:id="preferences-details-page_virt-web-console-overview">
<title>VirtualMachineClusterPreference details page</title>
<simpara>You configure a preference on the <emphasis role="strong">VirtualMachineClusterPreference details</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">VirtualMachineClusterPreference details</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Details</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure a preference by editing a form.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure a preference by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actions</emphasis> menu</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, <emphasis role="strong">Edit VirtualMachineClusterPreference</emphasis>, or <emphasis role="strong">Delete VirtualMachineClusterPreference</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="preferences-details-details_virt-web-console-overview">
<title>Details tab</title>
<simpara>You configure a preference by editing a form on the <emphasis role="strong">Details</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Details</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>VirtualMachineClusterPreference name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the labels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Annotations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the annotations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Created at</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Preference creation date.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Owner</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Preference owner.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="preferences-details-yaml_virt-web-console-overview">
<title>YAML tab</title>
<simpara>You configure a preference type by editing the YAML file on the <emphasis role="strong">YAML</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">YAML</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Save</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Save changes to the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Reload</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Discard your changes and reload the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cancel</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Exit the <emphasis role="strong">YAML</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Download the YAML file to your local machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
</section>
<section xml:id="bootablevolumes-page_virt-web-console-overview">
<title>Bootable volumes page</title>
<simpara>You view and manage available bootable volumes on the <emphasis role="strong">Bootable volumes</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">Bootable volumes</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Add volume</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Add a bootable volume by completing a form or by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Filter</emphasis> field</simpara></entry>
<entry align="left" valign="top"><simpara>Filter bootable volumes by operating system and resource type.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for bootable volumes by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table. The <emphasis role="strong">Namespace</emphasis> column is only displayed when <emphasis role="strong">All Projects</emphasis> is selected from the <emphasis role="strong">Projects</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Bootable volumes table</simpara></entry>
<entry align="left" valign="top"><simpara>List of bootable volumes.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a bootable volume to select <emphasis role="strong">Edit</emphasis>, <emphasis role="strong">Remove from list</emphasis>, or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<simpara>Click a bootable volume to view the <emphasis role="strong">PersistentVolumeClaim details</emphasis> page.</simpara>
<section xml:id="pvc-details-page_virt-web-console-overview">
<title>PersistentVolumeClaim details page</title>
<simpara>You configure the persistent volume claim (PVC) of a bootable volume on the <emphasis role="strong">PersistentVolumeClaim details</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">PersistentVolumeClaim details</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Details</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure the PVC by editing a form.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure the PVC by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Events</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>The <emphasis role="strong">Events</emphasis> tab displays a list of PVC events.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VolumeSnapshots</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>The <emphasis role="strong">VolumeSnapshots</emphasis> tab displays a list of volume snapshots.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actions</emphasis> menu</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Expand PVC</emphasis>, <emphasis role="strong">Create snapshot</emphasis>, <emphasis role="strong">Clone PVC</emphasis>, <emphasis role="strong">Edit labels</emphasis>, <emphasis role="strong">Edit annotations</emphasis>, <emphasis role="strong">Edit PersistentVolumeClaim</emphasis> or <emphasis role="strong">Delete PersistentVolumeClaim</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="pvc-details-details_virt-web-console-overview">
<title>Details tab</title>
<simpara>You configure the persistent volume claim (PVC) of the bootable volume by editing a form on the <emphasis role="strong">Details</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Details</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Namespace</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC namespace.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the labels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Annotations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to edit the annotations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Created at</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC creation date.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Owner</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC owner.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Status</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Status of the PVC, for example, <emphasis role="strong">Bound</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Requested capacity</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Requested capacity of the PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Capacity</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Capacity of the PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Used</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Used space of the PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Access modes</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC access modes.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Volume mode</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC volume mode.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">StorageClasses</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>PVC storage class.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">PersistentVolumes</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Persistent volume associated with the PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Conditions</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>Displays the status of the PVC.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="pvc-details-yaml_virt-web-console-overview">
<title>YAML tab</title>
<simpara>You configure the persistent volume claim of the bootable volume by editing the YAML file on the <emphasis role="strong">YAML</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">YAML</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Save</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Save changes to the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Reload</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Discard your changes and reload the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cancel</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Exit the <emphasis role="strong">YAML</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Download the YAML file to your local machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
</section>
<section xml:id="migrationpolicies-page_virt-web-console-overview">
<title>MigrationPolicies page</title>
<simpara>You manage migration policies for workloads on the <emphasis role="strong">MigrationPolicies</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">MigrationPolicies</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Create MigrationPolicy</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Create a migration policy by entering configurations and labels in a form or by editing a YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Search field</simpara></entry>
<entry align="left" valign="top"><simpara>Search for a migration policy by name or by label.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Manage columns</emphasis> icon</simpara></entry>
<entry align="left" valign="top"><simpara>Select up to 9 columns to display in the table. The <emphasis role="strong">Namespace</emphasis> column is only displayed when <emphasis role="strong">All Projects</emphasis> is selected from the <emphasis role="strong">Projects</emphasis> list.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">MigrationPolicies</emphasis> table</simpara></entry>
<entry align="left" valign="top"><simpara>List of migration policies.</simpara>
<simpara>Click the actions menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a migration policy to select <emphasis role="strong">Edit</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<simpara>Click a migration policy to view the <emphasis role="strong">MigrationPolicy details</emphasis> page.</simpara>
<section xml:id="migrationpolicy-details-page_virt-web-console-overview">
<title>MigrationPolicy details page</title>
<simpara>You configure a migration policy on the <emphasis role="strong">MigrationPolicy details</emphasis> page.</simpara>
<example>
<title><emphasis role="strong">MigrationPolicy details</emphasis> page</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Details</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure a migration policy by editing a form.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">YAML</emphasis> tab</simpara></entry>
<entry align="left" valign="top"><simpara>Configure a migration policy by editing a YAML configuration file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Actions</emphasis> menu</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Edit</emphasis> or <emphasis role="strong">Delete</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
<section xml:id="migrationpolicy-details-details_virt-web-console-overview">
<title>Details tab</title>
<simpara>You configure a custom template on the <emphasis role="strong">Details</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">Details</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Name</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Migration policy name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Description</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Migration policy description.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Configurations</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click the edit icon to update the migration policy configurations.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Bandwidth per migration</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Bandwidth request per migration. For unlimited bandwidth, set the value to <literal>0</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Auto converge</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>When auto converge is enabled, the performance and availability of the virtual machines might be reduced to ensure that migration is successful.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Post-copy</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Post-copy policy.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Completion timeout</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Completion timeout value in seconds.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Project labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click <emphasis role="strong">Edit</emphasis> to edit the project labels.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachine labels</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>Click <emphasis role="strong">Edit</emphasis> to edit the virtual machine labels.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
<section xml:id="migrationpolicy-details-yaml_virt-web-console-overview">
<title>YAML tab</title>
<simpara>You configure the migration polic by editing the YAML file on the <emphasis role="strong">YAML</emphasis> tab.</simpara>
<example>
<title><emphasis role="strong">YAML</emphasis> tab</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Element</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Save</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Save changes to the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Reload</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Discard your changes and reload the YAML file.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Cancel</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Exit the <emphasis role="strong">YAML</emphasis> tab.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Download</emphasis> button</simpara></entry>
<entry align="left" valign="top"><simpara>Download the YAML file to your local machine.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</example>
</section>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_installing">
<title>Installing</title>
<section xml:id="preparing-cluster-for-virt">
<title>Preparing your cluster for OpenShift Virtualization</title>

<simpara>Review this section before you install OpenShift Virtualization to ensure that your cluster meets the requirements.</simpara>
<important>
<variablelist>
<varlistentry>
<term>Installation method considerations</term>
<listitem>
<simpara>You can use any installation method, including user-provisioned, installer-provisioned, or assisted installer, to deploy OpenShift Container Platform. However, the installation method and the cluster topology might affect OpenShift Virtualization functionality, such as snapshots or <link linkend="live-migration_preparing-cluster-for-virt">live migration</link>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Red Hat OpenShift Data Foundation</term>
<listitem>
<simpara>If you deploy OpenShift Virtualization with Red Hat OpenShift Data Foundation, you must create a dedicated storage class for Windows virtual machine disks. See <link xlink:href="https://access.redhat.com/articles/6978371">Optimizing ODF PersistentVolumes for Windows VMs</link> for details.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>IPv6</term>
<listitem>
<simpara>You cannot run OpenShift Virtualization on a single-stack IPv6 cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
</important>
<formalpara>
<title>FIPS mode</title>
<para>If you install your cluster in <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-fips-mode_installing-fips">FIPS mode</link>, no additional setup is required for OpenShift Virtualization.</para>
</formalpara>
<section xml:id="supported-platforms_preparing-cluster-for-virt">
<title>Supported platforms</title>
<simpara>You can use the following platforms with OpenShift Virtualization:</simpara>
<itemizedlist>
<listitem>
<simpara>On-premise bare metal servers. See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#virt-planning-bare-metal-cluster-for-ocp-virt_preparing-to-install-on-bare-metal">Planning a bare metal cluster for OpenShift Virtualization</link>.</simpara>
</listitem>
<listitem>
<simpara>Amazon Web Services bare metal instances. See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#installing-aws-customizations">Installing a cluster on AWS with customizations</link>.</simpara>
</listitem>
<listitem>
<simpara>IBM Cloud&#174; Bare Metal Servers. See <link xlink:href="https://access.redhat.com/articles/6738731">Deploy OpenShift Virtualization on IBM Cloud&#174; Bare Metal nodes</link>.</simpara>
<important>
<simpara>Installing OpenShift Virtualization on IBM Cloud&#174; Bare Metal Servers is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
</listitem>
</itemizedlist>
<simpara>Bare metal instances or servers offered by other cloud providers are not supported.</simpara>
<section xml:id="virt-aws-bm_preparing-cluster-for-virt">
<title>OpenShift Virtualization on AWS bare metal</title>
<simpara>You can run OpenShift Virtualization on an Amazon Web Services (AWS) bare-metal OpenShift Container Platform cluster.</simpara>
<note>
<simpara>OpenShift Virtualization is also supported on Red Hat OpenShift Service on AWS (ROSA) Classic clusters, which have the same configuration requirements as AWS bare-metal clusters.</simpara>
</note>
<simpara>Before you set up your cluster, review the following summary of supported features and limitations:</simpara>
<variablelist>
<varlistentry>
<term>Installing</term>
<listitem>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist>
<listitem>
<simpara>You can install the cluster by using installer-provisioned infrastructure, ensuring that you specify bare-metal instance types for the worker nodes by editing the <literal>install-config.yaml</literal> file. For example, you can use the <literal>c5n.metal</literal> type value for a machine based on x86_64 architecture.</simpara>
<simpara>For more information, see the OpenShift Container Platform documentation about installing on AWS.</simpara>
</listitem>
</itemizedlist>
<variablelist>
<varlistentry>
<term>Accessing virtual machines (VMs)</term>
<listitem>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist>
<listitem>
<simpara>There is no change to how you access VMs by using the <literal>virtctl</literal> CLI tool or the OpenShift Container Platform web console.</simpara>
</listitem>
<listitem>
<simpara>You can expose VMs by using a <literal>NodePort</literal> or <literal>LoadBalancer</literal> service.</simpara>
<itemizedlist>
<listitem>
<simpara>The load balancer approach is preferable because OpenShift Container Platform automatically creates the load balancer in AWS and manages its lifecycle. A security group is also created for the load balancer, and you can use annotations to attach existing security groups. When you remove the service, OpenShift Container Platform removes the load balancer and its associated resources.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<variablelist>
<varlistentry>
<term>Networking</term>
<listitem>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist>
<listitem>
<simpara>You cannot use Single Root I/O Virtualization (SR-IOV) or bridge Container Network Interface (CNI) networks, including virtual LAN (VLAN). If your application requires a flat layer 2 network or control over the IP pool, consider using OVN-Kubernetes secondary overlay networks.</simpara>
</listitem>
</itemizedlist>
<variablelist>
<varlistentry>
<term>Storage</term>
<listitem>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist>
<listitem>
<simpara>You can use any storage solution that is certified by the storage vendor to work with the underlying platform.</simpara>
<important>
<simpara>AWS bare-metal and ROSA clusters might have different supported storage solutions. Ensure that you confirm support with your storage vendor.</simpara>
</important>
</listitem>
<listitem>
<simpara>Amazon Elastic File System (EFS) and Amazon Elastic Block Store (EBS) are not supported for use with OpenShift Virtualization due to performance and functionality limitations.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="virt-connecting-vm-to-ovn-secondary-network">Connecting a virtual machine to an OVN-Kubernetes secondary network</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-exposing-vm-with-service">Exposing a virtual machine by using a service</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-hardware-os-requirements_preparing-cluster-for-virt">
<title>Hardware and operating system requirements</title>
<simpara>Review the following hardware and operating system requirements for OpenShift Virtualization.</simpara>
<section xml:id="cpu-requirements_preparing-cluster-for-virt">
<title>CPU requirements</title>
<itemizedlist>
<listitem>
<simpara>Supported by Red Hat Enterprise Linux (RHEL) 9.</simpara>
<simpara>See <link xlink:href="https://catalog.redhat.com">Red Hat Ecosystem Catalog</link> for supported CPUs.</simpara>
<note>
<simpara>If your worker nodes have different CPUs, live migration failures might occur because different CPUs have different capabilities. You can mitigate this issue by ensuring that your worker nodes have CPUs with the appropriate capacity and by configuring node affinity rules for your virtual machines.</simpara>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-affinity-configuring-required_nodes-scheduler-node-affinity">Configuring a required node affinity rule</link> for details.</simpara>
</note>
</listitem>
<listitem>
<simpara>Support for AMD and Intel 64-bit architectures (x86-64-v2).</simpara>
</listitem>
<listitem>
<simpara>Support for Intel 64 or AMD64 CPU extensions.</simpara>
</listitem>
<listitem>
<simpara>Intel VT or AMD-V hardware virtualization extensions enabled.</simpara>
</listitem>
<listitem>
<simpara>NX (no execute) flag enabled.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="os-requirements_preparing-cluster-for-virt">
<title>Operating system requirements</title>
<itemizedlist>
<listitem>
<simpara>Red Hat Enterprise Linux CoreOS (RHCOS) installed on worker nodes.</simpara>
<simpara>See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/architecture/#rhcos-about_architecture-rhcos">About RHCOS</link> for details.</simpara>
<note>
<simpara>RHEL worker nodes are not supported.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="storage-requirements_preparing-cluster-for-virt">
<title>Storage requirements</title>
<itemizedlist>
<listitem>
<simpara>Supported by OpenShift Container Platform. See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#_optimizing-storage">Optimizing storage</link>.</simpara>
</listitem>
<listitem>
<simpara>You must create a default OpenShift Virtualization or OpenShift Container Platform storage class. The purpose of this is to address the unique storage needs of VM workloads and offer optimized performance, reliability, and user experience. If both OpenShift Virtualization and OpenShift Container Platform default storage classes exist, the OpenShift Virtualization class takes precedence when creating VM disks.</simpara>
<note>
<simpara>To mark a storage class as the default for virtualization workloads, set the annotation <literal>storageclass.kubevirt.io/is-default-virt-class</literal> to <literal>"true"</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>If the storage provisioner supports snapshots, you must associate a <literal>VolumeSnapshotClass</literal> object with the default storage class.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-about-storage-volumes-for-vm-disks_preparing-cluster-for-virt">
<title>About volume and access modes for virtual machine disks</title>
<simpara>If you use the storage API with known storage providers, the volume and access modes are selected automatically. However, if you use a storage class that does not have a storage profile, you must configure the volume and access mode.</simpara>
<simpara>For best results, use the <literal>ReadWriteMany</literal> (RWX) access mode and the <literal>Block</literal> volume mode. This is important for the following reasons:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>ReadWriteMany</literal> (RWX) access mode is required for live migration.</simpara>
</listitem>
<listitem>
<simpara>The <literal>Block</literal> volume mode performs significantly better than the <literal>Filesystem</literal> volume mode. This is because the <literal>Filesystem</literal> volume mode uses more storage layers, including a file system layer and a disk image file. These layers are not necessary for VM disk storage.</simpara>
<simpara>For example, if you use Red Hat OpenShift Data Foundation, Ceph RBD volumes are preferable to CephFS volumes.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>You cannot live migrate virtual machines with the following configurations:</simpara>
<itemizedlist>
<listitem>
<simpara>Storage volume with <literal>ReadWriteOnce</literal> (RWO) access mode</simpara>
</listitem>
<listitem>
<simpara>Passthrough features such as GPUs</simpara>
</listitem>
</itemizedlist>
<simpara>Do not set the <literal>evictionStrategy</literal> field to <literal>LiveMigrate</literal> for these virtual machines.</simpara>
</important>
</section>
</section>
</section>
<section xml:id="live-migration_preparing-cluster-for-virt">
<title>Live migration requirements</title>
<itemizedlist>
<listitem>
<simpara>Shared storage with <literal>ReadWriteMany</literal> (RWX) access mode.</simpara>
</listitem>
<listitem>
<simpara>Sufficient RAM and network bandwidth.</simpara>
<note>
<simpara>You must ensure that there is enough memory request capacity in the cluster to support node drains that result in live migrations. You can determine the approximate required spare memory by using the following calculation:</simpara>
<screen>Product of (Maximum number of nodes that can drain in parallel) and (Highest total VM memory request allocations across nodes)</screen>
<simpara>The default <link xlink:href="../../virt/live_migration/virt-configuring-live-migration.xml#virt-configuring-live-migration-limits_virt-configuring-live-migration">number of migrations that can run in parallel</link> in the cluster is 5.</simpara>
</note>
</listitem>
<listitem>
<simpara>If the virtual machine uses a host model CPU, the nodes must support the virtual machine&#8217;s host model CPU.</simpara>
</listitem>
<listitem>
<simpara>A <link linkend="virt-dedicated-network-live-migration">dedicated Multus network</link> for live migration is highly recommended. A dedicated network minimizes the effects of network saturation on tenant workloads during migration.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-cluster-resource-requirements_preparing-cluster-for-virt">
<title>Physical resource overhead requirements</title>
<simpara>OpenShift Virtualization is an add-on to OpenShift Container Platform and imposes additional overhead that you must account for when planning a cluster. Each cluster machine must accommodate the following overhead requirements in addition to the OpenShift Container Platform requirements. Oversubscribing the physical resources in a cluster can affect performance.</simpara>
<important>
<simpara>The numbers noted in this documentation are based on Red Hat&#8217;s test methodology and setup. These numbers can vary based on your own individual setup and environments.</simpara>
</important>
<bridgehead xml:id="memory-overhead_preparing-cluster-for-virt" renderas="sect4">Memory overhead</bridgehead>
<simpara>Calculate the memory overhead values for OpenShift Virtualization by using the equations below.</simpara>
<formalpara>
<title>Cluster memory overhead</title>
<para>
<screen>Memory overhead per infrastructure node â‰ˆ 150 MiB</screen>
</para>
</formalpara>
<screen>Memory overhead per worker node â‰ˆ 360 MiB</screen>
<simpara>Additionally, OpenShift Virtualization environment resources require a total of 2179 MiB of RAM that is spread across all infrastructure nodes.</simpara>
<formalpara>
<title>Virtual machine memory overhead</title>
<para>
<screen>Memory overhead per virtual machine â‰ˆ (1.002 Ã— requested memory) \
              + 218 MiB \ <co xml:id="CO2-1"/>
              + 8 MiB Ã— (number of vCPUs) \ <co xml:id="CO2-2"/>
              + 16 MiB Ã— (number of graphics devices) \ <co xml:id="CO2-3"/>
              + (additional memory overhead) <co xml:id="CO2-4"/></screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO2-1">
<para>Required for the processes that run in the <literal>virt-launcher</literal> pod.</para>
</callout>
<callout arearefs="CO2-2">
<para>Number of virtual CPUs requested by the virtual machine.</para>
</callout>
<callout arearefs="CO2-3">
<para>Number of virtual graphics cards requested by the virtual machine.</para>
</callout>
<callout arearefs="CO2-4">
<para>Additional memory overhead:</para>
<itemizedlist>
<listitem>
<simpara>If your environment includes a Single Root I/O Virtualization (SR-IOV) network device or a Graphics Processing Unit (GPU), allocate 1 GiB additional memory overhead for each device.</simpara>
</listitem>
<listitem>
<simpara>If Secure Encrypted Virtualization (SEV) is enabled, add 256 MiB.</simpara>
</listitem>
<listitem>
<simpara>If Trusted Platform Module (TPM) is enabled, add 53 MiB.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
<bridgehead xml:id="CPU-overhead_preparing-cluster-for-virt" renderas="sect4">CPU overhead</bridgehead>
<simpara>Calculate the cluster processor overhead requirements for OpenShift Virtualization by using the equation below. The CPU overhead per virtual machine depends on your individual setup.</simpara>
<formalpara>
<title>Cluster CPU overhead</title>
<para>
<screen>CPU overhead for infrastructure nodes â‰ˆ 4 cores</screen>
</para>
</formalpara>
<simpara>OpenShift Virtualization increases the overall utilization of cluster level services such as logging, routing, and monitoring. To account for this workload, ensure that nodes that host infrastructure components have capacity allocated for 4 additional cores (4000 millicores) distributed across those nodes.</simpara>
<screen>CPU overhead for worker nodes â‰ˆ 2 cores + CPU overhead per virtual machine</screen>
<simpara>Each worker node that hosts virtual machines must have capacity for 2 additional cores (2000 millicores) for OpenShift Virtualization management workloads in addition to the CPUs required for virtual machine workloads.</simpara>
<formalpara>
<title>Virtual machine CPU overhead</title>
<para>If dedicated CPUs are requested, there is a 1:1 impact on the cluster CPU overhead requirement. Otherwise, there are no specific rules about how many CPUs a virtual machine requires.</para>
</formalpara>
<bridgehead xml:id="storage-overhead_preparing-cluster-for-virt" renderas="sect4">Storage overhead</bridgehead>
<simpara>Use the guidelines below to estimate storage overhead requirements for your OpenShift Virtualization environment.</simpara>
<formalpara>
<title>Cluster storage overhead</title>
<para>
<screen>Aggregated storage overhead per node â‰ˆ 10 GiB</screen>
</para>
</formalpara>
<simpara>10 GiB is the estimated on-disk storage impact for each node in the cluster when you install OpenShift Virtualization.</simpara>
<formalpara>
<title>Virtual machine storage overhead</title>
<para>Storage overhead per virtual machine depends on specific requests for resource allocation within the virtual machine. The request could be for ephemeral storage on the node or storage resources hosted elsewhere in the cluster. OpenShift Virtualization does not currently allocate any additional ephemeral storage for the running container itself.</para>
</formalpara>
<formalpara>
<title>Example</title>
<para>As a cluster administrator, if you plan to host 10 virtual machines in the cluster, each with 1 GiB of RAM and 2 vCPUs, the memory impact across the cluster is 11.68 GiB. The estimated on-disk storage impact for each node in the cluster is 10 GiB and the CPU impact for worker nodes that host virtual machine workloads is a minimum of 2 cores.</para>
</formalpara>
</section>
<section xml:id="virt-sno-differences_preparing-cluster-for-virt">
<title>Single-node OpenShift differences</title>
<simpara>You can install OpenShift Virtualization on single-node OpenShift.</simpara>
<simpara>However, you should be aware that Single-node OpenShift does not support the following features:</simpara>
<itemizedlist>
<listitem>
<simpara>High availability</simpara>
</listitem>
<listitem>
<simpara>Pod disruption</simpara>
</listitem>
<listitem>
<simpara>Live migration</simpara>
</listitem>
<listitem>
<simpara>Virtual machines or templates that have an eviction strategy configured</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#openshift-storage-common-terms_storage-overview">Glossary of common terms for OpenShift Container Platform storage</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="object-maximums_preparing-cluster-for-virt">
<title>Object maximums</title>
<simpara>You must consider the following tested object maximums when planning your cluster:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#planning-your-environment-according-to-object-maximums">OpenShift Container Platform object maximums</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/articles/6571671">OpenShift Virtualization object maximums</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="cluster-high-availability-options_preparing-cluster-for-virt">
<title>Cluster high-availability options</title>
<simpara>You can configure one of the following high-availability (HA) options for your cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Automatic high availability for <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#ipi-install-overview">installer-provisioned infrastructure</link> (IPI) is available by deploying <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/machine_management/#machine-health-checks-about_deploying-machine-health-checks">machine health checks</link>.</simpara>
<note>
<simpara>In OpenShift Container Platform clusters installed using installer-provisioned infrastructure and with a properly configured <literal>MachineHealthCheck</literal> resource, if a node fails the machine health check and becomes unavailable to the cluster, it is recycled. What happens next with VMs that ran on the failed node depends on a series of conditions. See <link linkend="run-strategies">Run strategies</link> for more detailed information about the potential outcomes and how run strategies affect those outcomes.</simpara>
</note>
</listitem>
<listitem>
<simpara>Automatic high availability for both IPI and non-IPI is available by using the <emphasis role="strong">Node Health Check Operator</emphasis> on the OpenShift Container Platform cluster to deploy the <literal>NodeHealthCheck</literal> controller. The controller identifies unhealthy nodes and uses the Self Node Remediation Operator to remediate the unhealthy nodes. For more information on remediation, fencing, and maintaining nodes, see the <link xlink:href="https://access.redhat.com/documentation/en-us/workload_availability_for_red_hat_openshift/23.2/html-single/remediation_fencing_and_maintenance/index#about-remediation-fencing-maintenance">Workload Availability for Red Hat OpenShift</link> documentation.</simpara>
<important>
<simpara>Node Health Check Operator is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
</listitem>
<listitem>
<simpara>High availability for any platform is available by using either a monitoring system or a qualified human to monitor node availability. When a node is lost, shut it down and run <literal>oc delete node &lt;lost_node&gt;</literal>.</simpara>
<note>
<simpara>Without an external monitoring system or a qualified human monitoring node health, virtual machines lose high availability.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="installing-virt">
<title>Installing OpenShift Virtualization</title>

<simpara>Install OpenShift Virtualization to add virtualization functionality to your OpenShift Container Platform cluster.</simpara>
<important>
<simpara>If you install OpenShift Virtualization in a restricted environment with no internet connectivity, you must <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-restricted-networks">configure Operator Lifecycle Manager (OLM) for restricted networks</link>.</simpara>
<simpara>If you have limited internet connectivity, you can <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-configuring-proxy-support">configure proxy support in OLM</link> to access the OperatorHub.</simpara>
</important>
<section xml:id="installing-virt-operator_installing-virt">
<title>Installing the OpenShift Virtualization Operator</title>
<simpara>Install the OpenShift Virtualization Operator by using the OpenShift Container Platform web console or the command line.</simpara>
<section xml:id="virt-installing-virt-operator_installing-virt">
<title>Installing the OpenShift Virtualization Operator by using the web console</title>
<simpara>You can deploy the OpenShift Virtualization Operator by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install OpenShift Container Platform 4.14 on your cluster.</simpara>
</listitem>
<listitem>
<simpara>Log in to the OpenShift Container Platform web console as a user with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the <emphasis role="strong">Administrator</emphasis> perspective, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Filter by keyword</emphasis> field, type <emphasis role="strong">Virtualization</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">OpenShift Virtualization Operator</emphasis> tile with the <emphasis role="strong">Red Hat</emphasis> source label.</simpara>
</listitem>
<listitem>
<simpara>Read the information about the Operator and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Select <emphasis role="strong">stable</emphasis> from the list of available <emphasis role="strong">Update Channel</emphasis> options. This ensures that you install the version of OpenShift Virtualization that is compatible with your OpenShift Container Platform version.</simpara>
</listitem>
<listitem>
<simpara>For <emphasis role="strong">Installed Namespace</emphasis>, ensure that the <emphasis role="strong">Operator recommended namespace</emphasis> option is selected. This installs the Operator in the mandatory <literal>openshift-cnv</literal> namespace, which is automatically created if it does not exist.</simpara>
<warning>
<simpara>Attempting to install the OpenShift Virtualization Operator in a namespace other than <literal>openshift-cnv</literal> causes the installation to fail.</simpara>
</warning>
</listitem>
<listitem>
<simpara>For <emphasis role="strong">Approval Strategy</emphasis>, it is highly recommended that you select <emphasis role="strong">Automatic</emphasis>, which is the default value, so that OpenShift Virtualization automatically updates when a new version is available in the <emphasis role="strong">stable</emphasis> update channel.</simpara>
<simpara>While it is possible to select the <emphasis role="strong">Manual</emphasis> approval strategy, this is inadvisable because of the high risk that it presents to the supportability and functionality of your cluster. Only select <emphasis role="strong">Manual</emphasis> if you fully understand these risks and cannot use <emphasis role="strong">Automatic</emphasis>.</simpara>
<warning>
<simpara>Because OpenShift Virtualization is only supported when used with the corresponding OpenShift Container Platform version, missing OpenShift Virtualization updates can cause your cluster to become unsupported.</simpara>
</warning>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis> to make the Operator available to the <literal>openshift-cnv</literal> namespace.</simpara>
</listitem>
<listitem>
<simpara>When the Operator installs successfully, click <emphasis role="strong">Create HyperConverged</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Optional: Configure <emphasis role="strong">Infra</emphasis> and <emphasis role="strong">Workloads</emphasis> node placement options for OpenShift Virtualization components.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis> to launch OpenShift Virtualization.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and monitor the OpenShift Virtualization pods until they are all <emphasis role="strong">Running</emphasis>. After all the pods display the <emphasis role="strong">Running</emphasis> state, you can use OpenShift Virtualization.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="installing-virt-operator-cli_installing-virt">
<title>Installing the OpenShift Virtualization Operator by using the command line</title>
<simpara>Subscribe to the OpenShift Virtualization catalog and install the OpenShift Virtualization Operator by applying manifests to your cluster.</simpara>
<section xml:id="virt-subscribing-cli_installing-virt">
<title>Subscribing to the OpenShift Virtualization catalog by using the CLI</title>
<simpara>Before you install OpenShift Virtualization, you must subscribe to the OpenShift Virtualization catalog. Subscribing gives the <literal>openshift-cnv</literal> namespace access to the OpenShift Virtualization Operators.</simpara>
<simpara>To subscribe, configure <literal>Namespace</literal>, <literal>OperatorGroup</literal>, and <literal>Subscription</literal> objects by applying a single manifest to your cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install OpenShift Container Platform 4.14 on your cluster.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file that contains the following manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Namespace
metadata:
  name: openshift-cnv
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: kubevirt-hyperconverged-group
  namespace: openshift-cnv
spec:
  targetNamespaces:
    - openshift-cnv
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: hco-operatorhub
  namespace: openshift-cnv
spec:
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  name: kubevirt-hyperconverged
  startingCSV: kubevirt-hyperconverged-operator.v4.15.0
  channel: "stable" <co xml:id="CO3-1"/></programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para>Using the <literal>stable</literal> channel ensures that you install the version of
OpenShift Virtualization that is compatible with your OpenShift Container Platform version.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the required <literal>Namespace</literal>, <literal>OperatorGroup</literal>, and <literal>Subscription</literal> objects
for OpenShift Virtualization by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;file name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<note>
<simpara>You can <link linkend="virt-configuring-certificate-rotation">configure certificate rotation</link> parameters in the YAML file.</simpara>
</note>
</section>
<section xml:id="virt-deploying-operator-cli_installing-virt">
<title>Deploying the OpenShift Virtualization Operator by using the CLI</title>
<simpara>You can deploy the OpenShift Virtualization Operator by using the <literal>oc</literal> CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An active subscription to the OpenShift Virtualization catalog in the <literal>openshift-cnv</literal> namespace.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file that contains the following manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:</programlisting>
</listitem>
<listitem>
<simpara>Deploy the OpenShift Virtualization Operator by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Ensure that OpenShift Virtualization deployed successfully by watching the <literal>PHASE</literal> of the cluster service version (CSV) in the <literal>openshift-cnv</literal> namespace. Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch oc get csv -n openshift-cnv</programlisting>
<simpara>The following output displays if deployment was successful:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                      DISPLAY                    VERSION   REPLACES   PHASE
kubevirt-hyperconverged-operator.v4.15.0   OpenShift Virtualization   4.15.0                Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installing-virt-web-next-steps">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara>The <link linkend="virt-creating-hpp-basic-storage-pool_virt-configuring-local-storage-with-hpp">hostpath provisioner</link> is a local storage provisioner designed for OpenShift Virtualization. If you want to configure local storage for virtual machines, you must enable the hostpath provisioner first.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="uninstalling-virt">
<title>Uninstalling OpenShift Virtualization</title>

<simpara>You uninstall OpenShift Virtualization by using the web console or the command line interface (CLI) to delete the OpenShift Virtualization workloads, the Operator, and its resources.</simpara>
<section xml:id="uninstalling-virt-web-console_uninstalling-virt">
<title>Uninstalling OpenShift Virtualization by using the web console</title>
<simpara>You uninstall OpenShift Virtualization by using the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/web_console/#web-console-overview_web-console">web console</link> to perform the following tasks:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="virt-deleting-deployment-custom-resource_uninstalling-virt">Delete the <literal>HyperConverged</literal> CR</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="olm-deleting-operators-from-a-cluster-using-web-console_uninstalling-virt">Delete the OpenShift Virtualization Operator</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="deleting-a-namespace-using-the-web-console_uninstalling-virt">Delete the <literal>openshift-cnv</literal> namespace</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-deleting-virt-crds-web_uninstalling-virt">Delete the OpenShift Virtualization custom resource definitions (CRDs)</link>.</simpara>
</listitem>
</orderedlist>
<important>
<simpara>You must first delete all <link linkend="virt-delete-vm-web_virt-delete-vms">virtual machines</link>, and <link linkend="virt-deleting-vmis-cli_virt-manage-vmis">virtual machine instances</link>.</simpara>
<simpara>You cannot uninstall OpenShift Virtualization while its workloads remain on the cluster.</simpara>
</important>
<section xml:id="virt-deleting-deployment-custom-resource_uninstalling-virt">
<title>Deleting the HyperConverged custom resource</title>
<simpara>To uninstall OpenShift Virtualization, you first delete the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster using an account with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Select the OpenShift Virtualization Operator.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">OpenShift Virtualization Deployment</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside <literal>kubevirt-hyperconverged</literal> and select <emphasis role="strong">Delete HyperConverged</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis> in the confirmation window.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="olm-deleting-operators-from-a-cluster-using-web-console_uninstalling-virt">
<title>Deleting Operators from a cluster using the web console</title>
<simpara>Cluster administrators can delete installed Operators from a selected namespace by using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster web console using an account with
<literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> â†’ <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Scroll or enter a keyword into the <emphasis role="strong">Filter by name</emphasis> field to find the Operator that you want to remove. Then, click on it.</simpara>
</listitem>
<listitem>
<simpara>On the right side of the <emphasis role="strong">Operator Details</emphasis> page, select <emphasis role="strong">Uninstall Operator</emphasis> from the <emphasis role="strong">Actions</emphasis> list.</simpara>
<simpara>An <emphasis role="strong">Uninstall Operator?</emphasis> dialog box is displayed.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Uninstall</emphasis> to remove the Operator, Operator deployments, and pods. Following this action, the Operator stops running and no longer receives updates.</simpara>
<note>
<simpara>This action does not remove resources managed by the Operator, including custom resource definitions (CRDs) and custom resources (CRs). Dashboards and navigation items enabled by the web console and off-cluster resources that continue to run might need manual clean up. To remove these after uninstalling the Operator, you might need to manually delete the Operator CRDs.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="deleting-a-namespace-using-the-web-console_uninstalling-virt">
<title>Deleting a namespace using the web console</title>
<simpara>You can delete a namespace by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster using an account with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Namespaces</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Locate the namespace that you want to delete in the list of namespaces.</simpara>
</listitem>
<listitem>
<simpara>On the far right side of the namespace listing, select <emphasis role="strong">Delete Namespace</emphasis> from the
Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject>.</simpara>
</listitem>
<listitem>
<simpara>When the <emphasis role="strong">Delete Namespace</emphasis> pane opens, enter the name of the namespace that
you want to delete in the field.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-deleting-virt-crds-web_uninstalling-virt">
<title>Deleting OpenShift Virtualization custom resource definitions</title>
<simpara>You can delete the OpenShift Virtualization custom resource definitions (CRDs) by using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster using an account with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">CustomResourceDefinitions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Label</emphasis> filter and enter <literal>operators.coreos.com/kubevirt-hyperconverged.openshift-cnv</literal> in the <emphasis role="strong">Search</emphasis> field to display the OpenShift Virtualization CRDs.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside each CRD and select <emphasis role="strong">Delete CustomResourceDefinition</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-deleting-virt-cli_uninstalling-virt">
<title>Uninstalling OpenShift Virtualization by using the CLI</title>
<simpara>You can uninstall OpenShift Virtualization by using the OpenShift CLI (<literal>oc</literal>).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster using an account with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have deleted all virtual machines and virtual machine instances. You cannot uninstall OpenShift Virtualization while its workloads remain on the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>HyperConverged</literal> custom resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete HyperConverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Delete the OpenShift Virtualization Operator subscription:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete subscription kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Delete the OpenShift Virtualization <literal>ClusterServiceVersion</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete csv -n openshift-cnv -l operators.coreos.com/kubevirt-hyperconverged.openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Delete the OpenShift Virtualization namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete namespace openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>List the OpenShift Virtualization custom resource definitions (CRDs) by running the <literal>oc delete crd</literal> command with the <literal>dry-run</literal> option:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd --dry-run=client -l operators.coreos.com/kubevirt-hyperconverged.openshift-cnv</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>customresourcedefinition.apiextensions.k8s.io "cdis.cdi.kubevirt.io" deleted (dry run)
customresourcedefinition.apiextensions.k8s.io "hostpathprovisioners.hostpathprovisioner.kubevirt.io" deleted (dry run)
customresourcedefinition.apiextensions.k8s.io "hyperconvergeds.hco.kubevirt.io" deleted (dry run)
customresourcedefinition.apiextensions.k8s.io "kubevirts.kubevirt.io" deleted (dry run)
customresourcedefinition.apiextensions.k8s.io "networkaddonsconfigs.networkaddonsoperator.network.kubevirt.io" deleted (dry run)
customresourcedefinition.apiextensions.k8s.io "ssps.ssp.kubevirt.io" deleted (dry run)
customresourcedefinition.apiextensions.k8s.io "tektontasks.tektontasks.kubevirt.io" deleted (dry run)</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the CRDs by running the <literal>oc delete crd</literal> command without the <literal>dry-run</literal> option:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete crd -l operators.coreos.com/kubevirt-hyperconverged.openshift-cnv</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="virt-delete-vm-web_virt-delete-vms">Deleting virtual machines</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-deleting-vmis-cli_virt-manage-vmis">Deleting virtual machine instances</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_postinstallation-configuration">
<title>Postinstallation configuration</title>
<section xml:id="virt-post-install-config">
<title>Postinstallation configuration</title>

<simpara>The following procedures are typically performed after OpenShift Virtualization is installed. You can configure the components that are relevant for your environment:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-node-placement-virt-components">Node placement rules for OpenShift Virtualization Operators, workloads, and controllers</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-post-install-network-config">Network configuration</link>:</simpara>
<itemizedlist>
<listitem>
<simpara>Installing the Kubernetes NMState and SR-IOV Operators</simpara>
</listitem>
<listitem>
<simpara>Configuring a Linux bridge network for external access to virtual machines (VMs)</simpara>
</listitem>
<listitem>
<simpara>Configuring a dedicated secondary network for live migration</simpara>
</listitem>
<listitem>
<simpara>Configuring an SR-IOV network</simpara>
</listitem>
<listitem>
<simpara>Enabling the creation of load balancer services by using the OpenShift Container Platform web console</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link linkend="virt-post-install-storage-config">Storage configuration</link>:</simpara>
<itemizedlist>
<listitem>
<simpara>Defining a default storage class for the Container Storage Interface (CSI)</simpara>
</listitem>
<listitem>
<simpara>Configuring local storage by using the Hostpath Provisioner (HPP)</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-node-placement-virt-components">
<title>Specifying nodes for OpenShift Virtualization components</title>

<simpara>The default scheduling for virtual machines (VMs) on bare metal nodes is appropriate. Optionally, you can specify the nodes where you want to deploy OpenShift Virtualization Operators, workloads, and controllers by configuring node placement rules.</simpara>
<note>
<simpara>You can configure node placement rules for some components after installing OpenShift Virtualization, but virtual machines cannot be present if you want to configure node placement rules for workloads.</simpara>
</note>
<section xml:id="virt-about-node-placement-virt-components_virt-node-placement-virt-components">
<title>About node placement rules for OpenShift Virtualization components</title>
<simpara>You can use node placement rules for the following tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Deploy virtual machines only on nodes intended for virtualization workloads.</simpara>
</listitem>
<listitem>
<simpara>Deploy Operators only on infrastructure nodes.</simpara>
</listitem>
<listitem>
<simpara>Maintain separation between workloads.</simpara>
</listitem>
</itemizedlist>
<simpara>Depending on the object, you can use one or more of the following rule types:</simpara>
<variablelist>
<varlistentry>
<term><literal>nodeSelector</literal></term>
<listitem>
<simpara>Allows pods to be scheduled on nodes that are labeled with the key-value pair or pairs that you specify in this field. The node must have labels that exactly match all listed pairs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>affinity</literal></term>
<listitem>
<simpara>Enables you to use more expressive syntax to set rules that match nodes with pods. Affinity also allows for more nuance in how the rules are applied. For example, you can specify that a rule is a preference, not a requirement. If a rule is a preference, pods are still scheduled when the rule is not satisfied.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>tolerations</literal></term>
<listitem>
<simpara>Allows pods to be scheduled on nodes that have matching taints. If a taint is applied to a node, that
node only accepts pods that tolerate the taint.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="virt-applying-node-place-rules_virt-node-placement-virt-components">
<title>Applying node placement rules</title>
<simpara>You can apply node placement rules by editing a <literal>Subscription</literal>, <literal>HyperConverged</literal>, or <literal>HostPathProvisioner</literal> object using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The <literal>oc</literal> CLI tool is installed.</simpara>
</listitem>
<listitem>
<simpara>You are logged in with cluster administrator permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the object in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit &lt;resource_type&gt; &lt;resource_name&gt; -n {CNVNamespace}</programlisting>
</listitem>
<listitem>
<simpara>Save the file to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-node-placement-rule-examples_virt-node-placement-virt-components">
<title>Node placement rule examples</title>
<simpara>You can specify node placement rules for a OpenShift Virtualization component by editing a <literal>Subscription</literal>, <literal>HyperConverged</literal>, or <literal>HostPathProvisioner</literal> object.</simpara>
<section xml:id="subscription-object-node-placement-rules_virt-node-placement-virt-components">
<title>Subscription object node placement rule examples</title>
<simpara>To specify the nodes where OLM deploys the OpenShift Virtualization Operators, edit the <literal>Subscription</literal> object during OpenShift Virtualization installation.</simpara>
<simpara>Currently, you cannot configure node placement rules for the <literal>Subscription</literal> object by using the web console.</simpara>
<simpara>The <literal>Subscription</literal> object does not support the <literal>affinity</literal> node pplacement rule.</simpara>
<formalpara>
<title>Example <literal>Subscription</literal> object with <literal>nodeSelector</literal> rule</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: hco-operatorhub
  namespace: openshift-cnv
spec:
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  name: kubevirt-hyperconverged
  startingCSV: kubevirt-hyperconverged-operator.v4.15.0
  channel: "stable"
  config:
    nodeSelector:
      example.io/example-infra-key: example-infra-value <co xml:id="CO4-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO4-1">
<para>OLM deploys the OpenShift Virtualization Operators on nodes labeled <literal>example.io/example-infra-key = example-infra-value</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example <literal>Subscription</literal> object with <literal>tolerations</literal> rule</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: hco-operatorhub
  namespace: openshift-cnv
spec:
  source:  redhat-operators
  sourceNamespace: openshift-marketplace
  name: kubevirt-hyperconverged
  startingCSV: kubevirt-hyperconverged-operator.v4.15.0
  channel: "stable"
  config:
    tolerations:
    - key: "key"
      operator: "Equal"
      value: "virtualization" <co xml:id="CO5-1"/>
      effect: "NoSchedule"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO5-1">
<para>OLM deploys OpenShift Virtualization Operators on nodes labeled <literal>key = virtualization:NoSchedule</literal> taint. Only pods with the matching tolerations are scheduled on these nodes.</para>
</callout>
</calloutlist>
</section>
<section xml:id="hyperconverged-object-node-placement-rules_virt-node-placement-virt-components">
<title>HyperConverged object node placement rule example</title>
<simpara>To specify the nodes where OpenShift Virtualization deploys its components, you can edit the <literal>nodePlacement</literal> object in the HyperConverged custom resource (CR) file that you create during OpenShift Virtualization installation.</simpara>
<formalpara>
<title>Example <literal>HyperConverged</literal> object with <literal>nodeSelector</literal> rule</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  infra:
    nodePlacement:
      nodeSelector:
        example.io/example-infra-key: example-infra-value <co xml:id="CO6-1"/>
  workloads:
    nodePlacement:
      nodeSelector:
        example.io/example-workloads-key: example-workloads-value <co xml:id="CO6-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO6-1">
<para>Infrastructure resources are placed on nodes labeled <literal>example.io/example-infra-key = example-infra-value</literal>.</para>
</callout>
<callout arearefs="CO6-2">
<para>workloads are placed on nodes labeled <literal>example.io/example-workloads-key = example-workloads-value</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example <literal>HyperConverged</literal> object with <literal>affinity</literal> rule</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  infra:
    nodePlacement:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: example.io/example-infra-key
                operator: In
                values:
                - example-infra-value <co xml:id="CO7-1"/>
  workloads:
    nodePlacement:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            nodeSelectorTerms:
            - matchExpressions:
              - key: example.io/example-workloads-key <co xml:id="CO7-2"/>
                operator: In
                values:
                - example-workloads-value
          preferredDuringSchedulingIgnoredDuringExecution:
          - weight: 1
            preference:
              matchExpressions:
              - key: example.io/num-cpus
                operator: Gt
                values:
                - 8 <co xml:id="CO7-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO7-1">
<para>Infrastructure resources are placed on nodes labeled <literal>example.io/example-infra-key = example-value</literal>.</para>
</callout>
<callout arearefs="CO7-2">
<para>workloads are placed on nodes labeled <literal>example.io/example-workloads-key = example-workloads-value</literal>.</para>
</callout>
<callout arearefs="CO7-3">
<para>Nodes that have more than eight CPUs are preferred for workloads, but if they are not available, pods are still scheduled.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example <literal>HyperConverged</literal> object with <literal>tolerations</literal> rule</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  workloads:
    nodePlacement:
      tolerations: <co xml:id="CO8-1"/>
      - key: "key"
        operator: "Equal"
        value: "virtualization"
        effect: "NoSchedule"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO8-1">
<para>Nodes reserved for OpenShift Virtualization components are labeled with the <literal>key = virtualization:NoSchedule</literal> taint. Only pods with matching tolerations are scheduled on reserved nodes.</para>
</callout>
</calloutlist>
</section>
<section xml:id="hostpathprovisioner-object-node-placement-rules_virt-node-placement-virt-components">
<title>HostPathProvisioner object node placement rule example</title>
<simpara>You can edit the <literal>HostPathProvisioner</literal> object directly or by using the web console.</simpara>
<warning>
<simpara>You must schedule the hostpath provisioner and the OpenShift Virtualization components on the same nodes. Otherwise, virtualization pods that use the hostpath provisioner cannot run. You cannot run virtual machines.</simpara>
</warning>
<simpara>After you deploy a virtual machine (VM) with the hostpath provisioner (HPP) storage class, you can remove the hostpath provisioner pod from the same node by using the node selector. However, you must first revert that change, at least for that specific node, and wait for the pod to run before trying to delete the VM.</simpara>
<simpara>You can configure node placement rules by specifying <literal>nodeSelector</literal>, <literal>affinity</literal>, or <literal>tolerations</literal> for the <literal>spec.workload</literal> field of the <literal>HostPathProvisioner</literal> object that you create when you install the hostpath provisioner.</simpara>
<formalpara>
<title>Example <literal>HostPathProvisioner</literal> object with <literal>nodeSelector</literal> rule</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  pathConfig:
    path: "&lt;/path/to/backing/directory&gt;"
    useNamingPrefix: false
  workload:
    nodeSelector:
      example.io/example-workloads-key: example-workloads-value <co xml:id="CO9-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO9-1">
<para>Workloads are placed on nodes labeled <literal>example.io/example-workloads-key = example-workloads-value</literal>.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="additional-resources_virt-node-placement-virt-components" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-specifying-nodes-for-vms">Specifying nodes for virtual machines</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-affinity">Controlling pod placement on nodes using node affinity rules</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-taints-tolerations">Controlling pod placement using node taints</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-post-install-network-config">
<title>Postinstallation network configuration</title>

<simpara>By default, OpenShift Virtualization is installed with a single, internal pod network.</simpara>
<simpara>After you install OpenShift Virtualization, you can install networking Operators and configure additional networks.</simpara>
<section xml:id="installing-operators">
<title>Installing networking Operators</title>
<simpara>You must install the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#k8s-nmstate-about-the-k8s-nmstate-operator">Kubernetes NMState Operator</link> to configure a Linux bridge network for live migration or external access to virtual machines (VMs).</simpara>
<simpara>You can install the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#about-sriov">SR-IOV Operator</link> to manage SR-IOV network devices and network attachments.</simpara>
<section xml:id="installing-the-kubernetes-nmstate-operator-web-console_virt-post-install-network-config">
<title>Installing the Kubernetes NMState Operator by using the web console</title>
<simpara>You can install the Kubernetes NMState Operator by using the web console. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Select <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the search field below <emphasis role="strong">All Items</emphasis>, enter <literal>nmstate</literal> and click <emphasis role="strong">Enter</emphasis> to search for the Kubernetes NMState Operator.</simpara>
</listitem>
<listitem>
<simpara>Click on the Kubernetes NMState Operator search result.</simpara>
</listitem>
<listitem>
<simpara>Click on <emphasis role="strong">Install</emphasis> to open the <emphasis role="strong">Install Operator</emphasis> window.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis> to install the Operator.</simpara>
</listitem>
<listitem>
<simpara>After the Operator finishes installing, click <emphasis role="strong">View Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create Instance</emphasis> to open the dialog box for creating an instance of <literal>kubernetes-nmstate</literal>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Name</emphasis> field of the dialog box, ensure the name of the instance is <literal>nmstate.</literal></simpara>
<note>
<simpara>The name restriction is a known issue. The instance is a singleton for the entire cluster.</simpara>
</note>
</listitem>
<listitem>
<simpara>Accept the default settings and click <emphasis role="strong">Create</emphasis> to create the instance.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Summary</title>
<para>Once complete, the Operator has deployed the NMState State Controller as a daemon set across all of the cluster nodes.</para>
</formalpara>
</section>
<section xml:id="installing-sr-iov-operator_virt-post-install-network-config">
<title>Installing the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can install the Single Root I/O Virtualization (SR-IOV) Network Operator by using the OpenShift Container Platform CLI or the web console.</simpara>
<section xml:id="install-operator-cli_virt-post-install-network-config">
<title>CLI: Installing the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can install the Operator using the CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>An account with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To create the <literal>openshift-sriov-network-operator</literal> namespace, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
  annotations:
    workload.openshift.io/allowed: management
EOF</programlisting>
</listitem>
<listitem>
<simpara>To create an OperatorGroup CR, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
EOF</programlisting>
</listitem>
<listitem>
<simpara>Subscribe to the SR-IOV Network Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command to get the OpenShift Container Platform major and minor version. It is required for the <literal>channel</literal> value in the next
step.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ OC_VERSION=$(oc version -o yaml | grep openshiftVersion | \
    grep -o '[0-9]*[.][0-9]*' | head -1)</programlisting>
</listitem>
<listitem>
<simpara>To create a Subscription CR for the SR-IOV Network Operator, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF| oc create -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subscription
  namespace: openshift-sriov-network-operator
spec:
  channel: "${OC_VERSION}"
  name: sriov-network-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To verify that the Operator is installed, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-sriov-network-operator \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Name                                         Phase
sriov-network-operator.4.14.0-202310121402   Succeeded</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="install-operator-web-console_virt-post-install-network-config">
<title>Web console: Installing the SR-IOV Network Operator</title>
<simpara>As a cluster administrator, you can install the Operator using the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>An account with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the SR-IOV Network Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">SR-IOV Network Operator</emphasis> from the list of available Operators, and then click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, under <emphasis role="strong">Installed Namespace</emphasis>, select <emphasis role="strong">Operator recommended Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the SR-IOV Network Operator is installed successfully:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Ensure that <emphasis role="strong">SR-IOV Network Operator</emphasis> is listed in the <emphasis role="strong">openshift-sriov-network-operator</emphasis> project with a <emphasis role="strong">Status</emphasis> of <emphasis role="strong">InstallSucceeded</emphasis>.</simpara>
<note>
<simpara>During installation an Operator might display a <emphasis role="strong">Failed</emphasis> status.
If the installation later succeeds with an <emphasis role="strong">InstallSucceeded</emphasis> message, you can ignore the <emphasis role="strong">Failed</emphasis> message.</simpara>
</note>
<simpara>If the Operator does not appear as installed, to troubleshoot further:</simpara>
<itemizedlist>
<listitem>
<simpara>Inspect the <emphasis role="strong">Operator Subscriptions</emphasis> and <emphasis role="strong">Install Plans</emphasis> tabs for any failure or errors under <emphasis role="strong">Status</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Workloads</emphasis> &#8594; <emphasis role="strong">Pods</emphasis> page and check the logs for pods in the <literal>openshift-sriov-network-operator</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Check the namespace of the YAML file. If the annotation is missing, you can add the annotation <literal>workload.openshift.io/allowed=management</literal> to the Operator namespace with the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate ns/openshift-sriov-network-operator workload.openshift.io/allowed=management</programlisting>
<note>
<simpara>For single-node OpenShift clusters, the annotation <literal>workload.openshift.io/allowed=management</literal> is required for the namespace.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="configuring-linux-bridge-network">
<title>Configuring a Linux bridge network</title>
<simpara>After you install the Kubernetes NMState Operator, you can configure a Linux bridge network for live migration or external access to virtual machines (VMs).</simpara>
<section xml:id="virt-creating-linux-bridge-nncp_virt-post-install-network-config">
<title>Creating a Linux bridge NNCP</title>
<simpara>You can create a <literal>NodeNetworkConfigurationPolicy</literal> (NNCP) manifest for a Linux bridge network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the Kubernetes NMState Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>NodeNetworkConfigurationPolicy</literal> manifest. This example includes sample values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <co xml:id="CO10-1"/>
spec:
  desiredState:
    interfaces:
      - name: br1 <co xml:id="CO10-2"/>
        description: Linux bridge with eth1 as a port <co xml:id="CO10-3"/>
        type: linux-bridge <co xml:id="CO10-4"/>
        state: up <co xml:id="CO10-5"/>
        ipv4:
          enabled: false <co xml:id="CO10-6"/>
        bridge:
          options:
            stp:
              enabled: false <co xml:id="CO10-7"/>
          port:
            - name: eth1 <co xml:id="CO10-8"/></programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO10-2">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO10-3">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO10-4">
<para>The type of interface. This example creates a bridge.</para>
</callout>
<callout arearefs="CO10-5">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO10-6">
<para>Disables IPv4 in this example.</para>
</callout>
<callout arearefs="CO10-7">
<para>Disables STP in this example.</para>
</callout>
<callout arearefs="CO10-8">
<para>The node NIC to which the bridge is attached.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-creating-linux-bridge-nad-web_virt-post-install-network-config">
<title>Creating a Linux bridge NAD by using the web console</title>
<simpara>You can create a network attachment definition (NAD) to provide layer-2 networking to pods and virtual machines by using the OpenShift Container Platform web console.</simpara>
<simpara>A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.</simpara>
<warning>
<simpara>Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.</simpara>
</warning>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, click <emphasis role="strong">Networking</emphasis> &#8594; <emphasis role="strong">NetworkAttachmentDefinitions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create Network Attachment Definition</emphasis>.</simpara>
<note>
<simpara>The network attachment definition must be in the same namespace as the pod or virtual machine.</simpara>
</note>
</listitem>
<listitem>
<simpara>Enter a unique <emphasis role="strong">Name</emphasis> and optional <emphasis role="strong">Description</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">CNV Linux bridge</emphasis> from the <emphasis role="strong">Network Type</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Enter the name of the bridge in the <emphasis role="strong">Bridge Name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: If the resource has VLAN IDs configured, enter the ID numbers in the <emphasis role="strong">VLAN Tag Number</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">MAC Spoof Check</emphasis> to enable MAC spoof filtering. This feature provides security against a MAC spoofing attack by allowing only a single MAC address to exit the pod.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="next-steps_configuring-linux-bridge-network">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-attaching-vm-secondary-network-cli_virt-connecting-vm-to-linux-bridge">Attaching a virtual machine (VM) to a Linux bridge network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-network-live-migration">
<title>Configuring a network for live migration</title>
<simpara>After you have configured a Linux bridge network, you can configure a dedicated network for live migration. A dedicated network minimizes the effects of network saturation on tenant workloads during live migration.</simpara>
<section xml:id="virt-configuring-secondary-network-vm-live-migration_virt-post-install-network-config">
<title>Configuring a dedicated secondary network for live migration</title>
<simpara>To configure a dedicated secondary network for live migration, you must first create a bridge network attachment definition (NAD) by using the CLI. Then, you add the name of the <literal>NetworkAttachmentDefinition</literal> object to the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You logged in to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Each node has at least two Network Interface Cards (NICs).</simpara>
</listitem>
<listitem>
<simpara>The NICs for live migration are connected to the same VLAN.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>NetworkAttachmentDefinition</literal> manifest according to the following example:</simpara>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: my-secondary-network <co xml:id="CO11-1"/>
  namespace: openshift-cnv <co xml:id="CO11-2"/>
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "migration-bridge",
    "type": "macvlan",
    "master": "eth1", <co xml:id="CO11-3"/>
    "mode": "bridge",
    "ipam": {
      "type": "whereabouts", <co xml:id="CO11-4"/>
      "range": "10.200.5.0/24" <co xml:id="CO11-5"/>
    }
  }'</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO11-1">
<para>Specify the name of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO11-2 CO11-3">
<para>Specify the name of the NIC to be used for live migration.</para>
</callout>
<callout arearefs="CO11-4">
<para>Specify the name of the CNI plugin that provides the network for the NAD.</para>
</callout>
<callout arearefs="CO11-5">
<para>Specify an IP address range for the secondary network. This range must not overlap the IP addresses of the main network.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Add the name of the <literal>NetworkAttachmentDefinition</literal> object to the <literal>spec.liveMigrationConfig</literal> stanza of the <literal>HyperConverged</literal> CR:</simpara>
<formalpara>
<title>Example <literal>HyperConverged</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  liveMigrationConfig:
    completionTimeoutPerGiB: 800
    network: &lt;network&gt; <co xml:id="CO12-1"/>
    parallelMigrationsPerCluster: 5
    parallelOutboundMigrationsPerNode: 2
    progressTimeout: 150
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO12-1">
<para>Specify the name of the Multus <literal>NetworkAttachmentDefinition</literal> object to be used for live migrations.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor. The <literal>virt-handler</literal> pods restart and connect to the secondary network.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>When the node that the virtual machine runs on is placed into maintenance mode, the VM automatically migrates to another node in the cluster. You can verify that the migration occurred over the secondary network and not the default pod network by checking the target IP address in the virtual machine instance (VMI) metadata.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi &lt;vmi_name&gt; -o jsonpath='{.status.migrationState.targetNodeAddress}'</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-selecting-migration-network-ui_virt-post-install-network-config">
<title>Selecting a dedicated network by using the web console</title>
<simpara>You can select a dedicated network for live migration by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured a Multus network for live migration.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization &gt; Overview</emphasis> in the OpenShift Container Platform web console.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Settings</emphasis> tab and then click <emphasis role="strong">Live migration</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the network from the <emphasis role="strong">Live migration network</emphasis> list.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-sriov-network">
<title>Configuring an SR-IOV network</title>
<simpara>After you install the SR-IOV Operator, you can configure an SR-IOV network.</simpara>
<section xml:id="nw-sriov-configuring-device_virt-post-install-network-config">
<title>Configuring SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator adds the <literal>SriovNetworkNodePolicy.sriovnetwork.openshift.io</literal> CustomResourceDefinition to OpenShift Container Platform.
You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).</simpara>
<note>
<simpara>When applying the configuration specified in a <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.</simpara>
<simpara>It might take several minutes for a configuration change to apply.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have enough available nodes in your cluster to handle the evicted workload from drained nodes.</simpara>
</listitem>
<listitem>
<simpara>You have not selected any control plane nodes for SR-IOV network device configuration.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> object, and then save the YAML in the <literal>&lt;name&gt;-sriov-node-network.yaml</literal> file. Replace <literal>&lt;name&gt;</literal> with the name for this configuration.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: &lt;name&gt; <co xml:id="CO13-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO13-2"/>
spec:
  resourceName: &lt;sriov_resource_name&gt; <co xml:id="CO13-3"/>
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true" <co xml:id="CO13-4"/>
  priority: &lt;priority&gt; <co xml:id="CO13-5"/>
  mtu: &lt;mtu&gt; <co xml:id="CO13-6"/>
  numVfs: &lt;num&gt; <co xml:id="CO13-7"/>
  nicSelector: <co xml:id="CO13-8"/>
    vendor: "&lt;vendor_code&gt;" <co xml:id="CO13-9"/>
    deviceID: "&lt;device_id&gt;" <co xml:id="CO13-10"/>
    pfNames: ["&lt;pf_name&gt;", ...] <co xml:id="CO13-11"/>
    rootDevices: ["&lt;pci_bus_id&gt;", "..."] <co xml:id="CO13-12"/>
  deviceType: vfio-pci <co xml:id="CO13-13"/>
  isRdma: false <co xml:id="CO13-14"/></programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>Specify a name for the CR object.</para>
</callout>
<callout arearefs="CO13-2">
<para>Specify the namespace where the SR-IOV Operator is installed.</para>
</callout>
<callout arearefs="CO13-3">
<para>Specify the resource name of the SR-IOV device plugin. You can create multiple <literal>SriovNetworkNodePolicy</literal> objects for a resource name.</para>
</callout>
<callout arearefs="CO13-4">
<para>Specify the node selector to select which nodes are configured.
Only SR-IOV network devices on selected nodes are configured. The SR-IOV
Container Network Interface (CNI) plugin and device plugin are deployed only on selected nodes.</para>
</callout>
<callout arearefs="CO13-5">
<para>Optional: Specify an integer value between <literal>0</literal> and <literal>99</literal>. A smaller number gets higher priority, so a priority of <literal>10</literal> is higher than a priority of <literal>99</literal>. The default value is <literal>99</literal>.</para>
</callout>
<callout arearefs="CO13-6">
<para>Optional: Specify a value for the maximum transmission unit (MTU) of the virtual function. The maximum MTU value can vary for different NIC models.</para>
</callout>
<callout arearefs="CO13-7">
<para>Specify the number of the virtual functions (VF) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <literal>128</literal>.</para>
</callout>
<callout arearefs="CO13-8">
<para>The <literal>nicSelector</literal> mapping selects the Ethernet device for the Operator to configure. You do not need to specify values for all the parameters. It is recommended to identify the Ethernet adapter with enough precision to minimize the possibility of selecting an Ethernet device unintentionally.
If you specify <literal>rootDevices</literal>, you must also specify a value for <literal>vendor</literal>, <literal>deviceID</literal>, or <literal>pfNames</literal>.
If you specify both <literal>pfNames</literal> and <literal>rootDevices</literal> at the same time, ensure that they point to an identical device.</para>
</callout>
<callout arearefs="CO13-9">
<para>Optional: Specify the vendor hex code of the SR-IOV network device. The only allowed values are either <literal>8086</literal> or <literal>15b3</literal>.</para>
</callout>
<callout arearefs="CO13-10">
<para>Optional: Specify the device hex code of SR-IOV network device. The only allowed values are <literal>158b</literal>, <literal>1015</literal>, <literal>1017</literal>.</para>
</callout>
<callout arearefs="CO13-11">
<para>Optional: The parameter accepts an array of one or more physical function (PF) names for the Ethernet device.</para>
</callout>
<callout arearefs="CO13-12">
<para>The parameter accepts an array of one or more PCI bus addresses for the physical function of the Ethernet device. Provide the address in the following format: <literal>0000:02:00.1</literal>.</para>
</callout>
<callout arearefs="CO13-13">
<para>The <literal>vfio-pci</literal> driver type is required for virtual functions in OpenShift Virtualization.</para>
</callout>
<callout arearefs="CO13-14">
<para>Optional: Specify whether to enable remote direct memory access (RDMA) mode. For a Mellanox card, set <literal>isRdma</literal> to <literal>false</literal>. The default value is <literal>false</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>If <literal>isRDMA</literal> flag is set to <literal>true</literal>, you can continue to use the RDMA enabled VF as a normal network device.
A device can be used in either mode.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: Label the SR-IOV capable cluster nodes with <literal>SriovNetworkNodePolicy.Spec.NodeSelector</literal> if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;-sriov-node-network.yaml</programlisting>
<simpara>where <literal>&lt;name&gt;</literal> specifies the name for this configuration.</simpara>
<simpara>After applying the configuration update, all the pods in <literal>sriov-network-operator</literal> namespace transition to the <literal>Running</literal> status.</simpara>
</listitem>
<listitem>
<simpara>To verify that the SR-IOV network device is configured, enter the following command. Replace <literal>&lt;node_name&gt;</literal> with the name of a node with the SR-IOV network device that you just configured.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="next-steps_configuring-sriov-network">
<title>Next steps</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-attaching-vm-to-sriov-network_virt-connecting-vm-to-sriov">Attaching a virtual machine (VM) to an SR-IOV network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-enabling-load-balancer-service-web_virt-post-install-network-config">
<title>Enabling load balancer service creation by using the web console</title>
<simpara>You can enable the creation of load balancer services for a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured a load balancer for the cluster.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Overview</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Settings</emphasis> tab, click <emphasis role="strong">Cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Expand <emphasis role="strong">General settings</emphasis> and <emphasis role="strong">SSH configuration</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Set <emphasis role="strong">SSH over LoadBalancer service</emphasis> to on.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-post-install-storage-config">
<title>Postinstallation storage configuration</title>

<simpara>The following storage configuration tasks are mandatory:</simpara>
<itemizedlist>
<listitem>
<simpara>You must configure a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/postinstallation_configuration/#defining-storage-classes_post-install-storage-configuration">default storage class</link> for your cluster. Otherwise, the cluster cannot receive automated boot source updates.</simpara>
</listitem>
<listitem>
<simpara>You must configure <link linkend="virt-configuring-storage-profile">storage profiles</link> if your storage provider is not recognized by CDI. A storage profile provides recommended storage settings based on the associated storage class.</simpara>
</listitem>
</itemizedlist>
<simpara>Optional: You can configure local storage by using the hostpath provisioner (HPP).</simpara>
<simpara>See the <link linkend="virt-storage-config-overview">storage configuration overview</link> for more options, including configuring the Containerized Data Importer (CDI), data volumes, and automatic boot source updates.</simpara>
<section xml:id="configuring-local-storage-hpp">
<title>Configuring local storage by using the HPP</title>
<simpara>When you install the OpenShift Virtualization Operator, the Hostpath Provisioner (HPP) Operator is automatically installed. The HPP Operator creates the HPP provisioner.</simpara>
<simpara>The HPP is a local storage provisioner designed for OpenShift Virtualization. To use the HPP, you must create an HPP custom resource (CR).</simpara>
<important>
<simpara>HPP storage pools must not be in the same partition as the operating system. Otherwise, the storage pools might fill the operating system partition. If the operating system partition is full, performance can be effected or the node can become unstable or unusable.</simpara>
</important>
<section xml:id="virt-creating-storage-class-csi-driver_virt-post-install-storage-config">
<title>Creating a storage class for the CSI driver with the storagePools stanza</title>
<simpara>To use the hostpath provisioner (HPP) you must create an associated storage class for the Container Storage Interface (CSI) driver.</simpara>
<simpara>When you create a storage class, you set parameters that affect the dynamic provisioning of persistent volumes (PVs) that belong to that storage class. You cannot update a <literal>StorageClass</literal> object&#8217;s parameters after you create it.</simpara>
<note>
<simpara>Virtual machines use data volumes that are based on local PVs. Local PVs are bound to specific nodes. While a disk image is prepared for consumption by the virtual machine, it is possible that the virtual machine cannot be scheduled to the node where the local storage PV was previously pinned.</simpara>
<simpara>To solve this problem, use the Kubernetes pod scheduler to bind the persistent volume claim (PVC) to a PV on the correct node. By using the <literal>StorageClass</literal> value with <literal>volumeBindingMode</literal> parameter set to <literal>WaitForFirstConsumer</literal>, the binding and provisioning of the PV is delayed until a pod is created using the PVC.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>storageclass_csi.yaml</literal> file to define the storage class:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hostpath-csi
provisioner: kubevirt.io.hostpath-provisioner
reclaimPolicy: Delete <co xml:id="CO14-1"/>
volumeBindingMode: WaitForFirstConsumer <co xml:id="CO14-2"/>
parameters:
  storagePool: my-storage-pool <co xml:id="CO14-3"/></programlisting>
<calloutlist>
<callout arearefs="CO14-1">
<para>The two possible <literal>reclaimPolicy</literal> values are <literal>Delete</literal> and <literal>Retain</literal>. If you do not specify a value, the default value is <literal>Delete</literal>.</para>
</callout>
<callout arearefs="CO14-2">
<para>The <literal>volumeBindingMode</literal> parameter determines when dynamic provisioning and volume binding occur. Specify <literal>WaitForFirstConsumer</literal> to delay the binding and provisioning of a persistent volume (PV) until after a pod that uses the persistent volume claim (PVC) is created. This ensures that the PV meets the pod&#8217;s scheduling requirements.</para>
</callout>
<callout arearefs="CO14-3">
<para>Specify the name of the storage pool defined in the HPP CR.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file and exit.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>StorageClass</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f storageclass_csi.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_updating">
<title>Updating</title>
<section xml:id="upgrading-virt">
<title>Updating OpenShift Virtualization</title>

<simpara>Learn how Operator Lifecycle Manager (OLM) delivers z-stream and minor version updates for OpenShift Virtualization.</simpara>
<section xml:id="virt-rhel-9_upgrading-virt">
<title>OpenShift Virtualization on RHEL 9</title>
<simpara>OpenShift Virtualization 4.15 is based on Red Hat Enterprise Linux (RHEL) 9. You can update to OpenShift Virtualization 4.15 from a version that was based on RHEL 8 by following the standard OpenShift Virtualization update procedure. No additional steps are required.</simpara>
<simpara>As in previous versions, you can perform the update without disrupting running workloads. OpenShift Virtualization 4.15 supports live migration from RHEL 8 nodes to RHEL 9 nodes.</simpara>
<section xml:id="rhel-9-machine-type_upgrading-virt">
<title>RHEL 9 machine type</title>
<simpara>All VM templates that are included with OpenShift Virtualization now use the RHEL 9 machine type by default: <literal>machineType: pc-q35-rhel9.&lt;y&gt;.0</literal>, where <literal>&lt;y&gt;</literal> is a single digit corresponding to the latest minor version of RHEL 9. For example, the value <literal>pc-q35-rhel9.2.0</literal> is used for RHEL 9.2.</simpara>
<simpara>Updating OpenShift Virtualization does not change the <literal>machineType</literal> value of any existing VMs. These VMs continue to function as they did before the update. You can optionally change a VM&#8217;s machine type so that it can benefit from RHEL 9 improvements.</simpara>
<important>
<simpara>Before you change a VM&#8217;s <literal>machineType</literal> value, you must shut down the VM.</simpara>
</important>
</section>
</section>
<section xml:id="virt-about-upgrading-virt_upgrading-virt">
<title>About updating OpenShift Virtualization</title>
<itemizedlist>
<listitem>
<simpara>Operator Lifecycle Manager (OLM) manages the lifecycle of the OpenShift Virtualization Operator. The Marketplace Operator, which is deployed during OpenShift Container Platform installation, makes external Operators available to your cluster.</simpara>
</listitem>
<listitem>
<simpara>OLM provides z-stream and minor version updates for OpenShift Virtualization. Minor version updates become available when you update OpenShift Container Platform to the next minor version. You cannot update OpenShift Virtualization to the next minor version without first updating OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>OpenShift Virtualization subscriptions use a single update channel that is named <emphasis role="strong">stable</emphasis>. The <emphasis role="strong">stable</emphasis> channel ensures that your OpenShift Virtualization and OpenShift Container Platform versions are compatible.</simpara>
</listitem>
<listitem>
<simpara>If your subscription&#8217;s approval strategy is set to <emphasis role="strong">Automatic</emphasis>, the update process starts as soon as a new version of the Operator is available in the <emphasis role="strong">stable</emphasis> channel. It is highly recommended to use the <emphasis role="strong">Automatic</emphasis> approval strategy to maintain a supportable environment. Each minor version of OpenShift Virtualization is only supported if you run the corresponding OpenShift Container Platform version. For example, you must run OpenShift Virtualization 4.15 on OpenShift Container Platform 4.15.</simpara>
<itemizedlist>
<listitem>
<simpara>Though it is possible to select the <emphasis role="strong">Manual</emphasis> approval strategy, this is not recommended because it risks the supportability and functionality of your cluster. With the <emphasis role="strong">Manual</emphasis> approval strategy, you must manually approve every pending update. If OpenShift Container Platform and OpenShift Virtualization updates are out of sync, your cluster becomes unsupported.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The amount of time an update takes to complete depends on your network
connection. Most automatic updates complete within fifteen minutes.</simpara>
</listitem>
<listitem>
<simpara>Updating OpenShift Virtualization does not interrupt network connections.</simpara>
</listitem>
<listitem>
<simpara>Data volumes and their associated persistent volume claims are preserved during update.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>If you have virtual machines running that use hostpath provisioner storage, they cannot be live migrated and might block an OpenShift Container Platform cluster update.</simpara>
<simpara>As a workaround, you can reconfigure the virtual machines so that they can be powered off automatically during a cluster update. Remove the <literal>evictionStrategy: LiveMigrate</literal> field and set the <literal>runStrategy</literal> field to <literal>Always</literal>.</simpara>
</important>
<section xml:id="virt-about-workload-updates_upgrading-virt">
<title>About workload updates</title>
<simpara>When you update OpenShift Virtualization, virtual machine workloads, including <literal>libvirt</literal>, <literal>virt-launcher</literal>, and <literal>qemu</literal>, update automatically if they support live migration.</simpara>
<note>
<simpara>Each virtual machine has a <literal>virt-launcher</literal> pod that runs the virtual machine
instance (VMI). The <literal>virt-launcher</literal> pod runs an instance of <literal>libvirt</literal>, which is
used to manage the virtual machine (VM) process.</simpara>
</note>
<simpara>You can configure how workloads are updated by editing the <literal>spec.workloadUpdateStrategy</literal> stanza of the <literal>HyperConverged</literal> custom resource (CR). There are two available workload update methods: <literal>LiveMigrate</literal> and <literal>Evict</literal>.</simpara>
<simpara>Because the <literal>Evict</literal> method shuts down VMI pods, only the <literal>LiveMigrate</literal> update strategy is enabled by default.</simpara>
<simpara>When <literal>LiveMigrate</literal> is the only update strategy enabled:</simpara>
<itemizedlist>
<listitem>
<simpara>VMIs that support live migration are migrated during the update process. The VM guest moves into a new pod with the updated components enabled.</simpara>
</listitem>
<listitem>
<simpara>VMIs that do not support live migration are not disrupted or updated.</simpara>
<itemizedlist>
<listitem>
<simpara>If a VMI has the <literal>LiveMigrate</literal> eviction strategy but does not support live migration, it is not updated.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>If you enable both <literal>LiveMigrate</literal> and <literal>Evict</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara>VMIs that support live migration use the <literal>LiveMigrate</literal> update strategy.</simpara>
</listitem>
<listitem>
<simpara>VMIs that do not support live migration use the <literal>Evict</literal> update strategy. If a VMI is controlled by a <literal>VirtualMachine</literal> object that has <literal>runStrategy: Always</literal> set, a new VMI is created in a new pod with updated components.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="migration-attempts-timeouts_upgrading-virt" renderas="sect5">Migration attempts and timeouts</bridgehead>
<simpara>When updating workloads, live migration fails if a pod is in the <literal>Pending</literal> state for the following periods:</simpara>
<variablelist>
<varlistentry>
<term>5 minutes</term>
<listitem>
<simpara>If the pod is pending because it is <literal>Unschedulable</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>15 minutes</term>
<listitem>
<simpara>If the pod is stuck in the pending state for any reason.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>When a VMI fails to migrate, the <literal>virt-controller</literal> tries to migrate it again. It repeats this process until all migratable VMIs are running on new <literal>virt-launcher</literal> pods. If a VMI is improperly configured, however, these attempts can repeat indefinitely.</simpara>
<note>
<simpara>Each attempt corresponds to a migration object. Only the five most recent attempts are held in a buffer. This prevents migration objects from accumulating on the system while retaining information for debugging.</simpara>
</note>
</section>
<section xml:id="virt-about-eus-updates_upgrading-virt">
<title>About EUS-to-EUS updates</title>
<simpara>Every even-numbered minor version of OpenShift Container Platform, including 4.10 and 4.12, is an Extended Update Support (EUS) version. However, because Kubernetes design mandates serial minor version updates, you cannot directly update from one EUS version to the next.</simpara>
<simpara>After you update from the source EUS version to the next odd-numbered minor version, you must sequentially update OpenShift Virtualization to all z-stream releases of that minor version that are on your update path. When you have upgraded to the latest applicable z-stream version, you can then update OpenShift Container Platform to the target EUS minor version.</simpara>
<simpara>When the OpenShift Container Platform update succeeds, the corresponding update for OpenShift Virtualization becomes available. You can now update OpenShift Virtualization to the target EUS version.</simpara>
<section xml:id="preparing-to-update_upgrading-virt">
<title>Preparing to update</title>
<simpara>Before beginning an EUS-to-EUS update, you must:</simpara>
<itemizedlist>
<listitem>
<simpara>Pause worker nodes' machine config pools before you start an EUS-to-EUS update so that the workers are not rebooted twice.</simpara>
</listitem>
<listitem>
<simpara>Disable automatic workload updates before you begin the update process. This is to prevent OpenShift Virtualization from migrating or evicting your virtual machines (VMs) until you update to your target EUS version.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>By default, OpenShift Virtualization automatically updates workloads, such as the <literal>virt-launcher</literal> pod, when you update the OpenShift Virtualization Operator. You can configure this behavior in the <literal>spec.workloadUpdateStrategy</literal> stanza of the <literal>HyperConverged</literal> custom resource.</simpara>
</note>
<simpara>Learn more about <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/updating_clusters/#eus-eus-update">performing an EUS-to-EUS update</link>.</simpara>
</section>
</section>
</section>
<section xml:id="virt-preventing-workload-updates-during-eus-update_upgrading-virt">
<title>Preventing workload updates during an EUS-to-EUS update</title>
<simpara>When you update from one Extended Update Support (EUS) version to the next, you must manually disable automatic workload updates to prevent OpenShift Virtualization from migrating or evicting workloads during the update process.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are running an EUS version of OpenShift Container Platform and want to update to the next EUS version. You have not yet updated to the odd-numbered version in between.</simpara>
</listitem>
<listitem>
<simpara>You read "Preparing to perform an EUS-to-EUS update" and learned the caveats and requirements that pertain to your OpenShift Container Platform cluster.</simpara>
</listitem>
<listitem>
<simpara>You paused the worker nodes' machine config pools as directed by the OpenShift Container Platform documentation.</simpara>
</listitem>
<listitem>
<simpara>It is recommended that you use the default <emphasis role="strong">Automatic</emphasis> approval strategy. If you use the <emphasis role="strong">Manual</emphasis> approval strategy, you must approve all pending updates in the web console. For more details, refer to the "Manually approving a pending Operator update" section.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Back up the current <literal>workloadUpdateMethods</literal> configuration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ WORKLOAD_UPDATE_METHODS=$(oc get kv kubevirt-kubevirt-hyperconverged \
  -n openshift-cnv -o jsonpath='{.spec.workloadUpdateStrategy.workloadUpdateMethods}')</programlisting>
</listitem>
<listitem>
<simpara>Turn off all workload update methods by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op":"replace","path":"/spec/workloadUpdateStrategy/workloadUpdateMethods", "value":[]}]'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Ensure that the <literal>HyperConverged</literal> Operator is <literal>Upgradeable</literal> before you continue. Enter the following command and monitor the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv -o json | jq ".status.conditions"</programlisting>
<example>
<title>Example output</title>
<programlisting language="json" linenumbering="unnumbered">[
  {
    "lastTransitionTime": "2022-12-09T16:29:11Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "True",
    "type": "ReconcileComplete"
  },
  {
    "lastTransitionTime": "2022-12-09T20:30:10Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "True",
    "type": "Available"
  },
  {
    "lastTransitionTime": "2022-12-09T20:30:10Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "False",
    "type": "Progressing"
  },
  {
    "lastTransitionTime": "2022-12-09T16:39:11Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "False",
    "type": "Degraded"
  },
  {
    "lastTransitionTime": "2022-12-09T20:30:10Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "True",
    "type": "Upgradeable" <co xml:id="CO15-1"/>
  }
]</programlisting>
</example>
<calloutlist>
<callout arearefs="CO15-1">
<para>The OpenShift Virtualization Operator has the <literal>Upgradeable</literal> status.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Manually update your cluster from the source EUS version to the next minor version of OpenShift Container Platform:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm upgrade</programlisting>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Check the current version by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterversion</programlisting>
<note>
<simpara>Updating OpenShift Container Platform to the next version is a prerequisite for updating OpenShift Virtualization. For more details, refer to the "Updating clusters" section of the OpenShift Container Platform documentation.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Update OpenShift Virtualization.</simpara>
<itemizedlist>
<listitem>
<simpara>With the default <emphasis role="strong">Automatic</emphasis> approval strategy, OpenShift Virtualization automatically updates to the corresponding version after you update OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>If you use the <emphasis role="strong">Manual</emphasis> approval strategy, approve the pending updates by using the web console.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Monitor the OpenShift Virtualization update by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Update OpenShift Virtualization to every z-stream version that is available for the non-EUS minor version, monitoring each update by running the command shown in the previous step.</simpara>
</listitem>
<listitem>
<simpara>Confirm that OpenShift Virtualization successfully updated to the latest z-stream release of the non-EUS version by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv -o json | jq ".status.versions"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">[
  {
    "name": "operator",
    "version": "4.15.0"
  }
]</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Wait until the <literal>HyperConverged</literal> Operator has the <literal>Upgradeable</literal> status before you perform the next update. Enter the following command and monitor the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv -o json | jq ".status.conditions"</programlisting>
</listitem>
<listitem>
<simpara>Update OpenShift Container Platform to the target EUS version.</simpara>
</listitem>
<listitem>
<simpara>Confirm that the update succeeded by checking the cluster version:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusterversion</programlisting>
</listitem>
<listitem>
<simpara>Update OpenShift Virtualization to the target EUS version.</simpara>
<itemizedlist>
<listitem>
<simpara>With the default <emphasis role="strong">Automatic</emphasis> approval strategy, OpenShift Virtualization automatically updates to the corresponding version after you update OpenShift Container Platform.</simpara>
</listitem>
<listitem>
<simpara>If you use the <emphasis role="strong">Manual</emphasis> approval strategy, approve the pending updates by using the web console.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Monitor the OpenShift Virtualization update by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-cnv</programlisting>
<simpara>The update completes when the <literal>VERSION</literal> field matches the target EUS version and the <literal>PHASE</literal> field reads <literal>Succeeded</literal>.</simpara>
</listitem>
<listitem>
<simpara>Restore the workload update methods configuration that you backed up:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv --type json -p \
  "[{\"op\":\"add\",\"path\":\"/spec/workloadUpdateStrategy/workloadUpdateMethods\", \"value\":$WORKLOAD_UPDATE_METHODS}]"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched</programlisting>
</para>
</formalpara>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Check the status of VM migration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmim -A</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara>You can now unpause the worker nodes' machine config pools.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-configuring-workload-update-methods_upgrading-virt">
<title>Configuring workload update methods</title>
<simpara>You can configure workload update methods by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>To use live migration as an update method, you must first enable live migration in the cluster.</simpara>
<note>
<simpara>If a <literal>VirtualMachineInstance</literal> CR contains <literal>evictionStrategy: LiveMigrate</literal> and the virtual machine instance (VMI) does not support live migration, the VMI will not update.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To open the <literal>HyperConverged</literal> CR in your default editor, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>workloadUpdateStrategy</literal> stanza of the <literal>HyperConverged</literal> CR. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  workloadUpdateStrategy:
    workloadUpdateMethods: <co xml:id="CO16-1"/>
    - LiveMigrate <co xml:id="CO16-2"/>
    - Evict <co xml:id="CO16-3"/>
    batchEvictionSize: 10 <co xml:id="CO16-4"/>
    batchEvictionInterval: "1m0s" <co xml:id="CO16-5"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO16-1">
<para>The methods that can be used to perform automated workload updates. The available values are <literal>LiveMigrate</literal> and <literal>Evict</literal>. If you enable both options as shown in this example, updates use <literal>LiveMigrate</literal> for VMIs that support live migration and <literal>Evict</literal> for any VMIs that do not support live migration. To disable automatic workload updates, you can either remove the <literal>workloadUpdateStrategy</literal> stanza or set <literal>workloadUpdateMethods: []</literal> to leave the array empty.</para>
</callout>
<callout arearefs="CO16-2">
<para>The least disruptive update method. VMIs that support live migration are updated by migrating the virtual machine (VM) guest into a new pod with the updated components enabled. If <literal>LiveMigrate</literal> is the only workload update method listed, VMIs that do not support live migration are not disrupted or updated.</para>
</callout>
<callout arearefs="CO16-3">
<para>A disruptive method that shuts down VMI pods during upgrade. <literal>Evict</literal> is the only update method available if live migration is not enabled in the cluster. If a VMI is controlled by a <literal>VirtualMachine</literal> object that has <literal>runStrategy: Always</literal> configured, a new VMI is created in a new pod with updated components.</para>
</callout>
<callout arearefs="CO16-4">
<para>The number of VMIs that can be forced to be updated at a time by using the <literal>Evict</literal> method. This does not apply to the <literal>LiveMigrate</literal> method.</para>
</callout>
<callout arearefs="CO16-5">
<para>The interval to wait before evicting the next batch of workloads. This does not apply to the <literal>LiveMigrate</literal> method.</para>
</callout>
</calloutlist>
<note>
<simpara>You can configure live migration limits and timeouts by editing the <literal>spec.liveMigrationConfig</literal> stanza of the <literal>HyperConverged</literal> CR.</simpara>
</note>
</listitem>
<listitem>
<simpara>To apply your changes, save and exit the editor.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="approving-operator-upgrades_upgrading-virt">
<title>Approving pending Operator updates</title>
<section xml:id="olm-approving-pending-upgrade_upgrading-virt">
<title>Manually approving a pending Operator update</title>
<simpara>If an installed Operator has the approval strategy in its subscription set to <emphasis role="strong">Manual</emphasis>, when new updates are released in its current update channel, the update must be manually approved before installation can begin.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An Operator previously installed using Operator Lifecycle Manager (OLM).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective of the OpenShift Container Platform web console, navigate to <emphasis role="strong">Operators &#8594; Installed Operators</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Operators that have a pending update display a status with <emphasis role="strong">Upgrade available</emphasis>. Click the name of the Operator you want to update.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Subscription</emphasis> tab. Any updates requiring approval are displayed next to <emphasis role="strong">Upgrade status</emphasis>. For example, it might display <emphasis role="strong">1 requires approval</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">1 requires approval</emphasis>, then click <emphasis role="strong">Preview Install Plan</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Review the resources that are listed as available for update. When satisfied, click <emphasis role="strong">Approve</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate back to the <emphasis role="strong">Operators &#8594; Installed Operators</emphasis> page to monitor the progress of the update. When complete, the status changes to <emphasis role="strong">Succeeded</emphasis> and <emphasis role="strong">Up to date</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="monitoring-upgrade-status_upgrading-virt">
<title>Monitoring update status</title>
<section xml:id="virt-monitoring-upgrade-status_upgrading-virt">
<title>Monitoring OpenShift Virtualization upgrade status</title>
<simpara>To monitor the status of a OpenShift Virtualization Operator upgrade, watch the cluster service version (CSV) <literal>PHASE</literal>. You can also monitor the CSV conditions in the web console or by running the command provided here.</simpara>
<note>
<simpara>The <literal>PHASE</literal> and conditions values are approximations that are based on
available information.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Log in to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csv -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Review the output, checking the <literal>PHASE</literal> field. For example:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">VERSION  REPLACES                                        PHASE
4.9.0    kubevirt-hyperconverged-operator.v4.8.2         Installing
4.9.0    kubevirt-hyperconverged-operator.v4.9.0         Replacing</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Optional: Monitor the aggregated status of all OpenShift Virtualization component
conditions by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  -o=jsonpath='{range .status.conditions[*]}{.type}{"\t"}{.status}{"\t"}{.message}{"\n"}{end}'</programlisting>
<simpara>A successful upgrade results in the following output:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ReconcileComplete  True  Reconcile completed successfully
Available          True  Reconcile completed successfully
Progressing        False Reconcile completed successfully
Degraded           False Reconcile completed successfully
Upgradeable        True  Reconcile completed successfully</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-viewing-outdated-workloads_upgrading-virt">
<title>Viewing outdated OpenShift Virtualization workloads</title>
<simpara>You can view a list of outdated workloads by using the CLI.</simpara>
<note>
<simpara>If there are outdated virtualization pods in your cluster, the <literal>OutdatedVirtualMachineInstanceWorkloads</literal> alert fires.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To view a list of outdated virtual machine instances (VMIs), run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi -l kubevirt.io/outdatedLauncherImage --all-namespaces</programlisting>
</listitem>
</itemizedlist>
<note>
<simpara>Configure workload updates to ensure that VMIs update automatically.</simpara>
</note>
</section>
</section>
<section xml:id="additional-resources_upgrading-virt" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/updating_clusters/#eus-eus-update">Performing an EUS-to-EUS update</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-what-operators-are">What are Operators?</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-understanding-olm">Operator Lifecycle Manager concepts and resources</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-csv_olm-understanding-olm">Cluster service versions (CSVs)</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-about-live-migration">About live migration</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="eviction-strategies">Configuring eviction strategies</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-configuring-live-migration-limits_virt-configuring-live-migration">Configuring live migration limits and timeouts</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_virtual-machines">
<title>Virtual machines</title>
<section xml:id="_creating-vms-from-red-hat-images">
<title>Creating VMs from Red Hat images</title>
<section xml:id="virt-creating-vms-from-rh-images-overview">
<title>Creating virtual machines from Red Hat images overview</title>

<simpara>Red Hat images are <link linkend="virt-about-golden-images_virt-creating-vms-from-rh-images-overview">golden images</link>. They are published as container disks in a secure registry. The Containerized Data Importer (CDI) polls and imports the container disks into your cluster and stores them in the <literal>openshift-virtualization-os-images</literal> project as snapshots or persistent volume claims (PVCs).</simpara>
<simpara>Red Hat images are automatically updated. You can disable and re-enable automatic updates for these images. See <link linkend="managing-rh-boot-source-updates_virt-automatic-bootsource-updates">Managing Red Hat boot source updates</link>.</simpara>
<simpara>Cluster administrators can enable automatic subscription for Red Hat Enterprise Linux (RHEL) virtual machines in the OpenShift Virtualization <link linkend="overview-settings-cluster_virt-web-console-overview">web console</link>.</simpara>
<simpara>You can create virtual machines (VMs) from operating system images provided by Red Hat by using one of the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-creating-vms-from-templates">Creating a VM from a template by using the web console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-creating-vms-from-instance-types">Creating a VM from an instance type by using the web console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-creating-vms-from-cli">Creating a VM from a <literal>VirtualMachine</literal> manifest by using the command line</link></simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Do not create VMs in the default <literal>openshift-*</literal> namespaces. Instead, create a new namespace or use an existing namespace without the <literal>openshift</literal> prefix.</simpara>
</important>
<section xml:id="virt-about-golden-images_virt-creating-vms-from-rh-images-overview">
<title>About golden images</title>
<simpara>A golden image is a preconfigured snapshot of a virtual machine (VM) that you can use as a resource to deploy new VMs. For example, you can use golden images to provision the same system environment consistently and deploy systems more quickly and efficiently.</simpara>
<section xml:id="virt-how-golden-images-work_virt-creating-vms-from-rh-images-overview">
<title>How do golden images work?</title>
<simpara>Golden images are created by installing and configuring an operating system and software applications on a reference machine or virtual machine. This includes setting up the system, installing required drivers, applying patches and updates, and configuring specific options and preferences.</simpara>
<simpara>After the golden image is created, it is saved as a template or image file that can be replicated and deployed across multiple clusters. The golden image can be updated by its maintainer periodically to incorporate necessary software updates and patches, ensuring that the image remains up to date and secure, and newly created VMs are based on this updated image.</simpara>
</section>
<section xml:id="virt-golden-images-implementation_virt-creating-vms-from-rh-images-overview">
<title>Red Hat implementation of golden images</title>
<simpara>Red Hat publishes golden images as container disks in the registry for versions of Red Hat Enterprise Linux (RHEL). Container disks are virtual machine images that are stored as a container image in a container image registry. Any published image will automatically be made available in connected clusters after the installation of OpenShift Virtualization. After the images are available in a cluster, they are ready to use to create VMs.</simpara>
</section>
</section>
<section xml:id="virt-about-vms-and-boot-sources_virt-creating-vms-from-rh-images-overview">
<title>About VM boot sources</title>
<simpara>Virtual machines (VMs) consist of a VM definition and one or more disks that are backed by data volumes. VM templates enable you to create VMs using predefined specifications.</simpara>
<simpara>Every template requires a boot source, which is a fully configured disk image including configured drivers. Each template contains a VM definition with a pointer to the boot source. Each boot source has a predefined name and namespace. For some operating systems, a boot source is automatically provided. If it is not provided, then an administrator must prepare a custom boot source.</simpara>
<simpara>Provided boot sources are updated automatically to the latest version of the operating system. For auto-updated boot sources, persistent volume claims (PVCs) and volume snapshots are created with the cluster&#8217;s default storage class. If you select a different default storage class after configuration, you must delete the existing boot sources in the cluster namespace that are configured with the previous default storage class.</simpara>
</section>
</section>
<section xml:id="virt-creating-vms-from-instance-types">
<title>Creating virtual machines from instance types</title>

<simpara>You can create virtual machines (VMs) from instance types by using the OpenShift Container Platform web console.</simpara>
<section xml:id="virt-creating-vm-instancetype_virt-creating-vms-from-instance-types">
<title>Creating a VM from an instance type</title>
<simpara>You can create a virtual machine (VM) from an instance type by using the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> and click the <emphasis role="strong">InstanceTypes</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Select a bootable volume.</simpara>
<note>
<simpara>The volume table only lists volumes in the <literal>openshift-virtualization-os-images</literal> namespace that have the <literal>instancetype.kubevirt.io/default-preference</literal> label.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Optional: Click the star icon to designate a bootable volume as a favorite. Starred bootable volumes appear first in the volume list.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click an instance type tile and select the resource size appropriate for your workload.</simpara>
</listitem>
<listitem>
<simpara>If you have not already added a public SSH key to your project, click the edit icon beside <emphasis role="strong">Authorized SSH key</emphasis> in the <emphasis role="strong">VirtualMachine details</emphasis> section.</simpara>
</listitem>
<listitem>
<simpara>Select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Use existing</emphasis>: Select a secret from the secrets list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Add new</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Browse to the public SSH key file or paste the file in the key field.</simpara>
</listitem>
<listitem>
<simpara>Enter the secret name.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">Automatically apply this key to any new VirtualMachine you create in this project</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Optional: Click <emphasis role="strong">View YAML &amp; CLI</emphasis> to view the YAML file. Click <emphasis role="strong">CLI</emphasis> to view the CLI commands. You can also download or copy either the YAML file contents or the CLI commands.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>After the VM is created, you can monitor the status on the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</section>
</section>
<section xml:id="virt-creating-vms-from-templates">
<title>Creating virtual machines from templates</title>

<simpara>You can create virtual machines (VMs) from Red Hat templates by using the OpenShift Container Platform web console.</simpara>
<section xml:id="virt-about-templates">
<title>About VM templates</title>
<variablelist>
<varlistentry>
<term>Boot sources</term>
<listitem>
<simpara>You can expedite VM creation by using templates that have an available boot source. Templates with a boot source are labeled <emphasis role="strong">Available boot source</emphasis> if they do not have a custom label.</simpara>
<simpara>Templates without a boot source are labeled <emphasis role="strong">Boot source required</emphasis>. See <link linkend="virt-creating-vms-from-custom-images-overview">Creating virtual machines from custom images</link>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Customization</term>
<listitem>
<simpara>You can customize the disk source and VM parameters before you start the VM:</simpara>
<itemizedlist>
<listitem>
<simpara>See <link linkend="virt-vm-storage-volume-types_virt-creating-vms-from-templates">storage volume types</link> and <link linkend="virt-storage-wizard-fields-web_virt-creating-vms-from-templates">storage fields</link> for details about disk source settings.</simpara>
</listitem>
<listitem>
<simpara>See the <link linkend="virtualmachine-details-overview_virt-web-console-overview"><emphasis role="strong">Overview</emphasis></link>, <link linkend="virtualmachine-details-yaml_virt-web-console-overview"><emphasis role="strong">YAML</emphasis></link>, and <link linkend="virtualmachine-details-configuration_virt-web-console-overview"><emphasis role="strong">Configuration</emphasis></link> tab documentation for details about VM settings.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Single-node OpenShift</term>
<listitem>
<simpara>Due to differences in storage behavior, some templates are incompatible with single-node OpenShift. To ensure compatibility, do not set the <literal>evictionStrategy</literal> field for templates or VMs that use data volumes or storage profiles.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="virt-creating-vm-from-template_virt-creating-vms-from-templates">
<title>Creating a VM from a template</title>
<simpara>You can create a virtual machine (VM) from a template with an available boot source by using the OpenShift Container Platform web console.</simpara>
<simpara>Optional: You can customize template or VM parameters, such as data sources, cloud-init, or SSH keys, before you start the VM.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Boot source available</emphasis> to filter templates with boot sources.</simpara>
<simpara>The catalog displays the default templates. Click <emphasis role="strong">All Items</emphasis> to view all available templates for your filters.</simpara>
</listitem>
<listitem>
<simpara>Click a template tile to view its details.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Quick create VirtualMachine</emphasis> to create a VM from the template.</simpara>
<simpara>Optional: Customize the template or VM parameters:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Expand <emphasis role="strong">Storage</emphasis> or <emphasis role="strong">Optional parameters</emphasis> to edit data source settings.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine parameters</emphasis>.</simpara>
<simpara>The <emphasis role="strong">Customize and create VirtualMachine</emphasis> pane displays the <emphasis role="strong">Overview</emphasis>, <emphasis role="strong">YAML</emphasis>, <emphasis role="strong">Scheduling</emphasis>, <emphasis role="strong">Environment</emphasis>, <emphasis role="strong">Network interfaces</emphasis>, <emphasis role="strong">Disks</emphasis>, <emphasis role="strong">Scripts</emphasis>, and <emphasis role="strong">Metadata</emphasis> tabs.</simpara>
</listitem>
<listitem>
<simpara>Edit the parameters that must be set before the VM boots, such as cloud-init or a static SSH key.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
<simpara>The <emphasis role="strong">VirtualMachine details</emphasis> page displays the provisioning status.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<section xml:id="virt-vm-storage-volume-types_virt-creating-vms-from-templates">
<title>Storage volume types</title>
<table frame="all" rowsep="1" colsep="1">
<title>Storage volume types</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>ephemeral</simpara></entry>
<entry align="left" valign="top"><simpara>A local copy-on-write (COW) image that uses a network volume as a read-only backing store. The backing volume must be a <emphasis role="strong">PersistentVolumeClaim</emphasis>. The ephemeral image is created when the virtual machine starts and stores all writes locally. The ephemeral image is discarded when the virtual machine is stopped, restarted, or deleted. The backing volume (PVC) is not mutated in any way.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>persistentVolumeClaim</simpara></entry>
<entry align="left" valign="top"><simpara>Attaches an available PV to a virtual machine. Attaching a PV allows for the virtual machine data to persist between sessions.</simpara>
<simpara>Importing an existing virtual machine disk into a PVC by using CDI and attaching the PVC to a virtual machine instance is the recommended method for importing existing virtual machines into OpenShift Container Platform. There are some requirements for the disk to be used within a PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>dataVolume</simpara></entry>
<entry align="left" valign="top"><simpara>Data volumes build on the <literal>persistentVolumeClaim</literal> disk type by managing the process of preparing the virtual machine disk via an import, clone, or upload operation. VMs that use this volume type are guaranteed not to start until the volume is ready.</simpara>
<simpara>Specify <literal>type: dataVolume</literal> or <literal>type: ""</literal>. If you specify any other value for <literal>type</literal>, such as <literal>persistentVolumeClaim</literal>, a warning is displayed, and the virtual machine does not start.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>cloudInitNoCloud</simpara></entry>
<entry align="left" valign="top"><simpara>Attaches a disk that contains the referenced cloud-init NoCloud data source, providing user data and metadata to the virtual machine. A cloud-init installation is required inside the virtual machine disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>containerDisk</simpara></entry>
<entry align="left" valign="top"><simpara>References an image, such as a virtual machine disk, that is stored in the container image registry. The image is pulled from the registry and attached to the virtual machine as a disk when the virtual machine is launched.</simpara>
<simpara>A <literal>containerDisk</literal> volume is not limited to a single virtual machine and is useful for creating large numbers of virtual machine clones that do not require persistent storage.</simpara>
<simpara>Only RAW and QCOW2 formats are supported disk types for the container image registry. QCOW2 is recommended for reduced image size.</simpara>
<note>
<simpara>A <literal>containerDisk</literal> volume is ephemeral. It is discarded when the virtual machine is stopped, restarted, or deleted. A <literal>containerDisk</literal> volume is useful for read-only file systems such as CD-ROMs or for disposable virtual machines.</simpara>
</note></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>emptyDisk</simpara></entry>
<entry align="left" valign="top"><simpara>Creates an additional sparse QCOW2 disk that is tied to the life-cycle of the virtual machine interface. The data survives guest-initiated reboots in the virtual machine but is discarded when the virtual machine stops or is restarted from the web console. The empty disk is used to store application dependencies and data that otherwise exceeds the limited temporary file system of an ephemeral disk.</simpara>
<simpara>The disk <emphasis role="strong">capacity</emphasis> size must also be provided.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-storage-wizard-fields-web_virt-creating-vms-from-templates">
<title>Storage fields</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Blank (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Create an empty disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Import via URL (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Import content via URL (HTTP or HTTPS endpoint).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Use an existing PVC</simpara></entry>
<entry align="left" valign="top"><simpara>Use a PVC that is already available in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Clone existing PVC (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Select an existing PVC available in the cluster and clone it.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Import via Registry (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Import content via container registry.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Container (ephemeral)</simpara></entry>
<entry align="left" valign="top"><simpara>Upload content from a container located in a registry accessible from the cluster. The container disk should be used only for read-only filesystems such as CD-ROMs or temporary virtual machines.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Name</simpara></entry>
<entry align="left" valign="top"><simpara>Name of the disk. The name can contain lowercase letters (<literal>a-z</literal>), numbers (<literal>0-9</literal>), hyphens (<literal>-</literal>), and periods (<literal>.</literal>), up to a maximum of 253 characters. The first and last characters must be alphanumeric. The name must not contain uppercase letters, spaces, or special characters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Size</simpara></entry>
<entry align="left" valign="top"><simpara>Size of the disk in GiB.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Type</simpara></entry>
<entry align="left" valign="top"><simpara>Type of disk. Example: Disk or CD-ROM</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Interface</simpara></entry>
<entry align="left" valign="top"><simpara>Type of disk device. Supported interfaces are <emphasis role="strong">virtIO</emphasis>, <emphasis role="strong">SATA</emphasis>, and <emphasis role="strong">SCSI</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Storage Class</simpara></entry>
<entry align="left" valign="top"><simpara>The storage class that is used to create the disk.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<bridgehead xml:id="virt-storage-wizard-fields-advanced-web_virt-creating-vms-from-templates" renderas="sect6">Advanced storage settings</bridgehead>
<simpara>The following advanced storage settings are optional and available for <emphasis role="strong">Blank</emphasis>, <emphasis role="strong">Import via URL</emphasis>, and <emphasis role="strong">Clone existing PVC</emphasis> disks.</simpara>
<simpara>If you do not specify these parameters, the system uses the default storage profile values.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Parameter description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Volume Mode</simpara></entry>
<entry align="left" valign="top"><simpara>Filesystem</simpara></entry>
<entry align="left" valign="top"><simpara>Stores the virtual disk on a file system-based volume.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Block</simpara></entry>
<entry align="left" valign="top"><simpara>Stores the virtual disk directly on the block volume. Only use <literal>Block</literal> if the underlying storage supports it.</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Access Mode</simpara></entry>
<entry align="left" valign="top"><simpara>ReadWriteOnce (RWO)</simpara></entry>
<entry align="left" valign="top"><simpara>Volume can be mounted as read-write by a single node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ReadWriteMany (RWX)</simpara></entry>
<entry align="left" valign="top"><simpara>Volume can be mounted as read-write by many nodes at one time.</simpara>
<note>
<simpara>This mode is required for live migration.</simpara>
</note></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
</section>
<section xml:id="virt-creating-vms-from-cli">
<title>Creating virtual machines from the command line</title>

<simpara>You can create virtual machines (VMs) from the command line by editing or creating a <literal>VirtualMachine</literal> manifest.</simpara>
<section xml:id="virt-creating-vm-cli_virt-creating-vms-cli">
<title>Creating a VM from a VirtualMachine manifest</title>
<simpara>You can create a virtual machine (VM) from a <literal>VirtualMachine</literal> manifest.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest for your VM. The following example configures a Red Hat Enterprise Linux (RHEL) VM:</simpara>
<example>
<title>Example manifest for a RHEL VM</title>
<screen>apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    app: &lt;vm_name&gt; <co xml:id="CO17-1"/>
  name: &lt;vm_name&gt;
spec:
  dataVolumeTemplates:
  - apiVersion: cdi.kubevirt.io/v1beta1
    kind: DataVolume
    metadata:
      name: &lt;vm_name&gt;
    spec:
      sourceRef:
        kind: DataSource
        name: rhel9
        namespace: openshift-virtualization-os-images
      storage:
        resources:
          requests:
            storage: 30Gi
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/domain: &lt;vm_name&gt;
    spec:
      domain:
        cpu:
          cores: 1
          sockets: 2
          threads: 1
        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          interfaces:
          - masquerade: {}
            name: default
          rng: {}
        features:
          smm:
            enabled: true
        firmware:
          bootloader:
            efi: {}
        resources:
          requests:
            memory: 8Gi
      evictionStrategy: LiveMigrate
      networks:
      - name: default
        pod: {}
      volumes:
      - dataVolume:
          name: &lt;vm_name&gt;
        name: rootdisk
      - cloudInitNoCloud:
          userData: |-
            #cloud-config
            user: cloud-user
            password: '&lt;password&gt;' <co xml:id="CO17-2"/>
            chpasswd: { expire: False }
        name: cloudinitdisk</screen>
<calloutlist>
<callout arearefs="CO17-1">
<para>Specify the name of the virtual machine.</para>
</callout>
<callout arearefs="CO17-2">
<para>Specify the password for cloud-user.</para>
</callout>
</calloutlist>
</example>
</listitem>
<listitem>
<simpara>Create a virtual machine by using the manifest file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;vm_manifest_file&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: Start the virtual machine:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl start &lt;vm_name&gt;</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="_creating-vms-from-custom-images">
<title>Creating VMs from custom images</title>
<section xml:id="virt-creating-vms-from-custom-images-overview">
<title>Creating virtual machines from custom images overview</title>

<simpara>You can create virtual machines (VMs) from custom operating system images by using one of the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-creating-vms-from-container-disks">Importing the image as a container disk from a registry</link>.</simpara>
<simpara>Optional: You can enable auto updates for your container disks. See <link linkend="virt-automatic-bootsource-updates">Managing automatic boot source updates</link> for details.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-creating-vms-from-web-images">Importing the image from a web page</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-creating-vms-uploading-images">Uploading the image from a local machine</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-creating-vms-by-cloning-pvcs">Cloning a persistent volume claim (PVC) that contains the image</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>The Containerized Data Importer (CDI) imports the image into a PVC by using a data volume. You add the PVC to the VM by using the OpenShift Container Platform web console or command line.</simpara>
<important>
<simpara>You must install the <link linkend="virt-installing-qemu-guest-agent">QEMU guest agent</link> on VMs created from operating system images that are not provided by Red Hat.</simpara>
<simpara>You must also install <link linkend="installing-virtio-drivers">VirtIO drivers</link> on Windows VMs.</simpara>
<simpara>The QEMU guest agent is included with Red Hat images.</simpara>
</important>
</section>
<section xml:id="virt-creating-vms-from-container-disks">
<title>Creating VMs by using container disks</title>

<simpara>You can create virtual machines (VMs) by using container disks built from operating system images.</simpara>
<simpara>You can enable auto updates for your container disks. See <link linkend="virt-automatic-bootsource-updates">Managing automatic boot source updates</link> for details.</simpara>
<important>
<simpara>If the container disks are large, the I/O traffic might increase and cause worker nodes to be unavailable. You can perform the following tasks to resolve this issue:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#pruning-deployments_pruning-objects">Pruning <literal>DeploymentConfig</literal> objects</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-nodes-garbage-collection-configuring_nodes-nodes-configuring">Configuring garbage collection</link>.</simpara>
</listitem>
</itemizedlist>
</important>
<simpara>You create a VM from a container disk by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="virt-preparing-container-disk-for-vms_virt-creating-vms-from-container-disks">Build an operating system image into a container disk and upload it to your container registry</link>.</simpara>
</listitem>
<listitem>
<simpara>If your container registry does not have TLS, <link linkend="virt-disabling-tls-for-registry_virt-creating-vms-from-container-disks">configure your environment to disable TLS for your registry</link>.</simpara>
</listitem>
<listitem>
<simpara>Create a VM with the container disk as the disk source by using the <link linkend="virt-creating-vm-custom-image-web_virt-creating-vms-from-container-disks">web console</link> or the <link linkend="virt-creating-vm-import-cli_virt-creating-vms-from-container-disks">command line</link>.</simpara>
</listitem>
</orderedlist>
<important>
<simpara>You must install the <link linkend="virt-installing-qemu-guest-agent">QEMU guest agent</link> on VMs created from operating system images that are not provided by Red Hat.</simpara>
</important>
<section xml:id="virt-preparing-container-disk-for-vms_virt-creating-vms-from-container-disks">
<title>Building and uploading a container disk</title>
<simpara>You can build a virtual machine (VM) image into a container disk and upload it to a registry.</simpara>
<simpara>The size of a container disk is limited by the maximum layer size of the registry where the container disk is hosted.</simpara>
<note>
<simpara>For <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_quay/">Red Hat Quay</link>, you can change the maximum layer size by editing the YAML configuration file that is created when Red Hat Quay is first deployed.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have <literal>podman</literal> installed.</simpara>
</listitem>
<listitem>
<simpara>You must have a QCOW2 or RAW image file.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a Dockerfile to build the VM image into a container image. The VM image must be owned by QEMU, which has a UID of <literal>107</literal>, and placed in the <literal>/disk/</literal> directory inside the container. Permissions for the <literal>/disk/</literal> directory must then be set to <literal>0440</literal>.</simpara>
<simpara>The following example uses the Red Hat Universal Base Image (UBI) to handle these configuration changes in the first stage, and uses the minimal <literal>scratch</literal> image in the second stage to store the result:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &gt; Dockerfile &lt;&lt; EOF
FROM registry.access.redhat.com/ubi8/ubi:latest AS builder
ADD --chown=107:107 &lt;vm_image&gt;.qcow2 /disk/ \<co xml:id="CO18-1"/>
RUN chmod 0440 /disk/*

FROM scratch
COPY --from=builder /disk/* /disk/
EOF</programlisting>
<calloutlist>
<callout arearefs="CO18-1">
<para>Where <literal>&lt;vm_image&gt;</literal> is the image in either QCOW2 or RAW format. If you use a remote image, replace <literal>&lt;vm_image&gt;.qcow2</literal> with the complete URL.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Build and tag the container:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman build -t &lt;registry&gt;/&lt;container_disk_name&gt;:latest .</programlisting>
</listitem>
<listitem>
<simpara>Push the container image to the registry:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman push &lt;registry&gt;/&lt;container_disk_name&gt;:latest</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-disabling-tls-for-registry_virt-creating-vms-from-container-disks">
<title>Disabling TLS for a container registry</title>
<simpara>You can disable TLS (transport layer security) for one or more container registries by editing the <literal>insecureRegistries</literal> field of the <literal>HyperConverged</literal> custom resource.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Log in to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> custom resource and add a list of insecure registries to the <literal>spec.storageImport.insecureRegistries</literal> field.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  storageImport:
    insecureRegistries: <co xml:id="CO19-1"/>
      - "private-registry-example-1:5000"
      - "private-registry-example-2:5000"</programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>Replace the examples in this list with valid registry hostnames.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-creating-vm-custom-image-web_virt-creating-vms-from-container-disks">
<title>Creating a VM from a container disk by using the web console</title>
<simpara>You can create a virtual machine (VM) by importing a container disk from a container registry by using the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click a template tile without an available boot source.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Customize template parameters</emphasis> page, expand <emphasis role="strong">Storage</emphasis> and select <emphasis role="strong">Registry (creates PVC)</emphasis> from the <emphasis role="strong">Disk source</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Enter the container image URL. Example: <literal>https://mirror.arizona.edu/fedora/linux/releases/38/Cloud/x86_64/images/Fedora-Cloud-Base-38-1.6.x86_64.qcow2</literal></simpara>
</listitem>
<listitem>
<simpara>Set the disk size.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-vm-import-cli_virt-creating-vms-from-container-disks">
<title>Creating a VM from a container disk by using the command line</title>
<simpara>You can create a virtual machine (VM) from a container disk by using the command line.</simpara>
<simpara>When the virtual machine (VM) is created, the data volume with the container disk is imported into persistent storage.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access credentials for the container registry that contains the container disk.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If the container registry requires authentication, create a <literal>Secret</literal> manifest, specifying the credentials, and save it as a <literal>data-source-secret.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: data-source-secret
  labels:
    app: containerized-data-importer
type: Opaque
data:
  accessKeyId: "" <co xml:id="CO20-1"/>
  secretKey:   "" <co xml:id="CO20-2"/></programlisting>
<calloutlist>
<callout arearefs="CO20-1">
<para>Specify the Base64-encoded key ID or user name.</para>
</callout>
<callout arearefs="CO20-2">
<para>Specify the Base64-encoded secret key or password.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>Secret</literal> manifest by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f data-source-secret.yaml</programlisting>
</listitem>
<listitem>
<simpara>If the VM must communicate with servers that use self-signed certificates or certificates that are not signed by the system CA bundle, create a config map in the same namespace as the VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap tls-certs <co xml:id="CO21-1"/>
  --from-file=&lt;/path/to/file/ca.pem&gt; <co xml:id="CO21-2"/></programlisting>
<calloutlist>
<callout arearefs="CO21-1">
<para>Specify the config map name.</para>
</callout>
<callout arearefs="CO21-2">
<para>Specify the path to the CA certificate.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest and save it as a <literal>vm-fedora-datavolume.yaml</literal> file:</simpara>
<informalexample>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  creationTimestamp: null
  labels:
    kubevirt.io/vm: vm-fedora-datavolume
  name: vm-fedora-datavolume <co xml:id="CO22-1"/>
spec:
  dataVolumeTemplates:
  - metadata:
      creationTimestamp: null
      name: fedora-dv <co xml:id="CO22-2"/>
    spec:
      storage:
        resources:
          requests:
            storage: 10Gi <co xml:id="CO22-3"/>
        storageClassName: &lt;storage_class&gt; <co xml:id="CO22-4"/>
      source:
        registry:
          url: "docker://kubevirt/fedora-cloud-container-disk-demo:latest" <co xml:id="CO22-5"/>
          secretRef: data-source-secret <co xml:id="CO22-6"/>
          certConfigMap: tls-certs <co xml:id="CO22-7"/>
    status: {}
  running: true
  template:
    metadata:
      creationTimestamp: null
      labels:
        kubevirt.io/vm: vm-fedora-datavolume
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: datavolumedisk1
        machine:
          type: ""
        resources:
          requests:
            memory: 1.5Gi
      terminationGracePeriodSeconds: 180
      volumes:
      - dataVolume:
          name: fedora-dv
        name: datavolumedisk1
status: {}</programlisting>
<calloutlist>
<callout arearefs="CO22-1">
<para>Specify the name of the VM.</para>
</callout>
<callout arearefs="CO22-2">
<para>Specify the name of the data volume.</para>
</callout>
<callout arearefs="CO22-3">
<para>Specify the size of the storage requested for the data volume.</para>
</callout>
<callout arearefs="CO22-4">
<para>Optional: If you do not specify a storage class, the default storage class is used.</para>
</callout>
<callout arearefs="CO22-5">
<para>Specify the URL of the container registry.</para>
</callout>
<callout arearefs="CO22-6">
<para>Optional: Specify the secret name if you created a secret for the container registry access credentials.</para>
</callout>
<callout arearefs="CO22-7">
<para>Optional: Specify a CA certificate config map.</para>
</callout>
</calloutlist>
</informalexample>
</listitem>
<listitem>
<simpara>Create the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f vm-fedora-datavolume.yaml</programlisting>
<simpara>The <literal>oc create</literal> command creates the data volume and the VM. The CDI controller creates an underlying PVC with the correct annotation and the import process begins. When the import is complete, the data volume status changes to <literal>Succeeded</literal>. You can start the VM.</simpara>
<simpara>Data volume provisioning happens in the background, so there is no need to monitor the process.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>The importer pod downloads the container disk from the specified URL and stores it on the provisioned persistent volume. View the status of the importer pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods</programlisting>
</listitem>
<listitem>
<simpara>Monitor the data volume until its status is <literal>Succeeded</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe dv fedora-dv <co xml:id="CO23-1"/></programlisting>
<calloutlist>
<callout arearefs="CO23-1">
<para>Specify the data volume name that you defined in the <literal>VirtualMachine</literal> manifest.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that provisioning is complete and that the VM has started by accessing its serial console:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl console vm-fedora-datavolume</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-creating-vms-from-web-images">
<title>Creating VMs by importing images from web pages</title>

<simpara>You can create virtual machines (VMs) by importing operating system images from web pages.</simpara>
<important>
<simpara>You must install the <link linkend="virt-installing-qemu-guest-agent">QEMU guest agent</link> on VMs created from operating system images that are not provided by Red Hat.</simpara>
</important>
<section xml:id="virt-creating-vm-custom-image-web_virt-creating-vms-from-web-images">
<title>Creating a VM from an image on a web page by using the web console</title>
<simpara>You can create a virtual machine (VM) by importing an image from a web page by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to the web page that contains the image.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click a template tile without an available boot source.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Customize template parameters</emphasis> page, expand <emphasis role="strong">Storage</emphasis> and select <emphasis role="strong">URL (creates PVC)</emphasis> from the <emphasis role="strong">Disk source</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Enter the image URL. Example: <literal>https://access.redhat.com/downloads/content/69/ver=/rhel---7/7.9/x86_64/product-software</literal></simpara>
</listitem>
<listitem>
<simpara>Enter the container image URL. Example: <literal>https://mirror.arizona.edu/fedora/linux/releases/38/Cloud/x86_64/images/Fedora-Cloud-Base-38-1.6.x86_64.qcow2</literal></simpara>
</listitem>
<listitem>
<simpara>Set the disk size.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-vm-import-cli_virt-creating-vms-from-web-images">
<title>Creating a VM from an image on a web page by using the command line</title>
<simpara>You can create a virtual machine (VM) from an image on a web page by using the command line.</simpara>
<simpara>When the virtual machine (VM) is created, the data volume with the image is imported into persistent storage.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access credentials for the web page that contains the image.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If the web page requires authentication, create a <literal>Secret</literal> manifest, specifying the credentials, and save it as a <literal>data-source-secret.yaml</literal> file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: data-source-secret
  labels:
    app: containerized-data-importer
type: Opaque
data:
  accessKeyId: "" <co xml:id="CO24-1"/>
  secretKey:   "" <co xml:id="CO24-2"/></programlisting>
<calloutlist>
<callout arearefs="CO24-1">
<para>Specify the Base64-encoded key ID or user name.</para>
</callout>
<callout arearefs="CO24-2">
<para>Specify the Base64-encoded secret key or password.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>Secret</literal> manifest by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f data-source-secret.yaml</programlisting>
</listitem>
<listitem>
<simpara>If the VM must communicate with servers that use self-signed certificates or certificates that are not signed by the system CA bundle, create a config map in the same namespace as the VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create configmap tls-certs <co xml:id="CO25-1"/>
  --from-file=&lt;/path/to/file/ca.pem&gt; <co xml:id="CO25-2"/></programlisting>
<calloutlist>
<callout arearefs="CO25-1">
<para>Specify the config map name.</para>
</callout>
<callout arearefs="CO25-2">
<para>Specify the path to the CA certificate.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest and save it as a <literal>vm-fedora-datavolume.yaml</literal> file:</simpara>
<informalexample>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  creationTimestamp: null
  labels:
    kubevirt.io/vm: vm-fedora-datavolume
  name: vm-fedora-datavolume <co xml:id="CO26-1"/>
spec:
  dataVolumeTemplates:
  - metadata:
      creationTimestamp: null
      name: fedora-dv <co xml:id="CO26-2"/>
    spec:
      storage:
        resources:
          requests:
            storage: 10Gi <co xml:id="CO26-3"/>
        storageClassName: &lt;storage_class&gt; <co xml:id="CO26-4"/>
      source:
        http:
          url: "https://mirror.arizona.edu/fedora/linux/releases/35/Cloud/x86_64/images/Fedora-Cloud-Base-35-1.2.x86_64.qcow2" <co xml:id="CO26-5"/>
        registry:
          url: "docker://kubevirt/fedora-cloud-container-disk-demo:latest" <co xml:id="CO26-6"/>
          secretRef: data-source-secret <co xml:id="CO26-7"/>
          certConfigMap: tls-certs <co xml:id="CO26-8"/>
    status: {}
  running: true
  template:
    metadata:
      creationTimestamp: null
      labels:
        kubevirt.io/vm: vm-fedora-datavolume
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: datavolumedisk1
        machine:
          type: ""
        resources:
          requests:
            memory: 1.5Gi
      terminationGracePeriodSeconds: 180
      volumes:
      - dataVolume:
          name: fedora-dv
        name: datavolumedisk1
status: {}</programlisting>
<calloutlist>
<callout arearefs="CO26-1">
<para>Specify the name of the VM.</para>
</callout>
<callout arearefs="CO26-2">
<para>Specify the name of the data volume.</para>
</callout>
<callout arearefs="CO26-3">
<para>Specify the size of the storage requested for the data volume.</para>
</callout>
<callout arearefs="CO26-4">
<para>Optional: If you do not specify a storage class, the default storage class is used.</para>
</callout>
<callout arearefs="CO26-5 CO26-6">
<para>Specify the URL of the web page.</para>
</callout>
<callout arearefs="CO26-7">
<para>Optional: Specify the secret name if you created a secret for the web page access credentials.</para>
</callout>
<callout arearefs="CO26-8">
<para>Optional: Specify a CA certificate config map.</para>
</callout>
</calloutlist>
</informalexample>
</listitem>
<listitem>
<simpara>Create the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f vm-fedora-datavolume.yaml</programlisting>
<simpara>The <literal>oc create</literal> command creates the data volume and the VM. The CDI controller creates an underlying PVC with the correct annotation and the import process begins. When the import is complete, the data volume status changes to <literal>Succeeded</literal>. You can start the VM.</simpara>
<simpara>Data volume provisioning happens in the background, so there is no need to monitor the process.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>The importer pod downloads the image from the specified URL and stores it on the provisioned persistent volume. View the status of the importer pod by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods</programlisting>
</listitem>
<listitem>
<simpara>Monitor the data volume until its status is <literal>Succeeded</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe dv fedora-dv <co xml:id="CO27-1"/></programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para>Specify the data volume name that you defined in the <literal>VirtualMachine</literal> manifest.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that provisioning is complete and that the VM has started by accessing its serial console:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl console vm-fedora-datavolume</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-creating-vms-uploading-images">
<title>Creating VMs by uploading images</title>

<simpara>You can create virtual machines (VMs) by uploading operating system images from your local machine.</simpara>
<simpara>You can create a Windows VM by uploading a Windows image to a PVC. Then you clone the PVC when you create the VM.</simpara>
<important>
<simpara>You must install the <link linkend="virt-installing-qemu-guest-agent">QEMU guest agent</link> on VMs created from operating system images that are not provided by Red Hat.</simpara>
<simpara>You must also install <link linkend="installing-virtio-drivers">VirtIO drivers</link> on Windows VMs.</simpara>
</important>
<section xml:id="virt-creating-vm-uploaded-image-web_virt-creating-vms-uploading-images">
<title>Creating a VM from an uploaded image by using the web console</title>
<simpara>You can create a virtual machine (VM) from an uploaded operating system image by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have an <literal>IMG</literal>, <literal>ISO</literal>, or <literal>QCOW2</literal> image file.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click a template tile without an available boot source.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Customize template parameters</emphasis> page, expand <emphasis role="strong">Storage</emphasis> and select <emphasis role="strong">Upload (Upload a new file to a PVC)</emphasis> from the <emphasis role="strong">Disk source</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Browse to the image on your local machine and set the disk size.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-windows-vm_virt-creating-vms-uploading-images">
<title>Creating a Windows VM</title>
<simpara>You can create a Windows virtual machine (VM) by uploading a Windows image to a persistent volume claim (PVC) and then cloning the PVC when you create a VM by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created a Windows installation DVD or USB with the Windows Media Creation Tool. See <link xlink:href="https://www.microsoft.com/en-us/software-download/windows10%20">Create Windows 10 installation media</link> in the Microsoft documentation.</simpara>
</listitem>
<listitem>
<simpara>You created an <literal>autounattend.xml</literal> answer file. See <link xlink:href="https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/update-windows-settings-and-scripts-create-your-own-answer-file-sxs">Answer files (unattend.xml)</link> in the Microsoft documentation.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Upload the Windows image as a new PVC:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to <emphasis role="strong">Storage</emphasis> &#8594; <emphasis role="strong">PersistentVolumeClaims</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create PersistentVolumeClaim</emphasis> &#8594; <emphasis role="strong">With Data upload form</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Browse to the Windows image and select it.</simpara>
</listitem>
<listitem>
<simpara>Enter the PVC name, select the storage class and size and then click <emphasis role="strong">Upload</emphasis>.</simpara>
<simpara>The Windows image is uploaded to a PVC.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Configure a new VM by cloning the uploaded PVC:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a Windows template tile and click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Clone (clone PVC)</emphasis> from the <emphasis role="strong">Disk source</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Select the PVC project, the Windows image PVC, and the disk size.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Apply the answer file to the VM:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine parameters</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Sysprep</emphasis> section of the <emphasis role="strong">Scripts</emphasis> tab, click <emphasis role="strong">Edit</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Browse to the <literal>autounattend.xml</literal> answer file and click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Set the run strategy of the VM:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Clear <emphasis role="strong">Start this VirtualMachine after creation</emphasis> so that the VM does not start immediately.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">YAML</emphasis> tab, replace <literal>running:false</literal> with <literal>runStrategy: RerunOnFailure</literal> and click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Start</emphasis>.</simpara>
<simpara>The VM boots from the <literal>sysprep</literal> disk containing the <literal>autounattend.xml</literal> answer file.</simpara>
</listitem>
</orderedlist>
<section xml:id="virt-generalizing-windows-sysprep_virt-creating-vms-uploading-images">
<title>Generalizing a Windows VM image</title>
<simpara>You can generalize a Windows operating system image to remove all system-specific configuration data before you use the image to create a new virtual machine (VM).</simpara>
<simpara>Before generalizing the VM, you must ensure the <literal>sysprep</literal> tool cannot detect an answer file after the unattended Windows installation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A running Windows VM with the QEMU guest agent installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a Windows VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Disks</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside the <literal>sysprep</literal> disk and select <emphasis role="strong">Detach</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Detach</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Rename <literal>C:\Windows\Panther\unattend.xml</literal> to avoid detection by the <literal>sysprep</literal> tool.</simpara>
</listitem>
<listitem>
<simpara>Start the <literal>sysprep</literal> program by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">%WINDIR%\System32\Sysprep\sysprep.exe /generalize /shutdown /oobe /mode:vm</programlisting>
</listitem>
<listitem>
<simpara>After the <literal>sysprep</literal> tool completes, the Windows VM shuts down. The disk image of the VM is now available to use as an installation image for Windows VMs.</simpara>
</listitem>
</orderedlist>
<simpara>You can now specialize the VM.</simpara>
</section>
<section xml:id="virt-specializing-windows-sysprep_virt-creating-vms-uploading-images">
<title>Specializing a Windows VM image</title>
<simpara>Specializing a Windows virtual machine (VM) configures the computer-specific information from a generalized Windows image onto the VM.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a generalized Windows disk image.</simpara>
</listitem>
<listitem>
<simpara>You must create an <literal>unattend.xml</literal> answer file. See the <link xlink:href="https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/update-windows-settings-and-scripts-create-your-own-answer-file-sxs?view=windows-11">Microsoft documentation</link> for details.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a Windows template and click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">PVC (clone PVC)</emphasis> from the <emphasis role="strong">Disk source</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Select the PVC project and PVC name of the generalized Windows image.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine parameters</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Scripts</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Sysprep</emphasis> section, click <emphasis role="strong">Edit</emphasis>, browse to the <literal>unattend.xml</literal> answer file, and click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>During the initial boot, Windows uses the <literal>unattend.xml</literal> answer file to specialize the VM. The VM is now ready to use.</simpara>
<itemizedlist xml:id="additional-resources-creating-windows-vms" role="_additional-resources">
<title>Additional resources for creating Windows VMs</title>
<listitem>
<simpara><link xlink:href="https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/sysprep--generalize--a-windows-installation">Microsoft, Sysprep (Generalize) a Windows installation</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/generalize">Microsoft, generalize</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.microsoft.com/en-us/windows-hardware/manufacture/desktop/specialize">Microsoft, specialize</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-uploading-image-virtctl_virt-creating-vms-uploading-images">
<title>Creating a VM from an uploaded image by using the command line</title>
<simpara>You can upload an operating system image by using the <literal>virtctl</literal> command line tool. You can use an existing data volume or create a new data volume for the image.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have an <literal>ISO</literal>, <literal>IMG</literal>, or <literal>QCOW2</literal> operating system image file.</simpara>
</listitem>
<listitem>
<simpara>For best performance, compress the image file by using the <link xlink:href="https://libguestfs.org/virt-sparsify.1.html">virt-sparsify</link> tool or the <literal>xz</literal> or <literal>gzip</literal> utilities.</simpara>
</listitem>
<listitem>
<simpara>You must have <literal>virtctl</literal> installed.</simpara>
</listitem>
<listitem>
<simpara>The client machine must be configured to trust the OpenShift Container Platform router&#8217;s
certificate.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Upload the image by running the <literal>virtctl image-upload</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl image-upload dv &lt;datavolume_name&gt; \ <co xml:id="CO28-1"/>
  --size=&lt;datavolume_size&gt; \ <co xml:id="CO28-2"/>
  --image-path=&lt;/path/to/image&gt; \ <co xml:id="CO28-3"/></programlisting>
<calloutlist>
<callout arearefs="CO28-1">
<para>The name of the data volume.</para>
</callout>
<callout arearefs="CO28-2">
<para>The size of the data volume. For example: <literal>--size=500Mi</literal>, <literal>--size=1G</literal></para>
</callout>
<callout arearefs="CO28-3">
<para>The file path of the image.</para>
</callout>
</calloutlist>
<note>
<itemizedlist>
<listitem>
<simpara>If you do not want to create a new data volume, omit the <literal>--size</literal> parameter and include the <literal>--no-create</literal> flag.</simpara>
</listitem>
<listitem>
<simpara>When uploading a disk image to a PVC, the PVC size must be larger than the size of the uncompressed virtual disk.</simpara>
</listitem>
<listitem>
<simpara>To allow insecure server connections when using HTTPS, use the <literal>--insecure</literal> parameter. When you use the <literal>--insecure</literal> flag, the authenticity of the upload endpoint is <emphasis role="strong">not</emphasis> verified.</simpara>
</listitem>
</itemizedlist>
</note>
</listitem>
<listitem>
<simpara>Optional. To verify that a data volume was created, view all data volumes by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dvs</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-creating-vms-by-cloning-pvcs">
<title>Creating VMs by cloning PVCs</title>

<simpara>You can create virtual machines (VMs) by cloning existing persistent volume claims (PVCs) with custom images.</simpara>
<simpara>You clone a PVC by creating a data volume that references a source PVC.</simpara>
<simpara>You must install the <link linkend="virt-installing-qemu-guest-agent">QEMU guest agent</link> on VMs created from operating system images that are not provided by Red Hat.</simpara>
<section xml:id="virt-creating-vm-custom-image-web_virt-creating-vms-by-cloning-pvcs">
<title>Creating a VM from a PVC by using the web console</title>
<simpara>You can create a virtual machine (VM) by importing an image from a web page by using the OpenShift Container Platform web console.
You can create a virtual machine (VM) by cloning a persistent volume claim (PVC) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to the web page that contains the image.</simpara>
</listitem>
<listitem>
<simpara>You must have access to the namespace that contains the source PVC.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click a template tile without an available boot source.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Customize template parameters</emphasis> page, expand <emphasis role="strong">Storage</emphasis> and select <emphasis role="strong">PVC (clone PVC)</emphasis> from the <emphasis role="strong">Disk source</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Enter the image URL. Example: <literal>https://access.redhat.com/downloads/content/69/ver=/rhel---7/7.9/x86_64/product-software</literal></simpara>
</listitem>
<listitem>
<simpara>Enter the container image URL. Example: <literal>https://mirror.arizona.edu/fedora/linux/releases/38/Cloud/x86_64/images/Fedora-Cloud-Base-38-1.6.x86_64.qcow2</literal></simpara>
</listitem>
<listitem>
<simpara>Select the PVC project and the PVC name.</simpara>
</listitem>
<listitem>
<simpara>Set the disk size.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-vm-by-cloning-pvcs-cli">
<title>Creating a VM from a PVC by using the command line</title>
<simpara>You can create a virtual machine (VM) by cloning the persistent volume claim (PVC) of an existing VM by using the command line.</simpara>
<simpara>You can clone a PVC by using one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara>Cloning a PVC to a new data volume.</simpara>
<simpara>This method creates a data volume whose lifecycle is independent of the original VM. Deleting the original VM does not affect the new data volume or its associated PVC.</simpara>
</listitem>
<listitem>
<simpara>Cloning a PVC by creating a <literal>VirtualMachine</literal> manifest with a <literal>dataVolumeTemplates</literal> stanza.</simpara>
<simpara>This method creates a data volume whose lifecycle is dependent on the original VM. Deleting the original VM deletes the cloned data volume and its associated PVC.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-cloning-pvc-to-dv-cli_virt-creating-vms-by-cloning-pvcs">
<title>Cloning a PVC to a data volume</title>
<simpara>You can clone the persistent volume claim (PVC) of an existing virtual machine (VM) disk to a data volume by using the command line.</simpara>
<simpara>You create a data volume that references the original source PVC. The lifecycle of the new data volume is independent of the original VM. Deleting the original VM does not affect the new data volume or its associated PVC.</simpara>
<simpara>Cloning between different volume modes is supported for host-assisted cloning, such as cloning from a block persistent volume (PV) to a file system PV, as long as the source and target PVs belong to the <literal>kubevirt</literal> content type.</simpara>
<note>
<simpara>Smart-cloning is faster and more efficient than host-assisted cloning because it uses snapshots to clone PVCs. Smart-cloning is supported by storage providers that support snapshots, such as Red Hat OpenShift Data Foundation.</simpara>
<simpara>Cloning between different volume modes is not supported for smart-cloning.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The VM with the source PVC must be powered down.</simpara>
</listitem>
<listitem>
<simpara>If you clone a PVC to a different namespace, you must have permissions to create resources in the target namespace.</simpara>
</listitem>
<listitem>
<simpara>Additional prerequisites for smart-cloning:</simpara>
<itemizedlist>
<listitem>
<simpara>Your storage provider must support snapshots.</simpara>
</listitem>
<listitem>
<simpara>The source and target PVCs must have the same storage provider and volume mode.</simpara>
</listitem>
<listitem>
<simpara>The value of the <literal>driver</literal> key of the <literal>VolumeSnapshotClass</literal> object must match the value of the <literal>provisioner</literal> key of the <literal>StorageClass</literal> object as shown in the following example:</simpara>
<formalpara>
<title>Example <literal>VolumeSnapshotClass</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: VolumeSnapshotClass
apiVersion: snapshot.storage.k8s.io/v1
driver: openshift-storage.rbd.csi.ceph.com
# ...</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example <literal>StorageClass</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
# ...
provisioner: openshift-storage.rbd.csi.ceph.com</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>DataVolume</literal> manifest as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: &lt;datavolume&gt; <co xml:id="CO29-1"/>
spec:
  source:
    pvc:
      namespace: "&lt;source_namespace&gt;" <co xml:id="CO29-2"/>
      name: "&lt;my_vm_disk&gt;" <co xml:id="CO29-3"/>
  storage: {}</programlisting>
<calloutlist>
<callout arearefs="CO29-1">
<para>Specify the name of the new data volume.</para>
</callout>
<callout arearefs="CO29-2">
<para>Specify the namespace of the source PVC.</para>
</callout>
<callout arearefs="CO29-3">
<para>Specify the name of the source PVC.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the data volume by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;datavolume&gt;.yaml</programlisting>
<note>
<simpara>Data volumes prevent a VM from starting before the PVC is prepared. You can create a VM that references the new data volume while the
PVC is being cloned.</simpara>
</note>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-vm-cloning-pvc-data-volume-template_virt-creating-vms-by-cloning-pvcs">
<title>Creating a VM from a cloned PVC by using a data volume template</title>
<simpara>You can create a virtual machine (VM) that clones the persistent volume claim (PVC) of an existing VM by using a data volume template.</simpara>
<simpara>This method creates a data volume whose lifecycle is dependent on the original VM. Deleting the original VM deletes the cloned data volume and its associated PVC.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The VM with the source PVC must be powered down.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>VirtualMachine</literal> manifest as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: vm-dv-clone
  name: vm-dv-clone <co xml:id="CO30-1"/>
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-dv-clone
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: root-disk
        resources:
          requests:
            memory: 64M
      volumes:
      - dataVolume:
          name: favorite-clone
        name: root-disk
  dataVolumeTemplates:
  - metadata:
      name: favorite-clone
    spec:
      storage:
        accessModes:
        - ReadWriteOnce
        resources:
          requests:
            storage: 2Gi
      source:
        pvc:
          namespace: &lt;source_namespace&gt; <co xml:id="CO30-2"/>
          name: "&lt;source_pvc&gt;" <co xml:id="CO30-3"/></programlisting>
<calloutlist>
<callout arearefs="CO30-1">
<para>Specify the name of the VM.</para>
</callout>
<callout arearefs="CO30-2">
<para>Specify the namespace of the source PVC.</para>
</callout>
<callout arearefs="CO30-3">
<para>Specify the name of the source PVC.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the virtual machine with the PVC-cloned data volume:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;vm-clone-datavolumetemplate&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="virt-installing-qemu-guest-agent">
<title>Installing the QEMU guest agent and VirtIO drivers</title>

<simpara>The QEMU guest agent is a daemon that runs on the virtual machine (VM) and passes information to the host about the VM, users, file systems, and secondary networks.</simpara>
<simpara>You must install the QEMU guest agent on VMs created from operating system images that are not provided by Red Hat.</simpara>
<section xml:id="installing-qemu-guest-agent">
<title>Installing the QEMU guest agent</title>
<section xml:id="virt-installing-qemu-guest-agent-on-linux-vm_virt-installing-qemu-guest-agent">
<title>Installing the QEMU guest agent on a Linux VM</title>
<simpara>The <literal>qemu-guest-agent</literal> is widely available and available by default in Red Hat Enterprise Linux (RHEL) virtual machines (VMs). Install the agent and start the service.</simpara>
<note>
<simpara>To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.</simpara>
<simpara>The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the VM by using a console or SSH.</simpara>
</listitem>
<listitem>
<simpara>Install the QEMU guest agent by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ yum install -y qemu-guest-agent</programlisting>
</listitem>
<listitem>
<simpara>Ensure the service is persistent and start it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ systemctl enable --now qemu-guest-agent</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Run the following command to verify that <literal>AgentConnected</literal> is listed in the VM spec:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vm &lt;vm_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="installing-qemu-guest-agent-on-windows-vm_virt-installing-qemu-guest-agent">
<title>Installing the QEMU guest agent on a Windows VM</title>
<simpara>For Windows virtual machines (VMs), the QEMU guest agent is included in the VirtIO drivers. You can install the drivers during a Windows installation or on an existing Windows VM.</simpara>
<note>
<simpara>To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent.</simpara>
<simpara>The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the Windows guest operating system, use the <emphasis role="strong">File Explorer</emphasis> to navigate to the <literal>guest-agent</literal> directory in the <literal>virtio-win</literal> CD drive.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal>qemu-ga-x86_64.msi</literal> installer.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Obtain a list of network services by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ net start</programlisting>
</listitem>
<listitem>
<simpara>Verify that the output contains the <literal>QEMU Guest Agent</literal>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="installing-virtio-drivers">
<title>Installing VirtIO drivers on Windows VMs</title>
<simpara>VirtIO drivers are paravirtualized device drivers required for Microsoft Windows virtual machines (VMs) to run in OpenShift Virtualization. The drivers are shipped with the rest of the images and do not require a separate download.</simpara>
<simpara>The <literal>container-native-virtualization/virtio-win</literal> container disk must be attached to the VM as a SATA CD drive to enable driver installation. You can install VirtIO drivers during Windows installation or added to an existing Windows installation.</simpara>
<simpara>After the drivers are installed, the <literal>container-native-virtualization/virtio-win</literal> container disk can be removed from the VM.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Supported drivers</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Driver name</entry>
<entry align="left" valign="top">Hardware ID</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">viostor</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>VEN_1AF4&amp;DEV_1001<?asciidoc-br?>
VEN_1AF4&amp;DEV_1042</simpara></entry>
<entry align="left" valign="top"><simpara>The block driver. Sometimes labeled as an <emphasis role="strong">SCSI Controller</emphasis> in the <emphasis role="strong">Other devices</emphasis> group.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">viorng</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>VEN_1AF4&amp;DEV_1005<?asciidoc-br?>
VEN_1AF4&amp;DEV_1044</simpara></entry>
<entry align="left" valign="top"><simpara>The entropy source driver. Sometimes labeled as a <emphasis role="strong">PCI Device</emphasis> in the <emphasis role="strong">Other devices</emphasis> group.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">NetKVM</emphasis></simpara></entry>
<entry align="left" valign="top"><simpara>VEN_1AF4&amp;DEV_1000<?asciidoc-br?>
VEN_1AF4&amp;DEV_1041</simpara></entry>
<entry align="left" valign="top"><simpara>The network driver. Sometimes labeled as an <emphasis role="strong">Ethernet Controller</emphasis> in the <emphasis role="strong">Other devices</emphasis> group. Available only if a VirtIO NIC is configured.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<section xml:id="virt-attaching-virtio-disk-to-windows_virt-installing-qemu-guest-agent">
<title>Attaching VirtIO container disk to Windows VMs during installation</title>
<simpara>You must attach the VirtIO container disk to the Windows VM to install the necessary Windows drivers. This can be done during creation of the VM.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>When creating a Windows VM from a template, click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Mount Windows drivers disk</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Customize VirtualMachine parameters</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>After the VM is created, the <literal>virtio-win</literal> SATA CD disk will be attached to the VM.</simpara>
</section>
<section xml:id="virt-attaching-virtio-disk-to-windows-existing_virt-installing-qemu-guest-agent">
<title>Attaching VirtIO container disk to an existing Windows VM</title>
<simpara>You must attach the VirtIO container disk to the Windows VM to install the necessary Windows drivers. This can be done to an existing VM.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the existing Windows VM, and click <emphasis role="strong">Actions</emphasis> &#8594; <emphasis role="strong">Stop</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Go to <emphasis role="strong">VM Details</emphasis> &#8594; <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Disks</emphasis> and click <emphasis role="strong">Add disk</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Add <literal>windows-driver-disk</literal> from container source, set the <emphasis role="strong">Type</emphasis> to <emphasis role="strong">CD-ROM</emphasis>, and then set the <emphasis role="strong">Interface</emphasis> to <emphasis role="strong">SATA</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Start the VM, and connect to a graphical console.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-installing-virtio-drivers-installing-windows_virt-installing-qemu-guest-agent">
<title>Installing VirtIO drivers during Windows installation</title>
<simpara>You can install the VirtIO drivers while installing Windows on a virtual machine (VM).</simpara>
<note>
<simpara>This procedure uses a generic approach to the Windows installation and the installation method might differ between versions of Windows. See the documentation for the version of Windows that you are installing.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A storage device containing the <literal>virtio</literal> drivers must be attached to the VM.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the Windows operating system, use the <literal>File Explorer</literal> to navigate to the <literal>virtio-win</literal> CD drive.</simpara>
</listitem>
<listitem>
<simpara>Double-click the drive to run the appropriate installer for your VM.</simpara>
<simpara>For a 64-bit vCPU, select the <literal>virtio-win-gt-x64</literal> installer. 32-bit vCPUs are no longer supported.</simpara>
</listitem>
<listitem>
<simpara>Optional: During the <emphasis role="strong">Custom Setup</emphasis> step of the installer, select the device drivers you want to install. The recommended driver set is selected by default.</simpara>
</listitem>
<listitem>
<simpara>After the installation is complete, select <emphasis role="strong">Finish</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Reboot the VM.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Open the system disk on the PC. This is typically <literal>C:</literal>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Program Files</emphasis> &#8594; <emphasis role="strong">Virtio-Win</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>If the <emphasis role="strong">Virtio-Win</emphasis> directory is present and contains a sub-directory for each driver, the installation was successful.</simpara>
</section>
<section xml:id="virt-installing-virtio-drivers-existing-windows_virt-installing-qemu-guest-agent">
<title>Installing VirtIO drivers from a SATA CD drive on an existing Windows VM</title>
<simpara>You can install the VirtIO drivers from a SATA CD drive on an existing Windows virtual machine (VM).</simpara>
<note>
<simpara>This procedure uses a generic approach to adding drivers to Windows. See the installation documentation for your version of Windows for specific installation steps.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A storage device containing the virtio drivers must be attached to the VM as a SATA CD drive.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start the VM and connect to a graphical console.</simpara>
</listitem>
<listitem>
<simpara>Log in to a Windows user session.</simpara>
</listitem>
<listitem>
<simpara>Open <emphasis role="strong">Device Manager</emphasis> and expand <emphasis role="strong">Other devices</emphasis> to list any <emphasis role="strong">Unknown device</emphasis>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Open the <emphasis role="strong">Device Properties</emphasis> to identify the unknown device.</simpara>
</listitem>
<listitem>
<simpara>Right-click the device and select <emphasis role="strong">Properties</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Details</emphasis> tab and select <emphasis role="strong">Hardware Ids</emphasis> in the <emphasis role="strong">Property</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Compare the <emphasis role="strong">Value</emphasis> for the <emphasis role="strong">Hardware Ids</emphasis> with the supported VirtIO drivers.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Right-click the device and select <emphasis role="strong">Update Driver Software</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Browse my computer for driver software</emphasis> and browse to the attached
SATA CD drive, where the VirtIO drivers are located. The drivers are arranged
hierarchically according to their driver type, operating system,
and CPU architecture.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis> to install the driver.</simpara>
</listitem>
<listitem>
<simpara>Repeat this process for all the necessary VirtIO drivers.</simpara>
</listitem>
<listitem>
<simpara>After the driver installs, click <emphasis role="strong">Close</emphasis> to close the window.</simpara>
</listitem>
<listitem>
<simpara>Reboot the VM to complete the driver installation.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-adding-container-disk-as-cd_virt-installing-qemu-guest-agent">
<title>Installing VirtIO drivers from a container disk added as a SATA CD drive</title>
<simpara>You can install VirtIO drivers from a container disk that you add to a Windows virtual machine (VM) as a SATA CD drive.</simpara>
<tip>
<simpara>Downloading the <literal>container-native-virtualization/virtio-win</literal> container disk from the <link xlink:href="https://catalog.redhat.com/software/containers/search?q=virtio-win&amp;p=1">Red Hat Ecosystem Catalog</link> is not mandatory, because the container disk is downloaded from the Red Hat registry if it not already present in the cluster. However, downloading reduces the installation time.</simpara>
</tip>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to the Red Hat registry or to the downloaded <literal>container-native-virtualization/virtio-win</literal> container disk in a restricted environment.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the <literal>container-native-virtualization/virtio-win</literal> container disk as a CD drive by editing the <literal>VirtualMachine</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  domain:
    devices:
      disks:
        - name: virtiocontainerdisk
          bootOrder: 2 <co xml:id="CO31-1"/>
          cdrom:
            bus: sata
volumes:
  - containerDisk:
      image: container-native-virtualization/virtio-win
    name: virtiocontainerdisk</programlisting>
<calloutlist>
<callout arearefs="CO31-1">
<para>OpenShift Virtualization boots the VM disks in the order defined in the <literal>VirtualMachine</literal> manifest. You can either define other VM disks that boot before the <literal>container-native-virtualization/virtio-win</literal> container disk or use the optional <literal>bootOrder</literal> parameter to ensure the VM boots from the correct disk. If you configure the boot order for a disk, you must configure the boot order for the other disks.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the changes:</simpara>
<itemizedlist>
<listitem>
<simpara>If the VM is not running, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl start &lt;vm&gt;</programlisting>
</listitem>
<listitem>
<simpara>If the VM is running, reboot the VM or run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;vm.yaml&gt;</programlisting>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>After the VM has started, install the VirtIO drivers from the SATA CD drive.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="updating-virtio-drivers">
<title>Updating VirtIO drivers</title>
<section xml:id="virt-updating-virtio-drivers-windows_virt-installing-qemu-guest-agent">
<title>Updating VirtIO drivers on a Windows VM</title>
<simpara>Update the <literal>virtio</literal> drivers on a Windows virtual machine (VM) by using the Windows Update service.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cluster must be connected to the internet. Disconnected clusters cannot reach the Windows Update service.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the Windows Guest operating system, click the <emphasis role="strong">Windows</emphasis> key and select <emphasis role="strong">Settings</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Navigate to <emphasis role="strong">Windows Update</emphasis> &#8594; <emphasis role="strong">Advanced Options</emphasis> &#8594; <emphasis role="strong">Optional Updates</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Install all updates from <emphasis role="strong">Red Hat, Inc.</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Reboot the VM.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>On the Windows VM, navigate to the <emphasis role="strong">Device Manager</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a device.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Driver</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Driver Details</emphasis> and confirm that the <literal>virtio</literal> driver details displays the correct version.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
<section xml:id="virt-accessing-vm-consoles">
<title>Connecting to virtual machine consoles</title>

<simpara>You can connect to the following consoles to access running virtual machines (VMs):</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="vnc-console_virt-accessing-vm-consoles">VNC console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="serial-console_virt-accessing-vm-consoles">Serial console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="desktop-viewer_virt-accessing-vm-consoles">Desktop viewer for Windows VMs</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="vnc-console_virt-accessing-vm-consoles">
<title>Connecting to the VNC console</title>
<simpara>You can connect to the VNC console of a virtual machine by using the OpenShift Container Platform web console or the <literal>virtctl</literal> command line tool.</simpara>
<section xml:id="virt-connecting-to-vm-console-web_vnc-console">
<title>Connecting to the VNC console by using the web console</title>
<simpara>You can connect to the VNC console of a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<note>
<simpara>If you connect to a Windows VM with a vGPU assigned as a mediated device, you can switch between the default display and the vGPU display.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>On the <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> page, click a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Console</emphasis> tab. The VNC console session starts automatically.</simpara>
</listitem>
<listitem>
<simpara>Optional: To switch to the vGPU display of a Windows VM, select <emphasis role="strong">Ctl + Alt + 2</emphasis> from the <emphasis role="strong">Send key</emphasis> list.</simpara>
<itemizedlist>
<listitem>
<simpara>Select <emphasis role="strong">Ctl + Alt + 1</emphasis> from the <emphasis role="strong">Send key</emphasis> list to restore the default display.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To end the console session, click outside the console pane and then click <emphasis role="strong">Disconnect</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-connecting-vm-virtctl_vnc-console">
<title>Connecting to the VNC console by using virtctl</title>
<simpara>You can use the <literal>virtctl</literal> command line tool to connect to the VNC console of a running virtual machine.</simpara>
<note>
<simpara>If you run the <literal>virtctl vnc</literal> command on a remote machine over an SSH connection, you must forward the X session to your local machine by running the <literal>ssh</literal> command with the <literal>-X</literal> or <literal>-Y</literal> flags.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the <literal>virt-viewer</literal> package.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to start the console session:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl vnc &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>If the connection fails, run the following command to collect
troubleshooting information:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl vnc &lt;vm_name&gt; -v 4</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-temporary-token-VNC_vnc-console">
<title>Generating a temporary token for the VNC console</title>
<simpara>Generate a temporary authentication bearer token for the Kubernetes API to access the VNC of a virtual machine (VM).</simpara>
<note>
<simpara>Kubernetes also supports authentication using client certificates, instead of a bearer token, by modifying the curl command.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A running virtual machine with OpenShift Virtualization 4.14 or later and <link xlink:href="../../virt/about-virt/virt-architecture.xml#virt-about-ssp-operator_virt-architecture"><literal>ssp-operator</literal></link> 4.14 or later</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable the feature gate in the HyperConverged (<literal>HCO</literal>) custom resource (CR):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv --type json -p '[{"op": "replace", "path": "/spec/featureGates/deployVmConsoleProxy", "value": true}]'
# ...</programlisting>
</listitem>
<listitem>
<simpara>Generate a token by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --header "Authorization: Bearer ${TOKEN}" \
     "https://api.&lt;cluster_fqdn&gt;/apis/token.kubevirt.io/v1alpha1/namespaces/&lt;namespace&gt;/virtualmachines/&lt;vm_name&gt;/vnc?duration=&lt;duration&gt;" <co xml:id="CO32-1"/></programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>Duration can be in hours and minutes, with a minimum duration of 10 minutes. Example: <literal>5h30m</literal>. The token is valid for 10 minutes by default if this parameter is not set.</para>
</callout>
</calloutlist>
<simpara>Sample output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">{ "token": "eyJhb..." }</programlisting>
</listitem>
<listitem>
<simpara>Optional: Use the token provided in the output to create a variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export VNC_TOKEN="&lt;token&gt;"</programlisting>
</listitem>
</orderedlist>
<simpara>You can now use the token to access the VNC console of a VM.</simpara>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Log in to the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc login --token ${VNC_TOKEN}</programlisting>
</listitem>
<listitem>
<simpara>Use <literal>virtctl</literal> to test access to the VNC console of the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl vnc &lt;vm_name&gt; -n &lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="serial-console_virt-accessing-vm-consoles">
<title>Connecting to the serial console</title>
<simpara>You can connect to the serial console of a virtual machine by using the OpenShift Container Platform web console or the <literal>virtctl</literal> command line tool.</simpara>
<note>
<simpara>Running concurrent VNC connections to a single virtual machine is not currently supported.</simpara>
</note>
<section xml:id="virt-connecting-to-vm-console-web_serial-console">
<title>Connecting to the serial console by using the web console</title>
<simpara>You can connect to the serial console of a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>On the <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> page, click a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Console</emphasis> tab. The VNC console session starts automatically.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Disconnect</emphasis> to end the VNC console session. Otherwise, the VNC console session continues to run in the background.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Serial console</emphasis> from the console list.</simpara>
</listitem>
<listitem>
<simpara>To end the console session, click outside the console pane and then click <emphasis role="strong">Disconnect</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-connecting-vm-virtctl_serial-console">
<title>Connecting to the serial console by using virtctl</title>
<simpara>You can use the <literal>virtctl</literal> command line tool to connect to the serial console of a running virtual machine.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to start the console session:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl console &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Press <literal>Ctrl+]</literal> to end the console session.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="desktop-viewer_virt-accessing-vm-consoles">
<title>Connecting to the desktop viewer</title>
<simpara>You can connect to a Windows virtual machine (VM) by using the desktop viewer and the Remote Desktop Protocol (RDP).</simpara>
<section xml:id="virt-connecting-to-vm-console-web_desktop-viewer">
<title>Connecting to the desktop viewer by using the web console</title>
<simpara>You can connect to the desktop viewer of a Windows virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the QEMU guest agent on the Windows VM.</simpara>
</listitem>
<listitem>
<simpara>You have an RDP client installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>On the <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> page, click a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Console</emphasis> tab. The VNC console session starts automatically.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Disconnect</emphasis> to end the VNC console session. Otherwise, the VNC console session continues to run in the background.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Desktop viewer</emphasis> from the console list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create RDP Service</emphasis> to open the <emphasis role="strong">RDP Service</emphasis> dialog.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Expose RDP Service</emphasis> and click <emphasis role="strong">Save</emphasis> to create a node port service.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Launch Remote Desktop</emphasis> to download an <literal>.rdp</literal> file and launch the desktop viewer.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="virt-accessing-vm-ssh">
<title>Configuring SSH access to virtual machines</title>

<simpara>You can configure SSH access to virtual machines (VMs) by using the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="using-virtctl-ssh_virt-accessing-vm-ssh"><literal>virtctl ssh</literal> command</link></simpara>
<simpara>You create an SSH key pair, add the public key to a VM, and connect to the VM by running the <literal>virtctl ssh</literal> command with the private key.</simpara>
<simpara>You can add public SSH keys to Red Hat Enterprise Linux (RHEL) 9 VMs at runtime or at first boot to VMs with guest operating systems that can be configured by using a cloud-init data source.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-using-virtctl-port-forward-command_virt-accessing-vm-ssh"><literal>virtctl port-forward</literal> command</link></simpara>
<simpara>You add the <literal>virtctl port-foward</literal> command to your <literal>.ssh/config</literal> file and connect to the VM by using OpenSSH.</simpara>
</listitem>
<listitem>
<simpara><link linkend="using-services-ssh_virt-accessing-vm-ssh">Service</link></simpara>
<simpara>You create a service, associate the service with the VM, and connect to the IP address and port exposed by the service.</simpara>
</listitem>
<listitem>
<simpara><link linkend="using-secondary-networks-ssh_virt-accessing-vm-ssh">Secondary network</link></simpara>
<simpara>You configure a secondary network, attach a virtual machine (VM) to the secondary network interface, and connect to the DHCP-allocated IP address.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-access-configuration-considerations_virt-accessing-vm-ssh">
<title>Access configuration considerations</title>
<simpara>Each method for configuring access to a virtual machine (VM) has advantages and limitations, depending on the traffic load and client requirements.</simpara>
<simpara>Services provide excellent performance and are recommended for applications that are accessed from outside the cluster.</simpara>
<simpara>If the internal cluster network cannot handle the traffic load, you can configure a secondary network.</simpara>
<variablelist>
<varlistentry>
<term><literal>virtctl ssh</literal> and <literal>virtctl port-forwarding</literal> commands</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Simple to configure.</simpara>
</listitem>
<listitem>
<simpara>Recommended for troubleshooting VMs.</simpara>
</listitem>
<listitem>
<simpara><literal>virtctl port-forwarding</literal> recommended for automated configuration of VMs with Ansible.</simpara>
</listitem>
<listitem>
<simpara>Dynamic public SSH keys can be used to provision VMs with Ansible.</simpara>
</listitem>
<listitem>
<simpara>Not recommended for high-traffic applications like Rsync or Remote Desktop Protocol because of the burden on the API server.</simpara>
</listitem>
<listitem>
<simpara>The API server must be able to handle the traffic load.</simpara>
</listitem>
<listitem>
<simpara>The clients must be able to access the API server.</simpara>
</listitem>
<listitem>
<simpara>The clients must have access credentials for the cluster.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Cluster IP service</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>The internal cluster network must be able to handle the traffic load.</simpara>
</listitem>
<listitem>
<simpara>The clients must be able to access an internal cluster IP address.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Node port service</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>The internal cluster network must be able to handle the traffic load.</simpara>
</listitem>
<listitem>
<simpara>The clients must be able to access at least one node.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Load balancer service</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>A load balancer must be configured.</simpara>
</listitem>
<listitem>
<simpara>Each node must be able to handle the traffic load of one or more load balancer services.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>Secondary network</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Excellent performance because traffic does not go through the internal cluster network.</simpara>
</listitem>
<listitem>
<simpara>Allows a flexible approach to network topology.</simpara>
</listitem>
<listitem>
<simpara>Guest operating system must be configured with appropriate security because the VM is exposed directly to the secondary network. If a VM is compromised, an intruder could gain access to the secondary network.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="using-virtctl-ssh_virt-accessing-vm-ssh">
<title>Using virtctl ssh</title>
<simpara>You can add a public SSH key to a virtual machine (VM) and connect to the VM by running the <literal>virtctl ssh</literal> command.</simpara>
<simpara>This method is simple to configure. However, it is not recommended for high traffic loads because it places a burden on the API server.</simpara>
<section xml:id="virt-about-static-and-dynamic-ssh-keys_virt-accessing-vm-ssh">
<title>About static and dynamic SSH key management</title>
<simpara>You can add public SSH keys to virtual machines (VMs) statically at first boot or dynamically at runtime.</simpara>
<note>
<simpara>Only Red Hat Enterprise Linux (RHEL) 9 supports dynamic key injection.</simpara>
</note>
<bridgehead xml:id="static-key-management_virt-accessing-vm-ssh" renderas="sect5">Static SSH key management</bridgehead>
<simpara>You can add a statically managed SSH key to a VM with a guest operating system that supports configuration by using a cloud-init data source. The key is added to the virtual machine (VM) at first boot.</simpara>
<simpara>You can add the key by using one of the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara>Add a key to a single VM when you create it by using the web console or the command line.</simpara>
</listitem>
<listitem>
<simpara>Add a key to a project by using the web console. Afterwards, the key is automatically added to the VMs that you create in this project.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Use cases</title>
<listitem>
<simpara>As a VM owner, you can provision all your newly created VMs with a single key.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="dynamic-key-management_virt-accessing-vm-ssh" renderas="sect5">Dynamic SSH key management</bridgehead>
<simpara>You can enable dynamic SSH key management for a VM with Red Hat Enterprise Linux (RHEL) 9 installed. Afterwards, you can update the key during runtime. The key is added by the QEMU guest agent, which is installed with Red Hat boot sources.</simpara>
<simpara>You can disable dynamic key management for security reasons. Then, the VM inherits the key management setting of the image from which it was created.</simpara>
<itemizedlist>
<title>Use cases</title>
<listitem>
<simpara>Granting or revoking access to VMs: As a cluster administrator, you can grant or revoke remote VM access by adding or removing the keys of individual users from a <literal>Secret</literal> object that is applied to all VMs in a namespace.</simpara>
</listitem>
<listitem>
<simpara>User access: You can add your access credentials to all VMs that you create and manage.</simpara>
</listitem>
<listitem>
<simpara>Ansible provisioning:</simpara>
<itemizedlist>
<listitem>
<simpara>As an operations team member, you can create a single secret that contains all the keys used for Ansible provisioning.</simpara>
</listitem>
<listitem>
<simpara>As a VM owner, you can create a VM and attach the keys used for Ansible provisioning.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Key rotation:</simpara>
<itemizedlist>
<listitem>
<simpara>As a cluster administrator, you can rotate the Ansible provisioner keys used by VMs in a namespace.</simpara>
</listitem>
<listitem>
<simpara>As a workload owner, you can rotate the key for the VMs that you manage.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="static-key-management-vm">
<title>Static key management</title>
<simpara>You can add a statically managed public SSH key when you create a virtual machine (VM) by using the OpenShift Container Platform web console or the command line. The key is added as a cloud-init data source when the VM boots for the first time.</simpara>
<tip>
<simpara>You can also add the key to a project by using the OpenShift Container Platform web console. Afterwards, this key is added automatically to VMs that you create in the project.</simpara>
</tip>
<section xml:id="virt-adding-key-creating-vm-template_static-key">
<title>Adding a key when creating a VM from a template</title>
<simpara>You can add a statically managed public SSH key when you create a virtual machine (VM) by using the OpenShift Container Platform web console. The key is added to the VM as a cloud-init data source at first boot. This method does not affect cloud-init user data.</simpara>
<simpara>Optional: You can add a key to a project. Afterwards, this key is added automatically to VMs that you create in the project.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You generated an SSH key pair by running the <literal>ssh-keygen</literal> command.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click a template tile.</simpara>
<simpara>The guest operating system must support configuration from a cloud-init data source.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Scripts</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>If you have not already added a public SSH key to your project, click the edit icon beside <emphasis role="strong">Authorized SSH key</emphasis> and select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Use existing</emphasis>: Select a secret from the secrets list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Add new</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Browse to the SSH key file or paste the file in the key field.</simpara>
</listitem>
<listitem>
<simpara>Enter the secret name.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">Automatically apply this key to any new VirtualMachine you create in this project</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
<simpara>The <emphasis role="strong">VirtualMachine details</emphasis> page displays the progress of the VM creation.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Click the <emphasis role="strong">Scripts</emphasis> tab on the <emphasis role="strong">Configuration</emphasis> tab.</simpara>
<simpara>The secret name is displayed in the <emphasis role="strong">Authorized SSH key</emphasis> section.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-vm-instancetype_static-key">
<title>Adding a key when creating a VM from an instance type</title>
<simpara>You can create a virtual machine (VM) from an instance type by using the OpenShift Container Platform web console.
You can add a statically managed SSH key when you create a virtual machine (VM) from an instance type by using the OpenShift Container Platform web console. The key is added to the VM as a cloud-init data source at first boot. This method does not affect cloud-init user data.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> and click the <emphasis role="strong">InstanceTypes</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Select a bootable volume.</simpara>
<note>
<simpara>The volume table only lists volumes in the <literal>openshift-virtualization-os-images</literal> namespace that have the <literal>instancetype.kubevirt.io/default-preference</literal> label.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Optional: Click the star icon to designate a bootable volume as a favorite. Starred bootable volumes appear first in the volume list.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click an instance type tile and select the resource size appropriate for your workload.</simpara>
</listitem>
<listitem>
<simpara>If you have not already added a public SSH key to your project, click the edit icon beside <emphasis role="strong">Authorized SSH key</emphasis> in the <emphasis role="strong">VirtualMachine details</emphasis> section.</simpara>
</listitem>
<listitem>
<simpara>Select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Use existing</emphasis>: Select a secret from the secrets list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Add new</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Browse to the public SSH key file or paste the file in the key field.</simpara>
</listitem>
<listitem>
<simpara>Enter the secret name.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">Automatically apply this key to any new VirtualMachine you create in this project</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Optional: Click <emphasis role="strong">View YAML &amp; CLI</emphasis> to view the YAML file. Click <emphasis role="strong">CLI</emphasis> to view the CLI commands. You can also download or copy either the YAML file contents or the CLI commands.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>After the VM is created, you can monitor the status on the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</section>
<section xml:id="virt-adding-public-key-cli_static-key">
<title>Adding a key when creating a VM by using the command line</title>
<simpara>You can add a statically managed public SSH key when you create a virtual machine (VM) by using the command line. The key is added to the VM at first boot.</simpara>
<simpara>The key is added to the VM as a cloud-init data source. This method separates the access credentials from the application data in the cloud-init user data. This method does not affect cloud-init user data.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You generated an SSH key pair by running the <literal>ssh-keygen</literal> command.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a manifest file for a <literal>VirtualMachine</literal> object and a <literal>Secret</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  dataVolumeTemplates:
  - apiVersion: cdi.kubevirt.io/v1beta1
    kind: DataVolume
    metadata:
      name: example-vm-disk
    spec:
      sourceRef:
        kind: DataSource
        name: rhel9
        namespace: openshift-virtualization-os-images
      storage:
        resources:
          requests:
            storage: 30Gi
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/domain: example-vm
    spec:
      domain:
        cpu:
          cores: 1
          sockets: 2
          threads: 1
        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          interfaces:
          - masquerade: {}
            name: default
          rng: {}
        features:
          smm:
            enabled: true
        firmware:
          bootloader:
            efi: {}
        resources:
          requests:
            memory: 8Gi
      evictionStrategy: LiveMigrate
      networks:
      - name: default
        pod: {}
      volumes:
      - dataVolume:
          name: example-volume
        name: example-vm-disk
        - cloudInitNoCloud: <co xml:id="CO33-1"/>
            userData: |-
              #cloud-config
              user: cloud-user
              password: &lt;password&gt;
              chpasswd: { expire: False }
          name: cloudinitdisk
      accessCredentials:
        - sshPublicKey:
            propagationMethod:
              noCloud: {}
            source:
              secret:
                secretName: authorized-keys <co xml:id="CO33-2"/>
---
apiVersion: v1
kind: Secret
metadata:
  name: authorized-keys
data:
  key:  |
      MIIEpQIBAAKCAQEAulqb/Y... <co xml:id="CO33-3"/></programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>Specify the <literal>cloudInitNoCloud</literal> data source.</para>
</callout>
<callout arearefs="CO33-2">
<para>Specify the <literal>Secret</literal> object name.</para>
</callout>
<callout arearefs="CO33-3">
<para>Paste the public SSH key.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>VirtualMachine</literal> and <literal>Secret</literal> objects:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;manifest_file&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Start the VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl start vm example-vm</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Get the VM configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe vm example-vm -n example-namespace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  template:
    spec:
      accessCredentials:
        - sshPublicKey:
            propagationMethod:
              noCloud: {}
            source:
              secret:
                secretName: authorized-keys</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="adding-dynamic-key-vm">
<title>Dynamic key management</title>
<simpara>You can enable dynamic key injection for a virtual machine (VM) by using the OpenShift Container Platform web console or the command line. Then, you can update the key at runtime.</simpara>
<note>
<simpara>Only Red Hat Enterprise Linux (RHEL) 9 supports dynamic key injection.</simpara>
</note>
<simpara>If you disable dynamic key injection, the VM inherits the key management method of the image from which it was created.</simpara>
<section xml:id="virt-adding-key-creating-vm-template_dynamic-key">
<title>Enabling dynamic key injection when creating a VM from a template</title>
<simpara>You can enable dynamic public SSH key injection when you create a virtual machine (VM) from a template by using the OpenShift Container Platform web console. Then, you can update the key at runtime.</simpara>
<note>
<simpara>Only Red Hat Enterprise Linux (RHEL) 9 supports dynamic key injection.</simpara>
</note>
<simpara>The key is added to the VM by the QEMU guest agent, which is installed with RHEL 9.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You generated an SSH key pair by running the <literal>ssh-keygen</literal> command.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Red Hat Enterprise Linux 9 VM</emphasis> tile.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Customize VirtualMachine</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Next</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Scripts</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>If you have not already added a public SSH key to your project, click the edit icon beside <emphasis role="strong">Authorized SSH key</emphasis> and select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Use existing</emphasis>: Select a secret from the secrets list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Add new</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Browse to the SSH key file or paste the file in the key field.</simpara>
</listitem>
<listitem>
<simpara>Enter the secret name.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">Automatically apply this key to any new VirtualMachine you create in this project</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Set <emphasis role="strong">Dynamic SSH key injection</emphasis> to on.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
<simpara>The <emphasis role="strong">VirtualMachine details</emphasis> page displays the progress of the VM creation.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Click the <emphasis role="strong">Scripts</emphasis> tab on the <emphasis role="strong">Configuration</emphasis> tab.</simpara>
<simpara>The secret name is displayed in the <emphasis role="strong">Authorized SSH key</emphasis> section.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-vm-instancetype_dynamic-key">
<title>Enabling dynamic key injection when creating a VM from an instance type</title>
<simpara>You can create a virtual machine (VM) from an instance type by using the OpenShift Container Platform web console.
You can enable dynamic SSH key injection when you create a virtual machine (VM) from an instance type by using the OpenShift Container Platform web console. Then, you can add or revoke the key at runtime.</simpara>
<note>
<simpara>Only Red Hat Enterprise Linux (RHEL) 9 supports dynamic key injection.</simpara>
</note>
<simpara>The key is added to the VM by the QEMU guest agent, which is installed with RHEL 9.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Catalog</emphasis> and click the <emphasis role="strong">InstanceTypes</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Select a bootable volume.</simpara>
<note>
<simpara>The volume table only lists volumes in the <literal>openshift-virtualization-os-images</literal> namespace that have the <literal>instancetype.kubevirt.io/default-preference</literal> label.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Optional: Click the star icon to designate a bootable volume as a favorite. Starred bootable volumes appear first in the volume list.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click an instance type tile and select the resource size appropriate for your workload.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Red Hat Enterprise Linux 9 VM</emphasis> tile.</simpara>
</listitem>
<listitem>
<simpara>If you have not already added a public SSH key to your project, click the edit icon beside <emphasis role="strong">Authorized SSH key</emphasis> in the <emphasis role="strong">VirtualMachine details</emphasis> section.</simpara>
</listitem>
<listitem>
<simpara>Select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Use existing</emphasis>: Select a secret from the secrets list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Add new</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Browse to the public SSH key file or paste the file in the key field.</simpara>
</listitem>
<listitem>
<simpara>Enter the secret name.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">Automatically apply this key to any new VirtualMachine you create in this project</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Set <emphasis role="strong">Dynamic SSH key injection</emphasis> in the <emphasis role="strong">VirtualMachine details</emphasis> section to on.</simpara>
</listitem>
<listitem>
<simpara>Optional: Click <emphasis role="strong">View YAML &amp; CLI</emphasis> to view the YAML file. Click <emphasis role="strong">CLI</emphasis> to view the CLI commands. You can also download or copy either the YAML file contents or the CLI commands.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create VirtualMachine</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>After the VM is created, you can monitor the status on the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</section>
<section xml:id="virt-editing-vm-dynamic-key-injection_dynamic-key">
<title>Enabling dynamic SSH key injection by using the web console</title>
<simpara>You can enable dynamic key injection for a virtual machine (VM) by using the OpenShift Container Platform web console. Then, you can update the public SSH key at runtime.</simpara>
<simpara>The key is added to the VM by the QEMU guest agent, which is installed with Red Hat Enterprise Linux (RHEL) 9.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The guest operating system is RHEL 9.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Configure</emphasis> tab, click <emphasis role="strong">Scripts</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you have not already added a public SSH key to your project, click the edit icon beside <emphasis role="strong">Authorized SSH key</emphasis> and select one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Use existing</emphasis>: Select a secret from the secrets list.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Add new</emphasis>:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Browse to the SSH key file or paste the file in the key field.</simpara>
</listitem>
<listitem>
<simpara>Enter the secret name.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">Automatically apply this key to any new VirtualMachine you create in this project</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Set <emphasis role="strong">Dynamic SSH key injection</emphasis> to on.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-adding-public-key-cli_dynamic-key">
<title>Enabling dynamic key injection by using the command line</title>
<simpara>You can enable dynamic key injection for a virtual machine (VM) by using the command line. Then, you can update the public SSH key at runtime.</simpara>
<note>
<simpara>Only Red Hat Enterprise Linux (RHEL) 9 supports dynamic key injection.</simpara>
</note>
<simpara>The key is added to the VM by the QEMU guest agent, which is installed automatically with RHEL 9.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You generated an SSH key pair by running the <literal>ssh-keygen</literal> command.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a manifest file for a <literal>VirtualMachine</literal> object and a <literal>Secret</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  dataVolumeTemplates:
  - apiVersion: cdi.kubevirt.io/v1beta1
    kind: DataVolume
    metadata:
      name: example-vm-disk
    spec:
      sourceRef:
        kind: DataSource
        name: rhel9
        namespace: openshift-virtualization-os-images
      storage:
        resources:
          requests:
            storage: 30Gi
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/domain: example-vm
    spec:
      domain:
        cpu:
          cores: 1
          sockets: 2
          threads: 1
        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
          interfaces:
          - masquerade: {}
            name: default
          rng: {}
        features:
          smm:
            enabled: true
        firmware:
          bootloader:
            efi: {}
        resources:
          requests:
            memory: 8Gi
      evictionStrategy: LiveMigrate
      networks:
      - name: default
        pod: {}
      volumes:
      - dataVolume:
          name: example-volume
        name: example-vm-disk
        - cloudInitNoCloud: <co xml:id="CO34-1"/>
            userData: |-
              #cloud-config
              user: cloud-user
              password: &lt;password&gt;
              chpasswd: { expire: False }
              runcmd:
                - [ setsebool, -P, virt_qemu_ga_manage_ssh, on ]
          name: cloudinitdisk
      accessCredentials:
        - sshPublicKey:
            propagationMethod:
              qemuGuestAgent:
                users: ["user1","user2","fedora"] <co xml:id="CO34-2"/>
            source:
              secret:
                secretName: authorized-keys <co xml:id="CO34-3"/>
---
apiVersion: v1
kind: Secret
metadata:
  name: authorized-keys
data:
  key:  |
      MIIEpQIBAAKCAQEAulqb/Y... <co xml:id="CO34-4"/></programlisting>
<calloutlist>
<callout arearefs="CO34-1">
<para>Specify the <literal>cloudInitNoCloud</literal> data source.</para>
</callout>
<callout arearefs="CO34-2">
<para>Specify the user names.</para>
</callout>
<callout arearefs="CO34-3">
<para>Specify the <literal>Secret</literal> object name.</para>
</callout>
<callout arearefs="CO34-4">
<para>Paste the public SSH key.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>VirtualMachine</literal> and <literal>Secret</literal> objects:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;manifest_file&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Start the VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl start vm example-vm</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Get the VM configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe vm example-vm -n example-namespace</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  template:
    spec:
      accessCredentials:
        - sshPublicKey:
            propagationMethod:
              qemuGuestAgent:
                users: ["user1","user2","fedora"]
            source:
              secret:
                secretName: authorized-keys</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-using-virtctl-ssh-command_virt-accessing-vm-ssh">
<title>Using the virtctl ssh command</title>
<simpara>You can access a running virtual machine (VM) by using the <literal>virtcl ssh</literal> command.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>virtctl</literal> command line tool.</simpara>
</listitem>
<listitem>
<simpara>You added a public SSH key to the VM.</simpara>
</listitem>
<listitem>
<simpara>You have an SSH client installed.</simpara>
</listitem>
<listitem>
<simpara>The environment where you installed the <literal>virtctl</literal> tool has the cluster permissions required to access the VM. For example, you ran <literal>oc login</literal> or you set the <literal>KUBECONFIG</literal> environment variable.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the <literal>virtctl ssh</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl -n &lt;namespace&gt; ssh &lt;username&gt;@example-vm -i &lt;ssh_key&gt; <co xml:id="CO35-1"/></programlisting>
<calloutlist>
<callout arearefs="CO35-1">
<para>Specify the namespace, user name, and the SSH private key. The default SSH key location is <literal>/home/user/.ssh</literal>. If you save the key in a different location, you must specify the path.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl -n my-namespace ssh cloud-user@example-vm -i my-key</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<tip>
<simpara>You can copy the <literal>virtctl ssh</literal> command in the web console by selecting <emphasis role="strong">Copy SSH command</emphasis> from the options <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> menu beside a VM on the <link linkend="virtualmachines-page_virt-web-console-overview"><emphasis role="strong">VirtualMachines</emphasis> page</link>.</simpara>
</tip>
</section>
</section>
<section xml:id="virt-using-virtctl-port-forward-command_virt-accessing-vm-ssh">
<title>Using the virtctl port-forward command</title>
<simpara>You can access a running virtual machine (VM) by using a local OpenSSH client and the <literal>virtctl port-forward</literal> command. You can use this method with Ansible to automate the configuration of VMs.</simpara>
<simpara>This method is recommended for low-traffic applications because port-forwarding traffic is sent over the control plane. This method is not recommended for high-traffic applications such as Rsync or Remote Desktop Protocol because it places a heavy burden on the API server.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have OpenSSH installed.</simpara>
</listitem>
<listitem>
<simpara>You installed the <literal>virtctl</literal> command line tool.</simpara>
</listitem>
<listitem>
<simpara>The environment where you installed <literal>virtctl</literal> has the cluster permissions required to access the VM. For example, you ran <literal>oc login</literal> or you set the <literal>KUBECONFIG</literal> environment variable.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the following text to the <literal>~/.ssh/config</literal> file on your client machine:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">Host vm/*
  ProxyCommand virtctl port-forward --stdio=true %h %p</programlisting>
</listitem>
<listitem>
<simpara>Connect to the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh &lt;user&gt;@vm/&lt;vm_name&gt;.&lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="using-services-ssh_virt-accessing-vm-ssh">
<title>Using a service for SSH access</title>
<simpara>You can create a service for a virtual machine (VM) and connect to the IP address and port exposed by the service.</simpara>
<simpara>Services provide excellent performance and are recommended for applications that are accessed from outside the cluster or within the cluster. Ingress traffic is protected by firewalls.</simpara>
<simpara>If the cluster network cannot handle the traffic load, consider using a secondary network for VM access.</simpara>
<section xml:id="virt-about-services_virt-accessing-vm-ssh">
<title>About services</title>
<simpara>A Kubernetes service exposes network access for clients to an application running on a set of pods. Services offer abstraction, load balancing, and, in the case of the <literal>NodePort</literal> and <literal>LoadBalancer</literal> types, exposure to the outside world.</simpara>
<variablelist>
<varlistentry>
<term>ClusterIP</term>
<listitem>
<simpara>Exposes the service on an internal IP address and as a DNS name to other applications within the cluster. A single service can map to multiple virtual machines. When a client tries to connect to the service, the client&#8217;s request is load balanced among available backends. <literal>ClusterIP</literal> is the default service type.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>NodePort</term>
<listitem>
<simpara>Exposes the service on the same port of each selected node in the cluster. <literal>NodePort</literal> makes a port accessible from outside the cluster, as long as the node itself is externally accessible to the client.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>LoadBalancer</term>
<listitem>
<simpara>Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP address to the service.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>For on-premise clusters, you can configure a load balancing service by using the MetalLB Operator in layer 2 mode. The BGP mode is not supported. The MetalLB Operator is installed in the <literal>metallb-system</literal> namespace.</simpara>
</note>
</section>
<section xml:id="creating-services-ssh_virt-accessing-vm-ssh">
<title>Creating a service</title>
<simpara>You can create a service to expose a virtual machine (VM) by using the OpenShift Container Platform web console, <literal>virtctl</literal> command line tool, or a YAML file.</simpara>
<section xml:id="virt-enabling-load-balancer-service-web_virt-accessing-vm-ssh">
<title>Enabling load balancer service creation by using the web console</title>
<simpara>You can enable the creation of load balancer services for a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have configured a load balancer for the cluster.</simpara>
</listitem>
<listitem>
<simpara>You are logged in as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Overview</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Settings</emphasis> tab, click <emphasis role="strong">Cluster</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Expand <emphasis role="strong">General settings</emphasis> and <emphasis role="strong">SSH configuration</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Set <emphasis role="strong">SSH over LoadBalancer service</emphasis> to on.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-service-web_virt-accessing-vm-ssh">
<title>Creating a service by using the web console</title>
<simpara>You can create a node port or load balancer service for a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured the cluster network to support either a load balancer or a node port.</simpara>
</listitem>
<listitem>
<simpara>To create a load balancer service, you enabled the creation of load balancer services.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">VirtualMachines</emphasis> and select a virtual machine to view the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Details</emphasis> tab, select <emphasis role="strong">SSH over LoadBalancer</emphasis> from the <emphasis role="strong">SSH service type</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Optional: Click the copy icon to copy the <literal>SSH</literal> command to your clipboard.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Check the <emphasis role="strong">Services</emphasis> pane on the <emphasis role="strong">Details</emphasis> tab to view the new service.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-creating-service-virtctl_virt-accessing-vm-ssh">
<title>Creating a service by using virtctl</title>
<simpara>You can create a service for a virtual machine (VM) by using the <literal>virtctl</literal> command line tool.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>virtctl</literal> command line tool.</simpara>
</listitem>
<listitem>
<simpara>You configured the cluster network to support the service.</simpara>
</listitem>
<listitem>
<simpara>The environment where you installed <literal>virtctl</literal> has the cluster permissions required to access the VM. For example, you ran <literal>oc login</literal> or you set the <literal>KUBECONFIG</literal> environment variable.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl expose vm &lt;vm_name&gt; --name &lt;service_name&gt; --type &lt;service_type&gt; --port &lt;port&gt; <co xml:id="CO36-1"/></programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>Specify the <literal>ClusterIP</literal>, <literal>NodePort</literal>, or <literal>LoadBalancer</literal> service type.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl expose vm example-vm --name example-service --type NodePort --port 22</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify the service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get service</programlisting>
</listitem>
</itemizedlist>
<formalpara>
<title>Next steps</title>
<para>After you create a service with <literal>virtctl</literal>, you must add <literal>special: key</literal> to the <literal>spec.template.metadata.labels</literal> stanza of the <literal>VirtualMachine</literal> manifest. See <link linkend="virt-creating-service-cli_virt-accessing-vm-ssh">Creating a service by using the command line</link>.</para>
</formalpara>
</section>
<section xml:id="virt-creating-service-cli_virt-accessing-vm-ssh">
<title>Creating a service by using the command line</title>
<simpara>You can create a service and associate it with a virtual machine (VM) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured the cluster network to support the service.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest to add the label for service creation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  running: false
  template:
    metadata:
      labels:
        special: key <co xml:id="CO37-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO37-1">
<para>Add <literal>special: key</literal> to the <literal>spec.template.metadata.labels</literal> stanza.</para>
</callout>
</calloutlist>
<note>
<simpara>Labels on a virtual machine are passed through to the pod. The <literal>special: key</literal> label must match the label in the <literal>spec.selector</literal> attribute of the <literal>Service</literal> manifest.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save the <literal>VirtualMachine</literal> manifest file to apply your changes.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Service</literal> manifest to expose the VM:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: example-service
  namespace: example-namespace
spec:
# ...
  selector:
    special: key <co xml:id="CO38-1"/>
  type: NodePort <co xml:id="CO38-2"/></programlisting>
<calloutlist>
<callout arearefs="CO38-1">
<para>Specify the label that you added to the <literal>spec.template.metadata.labels</literal> stanza of the <literal>VirtualMachine</literal> manifest.</para>
</callout>
<callout arearefs="CO38-2">
<para>Specify <literal>ClusterIP</literal>, <literal>NodePort</literal>, or <literal>LoadBalancer</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the <literal>Service</literal> manifest file.</simpara>
</listitem>
<listitem>
<simpara>Create the service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f example-service.yaml</programlisting>
</listitem>
<listitem>
<simpara>Restart the VM to apply the changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Query the <literal>Service</literal> object to verify that it is available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get service -n example-namespace</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-connecting-service-ssh_virt-accessing-vm-ssh">
<title>Connecting to a VM exposed by a service by using SSH</title>
<simpara>You can connect to a virtual machine (VM) that is exposed by a service by using SSH.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created a service to expose the VM.</simpara>
</listitem>
<listitem>
<simpara>You have an SSH client installed.</simpara>
</listitem>
<listitem>
<simpara>You are logged in to the cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to access the VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh &lt;user_name&gt;@&lt;ip_address&gt; -p &lt;port&gt; <co xml:id="CO39-1"/></programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>Specify the cluster IP for a cluster IP service, the node IP for a node port service, or the external IP address for a load balancer service.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="using-secondary-networks-ssh_virt-accessing-vm-ssh">
<title>Using a secondary network for SSH access</title>
<simpara>You can configure a secondary network, attach a virtual machine (VM) to the secondary network interface, and connect to the DHCP-allocated IP address by using SSH.</simpara>
<important>
<simpara>Secondary networks provide excellent performance because the traffic is not handled by the cluster network stack. However, the VMs are exposed directly to the secondary network and are not protected by firewalls. If a VM is compromised, an intruder could gain access to the secondary network. You must configure appropriate security within the operating system of the VM if you use this method.</simpara>
</important>
<simpara>See the <link xlink:href="https://access.redhat.com/articles/6994974#networking-multus">Multus</link> and <link xlink:href="https://access.redhat.com/articles/6994974#networking-sriov">SR-IOV</link> documentation in the <link xlink:href="https://access.redhat.com/articles/6994974">OpenShift Virtualization Tuning &amp; Scaling Guide</link> for additional information about networking options.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured a secondary network such as <link linkend="virt-connecting-vm-to-linux-bridge">Linux bridge</link> or <link linkend="virt-connecting-vm-to-sriov">SR-IOV</link>.</simpara>
</listitem>
<listitem>
<simpara>You created a network attachment definition for a <link linkend="virt-creating-linux-bridge-nad-web_virt-connecting-vm-to-linux-bridge">Linux bridge network</link> or the SR-IOV Network Operator created a <link linkend="nw-sriov-network-attachment_virt-connecting-vm-to-sriov">network attachment definition</link> when you created an <literal>SriovNetwork</literal> object.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-vm-creating-nic-web_virt-accessing-vm-ssh">
<title>Configuring a VM network interface by using the web console</title>
<simpara>You can configure a network interface for a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created a network attachment definition for the network.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click a VM to view the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Configuration</emphasis> tab, click the <emphasis role="strong">Network interfaces</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add network interface</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the interface name and select the network attachment definition from the <emphasis role="strong">Network</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Restart the VM to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-connecting-secondary-network-ssh_virt-accessing-vm-ssh">
<title>Connecting to a VM attached to a secondary network by using SSH</title>
<simpara>You can connect to a virtual machine (VM) attached to a secondary network by using SSH.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You attached a VM to a secondary network with a DHCP server.</simpara>
</listitem>
<listitem>
<simpara>You have an SSH client installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the IP address of the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe vm &lt;vm_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen># ...
Interfaces:
  Interface Name:  eth0
  Ip Address:      10.244.0.37/24
  Ip Addresses:
    10.244.0.37/24
    fe80::858:aff:fef4:25/64
  Mac:             0a:58:0a:f4:00:25
  Name:            default
# ...</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Connect to the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh &lt;user_name&gt;@&lt;ip_address&gt; -i &lt;ssh_key&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh cloud-user@10.244.0.37 -i ~/.ssh/id_rsa_cloud-user</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>You can also <link linkend="virt-accessing-vm-secondary-network-fqdn">access a VM attached to a secondary network interface by using the cluster FQDN</link>.</simpara>
</note>
</section>
</section>
</section>
<section xml:id="virt-edit-vms">
<title>Editing virtual machines</title>

<simpara>You can update a virtual machine (VM) configuration by using the OpenShift Container Platform web console. You can update the <link linkend="virtualmachine-details-yaml_virt-web-console-overview">YAML file</link> or the <link linkend="virtualmachine-details-page_virt-web-console-overview"><emphasis role="strong">VirtualMachine details</emphasis> page</link>.</simpara>
<simpara>You can also edit a VM by using the command line.</simpara>
<section xml:id="virt-editing-vm-cli_virt-edit-vms">
<title>Editing a virtual machine by using the command line</title>
<simpara>You can edit a virtual machine (VM) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the <literal>oc</literal> CLI.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the virtual machine configuration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit vm &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Edit the YAML configuration.</simpara>
</listitem>
<listitem>
<simpara>If you edit a running virtual machine, you need to do one of the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Restart the virtual machine.</simpara>
</listitem>
<listitem>
<simpara>Run the following command for the new configuration to take effect:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply vm &lt;vm_name&gt;</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-add-disk-to-vm_virt-edit-vms">
<title>Adding a disk to a virtual machine</title>
<simpara>You can add a virtual disk to a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Disks</emphasis> tab, click <emphasis role="strong">Add disk</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Specify the <emphasis role="strong">Source</emphasis>, <emphasis role="strong">Name</emphasis>, <emphasis role="strong">Size</emphasis>, <emphasis role="strong">Type</emphasis>, <emphasis role="strong">Interface</emphasis>, and <emphasis role="strong">Storage Class</emphasis>.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Optional: You can enable preallocation if you use a blank disk source and require maximum write performance when creating data volumes. To do so, select the <emphasis role="strong">Enable preallocation</emphasis> checkbox.</simpara>
</listitem>
<listitem>
<simpara>Optional: You can clear <emphasis role="strong">Apply optimized StorageProfile settings</emphasis> to change the <emphasis role="strong">Volume Mode</emphasis> and <emphasis role="strong">Access Mode</emphasis> for the virtual disk. If you do not specify these parameters, the system uses the default values from the <literal>kubevirt-storage-class-defaults</literal> config map.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add</emphasis>.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>If the VM is running, you must restart the VM to apply the change.</simpara>
</note>
<section xml:id="virt-storage-wizard-fields-web_virt-edit-vms">
<title>Storage fields</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="75*"/>
<thead>
<row>
<entry align="left" valign="top">Field</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Blank (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Create an empty disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Import via URL (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Import content via URL (HTTP or HTTPS endpoint).</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Use an existing PVC</simpara></entry>
<entry align="left" valign="top"><simpara>Use a PVC that is already available in the cluster.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Clone existing PVC (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Select an existing PVC available in the cluster and clone it.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Import via Registry (creates PVC)</simpara></entry>
<entry align="left" valign="top"><simpara>Import content via container registry.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Container (ephemeral)</simpara></entry>
<entry align="left" valign="top"><simpara>Upload content from a container located in a registry accessible from the cluster. The container disk should be used only for read-only filesystems such as CD-ROMs or temporary virtual machines.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Name</simpara></entry>
<entry align="left" valign="top"><simpara>Name of the disk. The name can contain lowercase letters (<literal>a-z</literal>), numbers (<literal>0-9</literal>), hyphens (<literal>-</literal>), and periods (<literal>.</literal>), up to a maximum of 253 characters. The first and last characters must be alphanumeric. The name must not contain uppercase letters, spaces, or special characters.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Size</simpara></entry>
<entry align="left" valign="top"><simpara>Size of the disk in GiB.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Type</simpara></entry>
<entry align="left" valign="top"><simpara>Type of disk. Example: Disk or CD-ROM</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Interface</simpara></entry>
<entry align="left" valign="top"><simpara>Type of disk device. Supported interfaces are <emphasis role="strong">virtIO</emphasis>, <emphasis role="strong">SATA</emphasis>, and <emphasis role="strong">SCSI</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Storage Class</simpara></entry>
<entry align="left" valign="top"><simpara>The storage class that is used to create the disk.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<bridgehead xml:id="virt-storage-wizard-fields-advanced-web_virt-edit-vms" renderas="sect5">Advanced storage settings</bridgehead>
<simpara>The following advanced storage settings are optional and available for <emphasis role="strong">Blank</emphasis>, <emphasis role="strong">Import via URL</emphasis>, and <emphasis role="strong">Clone existing PVC</emphasis> disks.</simpara>
<simpara>If you do not specify these parameters, the system uses the default storage profile values.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="20*"/>
<colspec colname="col_3" colwidth="60*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Parameter description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Volume Mode</simpara></entry>
<entry align="left" valign="top"><simpara>Filesystem</simpara></entry>
<entry align="left" valign="top"><simpara>Stores the virtual disk on a file system-based volume.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Block</simpara></entry>
<entry align="left" valign="top"><simpara>Stores the virtual disk directly on the block volume. Only use <literal>Block</literal> if the underlying storage supports it.</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara>Access Mode</simpara></entry>
<entry align="left" valign="top"><simpara>ReadWriteOnce (RWO)</simpara></entry>
<entry align="left" valign="top"><simpara>Volume can be mounted as read-write by a single node.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>ReadWriteMany (RWX)</simpara></entry>
<entry align="left" valign="top"><simpara>Volume can be mounted as read-write by many nodes at one time.</simpara>
<note>
<simpara>This mode is required for live migration.</simpara>
</note></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="virt-adding-secret-configmap-service-account-to-vm_virt-edit-vms">
<title>Adding a secret, config map, or service account to a virtual machine</title>
<simpara>You add a secret, config map, or service account to a virtual machine by using the OpenShift Container Platform web console.</simpara>
<simpara>These resources are added to the virtual machine as disks. You then mount the secret, config map, or service account as you would mount any other disk.</simpara>
<simpara>If the virtual machine is running, changes do not take effect until you restart the virtual machine. The newly added resources are marked as pending changes at the top of the page.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The secret, config map, or service account that you want to add must exist in the same namespace as the target virtual machine.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a virtual machine to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Environment</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add Config Map, Secret or Service Account</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Select a resource</emphasis> and select a resource from the list. A six character serial number is automatically generated for the selected resource.</simpara>
</listitem>
<listitem>
<simpara>Optional: Click <emphasis role="strong">Reload</emphasis> to revert the environment to its last saved state.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>On the <emphasis role="strong">VirtualMachine details</emphasis> page, click <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Disks</emphasis> and verify that the resource is displayed in the list of disks.</simpara>
</listitem>
<listitem>
<simpara>Restart the virtual machine by clicking <emphasis role="strong">Actions</emphasis> &#8594; <emphasis role="strong">Restart</emphasis>.</simpara>
</listitem>
</orderedlist>
<simpara>You can now mount the secret, config map, or service account as you would mount any other disk.</simpara>
<bridgehead xml:id="additional-resources-configmaps" role="_additional-resources" renderas="sect4">Additional resources for config maps, secrets, and service accounts</bridgehead>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-pods-configmap-overview_builds-configmaps">Understanding config maps</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-pods-secrets-about">Providing sensitive data to pods</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/authentication_and_authorization/#service-accounts-overview">Understanding and creating service accounts</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-edit-boot-order">
<title>Editing boot order</title>

<simpara>You can update the values for a boot order list by using the web console or the CLI.</simpara>
<simpara>With <emphasis role="strong">Boot Order</emphasis> in the <emphasis role="strong">Virtual Machine Overview</emphasis> page, you can:</simpara>
<itemizedlist>
<listitem>
<simpara>Select a disk or network interface controller (NIC) and add it to the boot order list.</simpara>
</listitem>
<listitem>
<simpara>Edit the order of the disks or NICs in the boot order list.</simpara>
</listitem>
<listitem>
<simpara>Remove a disk or NIC from the boot order list, and return it back to the inventory of bootable sources.</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-add-boot-order-web_virt-edit-boot-order">
<title>Adding items to a boot order list in the web console</title>
<simpara>Add items to a boot order list by using the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a virtual machine to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Details</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the pencil icon that is located on the right side of <emphasis role="strong">Boot Order</emphasis>. If a YAML configuration does not exist, or if this is the first time that you are creating a boot order list, the following message displays: <emphasis role="strong">No resource selected. VM will attempt to boot from disks by order of appearance in YAML file.</emphasis></simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add Source</emphasis> and select a bootable disk or network interface controller (NIC) for the virtual machine.</simpara>
</listitem>
<listitem>
<simpara>Add any additional disks or NICs to the boot order list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>If the virtual machine is running, changes to <emphasis role="strong">Boot Order</emphasis> will not take effect until you restart the virtual machine.</simpara>
<simpara>You can view pending changes by clicking <emphasis role="strong">View Pending Changes</emphasis> on the right side of the <emphasis role="strong">Boot Order</emphasis> field. The <emphasis role="strong">Pending Changes</emphasis> banner at the
top of the page displays a list of all changes that will be applied when the virtual machine restarts.</simpara>
</note>
</section>
<section xml:id="virt-edit-boot-order-web_virt-edit-boot-order">
<title>Editing a boot order list in the web console</title>
<simpara>Edit the boot order list in the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a virtual machine to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Details</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the pencil icon that is located on the right side of <emphasis role="strong">Boot Order</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Choose the appropriate method to move the item in the boot order list:</simpara>
<itemizedlist>
<listitem>
<simpara>If you do not use a screen reader, hover over the arrow icon next to the item that you want to move, drag the item up or down, and drop it in a location of your choice.</simpara>
</listitem>
<listitem>
<simpara>If you use a screen reader, press the Up Arrow key or Down Arrow key to move the item in the boot order list. Then, press the <emphasis role="strong">Tab</emphasis> key to drop the item in a location of your choice.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>If the virtual machine is running, changes to the boot order list will not take effect until you restart the virtual machine.</simpara>
<simpara>You can view pending changes by clicking <emphasis role="strong">View Pending Changes</emphasis> on the right side of the <emphasis role="strong">Boot Order</emphasis> field. The <emphasis role="strong">Pending Changes</emphasis> banner
at the top of the page displays a list of all changes that will be applied when the virtual machine restarts.</simpara>
</note>
</section>
<section xml:id="virt-edit-boot-order-yaml-web_virt-edit-boot-order">
<title>Editing a boot order list in the YAML configuration file</title>
<simpara>Edit the boot order list in a YAML configuration file by using the CLI.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the YAML configuration file for the virtual machine by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit vm example</programlisting>
</listitem>
<listitem>
<simpara>Edit the YAML file and modify the values for the boot order associated with a disk or network interface controller (NIC). For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">disks:
  - bootOrder: 1 <co xml:id="CO40-1"/>
    disk:
      bus: virtio
    name: containerdisk
  - disk:
      bus: virtio
    name: cloudinitdisk
  - cdrom:
      bus: virtio
    name: cd-drive-1
interfaces:
  - boot Order: 2 <co xml:id="CO40-2"/>
    macAddress: '02:96:c4:00:00'
    masquerade: {}
    name: default</programlisting>
<calloutlist>
<callout arearefs="CO40-1">
<para>The boot order value specified for the disk.</para>
</callout>
<callout arearefs="CO40-2">
<para>The boot order value specified for the network interface controller.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the YAML file.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">reload the content</emphasis> to apply the updated boot order values from the YAML file to the boot order list in the web console.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-remove-boot-order-item-web_virt-edit-boot-order">
<title>Removing items from a boot order list in the web console</title>
<simpara>Remove items from a boot order list by using the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a virtual machine to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Details</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click the pencil icon that is located on the right side of <emphasis role="strong">Boot Order</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Remove</emphasis> icon <inlinemediaobject>
<imageobject>
<imagedata fileref="images/delete.png"/>
</imageobject>
<textobject><phrase>delete</phrase></textobject>
</inlinemediaobject> next to the item. The item is removed from the boot order list and saved in the list of available boot sources. If you remove all items from the boot order list, the following message displays: <emphasis role="strong">No resource selected. VM will attempt to boot from disks by order of appearance in YAML file.</emphasis></simpara>
</listitem>
</orderedlist>
<note>
<simpara>If the virtual machine is running, changes to <emphasis role="strong">Boot Order</emphasis> will not take effect until you restart the virtual machine.</simpara>
<simpara>You can view pending changes by clicking <emphasis role="strong">View Pending Changes</emphasis> on the right side of the <emphasis role="strong">Boot Order</emphasis> field. The <emphasis role="strong">Pending Changes</emphasis> banner at the top of the page displays a list of all changes that will be applied when the virtual machine restarts.</simpara>
</note>
</section>
</section>
<section xml:id="virt-delete-vms">
<title>Deleting virtual machines</title>

<simpara>You can delete a virtual machine from the web console or by using the <literal>oc</literal> command line interface.</simpara>
<section xml:id="virt-delete-vm-web_virt-delete-vms">
<title>Deleting a virtual machine using the web console</title>
<simpara>Deleting a virtual machine permanently removes it from the cluster.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a virtual machine and select <emphasis role="strong">Delete</emphasis>.</simpara>
<simpara>Alternatively, click the virtual machine name to open the <emphasis role="strong">VirtualMachine details</emphasis> page and click <emphasis role="strong">Actions</emphasis> &#8594; <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">With grace period</emphasis> or clear <emphasis role="strong">Delete disks</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis> to permanently delete the virtual machine.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-deleting-vms_virt-delete-vms">
<title>Deleting a virtual machine by using the CLI</title>
<simpara>You can delete a virtual machine by using the <literal>oc</literal> command line interface (CLI). The <literal>oc</literal> client enables you to perform actions on multiple virtual machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Identify the name of the virtual machine that you want to delete.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Delete the virtual machine by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vm &lt;vm_name&gt;</programlisting>
<note>
<simpara>This command only deletes a VM in the current project. Specify the
<literal>-n &lt;project_name&gt;</literal> option if the VM you want to delete is in
a different project or namespace.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-exporting-vms">
<title>Exporting virtual machines</title>

<simpara>You can export a virtual machine (VM) and its associated disks in order to import a VM into another cluster or to analyze the volume for forensic purposes.</simpara>
<simpara>You create a <literal>VirtualMachineExport</literal> custom resource (CR) by using the command line interface.</simpara>
<simpara>Alternatively, you can use the <link linkend="vm-volume-export-commands_virt-using-the-cli-tools"><literal>virtctl vmexport</literal> command</link> to create a <literal>VirtualMachineExport</literal> CR and to download exported volumes.</simpara>
<note>
<simpara>You can migrate virtual machines between OpenShift Virtualization clusters by using the <link xlink:href="https://access.redhat.com/products/migration-toolkits-virtualization">Migration Toolkit for Virtualization</link>.</simpara>
</note>
<section xml:id="virt-creating-virtualmachineexport_virt-exporting-vms">
<title>Creating a VirtualMachineExport custom resource</title>
<simpara>You can create a <literal>VirtualMachineExport</literal> custom resource (CR) to export the following objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Virtual machine (VM): Exports the persistent volume claims (PVCs) of a specified VM.</simpara>
</listitem>
<listitem>
<simpara>VM snapshot: Exports PVCs contained in a <literal>VirtualMachineSnapshot</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>PVC: Exports a PVC. If the PVC is used by another pod, such as the <literal>virt-launcher</literal> pod, the export remains in a <literal>Pending</literal> state until the PVC is no longer in use.</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>VirtualMachineExport</literal> CR creates internal and external links for the exported volumes. Internal links are valid within the cluster. External links can be accessed by using an <literal>Ingress</literal> or <literal>Route</literal>.</simpara>
<simpara>The export server supports the following file formats:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>raw</literal>: Raw disk image file.</simpara>
</listitem>
<listitem>
<simpara><literal>gzip</literal>: Compressed disk image file.</simpara>
</listitem>
<listitem>
<simpara><literal>dir</literal>: PVC directory and files.</simpara>
</listitem>
<listitem>
<simpara><literal>tar.gz</literal>: Compressed PVC file.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The VM must be shut down for a VM export.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>VirtualMachineExport</literal> manifest to export a volume from a <literal>VirtualMachine</literal>, <literal>VirtualMachineSnapshot</literal>, or <literal>PersistentVolumeClaim</literal> CR according to the following example and save it as <literal>example-export.yaml</literal>:</simpara>
<formalpara>
<title><literal>VirtualMachineExport</literal> example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: export.kubevirt.io/v1alpha1
kind: VirtualMachineExport
metadata:
  name: example-export
spec:
  source:
    apiGroup: "kubevirt.io" <co xml:id="CO41-1"/>
    kind: VirtualMachine <co xml:id="CO41-2"/>
    name: example-vm
  ttlDuration: 1h <co xml:id="CO41-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO41-1">
<para>Specify the appropriate API group:</para>
<itemizedlist>
<listitem>
<simpara><literal>"kubevirt.io"</literal> for <literal>VirtualMachine</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>"snapshot.kubevirt.io"</literal> for <literal>VirtualMachineSnapshot</literal>.</simpara>
</listitem>
<listitem>
<simpara><literal>""</literal> for <literal>PersistentVolumeClaim</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
<callout arearefs="CO41-2">
<para>Specify <literal>VirtualMachine</literal>, <literal>VirtualMachineSnapshot</literal>, or <literal>PersistentVolumeClaim</literal>.</para>
</callout>
<callout arearefs="CO41-3">
<para>Optional. The default duration is 2 hours.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>VirtualMachineExport</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f example-export.yaml</programlisting>
</listitem>
<listitem>
<simpara>Get the <literal>VirtualMachineExport</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmexport example-export -o yaml</programlisting>
<simpara>The internal and external links for the exported volumes are displayed in the <literal>status</literal> stanza:</simpara>
<formalpara>
<title>Output example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: export.kubevirt.io/v1alpha1
kind: VirtualMachineExport
metadata:
  name: example-export
  namespace: example
spec:
  source:
    apiGroup: ""
    kind: PersistentVolumeClaim
    name: example-pvc
  tokenSecretRef: example-token
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2022-06-21T14:10:09Z"
    reason: podReady
    status: "True"
    type: Ready
  - lastProbeTime: null
    lastTransitionTime: "2022-06-21T14:09:02Z"
    reason: pvcBound
    status: "True"
    type: PVCReady
  links:
    external: <co xml:id="CO42-1"/>
      cert: |-
        -----BEGIN CERTIFICATE-----
        ...
        -----END CERTIFICATE-----
      volumes:
      - formats:
        - format: raw
          url: https://vmexport-proxy.test.net/api/export.kubevirt.io/v1alpha1/namespaces/example/virtualmachineexports/example-export/volumes/example-disk/disk.img
        - format: gzip
          url: https://vmexport-proxy.test.net/api/export.kubevirt.io/v1alpha1/namespaces/example/virtualmachineexports/example-export/volumes/example-disk/disk.img.gz
        name: example-disk
    internal:  <co xml:id="CO42-2"/>
      cert: |-
        -----BEGIN CERTIFICATE-----
        ...
        -----END CERTIFICATE-----
      volumes:
      - formats:
        - format: raw
          url: https://virt-export-example-export.example.svc/volumes/example-disk/disk.img
        - format: gzip
          url: https://virt-export-example-export.example.svc/volumes/example-disk/disk.img.gz
        name: example-disk
  phase: Ready
  serviceName: virt-export-example-export</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO42-1">
<para>External links are accessible from outside the cluster by using an <literal>Ingress</literal> or <literal>Route</literal>.</para>
</callout>
<callout arearefs="CO42-2">
<para>Internal links are only valid inside the cluster.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-accessing-exported-vm-manifests_virt-exporting-vms">
<title>Accessing exported virtual machine manifests</title>
<simpara>After you export a virtual machine (VM) or snapshot, you can get the <literal>VirtualMachine</literal> manifest and related information from the export server.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You exported a virtual machine or VM snapshot by creating a <literal>VirtualMachineExport</literal> custom resource (CR).</simpara>
<note>
<simpara><literal>VirtualMachineExport</literal> objects that have the <literal>spec.source.kind: PersistentVolumeClaim</literal> parameter do not generate virtual machine manifests.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To access the manifests, you must first copy the certificates from the source cluster to the target cluster.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Log in to the source cluster.</simpara>
</listitem>
<listitem>
<simpara>Save the certificates to the <literal>cacert.crt</literal> file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmexport &lt;export_name&gt; -o jsonpath={.status.links.external.cert} &gt; cacert.crt <co xml:id="CO43-1"/></programlisting>
<calloutlist>
<callout arearefs="CO43-1">
<para>Replace <literal>&lt;export_name&gt;</literal> with the <literal>metadata.name</literal> value from the <literal>VirtualMachineExport</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Copy the <literal>cacert.crt</literal> file to the target cluster.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Decode the token in the source cluster and save it to the <literal>token_decode</literal> file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secret export-token-&lt;export_name&gt; -o jsonpath={.data.token} | base64 --decode &gt; token_decode <co xml:id="CO44-1"/></programlisting>
<calloutlist>
<callout arearefs="CO44-1">
<para>Replace <literal>&lt;export_name&gt;</literal> with the <literal>metadata.name</literal> value from the <literal>VirtualMachineExport</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Copy the <literal>token_decode</literal> file to the target cluster.</simpara>
</listitem>
<listitem>
<simpara>Get the <literal>VirtualMachineExport</literal> custom resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmexport &lt;export_name&gt; -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Review the <literal>status.links</literal> stanza, which is divided into <literal>external</literal> and <literal>internal</literal> sections. Note the <literal>manifests.url</literal> fields within each section:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: export.kubevirt.io/v1alpha1
kind: VirtualMachineExport
metadata:
  name: example-export
spec:
  source:
    apiGroup: "kubevirt.io"
    kind: VirtualMachine
    name: example-vm
  tokenSecretRef: example-token
status:
#...
  links:
    external:
#...
      manifests:
      - type: all
        url: https://vmexport-proxy.test.net/api/export.kubevirt.io/v1alpha1/namespaces/example/virtualmachineexports/example-export/external/manifests/all <co xml:id="CO45-1"/>
      - type: auth-header-secret
        url: https://vmexport-proxy.test.net/api/export.kubevirt.io/v1alpha1/namespaces/example/virtualmachineexports/example-export/external/manifests/secret <co xml:id="CO45-2"/>
    internal:
#...
      manifests:
      - type: all
        url: https://virt-export-export-pvc.default.svc/internal/manifests/all <co xml:id="CO45-3"/>
      - type: auth-header-secret
        url: https://virt-export-export-pvc.default.svc/internal/manifests/secret
  phase: Ready
  serviceName: virt-export-example-export</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO45-1">
<para>Contains the <literal>VirtualMachine</literal> manifest, <literal>DataVolume</literal> manifest, if present, and a <literal>ConfigMap</literal> manifest that contains the public certificate for the external URL&#8217;s ingress or route.</para>
</callout>
<callout arearefs="CO45-2">
<para>Contains a secret containing a header that is compatible with Containerized Data Importer (CDI). The header contains a text version of the export token.</para>
</callout>
<callout arearefs="CO45-3">
<para>Contains the <literal>VirtualMachine</literal> manifest, <literal>DataVolume</literal> manifest, if present, and a <literal>ConfigMap</literal> manifest that contains the certificate for the internal URL&#8217;s export server.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Log in to the target cluster.</simpara>
</listitem>
<listitem>
<simpara>Get the <literal>Secret</literal> manifest by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --cacert cacert.crt &lt;secret_manifest_url&gt; -H \ <co xml:id="CO46-1"/>
"x-kubevirt-export-token:token_decode" -H \ <co xml:id="CO46-2"/>
"Accept:application/yaml"</programlisting>
<calloutlist>
<callout arearefs="CO46-1">
<para>Replace <literal>&lt;secret_manifest_url&gt;</literal> with an <literal>auth-header-secret</literal> URL from the <literal>VirtualMachineExport</literal> YAML output.</para>
</callout>
<callout arearefs="CO46-2">
<para>Reference the <literal>token_decode</literal> file that you created earlier.</para>
</callout>
</calloutlist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --cacert cacert.crt https://vmexport-proxy.test.net/api/export.kubevirt.io/v1alpha1/namespaces/example/virtualmachineexports/example-export/external/manifests/secret -H "x-kubevirt-export-token:token_decode" -H "Accept:application/yaml"</programlisting>
</listitem>
<listitem>
<simpara>Get the manifests of <literal>type: all</literal>, such as the <literal>ConfigMap</literal> and <literal>VirtualMachine</literal> manifests, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --cacert cacert.crt &lt;all_manifest_url&gt; -H \ <co xml:id="CO47-1"/>
"x-kubevirt-export-token:token_decode" -H \ <co xml:id="CO47-2"/>
"Accept:application/yaml"</programlisting>
<calloutlist>
<callout arearefs="CO47-1">
<para>Replace <literal>&lt;all_manifest_url&gt;</literal> with a URL from the <literal>VirtualMachineExport</literal> YAML output.</para>
</callout>
<callout arearefs="CO47-2">
<para>Reference the <literal>token_decode</literal> file that you created earlier.</para>
</callout>
</calloutlist>
<simpara>For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl --cacert cacert.crt https://vmexport-proxy.test.net/api/export.kubevirt.io/v1alpha1/namespaces/example/virtualmachineexports/example-export/external/manifests/all -H "x-kubevirt-export-token:token_decode" -H "Accept:application/yaml"</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Next steps</title>
<listitem>
<simpara>You can now create the <literal>ConfigMap</literal> and <literal>VirtualMachine</literal> objects on the target cluster by using the exported manifests.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-manage-vmis">
<title>Managing virtual machine instances</title>

<simpara>If you have standalone virtual machine instances (VMIs) that were created independently outside of the OpenShift Virtualization environment, you can manage them by using the web console or by using <literal>oc</literal> or <link linkend="virt-using-the-cli-tools"><literal>virtctl</literal></link> commands from the command-line interface (CLI).</simpara>
<simpara>The <literal>virtctl</literal> command provides more virtualization options than the <literal>oc</literal> command. For example, you can use <literal>virtctl</literal> to pause a VM or expose a port.</simpara>
<section xml:id="virt-about-vmis_virt-manage-vmis">
<title>About virtual machine instances</title>
<simpara>A virtual machine instance (VMI) is a representation of a running virtual machine (VM). When a VMI is owned by a VM or by another object, you manage it through its owner in the web console or by using the <literal>oc</literal> command-line interface (CLI).</simpara>
<simpara>A standalone VMI is created and started independently with a script, through automation, or by using other methods in the CLI. In your environment, you might have standalone VMIs that were developed and started outside of the OpenShift Virtualization environment. You can continue to manage those standalone VMIs by using the CLI. You can also use the web console for specific tasks associated with standalone VMIs:</simpara>
<itemizedlist>
<listitem>
<simpara>List standalone VMIs and their details.</simpara>
</listitem>
<listitem>
<simpara>Edit labels and annotations for a standalone VMI.</simpara>
</listitem>
<listitem>
<simpara>Delete a standalone VMI.</simpara>
</listitem>
</itemizedlist>
<simpara>When you delete a VM, the associated VMI is automatically deleted. You delete a standalone VMI directly because it is not owned by VMs or other objects.</simpara>
<note>
<simpara>Before you uninstall OpenShift Virtualization, list and view the standalone VMIs by using the CLI or the web console. Then, delete any outstanding VMIs.</simpara>
</note>
</section>
<section xml:id="virt-listing-vmis-cli_virt-manage-vmis">
<title>Listing all virtual machine instances using the CLI</title>
<simpara>You can list all virtual machine instances (VMIs) in your cluster, including standalone VMIs and those owned by virtual machines, by using the <literal>oc</literal> command-line interface (CLI).</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>List all VMIs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmis -A</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-listing-vmis-web_virt-manage-vmis">
<title>Listing standalone virtual machine instances using the web console</title>
<simpara>Using the web console, you can list and view standalone virtual machine instances (VMIs) in your cluster that are not owned by virtual machines (VMs).</simpara>
<note>
<simpara>VMIs that are owned by VMs or other objects are not displayed in the web console. The web console displays only standalone VMIs. If you want to list all VMIs in your cluster, you must use the CLI.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
<simpara>You can identify a standalone VMI by a dark colored badge next to its name.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-editing-vmis-web_virt-manage-vmis">
<title>Editing a standalone virtual machine instance using the web console</title>
<simpara>You can edit the annotations and labels of a standalone virtual machine instance (VMI) using the web console. Other fields are not editable.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a standalone VMI to open the <emphasis role="strong">VirtualMachineInstance details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Details</emphasis> tab, click the pencil icon beside <emphasis role="strong">Annotations</emphasis> or <emphasis role="strong">Labels</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Make the relevant changes and click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-deleting-vmis-cli_virt-manage-vmis">
<title>Deleting a standalone virtual machine instance using the CLI</title>
<simpara>You can delete a standalone virtual machine instance (VMI) by using the <literal>oc</literal> command-line interface (CLI).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Identify the name of the VMI that you want to delete.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Delete the VMI by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vmi &lt;vmi_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-deleting-vmis-web_virt-manage-vmis">
<title>Deleting a standalone virtual machine instance using the web console</title>
<simpara>Delete a standalone virtual machine instance (VMI) from the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Actions</emphasis> &#8594; <emphasis role="strong">Delete VirtualMachineInstance</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the confirmation pop-up window, click <emphasis role="strong">Delete</emphasis> to permanently delete the standalone VMI.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-controlling-vm-states">
<title>Controlling virtual machine states</title>

<simpara>You can stop, start, restart, and unpause virtual machines from the web console.</simpara>
<simpara>You can use <link linkend="virt-using-the-cli-tools"><literal>virtctl</literal></link> to manage virtual machine states and perform other actions from the CLI. For example, you can use <literal>virtctl</literal> to force stop a VM or expose a port.</simpara>
<section xml:id="virt-starting-vm-web_virt-controlling-vm-states">
<title>Starting a virtual machine</title>
<simpara>You can start a virtual machine from the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Find the row that contains the virtual machine that you want to start.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the appropriate menu for your use case:</simpara>
<itemizedlist>
<listitem>
<simpara>To stay on this page, where you can perform actions on multiple virtual machines:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> located at the far right end of the row.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To view comprehensive information about the selected virtual machine before you start it:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Access the <emphasis role="strong">VirtualMachine details</emphasis> page by clicking the name of the virtual machine.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Actions</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Restart</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the confirmation window, click <emphasis role="strong">Start</emphasis> to start the virtual machine.</simpara>
</listitem>
</orderedlist>
<note>
<simpara>When you start virtual machine that is provisioned from a <literal>URL</literal> source for the first time, the virtual machine has a status of <emphasis role="strong">Importing</emphasis> while OpenShift Virtualization imports the container from the URL endpoint. Depending on the size of the image, this process might take several minutes.</simpara>
</note>
</section>
<section xml:id="virt-restarting-vm-web_virt-controlling-vm-states">
<title>Restarting a virtual machine</title>
<simpara>You can restart a running virtual machine from the web console.</simpara>
<important>
<simpara>To avoid errors, do not restart a virtual machine while it has a status of <emphasis role="strong">Importing</emphasis>.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Find the row that contains the virtual machine that you want to restart.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the appropriate menu for your use case:</simpara>
<itemizedlist>
<listitem>
<simpara>To stay on this page, where you can perform actions on multiple virtual machines:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> located at the far right end of the row.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To view comprehensive information about the selected virtual machine before
you restart it:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Access the <emphasis role="strong">VirtualMachine details</emphasis> page by clicking the name of the virtual
machine.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Actions</emphasis> &#8594; <emphasis role="strong">Restart</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>In the confirmation window, click <emphasis role="strong">Restart</emphasis> to restart the virtual machine.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-stopping-vm-web_virt-controlling-vm-states">
<title>Stopping a virtual machine</title>
<simpara>You can stop a virtual machine from the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594;  <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Find the row that contains the virtual machine that you want to stop.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the appropriate menu for your use case:</simpara>
<itemizedlist>
<listitem>
<simpara>To stay on this page, where you can perform actions on multiple virtual machines:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> located at the far right end of the row.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To view comprehensive information about the selected virtual machine before you stop it:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Access the <emphasis role="strong">VirtualMachine details</emphasis> page by clicking the name of the virtual machine.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Actions</emphasis> â†’ <emphasis role="strong">Stop</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>In the confirmation window, click <emphasis role="strong">Stop</emphasis> to stop the virtual machine.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-unpausing-vm-web_virt-controlling-vm-states">
<title>Unpausing a virtual machine</title>
<simpara>You can unpause a paused virtual machine from the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>At least one of your virtual machines must have a status of <emphasis role="strong">Paused</emphasis>.</simpara>
<note>
<simpara>You can pause virtual machines by using the <literal>virtctl</literal> client.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Find the row that contains the virtual machine that you want to unpause.</simpara>
</listitem>
<listitem>
<simpara>Navigate to the appropriate menu for your use case:</simpara>
<itemizedlist>
<listitem>
<simpara>To stay on this page, where you can perform actions on multiple virtual machines:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the <emphasis role="strong">Status</emphasis> column, click <emphasis role="strong">Paused</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>To view comprehensive information about the selected virtual machine before
you unpause it:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Access the <emphasis role="strong">VirtualMachine details</emphasis> page by clicking the name of the virtual
machine.</simpara>
</listitem>
<listitem>
<simpara>Click the pencil icon that is located on the right side of <emphasis role="strong">Status</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>In the confirmation window, click <emphasis role="strong">Unpause</emphasis> to unpause the virtual machine.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-using-vtpm-devices">
<title>Using virtual Trusted Platform Module devices</title>

<simpara>Add a virtual Trusted Platform Module (vTPM) device to a new or existing virtual
machine by editing the <literal>VirtualMachine</literal> (VM) or <literal>VirtualMachineInstance</literal> (VMI)
manifest.</simpara>
<section xml:id="virt-about-vtpm-devices_virt-using-vtpm-devices">
<title>About vTPM devices</title>
<simpara>A virtual Trusted Platform Module (vTPM) device functions like a
physical Trusted Platform Module (TPM) hardware chip.</simpara>
<simpara>You can use a vTPM device with any operating system, but Windows 11 requires
the presence of a TPM chip to install or boot. A vTPM device allows VMs created
from a Windows 11 image to function without a physical TPM chip.</simpara>
<simpara>If you do not enable vTPM, then the VM does not recognize a TPM device, even if
the node has one.</simpara>
<simpara>A vTPM device also protects virtual machines by storing secrets without physical hardware. OpenShift Virtualization supports persisting vTPM device state by using Persistent Volume Claims (PVCs) for VMs. You must specify the storage class to be used by the PVC by setting the <literal>vmStateStorageClass</literal> attribute in the <literal>HyperConverged</literal> custom resource (CR):</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  vmStateStorageClass: &lt;storage_class_name&gt;

# ...</programlisting>
<note>
<simpara>The storage class must be of type <literal>Filesystem</literal> and support the <literal>ReadWriteMany</literal> (RWX) access mode.</simpara>
</note>
</section>
<section xml:id="virt-adding-vtpm-to-vm_virt-using-vtpm-devices">
<title>Adding a vTPM device to a virtual machine</title>
<simpara>Adding a virtual Trusted Platform Module (vTPM) device to a virtual machine
(VM) allows you to run a VM created from a Windows 11 image without a physical
TPM device. A vTPM device also stores secrets for that VM.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have configured a Persistent Volume Claim (PVC) to use a storage class of type <literal>Filesystem</literal> that supports the <literal>ReadWriteMany</literal> (RWX) access mode. This is necessary for the vTPM device data to persist across VM reboots.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the following command to update the VM configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit vm &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Edit the VM specification to add the vTPM device. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
    name: example-vm
spec:
  template:
    spec:
      domain:
        devices:
          tpm:  <co xml:id="CO48-1"/>
            persistent: true <co xml:id="CO48-2"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO48-1">
<para>Adds the vTPM device to the VM.</para>
</callout>
<callout arearefs="CO48-2">
<para>Specifies that the vTPM device state persists after the VM is shut down. The default value is <literal>false</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To apply your changes, save and exit the editor.</simpara>
</listitem>
<listitem>
<simpara>Optional: If you edited a running virtual machine, you must restart it for
the changes to take effect.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-managing-vms-openshift-pipelines">
<title>Managing virtual machines with OpenShift Pipelines</title>

<simpara><link xlink:href="https://docs.openshift.com/pipelines/latest/about/understanding-openshift-pipelines.html">Red Hat OpenShift Pipelines</link> is a Kubernetes-native CI/CD framework that allows developers to design and run each step of the CI/CD pipeline in its own container.</simpara>
<simpara>The Scheduling, Scale, and Performance (SSP) Operator integrates OpenShift Virtualization with OpenShift Pipelines. The SSP Operator includes tasks and example pipelines that allow you to:</simpara>
<itemizedlist>
<listitem>
<simpara>Create and manage virtual machines (VMs), persistent volume claims (PVCs), and data volumes</simpara>
</listitem>
<listitem>
<simpara>Run commands in VMs</simpara>
</listitem>
<listitem>
<simpara>Manipulate disk images with <literal>libguestfs</literal> tools</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>Managing virtual machines with Red Hat OpenShift Pipelines is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="prerequisites_virt-managing-vms-openshift-pipelines">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have <link xlink:href="https://docs.openshift.com/pipelines/latest/install_config/installing-pipelines.html">installed OpenShift Pipelines</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-deploying-ssp_virt-managing-vms-openshift-pipelines">
<title>Deploying the Scheduling, Scale, and Performance (SSP) resources</title>
<simpara>The SSP Operator example Tekton Tasks and Pipelines are not deployed by default when you install OpenShift Virtualization. To deploy the SSP Operator&#8217;s Tekton resources, enable the <literal>deployTektonTaskResources</literal> feature gate in the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>spec.featureGates.deployTektonTaskResources</literal> field to <literal>true</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: kubevirt-hyperconverged
spec:
  tektonPipelinesNamespace: &lt;user_namespace&gt; <co xml:id="CO49-1"/>
  featureGates:
    deployTektonTaskResources: true <co xml:id="CO49-2"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO49-1">
<para>The namespace where the pipelines are to be run.</para>
</callout>
<callout arearefs="CO49-2">
<para>The feature gate to be enabled to deploy Tekton resources by SSP operator.</para>
</callout>
</calloutlist>
<note>
<simpara>The tasks and example pipelines remain available even if you disable the feature gate later.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-supported-ssp-tasks_virt-managing-vms-openshift-pipelines">
<title>Virtual machine tasks supported by the SSP Operator</title>
<simpara>The following table shows the tasks that are included as part of the SSP Operator.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Virtual machine tasks supported by the SSP Operator</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Task</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>create-vm-from-manifest</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a virtual machine from a provided manifest or with <literal>virtctl</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>create-vm-from-template</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create a virtual machine from a template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>copy-template</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Copy a virtual machine template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>modify-vm-template</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Modify a virtual machine template.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>modify-data-object</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Create or delete data volumes or data sources.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cleanup-vm</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Run a script or a command in a virtual machine and stop or delete the virtual machine afterward.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>disk-virt-customize</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Use the <literal>virt-customize</literal> tool to run a customization script on a target PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>disk-virt-sysprep</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Use the <literal>virt-sysprep</literal> tool to run a sysprep script on a target PVC.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>wait-for-vmi-status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Wait for a specific status of a virtual machine instance and fail or succeed based on the status.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>Virtual machine creation in pipelines now utilizes <literal>ClusterInstanceType</literal> and <literal>ClusterPreference</literal> instead of template-based tasks, which have been deprecated. The <literal>create-vm-from-template</literal>, <literal>copy-template</literal>, and <literal>modify-vm-template</literal> commands remain available but are not used in default pipeline tasks.</simpara>
</note>
</section>
<section xml:id="example-pipelines_virt-managing-vms-openshift-pipelines">
<title>Example pipelines</title>
<simpara>The SSP Operator includes the following example <literal>Pipeline</literal> manifests. You can run the example pipelines by using the web console or CLI.</simpara>
<simpara>You might have to run more than one installer pipeline if you need multiple versions of Windows. If you run more than one installer pipeline, each one requires unique parameters, such as the <literal>autounattend</literal> config map and base image name. For example, if you need Windows 10 and Windows 11 or Windows Server 2022 images, you have to run both the Windows efi installer pipeline and the Windows bios installer pipeline. However, if you need Windows 11 and Windows Server 2022 images, you have to run only the Windows efi installer pipeline.</simpara>
<variablelist>
<varlistentry>
<term>Windows EFI installer pipeline</term>
<listitem>
<simpara>This pipeline installs Windows 11 or Windows Server 2022 into a new data volume from a Windows installation image (ISO file). A custom answer file is used to run the installation process.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Windows BIOS installer pipeline</term>
<listitem>
<simpara>This pipeline installs Windows 10 into a new data volume from a Windows installation image, also called an ISO file. A custom answer file is used to run the installation process.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Windows customize pipeline</term>
<listitem>
<simpara>This pipeline clones the data volume of a basic Windows 10, 11, or Windows Server 2022 installation, customizes it by installing Microsoft SQL Server Express or Microsoft Visual Studio Code, and then creates a new image and template.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>The example pipelines use a config map file with <literal>sysprep</literal> predefined by OpenShift Container Platform and suitable for Microsoft ISO files. For ISO files pertaining to different Windows editions, it may be necessary to create a new config map file with a system-specific sysprep definition.</simpara>
</note>
<section xml:id="virt-running-tto-pipeline-web_virt-managing-vms-openshift-pipelines">
<title>Running the example pipelines using the web console</title>
<simpara>You can run the example pipelines from the <emphasis role="strong">Pipelines</emphasis> menu in the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Pipelines</emphasis> &#8594; <emphasis role="strong">Pipelines</emphasis> in the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a pipeline to open the <emphasis role="strong">Pipeline details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>From the <emphasis role="strong">Actions</emphasis> list, select <emphasis role="strong">Start</emphasis>. The <emphasis role="strong">Start Pipeline</emphasis> dialog is displayed.</simpara>
</listitem>
<listitem>
<simpara>Keep the default values for the parameters and then click <emphasis role="strong">Start</emphasis> to run the pipeline. The <emphasis role="strong">Details</emphasis> tab tracks the progress of each task and displays the pipeline status.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-running-tto-pipeline-cli_virt-managing-vms-openshift-pipelines">
<title>Running the example pipelines using the CLI</title>
<simpara>Use a <literal>PipelineRun</literal> resource to run the example pipelines. A <literal>PipelineRun</literal> object is the running instance of a pipeline. It instantiates a pipeline for execution with specific inputs, outputs, and execution parameters on a cluster. It also creates a <literal>TaskRun</literal> object for each task in the pipeline.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To run the Windows 10 installer pipeline, create the following <literal>PipelineRun</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: windows10-installer-run-
  labels:
    pipelinerun: windows10-installer-run
spec:
  params:
  - name: winImageDownloadURL
    value: &lt;link_to_windows_10_iso&gt; <co xml:id="CO50-1"/>
  pipelineRef:
    name: windows10-installer
  taskRunSpecs:
    - pipelineTaskName: copy-template
      taskServiceAccountName: copy-template-task
    - pipelineTaskName: modify-vm-template
      taskServiceAccountName: modify-vm-template-task
    - pipelineTaskName: create-vm-from-template
      taskServiceAccountName: create-vm-from-template-task
    - pipelineTaskName: wait-for-vmi-status
      taskServiceAccountName: wait-for-vmi-status-task
    - pipelineTaskName: create-base-dv
      taskServiceAccountName: modify-data-object-task
    - pipelineTaskName: cleanup-vm
      taskServiceAccountName: cleanup-vm-task
  status: {}</programlisting>
<calloutlist>
<callout arearefs="CO50-1">
<para>Specify the URL for the Windows 10 64-bit ISO file. The product language must be English (United States).</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>PipelineRun</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f windows10-installer-run.yaml</programlisting>
</listitem>
<listitem>
<simpara>To run the Windows 10 customize pipeline, create the following <literal>PipelineRun</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: tekton.dev/v1beta1
kind: PipelineRun
metadata:
  generateName: windows10-customize-run-
  labels:
    pipelinerun: windows10-customize-run
spec:
  params:
    - name: allowReplaceGoldenTemplate
      value: true
    - name: allowReplaceCustomizationTemplate
      value: true
  pipelineRef:
    name: windows10-customize
  taskRunSpecs:
    - pipelineTaskName: copy-template-customize
      taskServiceAccountName: copy-template-task
    - pipelineTaskName: modify-vm-template-customize
      taskServiceAccountName: modify-vm-template-task
    - pipelineTaskName: create-vm-from-template
      taskServiceAccountName: create-vm-from-template-task
    - pipelineTaskName: wait-for-vmi-status
      taskServiceAccountName: wait-for-vmi-status-task
    - pipelineTaskName: create-base-dv
      taskServiceAccountName: modify-data-object-task
    - pipelineTaskName: cleanup-vm
      taskServiceAccountName: cleanup-vm-task
    - pipelineTaskName: copy-template-golden
      taskServiceAccountName: copy-template-task
    - pipelineTaskName: modify-vm-template-golden
      taskServiceAccountName: modify-vm-template-task
status: {}</programlisting>
</listitem>
<listitem>
<simpara>Apply the <literal>PipelineRun</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f windows10-customize-run.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_virt-managing-vms-openshift-pipelines" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://docs.openshift.com/pipelines/latest/create/creating-applications-with-cicd-pipelines.html">Creating CI/CD solutions for applications using Red Hat OpenShift Pipelines</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-creating-windows-vm_virt-creating-vms-uploading-images">Creating a Windows VM</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_advanced-virtual-machine-management">
<title>Advanced virtual machine management</title>
<section xml:id="virt-working-with-resource-quotas-for-vms">
<title>Working with resource quotas for virtual machines</title>

<simpara>Create and manage resource quotas for virtual machines.</simpara>
<section xml:id="virt-setting-resource-quota-limits-for-vms_virt-working-with-resource-quotas-for-vms">
<title>Setting resource quota limits for virtual machines</title>
<simpara>Resource quotas that only use requests automatically work with virtual machines (VMs). If your resource quota uses limits, you must manually set resource limits on VMs. Resource limits must be at least 100 MiB larger than resource requests.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set limits for a VM by editing the <literal>VirtualMachine</literal> manifest. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: with-limits
spec:
  running: false
  template:
    spec:
      domain:
# ...
        resources:
          requests:
            memory: 128Mi
          limits:
            memory: 256Mi  <co xml:id="CO51-1"/></programlisting>
<calloutlist>
<callout arearefs="CO51-1">
<para>This configuration is supported because the <literal>limits.memory</literal> value is at least <literal>100Mi</literal> larger than the <literal>requests.memory</literal> value.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the <literal>VirtualMachine</literal> manifest.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_virt-working-with-resource-quotas-for-vms" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#quotas-setting-per-project">Resource quotas per project</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#quotas-setting-across-multiple-projects">Resource quotas across multiple projects</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-specifying-nodes-for-vms">
<title>Specifying nodes for virtual machines</title>

<simpara>You can place virtual machines (VMs) on specific nodes by using node placement rules.</simpara>
<section xml:id="virt-about-node-placement-vms_virt-specifying-nodes-for-vms">
<title>About node placement for virtual machines</title>
<simpara>To ensure that virtual machines (VMs) run on appropriate nodes, you can configure node placement rules. You might want to do this if:</simpara>
<itemizedlist>
<listitem>
<simpara>You have several VMs. To ensure fault tolerance, you want them to run on different nodes.</simpara>
</listitem>
<listitem>
<simpara>You have two chatty VMs. To avoid redundant inter-node routing, you want the VMs to run on the same node.</simpara>
</listitem>
<listitem>
<simpara>Your VMs require specific hardware features that are not present on all available nodes.</simpara>
</listitem>
<listitem>
<simpara>You have a pod that adds capabilities to a node, and you want to place a VM on that node so that it can use those capabilities.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Virtual machine placement relies on any existing node placement rules for workloads. If workloads are excluded from specific nodes on the component level, virtual machines cannot be placed on those nodes.</simpara>
</note>
<simpara>You can use the following rule types in the <literal>spec</literal> field of a <literal>VirtualMachine</literal> manifest:</simpara>
<variablelist>
<varlistentry>
<term><literal>nodeSelector</literal></term>
<listitem>
<simpara>Allows virtual machines to be scheduled on nodes that are labeled with the key-value pair or pairs that you specify in this field. The node must have labels that exactly match all listed pairs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>affinity</literal></term>
<listitem>
<simpara>Enables you to use more expressive syntax to set rules that match nodes with virtual machines. For example, you can specify that a rule is a preference, rather than a hard requirement, so that virtual machines are still scheduled if the rule is not satisfied. Pod affinity, pod anti-affinity, and node affinity are supported for virtual machine placement. Pod affinity works for virtual machines because the <literal>VirtualMachine</literal> workload type is based on the <literal>Pod</literal> object.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>tolerations</literal></term>
<listitem>
<simpara>Allows virtual machines to be scheduled on nodes that have matching taints. If a taint is applied to a node, that node only accepts virtual machines that tolerate the taint.</simpara>
<note>
<simpara>Affinity rules only apply during scheduling. OpenShift Container Platform does not reschedule running workloads if the constraints are no longer met.</simpara>
</note>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="node-placement-examples_virt-specifying-nodes-for-vms">
<title>Node placement examples</title>
<simpara>The following example YAML file snippets use <literal>nodePlacement</literal>, <literal>affinity</literal>, and <literal>tolerations</literal> fields to customize node placement for virtual machines.</simpara>
<section xml:id="virt-example-vm-node-placement-node-selector_virt-specifying-nodes-for-vms">
<title>Example: VM node placement with nodeSelector</title>
<simpara>In this example, the virtual machine requires a node that has metadata containing both <literal>example-key-1 = example-value-1</literal> and <literal>example-key-2 = example-value-2</literal> labels.</simpara>
<warning>
<simpara>If there are no nodes that fit this description, the virtual machine is not scheduled.</simpara>
</warning>
<formalpara>
<title>Example VM manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  name: example-vm-node-selector
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  template:
    spec:
      nodeSelector:
        example-key-1: example-value-1
        example-key-2: example-value-2
# ...</programlisting>
</para>
</formalpara>
</section>
<section xml:id="virt-example-vm-node-placement-pod-affinity_virt-specifying-nodes-for-vms">
<title>Example: VM node placement with pod affinity and pod anti-affinity</title>
<simpara>In this example, the VM must be scheduled on a node that has a running pod with the label <literal>example-key-1 = example-value-1</literal>. If there is no such pod running on any node, the VM is not scheduled.</simpara>
<simpara>If possible, the VM is not scheduled on a node that has any pod with the label <literal>example-key-2 = example-value-2</literal>. However, if all candidate nodes have a pod with this label, the scheduler ignores this constraint.</simpara>
<formalpara>
<title>Example VM manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  name: example-vm-pod-affinity
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  template:
    spec:
      affinity:
        podAffinity:
          requiredDuringSchedulingIgnoredDuringExecution: <co xml:id="CO52-1"/>
          - labelSelector:
              matchExpressions:
              - key: example-key-1
                operator: In
                values:
                - example-value-1
            topologyKey: kubernetes.io/hostname
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution: <co xml:id="CO52-2"/>
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: example-key-2
                  operator: In
                  values:
                  - example-value-2
              topologyKey: kubernetes.io/hostname
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO52-1">
<para>If you use the <literal>requiredDuringSchedulingIgnoredDuringExecution</literal> rule type, the VM is not scheduled if the constraint is not met.</para>
</callout>
<callout arearefs="CO52-2">
<para>If you use the <literal>preferredDuringSchedulingIgnoredDuringExecution</literal> rule type, the VM is still scheduled if the constraint is not met, as long as all required constraints are met.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-vm-node-placement-node-affinity_virt-specifying-nodes-for-vms">
<title>Example: VM node placement with node affinity</title>
<simpara>In this example, the VM must be scheduled on a node that has the label <literal>example.io/example-key = example-value-1</literal> or the label <literal>example.io/example-key = example-value-2</literal>. The constraint is met if only one of the labels is present on the node. If neither label is present, the VM is not scheduled.</simpara>
<simpara>If possible, the scheduler avoids nodes that have the label <literal>example-node-label-key = example-node-label-value</literal>. However, if all candidate nodes have this label, the scheduler ignores this constraint.</simpara>
<formalpara>
<title>Example VM manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  name: example-vm-node-affinity
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  template:
    spec:
      affinity:
        nodeAffinity:
          requiredDuringSchedulingIgnoredDuringExecution: <co xml:id="CO53-1"/>
            nodeSelectorTerms:
            - matchExpressions:
              - key: example.io/example-key
                operator: In
                values:
                - example-value-1
                - example-value-2
          preferredDuringSchedulingIgnoredDuringExecution: <co xml:id="CO53-2"/>
          - weight: 1
            preference:
              matchExpressions:
              - key: example-node-label-key
                operator: In
                values:
                - example-node-label-value
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO53-1">
<para>If you use the <literal>requiredDuringSchedulingIgnoredDuringExecution</literal> rule type, the VM is not scheduled if the constraint is not met.</para>
</callout>
<callout arearefs="CO53-2">
<para>If you use the <literal>preferredDuringSchedulingIgnoredDuringExecution</literal> rule type, the VM is still scheduled if the constraint is not met, as long as all required constraints are met.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-example-vm-node-placement-tolerations_virt-specifying-nodes-for-vms">
<title>Example: VM node placement with tolerations</title>
<simpara>In this example, nodes that are reserved for virtual machines are already labeled with the <literal>key=virtualization:NoSchedule</literal> taint. Because this virtual machine has matching <literal>tolerations</literal>, it can schedule onto the tainted nodes.</simpara>
<note>
<simpara>A virtual machine that tolerates a taint is not required to schedule onto a node with that taint.</simpara>
</note>
<formalpara>
<title>Example VM manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">metadata:
  name: example-vm-tolerations
apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  tolerations:
  - key: "key"
    operator: "Equal"
    value: "virtualization"
    effect: "NoSchedule"
# ...</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="additional-resources_virt-specifying-nodes-for-vms" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-node-placement-virt-components">Specifying nodes for virtualization components</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-selectors">Placing pods on specific nodes using node selectors</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-node-affinity">Controlling pod placement on nodes using node affinity rules</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-scheduler-taints-tolerations">Controlling pod placement using node taints</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-certificate-rotation">
<title>Configuring certificate rotation</title>
<simpara>Configure certificate rotation parameters to replace existing certificates.</simpara>

<section xml:id="virt-configuring-certificate-rotation_virt-configuring-certificate-rotation">
<title>Configuring certificate rotation</title>
<simpara>You can do this during OpenShift Virtualization installation in the web console or after installation in the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>spec.certConfig</literal> fields as shown in the following example. To avoid overloading the system, ensure that all values are greater than or equal to 10 minutes. Express all values as strings that comply with the <link xlink:href="https://golang.org/pkg/time/#ParseDuration">golang <literal>ParseDuration</literal> format</link>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  certConfig:
    ca:
      duration: 48h0m0s
      renewBefore: 24h0m0s <co xml:id="CO54-1"/>
    server:
      duration: 24h0m0s  <co xml:id="CO54-2"/>
      renewBefore: 12h0m0s  <co xml:id="CO54-3"/></programlisting>
<calloutlist>
<callout arearefs="CO54-1">
<para>The value of <literal>ca.renewBefore</literal> must be less than or equal to the value of <literal>ca.duration</literal>.</para>
</callout>
<callout arearefs="CO54-2">
<para>The value of <literal>server.duration</literal> must be less than or equal to the value of <literal>ca.duration</literal>.</para>
</callout>
<callout arearefs="CO54-3">
<para>The value of <literal>server.renewBefore</literal> must be less than or equal to the value of <literal>server.duration</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the YAML file to your cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-troubleshooting-cert-rotation-parameters_virt-configuring-certificate-rotation">
<title>Troubleshooting certificate rotation parameters</title>
<simpara>Deleting one or more <literal>certConfig</literal> values causes them to revert to the default values, unless the default values conflict with one of the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>The value of <literal>ca.renewBefore</literal> must be less than or equal to the value of <literal>ca.duration</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of <literal>server.duration</literal> must be less than or equal to the value of <literal>ca.duration</literal>.</simpara>
</listitem>
<listitem>
<simpara>The value of <literal>server.renewBefore</literal> must be less than or equal to the value of <literal>server.duration</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara>If the default values conflict with these conditions, you will receive an error.</simpara>
<simpara>If you remove the <literal>server.duration</literal> value in the following example, the default value of <literal>24h0m0s</literal> is greater than the value of <literal>ca.duration</literal>, conflicting with the specified conditions.</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">certConfig:
   ca:
     duration: 4h0m0s
     renewBefore: 1h0m0s
   server:
     duration: 4h0m0s
     renewBefore: 4h0m0s</programlisting>
</para>
</formalpara>
<simpara>This results in the following error message:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">error: hyperconvergeds.hco.kubevirt.io "kubevirt-hyperconverged" could not be patched: admission webhook "validate-hco.kubevirt.io" denied the request: spec.certConfig: ca.duration is smaller than server.duration</programlisting>
<simpara>The error message only mentions the first conflict. Review all certConfig values before you proceed.</simpara>
</section>
</section>
<section xml:id="virt-configuring-default-cpu-model">
<title>Configuring the default CPU model</title>
<simpara>Use the <literal>defaultCPUModel</literal> setting in the <literal>HyperConverged</literal> custom resource (CR) to define a cluster-wide default CPU model.</simpara>
<simpara>The virtual machine (VM) CPU model depends on the availability of CPU models within the VM and the cluster.</simpara>
<itemizedlist>
<listitem>
<simpara>If the VM does not have a defined CPU model:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>defaultCPUModel</literal> is automatically set using the CPU model defined at the cluster-wide level.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If both the VM and the cluster have a defined CPU model:</simpara>
<itemizedlist>
<listitem>
<simpara>The VMâ€™s CPU model takes precedence.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>If neither the VM nor the cluster have a defined CPU model:</simpara>
<itemizedlist>
<listitem>
<simpara>The host-model is automatically set using the CPU model defined at the host level.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>

<section xml:id="virt-configuring-default-cpu-model_virt-configuring-default-cpu-model">
<title>Configuring the default CPU model</title>
<simpara>Configure the <literal>defaultCPUModel</literal> by updating the <literal>HyperConverged</literal> custom resource (CR). You can change the <literal>defaultCPUModel</literal> while OpenShift Virtualization is running.</simpara>
<note>
<simpara>The <literal>defaultCPUModel</literal> is case sensitive.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (oc).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>defaultCPUModel</literal> field to the CR and set the value to the name of a CPU model that exists in the cluster:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
 name: kubevirt-hyperconverged
 namespace: openshift-cnv
spec:
  defaultCPUModel: "EPYC"</programlisting>
</listitem>
<listitem>
<simpara>Apply the YAML file to your cluster.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-uefi-mode-for-vms">
<title>Using UEFI mode for virtual machines</title>

<simpara>You can boot a virtual machine (VM) in Unified Extensible Firmware Interface (UEFI) mode.</simpara>
<section xml:id="virt-about-uefi-mode-for-vms_virt-uefi-mode-for-vms">
<title>About UEFI mode for virtual machines</title>
<simpara>Unified Extensible Firmware Interface (UEFI), like legacy BIOS, initializes hardware components and operating system image files when a computer starts. UEFI supports more modern features and customization options than BIOS, enabling faster boot times.</simpara>
<simpara>It stores all the information about initialization and startup in a file with a <literal>.efi</literal> extension, which is stored on a special partition called EFI System Partition (ESP). The ESP also contains the boot loader programs for the operating system that is installed on the computer.</simpara>
</section>
<section xml:id="virt-booting-vms-uefi-mode_virt-uefi-mode-for-vms">
<title>Booting virtual machines in UEFI mode</title>
<simpara>You can configure a virtual machine to boot in UEFI mode by editing the <literal>VirtualMachine</literal> manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit or create a <literal>VirtualMachine</literal> manifest file. Use the <literal>spec.firmware.bootloader</literal> stanza to configure UEFI mode:</simpara>
<formalpara>
<title>Booting in UEFI mode with secure boot active</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiversion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    special: vm-secureboot
  name: vm-secureboot
spec:
  template:
    metadata:
      labels:
        special: vm-secureboot
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: containerdisk
        features:
          acpi: {}
          smm:
            enabled: true <co xml:id="CO55-1"/>
        firmware:
          bootloader:
            efi:
              secureBoot: true <co xml:id="CO55-2"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO55-1">
<para>OpenShift Virtualization requires System Management Mode (<literal>SMM</literal>) to be enabled for Secure Boot in UEFI mode to occur.</para>
</callout>
<callout arearefs="CO55-2">
<para>OpenShift Virtualization supports a VM with or without Secure Boot when using UEFI mode. If Secure Boot is enabled, then UEFI mode is required. However, UEFI mode can be enabled without using Secure Boot.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the manifest to your cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-pxe-booting">
<title>Configuring PXE booting for virtual machines</title>

<simpara>PXE booting, or network booting, is available in OpenShift Virtualization.
Network booting allows a computer to boot and load an
operating system or other program without requiring a locally attached
storage device. For example, you can use it to choose your desired OS
image from a PXE server when deploying a new host.</simpara>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A Linux bridge must be <link linkend="virt-connecting-vm-to-linux-bridge">connected</link>.</simpara>
</listitem>
<listitem>
<simpara>The PXE server must be connected to the same VLAN as the bridge.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-pxe-booting-with-mac-address_pxe-booting">
<title>PXE booting with a specified MAC address</title>
<simpara>As an administrator, you can boot a client over the network by first creating a <literal>NetworkAttachmentDefinition</literal> object for your PXE network.
Then, reference the network attachment definition in your virtual machine instance configuration file before you start the virtual machine instance.
You can also specify a MAC address in the virtual machine instance configuration file, if required by the PXE server.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A Linux bridge must be connected.</simpara>
</listitem>
<listitem>
<simpara>The PXE server must be connected to the same VLAN as the bridge.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure a PXE network on the cluster:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the network attachment definition file for PXE network <literal>pxe-net-conf</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: pxe-net-conf
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "pxe-net-conf",
    "plugins": [
      {
        "type": "cnv-bridge",
        "bridge": "br1",
        "vlan": 1 <co xml:id="CO56-1"/>
      },
      {
        "type": "cnv-tuning" <co xml:id="CO56-2"/>
      }
    ]
  }'</programlisting>
<calloutlist>
<callout arearefs="CO56-1">
<para>Optional: The VLAN tag.</para>
</callout>
<callout arearefs="CO56-2">
<para>The <literal>cnv-tuning</literal> plugin provides support for custom MAC addresses.</para>
</callout>
</calloutlist>
<note>
<simpara>The virtual machine instance will be attached to the bridge <literal>br1</literal> through an access port with the requested VLAN.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the network attachment definition by using the file you created in the previous step:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f pxe-net-conf.yaml</programlisting>
</listitem>
<listitem>
<simpara>Edit the virtual machine instance configuration file to include the details of the interface and network.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Specify the network and MAC address, if required by the PXE server.
If the MAC address is not specified, a value is assigned automatically.</simpara>
<simpara>Ensure that <literal>bootOrder</literal> is set to <literal>1</literal> so that the interface boots first.
In this example, the interface is connected to a network called
<literal>&lt;pxe-net&gt;</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">interfaces:
- masquerade: {}
  name: default
- bridge: {}
  name: pxe-net
  macAddress: de:00:00:00:00:de
  bootOrder: 1</programlisting>
<note>
<simpara>Boot order is global for interfaces and disks.</simpara>
</note>
</listitem>
<listitem>
<simpara>Assign a boot device number to the disk to ensure proper booting after operating system provisioning.</simpara>
<simpara>Set the disk <literal>bootOrder</literal> value to <literal>2</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">devices:
  disks:
  - disk:
      bus: virtio
    name: containerdisk
    bootOrder: 2</programlisting>
</listitem>
<listitem>
<simpara>Specify that the network is connected to the previously created network attachment definition. In this scenario, <literal>&lt;pxe-net&gt;</literal> is connected to the network attachment definition called <literal>&lt;pxe-net-conf&gt;</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">networks:
- name: default
  pod: {}
- name: pxe-net
  multus:
    networkName: pxe-net-conf</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create the virtual machine instance:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f vmi-pxe-boot.yaml</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">  virtualmachineinstance.kubevirt.io "vmi-pxe-boot" created</programlisting>
</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Wait for the virtual machine instance to run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi vmi-pxe-boot -o yaml | grep -i phase
  phase: Running</programlisting>
</listitem>
<listitem>
<simpara>View the virtual machine instance using VNC:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl vnc vmi-pxe-boot</programlisting>
</listitem>
<listitem>
<simpara>Watch the boot screen to verify that the PXE boot is successful.</simpara>
</listitem>
<listitem>
<simpara>Log in to the virtual machine instance:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl console vmi-pxe-boot</programlisting>
</listitem>
<listitem>
<simpara>Verify the interfaces and MAC address on the virtual machine and that the interface connected to the bridge has the specified MAC address.
In this case, we used <literal>eth1</literal> for the PXE boot, without an IP address. The other interface, <literal>eth0</literal>, got an IP address from OpenShift Container Platform.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip addr</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...
3. eth1: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
   link/ether de:00:00:00:00:de brd ff:ff:ff:ff:ff:ff</programlisting>
</para>
</formalpara>
</section>
<section xml:id="virt-networking-glossary_pxe-booting">
<title>OpenShift Virtualization networking glossary</title>
<simpara>The following terms are used throughout OpenShift Virtualization documentation:</simpara>
<variablelist>
<varlistentry>
<term>Container Network Interface (CNI)</term>
<listitem>
<simpara>A <link xlink:href="https://www.cncf.io/">Cloud Native Computing Foundation</link>
project, focused on container network connectivity.
OpenShift Virtualization uses CNI plugins to build upon the basic Kubernetes networking functionality.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Multus</term>
<listitem>
<simpara>A "meta" CNI plugin that allows multiple CNIs to exist so that a pod or virtual machine can use the interfaces it needs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Custom resource definition (CRD)</term>
<listitem>
<simpara>A <link xlink:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Kubernetes</link>
API resource that allows you to define custom resources, or an object defined by using the CRD API resource.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Network attachment definition (NAD)</term>
<listitem>
<simpara>A CRD introduced by the Multus project that allows you to attach pods, virtual machines, and virtual machine instances to one or more networks.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Node network configuration policy (NNCP)</term>
<listitem>
<simpara>A CRD introduced by the nmstate project, describing the requested network configuration on nodes.
You update the node network configuration, including adding and removing interfaces, by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest to the cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="virt-using-huge-pages-with-vms">
<title>Using huge pages with virtual machines</title>

<simpara>You can use huge pages as backing memory for virtual machines in your cluster.</simpara>
<section xml:id="_prerequisites-2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Nodes must have <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#configuring-huge-pages_huge-pages">pre-allocated huge pages configured</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="what-huge-pages-do_virt-using-huge-pages-with-vms">
<title>What huge pages do</title>
<simpara>Memory is managed in blocks known as pages. On most systems, a page is 4Ki. 1Mi
of memory is equal to 256 pages; 1Gi of memory is 256,000 pages, and so on. CPUs
have a built-in memory management unit that manages a list of these pages in
hardware. The Translation Lookaside Buffer (TLB) is a small hardware cache of
virtual-to-physical page mappings. If the virtual address passed in a hardware
instruction can be found in the TLB, the mapping can be determined quickly. If
not, a TLB miss occurs, and the system falls back to slower, software-based
address translation, resulting in performance issues. Since the size of the TLB
is fixed, the only way to reduce the chance of a TLB miss is to increase the
page size.</simpara>
<simpara>A huge page is a memory page that is larger than 4Ki. On x86_64 architectures,
there are two common huge page sizes: 2Mi and 1Gi. Sizes vary on other
architectures. To use huge pages, code must be written so that
applications are aware of them. Transparent Huge Pages (THP) attempt to automate
the management of huge pages without application knowledge, but they have
limitations. In particular, they are limited to 2Mi page sizes. THP can lead to
performance degradation on nodes with high memory utilization or fragmentation
due to defragmenting efforts of THP, which can lock memory pages. For this
reason, some applications may be designed to (or recommend) usage of
pre-allocated huge pages instead of THP.</simpara>
<simpara>In OpenShift Virtualization, virtual machines can be configured to consume pre-allocated
huge pages.</simpara>
</section>
<section xml:id="virt-configuring-huge-pages-for-vms_virt-using-huge-pages-with-vms">
<title>Configuring huge pages for virtual machines</title>
<simpara>You can configure virtual machines to use pre-allocated huge pages by including the
<literal>memory.hugepages.pageSize</literal> and <literal>resources.requests.memory</literal> parameters in your virtual machine configuration.</simpara>
<simpara>The memory request must be divisible by the page size. For example, you cannot request <literal>500Mi</literal> memory with a page size of <literal>1Gi</literal>.</simpara>
<note>
<simpara>The memory layouts of the host and the guest OS are unrelated.
Huge pages requested in the virtual machine manifest apply to QEMU.
Huge pages inside the guest can only be configured based on the amount of available memory of the virtual machine instance.</simpara>
</note>
<simpara>If you edit a running virtual machine, the virtual machine must be rebooted for the changes to take effect.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Nodes must have pre-allocated huge pages configured.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In your virtual machine configuration, add the <literal>resources.requests.memory</literal> and
<literal>memory.hugepages.pageSize</literal> parameters to the <literal>spec.domain</literal>. The following configuration snippet is
for a virtual machine that requests a total of <literal>4Gi</literal> memory with a page size of <literal>1Gi</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: VirtualMachine
# ...
spec:
  domain:
    resources:
      requests:
        memory: "4Gi" <co xml:id="CO57-1"/>
    memory:
      hugepages:
        pageSize: "1Gi" <co xml:id="CO57-2"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO57-1">
<para>The total amount of memory requested for the virtual machine. This value must be divisible by the page size.</para>
</callout>
<callout arearefs="CO57-2">
<para>The size of each huge page. Valid values for x86_64 architecture are <literal>1Gi</literal> and <literal>2Mi</literal>. The page size must be smaller than the requested memory.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the virtual machine configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;virtual_machine&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-dedicated-resources-vm">
<title>Enabling dedicated resources for virtual machines</title>

<simpara>To improve performance, you can dedicate node resources, such as CPU, to a virtual machine.</simpara>
<section xml:id="virt-about-dedicated-resources_virt-dedicated-resources-vm">
<title>About dedicated resources</title>
<simpara>When you enable dedicated resources for your virtual machine, your virtual
machine&#8217;s workload is scheduled on CPUs that will not be used by other
processes. By using dedicated resources, you can improve the performance of the
virtual machine and the accuracy of latency predictions.</simpara>
</section>
<section xml:id="_prerequisites-3">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>The <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#">CPU Manager</link>  must be configured on the node. Verify that the node has the <literal>cpumanager = true</literal> label before scheduling virtual machine workloads.</simpara>
</listitem>
<listitem>
<simpara>The virtual machine must be powered off.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-enabling-dedicated-resources_virt-dedicated-resources-vm">
<title>Enabling dedicated resources for a virtual machine</title>
<simpara>You enable dedicated resources for a virtual machine in the <emphasis role="strong">Details</emphasis> tab. Virtual machines that were created from a Red Hat template can be configured with dedicated resources.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a virtual machine to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Configuration &#8594; Scheduling</emphasis> tab, click the edit icon beside <emphasis role="strong">Dedicated Resources</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Schedule this workload with dedicated resources (guaranteed policy)</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-schedule-vms">
<title>Scheduling virtual machines</title>

<simpara>You can schedule a virtual machine (VM) on a node by ensuring that the VM&#8217;s CPU model and policy attribute are matched for compatibility with the CPU models and policy attributes supported by the node.</simpara>
<section xml:id="policy-attributes_virt-schedule-vms">
<title>Policy attributes</title>
<simpara>You can schedule a virtual machine (VM) by specifying a policy attribute and a CPU feature that is matched for compatibility when the VM is scheduled on a node. A policy attribute specified for a VM determines how that VM is scheduled on a node.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="30*"/>
<colspec colname="col_2" colwidth="70*"/>
<thead>
<row>
<entry align="left" valign="top">Policy attribute</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>force</simpara></entry>
<entry align="left" valign="top"><simpara>The VM is forced to be scheduled on a node. This is true even if the host CPU does not support the VM&#8217;s CPU.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>require</simpara></entry>
<entry align="left" valign="top"><simpara>Default policy that applies to a VM if the VM is not configured with a specific CPU model and feature specification. If a node is not configured to support CPU node discovery with this default policy attribute or any one of the other policy attributes, VMs are not scheduled on that node. Either the host CPU must support the VM&#8217;s CPU or the hypervisor must be able to emulate the supported CPU model.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>optional</simpara></entry>
<entry align="left" valign="top"><simpara>The VM is added to a node if that VM is supported by the host&#8217;s physical machine CPU.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>disable</simpara></entry>
<entry align="left" valign="top"><simpara>The VM cannot be scheduled with CPU node discovery.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>forbid</simpara></entry>
<entry align="left" valign="top"><simpara>The VM is not scheduled even if the feature is supported by the host CPU and CPU node discovery is enabled.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="virt-setting-policy-attributes_virt-schedule-vms">
<title>Setting a policy attribute and CPU feature</title>
<simpara>You can set a policy attribute and CPU feature for each virtual machine (VM) to ensure that it is scheduled on a node according to policy and feature. The CPU feature that you set is verified to ensure that it is supported by the host CPU or emulated by the hypervisor.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>domain</literal> spec of your VM configuration file. The following example sets the CPU feature and the <literal>require</literal> policy for a virtual machine (VM):</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: myvm
spec:
  template:
    spec:
      domain:
        cpu:
          features:
            - name: apic <co xml:id="CO58-1"/>
              policy: require <co xml:id="CO58-2"/></programlisting>
<calloutlist>
<callout arearefs="CO58-1">
<para>Name of the CPU feature for the VM.</para>
</callout>
<callout arearefs="CO58-2">
<para>Policy attribute for the VM.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-schedule-supported-cpu-model-vms_virt-schedule-vms">
<title>Scheduling virtual machines with the supported CPU model</title>
<simpara>You can configure a CPU model for a virtual machine (VM) to schedule it on a node where its CPU model is supported.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>domain</literal> spec of your virtual machine configuration file. The following example shows a specific CPU model defined for a VM:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: myvm
spec:
  template:
    spec:
      domain:
        cpu:
          model: Conroe <co xml:id="CO59-1"/></programlisting>
<calloutlist>
<callout arearefs="CO59-1">
<para>CPU model for the VM.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-schedule-cpu-host-model-vms_virt-schedule-vms">
<title>Scheduling virtual machines with the host model</title>
<simpara>When the CPU model for a virtual machine (VM) is set to <literal>host-model</literal>, the VM inherits the CPU model of the node where it is scheduled.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>domain</literal> spec of your VM configuration file. The following example shows <literal>host-model</literal> being specified for the virtual machine:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt/v1alpha3
kind: VirtualMachine
metadata:
  name: myvm
spec:
  template:
    spec:
      domain:
        cpu:
          model: host-model <co xml:id="CO60-1"/></programlisting>
<calloutlist>
<callout arearefs="CO60-1">
<para>The VM that inherits the CPU model of the node where it is scheduled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-vm-custom-scheduler_virt-schedule-vms">
<title>Scheduling virtual machines with a custom scheduler</title>
<simpara>You can use a custom scheduler to schedule a virtual machine (VM) on a node.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A secondary scheduler is configured for your cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add the custom scheduler to the VM configuration by editing the <literal>VirtualMachine</literal> manifest. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-fedora
spec:
  running: true
  template:
    spec:
      schedulerName: my-scheduler <co xml:id="CO61-1"/>
      domain:
        devices:
          disks:
            - name: containerdisk
              disk:
                bus: virtio
# ...</programlisting>
<calloutlist>
<callout arearefs="CO61-1">
<para>The name of the custom scheduler. If the <literal>schedulerName</literal> value does not match an existing scheduler, the <literal>virt-launcher</literal> pod stays in a <literal>Pending</literal> state until the specified scheduler is found.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the VM is using the custom scheduler specified in the <literal>VirtualMachine</literal> manifest by checking the <literal>virt-launcher</literal> pod events:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>View the list of pods in your cluster by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
virt-launcher-vm-fedora-dpc87    2/2     Running   0          24m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Run the following command to display the pod events:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pod virt-launcher-vm-fedora-dpc87</programlisting>
<simpara>The value of the <literal>From</literal> field in the output verifies that the scheduler name matches the custom scheduler specified in the <literal>VirtualMachine</literal> manifest:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">[...]
Events:
  Type    Reason     Age   From              Message
  ----    ------     ----  ----              -------
  Normal  Scheduled  21m   my-scheduler  Successfully assigned default/virt-launcher-vm-fedora-dpc87 to node01
[...]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-secondary-scheduler-configuring-console_secondary-scheduler-configuring">Deploying a secondary scheduler</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-pci-passthrough">
<title>Configuring PCI passthrough</title>

<simpara>The Peripheral Component Interconnect (PCI) passthrough feature enables you to access and manage hardware devices from a virtual machine (VM). When PCI passthrough is configured, the PCI devices function as if they were physically attached to the guest operating system.</simpara>
<simpara>Cluster administrators can expose and manage host devices that are permitted to be used in the cluster by using the <literal>oc</literal> command-line interface (CLI).</simpara>
<section xml:id="virt-preparing-nodes-for-gpu-passthrough">
<title>Preparing nodes for GPU passthrough</title>
<simpara>You can prevent GPU operands from deploying on worker nodes that you designated for GPU passthrough.</simpara>
<section xml:id="virt-preventing-nvidia-operands-from-deploying-on-nodes_virt-configuring-pci-passthrough">
<title>Preventing NVIDIA GPU operands from deploying on nodes</title>
<simpara>If you use the <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/contents.html">NVIDIA GPU Operator</link> in your cluster, you can apply the <literal>nvidia.com/gpu.deploy.operands=false</literal> label to nodes that you do not want to configure for GPU or vGPU operands. This label prevents the creation of the pods that configure GPU or vGPU operands and terminates the pods if they already exist.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The OpenShift CLI (<literal>oc</literal>) is installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Label the node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node_name&gt; nvidia.com/gpu.deploy.operands=false <co xml:id="CO62-1"/></programlisting>
<calloutlist>
<callout arearefs="CO62-1">
<para>Replace <literal>&lt;node_name&gt;</literal> with the name of a node where you do not want to install the NVIDIA GPU operands.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the label was added to the node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Optional: If GPU operands were previously deployed on the node, verify their removal.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check the status of the pods in the <literal>nvidia-gpu-operator</literal> namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n nvidia-gpu-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             READY   STATUS        RESTARTS   AGE
gpu-operator-59469b8c5c-hw9wj    1/1     Running       0          8d
nvidia-sandbox-validator-7hx98   1/1     Running       0          8d
nvidia-sandbox-validator-hdb7p   1/1     Running       0          8d
nvidia-sandbox-validator-kxwj7   1/1     Terminating   0          9d
nvidia-vfio-manager-7w9fs        1/1     Running       0          8d
nvidia-vfio-manager-866pz        1/1     Running       0          8d
nvidia-vfio-manager-zqtck        1/1     Terminating   0          9d</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the pod status until the pods with <literal>Terminating</literal> status are removed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n nvidia-gpu-operator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             READY   STATUS    RESTARTS   AGE
gpu-operator-59469b8c5c-hw9wj    1/1     Running   0          8d
nvidia-sandbox-validator-7hx98   1/1     Running   0          8d
nvidia-sandbox-validator-hdb7p   1/1     Running   0          8d
nvidia-vfio-manager-7w9fs        1/1     Running   0          8d
nvidia-vfio-manager-866pz        1/1     Running   0          8d</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-preparing-host-devices-for-pci-passthrough">
<title>Preparing host devices for PCI passthrough</title>
<section xml:id="virt-about_pci-passthrough_virt-configuring-pci-passthrough">
<title>About preparing a host device for PCI passthrough</title>
<simpara>To prepare a host device for PCI passthrough by using the CLI, create a <literal>MachineConfig</literal> object and add kernel arguments to enable the Input-Output Memory Management Unit (IOMMU). Bind the PCI device to the Virtual Function I/O (VFIO) driver and then expose it in the cluster by editing the <literal>permittedHostDevices</literal> field of the <literal>HyperConverged</literal> custom resource (CR). The <literal>permittedHostDevices</literal> list is empty when you first install the OpenShift Virtualization Operator.</simpara>
<simpara>To remove a PCI host device from the cluster by using the CLI, delete the PCI device information from the <literal>HyperConverged</literal> CR.</simpara>
</section>
<section xml:id="virt-adding-kernel-arguments-enable-IOMMU_virt-configuring-pci-passthrough">
<title>Adding kernel arguments to enable the IOMMU driver</title>
<simpara>To enable the IOMMU driver in the kernel, create the <literal>MachineConfig</literal> object and add the kernel arguments.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have cluster administrator permissions.</simpara>
</listitem>
<listitem>
<simpara>Your CPU hardware is Intel or AMD.</simpara>
</listitem>
<listitem>
<simpara>You enabled Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>MachineConfig</literal> object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker <co xml:id="CO63-1"/>
  name: 100-worker-iommu <co xml:id="CO63-2"/>
spec:
  config:
    ignition:
      version: 3.2.0
  kernelArguments:
      - intel_iommu=on <co xml:id="CO63-3"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO63-1">
<para>Applies the new kernel argument only to worker nodes.</para>
</callout>
<callout arearefs="CO63-2">
<para>The <literal>name</literal> indicates the ranking of this kernel argument (100) among the machine configs and its purpose. If you have an AMD CPU, specify the kernel argument as <literal>amd_iommu=on</literal>.</para>
</callout>
<callout arearefs="CO63-3">
<para>Identifies the kernel argument as <literal>intel_iommu</literal> for an Intel CPU.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the new <literal>MachineConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f 100-worker-kernel-arg-iommu.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the new <literal>MachineConfig</literal> object was added.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get MachineConfig</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-binding-devices-vfio-driver_virt-configuring-pci-passthrough">
<title>Binding PCI devices to the VFIO driver</title>
<simpara>To bind PCI devices to the VFIO (Virtual Function I/O) driver, obtain the values for <literal>vendor-ID</literal> and <literal>device-ID</literal> from each device and create a list with the values. Add this list to the <literal>MachineConfig</literal> object. The <literal>MachineConfig</literal> Operator generates the <literal>/etc/modprobe.d/vfio.conf</literal> on the nodes with the PCI devices, and binds the PCI devices to the VFIO driver.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You added kernel arguments to enable IOMMU for the CPU.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run the <literal>lspci</literal> command to obtain the <literal>vendor-ID</literal> and the <literal>device-ID</literal> for the PCI device.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lspci -nnv | grep -i nvidia</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">02:01.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:1eb8] (rev a1)</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a Butane config file, <literal>100-worker-vfiopci.bu</literal>, binding the PCI device to the VFIO driver.</simpara>
<note>
<simpara>See "Creating machine configs with Butane" for information about Butane.</simpara>
</note>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">variant: openshift
version: 4.14.0
metadata:
  name: 100-worker-vfiopci
  labels:
    machineconfiguration.openshift.io/role: worker <co xml:id="CO64-1"/>
storage:
  files:
  - path: /etc/modprobe.d/vfio.conf
    mode: 0644
    overwrite: true
    contents:
      inline: |
        options vfio-pci ids=10de:1eb8 <co xml:id="CO64-2"/>
  - path: /etc/modules-load.d/vfio-pci.conf <co xml:id="CO64-3"/>
    mode: 0644
    overwrite: true
    contents:
      inline: vfio-pci</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO64-1">
<para>Applies the new kernel argument only to worker nodes.</para>
</callout>
<callout arearefs="CO64-2">
<para>Specify the previously determined <literal>vendor-ID</literal> value (<literal>10de</literal>) and the <literal>device-ID</literal> value (<literal>1eb8</literal>) to bind a single device to the VFIO driver. You can add a list of multiple devices with their vendor and device information.</para>
</callout>
<callout arearefs="CO64-3">
<para>The file that loads the vfio-pci kernel module on the worker nodes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Use Butane to generate a <literal>MachineConfig</literal> object file, <literal>100-worker-vfiopci.yaml</literal>, containing the configuration to be delivered to the worker nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ butane 100-worker-vfiopci.bu -o 100-worker-vfiopci.yaml</programlisting>
</listitem>
<listitem>
<simpara>Apply the <literal>MachineConfig</literal> object to the worker nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f 100-worker-vfiopci.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>MachineConfig</literal> object was added.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get MachineConfig</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                             GENERATEDBYCONTROLLER                      IGNITIONVERSION  AGE
00-master                        d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h
00-worker                        d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h
01-master-container-runtime      d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h
01-master-kubelet                d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h
01-worker-container-runtime      d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h
01-worker-kubelet                d3da910bfa9f4b599af4ed7f5ac270d55950a3a1   3.2.0            25h
100-worker-iommu                                                            3.2.0            30s
100-worker-vfiopci-configuration                                            3.2.0            30s</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the VFIO driver is loaded.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lspci -nnk -d 10de:</programlisting>
<simpara>The output confirms that the VFIO driver is being used.</simpara>
<formalpara>
<title>Example output</title>
<para>
<screen>04:00.0 3D controller [0302]: NVIDIA Corporation GP102GL [Tesla P40] [10de:1eb8] (rev a1)
        Subsystem: NVIDIA Corporation Device [10de:1eb8]
        Kernel driver in use: vfio-pci
        Kernel modules: nouveau</screen>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-exposing-pci-device-in-cluster-cli_virt-configuring-pci-passthrough">
<title>Exposing PCI host devices in the cluster using the CLI</title>
<simpara>To expose PCI host devices in the cluster, add details about the PCI devices to the <literal>spec.permittedHostDevices.pciHostDevices</literal> array of the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Add the PCI device information to the <literal>spec.permittedHostDevices.pciHostDevices</literal> array. For example:</simpara>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  permittedHostDevices: <co xml:id="CO65-1"/>
    pciHostDevices: <co xml:id="CO65-2"/>
    - pciDeviceSelector: "10DE:1DB6" <co xml:id="CO65-3"/>
      resourceName: "nvidia.com/GV100GL_Tesla_V100" <co xml:id="CO65-4"/>
    - pciDeviceSelector: "10DE:1EB8"
      resourceName: "nvidia.com/TU104GL_Tesla_T4"
    - pciDeviceSelector: "8086:6F54"
      resourceName: "intel.com/qat"
      externalResourceProvider: true <co xml:id="CO65-5"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO65-1">
<para>The host devices that are permitted to be used in the cluster.</para>
</callout>
<callout arearefs="CO65-2">
<para>The list of PCI devices available on the node.</para>
</callout>
<callout arearefs="CO65-3">
<para>The <literal>vendor-ID</literal> and the <literal>device-ID</literal> required to identify the PCI device.</para>
</callout>
<callout arearefs="CO65-4">
<para>The name of a PCI host device.</para>
</callout>
<callout arearefs="CO65-5">
<para>Optional: Setting this field to <literal>true</literal> indicates that the resource is provided by an external device plugin. OpenShift Virtualization allows the usage of this device in the cluster but leaves the allocation and monitoring to an external device plugin.</para>
</callout>
</calloutlist>
<note>
<simpara>The above example snippet shows two PCI host devices that are named <literal>nvidia.com/GV100GL_Tesla_V100</literal> and <literal>nvidia.com/TU104GL_Tesla_T4</literal> added to the list of permitted host devices in the <literal>HyperConverged</literal> CR. These devices have been tested and verified to work with OpenShift Virtualization.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the PCI host devices were added to the node by running the following command. The example output shows that there is one device each associated with the <literal>nvidia.com/GV100GL_Tesla_V100</literal>, <literal>nvidia.com/TU104GL_Tesla_T4</literal>, and <literal>intel.com/qat</literal> resource names.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Capacity:
  cpu:                            64
  devices.kubevirt.io/kvm:        110
  devices.kubevirt.io/tun:        110
  devices.kubevirt.io/vhost-net:  110
  ephemeral-storage:              915128Mi
  hugepages-1Gi:                  0
  hugepages-2Mi:                  0
  memory:                         131395264Ki
  nvidia.com/GV100GL_Tesla_V100   1
  nvidia.com/TU104GL_Tesla_T4     1
  intel.com/qat:                  1
  pods:                           250
Allocatable:
  cpu:                            63500m
  devices.kubevirt.io/kvm:        110
  devices.kubevirt.io/tun:        110
  devices.kubevirt.io/vhost-net:  110
  ephemeral-storage:              863623130526
  hugepages-1Gi:                  0
  hugepages-2Mi:                  0
  memory:                         130244288Ki
  nvidia.com/GV100GL_Tesla_V100   1
  nvidia.com/TU104GL_Tesla_T4     1
  intel.com/qat:                  1
  pods:                           250</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-removing-pci-device-from-cluster_virt-configuring-pci-passthrough">
<title>Removing PCI host devices from the cluster using the CLI</title>
<simpara>To remove a PCI host device from the cluster, delete the information for that device from the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Remove the PCI device information from the <literal>spec.permittedHostDevices.pciHostDevices</literal> array by deleting the <literal>pciDeviceSelector</literal>, <literal>resourceName</literal> and <literal>externalResourceProvider</literal> (if applicable) fields for the appropriate device. In this example, the <literal>intel.com/qat</literal> resource has been deleted.</simpara>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  permittedHostDevices:
    pciHostDevices:
    - pciDeviceSelector: "10DE:1DB6"
      resourceName: "nvidia.com/GV100GL_Tesla_V100"
    - pciDeviceSelector: "10DE:1EB8"
      resourceName: "nvidia.com/TU104GL_Tesla_T4"
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the PCI host device was removed from the node by running the following command. The example output shows that there are zero devices associated with the <literal>intel.com/qat</literal> resource name.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Capacity:
  cpu:                            64
  devices.kubevirt.io/kvm:        110
  devices.kubevirt.io/tun:        110
  devices.kubevirt.io/vhost-net:  110
  ephemeral-storage:              915128Mi
  hugepages-1Gi:                  0
  hugepages-2Mi:                  0
  memory:                         131395264Ki
  nvidia.com/GV100GL_Tesla_V100   1
  nvidia.com/TU104GL_Tesla_T4     1
  intel.com/qat:                  0
  pods:                           250
Allocatable:
  cpu:                            63500m
  devices.kubevirt.io/kvm:        110
  devices.kubevirt.io/tun:        110
  devices.kubevirt.io/vhost-net:  110
  ephemeral-storage:              863623130526
  hugepages-1Gi:                  0
  hugepages-2Mi:                  0
  memory:                         130244288Ki
  nvidia.com/GV100GL_Tesla_V100   1
  nvidia.com/TU104GL_Tesla_T4     1
  intel.com/qat:                  0
  pods:                           250</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-vms-for-pci-passthrough">
<title>Configuring virtual machines for PCI passthrough</title>
<simpara>After the PCI devices have been added to the cluster, you can assign them to virtual machines. The PCI devices are now available as if they are physically connected to the virtual machines.</simpara>
<section xml:id="virt-assigning-pci-device-virtual-machine_virt-configuring-pci-passthrough">
<title>Assigning a PCI device to a virtual machine</title>
<simpara>When a PCI device is available in a cluster, you can assign it to a virtual machine and enable PCI passthrough.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Assign the PCI device to a virtual machine as a host device.</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  domain:
    devices:
      hostDevices:
      - deviceName: nvidia.com/TU104GL_Tesla_T4 <co xml:id="CO66-1"/>
        name: hostdevices1</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO66-1">
<para>The name of the PCI device that is permitted on the cluster as a host device. The virtual machine can access this host device.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Use the following command to verify that the host device is available from the virtual machine.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lspci -nnk | grep NVIDIA</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ 02:01.0 3D controller [0302]: NVIDIA Corporation GV100GL [Tesla V100 PCIe 32GB] [10de:1eb8] (rev a1)</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_configuring-pci-passthrough" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-troubleshooting-enabling_intel_vt_x_and_amd_v_virtualization_hardware_extensions_in_bios">Enabling Intel VT-X and AMD-V Virtualization Hardware Extensions in BIOS</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_basic_system_settings/assembly_managing-file-permissions_configuring-basic-system-settings">Managing file permissions</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/postinstallation_configuration/#post-install-machine-configuration-tasks">Postinstallation machine configuration tasks</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-virtual-gpus">
<title>Configuring virtual GPUs</title>

<simpara>If you have graphics processing unit (GPU) cards, OpenShift Virtualization can automatically create virtual GPUs (vGPUs) that you can assign to virtual machines (VMs).</simpara>
<section xml:id="virt-about-using-virtual-gpus_virt-configuring-virtual-gpus">
<title>About using virtual GPUs with OpenShift Virtualization</title>
<simpara>Some graphics processing unit (GPU) cards support the creation of virtual GPUs (vGPUs). OpenShift Virtualization can automatically create vGPUs and other mediated devices if an administrator provides configuration details in the <literal>HyperConverged</literal> custom resource (CR). This automation is especially useful for large clusters.</simpara>
<note>
<simpara>Refer to your hardware vendor&#8217;s documentation for functionality and support details.</simpara>
</note>
<variablelist>
<varlistentry>
<term>Mediated device</term>
<listitem>
<simpara>A physical device that is divided into one or more virtual devices. A vGPU is a type of mediated device (mdev); the performance of the physical GPU is divided among the virtual devices. You can assign mediated devices to one or more virtual machines (VMs), but the number of guests must be compatible with your GPU. Some GPUs do not support multiple guests.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="preparing-hosts-mdevs_virt-configuring-virtual-gpus">
<title>Preparing hosts for mediated devices</title>
<simpara>You must enable the Input-Output Memory Management Unit (IOMMU) driver before you can configure mediated devices.</simpara>
<section xml:id="virt-adding-kernel-arguments-enable-IOMMU_virt-configuring-virtual-gpus">
<title>Adding kernel arguments to enable the IOMMU driver</title>
<simpara>To enable the IOMMU driver in the kernel, create the <literal>MachineConfig</literal> object and add the kernel arguments.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have cluster administrator permissions.</simpara>
</listitem>
<listitem>
<simpara>Your CPU hardware is Intel or AMD.</simpara>
</listitem>
<listitem>
<simpara>You enabled Intel Virtualization Technology for Directed I/O extensions or AMD IOMMU in the BIOS.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>MachineConfig</literal> object that identifies the kernel argument. The following example shows a kernel argument for an Intel CPU.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker <co xml:id="CO67-1"/>
  name: 100-worker-iommu <co xml:id="CO67-2"/>
spec:
  config:
    ignition:
      version: 3.2.0
  kernelArguments:
      - intel_iommu=on <co xml:id="CO67-3"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO67-1">
<para>Applies the new kernel argument only to worker nodes.</para>
</callout>
<callout arearefs="CO67-2">
<para>The <literal>name</literal> indicates the ranking of this kernel argument (100) among the machine configs and its purpose. If you have an AMD CPU, specify the kernel argument as <literal>amd_iommu=on</literal>.</para>
</callout>
<callout arearefs="CO67-3">
<para>Identifies the kernel argument as <literal>intel_iommu</literal> for an Intel CPU.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the new <literal>MachineConfig</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f 100-worker-kernel-arg-iommu.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the new <literal>MachineConfig</literal> object was added.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get MachineConfig</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-nvidia-gpu-operator_virt-configuring-virtual-gpus">
<title>Configuring the NVIDIA GPU Operator</title>
<simpara>You can use the NVIDIA GPU Operator to provision worker nodes for running GPU-accelerated virtual machines (VMs) in OpenShift Virtualization.</simpara>
<note>
<simpara>The NVIDIA GPU Operator is supported only by NVIDIA. For more information, see <link xlink:href="https://access.redhat.com/solutions/5174941">Obtaining Support from NVIDIA</link> in the Red Hat Knowledgebase.</simpara>
</note>
<section xml:id="about-using-nvidia-gpu_virt-configuring-virtual-gpus">
<title>About using the NVIDIA GPU Operator</title>
<simpara>You can use the NVIDIA GPU Operator with OpenShift Virtualization to rapidly provision worker nodes for running GPU-enabled virtual machines (VMs). The NVIDIA GPU Operator manages NVIDIA GPU resources in an OpenShift Container Platform cluster and automates tasks that are required when preparing nodes for GPU workloads.</simpara>
<simpara>Before you can deploy application workloads to a GPU resource, you must install components such as the NVIDIA drivers that enable the compute unified device architecture (CUDA), Kubernetes device plugin, container runtime, and other features, such as automatic node labeling and monitoring. By automating these tasks, you can quickly scale the GPU capacity of your infrastructure. The NVIDIA GPU Operator can especially facilitate provisioning complex artificial intelligence and machine learning (AI/ML) workloads.</simpara>
</section>
<section xml:id="virt-options-configuring-mdevs_virt-configuring-virtual-gpus">
<title>Options for configuring mediated devices</title>
<simpara>There are two available methods for configuring mediated devices when using the NVIDIA GPU Operator. The method that Red Hat tests uses OpenShift Virtualization features to schedule mediated devices, while the NVIDIA method only uses the GPU Operator.</simpara>
<variablelist>
<varlistentry>
<term>Using the NVIDIA GPU Operator to configure mediated devices</term>
<listitem>
<simpara>This method exclusively uses the NVIDIA GPU Operator to configure mediated devices. To use this method, refer to <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/openshift-virtualization.html">NVIDIA GPU Operator with OpenShift Virtualization</link> in the NVIDIA documentation.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Using OpenShift Virtualization to configure mediated devices</term>
<listitem>
<simpara>This method, which is tested by Red Hat, uses OpenShift Virtualization&#8217;s capabilities to configure mediated devices. In this case, the NVIDIA GPU Operator is only used for installing drivers with the NVIDIA vGPU Manager. The GPU Operator does not configure mediated devices.</simpara>
<simpara>When using the OpenShift Virtualization method, you still configure the GPU Operator by following <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/openshift-virtualization.html">the NVIDIA documentation</link>. However, this method differs from the NVIDIA documentation in the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara>You must not overwrite the default <literal>disableMDEVConfiguration: false</literal> setting in the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<important>
<simpara>Setting this feature gate as described in the <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/openshift-virtualization.html#prerequisites">NVIDIA documentation</link> prevents OpenShift Virtualization from configuring mediated devices.</simpara>
</important>
</listitem>
<listitem>
<simpara>You must configure your <literal>ClusterPolicy</literal> manifest so that it matches the following example:</simpara>
<formalpara>
<title>Example manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: ClusterPolicy
apiVersion: nvidia.com/v1
metadata:
  name: gpu-cluster-policy
spec:
  operator:
    defaultRuntime: crio
    use_ocp_driver_toolkit: true
    initContainer: {}
  sandboxWorkloads:
    enabled: true
    defaultWorkload: vm-vgpu
  driver:
    enabled: false <co xml:id="CO68-1"/>
  dcgmExporter: {}
  dcgm:
    enabled: true
  daemonsets: {}
  devicePlugin: {}
  gfd: {}
  migManager:
    enabled: true
  nodeStatusExporter:
    enabled: true
  mig:
    strategy: single
  toolkit:
    enabled: true
  validator:
    plugin:
      env:
        - name: WITH_WORKLOAD
          value: "true"
  vgpuManager:
    enabled: true <co xml:id="CO68-2"/>
    repository: &lt;vgpu_container_registry&gt; <co xml:id="CO68-3"/>
    image: &lt;vgpu_image_name&gt;
    version: nvidia-vgpu-manager
  vgpuDeviceManager:
    enabled: false <co xml:id="CO68-4"/>
    config:
      name: vgpu-devices-config
      default: default
  sandboxDevicePlugin:
    enabled: false <co xml:id="CO68-5"/>
  vfioManager:
    enabled: false <co xml:id="CO68-6"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO68-1">
<para>Set this value to <literal>false</literal>. Not required for VMs.</para>
</callout>
<callout arearefs="CO68-2">
<para>Set this value to <literal>true</literal>. Required for using vGPUs with VMs.</para>
</callout>
<callout arearefs="CO68-3">
<para>Substitute <literal>&lt;vgpu_container_registry&gt;</literal> with your registry value.</para>
</callout>
<callout arearefs="CO68-4">
<para>Set this value to <literal>false</literal> to allow OpenShift Virtualization to configure mediated devices instead of the NVIDIA GPU Operator.</para>
</callout>
<callout arearefs="CO68-5">
<para>Set this value to <literal>false</literal> to prevent discovery and advertising of the vGPU devices to the kubelet.</para>
</callout>
<callout arearefs="CO68-6">
<para>Set this value to <literal>false</literal> to prevent loading the <literal>vfio-pci</literal> driver. Instead, follow the OpenShift Virtualization documentation to configure PCI passthrough.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link linkend="virt-configuring-pci-passthrough">Configuring PCI passthrough</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="how-vgpus-are-assigned-to-nodes_virt-configuring-virtual-gpus">
<title>How vGPUs are assigned to nodes</title>
<simpara>For each physical device, OpenShift Virtualization configures the following values:</simpara>
<itemizedlist>
<listitem>
<simpara>A single mdev type.</simpara>
</listitem>
<listitem>
<simpara>The maximum number of instances of the selected <literal>mdev</literal> type.</simpara>
</listitem>
</itemizedlist>
<simpara>The cluster architecture affects how devices are created and assigned to nodes.</simpara>
<variablelist>
<varlistentry>
<term>Large cluster with multiple cards per node</term>
<listitem>
<simpara>On nodes with multiple cards that can support similar vGPU types, the relevant device types are created in a round-robin manner.
For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
mediatedDevicesConfiguration:
  mediatedDeviceTypes:
  - nvidia-222
  - nvidia-228
  - nvidia-105
  - nvidia-108
# ...</programlisting>
<simpara>In this scenario, each node has two cards, both of which support the following vGPU types:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">nvidia-105
# ...
nvidia-108
nvidia-217
nvidia-299
# ...</programlisting>
<simpara>On each node, OpenShift Virtualization creates the following vGPUs:</simpara>
<itemizedlist>
<listitem>
<simpara>16 vGPUs of type nvidia-105 on the first card.</simpara>
</listitem>
<listitem>
<simpara>2 vGPUs of type nvidia-108 on the second card.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>One node has a single card that supports more than one requested vGPU type</term>
<listitem>
<simpara>OpenShift Virtualization uses the supported type that comes first on the <literal>mediatedDeviceTypes</literal> list.</simpara>
<simpara>For example, the card on a node card supports <literal>nvidia-223</literal> and <literal>nvidia-224</literal>. The following <literal>mediatedDeviceTypes</literal> list is configured:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
mediatedDevicesConfiguration:
  mediatedDeviceTypes:
  - nvidia-22
  - nvidia-223
  - nvidia-224
# ...</programlisting>
<simpara>In this example, OpenShift Virtualization uses the <literal>nvidia-223</literal> type.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="managing-mediated-devices_virt-configuring-virtual-gpus">
<title>Managing mediated devices</title>
<simpara>Before you can assign mediated devices to virtual machines, you must create the devices and expose them to the cluster. You can also reconfigure and remove mediated devices.</simpara>
<section xml:id="virt-creating-exposing-mediated-devices_virt-configuring-virtual-gpus">
<title>Creating and exposing mediated devices</title>
<simpara>As an administrator, you can create mediated devices and expose them to the cluster by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You enabled the Input-Output Memory Management Unit (IOMMU) driver.</simpara>
</listitem>
<listitem>
<simpara>If your hardware vendor provides drivers, you installed them on the nodes where you want to create mediated devices.</simpara>
<itemizedlist>
<listitem>
<simpara>If you use NVIDIA cards, you <link xlink:href="https://docs.nvidia.com/datacenter/cloud-native/openshift/latest/openshift-virtualization.html">installed the NVIDIA GRID driver</link>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
<example>
<title>Example configuration file with mediated devices configured</title>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  mediatedDevicesConfiguration:
    mediatedDeviceTypes:
    - nvidia-231
    nodeMediatedDeviceTypes:
    - mediatedDeviceTypes:
      - nvidia-233
      nodeSelector:
        kubernetes.io/hostname: node-11.redhat.com
  permittedHostDevices:
    mediatedDevices:
    - mdevNameSelector: GRID T4-2Q
      resourceName: nvidia.com/GRID_T4-2Q
    - mdevNameSelector: GRID T4-8Q
      resourceName: nvidia.com/GRID_T4-8Q
# ...</programlisting>
</example>
</listitem>
<listitem>
<simpara>Create mediated devices by adding them to the <literal>spec.mediatedDevicesConfiguration</literal> stanza:</simpara>
<formalpara>
<title>Example YAML snippet</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  mediatedDevicesConfiguration:
    mediatedDeviceTypes: <co xml:id="CO69-1"/>
    - &lt;device_type&gt;
    nodeMediatedDeviceTypes: <co xml:id="CO69-2"/>
    - mediatedDeviceTypes: <co xml:id="CO69-3"/>
      - &lt;device_type&gt;
      nodeSelector: <co xml:id="CO69-4"/>
        &lt;node_selector_key&gt;: &lt;node_selector_value&gt;
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO69-1">
<para>Required: Configures global settings for the cluster.</para>
</callout>
<callout arearefs="CO69-2">
<para>Optional: Overrides the global configuration for a specific node or group of nodes. Must be used with the global <literal>mediatedDeviceTypes</literal> configuration.</para>
</callout>
<callout arearefs="CO69-3">
<para>Required if you use <literal>nodeMediatedDeviceTypes</literal>. Overrides the global <literal>mediatedDeviceTypes</literal> configuration for the specified nodes.</para>
</callout>
<callout arearefs="CO69-4">
<para>Required if you use <literal>nodeMediatedDeviceTypes</literal>. Must include a <literal>key:value</literal> pair.</para>
</callout>
</calloutlist>
<important>
<simpara>Before OpenShift Virtualization 4.14, the <literal>mediatedDeviceTypes</literal> field was named <literal>mediatedDevicesTypes</literal>. Ensure that you use the correct field name when configuring mediated devices.</simpara>
</important>
</listitem>
<listitem>
<simpara>Identify the name selector and resource name values for the devices that you want to expose to the cluster. You will add these values to the <literal>HyperConverged</literal> CR in the next step.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Find the <literal>resourceName</literal> value by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get $NODE -o json \
  | jq '.status.allocatable \
    | with_entries(select(.key | startswith("nvidia.com/"))) \
    | with_entries(select(.value != "0"))'</programlisting>
</listitem>
<listitem>
<simpara>Find the <literal>mdevNameSelector</literal> value by viewing the contents of <literal>/sys/bus/pci/devices/&lt;slot&gt;:&lt;bus&gt;:&lt;domain&gt;.&lt;function&gt;/mdev_supported_types/&lt;type&gt;/name</literal>, substituting the correct values for your system.</simpara>
<simpara>For example, the name file for the <literal>nvidia-231</literal> type contains the selector string <literal>GRID T4-2Q</literal>. Using <literal>GRID T4-2Q</literal> as the <literal>mdevNameSelector</literal> value allows nodes to use the <literal>nvidia-231</literal> type.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Expose the mediated devices to the cluster by adding the <literal>mdevNameSelector</literal> and <literal>resourceName</literal> values to the
<literal>spec.permittedHostDevices.mediatedDevices</literal> stanza of the <literal>HyperConverged</literal> CR:</simpara>
<formalpara>
<title>Example YAML snippet</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
  permittedHostDevices:
    mediatedDevices:
    - mdevNameSelector: GRID T4-2Q <co xml:id="CO70-1"/>
      resourceName: nvidia.com/GRID_T4-2Q <co xml:id="CO70-2"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO70-1">
<para>Exposes the mediated devices that map to this value on the host.</para>
</callout>
<callout arearefs="CO70-2">
<para>Matches the resource name that is allocated on the node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Optional: Confirm that a device was added to a specific node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe node &lt;node_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-changing-removing-mediated-devices_virt-configuring-virtual-gpus">
<title>About changing and removing mediated devices</title>
<simpara>You can reconfigure or remove mediated devices in several ways:</simpara>
<itemizedlist>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR and change the contents of the <literal>mediatedDeviceTypes</literal> stanza.</simpara>
</listitem>
<listitem>
<simpara>Change the node labels that match the <literal>nodeMediatedDeviceTypes</literal> node selector.</simpara>
</listitem>
<listitem>
<simpara>Remove the device information from the <literal>spec.mediatedDevicesConfiguration</literal> and <literal>spec.permittedHostDevices</literal> stanzas of the <literal>HyperConverged</literal> CR.</simpara>
<note>
<simpara>If you remove the device information from the <literal>spec.permittedHostDevices</literal> stanza without also removing it from the <literal>spec.mediatedDevicesConfiguration</literal> stanza, you cannot create a new mediated device type on the same node. To properly remove mediated devices, remove the device information from both stanzas.</simpara>
</note>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-removing-mediated-device-from-cluster-cli_virt-configuring-virtual-gpus">
<title>Removing mediated devices from the cluster</title>
<simpara>To remove a mediated device from the cluster, delete the information for that device from the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Remove the device information from the <literal>spec.mediatedDevicesConfiguration</literal> and <literal>spec.permittedHostDevices</literal> stanzas of the <literal>HyperConverged</literal> CR. Removing both entries ensures that you can later create a new mediated device type on the same node. For example:</simpara>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  mediatedDevicesConfiguration:
    mediatedDeviceTypes: <co xml:id="CO71-1"/>
      - nvidia-231
  permittedHostDevices:
    mediatedDevices: <co xml:id="CO71-2"/>
    - mdevNameSelector: GRID T4-2Q
      resourceName: nvidia.com/GRID_T4-2Q</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO71-1">
<para>To remove the <literal>nvidia-231</literal> device type, delete it from the <literal>mediatedDeviceTypes</literal> array.</para>
</callout>
<callout arearefs="CO71-2">
<para>To remove the <literal>GRID T4-2Q</literal> device, delete the <literal>mdevNameSelector</literal> field and its corresponding <literal>resourceName</literal> field.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="using-mediated-devices_virt-configuring-virtual-gpus">
<title>Using mediated devices</title>
<simpara>You can assign mediated devices to one or more virtual machines.</simpara>
<section xml:id="virt-assigning-mdev-vm-cli_virt-configuring-virtual-gpus">
<title>Assigning a vGPU to a VM by using the CLI</title>
<simpara>Assign mediated devices such as virtual GPUs (vGPUs) to virtual machines (VMs).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The mediated device is configured in the <literal>HyperConverged</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>The VM is stopped.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Assign the mediated device to a virtual machine (VM) by editing the <literal>spec.domain.devices.gpus</literal> stanza of the <literal>VirtualMachine</literal> manifest:</simpara>
<formalpara>
<title>Example virtual machine manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  domain:
    devices:
      gpus:
      - deviceName: nvidia.com/TU104GL_Tesla_T4 <co xml:id="CO72-1"/>
        name: gpu1 <co xml:id="CO72-2"/>
      - deviceName: nvidia.com/GRID_T4-2Q
        name: gpu2</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO72-1">
<para>The resource name associated with the mediated device.</para>
</callout>
<callout arearefs="CO72-2">
<para>A name to identify the device on the VM.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To verify that the device is available from the virtual machine, run the following command, substituting <literal>&lt;device_name&gt;</literal> with the <literal>deviceName</literal> value from the <literal>VirtualMachine</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lspci -nnk | grep &lt;device_name&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-assigning-vgpu-vm-web_virt-configuring-virtual-gpus">
<title>Assigning a vGPU to a VM by using the web console</title>
<simpara>You can assign virtual GPUs to virtual machines by using the OpenShift Container Platform web console.</simpara>
<note>
<simpara>You can add hardware devices to virtual machines created from customized templates or a YAML file. You cannot add devices to pre-supplied boot source templates for specific operating systems.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The vGPU is configured as a mediated device in your cluster.</simpara>
<itemizedlist>
<listitem>
<simpara>To view the devices that are connected to your cluster, click <emphasis role="strong">Compute</emphasis> &#8594; <emphasis role="strong">Hardware Devices</emphasis> from the side menu.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>The VM is stopped.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select the VM that you want to assign the device to.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Details</emphasis> tab, click <emphasis role="strong">GPU devices</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add GPU device</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter an identifying value in the <emphasis role="strong">Name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>From the <emphasis role="strong">Device name</emphasis> list, select the device that you want to add to the VM.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To confirm that the devices were added to the VM, click the <emphasis role="strong">YAML</emphasis> tab and review the <literal>VirtualMachine</literal> configuration. Mediated devices are added to the <literal>spec.domain.devices</literal> stanza.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_virt-configuring-virtual-gpus" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/virtualization_deployment_and_administration_guide/sect-troubleshooting-enabling_intel_vt_x_and_amd_v_virtualization_hardware_extensions_in_bios">Enabling Intel VT-X and AMD-V Virtualization Hardware Extensions in BIOS</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-enabling-descheduler-evictions">
<title>Enabling descheduler evictions on virtual machines</title>

<simpara>You can use the descheduler to evict pods so that the pods can be rescheduled onto more appropriate nodes. If the pod is a virtual machine, the pod eviction causes the virtual machine to be live migrated to another node.</simpara>
<important>
<simpara>Descheduler eviction for virtual machines is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="nodes-descheduler-profiles_virt-enabling-descheduler-evictions">
<title>Descheduler profiles</title>
<simpara>Use the Technology Preview <literal>DevPreviewLongLifecycle</literal> profile to enable the descheduler on a virtual machine. This is the only descheduler profile currently available for OpenShift Virtualization. To ensure proper scheduling, create VMs with CPU and memory requests for the expected load.</simpara>
<variablelist>
<varlistentry>
<term><literal>DevPreviewLongLifecycle</literal></term>
<listitem>
<simpara>This profile balances resource usage between nodes and enables the following strategies:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>RemovePodsHavingTooManyRestarts</literal>: removes pods whose containers have been restarted too many times and pods where the sum of restarts over all containers (including Init Containers) is more than 100. Restarting the VM guest operating system does not increase this count.</simpara>
</listitem>
<listitem>
<simpara><literal>LowNodeUtilization</literal>: evicts pods from overutilized nodes when there are any underutilized nodes. The destination node for the evicted pod will be determined by the scheduler.</simpara>
<itemizedlist>
<listitem>
<simpara>A node is considered underutilized if its usage is below 20% for all thresholds (CPU, memory, and number of pods).</simpara>
</listitem>
<listitem>
<simpara>A node is considered overutilized if its usage is above 50% for any of the thresholds (CPU, memory, and number of pods).</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="nodes-descheduler-installing_virt-enabling-descheduler-evictions">
<title>Installing the descheduler</title>
<simpara>The descheduler is not available by default. To enable the descheduler, you must install the Kube Descheduler Operator from OperatorHub and enable one or more descheduler profiles.</simpara>
<simpara>By default, the descheduler runs in predictive mode, which means that it only simulates pod evictions. You must change the mode to automatic for the descheduler to perform the pod evictions.</simpara>
<important>
<simpara>If you have enabled hosted control planes in your cluster, set a custom priority threshold to lower the chance that pods in the hosted control plane namespaces are evicted. Set the priority threshold class name to <literal>hypershift-control-plane</literal>, because it has the lowest priority value (<literal>100000000</literal>) of the hosted control plane priority classes.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You are logged in to OpenShift Container Platform as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Access to the OpenShift Container Platform web console.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the OpenShift Container Platform web console.</simpara>
</listitem>
<listitem>
<simpara>Create the required namespace for the Kube Descheduler Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to <emphasis role="strong">Administration</emphasis> &#8594; <emphasis role="strong">Namespaces</emphasis> and click <emphasis role="strong">Create Namespace</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter <literal>openshift-kube-descheduler-operator</literal> in the <emphasis role="strong">Name</emphasis> field, enter <literal>openshift.io/cluster-monitoring=true</literal> in the <emphasis role="strong">Labels</emphasis> field to enable descheduler metrics, and click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Install the Kube Descheduler Operator.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Type <emphasis role="strong">Kube Descheduler Operator</emphasis> into the filter box.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Kube Descheduler Operator</emphasis> and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Install Operator</emphasis> page, select <emphasis role="strong">A specific namespace on the cluster</emphasis>. Select <emphasis role="strong">openshift-kube-descheduler-operator</emphasis> from the drop-down menu.</simpara>
</listitem>
<listitem>
<simpara>Adjust the values for the <emphasis role="strong">Update Channel</emphasis> and <emphasis role="strong">Approval Strategy</emphasis> to the desired values.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a descheduler instance.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>From the <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> page, click the <emphasis role="strong">Kube Descheduler Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">Kube Descheduler</emphasis> tab and click <emphasis role="strong">Create KubeDescheduler</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Edit the settings as necessary.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>To evict pods instead of simulating the evictions, change the <emphasis role="strong">Mode</emphasis> field to <emphasis role="strong">Automatic</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Expand the <emphasis role="strong">Profiles</emphasis> section and select <literal>DevPreviewLongLifecycle</literal>. The <literal>AffinityAndTaints</literal> profile is enabled by default.</simpara>
<important>
<simpara>The only profile currently available for OpenShift Virtualization is <literal>DevPreviewLongLifecycle</literal>.</simpara>
</important>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>You can also configure the profiles and settings for the descheduler later using the OpenShift CLI (<literal>oc</literal>).</simpara>
</section>
<section xml:id="virt-enabling-descheduler-evictions_virt-enabling-descheduler-evictions">
<title>Enabling descheduler evictions on a virtual machine (VM)</title>
<simpara>After the descheduler is installed, you can enable descheduler evictions on your VM by adding an annotation to the <literal>VirtualMachine</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the descheduler in the OpenShift Container Platform web console or OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Ensure that the VM is not running.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Before starting the VM, add the <literal>descheduler.alpha.kubernetes.io/evict</literal> annotation to the <literal>VirtualMachine</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  template:
    metadata:
      annotations:
        descheduler.alpha.kubernetes.io/evict: "true"</programlisting>
</listitem>
<listitem>
<simpara>If you did not already set the <literal>DevPreviewLongLifecycle</literal> profile in the web console during installation, specify the <literal>DevPreviewLongLifecycle</literal> in the <literal>spec.profile</literal> section of the <literal>KubeDescheduler</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1
kind: KubeDescheduler
metadata:
  name: cluster
  namespace: openshift-kube-descheduler-operator
spec:
  deschedulingIntervalSeconds: 3600
  profiles:
  - DevPreviewLongLifecycle
  mode: Predictive <co xml:id="CO73-1"/></programlisting>
<calloutlist>
<callout arearefs="CO73-1">
<para>By default, the descheduler does not evict pods. To evict pods, set <literal>mode</literal> to <literal>Automatic</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<simpara>The descheduler is now enabled on the VM.</simpara>
</section>
<section xml:id="additional-resources_enabling-descheduler-evictions" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-descheduler-about">Descheduler overview</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-high-availability-for-vms">
<title>About high availability for virtual machines</title>

<simpara>You can enable high availability for virtual machines (VMs) by manually deleting a failed node to trigger VM failover or by configuring remediating nodes.</simpara>
<formalpara>
<title>Manually deleting a failed node</title>
<para>If a node fails and machine health checks are not deployed on your cluster, virtual machines with <literal>runStrategy: Always</literal> configured are not automatically relocated to healthy nodes. To trigger VM failover, you must manually delete the <literal>Node</literal> object.</para>
</formalpara>
<simpara>See <link linkend="virt-triggering-vm-failover-resolving-failed-node">Deleting a failed node to trigger virtual machine failover</link>.</simpara>
<formalpara>
<title>Configuring remediating nodes</title>
<para>You can configure remediating nodes by installing the Self Node Remediation Operator from the OperatorHub and enabling machine health checks or node remediation checks.</para>
</formalpara>
<simpara>For more information on remediation, fencing, and maintaining nodes, see the <link xlink:href="https://access.redhat.com/documentation/en-us/workload_availability_for_red_hat_openshift/23.2/html-single/remediation_fencing_and_maintenance/index#about-remediation-fencing-maintenance">Workload Availability for Red Hat OpenShift</link> documentation.</simpara>
</section>
<section xml:id="virt-vm-control-plane-tuning">
<title>Virtual machine control plane tuning</title>

<simpara>OpenShift Virtualization offers the following tuning options at the control-plane level:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>highBurst</literal> profile, which uses fixed <literal>QPS</literal> and <literal>burst</literal> rates, to create hundreds of virtual machines (VMs) in one batch</simpara>
</listitem>
<listitem>
<simpara>Migration setting adjustment based on workload type</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-configuring-highburst-profile_virt-control-plane-tuning">
<title>Configuring a highBurst profile</title>
<simpara>Use the <literal>highBurst</literal> profile to create and maintain a large number of virtual machines (VMs) in one cluster.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Apply the following patch to enable the <literal>highBurst</literal> tuning policy profile:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type=json -p='[{"op": "add", "path": "/spec/tuningPolicy", \
  "value": "highBurst"}]'</programlisting>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Run the following command to verify the <literal>highBurst</literal> tuning policy profile is enabled:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubevirt.kubevirt.io/kubevirt-kubevirt-hyperconverged \
  -n openshift-cnv -o go-template --template='{{range $config, \
  $value := .spec.configuration}} {{if eq $config "apiConfiguration" \
  "webhookConfiguration" "controllerConfiguration" "handlerConfiguration"}} \
  {{"\n"}} {{$config}} = {{$value}} {{end}} {{end}} {{"\n"}}</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="_vm-disks">
<title>VM disks</title>
<section xml:id="virt-hot-plugging-virtual-disks">
<title>Hot-plugging VM disks</title>

<simpara>You can add or remove virtual disks without stopping your virtual machine (VM) or virtual machine instance (VMI).</simpara>
<simpara>Only data volumes and persistent volume claims (PVCs) can be hot plugged and hot-unplugged. You cannot hot plug or hot-unplug container disks.</simpara>
<simpara>A hot plugged disk remains to the VM even after reboot. You must detach the disk to remove it from the VM.</simpara>
<simpara>You can make a hot plugged disk persistent so that it is permanently mounted on the VM.</simpara>
<note>
<simpara>Each VM has a <literal>virtio-scsi</literal> controller so that hot plugged disks can use the <literal>scsi</literal> bus. The <literal>virtio-scsi</literal> controller overcomes the limitations of <literal>virtio</literal> while retaining its performance advantages. It is highly scalable and supports hot plugging over 4 million disks.</simpara>
<simpara>Regular <literal>virtio</literal> is not available for hot plugged disks because it is not scalable. Each <literal>virtio</literal> disk uses one of the limited PCI Express (PCIe) slots in the VM. PCIe slots are also used by other devices and must be reserved in advance. Therefore, slots might not be available on demand.</simpara>
</note>
<section xml:id="virt-hot-plugging-disks-ui_virt-hot-plugging-virtual-disks">
<title>Hot plugging and hot unplugging a disk by using the web console</title>
<simpara>You can hot plug a disk by attaching it to a virtual machine (VM) while the VM is running by using the OpenShift Container Platform web console.</simpara>
<simpara>The hot plugged disk remains attached to the VM until you unplug it.</simpara>
<simpara>You can make a hot plugged disk persistent so that it is permanently mounted on the VM.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have a data volume or persistent volume claim (PVC) available for hot plugging.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select a running VM to view its details.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">VirtualMachine details</emphasis> page, click <emphasis role="strong">Configuration</emphasis> &#8594; <emphasis role="strong">Disks</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Add a hot plugged disk:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click <emphasis role="strong">Add disk</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Add disk (hot plugged)</emphasis> window, select the disk from the <emphasis role="strong">Source</emphasis> list and click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: Unplug a hot plugged disk:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside the disk and select <emphasis role="strong">Detach</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Detach</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Optional: Make a hot plugged disk persistent:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside the disk and select <emphasis role="strong">Make persistent</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Reboot the VM to apply the change.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-hot-plugging-disk-cli_virt-hot-plugging-virtual-disks">
<title>Hot plugging and hot unplugging a disk by using the command line</title>
<simpara>You can hot plug and hot unplug a disk while a virtual machine (VM) is running by using the command line.</simpara>
<simpara>You can make a hot plugged disk persistent so that it is permanently mounted on the VM.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have at least one data volume or persistent volume claim (PVC) available for hot plugging.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Hot plug a disk by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl addvolume &lt;virtual-machine|virtual-machine-instance&gt; \
  --volume-name=&lt;datavolume|PVC&gt; \
  [--persist] [--serial=&lt;label-name&gt;]</programlisting>
<itemizedlist>
<listitem>
<simpara>Use the optional <literal>--persist</literal> flag to add the hot plugged disk to the virtual machine specification as a permanently mounted virtual disk. Stop, restart, or reboot the virtual machine to permanently mount the virtual disk. After specifying the <literal>--persist</literal> flag, you can no longer hot plug or hot unplug the virtual disk. The <literal>--persist</literal> flag applies to virtual machines, not virtual machine instances.</simpara>
</listitem>
<listitem>
<simpara>The optional <literal>--serial</literal> flag allows you to add an alphanumeric string label of your choice. This helps you to identify the hot plugged disk in a guest virtual machine. If you do not specify this option, the label defaults to the name of the hot plugged data volume or PVC.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Hot unplug a disk by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl removevolume &lt;virtual-machine|virtual-machine-instance&gt; \
  --volume-name=&lt;datavolume|PVC&gt;</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-expanding-vm-disks">
<title>Expanding virtual machine disks</title>

<simpara>You can increase the size of a virtual machine (VM) disk by expanding the persistent volume claim (PVC) of the disk.</simpara>
<simpara>If your storage provider does not support volume expansion, you can expand the available virtual storage of a VM by adding blank data volumes.</simpara>
<simpara>You cannot reduce the size of a VM disk.</simpara>
<section xml:id="virt-expanding-vm-disk-pvc_virt-expanding-vm-disks">
<title>Expanding a VM disk PVC</title>
<simpara>You can increase the size of a virtual machine (VM) disk by expanding the persistent volume claim (PVC) of the disk.</simpara>
<simpara>If the PVC uses the file system volume mode, the disk image file expands to the available size while reserving some space for file system overhead.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>PersistentVolumeClaim</literal> manifest of the VM disk that you want to expand:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit pvc &lt;pvc_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Update the disk size:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
   name: vm-disk-expand
spec:
  accessModes:
     - ReadWriteMany
  resources:
    requests:
       storage: 3Gi <co xml:id="CO74-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO74-1">
<para>Specify the new disk size.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources for volume expansion</title>
<listitem>
<simpara><link xlink:href="https://docs.microsoft.com/en-us/windows-server/storage/disk-management/extend-a-basic-volume">Extending a basic volume in Windows</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/29095">Extending an existing file system partition without destroying data in Red Hat Enterprise Linux</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/24770">Extending a logical volume and its file system online in Red Hat Enterprise Linux</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-expanding-storage-with-data-volumes_virt-expanding-vm-disks">
<title>Expanding available virtual storage by adding blank data volumes</title>
<simpara>You can expand the available storage of a virtual machine (VM) by adding blank data volumes.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have at least one persistent volume.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>DataVolume</literal> manifest as shown in the following example:</simpara>
<formalpara>
<title>Example <literal>DataVolume</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: blank-image-datavolume
spec:
  source:
    blank: {}
  storage:
    resources:
      requests:
        storage: &lt;2Gi&gt; <co xml:id="CO75-1"/>
  storageClassName: "&lt;storage_class&gt;" <co xml:id="CO75-2"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO75-1">
<para>Specify the amount of available space requested for the data volume.</para>
</callout>
<callout arearefs="CO75-2">
<para>Optional: If you do not specify a storage class, the default storage class is used.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the data volume by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;blank-image-datavolume&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources for data volumes</title>
<listitem>
<simpara><link linkend="virt-using-preallocation-for-datavolumes">Configuring preallocation mode for data volumes</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-managing-data-volume-annotations">Managing data volume annotations</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-shared-volumes-for-vms">
<title>Configuring shared volumes for virtual machines</title>

<simpara>You can configure shared disks to allow multiple virtual machines (VMs) to share the same underlying storage. A shared disk&#8217;s volume must be block mode.</simpara>
<simpara>You configure disk sharing by exposing the storage as either of these types:</simpara>
<itemizedlist>
<listitem>
<simpara>An ordinary virtual machine disk</simpara>
</listitem>
<listitem>
<simpara>A logical unit number (LUN) device with an iSCSi connection and raw device mapping, as required for Windows Failover Clustering for shared volumes</simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-configuring-vm-disk-sharingvirt-configuring-shared-volumes-for-vms">
<title>Configuring disk sharing by using virtual machine disks</title>
<simpara>You can configure block volumes so that multiple virtual machines (VMs) can share storage.</simpara>
<simpara>The application running on the guest operating system determines the storage option you must configure for the VM. A disk of type <literal>disk</literal> exposes the volume as an ordinary disk to the VM.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The volume access mode must be <literal>ReadWriteMany</literal> (RWX) if the VMs that are sharing disks are running on different nodes.</simpara>
<simpara>If the VMs that are sharing disks are running on the same node, <literal>ReadWriteOnce</literal> (RWO) volume access mode is sufficient.</simpara>
</listitem>
<listitem>
<simpara>The storage provider must support the required Container Storage Interface (CSI) driver.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>VirtualMachine</literal> manifest for your VM to set the required values, as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: &lt;vm_name&gt;
spec:
  template:
# ...
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: rootdisk
            disk1: disk_one <co xml:id="CO76-1"/>
          - disk:
              bus: virtio
            name: cloudinitdisk
            disk2: disk_two
            shareable: true <co xml:id="CO76-2"/>
          interfaces:
          - masquerade: {}
            name: default</programlisting>
<calloutlist>
<callout arearefs="CO76-1">
<para>Identifies a device as a disk.</para>
</callout>
<callout arearefs="CO76-2">
<para>Identifies a shared disk.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the <literal>VirtualMachine</literal> manifest file to apply your changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-configuring-disk-sharing-lun_virt-configuring-shared-volumes-for-vms">
<title>Configuring disk sharing by using LUN</title>
<simpara>You can configure a LUN-backed virtual machine disk to be shared among multiple virtual machines by enabling SCSI persistent reservation. Enabling the shared option allows you to use advanced SCSI commands, such as those required for a Windows failover clustering implementation, against the underlying storage. Any disk to be shared must be in block mode.</simpara>
<simpara>A disk of type <literal>LUN</literal> exposes the volume as a LUN device to the VM. This allows the VM to execute arbitrary iSCSI command passthrough on the disk.</simpara>
<simpara>You reserve a LUN through the SCSI persistent reserve options to protect data on the VM from outside access. To enable the reservation, you configure the feature gate option. You then activate the option on the LUN disk to issue SCSI device-specific input and output controls (IOCTLs) that the VM requires.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The volume access mode must be <literal>ReadWriteMany</literal> (RWX) if the VMs that are sharing disks are running on different nodes.</simpara>
<simpara>If the VMs that are sharing disks are running on the same node, <literal>ReadWriteOnce</literal> (RWO) volume access mode is sufficient.</simpara>
</listitem>
<listitem>
<simpara>The storage provider must support a Container Storage Interface (CSI) driver that uses the SCSI protocol.</simpara>
</listitem>
<listitem>
<simpara>If you are a cluster administrator and intend to configure disk sharing by using LUN, you must enable the cluster&#8217;s feature gate on the <literal>HyperConverged</literal> custom resource (CR).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit or create the <literal>VirtualMachine</literal> manifest for your VM to set the required values, as shown in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-0
spec:
  template:
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: sata
            name: rootdisk
          - errorPolicy: report
            lun: <co xml:id="CO77-1"/>
              bus: scsi
              reservation: true <co xml:id="CO77-2"/>
            name: na-shared
            serial: shared1234
      volumes:
      - dataVolume:
          name: vm-0
        name: rootdisk
      - name: na-shared
        persistentVolumeClaim:
          claimName: pvc-na-share</programlisting>
<calloutlist>
<callout arearefs="CO77-1">
<para>Identifies a LUN disk.</para>
</callout>
<callout arearefs="CO77-2">
<para>Identifies that the persistent reservation is enabled.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the <literal>VirtualMachine</literal> manifest file to apply your changes.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-enabling-persistentreservation-feature-gate_virt-configuring-shared-volumes-for-vms">
<title>Enabling the PersistentReservation feature gate</title>
<simpara>You can enable the SCSI <literal>persistentReservation</literal> feature gate and allow a LUN-backed block mode virtual machine (VM) disk to be shared among multiple virtual machines.</simpara>
<simpara>The <literal>persistentReservation</literal> feature gate is disabled by default.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Cluster administrator privileges are required.</simpara>
</listitem>
<listitem>
<simpara>The volume access mode <literal>ReadWriteMany</literal> (RWX) is required if the VMs that are sharing disks are running on different nodes. If the VMs that are sharing disks are running on the same node, the <literal>ReadWriteOnce</literal> (RWO) volume access mode is sufficient.</simpara>
</listitem>
<listitem>
<simpara>The storage provider must support a Container Storage Interface (CSI) driver that uses the SCSI protocol.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Enable the <literal>persistentReservation</literal> feature gate by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op":"replace","path":"/spec/featureGates/persistentReservation", "value": true}]'</programlisting>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://www.qemu.org/docs/master/interop/pr-helper.html">Persistent reservation helper protocol</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://learn.microsoft.com/en-us/windows-server/failover-clustering/failover-clustering-overview">Failover Clustering in Windows Server and Azure Stack HCI</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_networking">
<title>Networking</title>
<section xml:id="virt-networking">
<title>Networking overview</title>

<simpara>OpenShift Virtualization provides advanced networking functionality by using custom resources and plugins. Virtual machines (VMs) are integrated with OpenShift Container Platform networking and its ecosystem.</simpara>
<section xml:id="virt-networking-glossary_virt-networking-overview">
<title>OpenShift Virtualization networking glossary</title>
<simpara>The following terms are used throughout OpenShift Virtualization documentation:</simpara>
<variablelist>
<varlistentry>
<term>Container Network Interface (CNI)</term>
<listitem>
<simpara>A <link xlink:href="https://www.cncf.io/">Cloud Native Computing Foundation</link>
project, focused on container network connectivity.
OpenShift Virtualization uses CNI plugins to build upon the basic Kubernetes networking functionality.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Multus</term>
<listitem>
<simpara>A "meta" CNI plugin that allows multiple CNIs to exist so that a pod or virtual machine can use the interfaces it needs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Custom resource definition (CRD)</term>
<listitem>
<simpara>A <link xlink:href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Kubernetes</link>
API resource that allows you to define custom resources, or an object defined by using the CRD API resource.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Network attachment definition (NAD)</term>
<listitem>
<simpara>A CRD introduced by the Multus project that allows you to attach pods, virtual machines, and virtual machine instances to one or more networks.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Node network configuration policy (NNCP)</term>
<listitem>
<simpara>A CRD introduced by the nmstate project, describing the requested network configuration on nodes.
You update the node network configuration, including adding and removing interfaces, by applying a <literal>NodeNetworkConfigurationPolicy</literal> manifest to the cluster.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="default-network-config">
<title>Using the default pod network</title>
<variablelist>
<varlistentry>
<term><link linkend="virt-connecting-vm-to-default-pod-network">Connecting a virtual machine to the default pod network</link></term>
<listitem>
<simpara>Each VM is connected by default to the default internal pod network. You can add or remove network interfaces by editing the VM specification.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-exposing-vm-with-service">Exposing a virtual machine as a service</link></term>
<listitem>
<simpara>You can expose a VM within the cluster or outside the cluster by creating a <literal>Service</literal> object. For on-premise clusters, you can configure a load balancing service by using the MetalLB Operator. You can <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#metallb-operator-install">install the MetalLB Operator</link> by using the OpenShift Container Platform web console or the CLI.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="secondary-network-config">
<title>Configuring VM secondary network interfaces</title>
<variablelist>
<varlistentry>
<term><link linkend="virt-connecting-vm-to-linux-bridge">Connecting a virtual machine to a Linux bridge network</link></term>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#k8s-nmstate-about-the-k8s-nmstate-operator">Install the Kubernetes NMState Operator</link> to configure Linux bridges, VLANs, and bondings for your secondary networks.</simpara>
<simpara>You can create a Linux bridge network and attach a VM to the network by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="virt-creating-linux-bridge-nncp_virt-connecting-vm-to-linux-bridge">Configure a Linux bridge network device</link> by creating a <literal>NodeNetworkConfigurationPolicy</literal> custom resource definition (CRD).</simpara>
</listitem>
<listitem>
<simpara><link linkend="creating-linux-bridge-nad">Configure a Linux bridge network</link> by creating a <literal>NetworkAttachmentDefinition</literal> CRD.</simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-vm-network-interface">Connect the VM to the Linux bridge network</link> by including the network details in the VM configuration.</simpara>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-connecting-vm-to-sriov">Connecting a virtual machine to an SR-IOV network</link></term>
<listitem>
<simpara>You can use Single Root I/O Virtualization (SR-IOV) network devices with additional networks on your OpenShift Container Platform cluster installed on bare metal or Red Hat OpenStack Platform (RHOSP) infrastructure for applications that require high bandwidth or low latency.</simpara>
<simpara>You must <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#installing-sriov-operator">install the SR-IOV Network Operator</link> on your cluster to manage SR-IOV network devices and network attachments.</simpara>
<simpara>You can connect a VM to an SR-IOV network by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="nw-sriov-configuring-device_virt-connecting-vm-to-sriov">Configure an SR-IOV network device</link> by creating a <literal>SriovNetworkNodePolicy</literal> CRD.</simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-sriov-network-attachment_virt-connecting-vm-to-sriov">Configure an SR-IOV network</link> by creating an <literal>SriovNetwork</literal> object.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-attaching-vm-to-sriov-network_virt-connecting-vm-to-sriov">Connect the VM to the SR-IOV network</link> by including the network details in the VM configuration.</simpara>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-connecting-vm-to-ovn-secondary-network">Connecting a virtual machine to an OVN-Kubernetes secondary network</link></term>
<listitem>
<simpara>You can connect a VM to an Open Virtual Network (OVN)-Kubernetes secondary network. OpenShift Virtualization supports the layer 2 and localnet topologies for OVN-Kubernetes.</simpara>
<itemizedlist>
<listitem>
<simpara>A layer 2 topology connects workloads by a cluster-wide logical switch. The OVN-Kubernetes Container Network Interface (CNI) plug-in uses the Geneve (Generic Network Virtualization Encapsulation) protocol to create an overlay network between nodes. You can use this overlay network to connect VMs on different nodes, without having to configure any additional physical networking infrastructure.</simpara>
</listitem>
<listitem>
<simpara>A localnet topology connects the secondary network to the physical underlay. This enables both east-west cluster traffic and access to services running outside the cluster, but it requires additional configuration of the underlying Open vSwitch (OVS) system on cluster nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>To configure an OVN-Kubernetes secondary network and attach a VM to that network, perform the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="creating-ovn-nad">Configure an OVN-Kubernetes secondary network</link> by creating a network attachment definition (NAD).</simpara>
<note>
<simpara>For localnet topology, you must <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#configuring-additional-network_ovn-kubernetes-configuration-for-a-localnet-topology">configure an OVS bridge</link> by creating a <literal>NodeNetworkConfigurationPolicy</literal> object before creating the NAD.</simpara>
</note>
</listitem>
<listitem>
<simpara><link linkend="attaching-vm-to-ovn-secondary-nw">Connect the VM to the OVN-Kubernetes secondary network</link> by adding the network details to the VM specification.</simpara>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-hot-plugging-network-interfaces">Hot plugging secondary network interfaces</link></term>
<listitem>
<simpara>You can add or remove secondary network interfaces without stopping your VM. OpenShift Virtualization supports hot plugging and hot unplugging for Linux bridge interfaces that use the VirtIO device driver.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-using-dpdk-with-sriov">Using DPDK with SR-IOV</link></term>
<listitem>
<simpara>The Data Plane Development Kit (DPDK) provides a set of libraries and drivers for fast packet processing. You can configure clusters and VMs to run DPDK workloads over SR-IOV networks.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-dedicated-network-live-migration">Configuring a dedicated network for live migration</link></term>
<listitem>
<simpara>You can configure a dedicated <link linkend="virt-connecting-vm-to-linux-bridge">Multus network</link> for live migration. A dedicated network minimizes the effects of network saturation on tenant workloads during live migration.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-accessing-vm-secondary-network-fqdn">Accessing a virtual machine by using the cluster FQDN</link></term>
<listitem>
<simpara>You can access a VM that is attached to a secondary network interface from outside the cluster by using its fully qualified domain name (FQDN).</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-configuring-viewing-ips-for-vms">Configuring and viewing IP addresses</link></term>
<listitem>
<simpara>You can configure an IP address of a secondary network interface when you create a VM. The IP address is provisioned with cloud-init. You can view the IP address of a VM by using the OpenShift Container Platform web console or the command line. The network information is collected by the QEMU guest agent.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="service-mesh-integration">
<title>Integrating with OpenShift Service Mesh</title>
<variablelist>
<varlistentry>
<term><link linkend="virt-connecting-vm-to-service-mesh">Connecting a virtual machine to a service mesh</link></term>
<listitem>
<simpara>OpenShift Virtualization is integrated with OpenShift Service Mesh. You can monitor, visualize, and control traffic between pods and virtual machines.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="managing-mac-address-pools">
<title>Managing MAC address pools</title>
<variablelist>
<varlistentry>
<term><link linkend="virt-using-mac-address-pool-for-vms">Managing MAC address pools for network interfaces</link></term>
<listitem>
<simpara>The KubeMacPool component allocates MAC addresses for VM network interfaces from a shared MAC address pool. This ensures that each network interface is assigned a unique MAC address. A virtual machine instance created from that VM retains the assigned MAC address across reboots.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="vm-ssh-access">
<title>Configuring SSH access</title>
<variablelist>
<varlistentry>
<term><link linkend="virt-accessing-vm-ssh">Configuring SSH access to virtual machines</link></term>
<listitem>
<simpara>You can configure SSH access to VMs by using the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="using-virtctl-ssh_virt-accessing-vm-ssh"><literal>virtctl ssh</literal> command</link></simpara>
<simpara>You create an SSH key pair, add the public key to a VM, and connect to the VM by running the <literal>virtctl ssh</literal> command with the private key.</simpara>
<simpara>You can add public SSH keys to Red Hat Enterprise Linux (RHEL) 9 VMs at runtime or at first boot to VMs with guest operating systems that can be configured by using a cloud-init data source.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-using-virtctl-port-forward-command_virt-accessing-vm-ssh"><literal>virtctl port-forward</literal> command</link></simpara>
<simpara>You add the <literal>virtctl port-foward</literal> command to your <literal>.ssh/config</literal> file and connect to the VM by using OpenSSH.</simpara>
</listitem>
<listitem>
<simpara><link linkend="using-services-ssh_virt-accessing-vm-ssh">Service</link></simpara>
<simpara>You create a service, associate the service with the VM, and connect to the IP address and port exposed by the service.</simpara>
</listitem>
<listitem>
<simpara><link linkend="using-secondary-networks-ssh_virt-accessing-vm-ssh">Secondary network</link></simpara>
<simpara>You configure a secondary network, attach a VM to the secondary network interface, and connect to its allocated IP address.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="virt-connecting-vm-to-default-pod-network">
<title>Connecting a virtual machine to the default pod network</title>

<simpara>You can connect a virtual machine to the default internal pod network by configuring its network interface to use the <literal>masquerade</literal> binding mode.</simpara>
<note>
<simpara>Traffic passing through network interfaces to the default pod network is interrupted during live migration.</simpara>
</note>
<section xml:id="virt-configuring-masquerade-mode-cli_virt-connecting-vm-to-default-pod-network">
<title>Configuring masquerade mode from the command line</title>
<simpara>You can use masquerade mode to hide a virtual machine&#8217;s outgoing traffic behind
the pod IP address. Masquerade mode uses Network Address Translation (NAT) to
connect virtual machines to the pod network backend through a Linux bridge.</simpara>
<simpara>Enable masquerade mode and allow traffic to enter the virtual machine by
editing your virtual machine configuration file.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The virtual machine must be configured to use DHCP to acquire IPv4 addresses.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>interfaces</literal> spec of your virtual machine configuration file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
spec:
  template:
    spec:
      domain:
        devices:
          interfaces:
            - name: default
              masquerade: {} <co xml:id="CO78-1"/>
              ports: <co xml:id="CO78-2"/>
                - port: 80
# ...
      networks:
      - name: default
        pod: {}</programlisting>
<calloutlist>
<callout arearefs="CO78-1">
<para>Connect using masquerade mode.</para>
</callout>
<callout arearefs="CO78-2">
<para>Optional: List the ports that you want to expose from the virtual machine, each specified by the <literal>port</literal> field. The <literal>port</literal> value must be a number between 0 and 65536. When the <literal>ports</literal> array is not used, all ports in the valid range are open to incoming traffic. In this example, incoming traffic is allowed on port <literal>80</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>Ports 49152 and 49153 are reserved for use by the libvirt platform and all other incoming traffic to these ports is dropped.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the virtual machine:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;vm-name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-configuring-masquerade-mode-dual-stack_virt-connecting-vm-to-default-pod-network">
<title>Configuring masquerade mode with dual-stack (IPv4 and IPv6)</title>
<simpara>You can configure a new virtual machine (VM) to use both IPv6 and IPv4 on the default pod network by using cloud-init.</simpara>
<simpara>The <literal>Network.pod.vmIPv6NetworkCIDR</literal> field in the virtual machine instance configuration determines the static IPv6 address of the VM and the gateway IP address. These are used by the virt-launcher pod to route IPv6 traffic to the virtual machine and are not used externally. The <literal>Network.pod.vmIPv6NetworkCIDR</literal> field specifies an IPv6 address block in Classless Inter-Domain Routing (CIDR) notation. The default value is <literal>fd10:0:2::2/120</literal>. You can edit this value based on your network requirements.</simpara>
<simpara>When the virtual machine is running, incoming and outgoing traffic for the virtual machine is routed to both the IPv4 address and the unique IPv6 address of the virt-launcher pod. The virt-launcher pod then routes the IPv4 traffic to the DHCP address of the virtual machine, and the IPv6 traffic to the statically set IPv6 address of the virtual machine.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The OpenShift Container Platform cluster must use the OVN-Kubernetes Container Network Interface (CNI) network plugin configured for dual-stack.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In a new virtual machine configuration, include an interface with <literal>masquerade</literal> and configure the IPv6 address and default gateway by using cloud-init.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm-ipv6
spec:
  template:
    spec:
      domain:
        devices:
          interfaces:
            - name: default
              masquerade: {} <co xml:id="CO79-1"/>
              ports:
                - port: 80 <co xml:id="CO79-2"/>
# ...
      networks:
      - name: default
        pod: {}
      volumes:
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                dhcp4: true
                addresses: [ fd10:0:2::2/120 ] <co xml:id="CO79-3"/>
                gateway6: fd10:0:2::1 <co xml:id="CO79-4"/></programlisting>
<calloutlist>
<callout arearefs="CO79-1">
<para>Connect using masquerade mode.</para>
</callout>
<callout arearefs="CO79-2">
<para>Allows incoming traffic on port 80 to the virtual machine.</para>
</callout>
<callout arearefs="CO79-3">
<para>The static IPv6 address as determined by the <literal>Network.pod.vmIPv6NetworkCIDR</literal> field in the virtual machine instance configuration. The default value is <literal>fd10:0:2::2/120</literal>.</para>
</callout>
<callout arearefs="CO79-4">
<para>The gateway IP address as determined by the <literal>Network.pod.vmIPv6NetworkCIDR</literal> field in the virtual machine instance configuration. The default value is <literal>fd10:0:2::1</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the virtual machine in the namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f example-vm-ipv6.yaml</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>To verify that IPv6 has been configured, start the virtual machine and view the interface status of the virtual machine instance to ensure it has an IPv6 address:</simpara>
</listitem>
</itemizedlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi &lt;vmi-name&gt; -o jsonpath="{.status.interfaces[*].ipAddresses}"</programlisting>
</section>
<section xml:id="virt-jumbo-frames-vm-pod-nw_virt-connecting-vm-to-default-pod-network">
<title>About jumbo frames support</title>
<simpara>When using the OVN-Kubernetes CNI plugin, you can send unfragmented jumbo frame packets between two virtual machines (VMs) that are connected on the default pod network. Jumbo frames have a maximum transmission unit (MTU) value greater than 1500 bytes.</simpara>
<simpara>The VM automatically gets the MTU value of the cluster network, set by the cluster administrator, in one of the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>libvirt</literal>: If the guest OS has the latest version of the VirtIO driver that can interpret incoming data via a Peripheral Component Interconnect (PCI) config register in the emulated device.</simpara>
</listitem>
<listitem>
<simpara>DHCP: If the guest DHCP client can read the MTU value from the DHCP server response.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>For Windows VMs that do not have a VirtIO driver, you must set the MTU manually by using <literal>netsh</literal> or a similar tool. This is because the Windows DHCP client does not read the MTU value.</simpara>
</note>
</section>
<section xml:id="additional-resources_virt-connecting-vm-to-default-pod-network" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#changing-cluster-network-mtu">Changing the MTU for the cluster network</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#optimizing-mtu_optimizing-networking">Optimizing the MTU for your network</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-exposing-vm-with-service">
<title>Exposing a virtual machine by using a service</title>

<simpara>You can expose a virtual machine within the cluster or outside the cluster by creating a <literal>Service</literal> object.</simpara>
<section xml:id="virt-about-services_virt-exposing-vm-with-service">
<title>About services</title>
<simpara>A Kubernetes service exposes network access for clients to an application running on a set of pods. Services offer abstraction, load balancing, and, in the case of the <literal>NodePort</literal> and <literal>LoadBalancer</literal> types, exposure to the outside world.</simpara>
<variablelist>
<varlistentry>
<term>ClusterIP</term>
<listitem>
<simpara>Exposes the service on an internal IP address and as a DNS name to other applications within the cluster. A single service can map to multiple virtual machines. When a client tries to connect to the service, the client&#8217;s request is load balanced among available backends. <literal>ClusterIP</literal> is the default service type.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>NodePort</term>
<listitem>
<simpara>Exposes the service on the same port of each selected node in the cluster. <literal>NodePort</literal> makes a port accessible from outside the cluster, as long as the node itself is externally accessible to the client.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>LoadBalancer</term>
<listitem>
<simpara>Creates an external load balancer in the current cloud (if supported) and assigns a fixed, external IP address to the service.</simpara>
</listitem>
</varlistentry>
</variablelist>
<note>
<simpara>For on-premise clusters, you can configure a load balancing service by using the MetalLB Operator in layer 2 mode. The BGP mode is not supported. The MetalLB Operator is installed in the <literal>metallb-system</literal> namespace.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#metallb-operator-install">Installing the MetalLB Operator</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#metallb-configure-services">Configuring services to use MetalLB</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-dual-stack-support-services_virt-exposing-vm-with-service">
<title>Dual-stack support</title>
<simpara>If IPv4 and IPv6 dual-stack networking is enabled for your cluster, you can create a service that uses IPv4, IPv6, or both, by defining the <literal>spec.ipFamilyPolicy</literal> and the <literal>spec.ipFamilies</literal> fields in the <literal>Service</literal> object.</simpara>
<simpara>The <literal>spec.ipFamilyPolicy</literal> field can be set to one of the following values:</simpara>
<variablelist>
<varlistentry>
<term>SingleStack</term>
<listitem>
<simpara>The control plane assigns a cluster IP address for the service based on the first configured service cluster IP range.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>PreferDualStack</term>
<listitem>
<simpara>The control plane assigns both IPv4 and IPv6 cluster IP addresses for the service on clusters that have dual-stack configured.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>RequireDualStack</term>
<listitem>
<simpara>This option fails for clusters that do not have dual-stack networking enabled. For clusters that have dual-stack configured, the behavior is the same as when the value is set to <literal>PreferDualStack</literal>. The control plane allocates cluster IP addresses from both IPv4 and IPv6 address ranges.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>You can define which IP family to use for single-stack or define the order of IP families for dual-stack by setting the <literal>spec.ipFamilies</literal> field to one of the following array values:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>[IPv4]</literal></simpara>
</listitem>
<listitem>
<simpara><literal>[IPv6]</literal></simpara>
</listitem>
<listitem>
<simpara><literal>[IPv4, IPv6]</literal></simpara>
</listitem>
<listitem>
<simpara><literal>[IPv6, IPv4]</literal></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-creating-service-cli_virt-exposing-vm-with-service">
<title>Creating a service by using the command line</title>
<simpara>You can create a service and associate it with a virtual machine (VM) by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured the cluster network to support the service.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest to add the label for service creation:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  running: false
  template:
    metadata:
      labels:
        special: key <co xml:id="CO80-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO80-1">
<para>Add <literal>special: key</literal> to the <literal>spec.template.metadata.labels</literal> stanza.</para>
</callout>
</calloutlist>
<note>
<simpara>Labels on a virtual machine are passed through to the pod. The <literal>special: key</literal> label must match the label in the <literal>spec.selector</literal> attribute of the <literal>Service</literal> manifest.</simpara>
</note>
</listitem>
<listitem>
<simpara>Save the <literal>VirtualMachine</literal> manifest file to apply your changes.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Service</literal> manifest to expose the VM:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Service
metadata:
  name: example-service
  namespace: example-namespace
spec:
# ...
  selector:
    special: key <co xml:id="CO81-1"/>
  type: NodePort <co xml:id="CO81-2"/></programlisting>
<calloutlist>
<callout arearefs="CO81-1">
<para>Specify the label that you added to the <literal>spec.template.metadata.labels</literal> stanza of the <literal>VirtualMachine</literal> manifest.</para>
</callout>
<callout arearefs="CO81-2">
<para>Specify <literal>ClusterIP</literal>, <literal>NodePort</literal>, or <literal>LoadBalancer</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the <literal>Service</literal> manifest file.</simpara>
</listitem>
<listitem>
<simpara>Create the service by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f example-service.yaml</programlisting>
</listitem>
<listitem>
<simpara>Restart the VM to apply the changes.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Query the <literal>Service</literal> object to verify that it is available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get service -n example-namespace</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional-resources_creating-service-vm" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#configuring-ingress-cluster-traffic-nodeport">Configuring ingress cluster traffic using a NodePort</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#configuring-ingress-cluster-traffic-load-balancer">Configuring ingress cluster traffic using a load balancer</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-connecting-vm-to-linux-bridge">
<title>Connecting a virtual machine to a Linux bridge network</title>

<simpara>By default, OpenShift Virtualization is installed with a single, internal pod network.</simpara>
<simpara>You can create a Linux bridge network and attach a virtual machine (VM) to the network by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="virt-creating-linux-bridge-nncp_virt-connecting-vm-to-linux-bridge">Create a Linux bridge node network configuration policy (NNCP)</link>.</simpara>
</listitem>
<listitem>
<simpara>Create a Linux bridge network attachment definition (NAD) by using the <link linkend="virt-creating-linux-bridge-nad-web_virt-connecting-vm-to-linux-bridge">web console</link> or the <link linkend="virt-creating-linux-bridge-nad-cli_virt-connecting-vm-to-linux-bridge">command line</link>.</simpara>
</listitem>
<listitem>
<simpara>Configure the VM to recognize the NAD by using the <link linkend="virt-vm-creating-nic-web_virt-connecting-vm-to-linux-bridge">web console</link> or the <link linkend="virt-attaching-vm-secondary-network-cli_virt-connecting-vm-to-linux-bridge">command line</link>.</simpara>
</listitem>
</orderedlist>
<section xml:id="virt-creating-linux-bridge-nncp_virt-connecting-vm-to-linux-bridge">
<title>Creating a Linux bridge NNCP</title>
<simpara>You can create a <literal>NodeNetworkConfigurationPolicy</literal> (NNCP) manifest for a Linux bridge network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the Kubernetes NMState Operator.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>NodeNetworkConfigurationPolicy</literal> manifest. This example includes sample values that you must replace with your own information.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <co xml:id="CO82-1"/>
spec:
  desiredState:
    interfaces:
      - name: br1 <co xml:id="CO82-2"/>
        description: Linux bridge with eth1 as a port <co xml:id="CO82-3"/>
        type: linux-bridge <co xml:id="CO82-4"/>
        state: up <co xml:id="CO82-5"/>
        ipv4:
          enabled: false <co xml:id="CO82-6"/>
        bridge:
          options:
            stp:
              enabled: false <co xml:id="CO82-7"/>
          port:
            - name: eth1 <co xml:id="CO82-8"/></programlisting>
<calloutlist>
<callout arearefs="CO82-1">
<para>Name of the policy.</para>
</callout>
<callout arearefs="CO82-2">
<para>Name of the interface.</para>
</callout>
<callout arearefs="CO82-3">
<para>Optional: Human-readable description of the interface.</para>
</callout>
<callout arearefs="CO82-4">
<para>The type of interface. This example creates a bridge.</para>
</callout>
<callout arearefs="CO82-5">
<para>The requested state for the interface after creation.</para>
</callout>
<callout arearefs="CO82-6">
<para>Disables IPv4 in this example.</para>
</callout>
<callout arearefs="CO82-7">
<para>Disables STP in this example.</para>
</callout>
<callout arearefs="CO82-8">
<para>The node NIC to which the bridge is attached.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-linux-bridge-nad">
<title>Creating a Linux bridge NAD</title>
<simpara>You can create a Linux bridge network attachment definition (NAD) by using the OpenShift Container Platform web console or command line.</simpara>
<section xml:id="virt-creating-linux-bridge-nad-web_virt-connecting-vm-to-linux-bridge">
<title>Creating a Linux bridge NAD by using the web console</title>
<simpara>You can create a network attachment definition (NAD) to provide layer-2 networking to pods and virtual machines by using the OpenShift Container Platform web console.</simpara>
<simpara>A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.</simpara>
<warning>
<simpara>Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.</simpara>
</warning>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the web console, click <emphasis role="strong">Networking</emphasis> &#8594; <emphasis role="strong">NetworkAttachmentDefinitions</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create Network Attachment Definition</emphasis>.</simpara>
<note>
<simpara>The network attachment definition must be in the same namespace as the pod or virtual machine.</simpara>
</note>
</listitem>
<listitem>
<simpara>Enter a unique <emphasis role="strong">Name</emphasis> and optional <emphasis role="strong">Description</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">CNV Linux bridge</emphasis> from the <emphasis role="strong">Network Type</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Enter the name of the bridge in the <emphasis role="strong">Bridge Name</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: If the resource has VLAN IDs configured, enter the ID numbers in the <emphasis role="strong">VLAN Tag Number</emphasis> field.</simpara>
</listitem>
<listitem>
<simpara>Optional: Select <emphasis role="strong">MAC Spoof Check</emphasis> to enable MAC spoof filtering. This feature provides security against a MAC spoofing attack by allowing only a single MAC address to exit the pod.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-linux-bridge-nad-cli_virt-connecting-vm-to-linux-bridge">
<title>Creating a Linux bridge NAD by using the command line</title>
<simpara>You can create a network attachment definition (NAD) to provide layer-2 networking to pods and virtual machines (VMs) by using the command line.</simpara>
<simpara>The NAD and the VM must be in the same namespace.</simpara>
<warning>
<simpara>Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The node must support nftables and the <literal>nft</literal> binary must be deployed to enable MAC spoof check.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the VM to the <literal>NetworkAttachmentDefinition</literal> configuration, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: bridge-network <co xml:id="CO83-1"/>
  annotations:
    k8s.v1.cni.cncf.io/resourceName: bridge.network.kubevirt.io/bridge-interface <co xml:id="CO83-2"/>
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": bridge-network, <co xml:id="CO83-3"/>
    "type": cnv-bridge, <co xml:id="CO83-4"/>
    "bridge": bridge-interface, <co xml:id="CO83-5"/>
    "macspoofchk": true, <co xml:id="CO83-6"/>
    "vlan": 100, <co xml:id="CO83-7"/>
    "preserveDefaultVlan": false <co xml:id="CO83-8"/>
  }'</programlisting>
<calloutlist>
<callout arearefs="CO83-1">
<para>The name for the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO83-2">
<para>Optional: Annotation key-value pair for node selection, where <literal>bridge-interface</literal> must match the name of a bridge configured on some nodes. If you add this annotation to your network attachment definition, your virtual machine instances will only run on the nodes that have the <literal>bridge-interface</literal> bridge connected.</para>
</callout>
<callout arearefs="CO83-3">
<para>The name for the configuration. It is recommended to match the configuration name to the <literal>name</literal> value of the network attachment definition.</para>
</callout>
<callout arearefs="CO83-4">
<para>The actual name of the Container Network Interface (CNI) plugin that provides the network for this network attachment definition. Do not change this field unless you want to use a different CNI.</para>
</callout>
<callout arearefs="CO83-5">
<para>The name of the Linux bridge configured on the node.</para>
</callout>
<callout arearefs="CO83-6">
<para>Optional: Flag to enable MAC spoof check. When set to <literal>true</literal>, you cannot change the MAC address of the pod or guest interface. This attribute provides security against a MAC spoofing attack by allowing only a single MAC address to exit the pod.</para>
</callout>
<callout arearefs="CO83-7">
<para>Optional: The VLAN tag. No additional VLAN configuration is required on the node network configuration policy.</para>
</callout>
<callout arearefs="CO83-8">
<para>Optional: Indicates whether the VM connects to the bridge through the default VLAN. The default value is <literal>true</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create the network attachment definition:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f network-attachment-definition.yaml <co xml:id="CO84-1"/></programlisting>
<calloutlist>
<callout arearefs="CO84-1">
<para>Where <literal>network-attachment-definition.yaml</literal> is the file name of the network attachment definition manifest.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the network attachment definition was created by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get network-attachment-definition bridge-network</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="configuring-vm-network-interface">
<title>Configuring a VM network interface</title>
<simpara>You can configure a virtual machine (VM) network interface by using the OpenShift Container Platform web console or command line.</simpara>
<section xml:id="virt-vm-creating-nic-web_virt-connecting-vm-to-linux-bridge">
<title>Configuring a VM network interface by using the web console</title>
<simpara>You can configure a network interface for a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created a network attachment definition for the network.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click a VM to view the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Configuration</emphasis> tab, click the <emphasis role="strong">Network interfaces</emphasis> tab.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Add network interface</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the interface name and select the network attachment definition from the <emphasis role="strong">Network</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Restart the VM to apply the changes.</simpara>
</listitem>
</orderedlist>
<bridgehead xml:id="virt-networking-wizard-fields-web_virt-connecting-vm-to-linux-bridge" renderas="sect5">Networking fields</bridgehead>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Name</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Name</simpara></entry>
<entry align="left" valign="top"><simpara>Name for the network interface controller.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Model</simpara></entry>
<entry align="left" valign="top"><simpara>Indicates the model of the network interface controller. Supported values are <emphasis role="strong">e1000e</emphasis> and <emphasis role="strong">virtio</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Network</simpara></entry>
<entry align="left" valign="top"><simpara>List of available network attachment definitions.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Type</simpara></entry>
<entry align="left" valign="top"><simpara>List of available binding methods. Select the binding method suitable for the network interface:</simpara>
<itemizedlist>
<listitem>
<simpara>Default pod network: <literal>masquerade</literal></simpara>
</listitem>
<listitem>
<simpara>Linux bridge network: <literal>bridge</literal></simpara>
</listitem>
<listitem>
<simpara>SR-IOV network: <literal>SR-IOV</literal></simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>MAC Address</simpara></entry>
<entry align="left" valign="top"><simpara>MAC address for the network interface controller. If a MAC address is not specified, one is assigned automatically.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="virt-attaching-vm-secondary-network-cli_virt-connecting-vm-to-linux-bridge">
<title>Configuring a VM network interface by using the command line</title>
<simpara>You can configure a virtual machine (VM) network interface for a bridge network by using the command line.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Shut down the virtual machine before editing the configuration. If you edit a running virtual machine, you must restart the virtual machine for the changes to take effect.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the bridge interface and the network attachment definition to the VM configuration as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
spec:
  template:
    spec:
      domain:
        devices:
          interfaces:
            - masquerade: {}
              name: default
            - bridge: {}
              name: bridge-net <co xml:id="CO85-1"/>
# ...
      networks:
        - name: default
          pod: {}
        - name: bridge-net <co xml:id="CO85-2"/>
          multus:
            networkName: a-bridge-network <co xml:id="CO85-3"/></programlisting>
<calloutlist>
<callout arearefs="CO85-1">
<para>The name of the bridge interface.</para>
</callout>
<callout arearefs="CO85-2">
<para>The name of the network. This value must match the <literal>name</literal> value of the corresponding <literal>spec.template.spec.domain.devices.interfaces</literal> entry.</para>
</callout>
<callout arearefs="CO85-3">
<para>The name of the network attachment definition.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f example-vm.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: If you edited a running virtual machine, you must restart it for the changes to take effect.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="virt-connecting-vm-to-sriov">
<title>Connecting a virtual machine to an SR-IOV network</title>

<simpara>You can connect a virtual machine (VM) to a Single Root I/O Virtualization (SR-IOV) network by performing the following steps:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="nw-sriov-configuring-device_virt-connecting-vm-to-sriov">Configuring an SR-IOV network device</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="nw-sriov-network-attachment_virt-connecting-vm-to-sriov">Configuring an SR-IOV network</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-attaching-vm-to-sriov-network_virt-connecting-vm-to-sriov">Connecting the VM to the SR-IOV network</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="nw-sriov-configuring-device_virt-connecting-vm-to-sriov">
<title>Configuring SR-IOV network devices</title>
<simpara>The SR-IOV Network Operator adds the <literal>SriovNetworkNodePolicy.sriovnetwork.openshift.io</literal> CustomResourceDefinition to OpenShift Container Platform.
You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).</simpara>
<note>
<simpara>When applying the configuration specified in a <literal>SriovNetworkNodePolicy</literal> object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.</simpara>
<simpara>It might take several minutes for a configuration change to apply.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have enough available nodes in your cluster to handle the evicted workload from drained nodes.</simpara>
</listitem>
<listitem>
<simpara>You have not selected any control plane nodes for SR-IOV network device configuration.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> object, and then save the YAML in the <literal>&lt;name&gt;-sriov-node-network.yaml</literal> file. Replace <literal>&lt;name&gt;</literal> with the name for this configuration.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: &lt;name&gt; <co xml:id="CO86-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO86-2"/>
spec:
  resourceName: &lt;sriov_resource_name&gt; <co xml:id="CO86-3"/>
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true" <co xml:id="CO86-4"/>
  priority: &lt;priority&gt; <co xml:id="CO86-5"/>
  mtu: &lt;mtu&gt; <co xml:id="CO86-6"/>
  numVfs: &lt;num&gt; <co xml:id="CO86-7"/>
  nicSelector: <co xml:id="CO86-8"/>
    vendor: "&lt;vendor_code&gt;" <co xml:id="CO86-9"/>
    deviceID: "&lt;device_id&gt;" <co xml:id="CO86-10"/>
    pfNames: ["&lt;pf_name&gt;", ...] <co xml:id="CO86-11"/>
    rootDevices: ["&lt;pci_bus_id&gt;", "..."] <co xml:id="CO86-12"/>
  deviceType: vfio-pci <co xml:id="CO86-13"/>
  isRdma: false <co xml:id="CO86-14"/></programlisting>
<calloutlist>
<callout arearefs="CO86-1">
<para>Specify a name for the CR object.</para>
</callout>
<callout arearefs="CO86-2">
<para>Specify the namespace where the SR-IOV Operator is installed.</para>
</callout>
<callout arearefs="CO86-3">
<para>Specify the resource name of the SR-IOV device plugin. You can create multiple <literal>SriovNetworkNodePolicy</literal> objects for a resource name.</para>
</callout>
<callout arearefs="CO86-4">
<para>Specify the node selector to select which nodes are configured.
Only SR-IOV network devices on selected nodes are configured. The SR-IOV
Container Network Interface (CNI) plugin and device plugin are deployed only on selected nodes.</para>
</callout>
<callout arearefs="CO86-5">
<para>Optional: Specify an integer value between <literal>0</literal> and <literal>99</literal>. A smaller number gets higher priority, so a priority of <literal>10</literal> is higher than a priority of <literal>99</literal>. The default value is <literal>99</literal>.</para>
</callout>
<callout arearefs="CO86-6">
<para>Optional: Specify a value for the maximum transmission unit (MTU) of the virtual function. The maximum MTU value can vary for different NIC models.</para>
</callout>
<callout arearefs="CO86-7">
<para>Specify the number of the virtual functions (VF) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than <literal>128</literal>.</para>
</callout>
<callout arearefs="CO86-8">
<para>The <literal>nicSelector</literal> mapping selects the Ethernet device for the Operator to configure. You do not need to specify values for all the parameters. It is recommended to identify the Ethernet adapter with enough precision to minimize the possibility of selecting an Ethernet device unintentionally.
If you specify <literal>rootDevices</literal>, you must also specify a value for <literal>vendor</literal>, <literal>deviceID</literal>, or <literal>pfNames</literal>.
If you specify both <literal>pfNames</literal> and <literal>rootDevices</literal> at the same time, ensure that they point to an identical device.</para>
</callout>
<callout arearefs="CO86-9">
<para>Optional: Specify the vendor hex code of the SR-IOV network device. The only allowed values are either <literal>8086</literal> or <literal>15b3</literal>.</para>
</callout>
<callout arearefs="CO86-10">
<para>Optional: Specify the device hex code of SR-IOV network device. The only allowed values are <literal>158b</literal>, <literal>1015</literal>, <literal>1017</literal>.</para>
</callout>
<callout arearefs="CO86-11">
<para>Optional: The parameter accepts an array of one or more physical function (PF) names for the Ethernet device.</para>
</callout>
<callout arearefs="CO86-12">
<para>The parameter accepts an array of one or more PCI bus addresses for the physical function of the Ethernet device. Provide the address in the following format: <literal>0000:02:00.1</literal>.</para>
</callout>
<callout arearefs="CO86-13">
<para>The <literal>vfio-pci</literal> driver type is required for virtual functions in OpenShift Virtualization.</para>
</callout>
<callout arearefs="CO86-14">
<para>Optional: Specify whether to enable remote direct memory access (RDMA) mode. For a Mellanox card, set <literal>isRdma</literal> to <literal>false</literal>. The default value is <literal>false</literal>.</para>
</callout>
</calloutlist>
<note>
<simpara>If <literal>isRDMA</literal> flag is set to <literal>true</literal>, you can continue to use the RDMA enabled VF as a normal network device.
A device can be used in either mode.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: Label the SR-IOV capable cluster nodes with <literal>SriovNetworkNodePolicy.Spec.NodeSelector</literal> if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>SriovNetworkNodePolicy</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;-sriov-node-network.yaml</programlisting>
<simpara>where <literal>&lt;name&gt;</literal> specifies the name for this configuration.</simpara>
<simpara>After applying the configuration update, all the pods in <literal>sriov-network-operator</literal> namespace transition to the <literal>Running</literal> status.</simpara>
</listitem>
<listitem>
<simpara>To verify that the SR-IOV network device is configured, enter the following command. Replace <literal>&lt;node_name&gt;</literal> with the name of a node with the SR-IOV network device that you just configured.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sriovnetworknodestates -n openshift-sriov-network-operator &lt;node_name&gt; -o jsonpath='{.status.syncStatus}'</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="nw-sriov-network-attachment_virt-connecting-vm-to-sriov">
<title>Configuring SR-IOV additional network</title>
<simpara>You can configure an additional network that uses SR-IOV hardware by creating an <literal>SriovNetwork</literal> object.</simpara>
<simpara>When you create an <literal>SriovNetwork</literal> object, the SR-IOV Network Operator automatically creates a <literal>NetworkAttachmentDefinition</literal> object.</simpara>
<note>
<simpara>Do not modify or delete an <literal>SriovNetwork</literal> object if it is attached to pods or virtual machines in a <literal>running</literal> state.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following <literal>SriovNetwork</literal> object, and then save the YAML in the <literal>&lt;name&gt;-sriov-network.yaml</literal> file. Replace <literal>&lt;name&gt;</literal> with a name for this additional network.</simpara>
</listitem>
</orderedlist>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: &lt;name&gt; <co xml:id="CO87-1"/>
  namespace: openshift-sriov-network-operator <co xml:id="CO87-2"/>
spec:
  resourceName: &lt;sriov_resource_name&gt; <co xml:id="CO87-3"/>
  networkNamespace: &lt;target_namespace&gt; <co xml:id="CO87-4"/>
  vlan: &lt;vlan&gt; <co xml:id="CO87-5"/>
  spoofChk: "&lt;spoof_check&gt;" <co xml:id="CO87-6"/>
  linkState: &lt;link_state&gt; <co xml:id="CO87-7"/>
  maxTxRate: &lt;max_tx_rate&gt; <co xml:id="CO87-8"/>
  minTxRate: &lt;min_rx_rate&gt; <co xml:id="CO87-9"/>
  vlanQoS: &lt;vlan_qos&gt; <co xml:id="CO87-10"/>
  trust: "&lt;trust_vf&gt;" <co xml:id="CO87-11"/>
  capabilities: &lt;capabilities&gt; <co xml:id="CO87-12"/></programlisting>
<calloutlist>
<callout arearefs="CO87-1">
<para>Replace <literal>&lt;name&gt;</literal> with a name for the object. The SR-IOV Network Operator creates a <literal>NetworkAttachmentDefinition</literal> object with same name.</para>
</callout>
<callout arearefs="CO87-2">
<para>Specify the namespace where the SR-IOV Network Operator is installed.</para>
</callout>
<callout arearefs="CO87-3">
<para>Replace <literal>&lt;sriov_resource_name&gt;</literal> with the value for the <literal>.spec.resourceName</literal> parameter from the <literal>SriovNetworkNodePolicy</literal> object that defines the SR-IOV hardware for this additional network.</para>
</callout>
<callout arearefs="CO87-4">
<para>Replace <literal>&lt;target_namespace&gt;</literal> with the target namespace for the SriovNetwork. Only pods or virtual machines in the target namespace can attach to the SriovNetwork.</para>
</callout>
<callout arearefs="CO87-5">
<para>Optional: Replace <literal>&lt;vlan&gt;</literal> with a Virtual LAN (VLAN) ID for the additional network. The integer value must be from <literal>0</literal> to <literal>4095</literal>. The default value is <literal>0</literal>.</para>
</callout>
<callout arearefs="CO87-6">
<para>Optional: Replace <literal>&lt;spoof_check&gt;</literal> with the spoof check mode of the VF. The allowed values are the strings <literal>"on"</literal> and <literal>"off"</literal>.</para>
<important>
<simpara>You must enclose the value you specify in quotes or the CR is rejected by the SR-IOV Network Operator.</simpara>
</important>
</callout>
<callout arearefs="CO87-7">
<para>Optional: Replace <literal>&lt;link_state&gt;</literal> with the link state of virtual function (VF). Allowed value are <literal>enable</literal>, <literal>disable</literal> and <literal>auto</literal>.</para>
</callout>
<callout arearefs="CO87-8">
<para>Optional: Replace <literal>&lt;max_tx_rate&gt;</literal> with a maximum transmission rate, in Mbps, for the VF.</para>
</callout>
<callout arearefs="CO87-9">
<para>Optional: Replace <literal>&lt;min_tx_rate&gt;</literal> with a minimum transmission rate, in Mbps, for the VF. This value should always be less than or equal to Maximum transmission rate.</para>
<note>
<simpara>Intel NICs do not support the <literal>minTxRate</literal> parameter. For more information, see <link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=1772847">BZ#1772847</link>.</simpara>
</note>
</callout>
<callout arearefs="CO87-10">
<para>Optional: Replace <literal>&lt;vlan_qos&gt;</literal> with an IEEE 802.1p priority level for the VF. The default value is <literal>0</literal>.</para>
</callout>
<callout arearefs="CO87-11">
<para>Optional: Replace <literal>&lt;trust_vf&gt;</literal> with the trust mode of the VF. The allowed values are the strings <literal>"on"</literal> and <literal>"off"</literal>.</para>
<important>
<simpara>You must enclose the value you specify in quotes or the CR is rejected by the SR-IOV Network Operator.</simpara>
</important>
</callout>
<callout arearefs="CO87-12">
<para>Optional: Replace <literal>&lt;capabilities&gt;</literal> with the capabilities to configure for this network.</para>
</callout>
</calloutlist>
<orderedlist numeration="arabic" startingnumber="2">
<listitem>
<simpara>To create the object, enter the following command. Replace <literal>&lt;name&gt;</literal> with a name for this additional network.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;name&gt;-sriov-network.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: To confirm that the <literal>NetworkAttachmentDefinition</literal> object associated with the <literal>SriovNetwork</literal> object that you created in the previous step exists, enter the following command. Replace <literal>&lt;namespace&gt;</literal> with the namespace you specified in the <literal>SriovNetwork</literal> object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get net-attach-def -n &lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-attaching-vm-to-sriov-network_virt-connecting-vm-to-sriov">
<title>Connecting a virtual machine to an SR-IOV network</title>
<simpara>You can connect the virtual machine (VM) to the SR-IOV network by including the network details in the VM configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Add the SR-IOV network details to the <literal>spec.domain.devices.interfaces</literal> and <literal>spec.networks</literal> stanzas of the VM configuration as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
spec:
  domain:
    devices:
      interfaces:
      - name: default
        masquerade: {}
      - name: nic1 <co xml:id="CO88-1"/>
        sriov: {}
  networks:
  - name: default
    pod: {}
  - name: nic1 <co xml:id="CO88-2"/>
    multus:
        networkName: sriov-network <co xml:id="CO88-3"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO88-1">
<para>Specify a unique name for the SR-IOV interface.</para>
</callout>
<callout arearefs="CO88-2">
<para>Specify the name of the SR-IOV interface. This must be the same as the <literal>interfaces.name</literal> that you defined earlier.</para>
</callout>
<callout arearefs="CO88-3">
<para>Specify the name of the SR-IOV network attachment definition.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the virtual machine configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;vm_sriov&gt;.yaml <co xml:id="CO89-1"/></programlisting>
<calloutlist>
<callout arearefs="CO89-1">
<para>The name of the virtual machine YAML file.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_virt-connecting-vm-to-sriov" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-using-dpdk-with-sriov">Configuring DPDK workloads for improved performance</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-using-dpdk-with-sriov">
<title>Using DPDK with SR-IOV</title>

<simpara>The Data Plane Development Kit (DPDK) provides a set of libraries and drivers for fast packet processing.</simpara>
<simpara>You can configure clusters and virtual machines (VMs) to run DPDK workloads over SR-IOV networks.</simpara>
<section xml:id="virt-configuring-cluster-dpdk_virt-using-dpdk-with-sriov">
<title>Configuring a cluster for DPDK workloads</title>
<simpara>You can configure an OpenShift Container Platform cluster to run Data Plane Development Kit (DPDK) workloads for improved network performance.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed the SR-IOV Network Operator.</simpara>
</listitem>
<listitem>
<simpara>You have installed the Node Tuning Operator.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Map your compute nodes topology to determine which Non-Uniform Memory Access (NUMA) CPUs are isolated for DPDK applications and which ones are reserved for the operating system (OS).</simpara>
</listitem>
<listitem>
<simpara>Label a subset of the compute nodes with a custom role; for example, <literal>worker-dpdk</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label node &lt;node_name&gt; node-role.kubernetes.io/worker-dpdk=""</programlisting>
</listitem>
<listitem>
<simpara>Create a new <literal>MachineConfigPool</literal> manifest that contains the <literal>worker-dpdk</literal> label in the <literal>spec.machineConfigSelector</literal> object:</simpara>
<formalpara>
<title>Example <literal>MachineConfigPool</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: worker-dpdk
  labels:
    machineconfiguration.openshift.io/role: worker-dpdk
spec:
  machineConfigSelector:
    matchExpressions:
      - key: machineconfiguration.openshift.io/role
        operator: In
        values:
          - worker
          - worker-dpdk
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/worker-dpdk: ""</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>PerformanceProfile</literal> manifest that applies to the labeled nodes and the machine config pool that you created in the previous steps. The performance profile specifies the CPUs that are isolated for DPDK applications and the CPUs that are reserved for house keeping.</simpara>
<formalpara>
<title>Example <literal>PerformanceProfile</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: performance.openshift.io/v2
kind: PerformanceProfile
metadata:
  name: profile-1
spec:
  cpu:
    isolated: 4-39,44-79
    reserved: 0-3,40-43
  globallyDisableIrqLoadBalancing: true
  hugepages:
    defaultHugepagesSize: 1G
    pages:
    - count: 8
      node: 0
      size: 1G
  net:
    userLevelNetworking: true
  nodeSelector:
    node-role.kubernetes.io/worker-dpdk: ""
  numa:
    topologyPolicy: single-numa-node</programlisting>
</para>
</formalpara>
<note>
<simpara>The compute nodes automatically restart after you apply the <literal>MachineConfigPool</literal> and <literal>PerformanceProfile</literal> manifests.</simpara>
</note>
</listitem>
<listitem>
<simpara>Retrieve the name of the generated <literal>RuntimeClass</literal> resource from the <literal>status.runtimeClass</literal> field of the <literal>PerformanceProfile</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get performanceprofiles.performance.openshift.io profile-1 -o=jsonpath='{.status.runtimeClass}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>Set the previously obtained <literal>RuntimeClass</literal> name as the default container runtime class for the <literal>virt-launcher</literal> pods by editing the <literal>HyperConverged</literal> custom resource (CR):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
    --type='json' -p='[{"op": "add", "path": "/spec/defaultRuntimeClass", "value":"&lt;runtimeclass-name&gt;"}]'</programlisting>
<note>
<simpara>Editing the <literal>HyperConverged</literal> CR changes a global setting that affects all VMs that are created after the change is applied.</simpara>
</note>
</listitem>
<listitem>
<simpara>If your DPDK-enabled compute nodes use Simultaneous multithreading (SMT), enable the <literal>AlignCPUs</literal> enabler by editing the <literal>HyperConverged</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
    --type='json' -p='[{"op": "replace", "path": "/spec/featureGates/alignCPUs", "value": true}]'</programlisting>
<note>
<simpara>Enabling <literal>AlignCPUs</literal> allows OpenShift Virtualization to request up to two additional dedicated CPUs to bring the total CPU count to an even parity when using
emulator thread isolation.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create an <literal>SriovNetworkNodePolicy</literal> object with the <literal>spec.deviceType</literal> field set to <literal>vfio-pci</literal>:</simpara>
<formalpara>
<title>Example <literal>SriovNetworkNodePolicy</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: policy-1
  namespace: openshift-sriov-network-operator
spec:
  resourceName: intel_nics_dpdk
  deviceType: vfio-pci
  mtu: 9000
  numVfs: 4
  priority: 99
  nicSelector:
    vendor: "8086"
    deviceID: "1572"
    pfNames:
      - eno3
    rootDevices:
      - "0000:19:00.2"
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true"</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources_configuring-cluster-dpdk">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#using-cpu-manager">Using CPU Manager and Topology Manager</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/scalability_and_performance/#configuring-huge-pages_huge-pages">Configuring huge pages</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/5688941">Creating a custom machine config pool</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-configuring-vm-project-dpdk_virt-using-dpdk-with-sriov">
<title>Configuring a project for DPDK workloads</title>
<simpara>You can configure the project to run DPDK workloads on SR-IOV hardware.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster is configured to run DPDK workloads.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a namespace for your DPDK applications:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create ns dpdk-checkup-ns</programlisting>
</listitem>
<listitem>
<simpara>Create an <literal>SriovNetwork</literal> object that references the <literal>SriovNetworkNodePolicy</literal> object. When you create an <literal>SriovNetwork</literal> object, the SR-IOV Network Operator automatically creates a <literal>NetworkAttachmentDefinition</literal> object.</simpara>
<formalpara>
<title>Example <literal>SriovNetwork</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetwork
metadata:
  name: dpdk-sriovnetwork
  namespace: openshift-sriov-network-operator
spec:
  ipam: |
    {
      "type": "host-local",
      "subnet": "10.56.217.0/24",
      "rangeStart": "10.56.217.171",
      "rangeEnd": "10.56.217.181",
      "routes": [{
        "dst": "0.0.0.0/0"
      }],
      "gateway": "10.56.217.1"
    }
  networkNamespace: dpdk-checkup-ns <co xml:id="CO90-1"/>
  resourceName: intel_nics_dpdk <co xml:id="CO90-2"/>
  spoofChk: "off"
  trust: "on"
  vlan: 1019</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO90-1">
<para>The namespace where the <literal>NetworkAttachmentDefinition</literal> object is deployed.</para>
</callout>
<callout arearefs="CO90-2">
<para>The value of the <literal>spec.resourceName</literal> attribute of the <literal>SriovNetworkNodePolicy</literal> object that was created when configuring the cluster for DPDK workloads.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: Run the virtual machine latency checkup to verify that the network is properly configured.</simpara>
</listitem>
<listitem>
<simpara>Optional: Run the DPDK checkup to verify that the namespace is ready for DPDK workloads.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources_configuring-project-dpdk">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#working-with-projects">Working with projects</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-measuring-latency-vm-secondary-network_virt-running-cluster-checkups">Virtual machine latency checkup</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-checking-cluster-dpdk-readiness_virt-running-cluster-checkups">DPDK checkup</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-configuring-vm-dpdk_virt-using-dpdk-with-sriov">
<title>Configuring a virtual machine for DPDK workloads</title>
<simpara>You can run Data Packet Development Kit (DPDK) workloads on virtual machines (VMs) to achieve lower latency and higher throughput for faster packet processing in the user space. DPDK uses the SR-IOV network for hardware-based I/O sharing.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your cluster is configured to run DPDK workloads.</simpara>
</listitem>
<listitem>
<simpara>You have created and configured the project in which the VM will run.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest to include information about the SR-IOV network interface, CPU topology, CRI-O annotations, and huge pages:</simpara>
<formalpara>
<title>Example <literal>VirtualMachine</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: rhel-dpdk-vm
spec:
  running: true
  template:
    metadata:
      annotations:
        cpu-load-balancing.crio.io: disable <co xml:id="CO91-1"/>
        cpu-quota.crio.io: disable <co xml:id="CO91-2"/>
        irq-load-balancing.crio.io: disable <co xml:id="CO91-3"/>
    spec:
      domain:
        cpu:
          sockets: 1 <co xml:id="CO91-4"/>
          cores: 5 <co xml:id="CO91-5"/>
          threads: 2
          dedicatedCpuPlacement: true
          isolateEmulatorThread: true
        interfaces:
          - masquerade: {}
            name: default
          - model: virtio
            name: nic-east
            pciAddress: '0000:07:00.0'
            sriov: {}
          networkInterfaceMultiqueue: true
          rng: {}
      memory:
        hugepages:
          pageSize: 1Gi <co xml:id="CO91-6"/>
          guest: 8Gi
      networks:
        - name: default
          pod: {}
        - multus:
            networkName: dpdk-net <co xml:id="CO91-7"/>
          name: nic-east
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO91-1">
<para>This annotation specifies that load balancing is disabled for CPUs that are used by the container.</para>
</callout>
<callout arearefs="CO91-2">
<para>This annotation specifies that the CPU quota is disabled for CPUs that are used by the container.</para>
</callout>
<callout arearefs="CO91-3">
<para>This annotation specifies that Interrupt Request (IRQ) load balancing is disabled for CPUs that are used by the container.</para>
</callout>
<callout arearefs="CO91-4">
<para>The number of sockets inside the VM. This field must be set to <literal>1</literal> for the CPUs to be scheduled from the same Non-Uniform Memory Access (NUMA) node.</para>
</callout>
<callout arearefs="CO91-5">
<para>The number of cores inside the VM. This must be a value greater than or equal to <literal>1</literal>. In this example, the VM is scheduled with 5 hyper-threads or 10 CPUs.</para>
</callout>
<callout arearefs="CO91-6">
<para>The size of the huge pages. The possible values for x86-64 architecture are 1Gi and 2Mi. In this example, the request is for 8 huge pages of size 1Gi.</para>
</callout>
<callout arearefs="CO91-7">
<para>The name of the SR-IOV <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save and exit the editor.</simpara>
</listitem>
<listitem>
<simpara>Apply the <literal>VirtualMachine</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Configure the guest operating system. The following example shows the configuration steps for RHEL 8 OS:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Configure huge pages by using the GRUB bootloader command-line interface. In the following example, 8 1G huge pages are specified.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ grubby --update-kernel=ALL --args="default_hugepagesz=1GB hugepagesz=1G hugepages=8"</programlisting>
</listitem>
<listitem>
<simpara>To achieve low-latency tuning by using the <literal>cpu-partitioning</literal> profile in the TuneD application, run the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ dnf install -y tuned-profiles-cpu-partitioning</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ echo isolated_cores=2-9 &gt; /etc/tuned/cpu-partitioning-variables.conf</programlisting>
<simpara>The first two CPUs (0 and 1) are set aside for house keeping tasks and the rest are isolated for the DPDK application.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ tuned-adm profile cpu-partitioning</programlisting>
</listitem>
<listitem>
<simpara>Override the SR-IOV NIC driver by using the <literal>driverctl</literal> device driver control utility:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ dnf install -y driverctl</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ driverctl set-override 0000:07:00.0 vfio-pci</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Restart the VM to apply the changes.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-connecting-vm-to-ovn-secondary-network">
<title>Connecting a virtual machine to an OVN-Kubernetes secondary network</title>

<simpara>You can connect a virtual machine (VM) to an Open Virtual Network (OVN)-Kubernetes secondary network. OpenShift Virtualization supports the layer 2 and localnet topologies for OVN-Kubernetes.</simpara>
<itemizedlist>
<listitem>
<simpara>A layer 2 topology connects workloads by a cluster-wide logical switch. The OVN-Kubernetes Container Network Interface (CNI) plug-in uses the Geneve (Generic Network Virtualization Encapsulation) protocol to create an overlay network between nodes. You can use this overlay network to connect VMs on different nodes, without having to configure any additional physical networking infrastructure.</simpara>
</listitem>
<listitem>
<simpara>A localnet topology connects the secondary network to the physical underlay. This enables both east-west cluster traffic and access to services running outside the cluster, but it requires additional configuration of the underlying Open vSwitch (OVS) system on cluster nodes.</simpara>
</listitem>
</itemizedlist>
<simpara>To configure an OVN-Kubernetes secondary network and attach a VM to that network, perform the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara><link linkend="creating-ovn-nad">Configure an OVN-Kubernetes secondary network</link> by creating a network attachment definition (NAD).</simpara>
<note>
<simpara>For localnet topology, you must <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#configuring-additional-network_ovn-kubernetes-configuration-for-a-localnet-topology">configure an OVS bridge</link> by creating a <literal>NodeNetworkConfigurationPolicy</literal> object before creating the NAD.</simpara>
</note>
</listitem>
<listitem>
<simpara><link linkend="attaching-vm-to-ovn-secondary-nw">Connect the VM to the OVN-Kubernetes secondary network</link> by adding the network details to the VM specification.</simpara>
</listitem>
</orderedlist>
<section xml:id="creating-ovn-nad">
<title>Creating an OVN-Kubernetes NAD</title>
<simpara>You can create an OVN-Kubernetes layer 2 or localnet network attachment definition (NAD) by using the OpenShift Container Platform web console or the CLI.</simpara>
<note>
<simpara>Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.</simpara>
</note>
<section xml:id="virt-creating-layer2-nad-cli_virt-connecting-vm-to-ovn-secondary-network">
<title>Creating a NAD for layer 2 topology using the CLI</title>
<simpara>You can create a network attachment definition (NAD) which describes how to attach a pod to the layer 2 overlay network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>NetworkAttachmentDefinition</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: l2-network
  namespace: my-namespace
spec:
  config: |2
    {
            "cniVersion": "0.3.1", <co xml:id="CO92-1"/>
            "name": "my-namespace-l2-network", <co xml:id="CO92-2"/>
            "type": "ovn-k8s-cni-overlay", <co xml:id="CO92-3"/>
            "topology":"layer2", <co xml:id="CO92-4"/>
            "mtu": 1300, <co xml:id="CO92-5"/>
            "netAttachDefName": "my-namespace/l2-network" <co xml:id="CO92-6"/>
    }</programlisting>
<calloutlist>
<callout arearefs="CO92-1">
<para>The CNI specification version. The required value is <literal>0.3.1</literal>.</para>
</callout>
<callout arearefs="CO92-2">
<para>The name of the network. This attribute is not namespaced. For example, you can have a network named <literal>l2-network</literal> referenced from two different <literal>NetworkAttachmentDefinition</literal> objects that exist in two different namespaces. This feature is useful to connect VMs in different namespaces.</para>
</callout>
<callout arearefs="CO92-3">
<para>The name of the CNI plug-in to be configured. The required value is <literal>ovn-k8s-cni-overlay</literal>.</para>
</callout>
<callout arearefs="CO92-4">
<para>The topological configuration for the network. The required value is <literal>layer2</literal>.</para>
</callout>
<callout arearefs="CO92-5">
<para>Optional: The maximum transmission unit (MTU) value. The default value is automatically set by the kernel.</para>
</callout>
<callout arearefs="CO92-6">
<para>The value of the <literal>namespace</literal> and <literal>name</literal> fields in the <literal>metadata</literal> stanza of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
</calloutlist>
<note>
<simpara>The above example configures a cluster-wide overlay without a subnet defined. This means that the logical switch implementing the network only provides layer 2 communication. You must configure an IP address when you create the virtual machine by either setting a static IP address or by deploying a DHCP server on the network for a dynamic IP address.</simpara>
</note>
</listitem>
<listitem>
<simpara>Apply the manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;filename&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-localnet-nad-cli_virt-connecting-vm-to-ovn-secondary-network">
<title>Creating a NAD for localnet topology using the CLI</title>
<simpara>You can create a network attachment definition (NAD) which describes how to attach a pod to the underlying physical network.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have installed the Kubernetes NMState Operator.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>NodeNetworkConfigurationPolicy</literal> object to map the OVN-Kubernetes secondary network to an Open vSwitch (OVS) bridge.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>NetworkAttachmentDefinition</literal> object:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: localnet-network
  namespace: default
spec:
  config: |2
    {
            "cniVersion": "0.3.1", <co xml:id="CO93-1"/>
            "name": "localnet-network", <co xml:id="CO93-2"/>
            "type": "ovn-k8s-cni-overlay", <co xml:id="CO93-3"/>
            "topology": "localnet", <co xml:id="CO93-4"/>
            "netAttachDefName": "default/localnet-network" <co xml:id="CO93-5"/>
    }</programlisting>
<calloutlist>
<callout arearefs="CO93-1">
<para>The CNI specification version. The required value is <literal>0.3.1</literal>.</para>
</callout>
<callout arearefs="CO93-2">
<para>The name of the network. This attribute must match the value of the <literal>spec.desiredState.ovn.bridge-mappings.localnet</literal> field of the <literal>NodeNetworkConfigurationPolicy</literal> object that defines the OVS bridge mapping.</para>
</callout>
<callout arearefs="CO93-3">
<para>The name of the CNI plug-in to be configured. The required value is <literal>ovn-k8s-cni-overlay</literal>.</para>
</callout>
<callout arearefs="CO93-4">
<para>The topological configuration for the network. The required value is <literal>localnet</literal>.</para>
</callout>
<callout arearefs="CO93-5">
<para>The value of the <literal>namespace</literal> and <literal>name</literal> fields in the <literal>metadata</literal> stanza of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;filename&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="attaching-vm-to-ovn-secondary-nw">
<title>Attaching a virtual machine to the OVN-Kubernetes secondary network</title>
<simpara>You can attach a virtual machine (VM) to the OVN-Kubernetes secondary network interface by using the OpenShift Container Platform web console or the CLI.</simpara>
<section xml:id="virt-attaching-vm-to-ovn-secondary-nw-cli_virt-connecting-vm-to-ovn-secondary-network">
<title>Attaching a virtual machine to an OVN-Kubernetes secondary network using the CLI</title>
<simpara>You can connect a virtual machine (VM) to the OVN-Kubernetes secondary network by including the network details in the VM configuration.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> manifest to add the OVN-Kubernetes secondary network interface details, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: vm-server
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          interfaces:
          - name: default
            masquerade: {}
          - name: secondary <co xml:id="CO94-1"/>
            bridge: {}
        resources:
          requests:
            memory: 1024Mi
      networks:
      - name: default
        pod: {}
      - name: secondary  <co xml:id="CO94-2"/>
        multus:
          networkName: &lt;nad_name&gt; <co xml:id="CO94-3"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO94-1">
<para>The name of the OVN-Kubernetes secondary interface.</para>
</callout>
<callout arearefs="CO94-2">
<para>The name of the network. This must match the value of the <literal>spec.template.spec.domain.devices.interfaces.name</literal> field.</para>
</callout>
<callout arearefs="CO94-3">
<para>The name of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>VirtualMachine</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;filename&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Optional: If you edited a running virtual machine, you must restart it for the changes to take effect.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_virt-connecting-vm-to-ovn-secondary-network" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#configuration-ovnk-additional-networks_configuring-additional-network">Configuration for an OVN-Kubernetes additional network</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#k8s-nmstate-about-the-k8s-nmstate-operator">About the Kubernetes NMState Operator</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-hot-plugging-network-interfaces">
<title>Hot plugging secondary network interfaces</title>

<simpara>You can add or remove secondary network interfaces without stopping your virtual machine (VM). OpenShift Virtualization supports hot plugging and hot unplugging for Linux bridge interfaces that use the VirtIO device driver.</simpara>
<important>
<simpara>Hot plugging and hot unplugging bridge network interfaces is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="virtio-limitations_virt-hot-plugging-network-interfaces">
<title>VirtIO limitations</title>
<simpara>Each VirtIO interface uses one of the limited Peripheral Connect Interface (PCI) slots in the VM. There are a total of 32 slots available. The PCI slots are also used by other devices and must be reserved in advance, therefore slots might not be available on demand. OpenShift Virtualization reserves up to four slots for hot plugging interfaces. This includes any existing plugged network interfaces. For example, if your VM has two existing plugged interfaces, you can hot plug two more network interfaces.</simpara>
<note>
<simpara>The actual number of slots available for hot plugging also depends on the machine type. For example, the default PCI topology for the q35 machine type supports hot plugging one additional PCIe device. For more information on PCI topology and hot plug support, see the <link xlink:href="https://libvirt.org/pci-hotplug.html">libvirt documentation</link>.</simpara>
</note>
<simpara>If you restart the VM after hot plugging an interface, that interface becomes part of the standard network interfaces.</simpara>
</section>
<section xml:id="virt-hot-plugging-bridge-network-interface_virt-hot-plugging-network-interfaces">
<title>Hot plugging a bridge network interface using the CLI</title>
<simpara>Hot plug a bridge network interface to a virtual machine (VM) while the VM is running.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A network attachment definition is configured in the same namespace as your VM.</simpara>
</listitem>
<listitem>
<simpara>You have installed the <literal>virtctl</literal> tool.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If the VM to which you want to hot plug the network interface is not running, start it by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl start &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Use the following command to hot plug a new network interface to the running VM. The <literal>virtctl addinterface</literal> command adds the new network interface to the VM and virtual machine instance (VMI) specification but does not attach it to the running VM.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl addinterface &lt;vm_name&gt; --network-attachment-definition-name &lt;net_attach_def_name&gt; --name &lt;interface_name&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;vm_name&gt;</term>
<listitem>
<simpara>Specifies the name of the <literal>VirtualMachine</literal> object.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>&lt;net_attach_def_name&gt;</term>
<listitem>
<simpara>Specifies the name of <literal>NetworkAttachmentDefinition</literal> object.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>&lt;interface_name&gt;</term>
<listitem>
<simpara>Specifies the name of the new network interface.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>To attach the network interface to the running VM, live migrate the VM by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl migrate &lt;vm_name&gt;</programlisting>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the VM live migration is successful by using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get VirtualMachineInstanceMigration -w</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                        PHASE             VMI
kubevirt-migrate-vm-lj62q   Scheduling        vm-fedora
kubevirt-migrate-vm-lj62q   Scheduled         vm-fedora
kubevirt-migrate-vm-lj62q   PreparingTarget   vm-fedora
kubevirt-migrate-vm-lj62q   TargetReady       vm-fedora
kubevirt-migrate-vm-lj62q   Running           vm-fedora
kubevirt-migrate-vm-lj62q   Succeeded         vm-fedora</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the new interface is added to the VM by checking the VMI status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi vm-fedora -ojsonpath="{ @.status.interfaces }"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">[
  {
    "infoSource": "domain, guest-agent",
    "interfaceName": "eth0",
    "ipAddress": "10.130.0.195",
    "ipAddresses": [
      "10.130.0.195",
      "fd02:0:0:3::43c"
    ],
    "mac": "52:54:00:0e:ab:25",
    "name": "default",
    "queueCount": 1
  },
  {
    "infoSource": "domain, guest-agent, multus-status",
    "interfaceName": "eth1",
    "mac": "02:d8:b8:00:00:2a",
    "name": "bridge-interface", <co xml:id="CO95-1"/>
    "queueCount": 1
  }
]</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO95-1">
<para>The hot plugged interface appears in the VMI status.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-hot-unplugging-bridge-network-interface_virt-hot-plugging-network-interfaces">
<title>Hot unplugging a bridge network interface using the CLI</title>
<simpara>You can remove a bridge network interface from a running virtual machine (VM).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your VM must be running.</simpara>
</listitem>
<listitem>
<simpara>The VM must be created on a cluster running OpenShift Virtualization 4.14 or later.</simpara>
</listitem>
<listitem>
<simpara>The VM must have a bridge network interface attached.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Hot unplug a bridge network interface by running the following command. The <literal>virtctl removeinterface</literal> command detaches the network interface from the guest, but the interface still exists in the pod.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl removeinterface &lt;vm_name&gt; --name &lt;interface_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Remove the interface from the pod by migrating the VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl migrate &lt;vm_name&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_virt-hot-plugging-network-interfaces" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="installing-virtctl_virt-using-the-cli-tools">Installing virtctl</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="creating-linux-bridge-nad">Creating a Linux bridge network attachment definition</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-connecting-vm-to-service-mesh">
<title>Connecting a virtual machine to a service mesh</title>

<simpara>OpenShift Virtualization is now integrated with OpenShift Service Mesh. You can monitor, visualize, and control traffic between pods that run virtual machine workloads on the default pod network with IPv4.</simpara>
<section xml:id="virt-adding-vm-to-service-mesh_virt-connecting-vm-to-service-mesh">
<title>Adding a virtual machine to a service mesh</title>
<simpara>To add a virtual machine (VM) workload to a service mesh, enable automatic sidecar injection in the VM configuration file by setting the <literal>sidecar.istio.io/inject</literal> annotation to <literal>true</literal>. Then expose your VM as a service to view your application in the mesh.</simpara>
<important>
<simpara>To avoid port conflicts, do not use ports used by the Istio sidecar proxy. These include ports 15000, 15001, 15006, 15008, 15020, 15021, and 15090.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the Service Mesh Operators.</simpara>
</listitem>
<listitem>
<simpara>You created the Service Mesh control plane.</simpara>
</listitem>
<listitem>
<simpara>You added the VM project to the Service Mesh member roll.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the VM configuration file to add the <literal>sidecar.istio.io/inject: "true"</literal> annotation:</simpara>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: vm-istio
  name: vm-istio
spec:
  runStrategy: Always
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-istio
        app: vm-istio <co xml:id="CO96-1"/>
      annotations:
        sidecar.istio.io/inject: "true" <co xml:id="CO96-2"/>
    spec:
      domain:
        devices:
          interfaces:
          - name: default
            masquerade: {} <co xml:id="CO96-3"/>
          disks:
          - disk:
              bus: virtio
            name: containerdisk
          - disk:
              bus: virtio
            name: cloudinitdisk
        resources:
          requests:
            memory: 1024M
      networks:
      - name: default
        pod: {}
      terminationGracePeriodSeconds: 180
      volumes:
      - containerDisk:
          image: registry:5000/kubevirt/fedora-cloud-container-disk-demo:devel
        name: containerdisk</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO96-1">
<para>The key/value pair (label) that must be matched to the service selector attribute.</para>
</callout>
<callout arearefs="CO96-2">
<para>The annotation to enable automatic sidecar injection.</para>
</callout>
<callout arearefs="CO96-3">
<para>The binding method (masquerade mode) for use with the default pod network.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the VM configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f &lt;vm_name&gt;.yaml <co xml:id="CO97-1"/></programlisting>
<calloutlist>
<callout arearefs="CO97-1">
<para>The name of the virtual machine YAML file.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>Service</literal> object to expose your VM to the service mesh.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  apiVersion: v1
  kind: Service
  metadata:
    name: vm-istio
  spec:
    selector:
      app: vm-istio <co xml:id="CO98-1"/>
    ports:
      - port: 8080
        name: http
        protocol: TCP</programlisting>
<calloutlist>
<callout arearefs="CO98-1">
<para>The service selector that determines the set of pods targeted by a service. This attribute corresponds to the <literal>spec.metadata.labels</literal> field in the VM configuration file. In the above example, the <literal>Service</literal> object named <literal>vm-istio</literal> targets TCP port 8080 on any pod with the label <literal>app=vm-istio</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;service_name&gt;.yaml <co xml:id="CO99-1"/></programlisting>
<calloutlist>
<callout arearefs="CO99-1">
<para>The name of the service YAML file.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_virt-connecting-vm-to-service-mesh" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/service_mesh/#installing-ossm">Installing the Service Mesh Operators</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/service_mesh/#ossm-create-smcp">Creating the Service Mesh control plane</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/service_mesh/#ossm-create-mesh">Adding projects to the Service Mesh member roll</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-dedicated-network-live-migration">
<title>Configuring a dedicated network for live migration</title>

<simpara>You can configure a dedicated <link linkend="virt-connecting-vm-to-linux-bridge">Multus network</link> for live migration. A dedicated network minimizes the effects of network saturation on tenant workloads during live migration.</simpara>
<section xml:id="virt-configuring-secondary-network-vm-live-migration_virt-dedicated-network-live-migration">
<title>Configuring a dedicated secondary network for live migration</title>
<simpara>To configure a dedicated secondary network for live migration, you must first create a bridge network attachment definition (NAD) by using the CLI. Then, you add the name of the <literal>NetworkAttachmentDefinition</literal> object to the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You logged in to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>Each node has at least two Network Interface Cards (NICs).</simpara>
</listitem>
<listitem>
<simpara>The NICs for live migration are connected to the same VLAN.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>NetworkAttachmentDefinition</literal> manifest according to the following example:</simpara>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: my-secondary-network <co xml:id="CO100-1"/>
  namespace: openshift-cnv <co xml:id="CO100-2"/>
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "migration-bridge",
    "type": "macvlan",
    "master": "eth1", <co xml:id="CO100-3"/>
    "mode": "bridge",
    "ipam": {
      "type": "whereabouts", <co xml:id="CO100-4"/>
      "range": "10.200.5.0/24" <co xml:id="CO100-5"/>
    }
  }'</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO100-1">
<para>Specify the name of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO100-2 CO100-3">
<para>Specify the name of the NIC to be used for live migration.</para>
</callout>
<callout arearefs="CO100-4">
<para>Specify the name of the CNI plugin that provides the network for the NAD.</para>
</callout>
<callout arearefs="CO100-5">
<para>Specify an IP address range for the secondary network. This range must not overlap the IP addresses of the main network.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Add the name of the <literal>NetworkAttachmentDefinition</literal> object to the <literal>spec.liveMigrationConfig</literal> stanza of the <literal>HyperConverged</literal> CR:</simpara>
<formalpara>
<title>Example <literal>HyperConverged</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  liveMigrationConfig:
    completionTimeoutPerGiB: 800
    network: &lt;network&gt; <co xml:id="CO101-1"/>
    parallelMigrationsPerCluster: 5
    parallelOutboundMigrationsPerNode: 2
    progressTimeout: 150
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO101-1">
<para>Specify the name of the Multus <literal>NetworkAttachmentDefinition</literal> object to be used for live migrations.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save your changes and exit the editor. The <literal>virt-handler</literal> pods restart and connect to the secondary network.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>When the node that the virtual machine runs on is placed into maintenance mode, the VM automatically migrates to another node in the cluster. You can verify that the migration occurred over the secondary network and not the default pod network by checking the target IP address in the virtual machine instance (VMI) metadata.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi &lt;vmi_name&gt; -o jsonpath='{.status.migrationState.targetNodeAddress}'</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-selecting-migration-network-ui_virt-dedicated-network-live-migration">
<title>Selecting a dedicated network by using the web console</title>
<simpara>You can select a dedicated network for live migration by using the OpenShift Container Platform web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You configured a Multus network for live migration.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization &gt; Overview</emphasis> in the OpenShift Container Platform web console.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Settings</emphasis> tab and then click <emphasis role="strong">Live migration</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the network from the <emphasis role="strong">Live migration network</emphasis> list.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_virt-migrating-vm-on-secondary-network" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../virt/live_migration/virt-configuring-live-migration.xml#virt-configuring-live-migration-limits_virt-configuring-live-migration">Configuring live migration limits and timeouts</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-viewing-ips-for-vms">
<title>Configuring and viewing IP addresses</title>

<simpara>You can configure an IP address when you create a virtual machine (VM). The IP address is provisioned with cloud-init.</simpara>
<simpara>You can view the IP address of a VM by using the OpenShift Container Platform web console or the command line. The network information is collected by the QEMU guest agent.</simpara>
<section xml:id="configuring-ips_virt-configuring-viewing-ips-for-vms">
<title>Configuring IP addresses for virtual machines</title>
<simpara>You can configure a static IP address when you create a virtual machine (VM) by using the web console or the command line.</simpara>
<simpara>You can configure a dynamic IP address when you create a VM by using the command line.</simpara>
<simpara>The IP address is provisioned with cloud-init.</simpara>
<section xml:id="virt-configuring-ip-vm-cli_virt-configuring-viewing-ips-for-vms">
<title>Configuring an IP address when creating a virtual machine by using the command line</title>
<simpara>You can configure a static or dynamic IP address when you create a virtual machine (VM). The IP address is provisioned with cloud-init.</simpara>
<note>
<simpara>If the VM is connected to the pod network, the pod network interface is the default route unless you update it.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The virtual machine is connected to a secondary network.</simpara>
</listitem>
<listitem>
<simpara>You have a DHCP server available on the secondary network to configure a dynamic IP for the virtual machine.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.template.spec.volumes.cloudInitNoCloud.networkData</literal> stanza of the virtual machine configuration:</simpara>
<itemizedlist>
<listitem>
<simpara>To configure a dynamic IP address, specify the interface name and enable DHCP:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: VirtualMachine
spec:
# ...
  template:
  # ...
    spec:
      volumes:
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth1: <co xml:id="CO102-1"/>
                dhcp4: true</programlisting>
<calloutlist>
<callout arearefs="CO102-1">
<para>Specify the interface name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>To configure a static IP, specify the interface name and the IP address:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: VirtualMachine
spec:
# ...
  template:
  # ...
    spec:
      volumes:
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth1: <co xml:id="CO103-1"/>
                addresses:
                - 10.10.10.14/24 <co xml:id="CO103-2"/></programlisting>
<calloutlist>
<callout arearefs="CO103-1">
<para>Specify the interface name.</para>
</callout>
<callout arearefs="CO103-2">
<para>Specify the static IP address.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="viewing-ips_virt-configuring-viewing-ips-for-vms">
<title>Viewing IP addresses of virtual machines</title>
<simpara>You can view the IP address of a VM by using the OpenShift Container Platform web console or the command line.</simpara>
<simpara>The network information is collected by the QEMU guest agent.</simpara>
<section xml:id="virt-viewing-vmi-ip-web_virt-configuring-viewing-ips-for-vms">
<title>Viewing the IP address of a virtual machine by using the web console</title>
<simpara>You can view the IP address of a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<note>
<simpara>You must install the QEMU guest agent on a VM to view the IP address of a secondary network interface. A pod network interface does not require the QEMU guest agent.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform console, click <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> from the side menu.</simpara>
</listitem>
<listitem>
<simpara>Select a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Details</emphasis> tab to view the IP address.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-viewing-vmi-ip-cli_virt-configuring-viewing-ips-for-vms">
<title>Viewing the IP address of a virtual machine by using the command line</title>
<simpara>You can view the IP address of a virtual machine (VM) by using the command line.</simpara>
<note>
<simpara>You must install the QEMU guest agent on a VM to view the IP address of a secondary network interface. A pod network interface does not require the QEMU guest agent.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Obtain the virtual machine instance configuration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe vmi &lt;vmi_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
Interfaces:
   Interface Name:  eth0
   Ip Address:      10.244.0.37/24
   Ip Addresses:
     10.244.0.37/24
     fe80::858:aff:fef4:25/64
   Mac:             0a:58:0a:f4:00:25
   Name:            default
   Interface Name:  v2
   Ip Address:      1.1.1.7/24
   Ip Addresses:
     1.1.1.7/24
     fe80::f4d9:70ff:fe13:9089/64
   Mac:             f6:d9:70:13:90:89
   Interface Name:  v1
   Ip Address:      1.1.1.1/24
   Ip Addresses:
     1.1.1.1/24
     1.1.1.2/24
     1.1.1.4/24
     2001:de7:0:f101::1/64
     2001:db8:0:f101::1/64
     fe80::1420:84ff:fe10:17aa/64
   Mac:             16:20:84:10:17:aa</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_virt-configuring-viewing-ips-for-vms" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-installing-qemu-guest-agent">Installing the QEMU guest agent</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-accessing-vm-secondary-network-fqdn">
<title>Accessing a virtual machine by using the cluster FQDN</title>

<simpara>You can access a virtual machine (VM) that is attached to a secondary network interface from outside the cluster by using the fully qualified domain name (FQDN) of the cluster.</simpara>
<important>
<simpara>Accessing VMs by using the cluster FQDN is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="virt-configuring-secondary-dns-server_virt-accessing-vm-secondary-network-fqdn">
<title>Configuring a DNS server for secondary networks</title>
<simpara>The Cluster Network Addons Operator (CNAO) deploys a Domain Name Server (DNS) server and monitoring components when you enable the <literal>deployKubeSecondaryDNS</literal> feature gate in the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You configured a load balancer for the cluster.</simpara>
</listitem>
<listitem>
<simpara>You logged in to the cluster with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a load balancer service to expose the DNS server outside the cluster by running the <literal>oc expose</literal> command according to the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose -n openshift-cnv deployment/secondary-dns --name=dns-lb \
  --type=LoadBalancer --port=53 --target-port=5353 --protocol='UDP'</programlisting>
</listitem>
<listitem>
<simpara>Retrieve the external IP address by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get service -n openshift-cnv</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME       TYPE             CLUSTER-IP     EXTERNAL-IP      PORT(S)          AGE
dns-lb     LoadBalancer     172.30.27.5    10.46.41.94      53:31829/TCP     5s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Enable the DNS server and monitoring components according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
    featureGates:
      deployKubeSecondaryDNS: true
    kubeSecondaryDNSNameServerIP: "10.46.41.94" <co xml:id="CO104-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO104-1">
<para>Specify the external IP address exposed by the load balancer service.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file and exit the editor.</simpara>
</listitem>
<listitem>
<simpara>Retrieve the cluster FQDN by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"> $ oc get dnses.config.openshift.io cluster -o jsonpath='{.spec.baseDomain}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">openshift.example.com</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Point to the DNS server by using one of the following methods:</simpara>
<itemizedlist>
<listitem>
<simpara>Add the <literal>kubeSecondaryDNSNameServerIP</literal> value to the <literal>resolv.conf</literal> file on your local machine.</simpara>
<note>
<simpara>Editing the <literal>resolv.conf</literal> file overwrites existing DNS settings.</simpara>
</note>
</listitem>
<listitem>
<simpara>Add the <literal>kubeSecondaryDNSNameServerIP</literal> value and the cluster FQDN to the enterprise DNS server records. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">vm.&lt;FQDN&gt;. IN NS ns.vm.&lt;FQDN&gt;.</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">ns.vm.&lt;FQDN&gt;. IN A 10.46.41.94</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-connecting-vm-secondarynw-fqdn_virt-accessing-vm-secondary-network-fqdn">
<title>Connecting to a VM on a secondary network by using the cluster FQDN</title>
<simpara>You can access a running virtual machine (VM) attached to a secondary network interface by using the fully qualified domain name (FQDN) of the cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the QEMU guest agent on the VM.</simpara>
</listitem>
<listitem>
<simpara>The IP address of the VM is public.</simpara>
</listitem>
<listitem>
<simpara>You configured the DNS server for secondary networks.</simpara>
</listitem>
<listitem>
<simpara>You retrieved the fully qualified domain name (FQDN) of the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the network interface name from the VM configuration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vm -n &lt;namespace&gt; &lt;vm_name&gt; -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
  namespace: example-namespace
spec:
  running: true
  template:
    spec:
      domain:
        devices:
          interfaces:
            - bridge: {}
              name: example-nic
# ...
      networks:
      - multus:
          networkName: bridge-conf
        name: example-nic <co xml:id="CO105-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO105-1">
<para>Note the name of the network interface.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Connect to the VM by using the <literal>ssh</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ssh &lt;user_name&gt;@&lt;interface_name&gt;.&lt;vm_name&gt;.&lt;namespace&gt;.vm.&lt;cluster_fqdn&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_accessing-vm-secondary-network-fqdn" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#configuring-ingress-cluster-traffic-load-balancer">Configuring ingress cluster traffic using a load balancer</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#about-metallb">Load balancing with MetalLB</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="configuring-ips_virt-configuring-viewing-ips-for-vms">Configuring IP addresses for virtual machines</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-using-mac-address-pool-for-vms">
<title>Managing MAC address pools for network interfaces</title>

<simpara>The <emphasis>KubeMacPool</emphasis> component allocates MAC addresses for virtual machine (VM) network interfaces from a shared MAC address pool. This ensures that each network interface is assigned a unique MAC address.</simpara>
<simpara>A virtual machine instance created from that VM retains the assigned MAC address across reboots.</simpara>
<note>
<simpara>KubeMacPool does not handle virtual machine instances created independently from a virtual machine.</simpara>
</note>
<section xml:id="virt-managing-kubemacpool-cli_virt-using-mac-address-pool-for-vms">
<title>Managing KubeMacPool by using the command line</title>
<simpara>You can disable and re-enable KubeMacPool by using the command line.</simpara>
<simpara>KubeMacPool is enabled by default.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To disable KubeMacPool in two namespaces, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace &lt;namespace1&gt; &lt;namespace2&gt; mutatevirtualmachines.kubemacpool.io=ignore</programlisting>
</listitem>
<listitem>
<simpara>To re-enable KubeMacPool in two namespaces, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label namespace &lt;namespace1&gt; &lt;namespace2&gt; mutatevirtualmachines.kubemacpool.io-</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_storage">
<title>Storage</title>
<section xml:id="virt-storage-config-overview">
<title>Storage configuration overview</title>

<simpara>You can configure a default storage class, storage profiles, Containerized Data Importer (CDI), data volumes, and automatic boot source updates.</simpara>
<section xml:id="storage-configuration-tasks">
<title>Storage</title>
<simpara>The following storage configuration tasks are mandatory:</simpara>
<variablelist>
<varlistentry>
<term><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/postinstallation_configuration/#defining-storage-classes_post-install-storage-configuration">Configure a default storage class</link></term>
<listitem>
<simpara>You must configure a default storage class for your cluster. Otherwise, the cluster cannot receive automated boot source updates.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-configuring-storage-profile">Configure storage profiles</link></term>
<listitem>
<simpara>You must configure storage profiles if your storage provider is not recognized by CDI. A storage profile provides recommended storage settings based on the associated storage class.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>The following storage configuration tasks are optional:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="virt-reserving-pvc-space-fs-overhead">Reserve additional PVC space for file system overhead</link></term>
<listitem>
<simpara>By default, 5.5% of a file system PVC is reserved for overhead, reducing the space available for VM disks by that amount. You can configure a different overhead value.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-configuring-local-storage-with-hpp">Configure local storage by using the hostpath provisioner</link></term>
<listitem>
<simpara>You can configure local storage for virtual machines by using the hostpath provisioner (HPP). When you install the OpenShift Virtualization Operator, the HPP Operator is automatically installed.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-enabling-user-permissions-to-clone-datavolumes">Configure user permissions to clone data volumes between namespaces</link></term>
<listitem>
<simpara>You can configure RBAC roles to enable users to clone data volumes between namespaces.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="cdi-configuration-tasks">
<title>Containerized Data Importer</title>
<simpara>You can perform the following Containerized Data Importer (CDI) configuration tasks:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="virt-configuring-cdi-for-namespace-resourcequota">Override the resource request limits of a namespace</link></term>
<listitem>
<simpara>You can configure CDI to import, upload, and clone VM disks into namespaces that are subject to CPU and memory resource restrictions.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-preparing-cdi-scratch-space">Configure CDI scratch space</link></term>
<listitem>
<simpara>CDI requires scratch space (temporary storage) to complete some operations, such as importing and uploading VM images. During this process, CDI provisions a scratch space PVC equal to the size of the PVC backing the destination data volume (DV).</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="dv-configuration-tasks">
<title>Data volumes</title>
<simpara>You can perform the following data volume configuration tasks:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="virt-using-preallocation-for-datavolumes">Enable preallocation for data volumes</link></term>
<listitem>
<simpara>CDI can preallocate disk space to improve write performance when creating data volumes. You can enable preallocation for specific data volumes.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-managing-data-volume-annotations">Manage data volume annotations</link></term>
<listitem>
<simpara>Data volume annotations allow you to manage pod behavior. You can add one or more annotations to a data volume, which then propagates to the created importer pods.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="boot-source-configuration">
<title>Boot source updates</title>
<simpara>You can perform the following boot source update configuration task:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="virt-automatic-bootsource-updates">Manage automatic boot source updates</link></term>
<listitem>
<simpara>Boot sources can make virtual machine (VM) creation more accessible and efficient for users. If automatic boot source updates are enabled, CDI imports, polls, and updates the images so that they are ready to be cloned for new VMs. By default, CDI automatically updates Red Hat boot sources. You can enable automatic updates for custom boot sources.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="virt-configuring-storage-profile">
<title>Configuring storage profiles</title>

<simpara>A storage profile provides recommended storage settings based on the associated storage class. A storage profile is allocated for each storage class.</simpara>
<simpara>If the Containerized Data Importer (CDI) does not recognize your storage provider, you must configure storage profiles.</simpara>
<simpara>For recognized storage types, CDI provides values that optimize the creation of PVCs.  However, you can configure automatic settings for a storage class if you customize the storage profile.</simpara>
<important>
<simpara>When using OpenShift Virtualization with Red Hat OpenShift Data Foundation, specify RBD block mode persistent volume claims (PVCs) when creating virtual machine disks. RBD block mode volumes are more efficient and provide better performance than Ceph FS or RBD filesystem-mode PVCs.</simpara>
<simpara>To specify RBD block mode PVCs, use the 'ocs-storagecluster-ceph-rbd' storage class and <literal>VolumeMode: Block</literal>.</simpara>
</important>
<section xml:id="virt-customizing-storage-profile_virt-configuring-storage-profile">
<title>Customizing the storage profile</title>
<simpara>You can specify default parameters by editing the <literal>StorageProfile</literal> object for the provisioner&#8217;s storage class. These default parameters only apply to the persistent volume claim (PVC) if they are not configured in the <literal>DataVolume</literal> object.</simpara>
<simpara>You cannot modify storage class parameters. To make changes, delete and re-create the storage class. You must then reapply any customizations that were previously made to the storage profile.</simpara>
<simpara>An empty <literal>status</literal> section in a storage profile indicates that a storage provisioner is not recognized by the Containerized Data Interface (CDI). Customizing a storage profile is necessary if you have a storage provisioner that is not recognized by CDI. In this case, the administrator sets appropriate values in the storage profile to ensure successful allocations.</simpara>
<warning>
<simpara>If you create a data volume and omit YAML attributes and these attributes are not defined in the storage profile, then the requested storage will not be allocated and the underlying persistent volume claim (PVC) will not be created.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Ensure that your planned configuration is supported by the storage class and its provider. Specifying an incompatible configuration in a storage profile causes volume provisioning to fail.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the storage profile. In this example, the provisioner is not recognized by CDI:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit -n openshift-cnv storageprofile &lt;storage_class&gt;</programlisting>
<formalpara>
<title>Example storage profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: &lt;unknown_provisioner_class&gt;
# ...
spec: {}
status:
  provisioner: &lt;unknown_provisioner&gt;
  storageClass: &lt;unknown_provisioner_class&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Provide the needed attribute values in the storage profile:</simpara>
<formalpara>
<title>Example storage profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: &lt;unknown_provisioner_class&gt;
# ...
spec:
  claimPropertySets:
  - accessModes:
    - ReadWriteOnce <co xml:id="CO106-1"/>
    volumeMode:
      Filesystem <co xml:id="CO106-2"/>
status:
  provisioner: &lt;unknown_provisioner&gt;
  storageClass: &lt;unknown_provisioner_class&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO106-1">
<para>The <literal>accessModes</literal> that you select.</para>
</callout>
<callout arearefs="CO106-2">
<para>The <literal>volumeMode</literal> that you select.</para>
</callout>
</calloutlist>
<simpara>After you save your changes, the selected values appear in the storage profile <literal>status</literal> element.</simpara>
</listitem>
</orderedlist>
<section xml:id="virt-customizing-storage-profile-default-cloning-strategy_virt-configuring-storage-profile">
<title>Setting a default cloning strategy using a storage profile</title>
<simpara>You can use storage profiles to set a default cloning method for a storage class, creating a <emphasis>cloning strategy</emphasis>. Setting cloning strategies can be helpful, for example, if your storage vendor only supports certain cloning methods. It also allows you to select a method that limits resource usage or maximizes performance.</simpara>
<simpara>Cloning strategies can be specified by setting the <literal>cloneStrategy</literal> attribute in a storage profile to one of these values:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>snapshot</literal> is used by default when snapshots are configured. This cloning strategy uses a temporary volume snapshot to clone the volume. The storage provisioner must support Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara><literal>copy</literal> uses a source pod and a target pod to copy data from the source volume to the target volume. Host-assisted cloning is the least efficient method of cloning.</simpara>
</listitem>
<listitem>
<simpara><literal>csi-clone</literal> uses the CSI clone API to efficiently clone an existing volume without using an interim volume snapshot. Unlike <literal>snapshot</literal> or <literal>copy</literal>, which are used by default if no storage profile is defined, CSI volume cloning is only used when you specify it in the <literal>StorageProfile</literal> object for the provisioner&#8217;s storage class.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>You can also set clone strategies using the CLI without modifying the default <literal>claimPropertySets</literal> in your YAML <literal>spec</literal> section.</simpara>
</note>
<formalpara>
<title>Example storage profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
  name: &lt;provisioner_class&gt;
# ...
spec:
  claimPropertySets:
  - accessModes:
    - ReadWriteOnce <co xml:id="CO107-1"/>
    volumeMode:
      Filesystem <co xml:id="CO107-2"/>
  cloneStrategy: csi-clone <co xml:id="CO107-3"/>
status:
  provisioner: &lt;provisioner&gt;
  storageClass: &lt;provisioner_class&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO107-1">
<para>Specify the access mode.</para>
</callout>
<callout arearefs="CO107-2">
<para>Specify the volume mode.</para>
</callout>
<callout arearefs="CO107-3">
<para>Specify the default cloning strategy.</para>
</callout>
</calloutlist>
</section>
</section>
</section>
<section xml:id="virt-automatic-bootsource-updates">
<title>Managing automatic boot source updates</title>

<simpara>You can manage automatic updates for the following boot sources:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="managing-rh-boot-source-updates_virt-automatic-bootsource-updates">All Red Hat boot sources</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="managing-custom-boot-source-updates_virt-automatic-bootsource-updates">All custom boot sources</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-disable-auto-updates-single-boot-source_virt-automatic-bootsource-updates">Individual Red Hat or custom boot sources</link></simpara>
</listitem>
</itemizedlist>
<simpara>Boot sources can make virtual machine (VM) creation more accessible and efficient for users. If automatic boot source updates are enabled, the Containerized Data Importer (CDI) imports, polls, and updates the images so that they are ready to be cloned for new VMs. By default, CDI automatically updates Red Hat boot sources.</simpara>
<section xml:id="managing-rh-boot-source-updates_virt-automatic-bootsource-updates">
<title>Managing Red Hat boot source updates</title>
<simpara>You can opt out of automatic updates for all system-defined boot sources by disabling the <literal>enableCommonBootImageImport</literal> feature gate. If you disable this feature gate, all <literal>DataImportCron</literal> objects are deleted. This does not remove previously imported boot source objects that store operating system images, though administrators can delete them manually.</simpara>
<simpara>When the <literal>enableCommonBootImageImport</literal> feature gate is disabled, <literal>DataSource</literal> objects are reset so that they no longer point to the original boot source. An administrator can manually provide a boot source by creating a new persistent volume claim (PVC) or volume snapshot for the <literal>DataSource</literal> object, then populating it with an operating system image.</simpara>
<section xml:id="virt-managing-auto-update-all-system-boot-sources_virt-automatic-bootsource-updates">
<title>Managing automatic updates for all system-defined boot sources</title>
<simpara>Disabling automatic boot source imports and updates can lower resource usage. In disconnected environments, disabling automatic boot source updates prevents <literal>CDIDataImportCronOutdated</literal> alerts from filling up logs.</simpara>
<simpara>To disable automatic updates for all system-defined boot sources, turn off the <literal>enableCommonBootImageImport</literal> feature gate by setting the value to <literal>false</literal>. Setting this value to <literal>true</literal> re-enables the feature gate and turns automatic updates back on.</simpara>
<note>
<simpara>Custom boot sources are not affected by this setting.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Toggle the feature gate for automatic boot source updates by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<listitem>
<simpara>To disable automatic boot source updates, set the <literal>spec.featureGates.enableCommonBootImageImport</literal> field in the <literal>HyperConverged</literal> CR to <literal>false</literal>. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op": "replace", "path": \
  "/spec/featureGates/enableCommonBootImageImport", \
  "value": false}]'</programlisting>
</listitem>
<listitem>
<simpara>To re-enable automatic boot source updates, set the <literal>spec.featureGates.enableCommonBootImageImport</literal> field in the <literal>HyperConverged</literal> CR to <literal>true</literal>. For example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hyperconverged kubevirt-hyperconverged -n openshift-cnv \
  --type json -p '[{"op": "replace", "path": \
  "/spec/featureGates/enableCommonBootImageImport", \
  "value": true}]'</programlisting>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="managing-custom-boot-source-updates_virt-automatic-bootsource-updates">
<title>Managing custom boot source updates</title>
<simpara><emphasis>Custom</emphasis> boot sources that are not provided by OpenShift Virtualization are not controlled by the feature gate. You must manage them individually by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<important>
<simpara>You must configure a storage class. Otherwise, the cluster cannot receive automated updates for custom boot sources. See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/postinstallation_configuration/#defining-storage-classes_post-install-storage-configuration">Defining a storage class</link> for details.</simpara>
</important>
<section xml:id="virt-configuring-storage-class-bootsource-update_virt-automatic-bootsource-updates">
<title>Configuring a storage class for custom boot source updates</title>
<simpara>Specify a new default storage class in the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<important>
<simpara>Boot sources are created from storage using the default storage class. If your cluster does not have a default storage class, you must define one before configuring automatic updates for custom boot sources.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Define a new storage class by entering a value in the <literal>storageClassName</literal> field:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  dataImportCronTemplates:
  - metadata:
      name: rhel8-image-cron
    spec:
      template:
        spec:
          storageClassName: &lt;new_storage_class&gt; <co xml:id="CO108-1"/>
#...</programlisting>
<calloutlist>
<callout arearefs="CO108-1">
<para>Define the storage class.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Remove the <literal>storageclass.kubernetes.io/is-default-class</literal> annotation from the current default storage class.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Retrieve the name of the current default storage class by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME PROVISIONER RECLAIMPOLICY VOLUMEBINDINGMODE ALLOWVOLUMEEXPANSION AGE
csi-manila-ceph manila.csi.openstack.org Delete Immediate false 11d
hostpath-csi-basic (default) kubevirt.io.hostpath-provisioner Delete WaitForFirstConsumer false 11d <co xml:id="CO109-1"/>
...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO109-1">
<para>In this example, the current default storage class is named <literal>hostpath-csi-basic</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Remove the annotation from the current default storage class by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageclass &lt;current_default_storage_class&gt; -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"false"}}}' <co xml:id="CO110-1"/></programlisting>
<calloutlist>
<callout arearefs="CO110-1">
<para>Replace <literal>&lt;current_default_storage_class&gt;</literal> with the <literal>storageClassName</literal> value of the default storage class.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Set the new storage class as the default by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageclass &lt;new_storage_class&gt; -p '{"metadata":{"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}' <co xml:id="CO111-1"/></programlisting>
<calloutlist>
<callout arearefs="CO111-1">
<para>Replace <literal>&lt;new_storage_class&gt;</literal> with the <literal>storageClassName</literal> value that you added to the <literal>HyperConverged</literal> CR.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-autoupdate-custom-bootsource_virt-automatic-bootsource-updates">
<title>Enabling automatic updates for custom boot sources</title>
<simpara>OpenShift Virtualization automatically updates system-defined boot sources by default, but does not automatically update custom boot sources. You must manually enable automatic updates by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cluster has a default storage class.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR, adding the appropriate template and boot source in the <literal>dataImportCronTemplates</literal> section. For example:</simpara>
<formalpara>
<title>Example custom resource</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  dataImportCronTemplates:
  - metadata:
      name: centos7-image-cron
      annotations:
        cdi.kubevirt.io/storage.bind.immediate.requested: "true" <co xml:id="CO112-1"/>
    spec:
      schedule: "0 */12 * * *" <co xml:id="CO112-2"/>
      template:
        spec:
          source:
            registry: <co xml:id="CO112-3"/>
              url: docker://quay.io/containerdisks/centos:7-2009
          storage:
            resources:
              requests:
                storage: 10Gi
      managedDataSource: centos7 <co xml:id="CO112-4"/>
      retentionPolicy: "None" <co xml:id="CO112-5"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO112-1">
<para>This annotation is required for storage classes with <literal>volumeBindingMode</literal> set to <literal>WaitForFirstConsumer</literal>.</para>
</callout>
<callout arearefs="CO112-2">
<para>Schedule for the job specified in cron format.</para>
</callout>
<callout arearefs="CO112-3">
<para>Use to create a data volume from a registry source. Use the default <literal>pod</literal> <literal>pullMethod</literal> and not <literal>node</literal> <literal>pullMethod</literal>, which is based on the <literal>node</literal> docker cache. The <literal>node</literal> docker cache is useful when a registry image is available via <literal>Container.Image</literal>, but the CDI importer is not authorized to access it.</para>
</callout>
<callout arearefs="CO112-4">
<para>For the custom image to be detected as an available boot source, the name of the image&#8217;s <literal>managedDataSource</literal> must match the name of the template&#8217;s <literal>DataSource</literal>, which is found under <literal>spec.dataVolumeTemplates.spec.sourceRef.name</literal> in the VM template YAML file.</para>
</callout>
<callout arearefs="CO112-5">
<para>Use <literal>All</literal> to retain data volumes and data sources when the cron job is deleted. Use <literal>None</literal> to delete data volumes and data sources when the cron job is deleted.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-enabling-volume-snapshot-boot-source_virt-automatic-bootsource-updates">
<title>Enabling volume snapshot boot sources</title>
<simpara>Enable volume snapshot boot sources by setting the parameter in the <literal>StorageProfile</literal> associated with the storage class that stores operating system base images. Although <literal>DataImportCron</literal> was originally designed to maintain only PVC sources, <literal>VolumeSnapshot</literal> sources scale better than PVC sources for certain storage types.</simpara>
<note>
<simpara>Use volume snapshots on a storage profile that is proven to scale better when cloning from a single snapshot.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have access to a volume snapshot with the operating system image.</simpara>
</listitem>
<listitem>
<simpara>The storage must support snapshotting.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the storage profile object that corresponds to the storage class used to provision boot sources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit storageprofile &lt;storage_class&gt;</programlisting>
</listitem>
<listitem>
<simpara>Review the <literal>dataImportCronSourceFormat</literal> specification of the <literal>StorageProfile</literal> to confirm whether or not the VM is using PVC or volume snapshot by default.</simpara>
</listitem>
<listitem>
<simpara>Edit the storage profile, if needed, by updating the <literal>dataImportCronSourceFormat</literal> specification to <literal>snapshot</literal>.</simpara>
<formalpara>
<title>Example storage profile</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: StorageProfile
metadata:
# ...
spec:
  dataImportCronSourceFormat: snapshot</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Open the storage profile object that corresponds to the storage class used to provision boot sources.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageprofile &lt;storage_class&gt;  -oyaml</programlisting>
</listitem>
<listitem>
<simpara>Confirm that the <literal>dataImportCronSourceFormat</literal> specification of the <literal>StorageProfile</literal> is set to 'snapshot', and that any <literal>DataSource</literal> objects that the <literal>DataImportCron</literal> points to now reference volume snapshots.</simpara>
</listitem>
</orderedlist>
<simpara>You can now use these boot sources to create virtual machines.</simpara>
</section>
</section>
<section xml:id="virt-disable-auto-updates-single-boot-source_virt-automatic-bootsource-updates">
<title>Disabling automatic updates for a single boot source</title>
<simpara>You can disable automatic updates for an individual boot source, whether it is custom or system-defined, by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HyperConverged</literal> CR in your default editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Disable automatic updates for an individual boot source by editing the <literal>spec.dataImportCronTemplates</literal> field.</simpara>
<variablelist>
<varlistentry>
<term>Custom boot source</term>
<listitem>
<itemizedlist>
<listitem>
<simpara>Remove the boot source from the <literal>spec.dataImportCronTemplates</literal> field. Automatic updates are disabled for custom boot sources by default.</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
<varlistentry>
<term>System-defined boot source</term>
<listitem>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Add the boot source to <literal>spec.dataImportCronTemplates</literal>.</simpara>
<note>
<simpara>Automatic updates are enabled by default for system-defined boot sources, but these boot sources are not listed in the CR unless you add them.</simpara>
</note>
</listitem>
<listitem>
<simpara>Set the value of the <literal>dataimportcrontemplate.kubevirt.io/enable</literal> annotation to <literal>false</literal>.</simpara>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  dataImportCronTemplates:
  - metadata:
      annotations:
        dataimportcrontemplate.kubevirt.io/enable: false
      name: rhel8-image-cron
# ...</programlisting>
</listitem>
</orderedlist>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Save the file.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-verify-status-bootsource-update_virt-automatic-bootsource-updates">
<title>Verifying the status of a boot source</title>
<simpara>You can determine if a boot source is system-defined or custom by viewing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View the contents of the <literal>HyperConverged</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get hyperconverged kubevirt-hyperconverged -n openshift-cnv -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
# ...
status:
# ...
  dataImportCronTemplates:
  - metadata:
      annotations:
        cdi.kubevirt.io/storage.bind.immediate.requested: "true"
      name: centos-7-image-cron
    spec:
      garbageCollect: Outdated
      managedDataSource: centos7
      schedule: 55 8/12 * * *
      template:
        metadata: {}
        spec:
          source:
            registry:
              url: docker://quay.io/containerdisks/centos:7-2009
          storage:
            resources:
              requests:
                storage: 30Gi
        status: {}
    status:
      commonTemplate: true <co xml:id="CO113-1"/>
# ...
  - metadata:
      annotations:
        cdi.kubevirt.io/storage.bind.immediate.requested: "true"
      name: user-defined-dic
    spec:
      garbageCollect: Outdated
      managedDataSource: user-defined-centos-stream8
      schedule: 55 8/12 * * *
      template:
        metadata: {}
        spec:
          source:
            registry:
              pullMethod: node
              url: docker://quay.io/containerdisks/centos-stream:8
          storage:
            resources:
              requests:
                storage: 30Gi
        status: {}
    status: {} <co xml:id="CO113-2"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO113-1">
<para>Indicates a system-defined boot source.</para>
</callout>
<callout arearefs="CO113-2">
<para>Indicates a custom boot source.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify the status of the boot source by reviewing the <literal>status.dataImportCronTemplates.status</literal> field.</simpara>
<itemizedlist>
<listitem>
<simpara>If the field contains <literal>commonTemplate: true</literal>, it is a system-defined boot source.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>status.dataImportCronTemplates.status</literal> field has the value <literal>{}</literal>, it is a custom boot source.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-reserving-pvc-space-fs-overhead">
<title>Reserving PVC space for file system overhead</title>

<simpara>When you add a virtual machine disk to a persistent volume claim (PVC) that uses the <literal>Filesystem</literal> volume mode, you must ensure that there is enough space on the PVC for the VM disk and for file system overhead, such as metadata.</simpara>
<simpara>By default, OpenShift Virtualization reserves 5.5% of the PVC space for overhead, reducing the space available for virtual machine disks by that amount.</simpara>
<simpara>You can configure a different overhead value by editing the <literal>HCO</literal> object. You can change the value globally and you can specify values for specific storage classes.</simpara>
<section xml:id="virt-overriding-default-fs-overhead-value_virt-reserving-pvc-space-fs-overhead">
<title>Overriding the default file system overhead value</title>
<simpara>Change the amount of persistent volume claim (PVC) space that the OpenShift Virtualization reserves for file system overhead by editing the <literal>spec.filesystemOverhead</literal> attribute of the <literal>HCO</literal> object.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open the <literal>HCO</literal> object for editing by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Edit the <literal>spec.filesystemOverhead</literal> fields, populating them with your chosen values:</simpara>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  filesystemOverhead:
    global: "&lt;new_global_value&gt;" <co xml:id="CO114-1"/>
    storageClass:
      &lt;storage_class_name&gt;: "&lt;new_value_for_this_storage_class&gt;" <co xml:id="CO114-2"/></programlisting>
<calloutlist>
<callout arearefs="CO114-1">
<para>The default file system overhead percentage used for any storage classes that do not already have a set value. For example, <literal>global: "0.07"</literal> reserves 7% of the PVC for file system overhead.</para>
</callout>
<callout arearefs="CO114-2">
<para>The file system overhead percentage for the specified storage class. For example, <literal>mystorageclass: "0.04"</literal> changes the default overhead value for PVCs in the <literal>mystorageclass</literal> storage class to 4%.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save and exit the editor to update the <literal>HCO</literal> object.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>View the <literal>CDIConfig</literal> status and verify your changes by running one of the following commands:</simpara>
<simpara>To generally verify changes to <literal>CDIConfig</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get cdiconfig -o yaml</programlisting>
<simpara>To view your specific changes to <literal>CDIConfig</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get cdiconfig -o jsonpath='{.items..status.filesystemOverhead}'</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-local-storage-with-hpp">
<title>Configuring local storage by using the hostpath provisioner</title>

<simpara>You can configure local storage for virtual machines by using the hostpath provisioner (HPP).</simpara>
<simpara>When you install the OpenShift Virtualization Operator, the Hostpath Provisioner Operator is automatically installed. HPP is a local storage provisioner designed for OpenShift Virtualization that is created by the Hostpath Provisioner Operator. To use HPP, you create an HPP custom resource (CR) with a basic storage pool.</simpara>
<section xml:id="virt-creating-hpp-basic-storage-pool_virt-configuring-local-storage-with-hpp">
<title>Creating a hostpath provisioner with a basic storage pool</title>
<simpara>You configure a hostpath provisioner (HPP) with a basic storage pool by creating an HPP custom resource (CR) with a <literal>storagePools</literal> stanza. The storage pool specifies the name and path used by the CSI driver.</simpara>
<important>
<simpara>Do not create storage pools in the same partition as the operating system. Otherwise, the operating system partition might become filled to capacity, which will impact performance or cause the node to become unstable or unusable.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The directories specified in <literal>spec.storagePools.path</literal> must have read/write access.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>hpp_cr.yaml</literal> file with a <literal>storagePools</literal> stanza as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools: <co xml:id="CO115-1"/>
  - name: any_name
    path: "/var/myvolumes" <co xml:id="CO115-2"/>
workload:
  nodeSelector:
    kubernetes.io/os: linux</programlisting>
<calloutlist>
<callout arearefs="CO115-1">
<para>The <literal>storagePools</literal> stanza is an array to which you can add multiple entries.</para>
</callout>
<callout arearefs="CO115-2">
<para>Specify the storage pool directories under this node path.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file and exit.</simpara>
</listitem>
<listitem>
<simpara>Create the HPP by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f hpp_cr.yaml</programlisting>
</listitem>
</orderedlist>
<section xml:id="virt-about-creating-storage-classes_virt-configuring-local-storage-with-hpp">
<title>About creating storage classes</title>
<simpara>When you create a storage class, you set parameters that affect the dynamic provisioning of persistent volumes (PVs) that belong to that storage class. You cannot update a <literal>StorageClass</literal> object&#8217;s parameters after you create it.</simpara>
<simpara>In order to use the hostpath provisioner (HPP) you must create an associated storage class for the CSI driver with the <literal>storagePools</literal> stanza.</simpara>
<note>
<simpara>Virtual machines use data volumes that are based on local PVs. Local PVs are bound to specific nodes. While the disk image is prepared for consumption by the virtual machine, it is possible that the virtual machine cannot be scheduled to the node where the local storage PV was previously pinned.</simpara>
<simpara>To solve this problem, use the Kubernetes pod scheduler to bind the persistent volume claim (PVC) to a PV on the correct node. By using the <literal>StorageClass</literal> value with <literal>volumeBindingMode</literal> parameter set to <literal>WaitForFirstConsumer</literal>, the binding and provisioning of the PV is delayed until a pod is created using the PVC.</simpara>
</note>
</section>
<section xml:id="virt-creating-storage-class-csi-driver_virt-configuring-local-storage-with-hpp">
<title>Creating a storage class for the CSI driver with the storagePools stanza</title>
<simpara>To use the hostpath provisioner (HPP) you must create an associated storage class for the Container Storage Interface (CSI) driver.</simpara>
<simpara>When you create a storage class, you set parameters that affect the dynamic provisioning of persistent volumes (PVs) that belong to that storage class. You cannot update a <literal>StorageClass</literal> object&#8217;s parameters after you create it.</simpara>
<note>
<simpara>Virtual machines use data volumes that are based on local PVs. Local PVs are bound to specific nodes. While a disk image is prepared for consumption by the virtual machine, it is possible that the virtual machine cannot be scheduled to the node where the local storage PV was previously pinned.</simpara>
<simpara>To solve this problem, use the Kubernetes pod scheduler to bind the persistent volume claim (PVC) to a PV on the correct node. By using the <literal>StorageClass</literal> value with <literal>volumeBindingMode</literal> parameter set to <literal>WaitForFirstConsumer</literal>, the binding and provisioning of the PV is delayed until a pod is created using the PVC.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>storageclass_csi.yaml</literal> file to define the storage class:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: hostpath-csi
provisioner: kubevirt.io.hostpath-provisioner
reclaimPolicy: Delete <co xml:id="CO116-1"/>
volumeBindingMode: WaitForFirstConsumer <co xml:id="CO116-2"/>
parameters:
  storagePool: my-storage-pool <co xml:id="CO116-3"/></programlisting>
<calloutlist>
<callout arearefs="CO116-1">
<para>The two possible <literal>reclaimPolicy</literal> values are <literal>Delete</literal> and <literal>Retain</literal>. If you do not specify a value, the default value is <literal>Delete</literal>.</para>
</callout>
<callout arearefs="CO116-2">
<para>The <literal>volumeBindingMode</literal> parameter determines when dynamic provisioning and volume binding occur. Specify <literal>WaitForFirstConsumer</literal> to delay the binding and provisioning of a persistent volume (PV) until after a pod that uses the persistent volume claim (PVC) is created. This ensures that the PV meets the pod&#8217;s scheduling requirements.</para>
</callout>
<callout arearefs="CO116-3">
<para>Specify the name of the storage pool defined in the HPP CR.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file and exit.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>StorageClass</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f storageclass_csi.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-about-storage-pools-pvc-templates_virt-configuring-local-storage-with-hpp">
<title>About storage pools created with PVC templates</title>
<simpara>If you have a single, large persistent volume (PV), you can create a storage pool by defining a PVC template in the hostpath provisioner (HPP) custom resource (CR).</simpara>
<simpara>A storage pool created with a PVC template can contain multiple HPP volumes. Splitting a PV into smaller volumes provides greater flexibility for data allocation.</simpara>
<simpara>The PVC template is based on the <literal>spec</literal> stanza of the <literal>PersistentVolumeClaim</literal> object:</simpara>
<formalpara>
<title>Example <literal>PersistentVolumeClaim</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: iso-pvc
spec:
  volumeMode: Block <co xml:id="CO117-1"/>
  storageClassName: my-storage-class
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 5Gi</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO117-1">
<para>This value is only required for block volume mode PVs.</para>
</callout>
</calloutlist>
<simpara>You define a storage pool using a <literal>pvcTemplate</literal> specification in the HPP CR. The Operator creates a PVC from the <literal>pvcTemplate</literal> specification for each node containing the HPP CSI driver. The PVC created from the PVC template consumes the single large PV, allowing the HPP to create smaller dynamic volumes.</simpara>
<simpara>You can combine basic storage pools with storage pools created from PVC templates.</simpara>
<section xml:id="virt-creating-storage-pool-pvc-template_virt-configuring-local-storage-with-hpp">
<title>Creating a storage pool with a PVC template</title>
<simpara>You can create a storage pool for multiple hostpath provisioner (HPP) volumes by specifying a PVC template in the HPP custom resource (CR).</simpara>
<important>
<simpara>Do not create storage pools in the same partition as the operating system. Otherwise, the operating system partition might become filled to capacity, which will impact performance or cause the node to become unstable or unusable.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The directories specified in <literal>spec.storagePools.path</literal> must have read/write access.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>hpp_pvc_template_pool.yaml</literal> file for the HPP CR that specifies a persistent volume (PVC) template in the <literal>storagePools</literal> stanza according to the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hostpathprovisioner.kubevirt.io/v1beta1
kind: HostPathProvisioner
metadata:
  name: hostpath-provisioner
spec:
  imagePullPolicy: IfNotPresent
  storagePools: <co xml:id="CO118-1"/>
  - name: my-storage-pool
    path: "/var/myvolumes" <co xml:id="CO118-2"/>
    pvcTemplate:
      volumeMode: Block <co xml:id="CO118-3"/>
      storageClassName: my-storage-class <co xml:id="CO118-4"/>
      accessModes:
      - ReadWriteOnce
      resources:
        requests:
          storage: 5Gi <co xml:id="CO118-5"/>
  workload:
    nodeSelector:
      kubernetes.io/os: linux</programlisting>
<calloutlist>
<callout arearefs="CO118-1">
<para>The <literal>storagePools</literal> stanza is an array that can contain both basic and PVC template storage pools.</para>
</callout>
<callout arearefs="CO118-2">
<para>Specify the storage pool directories under this node path.</para>
</callout>
<callout arearefs="CO118-3">
<para>Optional: The <literal>volumeMode</literal> parameter can be either <literal>Block</literal> or <literal>Filesystem</literal> as long as it matches the provisioned volume format. If no value is specified, the default is <literal>Filesystem</literal>. If the <literal>volumeMode</literal> is <literal>Block</literal>, the mounting pod creates an XFS file system on the block volume before mounting it.</para>
</callout>
<callout arearefs="CO118-4">
<para>If the <literal>storageClassName</literal> parameter is omitted, the default storage class is used to create PVCs. If you omit <literal>storageClassName</literal>, ensure that the HPP storage class is not the default storage class.</para>
</callout>
<callout arearefs="CO118-5">
<para>You can specify statically or dynamically provisioned storage. In either case, ensure the requested storage size is appropriate for the volume you want to virtually divide or the PVC cannot be bound to the large PV. If the storage class you are using uses dynamically provisioned storage, pick an allocation size that matches the size of a typical request.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the file and exit.</simpara>
</listitem>
<listitem>
<simpara>Create the HPP with a storage pool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f hpp_pvc_template_pool.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="virt-enabling-user-permissions-to-clone-datavolumes">
<title>Enabling user permissions to clone data volumes across namespaces</title>

<simpara>The isolating nature of namespaces means that users cannot by default
clone resources between namespaces.</simpara>
<simpara>To enable a user to clone a virtual machine to another namespace, a
user with the <literal>cluster-admin</literal> role must create a new cluster role. Bind
this cluster role to a user to enable them to clone virtual machines
to the destination namespace.</simpara>
<section xml:id="virt-creating-rbac-cloning-dvs_virt-enabling-user-permissions-to-clone-datavolumes">
<title>Creating RBAC resources for cloning data volumes</title>
<simpara>Create a new cluster role that enables permissions for all actions for the <literal>datavolumes</literal> resource.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have cluster admin privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>ClusterRole</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: &lt;datavolume-cloner&gt; <co xml:id="CO119-1"/>
rules:
- apiGroups: ["cdi.kubevirt.io"]
  resources: ["datavolumes/source"]
  verbs: ["*"]</programlisting>
<calloutlist>
<callout arearefs="CO119-1">
<para>Unique name for the cluster role.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the cluster role in the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;datavolume-cloner.yaml&gt; <co xml:id="CO120-1"/></programlisting>
<calloutlist>
<callout arearefs="CO120-1">
<para>The file name of the <literal>ClusterRole</literal> manifest created in the previous step.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>RoleBinding</literal> manifest that applies to both the source and destination namespaces and references
the cluster role created in the previous step.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: &lt;allow-clone-to-user&gt; <co xml:id="CO121-1"/>
  namespace: &lt;Source namespace&gt; <co xml:id="CO121-2"/>
subjects:
- kind: ServiceAccount
  name: default
  namespace: &lt;Destination namespace&gt; <co xml:id="CO121-3"/>
roleRef:
  kind: ClusterRole
  name: datavolume-cloner <co xml:id="CO121-4"/>
  apiGroup: rbac.authorization.k8s.io</programlisting>
<calloutlist>
<callout arearefs="CO121-1">
<para>Unique name for the role binding.</para>
</callout>
<callout arearefs="CO121-2">
<para>The namespace for the source data volume.</para>
</callout>
<callout arearefs="CO121-3">
<para>The namespace to which the data volume is cloned.</para>
</callout>
<callout arearefs="CO121-4">
<para>The name of the cluster role created in the previous step.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the role binding in the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;datavolume-cloner.yaml&gt; <co xml:id="CO122-1"/></programlisting>
<calloutlist>
<callout arearefs="CO122-1">
<para>The file name of the <literal>RoleBinding</literal> manifest created in the previous step.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-configuring-cdi-for-namespace-resourcequota">
<title>Configuring CDI to override CPU and memory quotas</title>

<simpara>You can configure the Containerized Data Importer (CDI) to import, upload, and clone virtual machine disks into namespaces that are subject to CPU and memory resource restrictions.</simpara>
<section xml:id="virt-about-cpu-and-memory-quota-namespace_virt-configuring-cdi-for-namespace-resourcequota">
<title>About CPU and memory quotas in a namespace</title>
<simpara>A <emphasis>resource quota</emphasis>, defined by the <literal>ResourceQuota</literal> object, imposes restrictions on a namespace that limit the total amount of compute resources that can be consumed by resources within that namespace.</simpara>
<simpara>The <literal>HyperConverged</literal> custom resource (CR) defines the user configuration for the Containerized Data Importer (CDI). The CPU and memory request and limit values are set to a default value of <literal>0</literal>.
This ensures that pods created by CDI that do not specify compute resource requirements are given the default values and are allowed to run in a namespace that is restricted with a quota.</simpara>
</section>
<section xml:id="virt-overriding-cpu-and-memory-defaults_virt-configuring-cdi-for-namespace-resourcequota">
<title>Overriding CPU and memory defaults</title>
<simpara>Modify the default settings for CPU and memory requests and limits for your use case by adding the <literal>spec.resourceRequirements.storageWorkloads</literal> stanza to the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>spec.resourceRequirements.storageWorkloads</literal> stanza to the CR, setting the values based on your use case. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  resourceRequirements:
    storageWorkloads:
      limits:
        cpu: "500m"
        memory: "2Gi"
      requests:
        cpu: "250m"
        memory: "1Gi"</programlisting>
</listitem>
<listitem>
<simpara>Save and exit the editor to update the <literal>HyperConverged</literal> CR.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-configuring-cdi-for-namespace-resourcequota_additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#quotas-setting-per-project">Resource quotas per project</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-preparing-cdi-scratch-space">
<title>Preparing CDI scratch space</title>

<section xml:id="virt-about-scratch-space_virt-preparing-cdi-scratch-space">
<title>About scratch space</title>
<simpara>The Containerized Data Importer (CDI) requires scratch space (temporary storage) to complete some operations, such as importing and uploading virtual machine images.
During this process, CDI provisions a scratch space PVC equal to the size of the PVC backing the destination data volume (DV).
The scratch space PVC is deleted after the operation completes or aborts.</simpara>
<simpara>You can define the storage class that is used to bind the scratch space PVC in the <literal>spec.scratchSpaceStorageClass</literal> field of the <literal>HyperConverged</literal> custom resource.</simpara>
<simpara>If the defined storage class does not match a storage class in the cluster, then the default storage class defined for the cluster is used.
If there is no default storage class defined in the cluster, the storage class used to provision the original DV or PVC is used.</simpara>
<note>
<simpara>CDI requires requesting scratch space with a <literal>file</literal> volume mode, regardless of the PVC backing the origin data volume.
If the origin PVC is backed by <literal>block</literal> volume mode, you must define a storage class capable of provisioning <literal>file</literal> volume mode PVCs.</simpara>
</note>
<bridgehead xml:id="_manual-provisioning" renderas="sect4">Manual provisioning</bridgehead>
<simpara>If there are no storage classes, CDI uses any PVCs in the project that match the size requirements for the image.
If there are no PVCs that match these requirements, the CDI import pod remains in a <emphasis role="strong">Pending</emphasis> state until an appropriate PVC is made available or until a timeout function kills the pod.</simpara>
</section>
<section xml:id="virt-operations-requiring-scratch-space_virt-preparing-cdi-scratch-space">
<title>CDI operations that require scratch space</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Reason</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Registry imports</simpara></entry>
<entry align="left" valign="top"><simpara>CDI must download the image to a scratch space and extract the layers to find the image file. The image file is then passed to QEMU-IMG for conversion to a raw disk.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Upload image</simpara></entry>
<entry align="left" valign="top"><simpara>QEMU-IMG does not accept input from STDIN. Instead, the image to upload is saved in scratch space before it can be passed to QEMU-IMG for conversion.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>HTTP imports of archived images</simpara></entry>
<entry align="left" valign="top"><simpara>QEMU-IMG does not know how to handle the archive formats CDI supports. Instead, the image is unarchived and saved into scratch space before it is passed to QEMU-IMG.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>HTTP imports of authenticated images</simpara></entry>
<entry align="left" valign="top"><simpara>QEMU-IMG inadequately handles authentication. Instead, the image is saved to scratch space and authenticated before it is passed to QEMU-IMG.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>HTTP imports of custom certificates</simpara></entry>
<entry align="left" valign="top"><simpara>QEMU-IMG inadequately handles custom certificates of HTTPS endpoints. Instead, CDI downloads the image to scratch space before passing the file to QEMU-IMG.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="virt-defining-storageclass_virt-preparing-cdi-scratch-space">
<title>Defining a storage class</title>
<simpara>You can define the storage class that the Containerized Data Importer (CDI) uses when allocating scratch space by adding the <literal>spec.scratchSpaceStorageClass</literal> field to the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>spec.scratchSpaceStorageClass</literal> field to the CR, setting the value to the name of a storage class that exists in the cluster:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  scratchSpaceStorageClass: "&lt;storage_class&gt;" <co xml:id="CO123-1"/></programlisting>
<calloutlist>
<callout arearefs="CO123-1">
<para>If you do not specify a storage class, CDI uses the storage class of the persistent volume claim that is being populated.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save and exit your default editor to update the <literal>HyperConverged</literal> CR.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-cdi-supported-operations-matrix_virt-preparing-cdi-scratch-space">
<title>CDI supported operations matrix</title>
<simpara>This matrix shows the supported CDI operations for content types against endpoints, and which of these operations requires scratch space.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="6">
<colspec colname="col_1" colwidth="16.6666*"/>
<colspec colname="col_2" colwidth="16.6666*"/>
<colspec colname="col_3" colwidth="16.6666*"/>
<colspec colname="col_4" colwidth="16.6666*"/>
<colspec colname="col_5" colwidth="16.6666*"/>
<colspec colname="col_6" colwidth="16.667*"/>
<thead>
<row>
<entry align="left" valign="top">Content types</entry>
<entry align="left" valign="top">HTTP</entry>
<entry align="left" valign="top">HTTPS</entry>
<entry align="left" valign="top">HTTP basic auth</entry>
<entry align="left" valign="top">Registry</entry>
<entry align="left" valign="top">Upload</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>KubeVirt (QCOW2)</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; QCOW2<?asciidoc-br?>
&#10003; GZ*<?asciidoc-br?>
&#10003; XZ*</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; QCOW2**<?asciidoc-br?>
&#10003; GZ*<?asciidoc-br?>
&#10003; XZ*</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; QCOW2<?asciidoc-br?>
&#10003; GZ*<?asciidoc-br?>
&#10003; XZ*</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; QCOW2*<?asciidoc-br?>
&#9633; GZ<?asciidoc-br?>
&#9633; XZ</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; QCOW2*<?asciidoc-br?>
&#10003; GZ*<?asciidoc-br?>
&#10003; XZ*</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>KubeVirt (RAW)</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; RAW<?asciidoc-br?>
&#10003; GZ<?asciidoc-br?>
&#10003; XZ</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; RAW<?asciidoc-br?>
&#10003; GZ<?asciidoc-br?>
&#10003; XZ</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; RAW<?asciidoc-br?>
&#10003; GZ<?asciidoc-br?>
&#10003; XZ</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; RAW*<?asciidoc-br?>
&#9633; GZ<?asciidoc-br?>
&#9633; XZ</simpara></entry>
<entry align="left" valign="top"><simpara>&#10003; RAW*<?asciidoc-br?>
&#10003; GZ*<?asciidoc-br?>
&#10003; XZ*</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<simpara>&#10003; Supported operation</simpara>
<simpara>&#9633; Unsupported operation</simpara>
<simpara>* Requires scratch space</simpara>
<simpara>** Requires scratch space if a custom certificate authority is required</simpara>
</section>
<section xml:id="virt-preparing-cdi-scratch-space-additional-resources" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#about_dynamic-provisioning">Dynamic provisioning</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-using-preallocation-for-datavolumes">
<title>Using preallocation for data volumes</title>

<simpara>The Containerized Data Importer can preallocate disk space to improve write performance when creating data volumes.</simpara>
<simpara>You can enable preallocation for specific data volumes.</simpara>
<section xml:id="virt-about-preallocation_virt-using-preallocation-for-datavolumes">
<title>About preallocation</title>
<simpara>The Containerized Data Importer (CDI) can use the QEMU preallocate mode for data volumes to improve write performance. You can use preallocation mode for importing and uploading operations and when creating blank data volumes.</simpara>
<simpara>If preallocation is enabled, CDI uses the better preallocation method depending on the underlying file system and device type:</simpara>
<variablelist>
<varlistentry>
<term><literal>fallocate</literal></term>
<listitem>
<simpara>If the file system supports it, CDI uses the operating system&#8217;s <literal>fallocate</literal> call to preallocate space by using the <literal>posix_fallocate</literal> function, which allocates blocks and marks them as uninitialized.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>full</literal></term>
<listitem>
<simpara>If <literal>fallocate</literal> mode cannot be used, <literal>full</literal> mode allocates space for the image by writing data to the underlying storage. Depending on the storage location, all the empty allocated space might be zeroed.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="virt-enabling-preallocation-for-dv_virt-using-preallocation-for-datavolumes">
<title>Enabling preallocation for a data volume</title>
<simpara>You can enable preallocation for specific data volumes by including the <literal>spec.preallocation</literal> field in the data volume manifest. You can enable preallocation mode in either the web console or by using the OpenShift CLI (<literal>oc</literal>).</simpara>
<simpara>Preallocation mode is supported for all CDI source types.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Specify the <literal>spec.preallocation</literal> field in the data volume manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: preallocated-datavolume
spec:
  source: <co xml:id="CO124-1"/>
    pvc:
      preallocation: true <co xml:id="CO124-2"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO124-1">
<para>All CDI source types support preallocation. However, preallocation is ignored for cloning operations.</para>
</callout>
<callout arearefs="CO124-2">
<para>The default value is <literal>false</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-managing-data-volume-annotations">
<title>Managing data volume annotations</title>

<simpara>Data volume (DV) annotations allow you to manage pod behavior. You can add one or more annotations to a data volume, which then propagates to the created importer pods.</simpara>
<section xml:id="virt-dv-annotations_virt-managing-data-volume-annotations">
<title>Example: Data volume annotations</title>
<simpara>This example shows how you can configure data volume (DV) annotations to control which network the importer pod uses. The <literal>v1.multus-cni.io/default-network: bridge-network</literal> annotation causes the pod to use the multus network named <literal>bridge-network</literal> as its default network.
If you want the importer pod to use both the default network from the cluster and the secondary multus network, use the <literal>k8s.v1.cni.cncf.io/networks: &lt;network_name&gt;</literal> annotation.</simpara>
<formalpara>
<title>Multus network annotation example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: cdi.kubevirt.io/v1beta1
kind: DataVolume
metadata:
  name: datavolume-example
  annotations:
    v1.multus-cni.io/default-network: bridge-network <co xml:id="CO125-1"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO125-1">
<para>Multus network annotation</para>
</callout>
</calloutlist>
</section>
</section>
</chapter>
<chapter xml:id="_live-migration">
<title>Live migration</title>
<section xml:id="virt-about-live-migration">
<title>About live migration</title>

<simpara>Live migration is the process of moving a running virtual machine (VM) to another node in the cluster without interrupting the virtual workload. By default, live migration traffic is encrypted using Transport Layer Security (TLS).</simpara>
<section xml:id="live-migration-requirements_virt-about-live-migration">
<title>Live migration requirements</title>
<simpara>Live migration has the following requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>The cluster must have shared storage with <literal>ReadWriteMany</literal> (RWX) access mode.</simpara>
</listitem>
<listitem>
<simpara>The cluster must have sufficient RAM and network bandwidth.</simpara>
<note>
<simpara>You must ensure that there is enough memory request capacity in the cluster to support node drains that result in live migrations. You can determine the approximate required spare memory by using the following calculation:</simpara>
<screen>Product of (Maximum number of nodes that can drain in parallel) and (Highest total VM memory request allocations across nodes)</screen>
<simpara>The default number of migrations that can run in parallel in the cluster is 5.</simpara>
</note>
</listitem>
<listitem>
<simpara>If a VM uses a host model CPU, the nodes must support the CPU.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-dedicated-network-live-migration">Configuring a dedicated Multus network</link> for live migration is highly recommended. A dedicated network minimizes the effects of network saturation on tenant workloads during migration.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="common-live-migration-tasks_virt-about-live-migration">
<title>Common live migration tasks</title>
<simpara>You can perform the following live migration tasks:</simpara>
<itemizedlist>
<listitem>
<simpara>Configure live migration settings:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-configuring-live-migration-limits_virt-configuring-live-migration">Limits and timeouts</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="overview-settings-cluster_virt-web-console-overview">Maximum number of migrations per node or cluster</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="overview-settings-cluster_virt-web-console-overview">Select a dedicated live migration network from existing networks</link></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara><link linkend="virt-initiating-live-migration">Initiate and cancel live migration</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="overview-migrations_virt-web-console-overview">Monitor the progress of all live migrations</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virtualmachine-details-metrics_virt-web-console-overview">View VM migration metrics</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional-resources_virt-about-live-migration">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-live-migration-metrics_virt-prometheus-queries">Prometheus queries for live migration</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/articles/6994974#vm-migration-tuning">VM migration tuning</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="run-strategies">VM run strategies</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="eviction-strategies">VM and cluster eviction strategies</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-configuring-live-migration">
<title>Configuring live migration</title>

<simpara>You can configure live migration settings to ensure that the migration processes do not overwhelm the cluster.</simpara>
<simpara>You can configure live migration policies to apply different migration configurations to groups of virtual machines (VMs).</simpara>
<section xml:id="live-migration-settings">
<title>Live migration settings</title>
<simpara>You can configure the following live migration settings:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-configuring-live-migration-limits_virt-configuring-live-migration">Limits and timeouts</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="overview-settings-cluster_virt-web-console-overview">Maximum number of migrations per node or cluster</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-configuring-live-migration-limits_virt-configuring-live-migration">
<title>Configuring live migration limits and timeouts</title>
<simpara>Configure live migration limits and timeouts for the cluster by updating the <literal>HyperConverged</literal> custom resource (CR), which is located in the
<literal>openshift-cnv</literal> namespace.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> CR and add the necessary live migration parameters:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
<formalpara>
<title>Example configuration file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  liveMigrationConfig:
    bandwidthPerMigration: 64Mi <co xml:id="CO126-1"/>
    completionTimeoutPerGiB: 800 <co xml:id="CO126-2"/>
    parallelMigrationsPerCluster: 5 <co xml:id="CO126-3"/>
    parallelOutboundMigrationsPerNode: 2 <co xml:id="CO126-4"/>
    progressTimeout: 150 <co xml:id="CO126-5"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO126-1">
<para>Bandwidth limit of each migration, where the value is the quantity of bytes per second. For example, a value of <literal>2048Mi</literal> means 2048 MiB/s. Default: <literal>0</literal>, which is unlimited.</para>
</callout>
<callout arearefs="CO126-2">
<para>The migration is canceled if it has not completed in this time, in seconds per GiB of memory. For example, a VM with 6GiB memory times out if it has not completed migration in 4800 seconds. If the <literal>Migration Method</literal> is <literal>BlockMigration</literal>, the size of the migrating disks is included in the calculation.</para>
</callout>
<callout arearefs="CO126-3">
<para>Number of migrations running in parallel in the cluster. Default: <literal>5</literal>.</para>
</callout>
<callout arearefs="CO126-4">
<para>Maximum number of outbound migrations per node. Default: <literal>2</literal>.</para>
</callout>
<callout arearefs="CO126-5">
<para>The migration is canceled if memory copy fails to make progress in this time, in seconds. Default: <literal>150</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>You can restore the default value for any <literal>spec.liveMigrationConfig</literal> field by deleting that key/value pair and saving the file. For example, delete <literal>progressTimeout: &lt;value&gt;</literal> to restore the default <literal>progressTimeout: 150</literal>.</simpara>
</note>
</section>
</section>
<section xml:id="live-migration-policies">
<title>Live migration policies</title>
<simpara>You can create live migration policies to apply different migration configurations to groups of VMs that are defined by VM or project labels.</simpara>
<tip>
<simpara>You can create live migration policies by using the <link linkend="migrationpolicies-page_virt-web-console-overview">web console</link>.</simpara>
</tip>
<section xml:id="virt-configuring-a-live-migration-policy_virt-configuring-live-migration">
<title>Creating a live migration policy by using the command line</title>
<simpara>You can create a live migration policy by using the command line. A live migration policy is applied to selected virtual machines (VMs) by using any combination of labels:</simpara>
<itemizedlist>
<listitem>
<simpara>VM labels such as <literal>size</literal>, <literal>os</literal>, or <literal>gpu</literal></simpara>
</listitem>
<listitem>
<simpara>Project labels such as <literal>priority</literal>, <literal>bandwidth</literal>, or <literal>hpc-workload</literal></simpara>
</listitem>
</itemizedlist>
<simpara>For the policy to apply to a specific group of VMs, all labels on the group of VMs must match the labels of the policy.</simpara>
<note>
<simpara>If multiple live migration policies apply to a VM, the policy with the greatest number of matching labels takes precedence.</simpara>
<simpara>If multiple policies meet this criteria, the policies are sorted by alphabetical order of the matching label keys, and the first one in that order takes precedence.</simpara>
</note>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>MigrationPolicy</literal> object as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: migrations.kubevirt.io/v1alpha1
kind: MigrationPolicy
metadata:
  name: &lt;migration_policy&gt;
spec:
  selectors:
    namespaceSelector: <co xml:id="CO127-1"/>
      hpc-workloads: "True"
      xyz-workloads-type: ""
    virtualMachineInstanceSelector: <co xml:id="CO127-2"/>
      workload-type: "db"
      operating-system: ""</programlisting>
<calloutlist>
<callout arearefs="CO127-1">
<para>Specify project labels.</para>
</callout>
<callout arearefs="CO127-2">
<para>Specify VM labels.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the migration policy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create migrationpolicy -f &lt;migration_policy&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_virt-configuring-live-migration" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-dedicated-network-live-migration">Configuring a dedicated Multus network for live migration</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-initiating-live-migration">
<title>Initiating and canceling live migration</title>

<simpara>You can initiate the live migration of a virtual machine (VM) to another node by using the <link linkend="virt-initiating-vm-migration-web_virt-initiating-live-migration">OpenShift Container Platform web console</link> or the <link linkend="virt-initiating-vm-migration-cli_virt-initiating-live-migration">command line</link>.</simpara>
<simpara>You can cancel a live migration by using the <link linkend="virt-canceling-vm-migration-web_virt-initiating-live-migration">web console</link> or the <link linkend="virt-canceling-vm-migration-cli_virt-initiating-live-migration">command line</link>. The VM remains on its original node.</simpara>
<tip>
<simpara>You can also initiate and cancel live migration by using the <literal>virtctl migrate &lt;vm_name&gt;</literal> and <literal>virtctl migrate-cancel &lt;vm_name&gt;</literal> commands.</simpara>
</tip>
<section xml:id="initating-live-migration_initiating-canceling">
<title>Initiating live migration</title>
<section xml:id="virt-initiating-vm-migration-web_virt-initiating-live-migration">
<title>Initiating live migration by using the web console</title>
<simpara>You can live migrate a running virtual machine (VM) to a different node in the cluster by using the OpenShift Container Platform web console.</simpara>
<note>
<simpara>The <emphasis role="strong">Migrate</emphasis> action is visible to all users but only cluster administrators can initiate a live migration.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The VM must be migratable.</simpara>
</listitem>
<listitem>
<simpara>If the VM is configured with a host model CPU, the cluster must have an available node that supports the CPU model.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Migrate</emphasis> from the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a VM.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Migrate</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-initiating-vm-migration-cli_virt-initiating-live-migration">
<title>Initiating live migration by using the command line</title>
<simpara>You can initiate the live migration of a running virtual machine (VM) by using the command line to create a <literal>VirtualMachineInstanceMigration</literal> object for the VM.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>VirtualMachineInstanceMigration</literal> manifest for the VM that you want to migrate:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: &lt;migration_name&gt;
spec:
  vmiName: &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create the object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;migration_name&gt;.yaml</programlisting>
<simpara>The <literal>VirtualMachineInstanceMigration</literal> object triggers a live migration of the VM. This object exists in the cluster for as long as the virtual machine instance is running, unless manually deleted.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Obtain the VM status by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe vmi &lt;vm_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
Status:
  Conditions:
    Last Probe Time:       &lt;nil&gt;
    Last Transition Time:  &lt;nil&gt;
    Status:                True
    Type:                  LiveMigratable
  Migration Method:  LiveMigration
  Migration State:
    Completed:                    true
    End Timestamp:                2018-12-24T06:19:42Z
    Migration UID:                d78c8962-0743-11e9-a540-fa163e0c69f1
    Source Node:                  node2.example.com
    Start Timestamp:              2018-12-24T06:19:35Z
    Target Node:                  node1.example.com
    Target Node Address:          10.9.0.18:43891
    Target Node Domain Detected:  true</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="canceling-live-migration_initiating-canceling">
<title>Canceling live migration</title>
<section xml:id="virt-canceling-vm-migration-web_virt-initiating-live-migration">
<title>Canceling live migration by using the web console</title>
<simpara>You can cancel the live migration of a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Cancel Migration</emphasis> on the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a VM.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-canceling-vm-migration-cli_virt-initiating-live-migration">
<title>Canceling live migration by using the command line</title>
<simpara>Cancel the live migration of a virtual machine by deleting the
<literal>VirtualMachineInstanceMigration</literal> object associated with the migration.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>VirtualMachineInstanceMigration</literal> object that triggered the live
migration, <literal>migration-job</literal> in this example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vmim migration-job</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_virt-initiating-live-migration" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="overview-migrations_virt-web-console-overview">Monitoring the progress of all live migrations by using the web console</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virtualmachine-details-metrics_virt-web-console-overview">Viewing VM migration metrics by using the web console</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="_nodes">
<title>Nodes</title>
<section xml:id="virt-node-maintenance">
<title>Node maintenance</title>

<simpara>Nodes can be placed into maintenance mode by using the <literal>oc adm</literal> utility or <literal>NodeMaintenance</literal> custom resources (CRs).</simpara>
<note>
<simpara>The <literal>node-maintenance-operator</literal> (NMO) is no longer shipped with OpenShift Virtualization. It is deployed as a standalone Operator from the <emphasis role="strong">OperatorHub</emphasis> in the OpenShift Container Platform web console or by using the OpenShift CLI (<literal>oc</literal>).</simpara>
<simpara>For more information on remediation, fencing, and maintaining nodes, see the <link xlink:href="https://access.redhat.com/documentation/en-us/workload_availability_for_red_hat_openshift/23.2/html-single/remediation_fencing_and_maintenance/index#about-remediation-fencing-maintenance">Workload Availability for Red Hat OpenShift</link> documentation.</simpara>
</note>
<important>
<simpara>Virtual machines (VMs) must have a persistent volume claim (PVC) with a shared <literal>ReadWriteMany</literal> (RWX) access mode to be live migrated.</simpara>
</important>
<simpara>The Node Maintenance Operator watches for new or deleted <literal>NodeMaintenance</literal> CRs. When a new <literal>NodeMaintenance</literal> CR is detected, no new workloads are scheduled and the node is cordoned off from the rest of the cluster. All pods that can be evicted are evicted from the node. When a <literal>NodeMaintenance</literal> CR is deleted, the node that is referenced in the CR is made available for new workloads.</simpara>
<note>
<simpara>Using a <literal>NodeMaintenance</literal> CR for node maintenance tasks achieves the same results as the <literal>oc adm cordon</literal> and <literal>oc adm drain</literal> commands using standard OpenShift Container Platform custom resource processing.</simpara>
</note>
<section xml:id="eviction-strategies">
<title>Eviction strategies</title>
<simpara>Placing a node into maintenance marks the node as unschedulable and drains all the VMs and pods from it.</simpara>
<simpara>You can configure eviction strategies for virtual machines (VMs) or for the cluster.</simpara>
<variablelist>
<varlistentry>
<term>VM eviction strategy</term>
<listitem>
<simpara>The VM <literal>LiveMigrate</literal> eviction strategy ensures that a virtual machine instance (VMI) is not interrupted if the node is placed into maintenance or drained. VMIs with this eviction strategy will be live migrated to another node.</simpara>
<simpara>You can configure eviction strategies for virtual machines (VMs) by using the <link linkend="virtualmachine-details-scheduling_virt-web-console-overview">web console</link> or the <link linkend="virt-configuring-a-live-migration-policy_virt-configuring-live-migration">command line</link>.</simpara>
<important>
<simpara>The default eviction strategy is <literal>LiveMigrate</literal>. A non-migratable VM with a <literal>LiveMigrate</literal> eviction strategy might prevent nodes from draining or block an infrastructure upgrade because the VM is not evicted from the node. This situation causes a migration to remain in a <literal>Pending</literal> or <literal>Scheduling</literal> state unless you shut down the VM manually.</simpara>
<simpara>You must set the eviction strategy of non-migratable VMs to <literal>LiveMigrateIfPossible</literal>, which does not block an upgrade, or to <literal>None</literal>, for VMs that should not be migrated.</simpara>
</important>
</listitem>
</varlistentry>
<varlistentry>
<term>Cluster eviction strategy</term>
<listitem>
<simpara>You can configure an eviction strategy for the cluster to prioritize workload continuity or infrastructure upgrade.</simpara>
</listitem>
</varlistentry>
</variablelist>
<important>
<simpara>Configuring a cluster eviction strategy is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<table frame="all" rowsep="1" colsep="1">
<title>Cluster eviction strategies</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="20*"/>
<colspec colname="col_2" colwidth="40*"/>
<colspec colname="col_3" colwidth="20*"/>
<colspec colname="col_4" colwidth="20*"/>
<thead>
<row>
<entry align="left" valign="top">Eviction strategy</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Interrupts workflow</entry>
<entry align="left" valign="top">Blocks upgrades</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>LiveMigrate</literal> <superscript>1</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Prioritizes workload continuity over upgrades.</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
<entry align="left" valign="top"><simpara>Yes <superscript>2</superscript></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>LiveMigrateIfPossible</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Prioritizes upgrades over workload continuity to ensure that the environment is updated.</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>None</literal> <superscript>3</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Shuts down VMs with no eviction strategy.</simpara></entry>
<entry align="left" valign="top"><simpara>Yes</simpara></entry>
<entry align="left" valign="top"><simpara>No</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>Default eviction strategy for multi-node clusters.</simpara>
</listitem>
<listitem>
<simpara>If a VM blocks an upgrade, you must shut down the VM manually.</simpara>
</listitem>
<listitem>
<simpara>Default eviction strategy for single-node OpenShift.</simpara>
</listitem>
</orderedlist>
</para>
<section xml:id="virt-configuring-vm-eviction-strategy-cli_virt-node-maintenance">
<title>Configuring a VM eviction strategy using the command line</title>
<simpara>You can configure an eviction strategy for a virtual machine (VM) by using the command line.</simpara>
<important>
<simpara>The default eviction strategy is <literal>LiveMigrate</literal>. A non-migratable VM with a <literal>LiveMigrate</literal> eviction strategy might prevent nodes from draining or block an infrastructure upgrade because the VM is not evicted from the node. This situation causes a migration to remain in a <literal>Pending</literal> or <literal>Scheduling</literal> state unless you shut down the VM manually.</simpara>
<simpara>You must set the eviction strategy of non-migratable VMs to <literal>LiveMigrateIfPossible</literal>, which does not block an upgrade, or to <literal>None</literal>, for VMs that should not be migrated.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit vm &lt;vm_name&gt; -n &lt;namespace&gt;</programlisting>
<formalpara>
<title>Example eviction strategy</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: &lt;vm_name&gt;
spec:
  template:
    spec:
      evictionStrategy: LiveMigrateIfPossible <co xml:id="CO128-1"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO128-1">
<para>Specify the eviction strategy. The default value is <literal>LiveMigrate</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Restart the VM to apply the changes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virtctl restart &lt;vm_name&gt; -n &lt;namespace&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-configuring-cluster-eviction-strategy-cli_virt-node-maintenance">
<title>Configuring a cluster eviction strategy by using the command line</title>
<simpara>You can configure an eviction strategy for a cluster by using the command line.</simpara>
<important>
<simpara>Configuring a cluster eviction strategy is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>hyperconverged</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Set the cluster eviction strategy as shown in the following example:</simpara>
<formalpara>
<title>Example cluster eviction strategy</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  evictionStrategy: LiveMigrate
# ...</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="run-strategies">
<title>Run strategies</title>
<simpara>A virtual machine (VM) configured with <literal>spec.running: true</literal> is immediately restarted. The <literal>spec.runStrategy</literal> key provides greater flexibility for determining how a VM behaves under certain conditions.</simpara>
<important>
<simpara>The <literal>spec.runStrategy</literal> and <literal>spec.running</literal> keys are mutually exclusive. Only one of them can be used.</simpara>
<simpara>A VM configuration with both keys is invalid.</simpara>
</important>
<section xml:id="virt-runstrategies-vms_virt-node-maintenance">
<title>Run strategies</title>
<simpara>The <literal>spec.runStrategy</literal> key has four possible values:</simpara>
<variablelist>
<varlistentry>
<term><literal>Always</literal></term>
<listitem>
<simpara>The virtual machine instance (VMI) is always present when a virtual machine (VM) is created on another node. A new VMI is created if the original stops for any reason. This is the same behavior as <literal>running: true</literal>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>RerunOnFailure</literal></term>
<listitem>
<simpara>The VMI is re-created on another node if the previous instance fails. The instance is not re-created if the VM stops successfully, such as when it is shut down.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Manual</literal></term>
<listitem>
<simpara>You control the VMI state manually with the <literal>start</literal>, <literal>stop</literal>, and <literal>restart</literal> virtctl client commands. The VM is not automatically restarted.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>Halted</literal></term>
<listitem>
<simpara>No VMI is present when a VM is created. This is the same behavior as <literal>running: false</literal>.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>Different combinations of the <literal>virtctl start</literal>, <literal>stop</literal> and <literal>restart</literal> commands affect the run strategy.</simpara>
<simpara>The following table describes a VM&#8217;s transition between states. The first column shows the VM&#8217;s initial run strategy. The remaining columns show a virtctl command and the new run strategy after that command is run.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Run strategy before and after <literal>virtctl</literal> commands</title>
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Initial run strategy</entry>
<entry align="left" valign="top">Start</entry>
<entry align="left" valign="top">Stop</entry>
<entry align="left" valign="top">Restart</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Always</simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
<entry align="left" valign="top"><simpara>Halted</simpara></entry>
<entry align="left" valign="top"><simpara>Always</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>RerunOnFailure</simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
<entry align="left" valign="top"><simpara>Halted</simpara></entry>
<entry align="left" valign="top"><simpara>RerunOnFailure</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Manual</simpara></entry>
<entry align="left" valign="top"><simpara>Manual</simpara></entry>
<entry align="left" valign="top"><simpara>Manual</simpara></entry>
<entry align="left" valign="top"><simpara>Manual</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Halted</simpara></entry>
<entry align="left" valign="top"><simpara>Always</simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
<entry align="left" valign="top"><simpara>-</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<note>
<simpara>If a node in a cluster installed by using installer-provisioned infrastructure fails the machine health check and is unavailable, VMs with <literal>runStrategy: Always</literal> or <literal>runStrategy: RerunOnFailure</literal> are rescheduled on a new node.</simpara>
</note>
</section>
<section xml:id="virt-configuring-runstrategy-vm_virt-node-maintenance">
<title>Configuring a VM run strategy by using the command line</title>
<simpara>You can configure a run strategy for a virtual machine (VM) by using the command line.</simpara>
<important>
<simpara>The <literal>spec.runStrategy</literal> and <literal>spec.running</literal> keys are mutually exclusive. A VM configuration that contains values for both keys is invalid.</simpara>
</important>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>VirtualMachine</literal> resource by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit vm &lt;vm_name&gt; -n &lt;namespace&gt;</programlisting>
<formalpara>
<title>Example run strategy</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
spec:
  runStrategy: Always
# ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-maintaining-bare-metal-nodes_virt-node-maintenance">
<title>Maintaining bare metal nodes</title>
<simpara>When you deploy OpenShift Container Platform on bare metal infrastructure, there are additional considerations that must be taken into account compared to deploying on cloud infrastructure. Unlike in cloud environments where the cluster nodes are considered ephemeral, re-provisioning a bare metal node requires significantly more time and effort for maintenance tasks.</simpara>
<simpara>When a bare metal node fails, for example, if a fatal kernel error happens or a NIC card hardware failure occurs, workloads on the failed node need to be restarted elsewhere else on the cluster while the problem node is repaired or replaced. Node maintenance mode allows cluster administrators to gracefully power down nodes, moving workloads to other parts of the cluster and ensuring workloads do not get interrupted. Detailed progress and node status details are provided during maintenance.</simpara>
</section>
<section xml:id="additional-resources_virt-node-maintenance" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-about-live-migration">About live migration</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-managing-node-labeling-obsolete-cpu-models">
<title>Managing node labeling for obsolete CPU models</title>

<simpara>You can schedule a virtual machine (VM) on a node as long as the VM CPU model and policy are supported by the node.</simpara>
<section xml:id="virt-about-node-labeling-obsolete-cpu-models_virt-managing-node-labeling-obsolete-cpu-models">
<title>About node labeling for obsolete CPU models</title>
<simpara>The OpenShift Virtualization Operator uses a predefined list of obsolete CPU models to ensure that a node supports only valid CPU models for scheduled VMs.</simpara>
<simpara>By default, the following CPU models are eliminated from the list of labels generated for the node:</simpara>
<example>
<title>Obsolete CPU models</title>
<screen>"486"
Conroe
athlon
core2duo
coreduo
kvm32
kvm64
n270
pentium
pentium2
pentium3
pentiumpro
phenom
qemu32
qemu64</screen>
</example>
<simpara>This predefined list is not visible in the <literal>HyperConverged</literal> CR. You cannot <emphasis>remove</emphasis> CPU models from this list, but you can add to the list by editing the <literal>spec.obsoleteCPUs.cpuModels</literal> field of the <literal>HyperConverged</literal> CR.</simpara>
</section>
<section xml:id="virt-about-node-labeling-cpu-features_virt-managing-node-labeling-obsolete-cpu-models">
<title>About node labeling for CPU features</title>
<simpara>Through the process of iteration, the base CPU features in the minimum CPU model are eliminated from the list of labels generated for the node.</simpara>
<simpara>For example:</simpara>
<itemizedlist>
<listitem>
<simpara>An environment might have two supported CPU models: <literal>Penryn</literal> and <literal>Haswell</literal>.</simpara>
</listitem>
<listitem>
<simpara>If <literal>Penryn</literal> is specified as the CPU model for <literal>minCPU</literal>, each base CPU feature for <literal>Penryn</literal> is compared to the list of CPU features supported by <literal>Haswell</literal>.</simpara>
<example>
<title>CPU features supported by <literal>Penryn</literal></title>
<screen>apic
clflush
cmov
cx16
cx8
de
fpu
fxsr
lahf_lm
lm
mca
mce
mmx
msr
mtrr
nx
pae
pat
pge
pni
pse
pse36
sep
sse
sse2
sse4.1
ssse3
syscall
tsc</screen>
</example>
<example>
<title>CPU features supported by <literal>Haswell</literal></title>
<screen>aes
apic
avx
avx2
bmi1
bmi2
clflush
cmov
cx16
cx8
de
erms
fma
fpu
fsgsbase
fxsr
hle
invpcid
lahf_lm
lm
mca
mce
mmx
movbe
msr
mtrr
nx
pae
pat
pcid
pclmuldq
pge
pni
popcnt
pse
pse36
rdtscp
rtm
sep
smep
sse
sse2
sse4.1
sse4.2
ssse3
syscall
tsc
tsc-deadline
x2apic
xsave</screen>
</example>
</listitem>
<listitem>
<simpara>If both <literal>Penryn</literal> and <literal>Haswell</literal> support a specific CPU feature, a label is not created for that feature. Labels are generated for CPU features that are supported only by <literal>Haswell</literal> and not by <literal>Penryn</literal>.</simpara>
<example>
<title>Node labels created for CPU features after iteration</title>
<screen>aes
avx
avx2
bmi1
bmi2
erms
fma
fsgsbase
hle
invpcid
movbe
pcid
pclmuldq
popcnt
rdtscp
rtm
sse4.2
tsc-deadline
x2apic
xsave</screen>
</example>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-configuring-obsolete-cpu-models_virt-managing-node-labeling-obsolete-cpu-models">
<title>Configuring obsolete CPU models</title>
<simpara>You can configure a list of obsolete CPU models by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>HyperConverged</literal> custom resource, specifying the obsolete CPU models in the <literal>obsoleteCPUs</literal> array. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: openshift-cnv
spec:
  obsoleteCPUs:
    cpuModels: <co xml:id="CO129-1"/>
      - "&lt;obsolete_cpu_1&gt;"
      - "&lt;obsolete_cpu_2&gt;"
    minCPUModel: "&lt;minimum_cpu_model&gt;" <co xml:id="CO129-2"/></programlisting>
<calloutlist>
<callout arearefs="CO129-1">
<para>Replace the example values in the <literal>cpuModels</literal> array with obsolete CPU models. Any value that you specify is added to a predefined list of obsolete CPU models. The predefined list is not visible in the CR.</para>
</callout>
<callout arearefs="CO129-2">
<para>Replace this value with the minimum CPU model that you want to use for basic CPU features. If you do not specify a value, <literal>Penryn</literal> is used by default.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-using-skip-node">
<title>Preventing node reconciliation</title>

<simpara>Use <literal>skip-node</literal> annotation to prevent the <literal>node-labeller</literal> from reconciling a node.</simpara>
<section xml:id="virt-using-skip-node_virt-preventing-node-reconciliation">
<title>Using skip-node annotation</title>
<simpara>If you want the <literal>node-labeller</literal> to skip a node, annotate that node by using the <literal>oc</literal> CLI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Annotate the node that you want to skip by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate node &lt;node_name&gt; node-labeller.kubevirt.io/skip-node=true <co xml:id="CO130-1"/></programlisting>
<calloutlist>
<callout arearefs="CO130-1">
<para>Replace <literal>&lt;node_name&gt;</literal> with the name of the relevant node to skip.</para>
</callout>
</calloutlist>
<simpara>Reconciliation resumes on the next cycle after the node annotation is removed or set to false.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="additional-resources_virt-preventing-node-reconciliation" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-managing-node-labeling-obsolete-cpu-models">Managing node labeling for obsolete CPU models</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-triggering-vm-failover-resolving-failed-node">
<title>Deleting a failed node to trigger virtual machine failover</title>

<simpara>If a node fails and <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/machine_management/#machine-health-checks-about_deploying-machine-health-checks">machine health checks</link> are not deployed on your cluster, virtual machines (VMs) with <literal>runStrategy: Always</literal> configured are not automatically relocated to healthy nodes. To trigger VM failover, you must manually delete the <literal>Node</literal> object.</simpara>
<note>
<simpara>If you installed your cluster by using <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/installing/#ipi-install-overview">installer-provisioned infrastructure</link> and you properly configured machine health checks, the following events occur:</simpara>
<itemizedlist>
<listitem>
<simpara>Failed nodes are automatically recycled.</simpara>
</listitem>
<listitem>
<simpara>Virtual machines with <link linkend="run-strategies"><literal>runStrategy</literal></link> set to <literal>Always</literal> or <literal>RerunOnFailure</literal> are automatically scheduled on healthy nodes.</simpara>
</listitem>
</itemizedlist>
</note>
<section xml:id="prerequisites_virt-triggering-vm-failover-resolving-failed-node">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>A node where a virtual machine was running has the <literal>NotReady</literal> <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-nodes-viewing-listing_nodes-nodes-viewing">condition</link>.</simpara>
</listitem>
<listitem>
<simpara>The virtual machine that was running on the failed node has <literal>runStrategy</literal> set to <literal>Always</literal>.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="nodes-nodes-working-deleting-bare-metal_virt-triggering-vm-failover-resolving-failed-node">
<title>Deleting nodes from a bare metal cluster</title>
<simpara>When you delete a node using the CLI, the node object is deleted in Kubernetes,
but the pods that exist on the node are not deleted. Any bare pods not backed by
a replication controller become inaccessible to OpenShift Container Platform. Pods backed by
replication controllers are rescheduled to other available nodes. You must
delete local manifest pods.</simpara>
<formalpara>
<title>Procedure</title>
<para>Delete a node from an OpenShift Container Platform cluster running on bare metal by completing
the following steps:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Mark the node as unschedulable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm cordon &lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Drain all pods on the node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm drain &lt;node_name&gt; --force=true</programlisting>
<simpara>This step might fail if the node is offline or unresponsive. Even if the node does not respond, it might still be running a workload that writes to shared storage. To avoid data corruption, power down the physical hardware before you proceed.</simpara>
</listitem>
<listitem>
<simpara>Delete the node from the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete node &lt;node_name&gt;</programlisting>
<simpara>Although the node object is now deleted from the cluster, it can still rejoin
the cluster after reboot or if the kubelet service is restarted. To permanently
delete the node and all its data, you must
<link xlink:href="https://access.redhat.com/solutions/84663">decommission the node</link>.</simpara>
</listitem>
<listitem>
<simpara>If you powered down the physical hardware, turn it back on so that the node can rejoin the cluster.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="verifying-vm-failover_virt-triggering-vm-failover-resolving-failed-node">
<title>Verifying virtual machine failover</title>
<simpara>After all resources are terminated on the unhealthy node, a new virtual machine instance (VMI) is automatically created on a healthy node for each relocated VM. To confirm that the VMI was created, view all VMIs by using the <literal>oc</literal> CLI.</simpara>
<section xml:id="virt-listing-vmis-cli_virt-triggering-vm-failover-resolving-failed-node">
<title>Listing all virtual machine instances using the CLI</title>
<simpara>You can list all virtual machine instances (VMIs) in your cluster, including standalone VMIs and those owned by virtual machines, by using the <literal>oc</literal> command-line interface (CLI).</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>List all VMIs by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmis -A</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_monitoring">
<title>Monitoring</title>
<section xml:id="virt-monitoring-overview">
<title>Monitoring overview</title>

<simpara>You can monitor the health of your cluster and virtual machines (VMs) with the following tools:</simpara>
<variablelist>
<varlistentry>
<term>Monitoring OpenShift Virtualization VMs health status</term>
<listitem>
<simpara>View the overall health of your OpenShift Virtualization environment in the web console by navigating to the <emphasis role="strong">Home</emphasis> &#8594; <emphasis role="strong">Overview</emphasis> page in the OpenShift Container Platform web console. The <emphasis role="strong">Status</emphasis> card displays the overall health of OpenShift Virtualization based on the alerts and conditions.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-running-cluster-checkups">OpenShift Container Platform cluster checkup framework</link></term>
<listitem>
<simpara>Run automated tests on your cluster with the OpenShift Container Platform cluster checkup framework to check the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>Network connectivity and latency between two VMs attached to a secondary network interface</simpara>
</listitem>
<listitem>
<simpara>VM running a Data Plane Development Kit (DPDK) workload with zero packet loss</simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<variablelist>
<varlistentry>
<term><link linkend="virt-prometheus-queries">Prometheus queries for virtual resources</link></term>
<listitem>
<simpara>Query vCPU, network, storage, and guest memory swapping usage and live migration progress.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-exposing-custom-metrics-for-vms">VM custom metrics</link></term>
<listitem>
<simpara>Configure the <literal>node-exporter</literal> service to expose internal VM metrics and processes.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-monitoring-vm-health">VM health checks</link></term>
<listitem>
<simpara>Configure readiness, liveness, and guest agent ping probes and a watchdog for VMs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-runbooks">Runbooks</link></term>
<listitem>
<simpara>Diagnose and resolve issues that trigger OpenShift Virtualization <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#managing-alerts">alerts</link> in the OpenShift Container Platform web console.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="virt-running-cluster-checkups">
<title>OpenShift Virtualization cluster checkup framework</title>

<simpara>OpenShift Virtualization includes the following predefined checkups that can be used for cluster maintenance and troubleshooting:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="virt-measuring-latency-vm-secondary-network_virt-running-cluster-checkups">Latency checkup</link></term>
<listitem>
<simpara>Verifies network connectivity and measures latency between two virtual machines (VMs) that are attached to a secondary network interface.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-checking-cluster-dpdk-readiness_virt-running-cluster-checkups">DPDK checkup</link></term>
<listitem>
<simpara>Verifies that a node can run a VM with a Data Plane Development Kit (DPDK) workload with zero packet loss.</simpara>
</listitem>
</varlistentry>
</variablelist>
<important>
<simpara>The OpenShift Virtualization cluster checkup framework is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="virt-about-cluster-checkup-framework_virt-running-cluster-checkups">
<title>About the OpenShift Virtualization cluster checkup framework</title>
<simpara>A <emphasis>checkup</emphasis> is an automated test workload that allows you to verify if a specific cluster functionality works as expected. The cluster checkup framework uses native Kubernetes resources to configure and execute the checkup.</simpara>
<simpara>By using predefined checkups, cluster administrators and developers can improve cluster maintainability, troubleshoot unexpected behavior, minimize errors, and save time. They can also review the results of the checkup and share them with experts for further analysis. Vendors can write and publish checkups for features or services that they provide and verify that their customer environments are configured correctly.</simpara>
<simpara>Running a predefined checkup in an existing namespace involves setting up a service account for the checkup, creating the <literal>Role</literal> and <literal>RoleBinding</literal> objects for the service account, enabling permissions for the checkup, and creating the input config map and the checkup job. You can run a checkup multiple times.</simpara>
<important>
<simpara>You must always:</simpara>
<itemizedlist>
<listitem>
<simpara>Verify that the checkup image is from a trustworthy source before applying it.</simpara>
</listitem>
<listitem>
<simpara>Review the checkup permissions before creating the <literal>Role</literal> and <literal>RoleBinding</literal> objects.</simpara>
</listitem>
</itemizedlist>
</important>
<section xml:id="virt-measuring-latency-vm-secondary-network_virt-running-cluster-checkups">
<title>Running a latency checkup</title>
<simpara>You use a predefined checkup to verify network connectivity and measure latency between two virtual machines (VMs) that are attached to a secondary network interface. The latency checkup uses the ping utility.</simpara>
<simpara>You run a latency checkup by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a service account, roles, and rolebindings to provide cluster access permissions to the latency checkup.</simpara>
</listitem>
<listitem>
<simpara>Create a config map to provide the input to run the checkup and to store the results.</simpara>
</listitem>
<listitem>
<simpara>Create a job to run the checkup.</simpara>
</listitem>
<listitem>
<simpara>Review the results in the config map.</simpara>
</listitem>
<listitem>
<simpara>Optional: To rerun the checkup, delete the existing config map and job and then create a new config map and job.</simpara>
</listitem>
<listitem>
<simpara>When you are finished, delete the latency checkup resources.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>The cluster has at least two worker nodes.</simpara>
</listitem>
<listitem>
<simpara>You configured a network attachment definition for a namespace.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>ServiceAccount</literal>, <literal>Role</literal>, and <literal>RoleBinding</literal> manifest for the latency checkup:</simpara>
<example>
<title>Example role manifest file</title>
<programlisting language="yaml" linenumbering="unnumbered">---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: vm-latency-checkup-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubevirt-vm-latency-checker
rules:
- apiGroups: ["kubevirt.io"]
  resources: ["virtualmachineinstances"]
  verbs: ["get", "create", "delete"]
- apiGroups: ["subresources.kubevirt.io"]
  resources: ["virtualmachineinstances/console"]
  verbs: ["get"]
- apiGroups: ["k8s.cni.cncf.io"]
  resources: ["network-attachment-definitions"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubevirt-vm-latency-checker
subjects:
- kind: ServiceAccount
  name: vm-latency-checkup-sa
roleRef:
  kind: Role
  name: kubevirt-vm-latency-checker
  apiGroup: rbac.authorization.k8s.io
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kiagnose-configmap-access
rules:
- apiGroups: [ "" ]
  resources: [ "configmaps" ]
  verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kiagnose-configmap-access
subjects:
- kind: ServiceAccount
  name: vm-latency-checkup-sa
roleRef:
  kind: Role
  name: kiagnose-configmap-access
  apiGroup: rbac.authorization.k8s.io</programlisting>
</example>
</listitem>
<listitem>
<simpara>Apply the <literal>ServiceAccount</literal>, <literal>Role</literal>, and <literal>RoleBinding</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;target_namespace&gt; -f &lt;latency_sa_roles_rolebinding&gt;.yaml <co xml:id="CO131-1"/></programlisting>
<calloutlist>
<callout arearefs="CO131-1">
<para><literal>&lt;target_namespace&gt;</literal> is the namespace where the checkup is to be run. This must be an existing namespace where the <literal>NetworkAttachmentDefinition</literal> object resides.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>ConfigMap</literal> manifest that contains the input parameters for the checkup:</simpara>
<formalpara>
<title>Example input config map</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: kubevirt-vm-latency-checkup-config
  labels:
    kiagnose/checkup-type: kubevirt-vm-latency
data:
  spec.timeout: 5m
  spec.param.networkAttachmentDefinitionNamespace: &lt;target_namespace&gt;
  spec.param.networkAttachmentDefinitionName: "blue-network" <co xml:id="CO132-1"/>
  spec.param.maxDesiredLatencyMilliseconds: "10" <co xml:id="CO132-2"/>
  spec.param.sampleDurationSeconds: "5" <co xml:id="CO132-3"/>
  spec.param.sourceNode: "worker1" <co xml:id="CO132-4"/>
  spec.param.targetNode: "worker2" <co xml:id="CO132-5"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO132-1">
<para>The name of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO132-2">
<para>Optional: The maximum desired latency, in milliseconds, between the virtual machines. If the measured latency exceeds this value, the checkup fails.</para>
</callout>
<callout arearefs="CO132-3">
<para>Optional: The duration of the latency check, in seconds.</para>
</callout>
<callout arearefs="CO132-4">
<para>Optional: When specified, latency is measured from this node to the target node. If the source node is specified, the <literal>spec.param.targetNode</literal> field cannot be empty.</para>
</callout>
<callout arearefs="CO132-5">
<para>Optional: When specified, latency is measured from the source node to this node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the config map manifest in the target namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;target_namespace&gt; -f &lt;latency_config_map&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Job</literal> manifest to run the checkup:</simpara>
<formalpara>
<title>Example job manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: batch/v1
kind: Job
metadata:
  name: kubevirt-vm-latency-checkup
  labels:
    kiagnose/checkup-type: kubevirt-vm-latency
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: vm-latency-checkup-sa
      restartPolicy: Never
      containers:
        - name: vm-latency-checkup
          image: registry.redhat.io/container-native-virtualization/vm-network-latency-checkup-rhel9:v4.14.0
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
            runAsNonRoot: true
            seccompProfile:
              type: "RuntimeDefault"
          env:
            - name: CONFIGMAP_NAMESPACE
              value: &lt;target_namespace&gt;
            - name: CONFIGMAP_NAME
              value: kubevirt-vm-latency-checkup-config
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the <literal>Job</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;target_namespace&gt; -f &lt;latency_job&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Wait for the job to complete:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc wait job kubevirt-vm-latency-checkup -n &lt;target_namespace&gt; --for condition=complete --timeout 6m</programlisting>
</listitem>
<listitem>
<simpara>Review the results of the latency checkup by running the following command. If the maximum measured latency is greater than the value of the <literal>spec.param.maxDesiredLatencyMilliseconds</literal> attribute, the checkup fails and returns an error.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap kubevirt-vm-latency-checkup-config -n &lt;target_namespace&gt; -o yaml</programlisting>
<formalpara>
<title>Example output config map (success)</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: kubevirt-vm-latency-checkup-config
  namespace: &lt;target_namespace&gt;
  labels:
    kiagnose/checkup-type: kubevirt-vm-latency
data:
  spec.timeout: 5m
  spec.param.networkAttachmentDefinitionNamespace: &lt;target_namespace&gt;
  spec.param.networkAttachmentDefinitionName: "blue-network"
  spec.param.maxDesiredLatencyMilliseconds: "10"
  spec.param.sampleDurationSeconds: "5"
  spec.param.sourceNode: "worker1"
  spec.param.targetNode: "worker2"
  status.succeeded: "true"
  status.failureReason: ""
  status.completionTimestamp: "2022-01-01T09:00:00Z"
  status.startTimestamp: "2022-01-01T09:00:07Z"
  status.result.avgLatencyNanoSec: "177000"
  status.result.maxLatencyNanoSec: "244000" <co xml:id="CO133-1"/>
  status.result.measurementDurationSec: "5"
  status.result.minLatencyNanoSec: "135000"
  status.result.sourceNode: "worker1"
  status.result.targetNode: "worker2"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO133-1">
<para>The maximum measured latency in nanoseconds.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Optional: To view the detailed job log in case of checkup failure, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs job.batch/kubevirt-vm-latency-checkup -n &lt;target_namespace&gt;</programlisting>
</listitem>
<listitem>
<simpara>Delete the job and config map that you previously created by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete job -n &lt;target_namespace&gt; kubevirt-vm-latency-checkup</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete config-map -n &lt;target_namespace&gt; kubevirt-vm-latency-checkup-config</programlisting>
</listitem>
<listitem>
<simpara>Optional: If you do not plan to run another checkup, delete the roles manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -f &lt;latency_sa_roles_rolebinding&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-checking-cluster-dpdk-readiness_virt-running-cluster-checkups">
<title>DPDK checkup</title>
<simpara>Use a predefined checkup to verify that your OpenShift Container Platform cluster node can run a virtual machine (VM) with a Data Plane Development Kit (DPDK) workload with zero packet loss. The DPDK checkup runs traffic between a traffic generator and a VM running a test DPDK application.</simpara>
<simpara>You run a DPDK checkup by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a service account, role, and role bindings for the DPDK checkup.</simpara>
</listitem>
<listitem>
<simpara>Create a config map to provide the input to run the checkup and to store the results.</simpara>
</listitem>
<listitem>
<simpara>Create a job to run the checkup.</simpara>
</listitem>
<listitem>
<simpara>Review the results in the config map.</simpara>
</listitem>
<listitem>
<simpara>Optional: To rerun the checkup, delete the existing config map and job and then create a new config map and job.</simpara>
</listitem>
<listitem>
<simpara>When you are finished, delete the DPDK checkup resources.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>The cluster is configured to run DPDK applications.</simpara>
</listitem>
<listitem>
<simpara>The project is configured to run DPDK applications.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>ServiceAccount</literal>, <literal>Role</literal>, and <literal>RoleBinding</literal> manifest for the DPDK checkup:</simpara>
<example>
<title>Example service account, role, and rolebinding manifest file</title>
<programlisting language="yaml" linenumbering="unnumbered">---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: dpdk-checkup-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kiagnose-configmap-access
rules:
  - apiGroups: [ "" ]
    resources: [ "configmaps" ]
    verbs: [ "get", "update" ]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kiagnose-configmap-access
subjects:
  - kind: ServiceAccount
    name: dpdk-checkup-sa
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kiagnose-configmap-access
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubevirt-dpdk-checker
rules:
  - apiGroups: [ "kubevirt.io" ]
    resources: [ "virtualmachineinstances" ]
    verbs: [ "create", "get", "delete" ]
  - apiGroups: [ "subresources.kubevirt.io" ]
    resources: [ "virtualmachineinstances/console" ]
    verbs: [ "get" ]
  - apiGroups: [ "" ]
    resources: [ "configmaps" ]
    verbs: [ "create", "delete" ]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubevirt-dpdk-checker
subjects:
  - kind: ServiceAccount
    name: dpdk-checkup-sa
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubevirt-dpdk-checker</programlisting>
</example>
</listitem>
<listitem>
<simpara>Apply the <literal>ServiceAccount</literal>, <literal>Role</literal>, and <literal>RoleBinding</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;target_namespace&gt; -f &lt;dpdk_sa_roles_rolebinding&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>ConfigMap</literal> manifest that contains the input parameters for the checkup:</simpara>
<formalpara>
<title>Example input config map</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: dpdk-checkup-config
  labels:
    kiagnose/checkup-type: kubevirt-dpdk
data:
  spec.timeout: 10m
  spec.param.networkAttachmentDefinitionName: &lt;network_name&gt; <co xml:id="CO134-1"/>
  spec.param.trafficGenContainerDiskImage: "quay.io/kiagnose/kubevirt-dpdk-checkup-traffic-gen:v0.3.1 <co xml:id="CO134-2"/>
  spec.param.vmUnderTestContainerDiskImage: "quay.io/kiagnose/kubevirt-dpdk-checkup-vm:v0.3.1" <co xml:id="CO134-3"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO134-1">
<para>The name of the <literal>NetworkAttachmentDefinition</literal> object.</para>
</callout>
<callout arearefs="CO134-2">
<para>The container disk image for the traffic generator. In this example, the image is pulled from the upstream Project Quay Container Registry.</para>
</callout>
<callout arearefs="CO134-3">
<para>The container disk image for the VM under test. In this example, the image is pulled from the upstream Project Quay Container Registry.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the <literal>ConfigMap</literal> manifest in the target namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;target_namespace&gt; -f &lt;dpdk_config_map&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Job</literal> manifest to run the checkup:</simpara>
<formalpara>
<title>Example job manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: batch/v1
kind: Job
metadata:
  name: dpdk-checkup
  labels:
    kiagnose/checkup-type: kubevirt-dpdk
spec:
  backoffLimit: 0
  template:
    spec:
      serviceAccountName: dpdk-checkup-sa
      restartPolicy: Never
      containers:
        - name: dpdk-checkup
          image: registry.redhat.io/container-native-virtualization/kubevirt-dpdk-checkup-rhel9:v4.14.0
          imagePullPolicy: Always
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop: ["ALL"]
            runAsNonRoot: true
            seccompProfile:
              type: "RuntimeDefault"
          env:
            - name: CONFIGMAP_NAMESPACE
              value: &lt;target-namespace&gt;
            - name: CONFIGMAP_NAME
              value: dpdk-checkup-config
            - name: POD_UID
              valueFrom:
                fieldRef:
                  fieldPath: metadata.uid</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the <literal>Job</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -n &lt;target_namespace&gt; -f &lt;dpdk_job&gt;.yaml</programlisting>
</listitem>
<listitem>
<simpara>Wait for the job to complete:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc wait job dpdk-checkup -n &lt;target_namespace&gt; --for condition=complete --timeout 10m</programlisting>
</listitem>
<listitem>
<simpara>Review the results of the checkup by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap dpdk-checkup-config -n &lt;target_namespace&gt; -o yaml</programlisting>
<formalpara>
<title>Example output config map (success)</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: dpdk-checkup-config
  labels:
    kiagnose/checkup-type: kubevirt-dpdk
data:
  spec.timeout: 10m
  spec.param.NetworkAttachmentDefinitionName: "dpdk-network-1"
  spec.param.trafficGenContainerDiskImage: "quay.io/kiagnose/kubevirt-dpdk-checkup-traffic-gen:v0.2.0"
  spec.param.vmUnderTestContainerDiskImage: "quay.io/kiagnose/kubevirt-dpdk-checkup-vm:v0.2.0"
  status.succeeded: "true" <co xml:id="CO135-1"/>
  status.failureReason: "" <co xml:id="CO135-2"/>
  status.startTimestamp: "2023-07-31T13:14:38Z" <co xml:id="CO135-3"/>
  status.completionTimestamp: "2023-07-31T13:19:41Z" <co xml:id="CO135-4"/>
  status.result.trafficGenSentPackets: "480000000" <co xml:id="CO135-5"/>
  status.result.trafficGenOutputErrorPackets: "0" <co xml:id="CO135-6"/>
  status.result.trafficGenInputErrorPackets: "0" <co xml:id="CO135-7"/>
  status.result.trafficGenActualNodeName: worker-dpdk1 <co xml:id="CO135-8"/>
  status.result.vmUnderTestActualNodeName: worker-dpdk2 <co xml:id="CO135-9"/>
  status.result.vmUnderTestReceivedPackets: "480000000" <co xml:id="CO135-10"/>
  status.result.vmUnderTestRxDroppedPackets: "0" <co xml:id="CO135-11"/>
  status.result.vmUnderTestTxDroppedPackets: "0" <co xml:id="CO135-12"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO135-1">
<para>Specifies if the checkup is successful (<literal>true</literal>) or not (<literal>false</literal>).</para>
</callout>
<callout arearefs="CO135-2">
<para>The reason for failure if the checkup fails.</para>
</callout>
<callout arearefs="CO135-3">
<para>The time when the checkup started, in RFC 3339 time format.</para>
</callout>
<callout arearefs="CO135-4">
<para>The time when the checkup has completed, in RFC 3339 time format.</para>
</callout>
<callout arearefs="CO135-5">
<para>The number of packets sent from the traffic generator.</para>
</callout>
<callout arearefs="CO135-6">
<para>The number of error packets sent from the traffic generator.</para>
</callout>
<callout arearefs="CO135-7">
<para>The number of error packets received by the traffic generator.</para>
</callout>
<callout arearefs="CO135-8">
<para>The node on which the traffic generator VM was scheduled.</para>
</callout>
<callout arearefs="CO135-9">
<para>The node on which the VM under test was scheduled.</para>
</callout>
<callout arearefs="CO135-10">
<para>The number of packets received on the VM under test.</para>
</callout>
<callout arearefs="CO135-11">
<para>The ingress traffic packets that were dropped by the DPDK application.</para>
</callout>
<callout arearefs="CO135-12">
<para>The egress traffic packets that were dropped from the DPDK application.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Delete the job and config map that you previously created by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete job -n &lt;target_namespace&gt; dpdk-checkup</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete config-map -n &lt;target_namespace&gt; dpdk-checkup-config</programlisting>
</listitem>
<listitem>
<simpara>Optional: If you do not plan to run another checkup, delete the <literal>ServiceAccount</literal>, <literal>Role</literal>, and <literal>RoleBinding</literal> manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -f &lt;dpdk_sa_roles_rolebinding&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<section xml:id="virt-dpdk-config-map-parameters_virt-running-cluster-checkups">
<title>DPDK checkup config map parameters</title>
<simpara>The following table shows the mandatory and optional parameters that you can set in the <literal>data</literal> stanza of the input <literal>ConfigMap</literal> manifest when you run a cluster DPDK readiness checkup:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>DPDK checkup config map input parameters</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Parameter</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Is Mandatory</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>spec.timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The time, in minutes, before the checkup fails.</simpara></entry>
<entry align="left" valign="top"><simpara>True</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.networkAttachmentDefinitionName</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The name of the <literal>NetworkAttachmentDefinition</literal> object of the SR-IOV NICs connected.</simpara></entry>
<entry align="left" valign="top"><simpara>True</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.trafficGenContainerDiskImage</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The container disk image for the traffic generator. The default value is <literal>quay.io/kiagnose/kubevirt-dpdk-checkup-traffic-gen:main</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.trafficGenTargetNodeName</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The node on which the traffic generator VM is to be scheduled. The node should be configured to allow DPDK traffic.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.trafficGenPacketsPerSecond</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The number of packets per second, in kilo (k) or million(m). The default value is 8m.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.vmUnderTestContainerDiskImage</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The container disk image for the VM under test. The default value is <literal>quay.io/kiagnose/kubevirt-dpdk-checkup-vm:main</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.vmUnderTestTargetNodeName</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The node on which the VM under test is to be scheduled. The node should be configured to allow DPDK traffic.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.testDuration</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The duration, in minutes, for which the traffic generator runs. The default value is 5 minutes.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.portBandwidthGbps</literal></simpara></entry>
<entry align="left" valign="top"><simpara>The maximum bandwidth of the SR-IOV NIC. The default value is 10Gbps.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>spec.param.verbose</literal></simpara></entry>
<entry align="left" valign="top"><simpara>When set to <literal>true</literal>, it increases the verbosity of the checkup log. The default value is <literal>false</literal>.</simpara></entry>
<entry align="left" valign="top"><simpara>False</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="virt-building-vm-containerdisk-image_virt-running-cluster-checkups">
<title>Building a container disk image for RHEL virtual machines</title>
<simpara>You can build a custom Red Hat Enterprise Linux (RHEL) 8 OS image in <literal>qcow2</literal> format and use it to create a container disk image. You can store the container disk image in a registry that is accessible from your cluster and specify the image location in the <literal>spec.param.vmContainerDiskImage</literal> attribute of the DPDK checkup config map.</simpara>
<simpara>To build a container disk image, you must create an image builder virtual machine (VM). The <emphasis>image builder VM</emphasis> is a RHEL 8 VM that can be used to build custom RHEL images.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The image builder VM must run RHEL 8.7 and must have a minimum of 2 CPU cores, 4 GiB RAM, and 20 GB of free space in the <literal>/var</literal> directory.</simpara>
</listitem>
<listitem>
<simpara>You have installed the image builder tool and its CLI (<literal>composer-cli</literal>) on the VM.</simpara>
</listitem>
<listitem>
<simpara>You have installed the <literal>virt-customize</literal> tool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># dnf install libguestfs-tools</programlisting>
</listitem>
<listitem>
<simpara>You have installed the Podman CLI tool (<literal>podman</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify that you can build a RHEL 8.7 image:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># composer-cli distros list</programlisting>
<note>
<simpara>To run the <literal>composer-cli</literal> commands as non-root, add your user to the <literal>weldr</literal> or <literal>root</literal> groups:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># usermod -a -G weldr user</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ newgrp weldr</programlisting>
</note>
</listitem>
<listitem>
<simpara>Enter the following command to create an image blueprint file in TOML format that contains the packages to be installed, kernel customizations, and the services to be disabled during boot time:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; dpdk-vm.toml
name = "dpdk_image"
description = "Image to use with the DPDK checkup"
version = "0.0.1"
distro = "rhel-87"

[[customizations.user]]
name = "root"
password = "redhat"

[[packages]]
name = "dpdk"

[[packages]]
name = "dpdk-tools"

[[packages]]
name = "driverctl"

[[packages]]
name = "tuned-profiles-cpu-partitioning"

[customizations.kernel]
append = "default_hugepagesz=1GB hugepagesz=1G hugepages=1"

[customizations.services]
disabled = ["NetworkManager-wait-online", "sshd"]
EOF</programlisting>
</listitem>
<listitem>
<simpara>Push the blueprint file to the image builder tool by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># composer-cli blueprints push dpdk-vm.toml</programlisting>
</listitem>
<listitem>
<simpara>Generate the system image by specifying the blueprint name and output file format. The Universally Unique Identifier (UUID) of the image is displayed when you start the compose process.</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># composer-cli compose start dpdk_image qcow2</programlisting>
</listitem>
<listitem>
<simpara>Wait for the compose process to complete. The compose status must show <literal>FINISHED</literal> before you can continue to the next step.</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># composer-cli compose status</programlisting>
</listitem>
<listitem>
<simpara>Enter the following command to download the <literal>qcow2</literal> image file by specifying its UUID:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># composer-cli compose image &lt;UUID&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create the customization scripts by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF &gt;customize-vm
#!/bin/bash

# Setup hugepages mount
mkdir -p /mnt/huge
echo "hugetlbfs /mnt/huge hugetlbfs defaults,pagesize=1GB 0 0" &gt;&gt; /etc/fstab

# Create vfio-noiommu.conf
echo "options vfio enable_unsafe_noiommu_mode=1" &gt; /etc/modprobe.d/vfio-noiommu.conf

# Enable guest-exec,guest-exec-status on the qemu-guest-agent configuration
sed -i '/^BLACKLIST_RPC=/ { s/guest-exec-status//; s/guest-exec//g }' /etc/sysconfig/qemu-ga
sed -i '/^BLACKLIST_RPC=/ { s/,\+/,/g; s/^,\|,$//g }' /etc/sysconfig/qemu-ga
EOF</programlisting>
</listitem>
<listitem>
<simpara>Use the <literal>virt-customize</literal> tool to customize the image generated by the image builder tool:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virt-customize -a &lt;UUID&gt;-disk.qcow2 --run=customize-vm --selinux-relabel</programlisting>
</listitem>
<listitem>
<simpara>To create a Dockerfile that contains all the commands to build the container disk image, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; Dockerfile
FROM scratch
COPY --chown=107:107 &lt;UUID&gt;-disk.qcow2 /disk/
EOF</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;UUID&gt;-disk.qcow2</term>
<listitem>
<simpara>Specifies the name of the custom image in <literal>qcow2</literal> format.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Build and tag the container by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman build . -t dpdk-rhel:latest</programlisting>
</listitem>
<listitem>
<simpara>Push the container disk image to a registry that is accessible from your cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ podman push dpdk-rhel:latest</programlisting>
</listitem>
<listitem>
<simpara>Provide a link to the container disk image in the <literal>spec.param.vmUnderTestContainerDiskImage</literal> attribute in the DPDK checkup config map.</simpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="additional-resources_running-cluster-checkups" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link linkend="virt-connecting-vm-to-linux-bridge">Attaching a virtual machine to multiple networks</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#example-vf-use-in-dpdk-mode-intel_using-dpdk-and-rdma">Using a virtual function in DPDK mode with an Intel NIC</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/networking/#nw-example-dpdk-line-rate_using-dpdk-and-rdma">Using SR-IOV and the Node Tuning Operator to achieve a DPDK line rate</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/composing_a_customized_rhel_system_image/installing-composer_composing-a-customized-rhel-system-image">Installing image builder</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/253273">How to register and subscribe a RHEL system to the Red Hat Customer Portal using Red Hat Subscription Manager</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-prometheus-queries">
<title>Prometheus queries for virtual resources</title>

<simpara>OpenShift Virtualization provides metrics that you can use to monitor the consumption of cluster infrastructure resources, including vCPU, network, storage, and guest memory swapping. You can also use metrics to query live migration status.</simpara>
<simpara>Use the OpenShift Container Platform monitoring dashboard to query virtualization metrics.</simpara>
<section xml:id="prerequisites_virt-prometheus-queries">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>To use the vCPU metric, the <literal>schedstats=enable</literal> kernel argument must be applied to the <literal>MachineConfig</literal> object. This kernel argument enables scheduler statistics used for debugging and performance tuning and adds a minor additional load to the scheduler. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/postinstallation_configuration/#nodes-nodes-kernel-arguments_post-install-machine-configuration-tasks">Adding kernel arguments to nodes</link>.</simpara>
</listitem>
<listitem>
<simpara>For guest memory swapping queries to return data, memory swapping must be enabled on the virtual guests.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-querying-metrics_virt-prometheus-queries">
<title>Querying metrics</title>
<simpara>The OpenShift Container Platform monitoring dashboard enables you to run Prometheus Query Language (PromQL) queries to examine metrics visualized on a plot. This functionality provides information about the state of a cluster and any user-defined workloads that you are monitoring.</simpara>
<simpara>As a cluster administrator, you can query metrics for all core OpenShift Container Platform and user-defined projects.</simpara>
<simpara>As a developer, you must specify a project name when querying metrics. You must have the required privileges to view metrics for the selected project.</simpara>
<section xml:id="querying-metrics-for-all-projects-as-an-administrator_virt-prometheus-queries">
<title>Querying metrics for all projects as a cluster administrator</title>
<simpara>As a
cluster administrator
or as a user with view permissions for all projects, you can access metrics for all default OpenShift Container Platform and user-defined projects in the Metrics UI.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> cluster role or with view permissions for all projects.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the <emphasis role="strong">Administrator</emphasis> perspective in the OpenShift Container Platform web console, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>To add one or more queries, do any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Create a custom query.</simpara></entry>
<entry align="left" valign="top"><simpara>Add your Prometheus Query Language (PromQL) query to the <emphasis role="strong">Expression</emphasis> field.</simpara><simpara>As you type a PromQL expression, autocomplete suggestions appear in a drop-down list. These suggestions include functions, metrics, labels, and time tokens.
You can use the keyboard arrows to select one of these suggested items and then press Enter to add the item to your expression. You can also move your mouse pointer over a suggested item to view a brief description of that item.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Add multiple queries.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Add query</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Duplicate an existing query.</simpara></entry>
<entry align="left" valign="top"><simpara>Select the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to the query, then choose <emphasis role="strong">Duplicate query</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Disable a query from being run.</simpara></entry>
<entry align="left" valign="top"><simpara>Select the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> next to the query and choose <emphasis role="strong">Disable query</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
<listitem>
<simpara>To run queries that you created, select <emphasis role="strong">Run queries</emphasis>. The metrics from the queries are visualized on the plot. If a query is invalid, the UI shows an error message.</simpara>
<note>
<simpara>Queries that operate on large amounts of data might time out or overload the browser when drawing time series graphs. To avoid this, select <emphasis role="strong">Hide graph</emphasis> and calibrate your query using only the metrics table. Then, after finding a feasible query, enable the plot to draw the graphs.</simpara>
</note>
<note>
<simpara>By default, the query table shows an expanded view that lists every metric and its current value. You can select <emphasis role="strong">Ë…</emphasis> to minimize the expanded view for a query.</simpara>
</note>
</listitem>
<listitem>
<simpara>Optional: The page URL now contains the queries you ran. To use this set of queries again in the future, save this URL.</simpara>
</listitem>
<listitem>
<simpara>Explore the visualized metrics. Initially, all metrics from all enabled queries are shown on the plot. You can select which metrics are shown by doing any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Hide all metrics from a query.</simpara></entry>
<entry align="left" valign="top"><simpara>Click the Options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> for the query and click <emphasis role="strong">Hide all series</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hide a specific metric.</simpara></entry>
<entry align="left" valign="top"><simpara>Go to the query table and click the colored square near the metric name.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Zoom into the plot and change the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Either:</simpara>
<itemizedlist>
<listitem>
<simpara>Visually select the time range by clicking and dragging on the plot horizontally.</simpara>
</listitem>
<listitem>
<simpara>Use the menu in the left upper corner to select the time range.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reset the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Reset zoom</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Display outputs for all queries at a specific point in time.</simpara></entry>
<entry align="left" valign="top"><simpara>Hold the mouse cursor on the plot at that point. The query outputs will appear in a pop-up box.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Hide the plot.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Hide graph</emphasis>.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
<section xml:id="querying-metrics-for-user-defined-projects-as-a-developer_virt-prometheus-queries">
<title>Querying metrics for user-defined projects as a developer</title>
<simpara>You can access metrics for a user-defined project as a developer or as a user with view permissions for the project.</simpara>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, the Metrics UI includes some predefined CPU, memory, bandwidth, and network packet queries for the selected project. You can also run custom Prometheus Query Language (PromQL) queries for CPU, memory, bandwidth, network packet and application metrics for the project.</simpara>
<note>
<simpara>Developers can only use the <emphasis role="strong">Developer</emphasis> perspective and not the <emphasis role="strong">Administrator</emphasis> perspective. As a developer, you can only query metrics for one project at a time.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a developer or as a user with view permissions for the project that you are viewing metrics for.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for user-defined projects.</simpara>
</listitem>
<listitem>
<simpara>You have deployed a service in a user-defined project.</simpara>
</listitem>
<listitem>
<simpara>You have created a <literal>ServiceMonitor</literal> custom resource definition (CRD) for the service to define how the service is monitored.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>From the <emphasis role="strong">Developer</emphasis> perspective in the OpenShift Container Platform web console, select <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the project that you want to view metrics for in the <emphasis role="strong">Project:</emphasis> list.</simpara>
</listitem>
<listitem>
<simpara>Select a query from the <emphasis role="strong">Select query</emphasis> list, or create a custom PromQL query based on the selected query by selecting <emphasis role="strong">Show PromQL</emphasis>. The metrics from the queries are visualized on the plot.</simpara>
<note>
<simpara>In the Developer perspective, you can only run one query at a time.</simpara>
</note>
</listitem>
<listitem>
<simpara>Explore the visualized metrics by doing any of the following:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Option</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>Zoom into the plot and change the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Either:</simpara>
<itemizedlist>
<listitem>
<simpara>Visually select the time range by clicking and dragging on the plot horizontally.</simpara>
</listitem>
<listitem>
<simpara>Use the menu in the left upper corner to select the time range.</simpara>
</listitem>
</itemizedlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Reset the time range.</simpara></entry>
<entry align="left" valign="top"><simpara>Select <emphasis role="strong">Reset zoom</emphasis>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Display outputs for all queries at a specific point in time.</simpara></entry>
<entry align="left" valign="top"><simpara>Hold the mouse cursor on the plot at that point. The query outputs appear in a pop-up box.</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-querying-metrics_virt-prometheus-queries">
<title>Virtualization metrics</title>
<simpara>The following metric descriptions include example Prometheus Query Language (PromQL) queries. These metrics are not an API and might change between versions.</simpara>
<note>
<simpara>The following examples use <literal>topk</literal> queries that specify a time period. If virtual machines are deleted during that time period, they can still appear in the query output.</simpara>
</note>
<section xml:id="virt-promql-vcpu-metrics_virt-prometheus-queries">
<title>vCPU metrics</title>
<simpara>The following query can identify virtual machines that are waiting for Input/Output (I/O):</simpara>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmi_vcpu_wait_seconds_total</literal></term>
<listitem>
<simpara>Returns the wait time (in seconds) for a virtual machine&#8217;s vCPU. Type: Counter.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>A value above '0' means that the vCPU wants to run, but the host scheduler cannot run it yet. This inability to run indicates that there is an issue with I/O.</simpara>
<note>
<simpara>To query the vCPU metric, the <literal>schedstats=enable</literal> kernel argument must first be applied to the <literal>MachineConfig</literal> object. This kernel argument enables scheduler statistics used for debugging and performance tuning and adds a minor additional load to the scheduler.</simpara>
</note>
<formalpara>
<title>Example vCPU wait time query</title>
<para>
<programlisting language="promql" linenumbering="unnumbered">topk(3, sum by (name, namespace) (rate(kubevirt_vmi_vcpu_wait_seconds_total[6m]))) &gt; 0 <co xml:id="CO136-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO136-1">
<para>This query returns the top 3 VMs waiting for I/O at every given moment over a six-minute time period.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-promql-network-metrics_virt-prometheus-queries">
<title>Network metrics</title>
<simpara>The following queries can identify virtual machines that are saturating the network:</simpara>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmi_network_receive_bytes_total</literal></term>
<listitem>
<simpara>Returns the total amount of traffic received (in bytes) on the virtual machine&#8217;s network. Type: Counter.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_network_transmit_bytes_total</literal></term>
<listitem>
<simpara>Returns the total amount of traffic transmitted (in bytes) on the virtual machine&#8217;s network. Type: Counter.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example network traffic query</title>
<para>
<programlisting language="promql" linenumbering="unnumbered">topk(3, sum by (name, namespace) (rate(kubevirt_vmi_network_receive_bytes_total[6m])) + sum by (name, namespace) (rate(kubevirt_vmi_network_transmit_bytes_total[6m]))) &gt; 0 <co xml:id="CO137-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO137-1">
<para>This query returns the top 3 VMs transmitting the most network traffic at every given moment over a six-minute time period.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-promql-storage-metrics_virt-prometheus-queries">
<title>Storage metrics</title>
<section xml:id="virt-storage-traffic_virt-prometheus-queries">
<title>Storage-related traffic</title>
<simpara>The following queries can identify VMs that are writing large amounts of data:</simpara>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmi_storage_read_traffic_bytes_total</literal></term>
<listitem>
<simpara>Returns the total amount (in bytes) of the virtual machine&#8217;s storage-related traffic. Type: Counter.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_storage_write_traffic_bytes_total</literal></term>
<listitem>
<simpara>Returns the total amount of storage writes (in bytes) of the virtual machine&#8217;s storage-related traffic. Type: Counter.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example storage-related traffic query</title>
<para>
<programlisting language="promql" linenumbering="unnumbered">topk(3, sum by (name, namespace) (rate(kubevirt_vmi_storage_read_traffic_bytes_total[6m])) + sum by (name, namespace) (rate(kubevirt_vmi_storage_write_traffic_bytes_total[6m]))) &gt; 0 <co xml:id="CO138-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO138-1">
<para>This query returns the top 3 VMs performing the most storage traffic at every given moment over a six-minute time period.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-storage-snapshot-data_virt-prometheus-queries">
<title>Storage snapshot data</title>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmsnapshot_disks_restored_from_source</literal></term>
<listitem>
<simpara>Returns the total number of virtual machine disks restored from the source virtual machine. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmsnapshot_disks_restored_from_source_bytes</literal></term>
<listitem>
<simpara>Returns the amount of space in bytes restored from the source virtual machine. Type: Gauge.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Examples of storage snapshot data queries</title>
<para>
<programlisting language="promql" linenumbering="unnumbered">kubevirt_vmsnapshot_disks_restored_from_source{vm_name="simple-vm", vm_namespace="default"} <co xml:id="CO139-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO139-1">
<para>This query returns the total number of virtual machine disks restored from the source virtual machine.</para>
</callout>
</calloutlist>
<programlisting language="promql" linenumbering="unnumbered">kubevirt_vmsnapshot_disks_restored_from_source_bytes{vm_name="simple-vm", vm_namespace="default"} <co xml:id="CO140-1"/></programlisting>
<calloutlist>
<callout arearefs="CO140-1">
<para>This query returns the amount of space in bytes restored from the source virtual machine.</para>
</callout>
</calloutlist>
</section>
<section xml:id="virt-iops_virt-prometheus-queries">
<title>I/O performance</title>
<simpara>The following queries can determine the I/O performance of storage devices:</simpara>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmi_storage_iops_read_total</literal></term>
<listitem>
<simpara>Returns the amount of write I/O operations the virtual machine is performing per second. Type: Counter.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_storage_iops_write_total</literal></term>
<listitem>
<simpara>Returns the amount of read I/O operations the virtual machine is performing per second. Type: Counter.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example I/O performance query</title>
<para>
<programlisting language="promql" linenumbering="unnumbered">topk(3, sum by (name, namespace) (rate(kubevirt_vmi_storage_iops_read_total[6m])) + sum by (name, namespace) (rate(kubevirt_vmi_storage_iops_write_total[6m]))) &gt; 0 <co xml:id="CO141-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO141-1">
<para>This query returns the top 3 VMs performing the most I/O operations per second at every given moment over a six-minute time period.</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="virt-promql-guest-memory-metrics_virt-prometheus-queries">
<title>Guest memory swapping metrics</title>
<simpara>The following queries can identify which swap-enabled guests are performing the most memory swapping:</simpara>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmi_memory_swap_in_traffic_bytes</literal></term>
<listitem>
<simpara>Returns the total amount (in bytes) of memory the virtual guest is swapping in. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_memory_swap_out_traffic_bytes</literal></term>
<listitem>
<simpara>Returns the total amount (in bytes) of memory the virtual guest is swapping out. Type: Gauge.</simpara>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Example memory swapping query</title>
<para>
<programlisting language="promql" linenumbering="unnumbered">topk(3, sum by (name, namespace) (rate(kubevirt_vmi_memory_swap_in_traffic_bytes[6m])) + sum by (name, namespace) (rate(kubevirt_vmi_memory_swap_out_traffic_bytes[6m]))) &gt; 0 <co xml:id="CO142-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO142-1">
<para>This query returns the top 3 VMs where the guest is performing the most memory swapping at every given moment over a six-minute time period.</para>
</callout>
</calloutlist>
<note>
<simpara>Memory swapping indicates that the virtual machine is under memory pressure. Increasing the memory allocation of the virtual machine can mitigate this issue.</simpara>
</note>
</section>
<section xml:id="virt-live-migration-metrics_virt-prometheus-queries">
<title>Live migration metrics</title>
<simpara>The following metrics can be queried to show live migration status:</simpara>
<variablelist>
<varlistentry>
<term><literal>kubevirt_vmi_migration_data_processed_bytes</literal></term>
<listitem>
<simpara>The amount of guest operating system data that has migrated to the new virtual machine (VM). Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migration_data_remaining_bytes</literal></term>
<listitem>
<simpara>The amount of guest operating system data that remains to be migrated. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migration_memory_transfer_rate_bytes</literal></term>
<listitem>
<simpara>The rate at which memory is becoming dirty in the guest operating system. Dirty memory is data that has been changed but not yet written to disk. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migrations_in_pending_phase</literal></term>
<listitem>
<simpara>The number of pending migrations. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migrations_in_scheduling_phase</literal></term>
<listitem>
<simpara>The number of scheduling migrations. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migrations_in_running_phase</literal></term>
<listitem>
<simpara>The number of running migrations. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migration_succeeded</literal></term>
<listitem>
<simpara>The number of successfully completed migrations. Type: Gauge.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>kubevirt_vmi_migration_failed</literal></term>
<listitem>
<simpara>The number of failed migrations. Type: Gauge.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="additional-resources_virt-prometheus-queries" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#monitoring-overview">Monitoring overview</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Querying Prometheus</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://prometheus.io/docs/prometheus/latest/querying/examples/">Prometheus query examples</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-exposing-custom-metrics-for-vms">
<title>Exposing custom metrics for virtual machines</title>

<simpara>OpenShift Container Platform includes a preconfigured, preinstalled, and self-updating monitoring stack that provides monitoring for core platform components. This monitoring stack is based on the Prometheus monitoring system. Prometheus is a time-series database and a rule evaluation engine for metrics.</simpara>
<simpara>In addition to using the OpenShift Container Platform monitoring stack, you can enable monitoring for user-defined projects by using the CLI and query custom metrics that are exposed for virtual machines through the <literal>node-exporter</literal> service.</simpara>
<section xml:id="virt-configuring-node-exporter-service_virt-exposing-custom-metrics-for-vms">
<title>Configuring the node exporter service</title>
<simpara>The node-exporter agent is deployed on every virtual machine in the cluster from which you want to collect metrics. Configure the node-exporter agent as a service to expose internal metrics and processes that are associated with virtual machines.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI <literal>oc</literal>.</simpara>
</listitem>
<listitem>
<simpara>Log in to the cluster as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Configure the <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-user-workload-monitoring</literal> project by setting <literal>enableUserWorkload</literal> to <literal>true</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>Service</literal> YAML file. In the following example, the file is called <literal>node-exporter-service.yaml</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: Service
apiVersion: v1
metadata:
  name: node-exporter-service <co xml:id="CO143-1"/>
  namespace: dynamation <co xml:id="CO143-2"/>
  labels:
    servicetype: metrics <co xml:id="CO143-3"/>
spec:
  ports:
    - name: exmet <co xml:id="CO143-4"/>
      protocol: TCP
      port: 9100 <co xml:id="CO143-5"/>
      targetPort: 9100 <co xml:id="CO143-6"/>
  type: ClusterIP
  selector:
    monitor: metrics <co xml:id="CO143-7"/></programlisting>
<calloutlist>
<callout arearefs="CO143-1">
<para>The node-exporter service that exposes the metrics from the virtual machines.</para>
</callout>
<callout arearefs="CO143-2">
<para>The namespace where the service is created.</para>
</callout>
<callout arearefs="CO143-3">
<para>The label for the service. The <literal>ServiceMonitor</literal> uses this label to match this service.</para>
</callout>
<callout arearefs="CO143-4">
<para>The name given to the port that exposes metrics on port 9100 for the <literal>ClusterIP</literal> service.</para>
</callout>
<callout arearefs="CO143-5">
<para>The target port used by <literal>node-exporter-service</literal> to listen for requests.</para>
</callout>
<callout arearefs="CO143-6">
<para>The TCP port number of the virtual machine that is configured with the <literal>monitor</literal> label.</para>
</callout>
<callout arearefs="CO143-7">
<para>The label used to match the virtual machine&#8217;s pods. In this example, any virtual machine&#8217;s pod with the label <literal>monitor</literal> and a value of <literal>metrics</literal> will be matched.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the node-exporter service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f node-exporter-service.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-configuring-vm-with-node-exporter-service_virt-exposing-custom-metrics-for-vms">
<title>Configuring a virtual machine with the node exporter service</title>
<simpara>Download the <literal>node-exporter</literal> file on to the virtual machine. Then, create a <literal>systemd</literal> service that runs the node-exporter service when the virtual machine boots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The pods for the component are running in the <literal>openshift-user-workload-monitoring</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Grant the <literal>monitoring-edit</literal> role to users who need to monitor this user-defined project.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log on to the virtual machine.</simpara>
</listitem>
<listitem>
<simpara>Download the <literal>node-exporter</literal> file on to the virtual machine by using the directory path that applies to the version of <literal>node-exporter</literal> file.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ wget https://github.com/prometheus/node_exporter/releases/download/v1.3.1/node_exporter-1.3.1.linux-amd64.tar.gz</programlisting>
</listitem>
<listitem>
<simpara>Extract the executable and place it in the <literal>/usr/bin</literal> directory.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo tar xvf node_exporter-1.3.1.linux-amd64.tar.gz \
    --directory /usr/bin --strip 1 "*/node_exporter"</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>node_exporter.service</literal> file in this directory path: <literal>/etc/systemd/system</literal>. This <literal>systemd</literal> service file runs the node-exporter service when the virtual machine reboots.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">[Unit]
Description=Prometheus Metrics Exporter
After=network.target
StartLimitIntervalSec=0

[Service]
Type=simple
Restart=always
RestartSec=1
User=root
ExecStart=/usr/bin/node_exporter

[Install]
WantedBy=multi-user.target</programlisting>
</listitem>
<listitem>
<simpara>Enable and start the <literal>systemd</literal> service.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo systemctl enable node_exporter.service
$ sudo systemctl start node_exporter.service</programlisting>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the node-exporter agent is reporting metrics from the virtual machine.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl http://localhost:9100/metrics</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">go_gc_duration_seconds{quantile="0"} 1.5244e-05
go_gc_duration_seconds{quantile="0.25"} 3.0449e-05
go_gc_duration_seconds{quantile="0.5"} 3.7913e-05</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-creating-custom-monitoring-label-for-vms_virt-exposing-custom-metrics-for-vms">
<title>Creating a custom monitoring label for virtual machines</title>
<simpara>To enable queries to multiple virtual machines from a single service, add a custom label in the virtual machine&#8217;s YAML file.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift Container Platform CLI <literal>oc</literal>.</simpara>
</listitem>
<listitem>
<simpara>Log in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
<listitem>
<simpara>Access to the web console for stop and restart a virtual machine.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>template</literal> spec of your virtual machine configuration file. In this example, the label <literal>monitor</literal> has the value <literal>metrics</literal>.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  template:
    metadata:
      labels:
        monitor: metrics</programlisting>
</listitem>
<listitem>
<simpara>Stop and restart the virtual machine to create a new pod with the label name given to the <literal>monitor</literal> label.</simpara>
</listitem>
</orderedlist>
<section xml:id="virt-querying-the-node-exporter-service-for-metrics-_virt-exposing-custom-metrics-for-vms">
<title>Querying the node-exporter service for metrics</title>
<simpara>Metrics are exposed for virtual machines through an HTTP service endpoint under the <literal>/metrics</literal> canonical name. When you query for metrics, Prometheus directly scrapes the metrics from the metrics endpoint exposed by the virtual machines and presents these metrics for viewing.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> privileges or the <literal>monitoring-edit</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for the user-defined project by configuring the node-exporter service.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the HTTP service endpoint by specifying the namespace for the service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get service -n &lt;namespace&gt; &lt;node-exporter-service&gt;</programlisting>
</listitem>
<listitem>
<simpara>To list all available metrics for the node-exporter service, query the <literal>metrics</literal> resource.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl http://&lt;172.30.226.162:9100&gt;/metrics | grep -vE "^#|^$"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">node_arp_entries{device="eth0"} 1
node_boot_time_seconds 1.643153218e+09
node_context_switches_total 4.4938158e+07
node_cooling_device_cur_state{name="0",type="Processor"} 0
node_cooling_device_max_state{name="0",type="Processor"} 0
node_cpu_guest_seconds_total{cpu="0",mode="nice"} 0
node_cpu_guest_seconds_total{cpu="0",mode="user"} 0
node_cpu_seconds_total{cpu="0",mode="idle"} 1.10586485e+06
node_cpu_seconds_total{cpu="0",mode="iowait"} 37.61
node_cpu_seconds_total{cpu="0",mode="irq"} 233.91
node_cpu_seconds_total{cpu="0",mode="nice"} 551.47
node_cpu_seconds_total{cpu="0",mode="softirq"} 87.3
node_cpu_seconds_total{cpu="0",mode="steal"} 86.12
node_cpu_seconds_total{cpu="0",mode="system"} 464.15
node_cpu_seconds_total{cpu="0",mode="user"} 1075.2
node_disk_discard_time_seconds_total{device="vda"} 0
node_disk_discard_time_seconds_total{device="vdb"} 0
node_disk_discarded_sectors_total{device="vda"} 0
node_disk_discarded_sectors_total{device="vdb"} 0
node_disk_discards_completed_total{device="vda"} 0
node_disk_discards_completed_total{device="vdb"} 0
node_disk_discards_merged_total{device="vda"} 0
node_disk_discards_merged_total{device="vdb"} 0
node_disk_info{device="vda",major="252",minor="0"} 1
node_disk_info{device="vdb",major="252",minor="16"} 1
node_disk_io_now{device="vda"} 0
node_disk_io_now{device="vdb"} 0
node_disk_io_time_seconds_total{device="vda"} 174
node_disk_io_time_seconds_total{device="vdb"} 0.054
node_disk_io_time_weighted_seconds_total{device="vda"} 259.79200000000003
node_disk_io_time_weighted_seconds_total{device="vdb"} 0.039
node_disk_read_bytes_total{device="vda"} 3.71867136e+08
node_disk_read_bytes_total{device="vdb"} 366592
node_disk_read_time_seconds_total{device="vda"} 19.128
node_disk_read_time_seconds_total{device="vdb"} 0.039
node_disk_reads_completed_total{device="vda"} 5619
node_disk_reads_completed_total{device="vdb"} 96
node_disk_reads_merged_total{device="vda"} 5
node_disk_reads_merged_total{device="vdb"} 0
node_disk_write_time_seconds_total{device="vda"} 240.66400000000002
node_disk_write_time_seconds_total{device="vdb"} 0
node_disk_writes_completed_total{device="vda"} 71584
node_disk_writes_completed_total{device="vdb"} 0
node_disk_writes_merged_total{device="vda"} 19761
node_disk_writes_merged_total{device="vdb"} 0
node_disk_written_bytes_total{device="vda"} 2.007924224e+09
node_disk_written_bytes_total{device="vdb"} 0</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-creating-servicemonitor-resource-for-node-exporter_virt-exposing-custom-metrics-for-vms">
<title>Creating a ServiceMonitor resource for the node exporter service</title>
<simpara>You can use a Prometheus client library and scrape metrics from the <literal>/metrics</literal> endpoint to access and view the metrics exposed by the node-exporter service. Use a <literal>ServiceMonitor</literal> custom resource definition (CRD) to monitor the node exporter service.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> privileges or the <literal>monitoring-edit</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for the user-defined project by configuring the node-exporter service.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the <literal>ServiceMonitor</literal> resource configuration. In this example, the service monitor matches any service with the label <literal>metrics</literal> and queries the <literal>exmet</literal> port every 30 seconds.</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    k8s-app: node-exporter-metrics-monitor
  name: node-exporter-metrics-monitor <co xml:id="CO144-1"/>
  namespace: dynamation <co xml:id="CO144-2"/>
spec:
  endpoints:
  - interval: 30s <co xml:id="CO144-3"/>
    port: exmet <co xml:id="CO144-4"/>
    scheme: http
  selector:
    matchLabels:
      servicetype: metrics</programlisting>
<calloutlist>
<callout arearefs="CO144-1">
<para>The name of the <literal>ServiceMonitor</literal>.</para>
</callout>
<callout arearefs="CO144-2">
<para>The namespace where the <literal>ServiceMonitor</literal> is created.</para>
</callout>
<callout arearefs="CO144-3">
<para>The interval at which the port will be queried.</para>
</callout>
<callout arearefs="CO144-4">
<para>The name of the port that is queried every 30 seconds</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the <literal>ServiceMonitor</literal> configuration for the node-exporter service.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f node-exporter-metrics-monitor.yaml</programlisting>
</listitem>
</orderedlist>
<section xml:id="virt-accessing-node-exporter-outside-cluster_virt-exposing-custom-metrics-for-vms">
<title>Accessing the node exporter service outside the cluster</title>
<simpara>You can access the node-exporter service outside the cluster and view the exposed metrics.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with <literal>cluster-admin</literal> privileges or the <literal>monitoring-edit</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have enabled monitoring for the user-defined project by configuring the node-exporter service.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Expose the node-exporter service.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose service -n &lt;namespace&gt; &lt;node_exporter_service_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the FQDN (Fully Qualified Domain Name) for the route.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route -o=custom-columns=NAME:.metadata.name,DNS:.spec.host</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    DNS
node-exporter-service   node-exporter-service-dynamation.apps.cluster.example.org</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Use the <literal>curl</literal> command to display metrics for the node-exporter service.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl -s http://node-exporter-service-dynamation.apps.cluster.example.org/metrics</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">go_gc_duration_seconds{quantile="0"} 1.5382e-05
go_gc_duration_seconds{quantile="0.25"} 3.1163e-05
go_gc_duration_seconds{quantile="0.5"} 3.8546e-05
go_gc_duration_seconds{quantile="0.75"} 4.9139e-05
go_gc_duration_seconds{quantile="1"} 0.000189423</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_virt-exposing-custom-metrics-for-vms" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#configuring-the-monitoring-stack">Configuring the monitoring stack</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#enabling-monitoring-for-user-defined-projects">Enabling monitoring for user-defined projects</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#managing-metrics">Managing metrics</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#reviewing-monitoring-dashboards">Reviewing monitoring dashboards</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#application-health">Monitoring application health by using health checks</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-pods-configmaps">Creating and using config maps</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-controlling-vm-states">Controlling virtual machine states</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-monitoring-vm-health">
<title>Virtual machine health checks</title>

<simpara>You can configure virtual machine (VM) health checks by defining readiness and liveness probes in the <literal>VirtualMachine</literal> resource.</simpara>
<section xml:id="virt-about-readiness-liveness-probes_virt-monitoring-vm-health">
<title>About readiness and liveness probes</title>
<simpara>Use readiness and liveness probes to detect and handle unhealthy virtual machines (VMs). You can include one or more probes in the specification of the VM to ensure that traffic does not reach a VM that is not ready for it and that a new VM is created when a VM becomes unresponsive.</simpara>
<simpara>A <emphasis>readiness probe</emphasis> determines whether a VM is ready to accept service requests. If the probe fails, the VM is removed from the list of available endpoints until the VM is ready.</simpara>
<simpara>A <emphasis>liveness probe</emphasis> determines whether a VM is responsive. If the probe fails, the VM is deleted and a new VM is created to restore responsiveness.</simpara>
<simpara>You can configure readiness and liveness probes by setting the <literal>spec.readinessProbe</literal> and the <literal>spec.livenessProbe</literal> fields of the <literal>VirtualMachine</literal> object. These fields support the following tests:</simpara>
<variablelist>
<varlistentry>
<term>HTTP GET</term>
<listitem>
<simpara>The probe determines the health of the VM by using a web hook. The test is successful if the HTTP response code is between 200 and 399. You can use an HTTP GET test with applications that return HTTP status codes when they are completely initialized.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>TCP socket</term>
<listitem>
<simpara>The probe attempts to open a socket to the VM. The VM is only considered healthy if the probe can establish a connection. You can use a TCP socket test with applications that do not start listening until initialization is complete.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Guest agent ping</term>
<listitem>
<simpara>The probe uses the <literal>guest-ping</literal> command to determine if the QEMU guest agent is running on the virtual machine.</simpara>
</listitem>
</varlistentry>
</variablelist>
<section xml:id="virt-define-http-readiness-probe_virt-monitoring-vm-health">
<title>Defining an HTTP readiness probe</title>
<simpara>Define an HTTP readiness probe by setting the <literal>spec.readinessProbe.httpGet</literal> field of the virtual machine (VM) configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Include details of the readiness probe in the VM configuration file.</simpara>
<formalpara>
<title>Sample readiness probe with an HTTP GET test</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  readinessProbe:
    httpGet: <co xml:id="CO145-1"/>
      port: 1500 <co xml:id="CO145-2"/>
      path: /healthz <co xml:id="CO145-3"/>
      httpHeaders:
      - name: Custom-Header
        value: Awesome
    initialDelaySeconds: 120 <co xml:id="CO145-4"/>
    periodSeconds: 20 <co xml:id="CO145-5"/>
    timeoutSeconds: 10 <co xml:id="CO145-6"/>
    failureThreshold: 3 <co xml:id="CO145-7"/>
    successThreshold: 3 <co xml:id="CO145-8"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO145-1">
<para>The HTTP GET request to perform to connect to the VM.</para>
</callout>
<callout arearefs="CO145-2">
<para>The port of the VM that the probe queries. In the above example, the probe queries port 1500.</para>
</callout>
<callout arearefs="CO145-3">
<para>The path to access on the HTTP server. In the above example, if the handler for the serverâ€™s /healthz path returns a success code, the VM is considered to be healthy. If the handler returns a failure code, the VM is removed from the list of available endpoints.</para>
</callout>
<callout arearefs="CO145-4">
<para>The time, in seconds, after the VM starts before the readiness probe is initiated.</para>
</callout>
<callout arearefs="CO145-5">
<para>The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than <literal>timeoutSeconds</literal>.</para>
</callout>
<callout arearefs="CO145-6">
<para>The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than <literal>periodSeconds</literal>.</para>
</callout>
<callout arearefs="CO145-7">
<para>The number of times that the probe is allowed to fail. The default is 3. After the specified number of attempts, the pod is marked <literal>Unready</literal>.</para>
</callout>
<callout arearefs="CO145-8">
<para>The number of times that the probe must report success, after a failure, to be considered successful. The default is 1.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-define-tcp-readiness-probe_virt-monitoring-vm-health">
<title>Defining a TCP readiness probe</title>
<simpara>Define a TCP readiness probe by setting the <literal>spec.readinessProbe.tcpSocket</literal> field of the virtual machine (VM) configuration.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Include details of the TCP readiness probe in the VM configuration file.</simpara>
<formalpara>
<title>Sample readiness probe with a TCP socket test</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  readinessProbe:
    initialDelaySeconds: 120 <co xml:id="CO146-1"/>
    periodSeconds: 20 <co xml:id="CO146-2"/>
    tcpSocket: <co xml:id="CO146-3"/>
      port: 1500 <co xml:id="CO146-4"/>
    timeoutSeconds: 10 <co xml:id="CO146-5"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO146-1">
<para>The time, in seconds, after the VM starts before the readiness probe is initiated.</para>
</callout>
<callout arearefs="CO146-2">
<para>The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than <literal>timeoutSeconds</literal>.</para>
</callout>
<callout arearefs="CO146-3">
<para>The TCP action to perform.</para>
</callout>
<callout arearefs="CO146-4">
<para>The port of the VM that the probe queries.</para>
</callout>
<callout arearefs="CO146-5">
<para>The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than <literal>periodSeconds</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-define-http-liveness-probe_virt-monitoring-vm-health">
<title>Defining an HTTP liveness probe</title>
<simpara>Define an HTTP liveness probe by setting the <literal>spec.livenessProbe.httpGet</literal> field of the virtual machine (VM) configuration. You can define both HTTP and TCP tests for liveness probes in the same way as readiness probes. This procedure configures a sample liveness probe with an HTTP GET test.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Include details of the HTTP liveness probe in the VM configuration file.</simpara>
<formalpara>
<title>Sample liveness probe with an HTTP GET test</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  livenessProbe:
    initialDelaySeconds: 120 <co xml:id="CO147-1"/>
    periodSeconds: 20 <co xml:id="CO147-2"/>
    httpGet: <co xml:id="CO147-3"/>
      port: 1500 <co xml:id="CO147-4"/>
      path: /healthz <co xml:id="CO147-5"/>
      httpHeaders:
      - name: Custom-Header
        value: Awesome
    timeoutSeconds: 10 <co xml:id="CO147-6"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO147-1">
<para>The time, in seconds, after the VM starts before the liveness probe is initiated.</para>
</callout>
<callout arearefs="CO147-2">
<para>The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than <literal>timeoutSeconds</literal>.</para>
</callout>
<callout arearefs="CO147-3">
<para>The HTTP GET request to perform to connect to the VM.</para>
</callout>
<callout arearefs="CO147-4">
<para>The port of the VM that the probe queries. In the above example, the probe queries port 1500. The VM installs and runs a minimal HTTP server on port 1500 via cloud-init.</para>
</callout>
<callout arearefs="CO147-5">
<para>The path to access on the HTTP server. In the above example, if the handler for the server&#8217;s <literal>/healthz</literal> path returns a success code, the VM is considered to be healthy. If the handler returns a failure code, the VM is deleted and a new VM is created.</para>
</callout>
<callout arearefs="CO147-6">
<para>The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than <literal>periodSeconds</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="watchdog_virt-monitoring-vm-health">
<title>Defining a watchdog</title>
<simpara>You can define a watchdog to monitor the health of the guest operating system by performing the following steps:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure a watchdog device for the virtual machine (VM).</simpara>
</listitem>
<listitem>
<simpara>Install the watchdog agent on the guest.</simpara>
</listitem>
</orderedlist>
<simpara>The watchdog device monitors the agent and performs one of the following actions if the guest operating system is unresponsive:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>poweroff</literal>: The VM powers down immediately. If <literal>spec.running</literal> is set to <literal>true</literal> or <literal>spec.runStrategy</literal> is not set to <literal>manual</literal>, then the VM reboots.</simpara>
</listitem>
<listitem>
<simpara><literal>reset</literal>: The VM reboots in place and the guest operating system cannot react.</simpara>
<note>
<simpara>The reboot time might cause liveness probes to time out. If cluster-level protections detect a failed liveness probe, the VM might be forcibly rescheduled, increasing the reboot time.</simpara>
</note>
</listitem>
<listitem>
<simpara><literal>shutdown</literal>: The VM gracefully powers down by stopping all services.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Watchdog is not available for Windows VMs.</simpara>
</note>
<section xml:id="virt-defining-watchdog-device-vm">
<title>Configuring a watchdog device for the virtual machine</title>
<simpara>You configure a watchdog device for the virtual machine (VM).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The VM must have kernel support for an <literal>i6300esb</literal> watchdog device. Red Hat Enterprise Linux (RHEL) images support <literal>i6300esb</literal>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>YAML</literal> file with the following contents:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: vm2-rhel84-watchdog
  name: &lt;vm-name&gt;
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm2-rhel84-watchdog
    spec:
      domain:
        devices:
          watchdog:
            name: &lt;watchdog&gt;
            i6300esb:
              action: "poweroff" <co xml:id="CO148-1"/>
# ...</programlisting>
<calloutlist>
<callout arearefs="CO148-1">
<para>Specify <literal>poweroff</literal>, <literal>reset</literal>, or <literal>shutdown</literal>.</para>
</callout>
</calloutlist>
<simpara>The example above configures the <literal>i6300esb</literal> watchdog device on a RHEL8 VM with the poweroff action and exposes the device as <literal>/dev/watchdog</literal>.</simpara>
<simpara>This device can now be used by the watchdog binary.</simpara>
</listitem>
<listitem>
<simpara>Apply the YAML file to your cluster by running the following command:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ oc apply -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Verification</title>
<para>
<important>
<simpara>This procedure is provided for testing watchdog functionality only and must not be run on production machines.</simpara>
</important>
</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Run the following command to verify that the VM is connected to the watchdog device:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ lspci | grep watchdog -i</programlisting>
</listitem>
<listitem>
<simpara>Run one of the following commands to confirm the watchdog is active:</simpara>
<itemizedlist>
<listitem>
<simpara>Trigger a kernel panic:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># echo c &gt; /proc/sysrq-trigger</programlisting>
</listitem>
<listitem>
<simpara>Stop the watchdog service:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># pkill -9 watchdog</programlisting>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-installing-watchdog-agent_virt-monitoring-vm-health">
<title>Installing the watchdog agent on the guest</title>
<simpara>You install the watchdog agent on the guest and start the <literal>watchdog</literal> service.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to the virtual machine as root user.</simpara>
</listitem>
<listitem>
<simpara>Install the <literal>watchdog</literal> package and its dependencies:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># yum install watchdog</programlisting>
</listitem>
<listitem>
<simpara>Uncomment the following line in the <literal>/etc/watchdog.conf</literal> file and save the changes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">#watchdog-device = /dev/watchdog</programlisting>
</listitem>
<listitem>
<simpara>Enable the <literal>watchdog</literal> service to start on boot:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># systemctl enable --now watchdog.service</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-define-guest-agent-ping-probe_virt-monitoring-vm-health">
<title>Defining a guest agent ping probe</title>
<simpara>Define a guest agent ping probe by setting the <literal>spec.readinessProbe.guestAgentPing</literal> field of the virtual machine (VM) configuration.</simpara>
<important>
<simpara>The guest agent ping probe is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The QEMU guest agent must be installed and enabled on the virtual machine.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Include details of the guest agent ping probe in the VM configuration file. For example:</simpara>
<formalpara>
<title>Sample guest agent ping probe</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
spec:
  readinessProbe:
    guestAgentPing: {} <co xml:id="CO149-1"/>
    initialDelaySeconds: 120 <co xml:id="CO149-2"/>
    periodSeconds: 20 <co xml:id="CO149-3"/>
    timeoutSeconds: 10 <co xml:id="CO149-4"/>
    failureThreshold: 3 <co xml:id="CO149-5"/>
    successThreshold: 3 <co xml:id="CO149-6"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO149-1">
<para>The guest agent ping probe to connect to the VM.</para>
</callout>
<callout arearefs="CO149-2">
<para>Optional: The time, in seconds, after the VM starts before the guest agent probe is initiated.</para>
</callout>
<callout arearefs="CO149-3">
<para>Optional: The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than <literal>timeoutSeconds</literal>.</para>
</callout>
<callout arearefs="CO149-4">
<para>Optional: The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than <literal>periodSeconds</literal>.</para>
</callout>
<callout arearefs="CO149-5">
<para>Optional: The number of times that the probe is allowed to fail. The default is 3. After the specified number of attempts, the pod is marked <literal>Unready</literal>.</para>
</callout>
<callout arearefs="CO149-6">
<para>Optional: The number of times that the probe must report success, after a failure, to be considered successful. The default is 1.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the VM by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;file_name&gt;.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_monitoring-vm-health" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/building_applications/#application-health">Monitoring application health by using health checks</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-runbooks">
<title>OpenShift Virtualization runbooks</title>

<simpara>You can use the procedures in these runbooks to diagnose and resolve issues that trigger OpenShift Virtualization <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#managing-alerts">alerts</link>.</simpara>
<simpara>OpenShift Virtualization alerts are displayed on the <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">Overview</emphasis> &#8594; <link linkend="overview-overview_virt-web-console-overview"><emphasis role="strong">Overview</emphasis> tab</link> in the web console.</simpara>
<section xml:id="virt-runbook-CDIDataImportCronOutdated">
<title>CDIDataImportCronOutdated</title>
<bridgehead xml:id="meaning-cdidataimportcronoutdated" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when <literal>DataImportCron</literal> cannot poll or import the latest disk
image versions.</simpara>
<simpara><literal>DataImportCron</literal> polls disk images, checking for the latest versions, and
imports the images as persistent volume claims (PVCs). This process ensures
that PVCs are updated to the latest version so that they can be used as
reliable clone sources or golden images for virtual machines (VMs).</simpara>
<simpara>For golden images, <emphasis>latest</emphasis> refers to the latest operating system of the
distribution. For other disk images, <emphasis>latest</emphasis> refers to the latest hash of the
image that is available.</simpara>
<bridgehead xml:id="impact-cdidataimportcronoutdated" renderas="sect4">Impact</bridgehead>
<simpara>VMs might be created from outdated disk images.</simpara>
<simpara>VMs might fail to start because no source PVC is available for cloning.</simpara>
<bridgehead xml:id="diagnosis-cdidataimportcronoutdated" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check the cluster for a default storage class:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sc</programlisting>
<simpara>The output displays the storage classes with <literal>(default)</literal> beside the name
of the default storage class. You must set a default storage class, either on
the cluster or in the <literal>DataImportCron</literal> specification, in order for the
<literal>DataImportCron</literal> to poll and import golden images. If no storage class is
defined, the DataVolume controller fails to create PVCs and the following
event is displayed: <literal>DataVolume.storage spec is missing accessMode and no
storageClass to choose profile</literal>.</simpara>
</listitem>
<listitem>
<simpara>Obtain the <literal>DataImportCron</literal> namespace and name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dataimportcron -A -o json | jq -r '.items[] | \
  select(.status.conditions[] | select(.type == "UpToDate" and \
  .status == "False")) | .metadata.namespace + "/" + .metadata.name'</programlisting>
</listitem>
<listitem>
<simpara>If a default storage class is not defined on the cluster, check the
<literal>DataImportCron</literal> specification for a default storage class:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dataimportcron &lt;dataimportcron&gt; -o yaml | \
  grep -B 5 storageClassName</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">      url: docker://.../cdi-func-test-tinycore
    storage:
      resources:
        requests:
          storage: 5Gi
    storageClassName: rook-ceph-block</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Obtain the name of the <literal>DataVolume</literal> associated with the <literal>DataImportCron</literal>
object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;namespace&gt; get dataimportcron &lt;dataimportcron&gt; -o json | \
  jq .status.lastImportedPVC.name</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>DataVolume</literal> log for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;namespace&gt; get dv &lt;datavolume&gt; -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>CDI_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export CDI_NAMESPACE="$(oc get deployment -A | \
  grep cdi-operator | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>cdi-deployment</literal> log for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n $CDI_NAMESPACE deployment/cdi-deployment</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cdidataimportcronoutdated" renderas="sect4">Mitigation</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set a default storage class, either on the cluster or in the <literal>DataImportCron</literal>
specification, to poll and import golden images. The updated Containerized Data
Importer (CDI) will resolve the issue within a few seconds.</simpara>
</listitem>
<listitem>
<simpara>If the issue does not resolve itself, delete the data volumes associated
with the affected <literal>DataImportCron</literal> objects. The CDI will recreate the data
volumes with the default storage class.</simpara>
</listitem>
<listitem>
<simpara>If your cluster is installed in a restricted network environment, disable
the <literal>enableCommonBootImageImport</literal> feature gate in order to opt out of automatic
updates:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch hco kubevirt-hyperconverged -n $CDI_NAMESPACE --type json \
  -p '[{"op": "replace", "path": \
  "/spec/featureGates/enableCommonBootImageImport", "value": false}]'</programlisting>
</listitem>
</orderedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case, attaching
the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDIDataVolumeUnusualRestartCount">
<title>CDIDataVolumeUnusualRestartCount</title>
<bridgehead xml:id="meaning-cdidatavolumeunusualrestartcount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a <literal>DataVolume</literal> object restarts more than three times.</simpara>
<bridgehead xml:id="impact-cdidatavolumeunusualrestartcount" renderas="sect4">Impact</bridgehead>
<simpara>Data volumes are responsible for importing and creating a virtual machine disk
on a persistent volume claim. If a data volume restarts more than three times,
these operations are unlikely to succeed. You must diagnose and resolve the issue.</simpara>
<bridgehead xml:id="diagnosis-cdidatavolumeunusualrestartcount" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Find Containerized Data Importer (CDI) pods with more than three restarts:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces -l app=containerized-data-importer -o=jsonpath='{range .items[?(@.status.containerStatuses[0].restartCount&gt;3)]}{.metadata.name}{"/"}{.metadata.namespace}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;namespace&gt; describe pods &lt;pod&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check the pod logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;namespace&gt; logs &lt;pod&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cdidatavolumeunusualrestartcount" renderas="sect4">Mitigation</bridgehead>
<simpara>Delete the data volume, resolve the issue, and create a new data volume.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the Diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDIDefaultStorageClassDegraded">
<title>CDIDefaultStorageClassDegraded</title>
<bridgehead xml:id="meaning-cdidefaultstorageclassdegraded" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when there is no default storage class that supports smart cloning
(CSI or snapshot-based) or the ReadWriteMany access mode.</simpara>
<bridgehead xml:id="impact-cdidefaultstorageclassdegraded" renderas="sect4">Impact</bridgehead>
<simpara>If the default storage class does not support smart cloning, the default cloning
method is host-assisted cloning, which is much less efficient.</simpara>
<simpara>If the default storage class does not support ReadWriteMany, virtual machines (VMs)
cannot be live migrated.</simpara>
<note>
<simpara>A default OpenShift Virtualization storage class has precedence over a
default OpenShift Container Platform storage class when creating a
VM disk.</simpara>
</note>
<bridgehead xml:id="diagnosis-cdidefaultstorageclassdegraded" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Get the default OpenShift Virtualization storage class by running the following
command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubevirt\.io/is-default-virt-class=="true")].metadata.name}'</programlisting>
</listitem>
<listitem>
<simpara>If a default OpenShift Virtualization storage class exists, check that it
supports ReadWriteMany by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageprofile &lt;storage_class&gt; -o json | jq '.status.claimPropertySets'| grep ReadWriteMany</programlisting>
</listitem>
<listitem>
<simpara>If there is no default OpenShift Virtualization storage class, get the
default OpenShift Container Platform storage class by running the following
command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubevirt\.io/is-default-class=="true")].metadata.name}'</programlisting>
</listitem>
<listitem>
<simpara>If a default OpenShift Container Platform storage class exists, check that it
supports ReadWriteMany by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageprofile &lt;storage_class&gt; -o json | jq '.status.claimPropertySets'| grep ReadWriteMany</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cdidefaultstorageclassdegraded" renderas="sect4">Mitigation</bridgehead>
<simpara>Ensure that you have a default storage class, either OpenShift Container Platform
or OpenShift Virtualization, and that the default storage class supports
smart cloning and ReadWriteMany.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case, attaching
the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDIMultipleDefaultVirtStorageClasses">
<title>CDIMultipleDefaultVirtStorageClasses</title>
<bridgehead xml:id="meaning-cdimultipledefaultvirtstorageclasses" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when more than one storage class has the annotation
<literal>storageclass.kubevirt.io/is-default-virt-class: "true"</literal>.</simpara>
<bridgehead xml:id="impact-cdimultipledefaultvirtstorageclasses" renderas="sect4">Impact</bridgehead>
<simpara>The <literal>storageclass.kubevirt.io/is-default-virt-class: "true"</literal> annotation
defines a default OpenShift Virtualization storage class.</simpara>
<simpara>If more than one default OpenShift Virtualization storage class
is defined, a data volume with no storage class specified
receives the most recently created default storage class.</simpara>
<bridgehead xml:id="diagnosis-cdimultipledefaultvirtstorageclasses" renderas="sect4">Diagnosis</bridgehead>
<simpara>Obtain a list of default OpenShift Virtualization storage classes by running
the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubevirt\.io/is-default-virt-class=="true")].metadata.name}'</programlisting>
<bridgehead xml:id="mitigation-cdimultipledefaultvirtstorageclasses" renderas="sect4">Mitigation</bridgehead>
<simpara>Ensure that only one default OpenShift Virtualization storage class
is defined by removing the annotation from the other storage classes.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case, attaching
the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDINoDefaultStorageClass">
<title>CDINoDefaultStorageClass</title>
<bridgehead xml:id="meaning-cdinodefaultstorageclass" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when no default OpenShift Container Platform or
OpenShift Virtualization storage class is defined.</simpara>
<bridgehead xml:id="impact-cdinodefaultstorageclass" renderas="sect4">Impact</bridgehead>
<simpara>If no default OpenShift Container Platform or OpenShift Virtualization storage
class is defined, a data volume requesting a default storage class (the storage
class is not specified), remains in a "pending" state.</simpara>
<bridgehead xml:id="diagnosis-cdinodefaultstorageclass" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check for a default OpenShift Container Platform storage class by running
the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubevirt\.io/is-default-class=="true")].metadata.name}'</programlisting>
</listitem>
<listitem>
<simpara>Check for a default OpenShift Virtualization storage class by running
the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get sc -o jsonpath='{.items[?(@.metadata.annotations.storageclass\.kubevirt\.io/is-default-virt-class=="true")].metadata.name}'</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cdinodefaultstorageclass" renderas="sect4">Mitigation</bridgehead>
<simpara>Create a default storage class for either OpenShift Container Platform or
OpenShift Virtualization or for both.</simpara>
<simpara>A default OpenShift Virtualization storage class has precedence over a default
OpenShift Container Platform storage class for creating a virtual machine disk image.</simpara>
<itemizedlist>
<listitem>
<simpara>Create a default OpenShift Container Platform storage class by running
the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageclass &lt;storage-class-name&gt; -p '{"metadata": {"annotations":{"storageclass.kubernetes.io/is-default-class":"true"}}}'</programlisting>
</listitem>
<listitem>
<simpara>Create a default OpenShift Virtualization storage class by running
the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageclass &lt;storage-class-name&gt; -p '{"metadata": {"annotations":{"storageclass.kubevirt.io/is-default-virt-class":"true"}}}'</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDINotReady">
<title>CDINotReady</title>
<bridgehead xml:id="meaning-cdinotready" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the Containerized Data Importer (CDI) is in
a degraded state:</simpara>
<itemizedlist>
<listitem>
<simpara>Not progressing</simpara>
</listitem>
<listitem>
<simpara>Not available to use</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-cdinotready" renderas="sect4">Impact</bridgehead>
<simpara>CDI is not usable, so users cannot build virtual machine disks on
persistent volume claims (PVCs) using CDI&#8217;s data volumes.
CDI components are not ready and they stopped progressing towards
a ready state.</simpara>
<bridgehead xml:id="diagnosis-cdinotready" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>CDI_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export CDI_NAMESPACE="$(oc get deployment -A | \
  grep cdi-operator | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check the CDI deployment for components that are not ready:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $CDI_NAMESPACE get deploy -l cdi.kubevirt.io</programlisting>
</listitem>
<listitem>
<simpara>Check the details of the failing pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $CDI_NAMESPACE describe pods &lt;pod&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check the logs of the failing pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $CDI_NAMESPACE logs &lt;pod&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cdinotready" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDIOperatorDown">
<title>CDIOperatorDown</title>
<bridgehead xml:id="meaning-cdioperatordown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the Containerized Data Importer (CDI) Operator is down.
The CDI Operator deploys and manages the CDI infrastructure components, such
as data volume and persistent volume claim (PVC) controllers. These controllers
help users build virtual machine disks on PVCs.</simpara>
<bridgehead xml:id="impact-cdioperatordown" renderas="sect4">Impact</bridgehead>
<simpara>The CDI components might fail to deploy or to stay in a required state. The
CDI installation might not function correctly.</simpara>
<bridgehead xml:id="diagnosis-cdioperatordown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>CDI_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export CDI_NAMESPACE="$(oc get deployment -A | grep cdi-operator | \
  awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check whether the <literal>cdi-operator</literal> pod is currently running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $CDI_NAMESPACE get pods -l name=cdi-operator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>cdi-operator</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $CDI_NAMESPACE describe pods -l name=cdi-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the log of the <literal>cdi-operator</literal> pod for errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $CDI_NAMESPACE logs -l name=cdi-operator</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cdioperatordown" renderas="sect4">Mitigation</bridgehead>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CDIStorageProfilesIncomplete">
<title>CDIStorageProfilesIncomplete</title>
<bridgehead xml:id="meaning-cdistorageprofilesincomplete" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a Containerized Data Importer (CDI) storage profile is
incomplete.</simpara>
<simpara>If a storage profile is incomplete, the CDI cannot infer persistent volume claim
(PVC) fields, such as <literal>volumeMode</literal> and  <literal>accessModes</literal>, which are required to
create a virtual machine (VM) disk.</simpara>
<bridgehead xml:id="impact-cdistorageprofilesincomplete" renderas="sect4">Impact</bridgehead>
<simpara>The CDI cannot create a VM disk on the PVC.</simpara>
<bridgehead xml:id="diagnosis-cdistorageprofilesincomplete" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Identify the incomplete storage profile:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageprofile &lt;storage_class&gt;</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-cdistorageprofilesincomplete" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>Add the missing storage profile information as in the following
example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch storageprofile local --type=merge -p '{"spec": \
  {"claimPropertySets": [{"accessModes": ["ReadWriteOnce"], \
  "volumeMode": "Filesystem"}]}}'</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-CnaoDown">
<title>CnaoDown</title>
<bridgehead xml:id="meaning-cnaodown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the Cluster Network Addons Operator (CNAO) is down.
The CNAO deploys additional networking components on top of the cluster.</simpara>
<bridgehead xml:id="impact-cnaodown" renderas="sect4">Impact</bridgehead>
<simpara>If the CNAO is not running, the cluster cannot reconcile changes to virtual
machine components. As a result, the changes might fail to take effect.</simpara>
<bridgehead xml:id="diagnosis-cnaodown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get deployment -A | \
  grep cluster-network-addons-operator | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>cluster-network-addons-operator</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l name=cluster-network-addons-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>cluster-network-addons-operator</literal> logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs -l name=cluster-network-addons-operator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>cluster-network-addons-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pods -l name=cluster-network-addons-operator</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-cnaodown" renderas="sect4">Mitigation</bridgehead>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-HCOInstallationIncomplete">
<title>HCOInstallationIncomplete</title>
<bridgehead xml:id="meaning-hcoinstallationincomplete" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the HyperConverged Cluster Operator (HCO) runs for
more than an hour without a <literal>HyperConverged</literal> custom resource (CR).</simpara>
<simpara>This alert has the following causes:</simpara>
<itemizedlist>
<listitem>
<simpara>During the installation process, you installed the HCO but you did not
create the <literal>HyperConverged</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>During the uninstall process, you removed the <literal>HyperConverged</literal> CR before
uninstalling the HCO and the HCO is still running.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-hcoinstallationincomplete" renderas="sect4">Mitigation</bridgehead>
<simpara>The mitigation depends on whether you are installing or uninstalling
the HCO:</simpara>
<itemizedlist>
<listitem>
<simpara>Complete the installation by creating a <literal>HyperConverged</literal> CR with its
default values:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: hco-operatorgroup
  namespace: kubevirt-hyperconverged
spec: {}
EOF</programlisting>
</listitem>
<listitem>
<simpara>Uninstall the HCO. If the uninstall process continues to run, you must
resolve that issue in order to cancel the alert.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-runbook-HPPNotReady">
<title>HPPNotReady</title>
<bridgehead xml:id="meaning-hppnotready" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a hostpath provisioner (HPP) installation is in a
degraded state.</simpara>
<simpara>The HPP dynamically provisions hostpath volumes to provide storage for
persistent volume claims (PVCs).</simpara>
<bridgehead xml:id="impact-hppnotready" renderas="sect4">Impact</bridgehead>
<simpara>HPP is not usable. Its components are not ready and they are not progressing
towards a ready state.</simpara>
<bridgehead xml:id="diagnosis-hppnotready" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>HPP_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export HPP_NAMESPACE="$(oc get deployment -A | \
  grep hostpath-provisioner-operator | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check for HPP components that are currently not ready:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE get all -l k8s-app=hostpath-provisioner</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the failing pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE describe pods &lt;pod&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check the logs of the failing pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE logs &lt;pod&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-hppnotready" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-HPPOperatorDown">
<title>HPPOperatorDown</title>
<bridgehead xml:id="meaning-hppoperatordown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the hostpath provisioner (HPP) Operator is down.</simpara>
<simpara>The HPP Operator deploys and manages the HPP infrastructure components, such
as the daemon set that provisions hostpath volumes.</simpara>
<bridgehead xml:id="impact-hppoperatordown" renderas="sect4">Impact</bridgehead>
<simpara>The HPP components might fail to deploy or to remain in the required state.
As a result, the HPP installation might not work correctly in the cluster.</simpara>
<bridgehead xml:id="diagnosis-hppoperatordown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure the <literal>HPP_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ HPP_NAMESPACE="$(oc get deployment -A | grep \
  hostpath-provisioner-operator | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check whether the <literal>hostpath-provisioner-operator</literal> pod is currently running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE get pods -l name=hostpath-provisioner-operator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>hostpath-provisioner-operator</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE describe pods -l name=hostpath-provisioner-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the log of the <literal>hostpath-provisioner-operator</literal> pod for errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE logs -l name=hostpath-provisioner-operator</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-hppoperatordown" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-HPPSharingPoolPathWithOS">
<title>HPPSharingPoolPathWithOS</title>
<bridgehead xml:id="meaning-hppsharingpoolpathwithos" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the hostpath provisioner (HPP) shares a file
system with other critical components, such as <literal>kubelet</literal> or the operating
system (OS).</simpara>
<simpara>HPP dynamically provisions hostpath volumes to provide storage for
persistent volume claims (PVCs).</simpara>
<bridgehead xml:id="impact-hppsharingpoolpathwithos" renderas="sect4">Impact</bridgehead>
<simpara>A shared hostpath pool puts pressure on the node&#8217;s disks. The node
might have degraded performance and stability.</simpara>
<bridgehead xml:id="diagnosis-hppsharingpoolpathwithos" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Configure the <literal>HPP_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export HPP_NAMESPACE="$(oc get deployment -A | \
  grep hostpath-provisioner-operator | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the status of the <literal>hostpath-provisioner-csi</literal> daemon set
pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE get pods | grep hostpath-provisioner-csi</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>hostpath-provisioner-csi</literal> logs to identify the shared
pool and path:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $HPP_NAMESPACE logs &lt;csi_daemonset&gt; -c hostpath-provisioner</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">I0208 15:21:03.769731       1 utils.go:221] pool (&lt;legacy, csi-data-dir&gt;/csi),
shares path with OS which can lead to node disk pressure</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-hppsharingpoolpathwithos" renderas="sect4">Mitigation</bridgehead>
<simpara>Using the data obtained in the Diagnosis section, try to prevent the
pool path from being shared with the OS. The specific steps vary based
on the node and other circumstances.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-KubemacpoolDown">
<title>KubemacpoolDown</title>
<bridgehead xml:id="meaning-kubemacpooldown" renderas="sect4">Meaning</bridgehead>
<simpara><literal>KubeMacPool</literal> is down. <literal>KubeMacPool</literal> is responsible for allocating MAC
addresses and preventing MAC address conflicts.</simpara>
<bridgehead xml:id="impact-kubemacpooldown" renderas="sect4">Impact</bridgehead>
<simpara>If <literal>KubeMacPool</literal> is down, <literal>VirtualMachine</literal> objects cannot be created.</simpara>
<bridgehead xml:id="diagnosis-kubemacpooldown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>KMP_NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KMP_NAMESPACE="$(oc get pod -A --no-headers -l \
  control-plane=mac-controller-manager | awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>KMP_NAME</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KMP_NAME="$(oc get pod -A --no-headers -l \
  control-plane=mac-controller-manager | awk '{print $2}')"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the <literal>KubeMacPool-manager</literal> pod details:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe pod -n $KMP_NAMESPACE $KMP_NAME</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>KubeMacPool-manager</literal> logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n $KMP_NAMESPACE $KMP_NAME</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-kubemacpooldown" renderas="sect4">Mitigation</bridgehead>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-KubeMacPoolDuplicateMacsFound">
<title>KubeMacPoolDuplicateMacsFound</title>
<bridgehead xml:id="meaning-kubemacpoolduplicatemacsfound" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when <literal>KubeMacPool</literal> detects duplicate MAC addresses.</simpara>
<simpara><literal>KubeMacPool</literal> is responsible for allocating MAC addresses and preventing MAC
address conflicts. When <literal>KubeMacPool</literal> starts, it scans the cluster for the MAC
addresses of virtual machines (VMs) in managed namespaces.</simpara>
<bridgehead xml:id="impact-kubemacpoolduplicatemacsfound" renderas="sect4">Impact</bridgehead>
<simpara>Duplicate MAC addresses on the same LAN might cause network issues.</simpara>
<bridgehead xml:id="diagnosis-kubemacpoolduplicatemacsfound" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Obtain the namespace and the name of the <literal>kubemacpool-mac-controller</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod -A -l control-plane=mac-controller-manager --no-headers \
  -o custom-columns=":metadata.namespace,:metadata.name"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the duplicate MAC addresses from the <literal>kubemacpool-mac-controller</literal>
logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n &lt;namespace&gt; &lt;kubemacpool_mac_controller&gt; | \
  grep "already allocated"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">mac address 02:00:ff:ff:ff:ff already allocated to
vm/kubemacpool-test/testvm, br1,
conflict with: vm/kubemacpool-test/testvm2, br1</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-kubemacpoolduplicatemacsfound" renderas="sect4">Mitigation</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Update the VMs to remove the duplicate MAC addresses.</simpara>
</listitem>
<listitem>
<simpara>Restart the <literal>kubemacpool-mac-controller</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pod -n &lt;namespace&gt; &lt;kubemacpool_mac_controller&gt;</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-runbook-KubeVirtComponentExceedsRequestedCPU">
<title>KubeVirtComponentExceedsRequestedCPU</title>
<bridgehead xml:id="meaning-kubevirtcomponentexceedsrequestedcpu" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a component&#8217;s CPU usage exceeds the requested limit.</simpara>
<bridgehead xml:id="impact-kubevirtcomponentexceedsrequestedcpu" renderas="sect4">Impact</bridgehead>
<simpara>Usage of CPU resources is not optimal and the node might be overloaded.</simpara>
<bridgehead xml:id="diagnosis-kubevirtcomponentexceedsrequestedcpu" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the component&#8217;s CPU request limit:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deployment &lt;component&gt; -o yaml | grep requests: -A 2</programlisting>
</listitem>
<listitem>
<simpara>Check the actual CPU usage by using a PromQL query:</simpara>
<programlisting language="text" linenumbering="unnumbered">node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
{namespace="$NAMESPACE",container="&lt;component&gt;"}</programlisting>
</listitem>
</orderedlist>
<simpara>See the
<link xlink:href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus documentation</link>
for more information.</simpara>
<bridgehead xml:id="mitigation-kubevirtcomponentexceedsrequestedcpu" renderas="sect4">Mitigation</bridgehead>
<simpara>Update the CPU request limit in the <literal>HCO</literal> custom resource.</simpara>
</section>
<section xml:id="virt-runbook-KubeVirtComponentExceedsRequestedMemory">
<title>KubeVirtComponentExceedsRequestedMemory</title>
<bridgehead xml:id="meaning-kubevirtcomponentexceedsrequestedmemory" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a component&#8217;s memory usage exceeds the requested limit.</simpara>
<bridgehead xml:id="impact-kubevirtcomponentexceedsrequestedmemory" renderas="sect4">Impact</bridgehead>
<simpara>Usage of memory resources is not optimal and the node might be overloaded.</simpara>
<bridgehead xml:id="diagnosis-kubevirtcomponentexceedsrequestedmemory" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the component&#8217;s memory request limit:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deployment &lt;component&gt; -o yaml | \
  grep requests: -A 2</programlisting>
</listitem>
<listitem>
<simpara>Check the actual memory usage by using a PromQL query:</simpara>
<programlisting language="text" linenumbering="unnumbered">container_memory_usage_bytes{namespace="$NAMESPACE",container="&lt;component&gt;"}</programlisting>
</listitem>
</orderedlist>
<simpara>See the
<link xlink:href="https://prometheus.io/docs/prometheus/latest/querying/basics/">Prometheus documentation</link>
for more information.</simpara>
<bridgehead xml:id="mitigation-kubevirtcomponentexceedsrequestedmemory" renderas="sect4">Mitigation</bridgehead>
<simpara>Update the memory request limit in the <literal>HCO</literal> custom resource.</simpara>
</section>
<section xml:id="virt-runbook-KubeVirtCRModified">
<title>KubeVirtCRModified</title>
<bridgehead xml:id="meaning-kubevirtcrmodified" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when an operand of the HyperConverged Cluster Operator (HCO)
is changed by someone or something other than HCO.</simpara>
<simpara>HCO configures OpenShift Virtualization and its supporting operators in an
opinionated way and overwrites its operands when there is an unexpected change
to them. Users must not modify the operands directly. The <literal>HyperConverged</literal>
custom resource is the source of truth for the configuration.</simpara>
<bridgehead xml:id="impact-kubevirtcrmodified" renderas="sect4">Impact</bridgehead>
<simpara>Changing the operands manually causes the cluster configuration to fluctuate
and might lead to instability.</simpara>
<bridgehead xml:id="diagnosis-kubevirtcrmodified" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Check the <literal>component_name</literal> value in the alert details to determine the operand
kind (<literal>kubevirt</literal>) and the operand name (<literal>kubevirt-kubevirt-hyperconverged</literal>)
that are being changed:</simpara>
<programlisting language="text" linenumbering="unnumbered">Labels
  alertname=KubevirtHyperconvergedClusterOperatorCRModification
  component_name=kubevirt/kubevirt-kubevirt-hyperconverged
  severity=warning</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-kubevirtcrmodified" renderas="sect4">Mitigation</bridgehead>
<simpara>Do not change the HCO operands directly. Use <literal>HyperConverged</literal> objects to configure
the cluster.</simpara>
<simpara>The alert resolves itself after 10 minutes if the operands are not changed manually.</simpara>
</section>
<section xml:id="virt-runbook-KubeVirtDeprecatedAPIRequested">
<title>KubeVirtDeprecatedAPIRequested</title>
<bridgehead xml:id="meaning-kubevirtdeprecatedapirequested" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a deprecated <literal>KubeVirt</literal> API is used.</simpara>
<bridgehead xml:id="impact-kubevirtdeprecatedapirequested" renderas="sect4">Impact</bridgehead>
<simpara>Using a deprecated API is not recommended because the request will
fail when the API is removed in a future release.</simpara>
<bridgehead xml:id="diagnosis-kubevirtdeprecatedapirequested" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Check the <emphasis role="strong">Description</emphasis> and <emphasis role="strong">Summary</emphasis> sections of the alert to identify the
deprecated API as in the following example:</simpara>
<simpara><emphasis role="strong">Description</emphasis></simpara>
<simpara><literal>Detected requests to the deprecated virtualmachines.kubevirt.io/v1alpha3 API.</literal></simpara>
<simpara><emphasis role="strong">Summary</emphasis></simpara>
<simpara><literal>2 requests were detected in the last 10 minutes.</literal></simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-kubevirtdeprecatedapirequested" renderas="sect4">Mitigation</bridgehead>
<simpara>Use fully supported APIs. The alert resolves itself after 10 minutes if the deprecated
API is not used.</simpara>
</section>
<section xml:id="virt-runbook-KubeVirtNoAvailableNodesToRunVMs">
<title>KubeVirtNoAvailableNodesToRunVMs</title>
<bridgehead xml:id="meaning-kubevirtnoavailablenodestorunvms" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the node CPUs in the cluster do not support virtualization
or the virtualization extensions are not enabled.</simpara>
<bridgehead xml:id="impact-kubevirtnoavailablenodestorunvms" renderas="sect4">Impact</bridgehead>
<simpara>The nodes must support virtualization and the virtualization features must be
enabled in the BIOS to run virtual machines (VMs).</simpara>
<bridgehead xml:id="diagnosis-kubevirtnoavailablenodestorunvms" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Check the nodes for hardware virtualization support:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -o json|jq '.items[]|{"name": .metadata.name, "kvm": .status.allocatable["devices.kubevirt.io/kvm"]}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">{
  "name": "shift-vwpsz-master-0",
  "kvm": null
}
{
  "name": "shift-vwpsz-master-1",
  "kvm": null
}
{
  "name": "shift-vwpsz-master-2",
  "kvm": null
}
{
  "name": "shift-vwpsz-worker-8bxkp",
  "kvm": "1k"
}
{
  "name": "shift-vwpsz-worker-ctgmc",
  "kvm": "1k"
}
{
  "name": "shift-vwpsz-worker-gl5zl",
  "kvm": "1k"
}</programlisting>
</para>
</formalpara>
<simpara>Nodes with <literal>"kvm": null</literal> or <literal>"kvm": 0</literal> do not support virtualization extensions.</simpara>
<simpara>Nodes with <literal>"kvm": "1k"</literal> do support virtualization extensions.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-kubevirtnoavailablenodestorunvms" renderas="sect4">Mitigation</bridgehead>
<simpara>Ensure that hardware and CPU virtualization extensions are enabled on all nodes
and that the nodes are correctly labeled.</simpara>
<simpara>See <link xlink:href="https://access.redhat.com/solutions/5106121">OpenShift Virtualization reports no nodes are available, cannot start VMs</link>
for details.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case.</simpara>
</section>
<section xml:id="virt-runbook-KubevirtVmHighMemoryUsage">
<title>KubevirtVmHighMemoryUsage</title>
<bridgehead xml:id="meaning-kubevirtvmhighmemoryusage" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a container hosting a virtual machine (VM) has less
than 20 MB free memory.</simpara>
<bridgehead xml:id="impact-kubevirtvmhighmemoryusage" renderas="sect4">Impact</bridgehead>
<simpara>The virtual machine running inside the container is terminated by the runtime
if the container&#8217;s memory limit is exceeded.</simpara>
<bridgehead xml:id="diagnosis-kubevirtvmhighmemoryusage" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Obtain the <literal>virt-launcher</literal> pod details:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pod &lt;virt-launcher&gt; -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Identify <literal>compute</literal> container processes with high memory usage in the
<literal>virt-launcher</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -it &lt;virt-launcher&gt; -c compute -- top</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-kubevirtvmhighmemoryusage" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>Increase the memory limit in the <literal>VirtualMachine</literal> specification as in
the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-name
    spec:
      domain:
        resources:
          limits:
            memory: 200Mi
          requests:
            memory: 128Mi</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-runbook-KubeVirtVMIExcessiveMigrations">
<title>KubeVirtVMIExcessiveMigrations</title>
<bridgehead xml:id="meaning-kubevirtvmiexcessivemigrations" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a virtual machine instance (VMI) live migrates more than
12 times over a period of 24 hours.</simpara>
<simpara>This migration rate is abnormally high, even during an upgrade. This alert might
indicate a problem in the cluster infrastructure, such as network disruptions
or insufficient resources.</simpara>
<bridgehead xml:id="impact-kubevirtvmiexcessivemigrations" renderas="sect4">Impact</bridgehead>
<simpara>A virtual machine (VM) that migrates too frequently might experience degraded
performance because memory page faults occur during the transition.</simpara>
<bridgehead xml:id="diagnosis-kubevirtvmiexcessivemigrations" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that the worker node has sufficient resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/worker= -o json | \
  jq .items[].status.allocatable</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "cpu": "3500m",
  "devices.kubevirt.io/kvm": "1k",
  "devices.kubevirt.io/sev": "0",
  "devices.kubevirt.io/tun": "1k",
  "devices.kubevirt.io/vhost-net": "1k",
  "ephemeral-storage": "38161122446",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "7000128Ki",
  "pods": "250"
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the status of the worker node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/worker= -o json | \
  jq .items[].status.conditions</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:12:02Z",
  "message": "kubelet has sufficient memory available",
  "reason": "KubeletHasSufficientMemory",
  "status": "False",
  "type": "MemoryPressure"
},
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:12:02Z",
  "message": "kubelet has no disk pressure",
  "reason": "KubeletHasNoDiskPressure",
  "status": "False",
  "type": "DiskPressure"
},
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:12:02Z",
  "message": "kubelet has sufficient PID available",
  "reason": "KubeletHasSufficientPID",
  "status": "False",
  "type": "PIDPressure"
},
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:24:15Z",
  "message": "kubelet is posting ready status",
  "reason": "KubeletReady",
  "status": "True",
  "type": "Ready"
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Log in to the worker node and verify that the <literal>kubelet</literal> service is running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ systemctl status kubelet</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>kubelet</literal> journal log for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ journalctl -r -u kubelet</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-kubevirtvmiexcessivemigrations" renderas="sect4">Mitigation</bridgehead>
<simpara>Ensure that the worker nodes have sufficient resources (CPU, memory, disk) to
run VM workloads without interruption.</simpara>
<simpara>If the problem persists, try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-LowKVMNodesCount">
<title>LowKVMNodesCount</title>
<bridgehead xml:id="meaning-lowkvmnodescount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when fewer than two nodes in the cluster have KVM resources.</simpara>
<bridgehead xml:id="impact-lowkvmnodescount" renderas="sect4">Impact</bridgehead>
<simpara>The cluster must have at least two nodes with KVM resources for live migration.</simpara>
<simpara>Virtual machines cannot be scheduled or run if no nodes have KVM resources.</simpara>
<bridgehead xml:id="diagnosis-lowkvmnodescount" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Identify the nodes with KVM resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -o jsonpath='{.items[*].status.allocatable}' | \
  grep devices.kubevirt.io/kvm</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-lowkvmnodescount" renderas="sect4">Mitigation</bridgehead>
<simpara>Install KVM on the nodes without KVM resources.</simpara>
</section>
<section xml:id="virt-runbook-LowReadyVirtControllersCount">
<title>LowReadyVirtControllersCount</title>
<bridgehead xml:id="meaning-lowreadyvirtcontrollerscount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when one or more <literal>virt-controller</literal> pods are running, but
none of these pods has been in the <literal>Ready</literal> state for the past 5 minutes.</simpara>
<simpara>A <literal>virt-controller</literal> device monitors the custom resource definitions (CRDs)
of a virtual machine instance (VMI) and manages the associated pods. The
device creates pods for VMIs and manages their lifecycle. The device is
critical for cluster-wide virtualization functionality.</simpara>
<bridgehead xml:id="impact-lowreadyvirtcontrollerscount" renderas="sect4">Impact</bridgehead>
<simpara>This alert indicates that a cluster-level failure might occur. Actions
related to VM lifecycle management, such as launching a new VMI or
shutting down an existing VMI, will fail.</simpara>
<bridgehead xml:id="diagnosis-lowreadyvirtcontrollerscount" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Verify a <literal>virt-controller</literal> device is available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -n $NAMESPACE virt-controller \
  -o jsonpath='{.status.readyReplicas}'</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-controller</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-controller -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-controller</literal> deployment to check for
status conditions, such as crashing pods or failures to pull images:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-controller</programlisting>
</listitem>
<listitem>
<simpara>Check if any problems occurred with the nodes. For example, they might
be in a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-lowreadyvirtcontrollerscount" renderas="sect4">Mitigation</bridgehead>
<simpara>This alert can have multiple causes, including the following:</simpara>
<itemizedlist>
<listitem>
<simpara>The cluster has insufficient memory.</simpara>
</listitem>
<listitem>
<simpara>The nodes are down.</simpara>
</listitem>
<listitem>
<simpara>The API server is overloaded. For example, the scheduler might be under
a heavy load and therefore not completely available.</simpara>
</listitem>
<listitem>
<simpara>There are network issues.</simpara>
</listitem>
</itemizedlist>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-LowReadyVirtOperatorsCount">
<title>LowReadyVirtOperatorsCount</title>
<bridgehead xml:id="meaning-lowreadyvirtoperatorscount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when one or more <literal>virt-operator</literal> pods are running, but
none of these pods has been in a <literal>Ready</literal> state for the last 10 minutes.</simpara>
<simpara>The <literal>virt-operator</literal> is the first Operator to start in a cluster. The <literal>virt-operator</literal>
deployment has a default replica of two <literal>virt-operator</literal> pods.</simpara>
<simpara>Its primary responsibilities include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Installing, live-updating, and live-upgrading a cluster</simpara>
</listitem>
<listitem>
<simpara>Monitoring the lifecycle of top-level controllers, such as <literal>virt-controller</literal>,
<literal>virt-handler</literal>, <literal>virt-launcher</literal>, and managing their reconciliation</simpara>
</listitem>
<listitem>
<simpara>Certain cluster-wide tasks, such as certificate rotation and infrastructure
management</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-lowreadyvirtoperatorscount" renderas="sect4">Impact</bridgehead>
<simpara>A cluster-level failure might occur. Critical cluster-wide management
functionalities, such as certification rotation, upgrade, and reconciliation of
controllers, might become unavailable. Such a state also triggers the
<literal>NoReadyVirtOperator</literal> alert.</simpara>
<simpara>The <literal>virt-operator</literal> is not directly responsible for virtual machines (VMs)
in the cluster. Therefore, its temporary unavailability does not significantly
affect VM workloads.</simpara>
<bridgehead xml:id="diagnosis-lowreadyvirtoperatorscount" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the name of the <literal>virt-operator</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-operator -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-operator</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check for node issues, such as a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-lowreadyvirtoperatorscount" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-LowVirtAPICount">
<title>LowVirtAPICount</title>
<bridgehead xml:id="meaning-lowvirtapicount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when only one available <literal>virt-api</literal> pod is detected during a
60-minute period, although at least two nodes are available for scheduling.</simpara>
<bridgehead xml:id="impact-lowvirtapicount" renderas="sect4">Impact</bridgehead>
<simpara>An API call outage might occur during node eviction because the <literal>virt-api</literal> pod
becomes a single point of failure.</simpara>
<bridgehead xml:id="diagnosis-lowvirtapicount" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the number of available <literal>virt-api</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -n $NAMESPACE virt-api \
  -o jsonpath='{.status.readyReplicas}'</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-api</literal> deployment for error conditions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-api -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the nodes for issues such as nodes in a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-lowvirtapicount" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and to resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-LowVirtControllersCount">
<title>LowVirtControllersCount</title>
<bridgehead xml:id="meaning-lowvirtcontrollerscount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a low number of <literal>virt-controller</literal> pods is detected. At
least one <literal>virt-controller</literal> pod must be available in order to ensure high
availability. The default number of replicas is 2.</simpara>
<simpara>A <literal>virt-controller</literal> device monitors the custom resource definitions (CRDs) of a
virtual machine instance (VMI) and manages the associated pods. The device
create pods for VMIs and manages the lifecycle of the pods. The device is
critical for cluster-wide virtualization functionality.</simpara>
<bridgehead xml:id="impact-lowvirtcontrollerscount" renderas="sect4">Impact</bridgehead>
<simpara>The responsiveness of OpenShift Virtualization might become negatively
affected. For example, certain requests might be missed.</simpara>
<simpara>In addition, if another <literal>virt-launcher</literal> instance terminates unexpectedly,
OpenShift Virtualization might become completely unresponsive.</simpara>
<bridgehead xml:id="diagnosis-lowvirtcontrollerscount" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Verify that running <literal>virt-controller</literal> pods are available:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-controller</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-launcher</literal> logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs &lt;virt-launcher&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-launcher</literal> pod to check for status conditions
such as unexpected termination or a <literal>NotReady</literal> state.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pod/&lt;virt-launcher&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-lowvirtcontrollerscount" renderas="sect4">Mitigation</bridgehead>
<simpara>This alert can have a variety of causes, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Not enough memory on the cluster</simpara>
</listitem>
<listitem>
<simpara>Nodes are down</simpara>
</listitem>
<listitem>
<simpara>The API server is overloaded. For example, the scheduler might be under a
heavy load and therefore not completely available.</simpara>
</listitem>
<listitem>
<simpara>Networking issues</simpara>
</listitem>
</itemizedlist>
<simpara>Identify the root cause and fix it, if possible.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-LowVirtOperatorCount">
<title>LowVirtOperatorCount</title>
<bridgehead xml:id="meaning-lowvirtoperatorcount" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when only one <literal>virt-operator</literal> pod in a <literal>Ready</literal> state has
been running for the last 60 minutes.</simpara>
<simpara>The <literal>virt-operator</literal> is the first Operator to start in a cluster. Its primary
responsibilities include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Installing, live-updating, and live-upgrading a cluster</simpara>
</listitem>
<listitem>
<simpara>Monitoring the lifecycle of top-level controllers, such as <literal>virt-controller</literal>,
<literal>virt-handler</literal>, <literal>virt-launcher</literal>, and managing their reconciliation</simpara>
</listitem>
<listitem>
<simpara>Certain cluster-wide tasks, such as certificate rotation and infrastructure
management</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-lowvirtoperatorcount" renderas="sect4">Impact</bridgehead>
<simpara>The <literal>virt-operator</literal> cannot provide high availability (HA) for the deployment.
HA requires two or more <literal>virt-operator</literal> pods in a <literal>Ready</literal> state. The default
deployment is two pods.</simpara>
<simpara>The <literal>virt-operator</literal> is not directly responsible for virtual machines (VMs)
in the cluster. Therefore, its decreased availability does not significantly
affect VM workloads.</simpara>
<bridgehead xml:id="diagnosis-lowvirtoperatorcount" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the states of the <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Review the logs of the affected <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs &lt;virt-operator&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the affected <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pod &lt;virt-operator&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-lowvirtoperatorcount" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the <link xlink:href="https://access.redhat.com">Customer Portal</link>
and open a support case, attaching the artifacts gathered during the Diagnosis
procedure.</simpara>
</section>
<section xml:id="virt-runbook-NetworkAddonsConfigNotReady">
<title>NetworkAddonsConfigNotReady</title>
<bridgehead xml:id="meaning-networkaddonsconfignotready" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the <literal>NetworkAddonsConfig</literal> custom resource (CR) of the
Cluster Network Addons Operator (CNAO) is not ready.</simpara>
<simpara>CNAO deploys additional networking components on the cluster. This alert indicates
that one of the deployed components is not ready.</simpara>
<bridgehead xml:id="impact-networkaddonsconfignotready" renderas="sect4">Impact</bridgehead>
<simpara>Network functionality is affected.</simpara>
<bridgehead xml:id="diagnosis-networkaddonsconfignotready" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check the status conditions of the <literal>NetworkAddonsConfig</literal> CR to identify the
deployment or daemon set that is not ready:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get networkaddonsconfig \
  -o custom-columns="":.status.conditions[*].message</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">DaemonSet "cluster-network-addons/macvtap-cni" update is being processed...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the component&#8217;s pod for errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n cluster-network-addons get daemonset &lt;pod&gt; -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the component&#8217;s logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n cluster-network-addons logs &lt;pod&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check the component&#8217;s details for error conditions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n cluster-network-addons describe &lt;pod&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-networkaddonsconfignotready" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-NoLeadingVirtOperator">
<title>NoLeadingVirtOperator</title>
<bridgehead xml:id="meaning-noleadingvirtoperator" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when no <literal>virt-operator</literal> pod with a leader lease has been detected
for 10 minutes, although the <literal>virt-operator</literal> pods are in a <literal>Ready</literal> state. The
alert indicates that no leader pod is available.</simpara>
<simpara>The <literal>virt-operator</literal> is the first Operator to start in a cluster. Its primary
responsibilities include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Installing, live updating, and live upgrading a cluster</simpara>
</listitem>
<listitem>
<simpara>Monitoring the lifecycle of top-level controllers, such as <literal>virt-controller</literal>,
<literal>virt-handler</literal>, <literal>virt-launcher</literal>, and managing their reconciliation</simpara>
</listitem>
<listitem>
<simpara>Certain cluster-wide tasks, such as certificate rotation and infrastructure
management</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>virt-operator</literal> deployment has a default replica of 2 pods, with one pod
holding a leader lease.</simpara>
<bridgehead xml:id="impact-noleadingvirtoperator" renderas="sect4">Impact</bridgehead>
<simpara>This alert indicates a failure at the level of the cluster. As a result, critical
cluster-wide management functionalities, such as certification rotation, upgrade,
and reconciliation of controllers, might not be available.</simpara>
<bridgehead xml:id="diagnosis-noleadingvirtoperator" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A -o \
  custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the status of the <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-operator</literal> pod logs to determine the leader status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs | grep lead</programlisting>
<simpara>Leader pod example:</simpara>
<programlisting language="text" linenumbering="unnumbered">{"component":"virt-operator","level":"info","msg":"Attempting to acquire
leader status","pos":"application.go:400","timestamp":"2021-11-30T12:15:18.635387Z"}
I1130 12:15:18.635452       1 leaderelection.go:243] attempting to acquire
leader lease &lt;namespace&gt;/virt-operator...
I1130 12:15:19.216582       1 leaderelection.go:253] successfully acquired
lease &lt;namespace&gt;/virt-operator
{"component":"virt-operator","level":"info","msg":"Started leading",
"pos":"application.go:385","timestamp":"2021-11-30T12:15:19.216836Z"}</programlisting>
<simpara>Non-leader pod example:</simpara>
<programlisting language="text" linenumbering="unnumbered">{"component":"virt-operator","level":"info","msg":"Attempting to acquire
leader status","pos":"application.go:400","timestamp":"2021-11-30T12:15:20.533696Z"}
I1130 12:15:20.533792       1 leaderelection.go:243] attempting to acquire
leader lease &lt;namespace&gt;/virt-operator...</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the affected <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pod &lt;virt-operator&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-noleadingvirtoperator" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to find
the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-NoReadyVirtController">
<title>NoReadyVirtController</title>
<bridgehead xml:id="meaning-noreadyvirtcontroller" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when no available <literal>virt-controller</literal> devices have been
detected for 5 minutes.</simpara>
<simpara>The <literal>virt-controller</literal> devices monitor the custom resource definitions of
virtual machine instances (VMIs) and manage the associated pods. The devices
create pods for VMIs and manage the lifecycle of the pods.</simpara>
<simpara>Therefore, <literal>virt-controller</literal> devices are critical for all cluster-wide
virtualization functionality.</simpara>
<bridgehead xml:id="impact-noreadyvirtcontroller" renderas="sect4">Impact</bridgehead>
<simpara>Any actions related to VM lifecycle management fail. This notably includes
launching a new VMI or shutting down an existing VMI.</simpara>
<bridgehead xml:id="diagnosis-noreadyvirtcontroller" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Verify the number of <literal>virt-controller</literal> devices:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -n $NAMESPACE virt-controller \
  -o jsonpath='{.status.readyReplicas}'</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-controller</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-controller -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-controller</literal> deployment to check for
status conditions such as crashing pods or failure to pull images:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-controller</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-controller</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ get pods -n $NAMESPACE | grep virt-controller</programlisting>
</listitem>
<listitem>
<simpara>Check the logs of the <literal>virt-controller</literal> pods for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n $NAMESPACE &lt;virt-controller&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check the nodes for problems, such as a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-noreadyvirtcontroller" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to find
the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-NoReadyVirtOperator">
<title>NoReadyVirtOperator</title>
<bridgehead xml:id="meaning-noreadyvirtoperator" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when no <literal>virt-operator</literal> pod in a <literal>Ready</literal> state has been
detected for 10 minutes.</simpara>
<simpara>The <literal>virt-operator</literal> is the first Operator to start in a cluster. Its primary
responsibilities include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Installing, live-updating, and live-upgrading a cluster</simpara>
</listitem>
<listitem>
<simpara>Monitoring the life cycle of top-level controllers, such as <literal>virt-controller</literal>,
<literal>virt-handler</literal>, <literal>virt-launcher</literal>, and managing their reconciliation</simpara>
</listitem>
<listitem>
<simpara>Certain cluster-wide tasks, such as certificate rotation and infrastructure
management</simpara>
</listitem>
</itemizedlist>
<simpara>The default deployment is two <literal>virt-operator</literal> pods.</simpara>
<bridgehead xml:id="impact-noreadyvirtoperator" renderas="sect4">Impact</bridgehead>
<simpara>This alert indicates a cluster-level failure. Critical cluster management
functionalities, such as certification rotation, upgrade, and reconciliation
of controllers, might not be not available.</simpara>
<simpara>The <literal>virt-operator</literal> is not directly responsible for virtual machines in
the cluster. Therefore, its temporary unavailability does not significantly
affect workloads.</simpara>
<bridgehead xml:id="diagnosis-noreadyvirtoperator" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the name of the <literal>virt-operator</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-operator -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Generate the description of the <literal>virt-operator</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check for node issues, such as a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-noreadyvirtoperator" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the <link xlink:href="https://access.redhat.com">Customer Portal</link>
and open a support case, attaching the artifacts gathered during the Diagnosis
procedure.</simpara>
</section>
<section xml:id="virt-runbook-OrphanedVirtualMachineInstances">
<title>OrphanedVirtualMachineInstances</title>
<bridgehead xml:id="meaning-orphanedvirtualmachineinstances" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a virtual machine instance (VMI), or <literal>virt-launcher</literal>
pod, runs on a node that does not have a running <literal>virt-handler</literal> pod.
Such a VMI is called <emphasis>orphaned</emphasis>.</simpara>
<bridgehead xml:id="impact-orphanedvirtualmachineinstances" renderas="sect4">Impact</bridgehead>
<simpara>Orphaned VMIs cannot be managed.</simpara>
<bridgehead xml:id="diagnosis-orphanedvirtualmachineinstances" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check the status of the <literal>virt-handler</literal> pods to view the nodes on
which they are running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods --all-namespaces -o wide -l kubevirt.io=virt-handler</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the VMIs to identify VMIs running on nodes
that do not have a running <literal>virt-handler</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmis --all-namespaces</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-handler</literal> daemon:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get daemonset virt-handler --all-namespaces</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE ...
virt-handler  2        2        2      2           2         ...</programlisting>
</para>
</formalpara>
<simpara>The daemon set is considered healthy if the <literal>Desired</literal>, <literal>Ready</literal>,
and <literal>Available</literal> columns contain the same value.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>virt-handler</literal> daemon set is not healthy, check the <literal>virt-handler</literal>
daemon set for pod deployment issues:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get daemonset virt-handler --all-namespaces -o yaml | jq .status</programlisting>
</listitem>
<listitem>
<simpara>Check the nodes for issues such as a <literal>NotReady</literal> status:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>spec.workloads</literal> stanza of the <literal>KubeVirt</literal> custom resource
(CR) for a workloads placement policy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubevirt kubevirt --all-namespaces -o yaml</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-orphanedvirtualmachineinstances" renderas="sect4">Mitigation</bridgehead>
<simpara>If a workloads placement policy is configured, add the node with the
VMI to the policy.</simpara>
<simpara>Possible causes for the removal of a <literal>virt-handler</literal> pod from a node
include changes to the node&#8217;s taints and tolerations or to a pod&#8217;s
scheduling rules.</simpara>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-OutdatedVirtualMachineInstanceWorkloads">
<title>OutdatedVirtualMachineInstanceWorkloads</title>
<bridgehead xml:id="meaning-outdatedvirtualmachineinstanceworkloads" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when running virtual machine instances (VMIs) in
outdated <literal>virt-launcher</literal> pods are detected 24 hours after the OpenShift
Virtualization control plane has been updated.</simpara>
<bridgehead xml:id="impact-outdatedvirtualmachineinstanceworkloads" renderas="sect4">Impact</bridgehead>
<simpara>Outdated VMIs might not have access to new OpenShift Virtualization
features.</simpara>
<simpara>Outdated VMIs will not receive the security fixes associated with
the <literal>virt-launcher</literal> pod update.</simpara>
<bridgehead xml:id="diagnosis-outdatedvirtualmachineinstanceworkloads" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Identify the outdated VMIs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi -l kubevirt.io/outdatedLauncherImage --all-namespaces</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>KubeVirt</literal> custom resource (CR) to determine whether
<literal>workloadUpdateMethods</literal> is configured in the <literal>workloadUpdateStrategy</literal>
stanza:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubevirt --all-namespaces -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check each outdated VMI to determine whether it is live-migratable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi &lt;vmi&gt; -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachineInstance
# ...
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: null
      message: cannot migrate VMI which does not use masquerade
      to connect to the pod network
      reason: InterfaceNotLiveMigratable
      status: "False"
      type: LiveMigratable</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-outdatedvirtualmachineinstanceworkloads" renderas="sect4">Mitigation</bridgehead>
<bridgehead xml:id="configuring-automated-workload-updates-outdatedvirtualmachineinstanceworkloads" renderas="sect5">Configuring automated workload updates</bridgehead>
<simpara>Update the <literal>HyperConverged</literal> CR to enable automatic workload updates.</simpara>
<bridgehead xml:id="stopping-a-vm-associated-with-a-non-live-migratable-vmi-outdatedvirtualmachineinstanceworkloads" renderas="sect5">Stopping a VM associated with a non-live-migratable VMI</bridgehead>
<itemizedlist>
<listitem>
<simpara>If a VMI is not live-migratable and if <literal>runStrategy: always</literal> is
set in the corresponding <literal>VirtualMachine</literal> object, you can update the
VMI by manually stopping the virtual machine (VM):</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ virctl stop --namespace &lt;namespace&gt; &lt;vm&gt;</programlisting>
</listitem>
</itemizedlist>
<simpara>A new VMI spins up immediately in an updated <literal>virt-launcher</literal> pod to
replace the stopped VMI. This is the equivalent of a restart action.</simpara>
<note>
<simpara>Manually stopping a <emphasis>live-migratable</emphasis> VM is destructive and
not recommended because it interrupts the workload.</simpara>
</note>
<bridgehead xml:id="migrating-a-live-migratable-vmi-outdatedvirtualmachineinstanceworkloads" renderas="sect5">Migrating a live-migratable VMI</bridgehead>
<simpara>If a VMI is live-migratable, you can update it by creating a <literal>VirtualMachineInstanceMigration</literal>
object that targets a specific running VMI. The VMI is migrated into
an updated <literal>virt-launcher</literal> pod.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a <literal>VirtualMachineInstanceMigration</literal> manifest and save it
as <literal>migration.yaml</literal>:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: &lt;migration_name&gt;
  namespace: &lt;namespace&gt;
spec:
  vmiName: &lt;vmi_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>VirtualMachineInstanceMigration</literal> object to trigger the
migration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f migration.yaml</programlisting>
</listitem>
</orderedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-SingleStackIPv6Unsupported">
<title>SingleStackIPv6Unsupported</title>
<bridgehead xml:id="meaning-singlestackipv6unsupported" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when you install OpenShift Virtualization on a single stack
IPv6 cluster.</simpara>
<bridgehead xml:id="impact-singlestackipv6unsupported" renderas="sect4">Impact</bridgehead>
<simpara>You cannot create virtual machines.</simpara>
<bridgehead xml:id="diagnosis-singlestackipv6unsupported" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Check the cluster network configuration by running the following command:</simpara>
<programlisting language="shell" linenumbering="unnumbered">$ oc get network.config cluster -o yaml</programlisting>
<simpara>The output displays only an IPv6 CIDR for the cluster network.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork:
  - cidr: fd02::/48
    hostPrefix: 64</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-singlestackipv6unsupported" renderas="sect4">Mitigation</bridgehead>
<simpara>Install OpenShift Virtualization on a single stack IPv4 cluster or on a
dual stack IPv4/IPv6 cluster.</simpara>
</section>
<section xml:id="virt-runbook-SSPCommonTemplatesModificationReverted">
<title>SSPCommonTemplatesModificationReverted</title>
<bridgehead xml:id="meaning-sspcommontemplatesmodificationreverted" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the Scheduling, Scale, and Performance (SSP) Operator
reverts changes to common templates as part of its reconciliation procedure.</simpara>
<simpara>The SSP Operator deploys and reconciles the common templates and the Template
Validator. If a user or script changes a common template, the changes are reverted
by the SSP Operator.</simpara>
<bridgehead xml:id="impact-sspcommontemplatesmodificationreverted" renderas="sect4">Impact</bridgehead>
<simpara>Changes to common templates are overwritten.</simpara>
<bridgehead xml:id="diagnosis-sspcommontemplatesmodificationreverted" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>ssp-operator</literal> logs for templates with reverted changes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs --tail=-1 -l control-plane=ssp-operator | \
  grep 'common template' -C 3</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-sspcommontemplatesmodificationreverted" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify and resolve the cause of the changes.</simpara>
<simpara>Ensure that changes are made only to copies of templates, and not to the templates
themselves.</simpara>
</section>
<section xml:id="virt-runbook-SSPDown">
<title>SSPDown</title>
<bridgehead xml:id="meaning-sspdown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when all the Scheduling, Scale and Performance (SSP) Operator
pods are down.</simpara>
<simpara>The SSP Operator is responsible for deploying and reconciling the common
templates and the Template Validator.</simpara>
<bridgehead xml:id="impact-sspdown" renderas="sect4">Impact</bridgehead>
<simpara>Dependent components might not be deployed. Changes in the components might
not be reconciled. As a result, the common templates and/or the Template
Validator might not be updated or reset if they fail.</simpara>
<bridgehead xml:id="diagnosis-sspdown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>ssp-operator</literal> pods.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l control-plane=ssp-operator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>ssp-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pods -l control-plane=ssp-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>ssp-operator</literal> logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs --tail=-1 -l control-plane=ssp-operator</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-sspdown" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-SSPFailingToReconcile">
<title>SSPFailingToReconcile</title>
<bridgehead xml:id="meaning-sspfailingtoreconcile" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the reconcile cycle of the Scheduling, Scale and
Performance (SSP) Operator fails repeatedly, although the SSP Operator
is running.</simpara>
<simpara>The SSP Operator is responsible for deploying and reconciling the common
templates and the Template Validator.</simpara>
<bridgehead xml:id="impact-sspfailingtoreconcile" renderas="sect4">Impact</bridgehead>
<simpara>Dependent components might not be deployed. Changes in the components might
not be reconciled. As a result, the common templates or the Template
Validator might not be updated or reset if they fail.</simpara>
<bridgehead xml:id="diagnosis-sspfailingtoreconcile" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Export the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>ssp-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pods -l control-plane=ssp-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>ssp-operator</literal> logs for errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs --tail=-1 -l control-plane=ssp-operator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the status of the <literal>virt-template-validator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l name=virt-template-validator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-template-validator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pods -l name=virt-template-validator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-template-validator</literal> logs for errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs --tail=-1 -l name=virt-template-validator</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-sspfailingtoreconcile" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-SSPHighRateRejectedVms">
<title>SSPHighRateRejectedVms</title>
<bridgehead xml:id="meaning-ssphighraterejectedvms" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a user or script attempts to create or modify a large
number of virtual machines (VMs), using an invalid configuration.</simpara>
<bridgehead xml:id="impact-ssphighraterejectedvms" renderas="sect4">Impact</bridgehead>
<simpara>The VMs are not created or modified. As a result, the environment might not
behave as expected.</simpara>
<bridgehead xml:id="diagnosis-ssphighraterejectedvms" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Export the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-template-validator</literal> logs for errors that might indicate the
cause:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs --tail=-1 -l name=virt-template-validator</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">{"component":"kubevirt-template-validator","level":"info","msg":"evalution
summary for ubuntu-3166wmdbbfkroku0:\nminimal-required-memory applied: FAIL,
value 1073741824 is lower than minimum [2147483648]\n\nsucceeded=false",
"pos":"admission.go:25","timestamp":"2021-09-28T17:59:10.934470Z"}</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-ssphighraterejectedvms" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-SSPTemplateValidatorDown">
<title>SSPTemplateValidatorDown</title>
<bridgehead xml:id="meaning-ssptemplatevalidatordown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when all the Template Validator pods are down.</simpara>
<simpara>The Template Validator checks virtual machines (VMs) to ensure that they
do not violate their templates.</simpara>
<bridgehead xml:id="impact-ssptemplatevalidatordown" renderas="sect4">Impact</bridgehead>
<simpara>VMs are not validated against their templates. As a result, VMs might be
created with specifications that do not match their respective workloads.</simpara>
<bridgehead xml:id="diagnosis-ssptemplatevalidatordown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the status of the <literal>virt-template-validator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l name=virt-template-validator</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-template-validator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pods -l name=virt-template-validator</programlisting>
</listitem>
<listitem>
<simpara>Check the  <literal>virt-template-validator</literal> logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs --tail=-1 -l name=virt-template-validator</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-ssptemplatevalidatordown" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-UnsupportedHCOModification">
<title>UnsupportedHCOModification</title>
<bridgehead xml:id="meaning-unsupportedhcomodification" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when a JSON Patch annotation is used to change an operand
of the HyperConverged Cluster Operator (HCO).</simpara>
<simpara>HCO configures OpenShift Virtualization and its supporting operators in
an opinionated way and overwrites its operands when there is an unexpected
change to them. Users must not modify the operands directly.</simpara>
<simpara>However, if a change is required and it is not supported by the HCO API,
you can force HCO to set a change in an operator by using JSON Patch annotations.
These changes are not reverted by HCO during its reconciliation process.</simpara>
<bridgehead xml:id="impact-unsupportedhcomodification" renderas="sect4">Impact</bridgehead>
<simpara>Incorrect use of JSON Patch annotations might lead to unexpected results
or an unstable environment.</simpara>
<simpara>Upgrading a system with JSON Patch annotations is dangerous because the
structure of the component custom resources might change.</simpara>
<bridgehead xml:id="diagnosis-unsupportedhcomodification" renderas="sect4">Diagnosis</bridgehead>
<itemizedlist>
<listitem>
<simpara>Check the <literal>annotation_name</literal> in the alert details to identify the JSON
Patch annotation:</simpara>
<programlisting language="text" linenumbering="unnumbered">Labels
  alertname=KubevirtHyperconvergedClusterOperatorUSModification
  annotation_name=kubevirt.kubevirt.io/jsonpatch
  severity=info</programlisting>
</listitem>
</itemizedlist>
<bridgehead xml:id="mitigation-unsupportedhcomodification" renderas="sect4">Mitigation</bridgehead>
<simpara>It is best to use the HCO API to change an operand. However, if the change
can only be done with a JSON Patch annotation, proceed with caution.</simpara>
<simpara>Remove JSON Patch annotations before upgrade to avoid potential issues.</simpara>
</section>
<section xml:id="virt-runbook-VirtAPIDown">
<title>VirtAPIDown</title>
<bridgehead xml:id="meaning-virtapidown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when all the API Server pods are down.</simpara>
<bridgehead xml:id="impact-virtapidown" renderas="sect4">Impact</bridgehead>
<simpara>OpenShift Virtualization objects cannot send API calls.</simpara>
<bridgehead xml:id="diagnosis-virtapidown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-api</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-api</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-api -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-api</literal> deployment details for issues such as crashing pods or
image pull failures:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-api</programlisting>
</listitem>
<listitem>
<simpara>Check for issues such as nodes in a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtapidown" renderas="sect4">Mitigation</bridgehead>
<simpara>Try to identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtApiRESTErrorsBurst">
<title>VirtApiRESTErrorsBurst</title>
<bridgehead xml:id="meaning-virtapiresterrorsburst" renderas="sect4">Meaning</bridgehead>
<simpara>More than 80% of REST calls have failed in the <literal>virt-api</literal> pods in the last
5 minutes.</simpara>
<bridgehead xml:id="impact-virtapiresterrorsburst" renderas="sect4">Impact</bridgehead>
<simpara>A very high rate of failed REST calls to <literal>virt-api</literal> might lead to slow
response and execution of API calls, and potentially to API calls being
completely dismissed.</simpara>
<simpara>However, currently running virtual machine workloads are not likely to
be affected.</simpara>
<bridgehead xml:id="diagnosis-virtapiresterrorsburst" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Obtain the list of <literal>virt-api</literal> pods on your deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-api</literal> logs for error messages:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n $NAMESPACE &lt;virt-api&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-api</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe -n $NAMESPACE &lt;virt-api&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check if any problems occurred with the nodes. For example, they might
be in a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-api</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-api -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-api</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-api</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtapiresterrorsburst" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtApiRESTErrorsHigh">
<title>VirtApiRESTErrorsHigh</title>
<bridgehead xml:id="meaning-virtapiresterrorshigh" renderas="sect4">Meaning</bridgehead>
<simpara>More than 5% of REST calls have failed in the <literal>virt-api</literal> pods in the last 60 minutes.</simpara>
<bridgehead xml:id="impact-virtapiresterrorshigh" renderas="sect4">Impact</bridgehead>
<simpara>A high rate of failed REST calls to <literal>virt-api</literal> might lead to slow response and
execution of API calls.</simpara>
<simpara>However, currently running virtual machine workloads are not likely to be affected.</simpara>
<bridgehead xml:id="diagnosis-virtapiresterrorshigh" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable as follows:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-api</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-api</literal> logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n  $NAMESPACE &lt;virt-api&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-api</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe -n $NAMESPACE &lt;virt-api&gt;</programlisting>
</listitem>
<listitem>
<simpara>Check if any problems occurred with the nodes. For example, they might be in
a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-api</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-api -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-api</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-api</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtapiresterrorshigh" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtControllerDown">
<title>VirtControllerDown</title>
<bridgehead xml:id="meaning-virtcontrollerdown" renderas="sect4">Meaning</bridgehead>
<simpara>No running <literal>virt-controller</literal> pod has been detected for 5 minutes.</simpara>
<bridgehead xml:id="impact-virtcontrollerdown" renderas="sect4">Impact</bridgehead>
<simpara>Any actions related to virtual machine (VM) lifecycle management fail.
This notably includes launching a new virtual machine instance (VMI)
or shutting down an existing VMI.</simpara>
<bridgehead xml:id="diagnosis-virtcontrollerdown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-controller</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get deployment -n $NAMESPACE virt-controller -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Review the logs of the <literal>virt-controller</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get logs &lt;virt-controller&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtcontrollerdown" renderas="sect4">Mitigation</bridgehead>
<simpara>This alert can have a variety of causes, including the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Node resource exhaustion</simpara>
</listitem>
<listitem>
<simpara>Not enough memory on the cluster</simpara>
</listitem>
<listitem>
<simpara>Nodes are down</simpara>
</listitem>
<listitem>
<simpara>The API server is overloaded. For example, the scheduler might be
under a heavy load and therefore not completely available.</simpara>
</listitem>
<listitem>
<simpara>Networking issues</simpara>
</listitem>
</itemizedlist>
<simpara>Identify the root cause and fix it, if possible.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtControllerRESTErrorsBurst">
<title>VirtControllerRESTErrorsBurst</title>
<bridgehead xml:id="meaning-virtcontrollerresterrorsburst" renderas="sect4">Meaning</bridgehead>
<simpara>More than 80% of REST calls in <literal>virt-controller</literal> pods failed in the last 5
minutes.</simpara>
<simpara>The <literal>virt-controller</literal> has likely fully lost the connection to the API server.</simpara>
<simpara>This error is frequently caused by one of the following problems:</simpara>
<itemizedlist>
<listitem>
<simpara>The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.</simpara>
</listitem>
<listitem>
<simpara>The <literal>virt-controller</literal> pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-virtcontrollerresterrorsburst" renderas="sect4">Impact</bridgehead>
<simpara>Status updates are not propagated and actions like migrations cannot take place.
However, running workloads are not impacted.</simpara>
<bridgehead xml:id="diagnosis-virtcontrollerresterrorsburst" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>List the available <literal>virt-controller</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-controller</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-controller</literal> logs for error messages when connecting to the
API server:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n  $NAMESPACE &lt;virt-controller&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtcontrollerresterrorsburst" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>If the <literal>virt-controller</literal> pod cannot connect to the API server, delete the
pod to force a restart:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n $NAMESPACE &lt;virt-controller&gt;</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtControllerRESTErrorsHigh">
<title>VirtControllerRESTErrorsHigh</title>
<bridgehead xml:id="meaning-virtcontrollerresterrorshigh" renderas="sect4">Meaning</bridgehead>
<simpara>More than 5% of REST calls failed in <literal>virt-controller</literal> in the last 60 minutes.</simpara>
<simpara>This is most likely because <literal>virt-controller</literal> has partially lost connection
to the API server.</simpara>
<simpara>This error is frequently caused by one of the following problems:</simpara>
<itemizedlist>
<listitem>
<simpara>The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.</simpara>
</listitem>
<listitem>
<simpara>The <literal>virt-controller</literal> pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-virtcontrollerresterrorshigh" renderas="sect4">Impact</bridgehead>
<simpara>Node-related actions, such as starting and migrating, and scheduling virtual
machines, are delayed. Running workloads are not affected, but reporting
their current status might be delayed.</simpara>
<bridgehead xml:id="diagnosis-virtcontrollerresterrorshigh" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>List the available <literal>virt-controller</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-controller</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-controller</literal> logs for error messages when connecting
to the API server:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n  $NAMESPACE &lt;virt-controller&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtcontrollerresterrorshigh" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>If the <literal>virt-controller</literal> pod cannot connect to the API server, delete
the pod to force a restart:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n $NAMESPACE &lt;virt-controller&gt;</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtHandlerDaemonSetRolloutFailing">
<title>VirtHandlerDaemonSetRolloutFailing</title>
<bridgehead xml:id="meaning-virthandlerdaemonsetrolloutfailing" renderas="sect4">Meaning</bridgehead>
<simpara>The <literal>virt-handler</literal> daemon set has failed to deploy on one or more worker
nodes after 15 minutes.</simpara>
<bridgehead xml:id="impact-virthandlerdaemonsetrolloutfailing" renderas="sect4">Impact</bridgehead>
<simpara>This alert is a warning. It does not indicate that all <literal>virt-handler</literal> daemon
sets have failed to deploy. Therefore, the normal lifecycle of virtual
machines is not affected unless the cluster is overloaded.</simpara>
<bridgehead xml:id="diagnosis-virthandlerdaemonsetrolloutfailing" renderas="sect4">Diagnosis</bridgehead>
<simpara>Identify worker nodes that do not have a running <literal>virt-handler</literal> pod:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Export the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-handler</literal> pods to identify pods that have
not deployed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler</programlisting>
</listitem>
<listitem>
<simpara>Obtain the name of the worker node of the <literal>virt-handler</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pod &lt;virt-handler&gt; -o jsonpath='{.spec.nodeName}'</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virthandlerdaemonsetrolloutfailing" renderas="sect4">Mitigation</bridgehead>
<simpara>If the <literal>virt-handler</literal> pods failed to deploy because of insufficient resources,
you can delete other pods on the affected worker node.</simpara>
</section>
<section xml:id="virt-runbook-VirtHandlerRESTErrorsBurst">
<title>VirtHandlerRESTErrorsBurst</title>
<bridgehead xml:id="meaning-virthandlerresterrorsburst" renderas="sect4">Meaning</bridgehead>
<simpara>More than 80% of REST calls failed in <literal>virt-handler</literal> in the last 5 minutes.
This alert usually indicates that the <literal>virt-handler</literal> pods cannot connect
to the API server.</simpara>
<simpara>This error is frequently caused by one of the following problems:</simpara>
<itemizedlist>
<listitem>
<simpara>The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.</simpara>
</listitem>
<listitem>
<simpara>The <literal>virt-handler</literal> pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-virthandlerresterrorsburst" renderas="sect4">Impact</bridgehead>
<simpara>Status updates are not propagated and node-related actions, such as migrations,
fail. However, running workloads on the affected node are not impacted.</simpara>
<bridgehead xml:id="diagnosis-virthandlerresterrorsburst" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-handler</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-handler</literal> logs for error messages when connecting to
the API server:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n  $NAMESPACE &lt;virt-handler&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virthandlerresterrorsburst" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>If the <literal>virt-handler</literal> cannot connect to the API server, delete the pod
to force a restart:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n $NAMESPACE &lt;virt-handler&gt;</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtHandlerRESTErrorsHigh">
<title>VirtHandlerRESTErrorsHigh</title>
<bridgehead xml:id="meaning-virthandlerresterrorshigh" renderas="sect4">Meaning</bridgehead>
<simpara>More than 5% of REST calls failed in <literal>virt-handler</literal> in the last 60 minutes.
This alert usually indicates that the <literal>virt-handler</literal> pods have partially
lost connection to the API server.</simpara>
<simpara>This error is frequently caused by one of the following problems:</simpara>
<itemizedlist>
<listitem>
<simpara>The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.</simpara>
</listitem>
<listitem>
<simpara>The <literal>virt-handler</literal> pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-virthandlerresterrorshigh" renderas="sect4">Impact</bridgehead>
<simpara>Node-related actions, such as starting and migrating workloads, are delayed
on the node that <literal>virt-handler</literal> is running on. Running workloads are not
affected, but reporting their current status might be delayed.</simpara>
<bridgehead xml:id="diagnosis-virthandlerresterrorshigh" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>List the available <literal>virt-handler</literal> pods to identify the failing
<literal>virt-handler</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler</programlisting>
</listitem>
<listitem>
<simpara>Check the failing <literal>virt-handler</literal> pod log for API server
connectivity errors:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n $NAMESPACE &lt;virt-handler&gt;</programlisting>
<simpara>Example error message:</simpara>
<programlisting language="json" linenumbering="unnumbered">{"component":"virt-handler","level":"error","msg":"Can't patch node my-node","pos":"heartbeat.go:96","reason":"the server has received too many API requests and has asked us to try again later","timestamp":"2023-11-06T11:11:41.099883Z","uid":"132c50c2-8d82-4e49-8857-dc737adcd6cc"}</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virthandlerresterrorshigh" renderas="sect4">Mitigation</bridgehead>
<simpara>Delete the pod to force a restart:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n $NAMESPACE &lt;virt-handler&gt;</programlisting>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtOperatorDown">
<title>VirtOperatorDown</title>
<bridgehead xml:id="meaning-virtoperatordown" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when no <literal>virt-operator</literal> pod in the <literal>Running</literal> state has
been detected for 10 minutes.</simpara>
<simpara>The <literal>virt-operator</literal> is the first Operator to start in a cluster. Its primary
responsibilities include the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Installing, live-updating, and live-upgrading a cluster</simpara>
</listitem>
<listitem>
<simpara>Monitoring the life cycle of top-level controllers, such as <literal>virt-controller</literal>,
<literal>virt-handler</literal>, <literal>virt-launcher</literal>, and managing their reconciliation</simpara>
</listitem>
<listitem>
<simpara>Certain cluster-wide tasks, such as certificate rotation and infrastructure
management</simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>virt-operator</literal> deployment has a default replica of 2 pods.</simpara>
<bridgehead xml:id="impact-virtoperatordown" renderas="sect4">Impact</bridgehead>
<simpara>This alert indicates a failure at the level of the cluster. Critical cluster-wide
management functionalities, such as certification rotation, upgrade, and
reconciliation of controllers, might not be available.</simpara>
<simpara>The <literal>virt-operator</literal> is not directly responsible for virtual machines (VMs)
in the cluster. Therefore, its temporary unavailability does not significantly
affect VM workloads.</simpara>
<bridgehead xml:id="diagnosis-virtoperatordown" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-operator</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get deploy virt-operator -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-operator</literal> deployment:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe deploy virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check for node issues, such as a <literal>NotReady</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtoperatordown" renderas="sect4">Mitigation</bridgehead>
<simpara>Based on the information obtained during the diagnosis procedure, try to find
the root cause and resolve the issue.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtOperatorRESTErrorsBurst">
<title>VirtOperatorRESTErrorsBurst</title>
<bridgehead xml:id="meaning-virtoperatorresterrorsburst" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when more than 80% of the REST calls in the <literal>virt-operator</literal>
pods failed in the last 5 minutes. This usually indicates that the <literal>virt-operator</literal>
pods cannot connect to the API server.</simpara>
<simpara>This error is frequently caused by one of the following problems:</simpara>
<itemizedlist>
<listitem>
<simpara>The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.</simpara>
</listitem>
<listitem>
<simpara>The <literal>virt-operator</literal> pod cannot reach the API server. This is commonly caused
by DNS issues on the node and networking connectivity issues.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-virtoperatorresterrorsburst" renderas="sect4">Impact</bridgehead>
<simpara>Cluster-level actions, such as upgrading and controller reconciliation, might
not be available.</simpara>
<simpara>However, workloads such as virtual machines (VMs) and VM instances
(VMIs) are not likely to be affected.</simpara>
<bridgehead xml:id="diagnosis-virtoperatorresterrorsburst" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-operator</literal> logs for error messages when connecting to the
API server:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs &lt;virt-operator&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-operator</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pod &lt;virt-operator&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtoperatorresterrorsburst" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>If the <literal>virt-operator</literal> pod cannot connect to the API server, delete the pod
to force a restart:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n $NAMESPACE &lt;virt-operator&gt;</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtOperatorRESTErrorsHigh">
<title>VirtOperatorRESTErrorsHigh</title>
<bridgehead xml:id="meaning-virtoperatorresterrorshigh" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when more than 5% of the REST calls in <literal>virt-operator</literal> pods
failed in the last 60 minutes. This usually indicates the <literal>virt-operator</literal> pods
cannot connect to the API server.</simpara>
<simpara>This error is frequently caused by one of the following problems:</simpara>
<itemizedlist>
<listitem>
<simpara>The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.</simpara>
</listitem>
<listitem>
<simpara>The <literal>virt-operator</literal> pod cannot reach the API server. This is commonly caused
by DNS issues on the node and networking connectivity issues.</simpara>
</listitem>
</itemizedlist>
<bridgehead xml:id="impact-virtoperatorresterrorshigh" renderas="sect4">Impact</bridgehead>
<simpara>Cluster-level actions, such as upgrading and controller reconciliation, might
be delayed.</simpara>
<simpara>However, workloads such as virtual machines (VMs) and VM instances
(VMIs) are not likely to be affected.</simpara>
<bridgehead xml:id="diagnosis-virtoperatorresterrorshigh" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Set the <literal>NAMESPACE</literal> environment variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"</programlisting>
</listitem>
<listitem>
<simpara>Check the status of the <literal>virt-operator</literal> pods:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator</programlisting>
</listitem>
<listitem>
<simpara>Check the <literal>virt-operator</literal> logs for error messages when connecting to the
API server:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE logs &lt;virt-operator&gt;</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the <literal>virt-operator</literal> pod:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n $NAMESPACE describe pod &lt;virt-operator&gt;</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtoperatorresterrorshigh" renderas="sect4">Mitigation</bridgehead>
<itemizedlist>
<listitem>
<simpara>If the <literal>virt-operator</literal> pod cannot connect to the API server, delete the pod
to force a restart:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete -n $NAMESPACE &lt;virt-operator&gt;</programlisting>
</listitem>
</itemizedlist>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VirtualMachineCRCErrors">
<title>VirtualMachineCRCErrors</title>
<bridgehead xml:id="meaning-virtualmachinecrcerrors" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the storage class is incorrectly configured.
A system-wide, shared dummy page causes CRC errors when data is
written and read across different processes or threads.</simpara>
<bridgehead xml:id="impact-virtualmachinecrcerrors" renderas="sect4">Impact</bridgehead>
<simpara>A large number of CRC errors might cause the cluster to display
severe performance degradation.</simpara>
<bridgehead xml:id="diagnosis-virtualmachinecrcerrors" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Navigate to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Obtain a list of virtual machines with incorrectly configured storage classes
by running the following PromQL query:</simpara>
<programlisting language="text" linenumbering="unnumbered">kubevirt_ssp_vm_rbd_volume{rxbounce_enabled="false", volume_mode="Block"} == 1</programlisting>
<simpara>The output displays a list of virtual machines that use a storage
class without <literal>rxbounce_enabled</literal>.</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="text" linenumbering="unnumbered">kubevirt_ssp_vm_rbd_volume{name="testvmi-gwgdqp22k7", namespace="test_ns", pv_name="testvmi-gwgdqp22k7", rxbounce_enabled="false", volume_mode="Block"} 1</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Obtain the storage class name by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pv &lt;pv_name&gt; -o=jsonpath='{.spec.storageClassName}'</programlisting>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-virtualmachinecrcerrors" renderas="sect4">Mitigation</bridgehead>
<simpara>Add the <literal>krbd:rxbounce</literal> map option to the storage class configuration to use
a bounce buffer when receiving data:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: vm-sc
parameters:
  # ...
  mounter: rbd
  mapOptions: "krbd:rxbounce"
provisioner: openshift-storage.rbd.csi.ceph.com
# ...</programlisting>
<simpara>The <literal>krbd:rxbounce</literal> option creates a bounce buffer to receive data. The default
behavior is for the destination buffer to receive data directly. A bounce buffer
is required if the stability of the destination buffer cannot be guaranteed.</simpara>
<simpara>See <link xlink:href="https://access.redhat.com/articles/6978371">Optimizing ODF PersistentVolumes for Windows VMs</link>
for details.</simpara>
<simpara>If you cannot resolve the issue, log in to the
<link xlink:href="https://access.redhat.com">Customer Portal</link> and open a support case,
attaching the artifacts gathered during the diagnosis procedure.</simpara>
</section>
<section xml:id="virt-runbook-VMCannotBeEvicted">
<title>VMCannotBeEvicted</title>
<bridgehead xml:id="meaning-vmcannotbeevicted" renderas="sect4">Meaning</bridgehead>
<simpara>This alert fires when the eviction strategy of a virtual machine (VM) is set
to <literal>LiveMigration</literal> but the VM is not migratable.</simpara>
<bridgehead xml:id="impact-vmcannotbeevicted" renderas="sect4">Impact</bridgehead>
<simpara>Non-migratable VMs prevent node eviction. This condition affects operations
such as node drain and updates.</simpara>
<bridgehead xml:id="diagnosis-vmcannotbeevicted" renderas="sect4">Diagnosis</bridgehead>
<orderedlist numeration="arabic">
<listitem>
<simpara>Check the VMI configuration to determine whether the value of
<literal>evictionStrategy</literal> is <literal>LiveMigrate</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmis -o yaml</programlisting>
</listitem>
<listitem>
<simpara>Check for a <literal>False</literal> status in the <literal>LIVE-MIGRATABLE</literal> column to identify VMIs
that are not migratable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmis -o wide</programlisting>
</listitem>
<listitem>
<simpara>Obtain the details of the VMI and check <literal>spec.conditions</literal> to identify the
issue:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmi &lt;vmi&gt; -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: null
    message: cannot migrate VMI which does not use masquerade to connect
    to the pod network
    reason: InterfaceNotLiveMigratable
    status: "False"
    type: LiveMigratable</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<bridgehead xml:id="mitigation-vmcannotbeevicted" renderas="sect4">Mitigation</bridgehead>
<simpara>Set the <literal>evictionStrategy</literal> of the VMI to <literal>shutdown</literal> or resolve the issue that
prevents the VMI from migrating.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="_support">
<title>Support</title>
<section xml:id="virt-support-overview">
<title>Support overview</title>

<simpara>You can collect data about your environment, monitor the health of your cluster and virtual machines (VMs), and troubleshoot OpenShift Virtualization resources with the following tools.</simpara>
<section xml:id="virt-web-console_virt-support-overview">
<title>Web console</title>
<simpara>The OpenShift Container Platform web console displays resource usage, alerts, events, and trends for your cluster and for OpenShift Virtualization components and resources.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Web console pages for monitoring and troubleshooting</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Page</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Overview</emphasis> page</simpara></entry>
<entry align="left" valign="top"><simpara>Cluster details, status, alerts, inventory, and resource usage</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Virtualization</emphasis> &#8594; <link linkend="overview-overview_virt-web-console-overview"><emphasis role="strong">Overview</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>OpenShift Virtualization resources, usage, alerts, and status</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Virtualization</emphasis> &#8594; <link linkend="overview-top-consumers_virt-web-console-overview"><emphasis role="strong">Top consumers</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Top consumers of CPU, memory, and storage</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">Virtualization</emphasis> &#8594; <link linkend="overview-migrations_virt-web-console-overview"><emphasis role="strong">Migrations</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>Progress of live migrations</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachines</emphasis> &#8594; <emphasis role="strong">VirtualMachine</emphasis> &#8594; <emphasis role="strong">VirtualMachine details</emphasis> &#8594; <link linkend="virtualmachine-details-metrics_virt-web-console-overview"><emphasis role="strong">Metrics</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>VM resource usage, storage, network, and migration</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachines</emphasis> &#8594; <emphasis role="strong">VirtualMachine</emphasis> &#8594; <emphasis role="strong">VirtualMachine details</emphasis> &#8594; <link linkend="virtualmachine-details-events_virt-web-console-overview"><emphasis role="strong">Events</emphasis> tab</link></simpara></entry>
<entry align="left" valign="top"><simpara>List of VM events</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><emphasis role="strong">VirtualMachines</emphasis> &#8594; <emphasis role="strong">VirtualMachine</emphasis> &#8594; <emphasis role="strong">VirtualMachine details</emphasis> &#8594; <link linkend="virtualmachine-details-diagnostics_virt-web-console-overview"><emphasis role="strong">Diagnostics tab</emphasis></link></simpara></entry>
<entry align="left" valign="top"><simpara>VM status conditions and volume snapshot status</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
</section>
<section xml:id="collecting-data-for-red-hat-support_virt-support-overview">
<title>Collecting data for Red Hat Support</title>
<simpara>When you submit a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#support-submitting-a-case_getting-support">support case</link> to Red Hat Support, it is helpful to provide debugging information. You can gather debugging information by performing the following steps:</simpara>
<variablelist>
<varlistentry>
<term><link linkend="virt-collecting-data-about-your-environment_virt-collecting-virt-data">Collecting data about your environment</link></term>
<listitem>
<simpara>Configure Prometheus and Alertmanager and collect <literal>must-gather</literal> data for OpenShift Container Platform and OpenShift Virtualization.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-collecting-data-about-vms_virt-collecting-virt-data">Collecting data about VMs</link></term>
<listitem>
<simpara>Collect <literal>must-gather</literal> data and memory dumps from VMs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-using-virt-must-gather_virt-collecting-virt-data"><literal>must-gather</literal> tool for OpenShift Virtualization</link></term>
<listitem>
<simpara>Configure and use the <literal>must-gather</literal> tool.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="troubleshooting_virt-support-overview">
<title>Troubleshooting</title>
<simpara>Troubleshoot OpenShift Virtualization components and VMs and resolve issues that trigger alerts in the web console.</simpara>
<variablelist>
<varlistentry>
<term><link linkend="events_virt-troubleshooting">Events</link></term>
<listitem>
<simpara>View important life-cycle information for VMs, namespaces, and resources.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="virt-logs_virt-troubleshooting">Logs</link></term>
<listitem>
<simpara>View and configure logs for OpenShift Virtualization components and VMs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><link linkend="troubleshooting-data-volumes_virt-troubleshooting">Troubleshooting data volumes</link></term>
<listitem>
<simpara>Troubleshoot data volumes by analyzing conditions and events.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="virt-collecting-virt-data">
<title>Collecting data for Red Hat Support</title>

<simpara>When you submit a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#support-submitting-a-case_getting-support">support case</link> to Red Hat Support, it is helpful to provide debugging information for OpenShift Container Platform and OpenShift Virtualization by using the following tools:</simpara>
<variablelist>
<varlistentry>
<term>must-gather tool</term>
<listitem>
<simpara>The <literal>must-gather</literal> tool collects diagnostic information, including resource definitions and service logs.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Prometheus</term>
<listitem>
<simpara>Prometheus is a time-series database and a rule evaluation engine for metrics. Prometheus sends alerts to Alertmanager for processing.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Alertmanager</term>
<listitem>
<simpara>The Alertmanager service handles alerts received from Prometheus. The Alertmanager is also responsible for sending the alerts to external notification systems.</simpara>
</listitem>
</varlistentry>
</variablelist>
<simpara>For information about the OpenShift Container Platform monitoring stack, see <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#about-openshift-monitoring">About OpenShift Container Platform monitoring</link>.</simpara>
<section xml:id="virt-collecting-data-about-your-environment_virt-collecting-virt-data">
<title>Collecting data about your environment</title>
<simpara>Collecting data about your environment minimizes the time required to analyze and determine the root cause.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#modifying-retention-time-for-prometheus-metrics-data_configuring-the-monitoring-stack">Set the retention time for Prometheus metrics data</link> to a minimum of seven days.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#sending-notifications-to-external-systems_managing-alerts">Configure the Alertmanager to capture relevant alerts and to send alert notifications to a dedicated mailbox</link> so that they can be viewed and persisted outside the cluster.</simpara>
</listitem>
<listitem>
<simpara>Record the exact number of affected nodes and virtual machines.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/support/#support_gathering_data_gathering-cluster-data">Collect must-gather data for the cluster</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.15/html-single/troubleshooting_openshift_data_foundation/index#downloading-log-files-and-diagnostic-information_rhodf">Collect must-gather data for Red Hat OpenShift Data Foundation</link>, if necessary.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-using-virt-must-gather_virt-collecting-virt-data">Collect must-gather data for OpenShift Virtualization</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/monitoring/#querying-metrics-for-all-projects-as-an-administrator_managing-metrics">Collect Prometheus metrics for the cluster</link>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-collecting-data-about-vms_virt-collecting-virt-data">
<title>Collecting data about virtual machines</title>
<simpara>Collecting data about malfunctioning virtual machines (VMs) minimizes the time required to analyze and determine the root cause.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Linux VMs: <link linkend="virt-installing-qemu-guest-agent-on-linux-vm_virt-installing-qemu-guest-agent">Install the latest QEMU guest agent</link>.</simpara>
</listitem>
<listitem>
<simpara>Windows VMs:</simpara>
<itemizedlist>
<listitem>
<simpara>Record the Windows patch update details.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/6957701">Install the latest VirtIO drivers</link>.</simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-installing-virtio-drivers-existing-windows_virt-installing-qemu-guest-agent">Install the latest QEMU guest agent</link>.</simpara>
</listitem>
<listitem>
<simpara>If Remote Desktop Protocol (RDP) is enabled, connect by using the <link linkend="desktop-viewer_virt-accessing-vm-consoles">desktop viewer</link> to determine whether there is a problem with the connection software.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara><link linkend="virt-must-gather-options_virt-collecting-virt-data">Collect must-gather data for the VMs</link> using the <literal>/usr/bin/gather</literal> script.</simpara>
</listitem>
<listitem>
<simpara>Collect screenshots of VMs that have crashed <emphasis>before</emphasis> you restart them.</simpara>
</listitem>
<listitem>
<simpara><link linkend="vm-memory-dump-commands_virt-using-the-cli-tools">Collect memory dumps from VMs</link> <emphasis>before</emphasis> remediation attempts.</simpara>
</listitem>
<listitem>
<simpara>Record factors that the malfunctioning VMs have in common. For example, the VMs have the same host or network.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-using-virt-must-gather_virt-collecting-virt-data">
<title>Using the must-gather tool for OpenShift Virtualization</title>
<simpara>You can collect data about OpenShift Virtualization resources by running the <literal>must-gather</literal> command with the OpenShift Virtualization image.</simpara>
<simpara>The default data collection includes information about the following resources:</simpara>
<itemizedlist>
<listitem>
<simpara>OpenShift Virtualization Operator namespaces, including child objects</simpara>
</listitem>
<listitem>
<simpara>OpenShift Virtualization custom resource definitions</simpara>
</listitem>
<listitem>
<simpara>Namespaces that contain virtual machines</simpara>
</listitem>
<listitem>
<simpara>Basic virtual machine definitions</simpara>
</listitem>
</itemizedlist>
<simpara>Instance types information is not currently collected by default; you can, however, run a command to optionally collect it.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Run the following command to collect data about OpenShift Virtualization:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.15.0 \
  -- /usr/bin/gather</programlisting>
</listitem>
</itemizedlist>
<section xml:id="virt-must-gather-options_virt-collecting-virt-data">
<title>must-gather tool options</title>
<simpara>You can specify a combination of scripts and environment variables for the following options:</simpara>
<itemizedlist>
<listitem>
<simpara>Collecting detailed virtual machine (VM) information from a namespace</simpara>
</listitem>
<listitem>
<simpara>Collecting detailed information about specified VMs</simpara>
</listitem>
<listitem>
<simpara>Collecting image, image-stream, and image-stream-tags information</simpara>
</listitem>
<listitem>
<simpara>Limiting the maximum number of parallel processes used by the <literal>must-gather</literal> tool</simpara>
</listitem>
</itemizedlist>
<section xml:id="parameters">
<title>Parameters</title>
<formalpara>
<title>Environment variables</title>
<para>You can specify environment variables for a compatible script.</para>
</formalpara>
<variablelist>
<varlistentry>
<term><literal>NS=&lt;namespace_name&gt;</literal></term>
<listitem>
<simpara>Collect virtual machine information, including <literal>virt-launcher</literal> pod details, from the namespace that you specify. The <literal>VirtualMachine</literal> and <literal>VirtualMachineInstance</literal> CR data is collected for all namespaces.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>VM=&lt;vm_name&gt;</literal></term>
<listitem>
<simpara>Collect details about a particular virtual machine. To use this option, you must also specify a namespace by using the <literal>NS</literal> environment variable.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>PROS=&lt;number_of_processes&gt;</literal></term>
<listitem>
<simpara>Modify the maximum number of parallel processes that the <literal>must-gather</literal> tool uses. The default value is <literal>5</literal>.</simpara>
<important>
<simpara>Using too many parallel processes can cause performance issues. Increasing the maximum number of parallel processes is not recommended.</simpara>
</important>
</listitem>
</varlistentry>
</variablelist>
<formalpara>
<title>Scripts</title>
<para>Each script is compatible only with certain environment variable combinations.</para>
</formalpara>
<variablelist>
<varlistentry>
<term><literal>/usr/bin/gather</literal></term>
<listitem>
<simpara>Use the default <literal>must-gather</literal> script, which collects cluster data from all namespaces and includes only basic VM information. This script is compatible only with the <literal>PROS</literal> variable.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>/usr/bin/gather --vms_details</literal></term>
<listitem>
<simpara>Collect VM log files, VM definitions, control-plane logs, and namespaces that belong to OpenShift Virtualization resources. Specifying namespaces includes their child objects. If you use this parameter without specifying a namespace or VM, the <literal>must-gather</literal> tool collects this data for all VMs in the cluster. This script is compatible with all environment variables, but you must specify a namespace if you use the <literal>VM</literal> variable.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>/usr/bin/gather --images</literal></term>
<listitem>
<simpara>Collect image, image-stream, and image-stream-tags custom resource information. This script is compatible only with the <literal>PROS</literal> variable.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term><literal>/usr/bin/gather --instancetypes</literal></term>
<listitem>
<simpara>Collect instance types information. This information is not currently collected by default; you can, however, optionally collect it.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="usage-and-examples_virt-collecting-virt-data">
<title>Usage and examples</title>
<simpara>Environment variables are optional. You can run a script by itself or with one or more compatible environment variables.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Compatible parameters</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="50*"/>
<colspec colname="col_2" colwidth="50*"/>
<thead>
<row>
<entry align="left" valign="top">Script</entry>
<entry align="left" valign="top">Compatible environment variable</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>/usr/bin/gather</literal></simpara></entry>
<entry align="left" valign="top"><simpara>* <literal>PROS=&lt;number_of_processes&gt;</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>/usr/bin/gather --vms_details</literal></simpara></entry>
<entry align="left" valign="top"><simpara>* For a namespace: <literal>NS=&lt;namespace_name&gt;</literal></simpara><simpara>* For a VM: <literal>VM=&lt;vm_name&gt; NS=&lt;namespace_name&gt;</literal></simpara><simpara>* <literal>PROS=&lt;number_of_processes&gt;</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>/usr/bin/gather --images</literal></simpara></entry>
<entry align="left" valign="top"><simpara>* <literal>PROS=&lt;number_of_processes&gt;</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Syntax</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.15.0 \
  -- &lt;environment_variable_1&gt; &lt;environment_variable_2&gt; &lt;script_name&gt;</programlisting>
</para>
</formalpara>
<formalpara>
<title>Default data collection parallel processes</title>
<para>By default, five processes run in parallel.</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.15.0 \
  -- PROS=5 /usr/bin/gather <co xml:id="CO150-1"/></programlisting>
<calloutlist>
<callout arearefs="CO150-1">
<para>You can modify the number of parallel processes by changing the default.</para>
</callout>
</calloutlist>
<formalpara>
<title>Detailed VM information</title>
<para>The following command collects detailed VM information for the <literal>my-vm</literal> VM in the <literal>mynamespace</literal> namespace:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.15.0 \
  -- NS=mynamespace VM=my-vm /usr/bin/gather --vms_details <co xml:id="CO151-1"/></programlisting>
<calloutlist>
<callout arearefs="CO151-1">
<para>The <literal>NS</literal> environment variable is mandatory if you use the <literal>VM</literal> environment variable.</para>
</callout>
</calloutlist>
<formalpara>
<title>Image, image-stream, and image-stream-tags information</title>
<para>The following command collects image, image-stream, and image-stream-tags information from the cluster:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.15.0 \
  /usr/bin/gather --images</programlisting>
<formalpara>
<title>Instance types information</title>
<para>The following command collects instance types information from the cluster:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather \
  --image=registry.redhat.io/container-native-virtualization/cnv-must-gather-rhel9:v4.15.0 \
  /usr/bin/gather --instancetypes</programlisting>
</section>
</section>
</section>
</section>
<section xml:id="virt-troubleshooting">
<title>Troubleshooting</title>
<simpara>OpenShift Virtualization provides tools and logs for troubleshooting virtual machines and virtualization components.</simpara>

<simpara>You can troubleshoot OpenShift Virtualization components by using the <link linkend="virt-web-console_virt-support-overview">tools provided in the web console</link> or by using the <literal>oc</literal> CLI tool.</simpara>
<section xml:id="events_virt-troubleshooting">
<title>Events</title>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-containers-events">OpenShift Container Platform events</link> are records of important life-cycle information and are useful for monitoring and troubleshooting virtual machine, namespace, and resource issues.</simpara>
<itemizedlist>
<listitem>
<simpara>VM events: Navigate to the <link linkend="virtualmachine-details-events_virt-web-console-overview"><emphasis role="strong">Events</emphasis> tab</link> of the <emphasis role="strong">VirtualMachine details</emphasis> page in the web console.</simpara>
<variablelist>
<varlistentry>
<term>Namespace events</term>
<listitem>
<simpara>You can view namespace events by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get events -n &lt;namespace&gt;</programlisting>
<simpara>See the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/nodes/#nodes-containers-events-list_nodes-containers-events">list of events</link> for details about specific events.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Resource events</term>
<listitem>
<simpara>You can view resource events by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe &lt;resource&gt; &lt;resource_name&gt;</programlisting>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</section>
<section xml:id="virt-logs_virt-troubleshooting">
<title>Logs</title>
<simpara>You can review the following logs for troubleshooting:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="viewing-virt-component-pods_virt-troubleshooting">Virtual machine</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-viewing-logs-cli_virt-troubleshooting">OpenShift Virtualization pod</link></simpara>
</listitem>
<listitem>
<simpara><link linkend="virt-viewing-logs-loki_virt-troubleshooting">Aggregated OpenShift Virtualization logs</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="virt-viewing-virtual-machine-logs-web_virt-troubleshooting">
<title>Viewing virtual machine logs with the web console</title>
<simpara>You can view virtual machine logs with the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select a virtual machine to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>On the <emphasis role="strong">Details</emphasis> tab, click the pod name to open the <emphasis role="strong">Pod details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Logs</emphasis> tab to view the logs.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="viewing-virt-component-pods_virt-troubleshooting">
<title>Viewing OpenShift Virtualization pod logs</title>
<simpara>You can view logs for OpenShift Virtualization pods by using the <literal>oc</literal> CLI tool.</simpara>
<simpara>You can configure the verbosity level of the logs by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<section xml:id="virt-viewing-logs-cli_virt-troubleshooting">
<title>Viewing OpenShift Virtualization pod logs with the CLI</title>
<simpara>You can view logs for the OpenShift Virtualization pods by using the <literal>oc</literal> CLI tool.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>View a list of pods in the OpenShift Virtualization namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-cnv</programlisting>
<example>
<title>Example output</title>
<programlisting language="terminal" linenumbering="unnumbered">NAME                               READY   STATUS    RESTARTS   AGE
disks-images-provider-7gqbc        1/1     Running   0          32m
disks-images-provider-vg4kx        1/1     Running   0          32m
virt-api-57fcc4497b-7qfmc          1/1     Running   0          31m
virt-api-57fcc4497b-tx9nc          1/1     Running   0          31m
virt-controller-76c784655f-7fp6m   1/1     Running   0          30m
virt-controller-76c784655f-f4pbd   1/1     Running   0          30m
virt-handler-2m86x                 1/1     Running   0          30m
virt-handler-9qs6z                 1/1     Running   0          30m
virt-operator-7ccfdbf65f-q5snk     1/1     Running   0          32m
virt-operator-7ccfdbf65f-vllz8     1/1     Running   0          32m</programlisting>
</example>
</listitem>
<listitem>
<simpara>View the pod log by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs -n openshift-cnv &lt;pod_name&gt;</programlisting>
<note>
<simpara>If a pod fails to start, you can use the <literal>--previous</literal> option to view logs from the last attempt.</simpara>
<simpara>To monitor log output in real time, use the <literal>-f</literal> option.</simpara>
</note>
<example>
<title>Example output</title>
<programlisting language="terminal" linenumbering="unnumbered">{"component":"virt-handler","level":"info","msg":"set verbosity to 2","pos":"virt-handler.go:453","timestamp":"2022-04-17T08:58:37.373695Z"}
{"component":"virt-handler","level":"info","msg":"set verbosity to 2","pos":"virt-handler.go:453","timestamp":"2022-04-17T08:58:37.373726Z"}
{"component":"virt-handler","level":"info","msg":"setting rate limiter to 5 QPS and 10 Burst","pos":"virt-handler.go:462","timestamp":"2022-04-17T08:58:37.373782Z"}
{"component":"virt-handler","level":"info","msg":"CPU features of a minimum baseline CPU model: map[apic:true clflush:true cmov:true cx16:true cx8:true de:true fpu:true fxsr:true lahf_lm:true lm:true mca:true mce:true mmx:true msr:true mtrr:true nx:true pae:true pat:true pge:true pni:true pse:true pse36:true sep:true sse:true sse2:true sse4.1:true ssse3:true syscall:true tsc:true]","pos":"cpu_plugin.go:96","timestamp":"2022-04-17T08:58:37.390221Z"}
{"component":"virt-handler","level":"warning","msg":"host model mode is expected to contain only one model","pos":"cpu_plugin.go:103","timestamp":"2022-04-17T08:58:37.390263Z"}
{"component":"virt-handler","level":"info","msg":"node-labeller is running","pos":"node_labeller.go:94","timestamp":"2022-04-17T08:58:37.391011Z"}</programlisting>
</example>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-configuring-pod-log-verbosity_virt-troubleshooting">
<title>Configuring OpenShift Virtualization pod log verbosity</title>
<simpara>You can configure the verbosity level of OpenShift Virtualization pod logs by editing the <literal>HyperConverged</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>To set log verbosity for specific components, open the <literal>HyperConverged</literal> CR in your default text editor by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit hyperconverged kubevirt-hyperconverged -n openshift-cnv</programlisting>
</listitem>
<listitem>
<simpara>Set the log level for one or more components by editing the <literal>spec.logVerbosityConfig</literal> stanza. For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  logVerbosityConfig:
    kubevirt:
      virtAPI: 5 <co xml:id="CO152-1"/>
      virtController: 4
      virtHandler: 3
      virtLauncher: 2
      virtOperator: 6</programlisting>
<calloutlist>
<callout arearefs="CO152-1">
<para>The log verbosity value must be an integer in the range <literal>1â€“9</literal>, where a higher number indicates a more detailed log. In this example, the <literal>virtAPI</literal> component logs are exposed if their priority level is <literal>5</literal> or higher.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply your changes by saving and exiting the editor.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-common-error-messages_virt-troubleshooting">
<title>Common error messages</title>
<simpara>The following error messages might appear in OpenShift Virtualization logs:</simpara>
<variablelist>
<varlistentry>
<term><literal>ErrImagePull</literal> or <literal>ImagePullBackOff</literal></term>
<listitem>
<simpara>Indicates an incorrect deployment configuration or problems with the images that are referenced.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</section>
<section xml:id="virt-viewing-logs-loki_virt-troubleshooting">
<title>Viewing aggregated OpenShift Virtualization logs with the LokiStack</title>
<simpara>You can view aggregated logs for OpenShift Virtualization pods and containers by using the LokiStack in the web console.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You deployed the LokiStack.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Logs</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">application</emphasis>, for <literal>virt-launcher</literal> pod logs, or <emphasis role="strong">infrastructure</emphasis>, for OpenShift Virtualization control plane pods and containers, from the log type list.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Show Query</emphasis> to display the query field.</simpara>
</listitem>
<listitem>
<simpara>Enter the LogQL query in the query field and click <emphasis role="strong">Run Query</emphasis> to display the filtered logs.</simpara>
</listitem>
</orderedlist>
<section xml:id="virt-loki-log-queries_virt-troubleshooting">
<title>OpenShift Virtualization LogQL queries</title>
<simpara>You can view and filter aggregated logs for OpenShift Virtualization components by running Loki Query Language (LogQL) queries on the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Logs</emphasis> page in the web console.</simpara>
<simpara>The default log type is <emphasis>infrastructure</emphasis>. The <literal>virt-launcher</literal> log type is <emphasis>application</emphasis>.</simpara>
<simpara>Optional: You can include or exclude strings or regular expressions by using line filter expressions.</simpara>
<note>
<simpara>If the query matches a large number of logs, the query might time out.</simpara>
</note>
<table frame="all" rowsep="1" colsep="1">
<title>OpenShift Virtualization LogQL example queries</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="14.2857*"/>
<colspec colname="col_2" colwidth="85.7143*"/>
<thead>
<row>
<entry align="left" valign="top">Component</entry>
<entry align="left" valign="top">LogQL query</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>All</simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>cdi-apiserver</literal></simpara>
<simpara><literal>cdi-deployment</literal></simpara>
<simpara><literal>cdi-operator</literal></simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"
|kubernetes_labels_app_kubernetes_io_component="storage"</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>hco-operator</literal></simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"
|kubernetes_labels_app_kubernetes_io_component="deployment"</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kubemacpool</literal></simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"
|kubernetes_labels_app_kubernetes_io_component="network"</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-api</literal></simpara>
<simpara><literal>virt-controller</literal></simpara>
<simpara><literal>virt-handler</literal></simpara>
<simpara><literal>virt-operator</literal></simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"
|kubernetes_labels_app_kubernetes_io_component="compute"</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>ssp-operator</literal></simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"
|kubernetes_labels_app_kubernetes_io_component="schedule"</programlisting></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Container</simpara></entry>
<entry align="left" valign="top"><programlisting language="text" linenumbering="unnumbered">{log_type=~".+",kubernetes_container_name=~"&lt;container&gt;|&lt;container&gt;"} <co xml:id="CO153-1"/>
|json|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"</programlisting>
<calloutlist>
<callout arearefs="CO153-1">
<para>Specify one or more containers separated by a pipe (<literal>|</literal>).</para>
</callout>
</calloutlist></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>virt-launcher</literal></simpara></entry>
<entry align="left" valign="top"><simpara>You must select <emphasis role="strong">application</emphasis> from the log type list before running this query.</simpara>
<programlisting language="text" linenumbering="unnumbered">{log_type=~".+", kubernetes_container_name="compute"}|json
|!= "custom-ga-command" <co xml:id="CO154-1"/></programlisting>
<calloutlist>
<callout arearefs="CO154-1">
<para><literal>|!= "custom-ga-command"</literal> excludes libvirt logs that contain the string <literal>custom-ga-command</literal>. (<link xlink:href="https://bugzilla.redhat.com/show_bug.cgi?id=2177684"><emphasis role="strong">BZ#2177684</emphasis></link>)</para>
</callout>
</calloutlist></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>You can filter log lines to include or exclude strings or regular expressions by using line filter expressions.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Line filter expressions</title>
<tgroup cols="2">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="66.6667*"/>
<thead>
<row>
<entry align="left" valign="top">Line filter expression</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>|= "&lt;string&gt;"</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Log line contains string</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>!= "&lt;string&gt;"</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Log line does not contain string</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>|~ "&lt;regex&gt;"</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Log line contains regular expression</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>!~ "&lt;regex&gt;"</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Log line does not contain regular expression</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<formalpara>
<title>Example line filter expression</title>
<para>
<programlisting language="text" linenumbering="unnumbered">{log_type=~".+"}|json
|kubernetes_labels_app_kubernetes_io_part_of="hyperconverged-cluster"
|= "error" != "timeout"</programlisting>
</para>
</formalpara>
</section>
<section xml:id="additional-resources_virt-troubleshooting" role="_additional-resources">
<title>Additional resources for LokiStack and LogQL</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#about-log-storage">About log storage</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/logging/#cluster-logging-loki-deploy_installing-log-storage">Deploying the LokiStack</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://grafana.com/docs/loki/latest/logql/log_queries/">LogQL log queries</link> in the Grafana documentation</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="troubleshooting-data-volumes_virt-troubleshooting">
<title>Troubleshooting data volumes</title>
<simpara>You can check the <literal>Conditions</literal> and <literal>Events</literal> sections of the <literal>DataVolume</literal> object to analyze and resolve issues.</simpara>
<section xml:id="virt-about-dv-conditions-and-events.adoc_virt-troubleshooting">
<title>About data volume conditions and events</title>
<simpara>You can diagnose data volume issues by examining the output of the <literal>Conditions</literal> and <literal>Events</literal> sections
generated by the command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe dv &lt;DataVolume&gt;</programlisting>
<simpara>The <literal>Conditions</literal> section displays the following <literal>Types</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Bound</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Running</literal></simpara>
</listitem>
<listitem>
<simpara><literal>Ready</literal></simpara>
</listitem>
</itemizedlist>
<simpara>The <literal>Events</literal> section provides the following additional information:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Type</literal> of event</simpara>
</listitem>
<listitem>
<simpara><literal>Reason</literal> for logging</simpara>
</listitem>
<listitem>
<simpara><literal>Source</literal> of the event</simpara>
</listitem>
<listitem>
<simpara><literal>Message</literal> containing additional diagnostic information.</simpara>
</listitem>
</itemizedlist>
<simpara>The output from <literal>oc describe</literal> does not always contains <literal>Events</literal>.</simpara>
<simpara>An event is generated when the <literal>Status</literal>, <literal>Reason</literal>, or <literal>Message</literal> changes.
Both conditions and events react to changes in the state of the data volume.</simpara>
<simpara>For example, if you misspell the URL during an import operation, the import
generates a 404 message. That message change generates an event with a reason.
The output in the <literal>Conditions</literal> section is updated as well.</simpara>
</section>
<section xml:id="virt-analyzing-datavolume-conditions-and-events_virt-troubleshooting">
<title>Analyzing data volume conditions and events</title>
<simpara>By inspecting the <literal>Conditions</literal> and <literal>Events</literal> sections generated by the <literal>describe</literal>
command, you determine the state of the data volume
in relation to persistent volume claims (PVCs), and whether or
not an operation is actively running or completed. You might also receive messages
that offer specific details about the status of the data volume, and how
it came to be in its current state.</simpara>
<simpara>There are many different combinations of conditions. Each must be evaluated in its unique context.</simpara>
<simpara>Examples of various combinations follow.</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Bound</literal> - A successfully bound PVC displays in this example.</simpara>
<simpara>Note that the <literal>Type</literal> is <literal>Bound</literal>, so the <literal>Status</literal> is <literal>True</literal>.
If the PVC is not bound, the <literal>Status</literal> is <literal>False</literal>.</simpara>
<simpara>When the PVC is bound, an event is generated stating that the PVC is bound.
In this case, the <literal>Reason</literal> is <literal>Bound</literal> and <literal>Status</literal> is <literal>True</literal>.
The <literal>Message</literal> indicates which PVC owns the data volume.</simpara>
<simpara><literal>Message</literal>, in the <literal>Events</literal> section, provides further details including how
long the PVC has been bound (<literal>Age</literal>) and by what resource (<literal>From</literal>),
in this case <literal>datavolume-controller</literal>:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Status:
  Conditions:
    Last Heart Beat Time:  2020-07-15T03:58:24Z
    Last Transition Time:  2020-07-15T03:58:24Z
    Message:               PVC win10-rootdisk Bound
    Reason:                Bound
    Status:                True
    Type:                  Bound
...
  Events:
    Type     Reason     Age    From                   Message
    ----     ------     ----   ----                   -------
    Normal   Bound      24s    datavolume-controller  PVC example-dv Bound</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara><literal>Running</literal> - In this case, note that <literal>Type</literal> is <literal>Running</literal> and <literal>Status</literal> is <literal>False</literal>,
indicating that an event has occurred that caused an attempted
operation to fail, changing the Status from <literal>True</literal> to <literal>False</literal>.</simpara>
<simpara>However, note that <literal>Reason</literal> is <literal>Completed</literal> and the <literal>Message</literal> field indicates
<literal>Import Complete</literal>.</simpara>
<simpara>In the <literal>Events</literal> section, the <literal>Reason</literal> and <literal>Message</literal> contain additional
troubleshooting information about the failed operation. In this example,
the <literal>Message</literal> displays an inability to connect due to a <literal>404</literal>, listed in the
<literal>Events</literal> section&#8217;s first <literal>Warning</literal>.</simpara>
<simpara>From this information, you conclude that an import operation was running,
creating contention for other operations that are
attempting to access the data volume:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Status:
  Conditions:
    Last Heart Beat Time:  2020-07-15T04:31:39Z
    Last Transition Time:  2020-07-15T04:31:39Z
    Message:               Import Complete
    Reason:                Completed
    Status:                False
    Type:                  Running
...
  Events:
    Type     Reason       Age                From                   Message
    ----     ------       ----               ----                   -------
    Warning  Error        12s (x2 over 14s)  datavolume-controller  Unable to connect
    to http data source: expected status code 200, got 404. Status: 404 Not Found</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara><literal>Ready</literal> â€“ If <literal>Type</literal> is <literal>Ready</literal> and <literal>Status</literal> is <literal>True</literal>, then the data volume is ready
to be used, as in the following example. If the data volume is not ready to be
used, the <literal>Status</literal> is <literal>False</literal>:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Status:
  Conditions:
    Last Heart Beat Time: 2020-07-15T04:31:39Z
    Last Transition Time:  2020-07-15T04:31:39Z
    Status:                True
    Type:                  Ready</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</chapter>
<chapter xml:id="_backup-and-restore">
<title>Backup and restore</title>
<section xml:id="virt-backup-restore-snapshots">
<title>Backup and restore by using VM snapshots</title>

<simpara>You can back up and restore virtual machines (VMs) by using snapshots. Snapshots are supported by the following storage providers:</simpara>
<itemizedlist>
<listitem>
<simpara>Red Hat OpenShift Data Foundation</simpara>
</listitem>
<listitem>
<simpara>Any other cloud storage provider with the Container Storage Interface (CSI) driver that supports the Kubernetes Volume Snapshot API</simpara>
</listitem>
</itemizedlist>
<simpara>Online snapshots have a default time deadline of five minutes (<literal>5m</literal>) that can be changed, if needed.</simpara>
<important>
<simpara>Online snapshots are supported for virtual machines that have hot plugged virtual disks. However, hot plugged disks that are not in the virtual machine specification are not included in the snapshot.</simpara>
</important>
<simpara>To create snapshots of an online (Running state) VM with the highest integrity, install the QEMU guest agent if it is not included with your operating system. The QEMU guest agent is included with the default Red Hat templates.</simpara>
<simpara>The QEMU guest agent takes a consistent snapshot by attempting to quiesce the VM file system as much as possible, depending on the system workload. This ensures that in-flight I/O is written to the disk before the snapshot is taken. If the guest agent is not present, quiescing is not possible and a best-effort snapshot is taken. The conditions under which the snapshot was taken are reflected in the snapshot indications that are displayed in the web console or CLI.</simpara>
<section xml:id="virt-about-vm-snapshots_virt-backup-restore-snapshots">
<title>About snapshots</title>
<simpara>A <emphasis>snapshot</emphasis> represents the state and data of a virtual machine (VM) at a specific point in time. You can use a snapshot to restore an existing VM to a previous state (represented by
the snapshot) for backup and disaster recovery or to rapidly roll back to a previous development version.</simpara>
<simpara>A VM snapshot is created from a VM that is powered off (Stopped state) or powered on (Running state).</simpara>
<simpara>When taking a snapshot of a running VM, the controller checks that the QEMU guest agent is installed and running. If so, it freezes the VM file system before taking the snapshot, and thaws the file system after the snapshot is taken.</simpara>
<simpara>The snapshot stores a copy of each Container Storage Interface (CSI) volume attached to the VM and a copy of the VM specification and metadata. Snapshots cannot be changed after creation.</simpara>
<simpara>You can perform the following snapshot actions:</simpara>
<itemizedlist>
<listitem>
<simpara>Create a new snapshot</simpara>
</listitem>
<listitem>
<simpara>List all snapshots attached to a specific VM</simpara>
</listitem>
<listitem>
<simpara>Restore a VM from a snapshot</simpara>
</listitem>
<listitem>
<simpara>Delete an existing VM snapshot</simpara>
</listitem>
</itemizedlist>
<formalpara>
<title>VM snapshot controller and custom resources</title>
<para>The VM snapshot feature introduces three new API objects defined as custom resource definitions (CRDs) for managing snapshots:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><literal>VirtualMachineSnapshot</literal>: Represents a user request to create a snapshot. It contains information about the current state of the VM.</simpara>
</listitem>
<listitem>
<simpara><literal>VirtualMachineSnapshotContent</literal>: Represents a provisioned resource on the cluster (a snapshot). It is created by the VM snapshot controller and contains references to all resources required to restore the VM.</simpara>
</listitem>
<listitem>
<simpara><literal>VirtualMachineRestore</literal>: Represents a user request to restore a VM from a snapshot.</simpara>
</listitem>
</itemizedlist>
<simpara>The VM snapshot controller binds a <literal>VirtualMachineSnapshotContent</literal> object with the <literal>VirtualMachineSnapshot</literal> object for which it was created, with a one-to-one mapping.</simpara>
</section>
<section xml:id="creating-snapshots_virt-backup-restore-snapshots">
<title>Creating snapshots</title>
<simpara>You can create snapshots of virtual machines (VMs) by using the OpenShift Container Platform web console or the command line.</simpara>
<section xml:id="virt-creating-vm-snapshot-web_virt-backup-restore-snapshots">
<title>Creating a snapshot by using the web console</title>
<simpara>You can create a snapshot of a virtual machine (VM) by using the OpenShift Container Platform web console.</simpara>
<simpara>The VM snapshot includes disks that meet the following requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>Either a data volume or a persistent volume claim</simpara>
</listitem>
<listitem>
<simpara>Belong to a storage class that supports Container Storage Interface (CSI) volume snapshots</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>If the VM is running, click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Stop</emphasis> to power it down.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Snapshots</emphasis> tab and then click <emphasis role="strong">Take Snapshot</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Enter the snapshot name.</simpara>
</listitem>
<listitem>
<simpara>Expand <emphasis role="strong">Disks included in this Snapshot</emphasis> to see the storage volumes to be included in the snapshot.</simpara>
</listitem>
<listitem>
<simpara>If your VM has disks that cannot be included in the snapshot and you wish to proceed, select <emphasis role="strong">I am aware of this warning and wish to proceed</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Save</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-creating-vm-snapshot-cli_virt-backup-restore-snapshots">
<title>Creating a snapshot by using the command line</title>
<simpara>You can create a virtual machine (VM) snapshot for an offline or online VM by creating a <literal>VirtualMachineSnapshot</literal> object.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Ensure that the persistent volume claims (PVCs) are in a storage class that supports Container Storage Interface (CSI) volume snapshots.</simpara>
</listitem>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>Optional: Power down the VM for which you want to create a snapshot.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file to define a <literal>VirtualMachineSnapshot</literal> object that specifies the name of the new <literal>VirtualMachineSnapshot</literal> and the name of the source VM as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.kubevirt.io/v1beta1
kind: VirtualMachineSnapshot
metadata:
  name: &lt;snapshot_name&gt;
spec:
  source:
    apiGroup: kubevirt.io
    kind: VirtualMachine
    name: &lt;vm_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>VirtualMachineSnapshot</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;snapshot_name&gt;.yaml</programlisting>
<simpara>The snapshot controller creates a <literal>VirtualMachineSnapshotContent</literal> object, binds it to the <literal>VirtualMachineSnapshot</literal>, and updates the <literal>status</literal> and <literal>readyToUse</literal> fields of the <literal>VirtualMachineSnapshot</literal> object.</simpara>
</listitem>
<listitem>
<simpara>Optional: If you are taking an online snapshot, you can use the <literal>wait</literal> command and monitor the status of the snapshot:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc wait &lt;vm_name&gt; &lt;snapshot_name&gt; --for condition=Ready</programlisting>
</listitem>
<listitem>
<simpara>Verify the status of the snapshot:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>InProgress</literal> - The online snapshot operation is still in progress.</simpara>
</listitem>
<listitem>
<simpara><literal>Succeeded</literal> - The online snapshot operation completed successfully.</simpara>
</listitem>
<listitem>
<simpara><literal>Failed</literal> - The online snapshot operaton failed.</simpara>
<note>
<simpara>Online snapshots have a default time deadline of five minutes (<literal>5m</literal>). If the snapshot does not complete successfully in five minutes, the status is set to <literal>failed</literal>. Afterwards, the file system will be thawed and the VM unfrozen but the status remains <literal>failed</literal> until you delete the failed snapshot image.</simpara>
<simpara>To change the default time deadline, add the <literal>FailureDeadline</literal> attribute to the VM snapshot spec with the time designated in minutes (<literal>m</literal>) or in seconds (<literal>s</literal>) that you want to specify before the snapshot operation times out.</simpara>
<simpara>To set no deadline, you can specify <literal>0</literal>, though this is generally not recommended, as it can result in an unresponsive VM.</simpara>
<simpara>If you do not specify a unit of time such as <literal>m</literal> or <literal>s</literal>, the default is seconds (<literal>s</literal>).</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that the <literal>VirtualMachineSnapshot</literal> object is created and bound with <literal>VirtualMachineSnapshotContent</literal> and that the <literal>readyToUse</literal> flag is set to <literal>true</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe vmsnapshot &lt;snapshot_name&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.kubevirt.io/v1beta1
kind: VirtualMachineSnapshot
metadata:
  creationTimestamp: "2020-09-30T14:41:51Z"
  finalizers:
  - snapshot.kubevirt.io/vmsnapshot-protection
  generation: 5
  name: mysnap
  namespace: default
  resourceVersion: "3897"
  selfLink: /apis/snapshot.kubevirt.io/v1beta1/namespaces/default/virtualmachinesnapshots/my-vmsnapshot
  uid: 28eedf08-5d6a-42c1-969c-2eda58e2a78d
spec:
  source:
    apiGroup: kubevirt.io
    kind: VirtualMachine
    name: my-vm
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: "2020-09-30T14:42:03Z"
    reason: Operation complete
    status: "False" <co xml:id="CO155-1"/>
    type: Progressing
  - lastProbeTime: null
    lastTransitionTime: "2020-09-30T14:42:03Z"
    reason: Operation complete
    status: "True" <co xml:id="CO155-2"/>
    type: Ready
  creationTime: "2020-09-30T14:42:03Z"
  readyToUse: true <co xml:id="CO155-3"/>
  sourceUID: 355897f3-73a0-4ec4-83d3-3c2df9486f4f
  virtualMachineSnapshotContentName: vmsnapshot-content-28eedf08-5d6a-42c1-969c-2eda58e2a78d <co xml:id="CO155-4"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO155-1">
<para>The <literal>status</literal> field of the <literal>Progressing</literal> condition specifies if the snapshot is still being created.</para>
</callout>
<callout arearefs="CO155-2">
<para>The <literal>status</literal> field of the <literal>Ready</literal> condition specifies if the snapshot creation process is complete.</para>
</callout>
<callout arearefs="CO155-3">
<para>Specifies if the snapshot is ready to be used.</para>
</callout>
<callout arearefs="CO155-4">
<para>Specifies that the snapshot is bound to a <literal>VirtualMachineSnapshotContent</literal> object created by the snapshot controller.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Check the <literal>spec:volumeBackups</literal> property of the <literal>VirtualMachineSnapshotContent</literal> resource to verify that the expected PVCs are included in the snapshot.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="virt-verifying-online-snapshot-creation-with-snapshot-indications_virt-backup-restore-snapshots">
<title>Verifying online snapshots by using snapshot indications</title>
<simpara>Snapshot indications are contextual information about online virtual machine (VM) snapshot operations. Indications are not available for offline virtual machine (VM) snapshot operations. Indications are helpful in describing details about the online snapshot creation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have attempted to create an online VM snapshot.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Display the output from the snapshot indications by doing one of the following:</simpara>
<itemizedlist>
<listitem>
<simpara>For snapshots created by using the command line, view indicator output in the <literal>status</literal> stanza of the <literal>VirtualMachineSnapshot</literal> object YAML.</simpara>
</listitem>
<listitem>
<simpara>For snapshots created by using the web console, click <emphasis role="strong">VirtualMachineSnapshot</emphasis> &#8594; <emphasis role="strong">Status</emphasis> in the <emphasis role="strong">Snapshot details</emphasis> screen.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Verify the status of your online VM snapshot:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Online</literal> indicates that the VM was running during online snapshot creation.</simpara>
</listitem>
<listitem>
<simpara><literal>NoGuestAgent</literal> indicates that the QEMU guest agent was not running during online snapshot creation. The QEMU guest agent could not be used to freeze and thaw the file system, either because the QEMU guest agent was not installed or running or due to another error.</simpara>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="restoring-vms-from-snapshots_virt-backup-restore-snapshots">
<title>Restoring virtual machines from snapshots</title>
<simpara>You can restore virtual machines (VMs) from snapshots by using the OpenShift Container Platform web console or the command line.</simpara>
<section xml:id="virt-restoring-vm-from-snapshot-web_virt-backup-restore-snapshots">
<title>Restoring a VM from a snapshot by using the web console</title>
<simpara>You can restore a virtual machine (VM) to a previous configuration represented by a snapshot in the OpenShift Container Platform web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>If the VM is running, click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Stop</emphasis> to power it down.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Snapshots</emphasis> tab to view a list of snapshots associated with the VM.</simpara>
</listitem>
<listitem>
<simpara>Select a snapshot to open the <emphasis role="strong">Snapshot Details</emphasis> screen.</simpara>
</listitem>
<listitem>
<simpara>Click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> and select <emphasis role="strong">Restore VirtualMachineSnapshot</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Restore</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-restoring-vm-from-snapshot-cli_virt-backup-restore-snapshots">
<title>Restoring a VM from a snapshot by using the command line</title>
<simpara>You can restore an existing virtual machine (VM) to a previous configuration by using the command line. You can only restore from an offline VM snapshot.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Power down the VM you want to restore.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file to define a <literal>VirtualMachineRestore</literal> object that specifies the name of the VM you want to restore and the name of the snapshot to be used as the source as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.kubevirt.io/v1beta1
kind: VirtualMachineRestore
metadata:
  name: &lt;vm_restore&gt;
spec:
  target:
    apiGroup: kubevirt.io
    kind: VirtualMachine
    name: &lt;vm_name&gt;
  virtualMachineSnapshotName: &lt;snapshot_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>VirtualMachineRestore</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f &lt;vm_restore&gt;.yaml</programlisting>
<simpara>The snapshot controller updates the status fields of the <literal>VirtualMachineRestore</literal> object and replaces the existing VM configuration with the snapshot content.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the VM is restored to the previous state represented by the snapshot and that the <literal>complete</literal> flag is set to <literal>true</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmrestore &lt;vm_restore&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.kubevirt.io/v1beta1
kind: VirtualMachineRestore
metadata:
creationTimestamp: "2020-09-30T14:46:27Z"
generation: 5
name: my-vmrestore
namespace: default
ownerReferences:
- apiVersion: kubevirt.io/v1
  blockOwnerDeletion: true
  controller: true
  kind: VirtualMachine
  name: my-vm
  uid: 355897f3-73a0-4ec4-83d3-3c2df9486f4f
  resourceVersion: "5512"
  selfLink: /apis/snapshot.kubevirt.io/v1beta1/namespaces/default/virtualmachinerestores/my-vmrestore
  uid: 71c679a8-136e-46b0-b9b5-f57175a6a041
  spec:
    target:
      apiGroup: kubevirt.io
      kind: VirtualMachine
      name: my-vm
  virtualMachineSnapshotName: my-vmsnapshot
  status:
  complete: true <co xml:id="CO156-1"/>
  conditions:
  - lastProbeTime: null
  lastTransitionTime: "2020-09-30T14:46:28Z"
  reason: Operation complete
  status: "False" <co xml:id="CO156-2"/>
  type: Progressing
  - lastProbeTime: null
  lastTransitionTime: "2020-09-30T14:46:28Z"
  reason: Operation complete
  status: "True" <co xml:id="CO156-3"/>
  type: Ready
  deletedDataVolumes:
  - test-dv1
  restoreTime: "2020-09-30T14:46:28Z"
  restores:
  - dataVolumeName: restore-71c679a8-136e-46b0-b9b5-f57175a6a041-datavolumedisk1
  persistentVolumeClaim: restore-71c679a8-136e-46b0-b9b5-f57175a6a041-datavolumedisk1
  volumeName: datavolumedisk1
  volumeSnapshotName: vmsnapshot-28eedf08-5d6a-42c1-969c-2eda58e2a78d-volume-datavolumedisk1</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO156-1">
<para>Specifies if the process of restoring the VM to the state represented by the snapshot is complete.</para>
</callout>
<callout arearefs="CO156-2">
<para>The <literal>status</literal> field of the <literal>Progressing</literal> condition specifies if the VM is still being restored.</para>
</callout>
<callout arearefs="CO156-3">
<para>The <literal>status</literal> field of the <literal>Ready</literal> condition specifies if the VM restoration process is complete.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="deleting-snapshots_virt-backup-restore-snapshots">
<title>Deleting snapshots</title>
<simpara>You can delete snapshots of virtual machines (VMs) by using the OpenShift Container Platform web console or the command line.</simpara>
<section xml:id="virt-deleting-vm-snapshot-web_virt-backup-restore-snapshots">
<title>Deleting a snapshot by using the web console</title>
<simpara>You can delete an existing virtual machine (VM) snapshot by using the web console.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to <emphasis role="strong">Virtualization</emphasis> &#8594; <emphasis role="strong">VirtualMachines</emphasis> in the web console.</simpara>
</listitem>
<listitem>
<simpara>Select a VM to open the <emphasis role="strong">VirtualMachine details</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Click the <emphasis role="strong">Snapshots</emphasis> tab to view a list of snapshots associated with the VM.</simpara>
</listitem>
<listitem>
<simpara>Click the options menu <inlinemediaobject>
<imageobject>
<imagedata fileref="images/kebab.png"/>
</imageobject>
<textobject><phrase>kebab</phrase></textobject>
</inlinemediaobject> beside a snapshot and select <emphasis role="strong">Delete VirtualMachineSnapshot</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Delete</emphasis>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="virt-deleting-vm-snapshot-cli_virt-backup-restore-snapshots">
<title>Deleting a virtual machine snapshot in the CLI</title>
<simpara>You can delete an existing virtual machine (VM) snapshot by deleting the appropriate <literal>VirtualMachineSnapshot</literal> object.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Install the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Delete the <literal>VirtualMachineSnapshot</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vmsnapshot &lt;snapshot_name&gt;</programlisting>
<simpara>The snapshot controller deletes the <literal>VirtualMachineSnapshot</literal> along with the associated <literal>VirtualMachineSnapshotContent</literal> object.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the snapshot is deleted and no longer attached to this VM:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vmsnapshot</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_additional-resources" role="_additional-resources-snapshots">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-csi-snapshots">CSI Volume Snapshots</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-installing-configuring-oadp">
<title>Installing and configuring OADP</title>

<simpara>As a cluster administrator, you install the OpenShift API for Data Protection (OADP) by installing the OADP Operator. The Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<simpara>You create a default <literal>Secret</literal> for your backup storage provider and then you install the Data Protection Application.</simpara>
<section xml:id="oadp-installing-operator_virt-installing-configuring-oadp">
<title>Installing the OADP Operator</title>
<simpara>You install the OpenShift API for Data Protection (OADP) Operator on OpenShift Container Platform 4.14 by using Operator Lifecycle Manager (OLM).</simpara>
<simpara>The OADP Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Use the <emphasis role="strong">Filter by keyword</emphasis> field to find the <emphasis role="strong">OADP Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">OADP Operator</emphasis> and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis> to install the Operator in the <literal>openshift-adp</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> to verify the installation.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-about-backup-snapshot-locations_virt-installing-configuring-oadp">
<title>About backup and snapshot locations and their secrets</title>
<simpara>You specify backup and snapshot locations and their secrets in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<bridgehead xml:id="backup-locations_virt-installing-configuring-oadp" renderas="sect4">Backup locations</bridgehead>
<simpara>You specify AWS S3-compatible object storage, such as Multicloud Object Gateway or MinIO, as a backup location.</simpara>
<simpara>Velero backs up OpenShift Container Platform resources, Kubernetes objects, and internal images as an archive file on object storage.</simpara>
<bridgehead xml:id="snapshot-locations_virt-installing-configuring-oadp" renderas="sect4">Snapshot locations</bridgehead>
<simpara>If you use your cloud provider&#8217;s native snapshot API to back up persistent volumes, you must specify the cloud provider as the snapshot location.</simpara>
<simpara>If you use Container Storage Interface (CSI) snapshots, you do not need to specify a snapshot location because you will create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
<simpara>If you use File System Backup (FSB), you do not need to specify a snapshot location because FSB backs up the file system on object storage.</simpara>
<bridgehead xml:id="secrets_virt-installing-configuring-oadp" renderas="sect4">Secrets</bridgehead>
<simpara>If the backup and snapshot locations use the same credentials or if you do not require a snapshot location, you create a default <literal>Secret</literal>.</simpara>
<simpara>If the backup and snapshot locations use different credentials, you create two secret objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Custom <literal>Secret</literal> for the backup location, which you specify in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Default <literal>Secret</literal> for the snapshot location, which is not referenced in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The Data Protection Application requires a default <literal>Secret</literal>. Otherwise, the installation will fail.</simpara>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file.</simpara>
</important>
<section xml:id="oadp-creating-default-secret_virt-installing-configuring-oadp">
<title>Creating a default Secret</title>
<simpara>You create a default <literal>Secret</literal> if your backup and snapshot locations use the same credentials or if you do not require a snapshot location.</simpara>
<note>
<simpara>The <literal>DataProtectionApplication</literal> custom resource (CR) requires a default <literal>Secret</literal>.  Otherwise, the installation will fail. If the name of the backup location <literal>Secret</literal> is not specified, the default name is used.</simpara>
<simpara>If you do not want to use the backup location credentials during the installation, you can create a <literal>Secret</literal> with the default name by using an empty <literal>credentials-velero</literal> file.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your object storage and cloud storage, if any, must use the same credentials.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must create a <literal>credentials-velero</literal> file for the object storage in the appropriate format.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Secret</literal> with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
</itemizedlist>
<simpara>The <literal>Secret</literal> is referenced in the <literal>spec.backupLocations.credential</literal> block of the <literal>DataProtectionApplication</literal> CR when you install the Data Protection Application.</simpara>
</section>
</section>
<section xml:id="configuring-dpa-ocs">
<title>Configuring the Data Protection Application</title>
<simpara>You can configure the Data Protection Application by setting Velero resource allocations or enabling self-signed CA certificates.</simpara>
<section xml:id="oadp-setting-resource-limits-and-requests_virt-installing-configuring-oadp">
<title>Setting Velero CPU and memory resource allocations</title>
<simpara>You set the CPU and memory resource allocations for the <literal>Velero</literal> pod by editing the  <literal>DataProtectionApplication</literal> custom resource (CR) manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.podConfig.ResourceAllocations</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  configuration:
    velero:
      podConfig:
        nodeSelector: &lt;node selector&gt; <co xml:id="CO157-1"/>
        resourceAllocations: <co xml:id="CO157-2"/>
          limits:
            cpu: "1"
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 256Mi</programlisting>
<calloutlist>
<callout arearefs="CO157-1">
<para>Specify the node selector to be supplied to Velero podSpec.</para>
</callout>
<callout arearefs="CO157-2">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>Kopia is an option in OADP 1.3 and later releases. You can use Kopia for file system backups, and Kopia is your only option for Data Mover cases with the built-in Data Mover.</simpara>
<simpara>Kopia is more resource intensive than Restic, and you might need to adjust the CPU and memory requirements accordingly.</simpara>
</note>
</section>
<section xml:id="oadp-self-signed-certificate_virt-installing-configuring-oadp">
<title>Enabling self-signed CA certificates</title>
<simpara>You must enable a self-signed CA certificate for object storage by editing the <literal>DataProtectionApplication</literal> custom resource (CR) manifest to prevent a <literal>certificate signed by unknown authority</literal> error.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.backupLocations.velero.objectStorage.caCert</literal> parameter and <literal>spec.backupLocations.velero.config</literal> parameters of the <literal>DataProtectionApplication</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket&gt;
          prefix: &lt;prefix&gt;
          caCert: &lt;base64_encoded_cert_string&gt; <co xml:id="CO158-1"/>
        config:
          insecureSkipTLSVerify: "false" <co xml:id="CO158-2"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO158-1">
<para>Specify the Base64-encoded CA certificate string.</para>
</callout>
<callout arearefs="CO158-2">
<para>The <literal>insecureSkipTLSVerify</literal> configuration can be set to either <literal>"true"</literal> or <literal>"false"</literal>. If set to <literal>"true"</literal>, SSL/TLS security is disabled. If set to <literal>"false"</literal>, SSL/TLS security is enabled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="oadp-using-ca-certificates-with-velero-command-aliased-for-velero-deployment_virt-installing-configuring-oadp">
<title>Using CA certificates with the velero command aliased for Velero deployment</title>
<simpara>You might want to use the Velero CLI without installing it locally on your system by creating an alias for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To use an aliased Velero command, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
<listitem>
<simpara>Check that the alias is working by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero version
Client:
	Version: v1.12.1-OADP
	Git commit: -
Server:
	Version: v1.12.1-OADP</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To use a CA certificate with this command, you can add a certificate to the Velero deployment by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CA_CERT=$(oc -n openshift-adp get dataprotectionapplications.oadp.openshift.io &lt;dpa-name&gt; -o jsonpath='{.spec.backupLocations[0].velero.objectStorage.caCert}')

$ [[ -n $CA_CERT ]] &amp;&amp; echo "$CA_CERT" | base64 -d | oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "cat &gt; /tmp/your-cacert.txt" || echo "DPA BSL has no caCert"</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ velero -n openshift-adp describe backup &lt;backup-name&gt; --details --cacert /tmp/your-cacert.txt</programlisting>
</listitem>
<listitem>
<simpara>If the Velero pod restarts, the <literal>/tmp/your-cacert.txt</literal> file disappears, and you must re-create the <literal>/tmp/your-cacert.txt</literal> file by re-running the commands from the previous step.</simpara>
</listitem>
<listitem>
<simpara>You can check if the <literal>/tmp/your-cacert.txt</literal> file still exists, in the file location where you stored it, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "ls /tmp/your-cacert.txt"
/tmp/your-cacert.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<simpara>In a future release of OpenShift API for Data Protection (OADP), we plan to mount the certificate to the Velero pod so that this step is not required.</simpara>
</section>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-2-and-earlier_virt-installing-configuring-oadp">
<title>Installing the Data Protection Application 1.2 and earlier</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
<note>
<simpara>Velero creates a secret named <literal>velero-repo-credentials</literal> in the OADP namespace, which contains a default backup repository password.
You can update the secret with your own password encoded as base64 <emphasis role="strong">before</emphasis> you run your first backup targeted to the backup repository. The value of the key to update is <literal>Data[repository-password]</literal>.</simpara>
<simpara>After you create your DPA, the first time that you run a backup targeted to the backup repository, Velero creates a backup repository whose secret is <literal>velero-repo-credentials</literal>, which contains either the default password or the one you replaced it with.
If you update the secret password <emphasis role="strong">after</emphasis> the first backup, the new password will not match the password in <literal>velero-repo-credentials</literal>, and therefore, Velero will not be able to connect with the older backups.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
  configuration:
    velero:
      defaultPlugins:
        - kubevirt <co xml:id="CO159-1"/>
        - gcp <co xml:id="CO159-2"/>
        - csi <co xml:id="CO159-3"/>
        - openshift <co xml:id="CO159-4"/>
      resourceTimeout: 10m <co xml:id="CO159-5"/>
    restic:
      enable: true <co xml:id="CO159-6"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO159-7"/>
  backupLocations:
    - velero:
        provider: gcp <co xml:id="CO159-8"/>
        default: true
        credential:
          key: cloud
          name: &lt;default_secret&gt; <co xml:id="CO159-9"/>
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO159-10"/>
          prefix: &lt;prefix&gt; <co xml:id="CO159-11"/></programlisting>
<calloutlist>
<callout arearefs="CO159-1">
<para>The <literal>kubevirt</literal> plugin is mandatory for OpenShift Virtualization.</para>
</callout>
<callout arearefs="CO159-2">
<para>Specify the plugin for the backup provider, for example, <literal>gcp</literal>, if it exists.</para>
</callout>
<callout arearefs="CO159-3">
<para>The <literal>csi</literal> plugin is mandatory for backing up PVs with CSI snapshots. The <literal>csi</literal> plugin uses the <link xlink:href="https://velero.io/docs/main/csi/">Velero CSI beta snapshot APIs</link>. You do not need to configure a snapshot location.</para>
</callout>
<callout arearefs="CO159-4">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO159-5">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO159-6">
<para>Set this value to <literal>false</literal> if you want to disable the Restic installation. Restic deploys a daemon set, which means that Restic pods run on each working node. In OADP version 1.2 and later, you can configure Restic for backups by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR. In OADP version 1.1, add <literal>spec.defaultVolumesToRestic: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO159-7">
<para>Specify on which nodes Restic is available. By default, Restic runs on all nodes.</para>
</callout>
<callout arearefs="CO159-8">
<para>Specify the backup provider.</para>
</callout>
<callout arearefs="CO159-9">
<para>Specify the correct default name for the <literal>Secret</literal>, for example, <literal>cloud-credentials-gcp</literal>, if you use a default plugin for the backup provider. If specifying a custom name, then the custom name is used for the backup location. If you do not specify a <literal>Secret</literal> name, the default name is used.</para>
</callout>
<callout arearefs="CO159-10">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO159-11">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-2_virt-installing-configuring-oadp">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-3_virt-installing-configuring-oadp">
<title>Installing the Data Protection Application 1.3</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp <co xml:id="CO160-1"/>
spec:
  configuration:
    velero:
      defaultPlugins:
        - kubevirt <co xml:id="CO160-2"/>
        - gcp <co xml:id="CO160-3"/>
        - csi <co xml:id="CO160-4"/>
        - openshift <co xml:id="CO160-5"/>
      resourceTimeout: 10m <co xml:id="CO160-6"/>
    nodeAgent: <co xml:id="CO160-7"/>
      enable: true <co xml:id="CO160-8"/>
      uploaderType: kopia <co xml:id="CO160-9"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO160-10"/>
  backupLocations:
    - velero:
        provider: gcp <co xml:id="CO160-11"/>
        default: true
        credential:
          key: cloud
          name: &lt;default_secret&gt; <co xml:id="CO160-12"/>
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO160-13"/>
          prefix: &lt;prefix&gt; <co xml:id="CO160-14"/></programlisting>
<calloutlist>
<callout arearefs="CO160-1">
<para>The default namespace for OADP is <literal>openshift-adp</literal>. The namespace is a variable and is configurable.</para>
</callout>
<callout arearefs="CO160-2">
<para>The <literal>kubevirt</literal> plugin is mandatory for OpenShift Virtualization.</para>
</callout>
<callout arearefs="CO160-3">
<para>Specify the plugin for the backup provider, for example, <literal>gcp</literal>, if it exists.</para>
</callout>
<callout arearefs="CO160-4">
<para>The <literal>csi</literal> plugin is mandatory for backing up PVs with CSI snapshots. The <literal>csi</literal> plugin uses the <link xlink:href="https://velero.io/docs/main/csi/">Velero CSI beta snapshot APIs</link>. You do not need to configure a snapshot location.</para>
</callout>
<callout arearefs="CO160-5">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO160-6">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO160-7">
<para>The administrative agent that routes the administrative requests to servers.</para>
</callout>
<callout arearefs="CO160-8">
<para>Set this value to <literal>true</literal> if you want to enable <literal>nodeAgent</literal> and perform File System Backup.</para>
</callout>
<callout arearefs="CO160-9">
<para>Enter <literal>kopia</literal> or <literal>restic</literal> as your uploader. You cannot change the selection after the installation. For the Built-in DataMover you must use Kopia. The <literal>nodeAgent</literal> deploys a daemon set, which means that the <literal>nodeAgent</literal> pods run on each working node. You can configure File System Backup by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO160-10">
<para>Specify the nodes on which Kopia or Restic are available. By default, Kopia or Restic run on all nodes.</para>
</callout>
<callout arearefs="CO160-11">
<para>Specify the backup provider.</para>
</callout>
<callout arearefs="CO160-12">
<para>Specify the correct default name for the <literal>Secret</literal>, for example, <literal>cloud-credentials-gcp</literal>, if you use a default plugin for the backup provider. If specifying a custom name, then the custom name is used for the backup location. If you do not specify a <literal>Secret</literal> name, the default name is used.</para>
</callout>
<callout arearefs="CO160-13">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO160-14">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-3_virt-installing-configuring-oadp">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-enabling-csi-dpa_virt-installing-configuring-oadp">
<title>Enabling CSI in the DataProtectionApplication CR</title>
<simpara>You enable the Container Storage Interface (CSI) in the <literal>DataProtectionApplication</literal> custom resource (CR) in order to back up persistent volumes with CSI snapshots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - csi <co xml:id="CO161-1"/></programlisting>
<calloutlist>
<callout arearefs="CO161-1">
<para>Add the <literal>csi</literal> default plugin.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="uninstalling-oadp_virt-installing-configuring-oadp">
<title>Uninstalling OADP</title>
<simpara>You uninstall the OpenShift API for Data Protection (OADP) by deleting the OADP Operator. See <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/operators/#olm-deleting-operators-from-cluster">Deleting Operators from a cluster</link> for details.</simpara>
</section>
</section>
<section xml:id="virt-backup-restore-overview">
<title>Backing up and restoring virtual machines</title>

<simpara>Back up and restore virtual machines by using the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#application-backup-restore-operations-overview">OpenShift API for Data Protection (OADP)</link>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Install the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#about-installing-oadp">OADP Operator</link> according to the instructions for your storage provider.</simpara>
</listitem>
<listitem>
<simpara>Install the <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#oadp-installing-dpa_installing-oadp-ocs">Data Protection Application</link> with the <literal>kubevirt</literal> and <literal>openshift</literal> <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#oadp-plugins_oadp-features-plugins">plugins</link>.</simpara>
</listitem>
<listitem>
<simpara>Back up virtual machines by creating a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#backing-up-applications"><literal>Backup</literal> custom resource (CR)</link>.</simpara>
</listitem>
<listitem>
<simpara>Restore the <literal>Backup</literal> CR by creating a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#restoring-applications"><literal>Restore</literal> CR</link>.</simpara>
</listitem>
</orderedlist>
<section xml:id="additional-resources_virt-backup-restore-overview" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#oadp-features-plugins">OADP features and plugins</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#troubleshooting">Troubleshooting</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-backing-up-vms">
<title>Backing up virtual machines</title>

<simpara>You back up virtual machines (VMs) by creating an OpenShift API for Data Protection (OADP) <link linkend="oadp-creating-backup-cr_virt-backing-up-vms"><literal>Backup</literal> custom resource (CR)</link>.</simpara>
<simpara>The <literal>Backup</literal> CR performs the following actions:</simpara>
<itemizedlist>
<listitem>
<simpara>Backs up OpenShift Virtualization resources by creating an archive file on S3-compatible object storage, such as <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#installing-oadp-mcg">Multicloud Object Gateway</link>, Noobaa, or Minio.</simpara>
</listitem>
<listitem>
<simpara>Backs up VM disks by using one of the following options:</simpara>
<itemizedlist>
<listitem>
<simpara><link linkend="oadp-backing-up-pvs-csi_virt-backing-up-vms">Container Storage Interface (CSI) snapshots</link> on CSI-enabled cloud storage, such as Ceph RBD or Ceph FS.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#backing-up-applications">Backing up applications with File System Backup: Kopia or Restic</link> on object storage.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>OADP provides backup hooks to freeze the VM file system before the backup operation and unfreeze it when the backup is complete.</simpara>
<simpara>The <literal>kubevirt-controller</literal> creates the <literal>virt-launcher</literal> pods with annotations that enable Velero to run the <literal>virt-freezer</literal> binary before and after the backup operation.</simpara>
<simpara>The <literal>freeze</literal> and <literal>unfreeze</literal> APIs are subresources of the VM snapshot API. See <link linkend="virt-about-vm-snapshots_virt-backup-restore-snapshots">About virtual machine snapshots</link> for details.</simpara>
</note>
<simpara>You can add <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#backing-up-applications">hooks</link> to the <literal>Backup</literal> CR to run commands on specific VMs before or after the backup operation.</simpara>
<simpara>You schedule a backup by creating a <link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/backup_and_restore/#backing-up-applications"><literal>Schedule</literal> CR</link> instead of a <literal>Backup</literal> CR.</simpara>
<section xml:id="oadp-creating-backup-cr_virt-backing-up-vms">
<title>Creating a Backup CR</title>
<simpara>You back up Kubernetes images, internal images, and persistent volumes (PVs) by creating a <literal>Backup</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
<listitem>
<simpara>Backup location prerequisites:</simpara>
<itemizedlist>
<listitem>
<simpara>You must have S3 object storage configured for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must have a backup location configured in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Snapshot location prerequisites:</simpara>
<itemizedlist>
<listitem>
<simpara>Your cloud provider must have a native snapshot API or support Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>For CSI snapshots, you must create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
</listitem>
<listitem>
<simpara>You must have a volume location configured in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the <literal>backupStorageLocations</literal> CRs by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocations -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE       NAME              PHASE       LAST VALIDATED   AGE   DEFAULT
openshift-adp   velero-sample-1   Available   11s              31m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>Backup</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup&gt;
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  hooks: {}
  includedNamespaces:
  - &lt;namespace&gt; <co xml:id="CO162-1"/>
  includedResources: [] <co xml:id="CO162-2"/>
  excludedResources: [] <co xml:id="CO162-3"/>
  storageLocation: &lt;velero-sample-1&gt; <co xml:id="CO162-4"/>
  ttl: 720h0m0s
  labelSelector: <co xml:id="CO162-5"/>
    matchLabels:
      app=&lt;label_1&gt;
      app=&lt;label_2&gt;
      app=&lt;label_3&gt;
  orLabelSelectors: <co xml:id="CO162-6"/>
  - matchLabels:
      app=&lt;label_1&gt;
      app=&lt;label_2&gt;
      app=&lt;label_3&gt;</programlisting>
<calloutlist>
<callout arearefs="CO162-1">
<para>Specify an array of namespaces to back up.</para>
</callout>
<callout arearefs="CO162-2">
<para>Optional: Specify an array of resources to include in the backup. Resources might be shortcuts (for example, 'po' for 'pods') or fully-qualified. If unspecified, all resources are included.</para>
</callout>
<callout arearefs="CO162-3">
<para>Optional: Specify an array of resources to exclude from the backup. Resources might be shortcuts (for example, 'po' for 'pods') or fully-qualified.</para>
</callout>
<callout arearefs="CO162-4">
<para>Specify the name of the <literal>backupStorageLocations</literal> CR.</para>
</callout>
<callout arearefs="CO162-5">
<para>Map of {key,value} pairs of backup resources that have <emphasis role="strong">all</emphasis> of the specified labels.</para>
</callout>
<callout arearefs="CO162-6">
<para>Map of {key,value} pairs of backup resources that have <emphasis role="strong">one or more</emphasis> of the specified labels.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the status of the <literal>Backup</literal> CR is <literal>Completed</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backup -n openshift-adp &lt;backup&gt; -o jsonpath='{.status.phase}'</programlisting>
</listitem>
</orderedlist>
<section xml:id="oadp-backing-up-pvs-csi_virt-backing-up-vms">
<title>Backing up persistent volumes with CSI snapshots</title>
<simpara>You back up persistent volumes with Container Storage Interface (CSI) snapshots by editing the <literal>VolumeSnapshotClass</literal> custom resource (CR) of the cloud storage before you create the <literal>Backup</literal> CR.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
<listitem>
<simpara>You must enable CSI in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add the <literal>metadata.labels.velero.io/csi-volumesnapshot-class: "true"</literal> key-value pair to the <literal>VolumeSnapshotClass</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: &lt;volume_snapshot_class_name&gt;
  labels:
    velero.io/csi-volumesnapshot-class: "true"
driver: &lt;csi_driver&gt;
deletionPolicy: Retain</programlisting>
</listitem>
</itemizedlist>
<simpara>You can now create a <literal>Backup</literal> CR.</simpara>
</section>
<section xml:id="oadp-backing-up-applications-restic_backing-up-applications">
<title>Backing up applications with Restic</title>
<simpara>You back up Kubernetes resources, internal images, and persistent volumes with Restic by editing the <literal>Backup</literal> custom resource (CR).</simpara>
<simpara>You do not need to specify a snapshot location in the <literal>DataProtectionApplication</literal> CR.</simpara>
<important>
<simpara>Restic does not support backing up <literal>hostPath</literal> volumes. For more information, see <link xlink:href="https://velero.io/docs/v1.12/restic/#limitations">additional Restic limitations</link>.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>You must not disable the default Restic installation by setting <literal>spec.configuration.restic.enable</literal> to <literal>false</literal> in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>Backup</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup&gt;
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  defaultVolumesToFsBackup: true <co xml:id="CO163-1"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO163-1">
<para>In OADP version 1.2 and later, add the <literal>defaultVolumesToFsBackup: true</literal> setting within the <literal>spec</literal> block. In OADP version 1.1, add <literal>defaultVolumesToRestic: true</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-creating-backup-hooks_backing-up-applications">
<title>Creating backup hooks</title>
<simpara>You create backup hooks to run commands in a container in a pod by editing the <literal>Backup</literal> custom resource (CR).</simpara>
<simpara><emphasis>Pre</emphasis> hooks run before the pod is backed up. <emphasis>Post</emphasis> hooks run after the backup.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add a hook to the <literal>spec.hooks</literal> block of the <literal>Backup</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup&gt;
  namespace: openshift-adp
spec:
  hooks:
    resources:
      - name: &lt;hook_name&gt;
        includedNamespaces:
        - &lt;namespace&gt; <co xml:id="CO164-1"/>
        excludedNamespaces: <co xml:id="CO164-2"/>
        - &lt;namespace&gt;
        includedResources: []
        - pods <co xml:id="CO164-3"/>
        excludedResources: [] <co xml:id="CO164-4"/>
        labelSelector: <co xml:id="CO164-5"/>
          matchLabels:
            app: velero
            component: server
        pre: <co xml:id="CO164-6"/>
          - exec:
              container: &lt;container&gt; <co xml:id="CO164-7"/>
              command:
              - /bin/uname <co xml:id="CO164-8"/>
              - -a
              onError: Fail <co xml:id="CO164-9"/>
              timeout: 30s <co xml:id="CO164-10"/>
        post: <co xml:id="CO164-11"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO164-1">
<para>Optional: You can specify namespaces to which the hook applies. If this value is not specified, the hook applies to all namespaces.</para>
</callout>
<callout arearefs="CO164-2">
<para>Optional: You can specify namespaces to which the hook does not apply.</para>
</callout>
<callout arearefs="CO164-3">
<para>Currently, pods are the only supported resource that hooks can apply to.</para>
</callout>
<callout arearefs="CO164-4">
<para>Optional: You can specify resources to which the hook does not apply.</para>
</callout>
<callout arearefs="CO164-5">
<para>Optional: This hook only applies to objects matching the label. If this value is not specified, the hook applies to all namespaces.</para>
</callout>
<callout arearefs="CO164-6">
<para>Array of hooks to run before the backup.</para>
</callout>
<callout arearefs="CO164-7">
<para>Optional: If the container is not specified, the command runs in the first container in the pod.</para>
</callout>
<callout arearefs="CO164-8">
<para>This is the entrypoint for the init container being added.</para>
</callout>
<callout arearefs="CO164-9">
<para>Allowed values for error handling are <literal>Fail</literal> and <literal>Continue</literal>. The default is <literal>Fail</literal>.</para>
</callout>
<callout arearefs="CO164-10">
<para>Optional: How long to wait for the commands to run. The default is <literal>30s</literal>.</para>
</callout>
<callout arearefs="CO164-11">
<para>This block defines an array of hooks to run after the backup, with the same parameters as the pre-backup hooks.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="additional-resources_virt-backing-up-vms">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html-single/storage/#persistent-storage-csi-snapshots-overview_persistent-storage-csi-snapshots">Overview of CSI volume snapshots</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="virt-restoring-vms">
<title>Restoring virtual machines</title>

<simpara>You restore an OpenShift API for Data Protection (OADP) <literal>Backup</literal> custom resource (CR) by creating a <link linkend="oadp-creating-restore-cr_virt-restoring-vms"><literal>Restore</literal> CR</link>.</simpara>
<simpara>You can add <link linkend="oadp-creating-restore-hooks_virt-restoring-vms">hooks</link> to the <literal>Restore</literal> CR to run commands in init containers, before the application container starts, or in the application container itself.</simpara>
<section xml:id="oadp-creating-restore-cr_virt-restoring-vms">
<title>Creating a Restore CR</title>
<simpara>You restore a <literal>Backup</literal> custom resource (CR) by creating a <literal>Restore</literal> CR.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
<listitem>
<simpara>You must have a Velero <literal>Backup</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>The persistent volume (PV) capacity must match the requested size at backup time. Adjust the requested size if needed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Restore</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore&gt;
  namespace: openshift-adp
spec:
  backupName: &lt;backup&gt; <co xml:id="CO165-1"/>
  includedResources: [] <co xml:id="CO165-2"/>
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
  restorePVs: true <co xml:id="CO165-3"/></programlisting>
<calloutlist>
<callout arearefs="CO165-1">
<para>Name of the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO165-2">
<para>Optional: Specify an array of resources to include in the restore process. Resources might be shortcuts (for example, <literal>po</literal> for <literal>pods</literal>) or fully-qualified. If unspecified, all resources are included.</para>
</callout>
<callout arearefs="CO165-3">
<para>Optional: The <literal>restorePVs</literal> parameter can be set to <literal>false</literal> to turn off restore of <literal>PersistentVolumes</literal> from <literal>VolumeSnapshot</literal> of Container Storage Interface (CSI) snapshots or from native snapshots when <literal>VolumeSnapshotLocation</literal> is configured.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the status of the <literal>Restore</literal> CR is <literal>Completed</literal> by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get restore -n openshift-adp &lt;restore&gt; -o jsonpath='{.status.phase}'</programlisting>
</listitem>
<listitem>
<simpara>Verify that the backup resources have been restored by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n &lt;namespace&gt; <co xml:id="CO166-1"/></programlisting>
<calloutlist>
<callout arearefs="CO166-1">
<para>Namespace that you backed up.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If you use Restic to restore <literal>DeploymentConfig</literal> objects or if you use post-restore hooks, run the <literal>dc-restic-post-restore.sh</literal> cleanup script by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ bash dc-restic-post-restore.sh &lt;restore-name&gt;</programlisting>
<note>
<simpara>During the restore process, the OADP Velero plug-ins scale down the <literal>DeploymentConfig</literal> objects and restore the pods as standalone pods. This is done to prevent the cluster from deleting the restored <literal>DeploymentConfig</literal> pods immediately on restore and to allow Restic and post-restore hooks to complete their actions on the restored pods. The cleanup script shown below removes these disconnected pods and scales any <literal>DeploymentConfig</literal> objects back up to the appropriate number of replicas.</simpara>
</note>
<example>
<title><literal>dc-restic-post-restore.sh</literal> cleanup script</title>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
set -e

# if sha256sum exists, use it to check the integrity of the file
if command -v sha256sum &gt;/dev/null 2&gt;&amp;1; then
  CHECKSUM_CMD="sha256sum"
else
  CHECKSUM_CMD="shasum -a 256"
fi

label_name () {
    if [ "${#1}" -le "63" ]; then
	echo $1
	return
    fi
    sha=$(echo -n $1|$CHECKSUM_CMD)
    echo "${1:0:57}${sha:0:6}"
}

OADP_NAMESPACE=${OADP_NAMESPACE:=openshift-adp}

if [[ $# -ne 1 ]]; then
    echo "usage: ${BASH_SOURCE} restore-name"
    exit 1
fi

echo using OADP Namespace $OADP_NAMESPACE
echo restore: $1

label=$(label_name $1)
echo label: $label

echo Deleting disconnected restore pods
oc delete pods -l oadp.openshift.io/disconnected-from-dc=$label

for dc in $(oc get dc --all-namespaces -l oadp.openshift.io/replicas-modified=$label -o jsonpath='{range .items[*]}{.metadata.namespace}{","}{.metadata.name}{","}{.metadata.annotations.oadp\.openshift\.io/original-replicas}{","}{.metadata.annotations.oadp\.openshift\.io/original-paused}{"\n"}')
do
    IFS=',' read -ra dc_arr &lt;&lt;&lt; "$dc"
    if [ ${#dc_arr[0]} -gt 0 ]; then
	echo Found deployment ${dc_arr[0]}/${dc_arr[1]}, setting replicas: ${dc_arr[2]}, paused: ${dc_arr[3]}
	cat &lt;&lt;EOF | oc patch dc  -n ${dc_arr[0]} ${dc_arr[1]} --patch-file /dev/stdin
spec:
  replicas: ${dc_arr[2]}
  paused: ${dc_arr[3]}
EOF
    fi
done</programlisting>
</example>
</listitem>
</orderedlist>
<section xml:id="oadp-creating-restore-hooks_virt-restoring-vms">
<title>Creating restore hooks</title>
<simpara>You create restore hooks to run commands in a container in a pod by editing the <literal>Restore</literal> custom resource (CR).</simpara>
<simpara>You can create two types of restore hooks:</simpara>
<itemizedlist>
<listitem>
<simpara>An <literal>init</literal> hook adds an init container to a pod to perform setup tasks before the application container starts.</simpara>
<simpara>If you restore a Restic backup, the <literal>restic-wait</literal> init container is added before the restore hook init container.</simpara>
</listitem>
<listitem>
<simpara>An <literal>exec</literal> hook runs commands or scripts in a container of a restored pod.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add a hook to the <literal>spec.hooks</literal> block of the <literal>Restore</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore&gt;
  namespace: openshift-adp
spec:
  hooks:
    resources:
      - name: &lt;hook_name&gt;
        includedNamespaces:
        - &lt;namespace&gt; <co xml:id="CO167-1"/>
        excludedNamespaces:
        - &lt;namespace&gt;
        includedResources:
        - pods <co xml:id="CO167-2"/>
        excludedResources: []
        labelSelector: <co xml:id="CO167-3"/>
          matchLabels:
            app: velero
            component: server
        postHooks:
        - init:
            initContainers:
            - name: restore-hook-init
              image: alpine:latest
              volumeMounts:
              - mountPath: /restores/pvc1-vm
                name: pvc1-vm
              command:
              - /bin/ash
              - -c
            timeout: <co xml:id="CO167-4"/>
        - exec:
            container: &lt;container&gt; <co xml:id="CO167-5"/>
            command:
            - /bin/bash <co xml:id="CO167-6"/>
            - -c
            - "psql &lt; /backup/backup.sql"
            waitTimeout: 5m <co xml:id="CO167-7"/>
            execTimeout: 1m <co xml:id="CO167-8"/>
            onError: Continue <co xml:id="CO167-9"/></programlisting>
<calloutlist>
<callout arearefs="CO167-1">
<para>Optional: Array of namespaces to which the hook applies. If this value is not specified, the hook applies to all namespaces.</para>
</callout>
<callout arearefs="CO167-2">
<para>Currently, pods are the only supported resource that hooks can apply to.</para>
</callout>
<callout arearefs="CO167-3">
<para>Optional: This hook only applies to objects matching the label selector.</para>
</callout>
<callout arearefs="CO167-4">
<para>Optional: Timeout specifies the maximum length of time Velero waits for <literal>initContainers</literal> to complete.</para>
</callout>
<callout arearefs="CO167-5">
<para>Optional: If the container is not specified, the command runs in the first container in the pod.</para>
</callout>
<callout arearefs="CO167-6">
<para>This is the entrypoint for the init container being added.</para>
</callout>
<callout arearefs="CO167-7">
<para>Optional: How long to wait for a container to become ready. This should be long enough for the container to start and for any preceding hooks in the same container to complete. If not set, the restore process waits indefinitely.</para>
</callout>
<callout arearefs="CO167-8">
<para>Optional: How long to wait for the commands to run. The default is <literal>30s</literal>.</para>
</callout>
<callout arearefs="CO167-9">
<para>Allowed values for error handling are <literal>Fail</literal> and <literal>Continue</literal>:</para>
<itemizedlist>
<listitem>
<simpara><literal>Continue</literal>: Only command failures are logged.</simpara>
</listitem>
<listitem>
<simpara><literal>Fail</literal>: No more restore hooks run in any container in any pod. The status of the <literal>Restore</literal> CR will be <literal>PartiallyFailed</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="virt-disaster-recovery">
<title>Disaster recovery</title>

<simpara>The disaster recovery documentation provides information for administrators on how to recover from certain disaster scenarios that might occur with an OpenShift Container Platform cluster. As an administrator, you must plan your OpenShift Virtualization deployment in advance in order to take advantage of disaster recovery.</simpara>
<variablelist>
<varlistentry>
<term>Metropolitan Disaster Recovery (Metro-DR)</term>
<listitem>
<simpara>Metro-DR provides two-way synchronous data replication between managed OpenShift Virtualization clusters installed on primary and secondary sites. Use Metro-DR during a site disaster to fail applications from the primary to the secondary site, and to relocate the application back to the primary site after restoring the disaster site. This synchronous solution is only available to metropolitan distance data centers with a 10 millisecond latency or less. For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.14/html-single/configuring_openshift_data_foundation_disaster_recovery_for_openshift_workloads/index#metro-dr-solution">Metro-DR solution for OpenShift Data Foundation</link>.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
</chapter>
</book>
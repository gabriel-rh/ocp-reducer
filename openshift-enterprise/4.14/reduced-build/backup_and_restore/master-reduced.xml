<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE book [
<!ENTITY % sgml.features "IGNORE">
<!ENTITY % xml.features "INCLUDE">
<!ENTITY % DOCBOOK_ENTS PUBLIC "-//OASIS//ENTITIES DocBook Character Entities V4.5//EN" "http://www.oasis-open.org/docbook/xml/4.5/dbcentx.mod">
%DOCBOOK_ENTS;
]>
<?asciidoc-toc?>
<?asciidoc-numbered?>
<book xmlns="http://docbook.org/ns/docbook" xmlns:xl="http://www.w3.org/1999/xlink" version="5.0" xml:lang="en">
<info>
<title>Backup and restore</title>
<date>2024-02-20</date>
</info>
<chapter xml:id="backup-restore-overview">
<title>Backup and restore</title>

<section xml:id="control-plane-backup-restore-operations-overview">
<title>Control plane backup and restore operations</title>
<simpara>As a cluster administrator, you might need to stop an OpenShift Container Platform cluster for a period and restart it later. Some reasons for restarting a cluster are that you need to perform maintenance on a cluster or want to reduce resource costs. In OpenShift Container Platform, you can perform a <link xlink:href="../backup_and_restore/graceful-cluster-shutdown.xml#graceful-shutdown-cluster">graceful shutdown of a cluster</link> so that you can easily restart the cluster later.</simpara>
<simpara>You must <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.xml#backup-etcd">back up etcd data</link> before shutting down a cluster; etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects. An etcd backup plays a crucial role in disaster recovery. In OpenShift Container Platform, you can also <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/replacing-unhealthy-etcd-member.xml#replacing-unhealthy-etcd-member">replace an unhealthy etcd member</link>.</simpara>
<simpara>When you want to get your cluster running again, <link xlink:href="../backup_and_restore/graceful-cluster-restart.xml#graceful-restart-cluster">restart the cluster gracefully</link>.</simpara>
<note>
<simpara>A cluster&#8217;s certificates expire one year after the installation date. You can shut down a cluster and expect it to restart gracefully while the certificates are still valid. Although the cluster automatically retrieves the expired control plane certificates, you must still <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-3-expired-certs.xml#dr-recovering-expired-certs">approve the certificate signing requests (CSRs)</link>.</simpara>
</note>
<simpara>You might run into several situations where OpenShift Container Platform  does not work as expected, such as:</simpara>
<itemizedlist>
<listitem>
<simpara>You have a cluster that is not functional after the restart because of unexpected conditions, such as node failure or network connectivity issues.</simpara>
</listitem>
<listitem>
<simpara>You have deleted something critical in the cluster by mistake.</simpara>
</listitem>
<listitem>
<simpara>You have lost the majority of your control plane hosts, leading to etcd quorum loss.</simpara>
</listitem>
</itemizedlist>
<simpara>You can always recover from a disaster situation by <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">restoring your cluster to its previous state</link> using the saved etcd snapshots.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../machine_management/deleting-machine.xml#machine-lifecycle-hook-deletion-etcd_deleting-machine">Quorum protection with machine lifecycle hooks</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="application-backup-restore-operations-overview">
<title>Application backup and restore operations</title>
<simpara>As a cluster administrator, you can back up and restore applications running on OpenShift Container Platform by using the OpenShift API for Data Protection (OADP).</simpara>
<simpara>OADP backs up and restores Kubernetes resources and internal images, at the granularity of a namespace, by using the version of Velero that is appropriate for the version of OADP you install, according to the table in <link xlink:href="../backup_and_restore/application_backup_and_restore/troubleshooting.xml#velero-obtaining-by-downloading_oadp-troubleshooting">Downloading the Velero CLI tool</link>.  OADP backs up and restores persistent volumes (PVs) by using snapshots or Restic. For details, see <link xlink:href="../backup_and_restore/application_backup_and_restore/oadp-features-plugins.xml#oadp-features_oadp-features-plugins">OADP features</link>.</simpara>
<section xml:id="oadp-requirements">
<title>OADP requirements</title>
<simpara>OADP has the following requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>You must be logged in as a user with a <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have object storage for storing backups, such as one of the following storage types:</simpara>
<itemizedlist>
<listitem>
<simpara>OpenShift Data Foundation</simpara>
</listitem>
<listitem>
<simpara>Amazon Web Services</simpara>
</listitem>
<listitem>
<simpara>Microsoft Azure</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Platform</simpara>
</listitem>
<listitem>
<simpara>S3-compatible object storage</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>If you want to use CSI backup on OCP 4.11 and later, install OADP 1.1.<emphasis>x</emphasis>.</simpara>
<simpara>OADP 1.0.<emphasis>x</emphasis> does not support CSI backup on OCP 4.11 and later. OADP 1.0.<emphasis>x</emphasis> includes Velero 1.7.<emphasis>x</emphasis> and expects the API group <literal>snapshot.storage.k8s.io/v1beta1</literal>, which is not present on OCP 4.11 and later.</simpara>
</note>
<important>
<simpara>The <literal>CloudStorage</literal> API for S3 storage is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>To back up PVs with snapshots, you must have cloud storage that has a native snapshot API or supports Container Storage Interface (CSI) snapshots, such as the following providers:</simpara>
<itemizedlist>
<listitem>
<simpara>Amazon Web Services</simpara>
</listitem>
<listitem>
<simpara>Microsoft Azure</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Platform</simpara>
</listitem>
<listitem>
<simpara>CSI snapshot-enabled cloud storage, such as Ceph RBD or Ceph FS</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>If you do not want to back up PVs by using snapshots, you can use <link xlink:href="https://restic.net/">Restic</link>, which is installed by the OADP Operator by default.</simpara>
</note>
</section>
<section xml:id="backing-up-and-restoring-applications">
<title>Backing up and restoring applications</title>
<simpara>You back up applications by creating a <literal>Backup</literal> custom resource (CR). See <link xlink:href="../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-creating-backup-cr.xml#backing-up-applications">Creating a Backup CR</link>. You can configure the following backup options:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-creating-backup-hooks-doc.xml#backing-up-applications">Creating backup hooks</link> to run commands before or after the backup operation</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-scheduling-backups-doc.xml#backing-up-applications">Scheduling backups</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-backing-up-applications-restic-doc.xml#backing-up-applications">Backing up applications with File System Backup: Kopia or Restic</link></simpara>
</listitem>
<listitem>
<simpara>You restore application backups by creating a <literal>Restore</literal> (CR). See <link xlink:href="../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/restoring-applications.xml#oadp-creating-restore-cr_restoring-applications">Creating a Restore CR</link>.</simpara>
</listitem>
<listitem>
<simpara>You can configure <link xlink:href="../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/restoring-applications.xml#oadp-creating-restore-hooks_restoring-applications">restore hooks</link> to run commands in init containers or in the application container during the restore operation.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
</chapter>
<chapter xml:id="graceful-shutdown-cluster">
<title>Shutting down the cluster gracefully</title>

<simpara>This document describes the process to gracefully shut down your cluster. You might need to temporarily shut down your cluster for maintenance reasons, or to save on resource costs.</simpara>
<section xml:id="_prerequisites">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Take an <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.xml#backing-up-etcd-data_backup-etcd">etcd backup</link> prior to shutting down the cluster.</simpara>
<important>
<simpara>It is important to take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues when restarting the cluster.</simpara>
<simpara>For example, the following conditions can cause the restarted cluster to malfunction:</simpara>
<itemizedlist>
<listitem>
<simpara>etcd data corruption during shutdown</simpara>
</listitem>
<listitem>
<simpara>Node failure due to hardware</simpara>
</listitem>
<listitem>
<simpara>Network connectivity issues</simpara>
</listitem>
</itemizedlist>
<simpara>If your cluster fails to recover, follow the steps to <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">restore to a previous cluster state</link>.</simpara>
</important>
</listitem>
</itemizedlist>
</section>
<section xml:id="graceful-shutdown_graceful-shutdown-cluster">
<title>Shutting down the cluster</title>
<simpara>You can shut down your cluster in a graceful manner so that it can be restarted at a later date.</simpara>
<note>
<simpara>You can shut down a cluster until a year from the installation date and expect it to restart gracefully. After a year from the installation date, the cluster certificates expire.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have taken an etcd backup.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If you are shutting the cluster down for an extended period, determine the date on which certificates expire and run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-kube-apiserver-operator get secret kube-apiserver-to-kubelet-signer -o jsonpath='{.metadata.annotations.auth\.openshift\.io/certificate-not-after}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">2022-08-05T14:37:50Zuser@user:~ $ <co xml:id="CO1-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO1-1">
<para>To ensure that the cluster can restart gracefully, plan to restart it on or before the specified date. As the cluster restarts, the process might require you to manually approve the pending certificate signing requests (CSRs) to recover kubelet certificates.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Mark all the nodes in the cluster as unschedulable. You can do this from your cloud provider&#8217;s web console, or by running the following loop:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}'); do echo ${node} ; oc adm cordon ${node} ; done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ci-ln-mgdnf4b-72292-n547t-master-0
node/ci-ln-mgdnf4b-72292-n547t-master-0 cordoned
ci-ln-mgdnf4b-72292-n547t-master-1
node/ci-ln-mgdnf4b-72292-n547t-master-1 cordoned
ci-ln-mgdnf4b-72292-n547t-master-2
node/ci-ln-mgdnf4b-72292-n547t-master-2 cordoned
ci-ln-mgdnf4b-72292-n547t-worker-a-s7ntl
node/ci-ln-mgdnf4b-72292-n547t-worker-a-s7ntl cordoned
ci-ln-mgdnf4b-72292-n547t-worker-b-cmc9k
node/ci-ln-mgdnf4b-72292-n547t-worker-b-cmc9k cordoned
ci-ln-mgdnf4b-72292-n547t-worker-c-vcmtn
node/ci-ln-mgdnf4b-72292-n547t-worker-c-vcmtn cordoned</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Evacuate the pods using the following method:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for node in $(oc get nodes -l node-role.kubernetes.io/worker -o jsonpath='{.items[*].metadata.name}'); do echo ${node} ; oc adm drain ${node} --delete-emptydir-data --ignore-daemonsets=true --timeout=15s ; done</programlisting>
</listitem>
<listitem>
<simpara>Shut down all of the nodes in the cluster. You can do this from your cloud provider’s web console, or by running the following loop:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}'); do oc debug node/${node} -- chroot /host shutdown -h 1; done</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Starting pod/ip-10-0-130-169us-east-2computeinternal-debug ...
To use host binaries, run `chroot /host`
Shutdown scheduled for Mon 2021-09-13 09:36:17 UTC, use 'shutdown -c' to cancel.
Removing debug pod ...
Starting pod/ip-10-0-150-116us-east-2computeinternal-debug ...
To use host binaries, run `chroot /host`
Shutdown scheduled for Mon 2021-09-13 09:36:29 UTC, use 'shutdown -c' to cancel.</programlisting>
</para>
</formalpara>
<simpara>Shutting down the nodes using one of these methods allows pods to terminate gracefully, which reduces the chance for data corruption.</simpara>
<note>
<simpara>Adjust the shut down time to be longer for large-scale clusters:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for node in $(oc get nodes -o jsonpath='{.items[*].metadata.name}'); do oc debug node/${node} -- chroot /host shutdown -h 10; done</programlisting>
</note>
<note>
<simpara>It is not necessary to drain control plane nodes of the standard pods that ship with OpenShift Container Platform prior to shutdown.
Cluster administrators are responsible for ensuring a clean restart of their own workloads after the cluster is restarted. If you drained control plane nodes prior to shutdown because of custom workloads, you must mark the control plane nodes as schedulable before the cluster will be functional again after restart.</simpara>
</note>
</listitem>
<listitem>
<simpara>Shut off any cluster dependencies that are no longer needed, such as external storage or an LDAP server. Be sure to consult your vendor&#8217;s documentation before doing so.</simpara>
<important>
<simpara>If you deployed your cluster on a cloud-provider platform, do not shut down, suspend, or delete the associated cloud resources. If you delete the cloud resources of a suspended virtual machine, OpenShift Container Platform might not restore successfully.</simpara>
</important>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_restarting-restoring-cluster" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../backup_and_restore/graceful-cluster-restart.xml#graceful-restart-cluster">Restarting the cluster gracefully</link></simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="graceful-restart-cluster">
<title>Restarting the cluster gracefully</title>

<simpara>This document describes the process to restart your cluster after a graceful shutdown.</simpara>
<simpara>Even though the cluster is expected to be functional after the restart, the cluster might not recover due to unexpected conditions, for example:</simpara>
<itemizedlist>
<listitem>
<simpara>etcd data corruption during shutdown</simpara>
</listitem>
<listitem>
<simpara>Node failure due to hardware</simpara>
</listitem>
<listitem>
<simpara>Network connectivity issues</simpara>
</listitem>
</itemizedlist>
<simpara>If your cluster fails to recover, follow the steps to <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">restore to a previous cluster state</link>.</simpara>
<section xml:id="_prerequisites_2">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You have <link xlink:href="../backup_and_restore/graceful-cluster-shutdown.xml#graceful-shutdown-cluster">gracefully shut down your cluster</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="graceful-restart_graceful-restart-cluster">
<title>Restarting the cluster</title>
<simpara>You can restart your cluster after it has been shut down gracefully.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>This procedure assumes that you gracefully shut down the cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Power on any cluster dependencies, such as external storage or an LDAP server.</simpara>
</listitem>
<listitem>
<simpara>Start all cluster machines.</simpara>
<simpara>Use the appropriate method for your cloud environment to start the machines, for example, from your cloud provider&#8217;s web console.</simpara>
<simpara>Wait approximately 10 minutes before continuing to check the status of control plane nodes.</simpara>
</listitem>
<listitem>
<simpara>Verify that all control plane nodes are ready.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/master</programlisting>
<simpara>The control plane nodes are ready if the status is <literal>Ready</literal>, as shown in the following output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-168-251.ec2.internal   Ready    master   75m   v1.28.5
ip-10-0-170-223.ec2.internal   Ready    master   75m   v1.28.5
ip-10-0-211-16.ec2.internal    Ready    master   75m   v1.28.5</programlisting>
</listitem>
<listitem>
<simpara>If the control plane nodes are <emphasis>not</emphasis> ready, then check whether there are any pending certificate signing requests (CSRs) that must be approved.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of current CSRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
</listitem>
<listitem>
<simpara>Review the details of a CSR to verify that it is valid:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe csr &lt;csr_name&gt; <co xml:id="CO2-1"/></programlisting>
<calloutlist>
<callout arearefs="CO2-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Approve each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>After the control plane nodes are ready, verify that all worker nodes are ready.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/worker</programlisting>
<simpara>The worker nodes are ready if the status is <literal>Ready</literal>, as shown in the following output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-179-95.ec2.internal    Ready    worker   64m   v1.28.5
ip-10-0-182-134.ec2.internal   Ready    worker   64m   v1.28.5
ip-10-0-250-100.ec2.internal   Ready    worker   64m   v1.28.5</programlisting>
</listitem>
<listitem>
<simpara>If the worker nodes are <emphasis>not</emphasis> ready, then check whether there are any pending certificate signing requests (CSRs) that must be approved.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of current CSRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
</listitem>
<listitem>
<simpara>Review the details of a CSR to verify that it is valid:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe csr &lt;csr_name&gt; <co xml:id="CO3-1"/></programlisting>
<calloutlist>
<callout arearefs="CO3-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Approve each valid CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the cluster started properly.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check that there are no degraded cluster Operators.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusteroperators</programlisting>
<simpara>Check that there are no cluster Operators with the <literal>DEGRADED</literal> condition set to <literal>True</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                       VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE
authentication                             4.14.0    True        False         False      59m
cloud-credential                           4.14.0    True        False         False      85m
cluster-autoscaler                         4.14.0    True        False         False      73m
config-operator                            4.14.0    True        False         False      73m
console                                    4.14.0    True        False         False      62m
csi-snapshot-controller                    4.14.0    True        False         False      66m
dns                                        4.14.0    True        False         False      76m
etcd                                       4.14.0    True        False         False      76m
...</programlisting>
</listitem>
<listitem>
<simpara>Check that all nodes are in the <literal>Ready</literal> state:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<simpara>Check that the status for all nodes is <literal>Ready</literal>.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS   ROLES    AGE   VERSION
ip-10-0-168-251.ec2.internal   Ready    master   82m   v1.28.5
ip-10-0-170-223.ec2.internal   Ready    master   82m   v1.28.5
ip-10-0-179-95.ec2.internal    Ready    worker   70m   v1.28.5
ip-10-0-182-134.ec2.internal   Ready    worker   70m   v1.28.5
ip-10-0-211-16.ec2.internal    Ready    master   82m   v1.28.5
ip-10-0-250-100.ec2.internal   Ready    worker   69m   v1.28.5</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>If the cluster did not start properly, you might need to restore your cluster using an etcd backup.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara>See <link xlink:href="../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">Restoring to a previous cluster state</link> for how to use an etcd backup to restore if your cluster failed to recover after restarting.</simpara>
</listitem>
</itemizedlist>
</section>
</chapter>
<chapter xml:id="_oadp_application_backup_and_restore">
<title>OADP Application backup and restore</title>
<section xml:id="oadp-introduction">
<title>Introduction to OpenShift API for Data Protection</title>

<simpara>The OpenShift API for Data Protection (OADP) product safeguards customer applications on OpenShift Container Platform. It offers comprehensive disaster recovery protection, covering OpenShift Container Platform applications, application-related cluster resources, persistent volumes, and internal images. OADP is also capable of backing up both containerized applications and virtual machines (VMs).</simpara>
<simpara>However, OADP does not serve as a disaster recovery solution for <link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.xml#backup-etcd">etcd</link> or OpenShift Operators.</simpara>
<section xml:id="oadp-apis_oadp-api">
<title>OpenShift API for Data Protection APIs</title>
<simpara>OpenShift API for Data Protection (OADP) provides APIs that enable multiple approaches to customizing backups and preventing the inclusion of unnecessary or inappropriate resources.</simpara>
<simpara>OADP provides the following APIs:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/backing-up-applications.xml#backing-up-applications">Backup</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/restoring-applications.xml#restoring-applications">Restore</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-scheduling-backups-doc.xml#oadp-scheduling-backups-doc">Schedule</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.xml#oadp-about-backup-snapshot-locations_installing-oadp-aws">BackupStorageLocation</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-backing-up-pvs-csi-doc.xml#oadp-backing-up-pvs-csi-doc">VolumeSnapshotLocation</link></simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.xml#backup-etcd">Backing up etcd</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_oadp_release_notes">
<title>OADP release notes</title>
<section xml:id="oadp-release-notes">
<title>OADP 1.3 release notes</title>

<simpara>The release notes for OpenShift API for Data Protection (OADP) 1.3 describe new features and enhancements, deprecated features, product recommendations, known issues, and resolved issues.</simpara>
<section xml:id="oadp-release-notes-1-3-0_oadp-release-notes">
<title>OADP 1.3.0 release notes</title>
<simpara>The OpenShift API for Data Protection (OADP) 1.3.0 release notes lists new features, resolved issues and bugs, and known issues.</simpara>
<section xml:id="new-features-1-3-0_oadp-release-notes">
<title>New features</title>
<formalpara>
<title>Velero built-in DataMover</title>
<para>OADP 1.3 includes a built-in Data Mover that you can use to move Container Storage Interface (CSI) volume snapshots to a remote object store. The built-in Data Mover allows you to restore stateful applications from the remote object store if a failure, accidental deletion, or corruption of the cluster occurs. It uses Kopia as the uploader mechanism to read the snapshot data and to write to the Unified Repository.</para>
</formalpara>
<important>
<simpara>Velero built-in DataMover is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<formalpara>
<title>Backing up applications with File System Backup: Kopia or Restic</title>
<para>Velero’s File System Backup (FSB) supports two backup libraries: the Restic path and the Kopia path.</para>
</formalpara>
<simpara>Velero allows users to select between the two paths.</simpara>
<simpara>For backup, specify the path during the installation through the <literal>uploader-type</literal> flag. The valid value is either <literal>restic</literal> or <literal>kopia</literal>. This field defaults to <literal>kopia</literal> if the value is not specified. The selection cannot be changed after the installation.</simpara>
<formalpara>
<title>GCP Cloud authentication</title>
<para>Google Cloud Platform (GCP) authentication enables you to use short-lived Google credentials.</para>
</formalpara>
<simpara>GCP with Workload Identity Federation enables you to use Identity and Access Management (IAM) to grant external identities IAM roles, including the ability to impersonate service accounts. This eliminates the maintenance and security risks associated with service account keys.</simpara>
<formalpara>
<title>AWS ROSA STS authentication</title>
<para>You can use OpenShift API for Data Protection (OADP) with Red Hat OpenShift Service on AWS (ROSA) clusters to backup and restore application data.</para>
</formalpara>
<simpara>ROSA provides seamless integration with a wide range of AWS compute, database, analytics, machine learning, networking, mobile, and other services to speed up the building and delivering of differentiating experiences to your customers.</simpara>
<simpara>You can subscribe to the service directly from your AWS account.</simpara>
<simpara>After the clusters are created, you can operate your clusters by using the OpenShift web console. The ROSA service also uses OpenShift APIs and command-line interface (CLI) tools.</simpara>
</section>
<section xml:id="resolved-issues-1-3-0_oadp-release-notes">
<title>Resolved issues</title>
<formalpara>
<title>ACM applications were removed and re-created on managed clusters after restore</title>
<para>Applications on managed clusters were deleted and re-created upon restore activation. OpenShift API for Data Protection (OADP 1.2) backup and restore process is faster than the older versions. The OADP performance change caused this behavior when restoring ACM resources. Therefore, some resources were restored before other resources, which caused the removal of the applications from managed clusters.
<link xlink:href="https://issues.redhat.com/browse/OADP-2686">OADP-2686</link></para>
</formalpara>
<formalpara>
<title>Restic restore was partially failing due to Pod Security standard</title>
<para>During interoperability testing, OpenShift Container Platform 4.14 had the pod Security mode set to <literal>enforce</literal>, which caused the pod to be denied. This was caused due to the restore order. The pod was getting created before the security context constraints (SCC) resource, since the pod violated the <literal>podSecurity</literal> standard, it denied the pod. When setting the restore priority field on the Velero server, restore is successful. <link xlink:href="https://issues.redhat.com/browse/OADP-2688">OADP-2688</link></para>
</formalpara>
<formalpara>
<title>Possible pod volume backup failure if Velero is installed in several namespaces</title>
<para>There was a regresssion in Pod Volume Backup (PVB) functionality when Velero was installed in several namespaces. The PVB controller was not properly limiting itself to PVBs in its own namespace.
<link xlink:href="https://issues.redhat.com/browse/OADP-2308">OADP-2308</link></para>
</formalpara>
<formalpara>
<title>OADP Velero plugins returning "received EOF, stopping recv loop" message</title>
<para>In OADP, Velero plugins were started as separate processes. When the Velero operation completes, either successfully or not, they exit. Therefore, if you see a <literal>received EOF, stopping recv loop</literal> messages in debug logs, it does not mean an error occurred, it means that a plugin operation has completed. <link xlink:href="https://issues.redhat.com/browse/OADP-2176">OADP-2176</link></para>
</formalpara>
<formalpara>
<title>CVE-2023-39325 Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</title>
<para>In previous releases of OADP, the HTTP/2 protocol was susceptible to a denial of service attack because request cancellation could reset multiple streams quickly. The server had to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection. This resulted in a denial of service due to server resource consumption.</para>
</formalpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</link></simpara>
<simpara>For a complete list of all issues resolved in this release, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12422837">OADP 1.3.0 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues-1-3-0_oadp-release-notes">
<title>Known issues</title>
<formalpara>
<title>CSI plugin errors on nil pointer when csiSnapshotTimeout is set to a short duration</title>
<para>The CSI plugin errors on nil pointer when <literal>csiSnapshotTimeout</literal> is set to a short duration. Sometimes it succeeds to complete the snapshot within a short duration, but often it panics with the backup <literal>PartiallyFailed</literal> with the following error: <literal>plugin panicked: runtime error: invalid memory address or nil pointer dereference</literal>.</para>
</formalpara>
<formalpara>
<title>Backup is marked as PartiallyFailed when volumeSnapshotContent CR has an error</title>
<para>If any of the <literal>VolumeSnapshotContent</literal> CRs have an error related to removing the <literal>VolumeSnapshotBeingCreated</literal> annotation, it moves the backup to the <literal>WaitingForPluginOperationsPartiallyFailed</literal> phase. <link xlink:href="https://issues.redhat.com/browse/OADP-2871">OADP-2871</link></para>
</formalpara>
<formalpara>
<title>Performance issues when restoring 30,000 resources for the first time</title>
<para>When restoring 30,000 resources for the first time, without an existing-resource-policy, it takes twice as long to restore them, than it takes during the second and third try with an existing-resource-policy set to <literal>update</literal>. <link xlink:href="https://issues.redhat.com/browse/OADP-3071">OADP-3071</link></para>
</formalpara>
<formalpara>
<title>Post restore hooks might start running before Datadownload operation has released the related PV</title>
<para>Due to the asynchronous nature of the Data Mover operation, a post-hook might be attempted before the related pods persistent volumes (PVs) are released by the Data Mover persistent volume claim (PVC).</para>
</formalpara>
<formalpara>
<title>GCP-Workload Identity Federation VSL backup PartiallyFailed</title>
<para>VSL backup <literal>PartiallyFailed</literal> when GCP workload identity is configured on GCP.</para>
</formalpara>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12422838">OADP 1.3.0 known issues</link> in Jira.</simpara>
</section>
<section xml:id="upgrade-notes-1-3-0_oadp-release-notes">
<title>Upgrade notes</title>
<note>
<simpara>Always upgrade to the next minor version. <emphasis role="strong">Do not</emphasis> skip versions. To update to a later version, upgrade only one channel at a time. For example, to upgrade from OpenShift API for Data Protection (OADP) 1.1 to 1.3, upgrade first to 1.2, and then to 1.3.</simpara>
</note>
<section xml:id="changes-oadp-1-2-to-1-3_oadp-release-notes">
<title>Changes from OADP 1.2 to 1.3</title>
<simpara>The Velero server has been updated from version 1.11 to 1.12.</simpara>
<simpara>OpenShift API for Data Protection (OADP) 1.3 uses the Velero built-in Data Mover instead of the VolumeSnapshotMover (VSM) or the Volsync Data Mover.</simpara>
<simpara>This changes the following:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>spec.features.dataMover</literal> field and the VSM plugin are not compatible with OADP 1.3, and you must remove the configuration from the <literal>DataProtectionApplication</literal> (DPA) configuration.</simpara>
</listitem>
<listitem>
<simpara>The Volsync Operator is no longer required for Data Mover functionality, and you can remove it.</simpara>
</listitem>
<listitem>
<simpara>The custom resource definitions <literal>volumesnapshotbackups.datamover.oadp.openshift.io</literal> and <literal>volumesnapshotrestores.datamover.oadp.openshift.io</literal> are no longer required, and you can remove them.</simpara>
</listitem>
<listitem>
<simpara>The secrets used for the OADP-1.2 Data Mover are no longer required, and you can remove them.</simpara>
</listitem>
</itemizedlist>
<simpara>OADP 1.3 supports Kopia, which is an alternative file system backup tool to Restic.</simpara>
<itemizedlist>
<listitem>
<simpara>To employ Kopia, use the new <literal>spec.configuration.nodeAgent</literal> field as shown in the following example:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    nodeAgent:
      enable: true
      uploaderType: kopia
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>The <literal>spec.configuration.restic</literal> field is deprecated in OADP 1.3 and will be removed in a future version of OADP. To avoid seeing deprecation warnings, remove the <literal>restic</literal> key and its values, and use the following new syntax:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    nodeAgent:
      enable: true
      uploaderType: restic
# ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
<note>
<simpara>In a future OADP release, it is planned that the <literal>kopia</literal> tool will become the default <literal>uploaderType</literal> value.</simpara>
</note>
</section>
<section xml:id="upgrade-steps-1-3-0_oadp-release-notes">
<title>Upgrading steps</title>

</section>
<section xml:id="oadp-upgrade-from-oadp-data-mover-1-2-0_oadp-release-notes">
<title>Upgrading from OADP 1.2 Technology Preview Data Mover</title>
<simpara>OpenShift API for Data Protection (OADP) 1.2 Data Mover backups <emphasis role="strong">cannot</emphasis> be restored with OADP 1.3. To prevent a gap in the data protection of your applications, complete the following steps before upgrading to OADP 1.3:</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If your cluster backups are sufficient and Container Storage Interface (CSI) storage is available,
back up the applications with a CSI backup.</simpara>
</listitem>
<listitem>
<simpara>If you require off cluster backups:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Back up the applications with a file system backup that uses the <literal>--default-volumes-to-fs-backup=true or backup.spec.defaultVolumesToFsBackup</literal> options.</simpara>
</listitem>
<listitem>
<simpara>Back up the applications with your object storage plugins, for example, <literal>velero-plugin-for-aws</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<note>
<simpara>To restore OADP 1.2 Data Mover backup, you must uninstall OADP, and install and configure OADP 1.2.</simpara>
</note>
</section>
<section xml:id="oadp-backing-up-dpa-configuration-1-3-0_oadp-release-notes">
<title>Backing up the DPA configuration</title>
<simpara>You must back up your current <literal>DataProtectionApplication</literal> (DPA) configuration.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Save your current DPA configuration by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa -n openshift-adp -o yaml &gt; dpa.orig.backup</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-upgrading-dpa-operator-1-3-0_oadp-release-notes">
<title>Upgrading the OADP Operator</title>
<simpara>Use the following sequence when upgrading the OpenShift API for Data Protection (OADP) Operator.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Change your subscription channel for the OADP Operator from <literal>stable-1.2</literal> to <literal>stable-1.3</literal>.</simpara>
</listitem>
<listitem>
<simpara>Allow time for the Operator and containers to update and restart.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../../operators/admin/olm-upgrading-operators.xml#olm-changing-update-channel_olm-upgrading-operators">Updating installed Operators</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-converting-dpa-to-new-version-1-3-0_oadp-release-notes">
<title>Converting DPA to the new version</title>
<simpara>If you need to move backups off cluster with the Data Mover, reconfigure the <literal>DataProtectionApplication</literal> (DPA) manifest as follows.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Provided APIs</emphasis> section, click <emphasis role="strong">View more</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> to display the current DPA parameters.</simpara>
<formalpara>
<title>Example current DPA</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    features:
      dataMover:
      enable: true
      credentialName: dm-credentials
    velero:
      defaultPlugins:
      - vsm
      - csi
      - openshift
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Update the DPA parameters:</simpara>
<itemizedlist>
<listitem>
<simpara>Remove the <literal>features.dataMover</literal> key and values from the DPA.</simpara>
</listitem>
<listitem>
<simpara>Remove the VolumeSnapshotMover (VSM) plugin.</simpara>
</listitem>
<listitem>
<simpara>Add the <literal>nodeAgent</literal> key and values.</simpara>
<formalpara>
<title>Example updated DPA</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    nodeAgent:
      enable: true
      uploaderType: kopia
    velero:
      defaultPlugins:
      - csi
      - openshift
# ...</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Wait for the DPA to reconcile successfully.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="verifying-upgrade-1-3-0_oadp-release-notes">
<title>Verifying the upgrade</title>
<simpara>Use the following procedure to verify the upgrade.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<simpara>In OADP 1.3 you can start data movement off cluster per backup versus creating a <literal>DataProtectionApplication</literal> (DPA) configuration.</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero backup create example-backup --include-namespaces mysql-persistent --snapshot-move-data=true</programlisting>
</para>
</formalpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: example-backup
  namespace: openshift-adp
spec:
  snapshotMoveData: true
  includedNamespaces:
  - mysql-persistent
  storageLocation: dpa-sample-1
  ttl: 720h0m0s
# ...</programlisting>
</para>
</formalpara>
</section>
</section>
</section>
</section>
<section xml:id="oadp-release-notes-1-2">
<title>OADP 1.2 release notes</title>

<simpara>The release notes for OpenShift API for Data Protection (OADP) 1.2 describe new features and enhancements, deprecated features, product recommendations, known issues, and resolved issues.</simpara>
<section xml:id="migration-oadp-release-notes-1-2-3_oadp-release-notes">
<title>OADP 1.2.3 release notes</title>
<section xml:id="new-features-1-2-3_oadp-release-notes">
<title>New features</title>
<simpara>There are no new features in the release of OpenShift API for Data Protection (OADP) 1.2.3.</simpara>
</section>
<section xml:id="resolved-issues-1-2-3_oadp-release-notes">
<title>Resolved issues</title>
<simpara>The following highlighted issues are resolved in OADP 1.2.3:</simpara>
<formalpara>
<title>Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</title>
<para>In previous releases of OADP 1.2, the HTTP/2 protocol was susceptible to a denial of service attack because request cancellation could reset multiple streams quickly. The server had to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection. This resulted in a denial of service due to server resource consumption. For a list of all OADP issues associated with this CVE, see the following <link xlink:href="https://issues.redhat.com/browse/OADP-2868?filter=12421248">Jira list</link>.</para>
</formalpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</link>.</simpara>
<simpara>For a complete list of all issues resolved in the release of OADP 1.2.3, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-2094?filter=12422262">OADP 1.2.3 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues-1-2-3_oadp-release-notes">
<title>Known issues</title>
<simpara>There are no known issues in the release of OADP 1.2.3.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-2-2_oadp-release-notes">
<title>OADP 1.2.2 release notes</title>
<section xml:id="new-features-1-2-2_oadp-release-notes">
<title>New features</title>
<simpara>There are no new features in the release of OpenShift API for Data Protection (OADP) 1.2.2.</simpara>
</section>
<section xml:id="resolved-issues-1-2-2_oadp-release-notes">
<title>Resolved issues</title>
<simpara>The following highlighted issues are resolved in OADP 1.2.2:</simpara>
<formalpara>
<title>Restic restore partially failed due to a Pod Security standard</title>
<para>In previous releases of OADP 1.2, OpenShift Container Platform 4.14 enforced a pod security admission (PSA) policy that hindered the readiness of pods during a Restic restore process.</para>
</formalpara>
<simpara>This issue has been resolved in the release of OADP 1.2.2, and also OADP 1.1.6. Therefore, it is recommended that users upgrade to these releases.</simpara>
<simpara>For more information, see <link xlink:href="https://docs.openshift.com/container-platform/4.14/backup_and_restore/application_backup_and_restore/troubleshooting.html#oadp-restic-restore-failing-psa-policy_oadp-troubleshooting">Restic restore partially failing on OCP 4.14 due to changed PSA policy</link>. <link xlink:href="https://issues.redhat.com/browse/OADP-2094">(OADP-2094)</link></simpara>
<formalpara>
<title>Backup of an app with internal images partially failed with plugin panicked error</title>
<para>In previous releases of OADP 1.2, the backup of an application with internal images partially failed with plugin panicked error returned. The backup partially fails with this error in the Velero logs:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">time="2022-11-23T15:40:46Z" level=info msg="1 errors encountered backup up item" backup=openshift-adp/django-persistent-67a5b83d-6b44-11ed-9cba-902e163f806c logSource="/remote-source/velero/app/pkg/backup/backup.go:413" name=django-psql-persistent
time="2022-11-23T15:40:46Z" level=error msg="Error backing up item" backup=openshift-adp/django-persistent-67a5b83d-6b44-11ed-9cba-902e163f8</programlisting>
<simpara>This issue has been resolved in OADP 1.2.2. <link xlink:href="https://issues.redhat.com/browse/OADP-1057">(OADP-1057)</link>.</simpara>
<formalpara>
<title>ACM cluster restore was not functioning as expected due to restore order</title>
<para>In previous releases of OADP 1.2, ACM cluster restore was not functioning as expected due to restore order. ACM applications were removed and re-created on managed clusters after restore activation. <link xlink:href="https://issues.redhat.com/browse/OADP-2505">(OADP-2505)</link></para>
</formalpara>
<formalpara>
<title>VM&#8217;s using filesystemOverhead failed when backing up and restoring due to volume size mismatch</title>
<para>In previous releases of OADP 1.2, due to storage provider implementation choices, whenever there was a difference between the application persistent volume claims (PVCs) storage request and the snapshot size of the same PVC, VM&#8217;s using filesystemOverhead failed when backing up and restoring. This issue has been resolved in the Data Mover of OADP 1.2.2. <link xlink:href="https://issues.redhat.com/browse/OADP-2144">(OADP-2144)</link></para>
</formalpara>
<formalpara>
<title>OADP did not contain an option to set VolSync replication source prune interval</title>
<para>In previous releases of OADP 1.2, there was no option to set the VolSync replication source <literal>pruneInterval</literal>. <link xlink:href="https://issues.redhat.com/browse/OADP-2052">(OADP-2052)</link></para>
</formalpara>
<formalpara>
<title>Possible pod volume backup failure if Velero was installed in multiple namespaces</title>
<para>In previous releases of OADP 1.2, there was a possibility of pod volume backup failure if Velero was installed in multiple namespaces. <link xlink:href="https://issues.redhat.com/browse/OADP-2409">(OADP-2409)</link></para>
</formalpara>
<formalpara>
<title>Backup Storage Locations moved to unavailable phase when VSL uses custom secret</title>
<para>In previous releases of OADP 1.2, Backup Storage Locations moved to unavailable phase when Volume Snapshot Location used custom secret. <link xlink:href="https://issues.redhat.com/browse/OADP-1737">(OADP-1737)</link></para>
</formalpara>
<simpara>For a complete list of all issues resolved in the release of OADP 1.2.2, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-2094?filter=12422262">OADP 1.2.2 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues-1-2-2_oadp-release-notes">
<title>Known issues</title>
<simpara>The following issues have been highlighted as known issues in the release of OADP 1.2.2:</simpara>
<formalpara>
<title>Must-gather command fails to remove ClusterRoleBinding resources</title>
<para>The <literal>oc adm must-gather</literal> command fails to remove <literal>ClusterRoleBinding</literal> resources, which are left on cluster due to admission webhook. Therefore, requests for the removal of the <literal>ClusterRoleBinding</literal> resources are denied. <link xlink:href="https://issues.redhat.com/browse/OADP-2773">(OADP-27730)</link></para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">admission webhook "clusterrolebindings-validation.managed.openshift.io" denied the request: Deleting ClusterRoleBinding must-gather-p7vwj is not allowed</programlisting>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-2773?filter=12422263">OADP 1.2.2 known issues</link> in Jira.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-2-1_oadp-release-notes">
<title>OADP 1.2.1 release notes</title>
<section xml:id="new-features-1-2-1_oadp-release-notes">
<title>New features</title>
<simpara>There are no new features in the release of OpenShift API for Data Protection (OADP) 1.2.1.</simpara>
</section>
<section xml:id="resolved-issues-1-2-1_oadp-release-notes">
<title>Resolved issues</title>
<simpara>For a complete list of all issues resolved in the release of OADP 1.2.1, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12417849">OADP 1.2.1 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues-1-2-1_oadp-release-notes">
<title>Known issues</title>
<simpara>The following issues have been highlighted as known issues in the release of OADP 1.2.1:</simpara>
<formalpara>
<title>DataMover Restic retain and prune policies do not work as expected</title>
<para>The retention and prune features provided by VolSync and Restic are not working as expected. Because there is no working option to set the prune interval on VolSync replication, you have to manage and prune remotely stored backups on S3 storage outside of OADP. For more details, see:</para>
</formalpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-2052">OADP-2052</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-2048">OADP-2048</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-2175">OADP-2175</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-1690">OADP-1690</link></simpara>
</listitem>
</itemizedlist>
<important>
<simpara>OADP Data Mover is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-2257?filter=12418892">OADP 1.2.1 known issues</link> in Jira.</simpara>
</section>
</section>
<section xml:id="oadp-release-notes-1-2-0_oadp-release-notes">
<title>OADP 1.2.0 release notes</title>
<simpara>The OADP 1.2.0 release notes include information about new features, bug fixes, and known issues.</simpara>
<section xml:id="new-features_oadp-release-notes">
<title>New features</title>
<formalpara>
<title>Resource timeouts</title>
<para>The new <literal>resourceTimeout</literal> option specifies the timeout duration in minutes for waiting on various Velero resources. This option applies to resources such as Velero CRD availability, <literal>volumeSnapshot</literal> deletion, and backup repository availability. The default duration is 10 minutes.</para>
</formalpara>
<formalpara>
<title>AWS S3 compatible backup storage providers</title>
<para>You can back up objects and snapshots on AWS S3 compatible providers.</para>
</formalpara>
<section xml:id="new-features-tech-preview-1-2-0_oadp-release-notes">
<title>Technical preview features</title>
<formalpara>
<title>Data Mover</title>
<para>The OADP Data Mover enables you to back up Container Storage Interface (CSI) volume snapshots to a remote object store. When you enable Data Mover, you can restore stateful applications using CSI volume snapshots pulled from the object store in case of accidental cluster deletion, cluster failure, or data corruption.</para>
</formalpara>
<important>
<simpara>OADP Data Mover is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
</section>
</section>
<section xml:id="fixed-bugs-1-2-0_oadp-release-notes">
<title>Resolved issues</title>
<simpara>For a complete list of all issues resolved in this release, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12418878">OADP 1.2.0 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues-1-2-0_oadp-release-notes">
<title>Known issues</title>
<simpara>The following issues have been highlighted as known issues in the release of OADP 1.2.0:</simpara>
<formalpara>
<title>Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</title>
<para>The HTTP/2 protocol is susceptible to a denial of service attack because request cancellation can reset multiple streams quickly. The server has to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection. This results in a denial of service due to server resource consumption.</para>
</formalpara>
<simpara>It is advised to upgrade to OADP 1.2.3, which resolves this issue.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</link>.</simpara>
<formalpara>
<title>An incorrect hostname can be created when changing a hostname in a generated route.</title>
<para>By default, the OpenShift Container Platform cluster makes sure that the <literal>openshift.io/host.generated: true</literal> annotation is turned on and fills in the field for both the routes that are generated and those that are not generated.</para>
</formalpara>
<simpara>You cannot modify the value for the <literal>.spec.host</literal> field based on the base domain name of your cluster in the generated and non-generated routes.</simpara>
<simpara>If you modify the value for the <literal>.spec.host</literal> field, it is not possible to restore the default value that was generated by the OpenShift Container Platform cluster. After you restore your OpenShift Container Platform cluster, the Operator resets the value for the field.</simpara>
</section>
<section xml:id="Upgrade-notes-1-2-0_oadp-release-notes">
<title>Upgrade notes</title>
<note>
<simpara>Always upgrade to the next minor version. <emphasis role="strong">Do not</emphasis> skip versions. To update to a later version, upgrade only one channel at a time. For example, to upgrade from OpenShift API for Data Protection (OADP) 1.1 to 1.3, upgrade first to 1.2, then to 1.3.</simpara>
</note>
<section xml:id="changes-oadp-1-1-to-1-2_oadp-release-notes">
<title>Changes from OADP 1.1 to 1.2</title>
<simpara>The Velero server was updated from version 1.9 to 1.11.</simpara>
<simpara>In OADP 1.2, the <literal>DataProtectionApplication</literal> (DPA) configuration <literal>spec.configuration.velero.args</literal> has the following changes:</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>default-volumes-to-restic</literal> field was renamed to <literal>default-volumes-to-fs-backup</literal>. If you use <literal>spec.velero</literal>, you must add it again with the new name to your DPA after upgrading OADP.</simpara>
</listitem>
<listitem>
<simpara>The <literal>default-volumes-to-restic</literal> field was renamed to <literal>default-volumes-to-fs-backup</literal>. If you use <literal>spec.velero</literal>, you must add it again with the new name to your DPA after upgrading OADP.</simpara>
</listitem>
<listitem>
<simpara>The <literal>restic-timeout</literal> field was renamed to <literal>fs-backup-timeout</literal>. If you use <literal>spec.velero</literal>, you must add it again with the new name to your DPA after upgrading OADP.</simpara>
</listitem>
<listitem>
<simpara>The <literal>restic</literal> daemon set was renamed to <literal>node-agent</literal>. OADP automatically updates the name of the daemon set.</simpara>
</listitem>
<listitem>
<simpara>The custom resource definition <literal>resticrepositories.velero.io</literal> was renamed to <literal>backuprepositories.velero.io</literal>.</simpara>
</listitem>
<listitem>
<simpara>The custom resource definition <literal>resticrepositories.velero.io</literal> can be removed from the cluster.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="upgrade-steps-1-2-0_oadp-release-notes">
<title>Upgrading steps</title>

</section>
<section xml:id="oadp-backing-up-dpa-configuration-1-2-0_oadp-release-notes">
<title>Backing up the DPA configuration</title>
<simpara>You must back up your current <literal>DataProtectionApplication</literal> (DPA) configuration.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Save your current DPA configuration by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa -n openshift-adp -o yaml &gt; dpa.orig.backup</programlisting>
</para>
</formalpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-upgrading-dpa-operator-1-2-0_oadp-release-notes">
<title>Upgrading the OADP Operator</title>
<simpara>Use the following sequence when upgrading the OpenShift API for Data Protection (OADP) Operator.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Change your subscription channel for the OADP Operator from <literal>stable-1.1</literal> to <literal>stable-1.2</literal>.</simpara>
</listitem>
<listitem>
<simpara>Allow time for the Operator and containers to update and restart.</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.xml#migration-configuring-aws-s3_installing-oadp-aws">Configuring Amazon Web Services</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/oadp-using-data-mover-for-csi-snapshots-doc.xml#oadp-using-data-mover-for-csi-snapshots-doc">Using Data Mover for CSI snapshots</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../operators/admin/olm-upgrading-operators.xml#olm-changing-update-channel_olm-upgrading-operators">Updating installed Operators</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-converting-to-new-dpa-1-2-0_oadp-release-notes">
<title>Converting DPA to the new version</title>
<simpara>If you use the fields that were updated in the <literal>spec.configuration.velero.args</literal> stanza, you must configure your <literal>DataProtectionApplication</literal> (DPA) manifest to use the new parameter names.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> → <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Select <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> to display the current DPA parameters.</simpara>
<formalpara>
<title>Example current DPA</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    velero:
      args:
        default-volumes-to-restic: true
        default-restic-prune-frequency: 6000
        restic-timeout: 600
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Update the DPA parameters:</simpara>
</listitem>
<listitem>
<simpara>Update the DPA parameter names without changing their values:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Change the <literal>default-volumes-to-restic</literal> key to <literal>default-volumes-to-fs-backup</literal>.</simpara>
</listitem>
<listitem>
<simpara>Change the <literal>default-restic-prune-frequency</literal> key to <literal>default-repo-maintain-frequency</literal>.</simpara>
</listitem>
<listitem>
<simpara>Change the <literal>restic-timeout</literal> key to <literal>fs-backup-timeout</literal>.</simpara>
</listitem>
</orderedlist>
<formalpara>
<title>Example updated DPA</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    velero:
      args:
        default-volumes-to-fs-backup: true
        default-repo-maintain-frequency: 6000
        fs-backup-timeout: 600
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Wait for the DPA to reconcile successfully.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="verifying-upgrade-1-2-0_oadp-release-notes">
<title>Verifying the upgrade</title>
<simpara>Use the following procedure to verify the upgrade.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
<section xml:id="oadp-release-notes-1-1">
<title>OADP 1.1 release notes</title>

<simpara>The release notes for OpenShift API for Data Protection (OADP) 1.1 describe new features and enhancements, deprecated features, product recommendations, known issues, and resolved issues.</simpara>
<section xml:id="migration-oadp-release-notes-1-1-7_oadp-release-notes">
<title>OADP 1.1.7 release notes</title>
<simpara>The OADP 1.1.7 release notes lists any resolved issues and known issues.</simpara>
<section xml:id="resolved-issues1.1.7_oadp-release-notes">
<title>Resolved issues</title>
<simpara>The following highlighted issues are resolved in OADP 1.1.7:</simpara>
<formalpara>
<title>Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</title>
<para>In previous releases of OADP 1.1, the HTTP/2 protocol was susceptible to a denial of service attack because request cancellation could reset multiple streams quickly. The server had to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection. This resulted in a denial of service due to server resource consumption. For a list of all OADP issues associated with this CVE, see the following <link xlink:href="https://issues.redhat.com/browse/OADP-2868?filter=12421248">Jira list</link>.</para>
</formalpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</link>.</simpara>
<simpara>For a complete list of all issues resolved in the release of OADP 1.1.7, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-2094?filter=12422262">OADP 1.1.7 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues1.1.7_oadp-release-notes">
<title>Known issues</title>
<simpara>There are no known issues in the release of OADP 1.1.7.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-1-6_oadp-release-notes">
<title>OADP 1.1.6 release notes</title>
<simpara>The OADP 1.1.6 release notes lists any new features, resolved issues and bugs, and known issues.</simpara>
<section xml:id="resolved-issues1.1.6_oadp-release-notes">
<title>Resolved issues</title>
<formalpara>
<title>Restic restore partially failing due to Pod Security standard</title>
<para>OCP 4.14 introduced pod security standards that meant the <literal>privileged</literal> profile is <literal>enforced</literal>. In previous releases of OADP, this profile caused the pod to receive <literal>permission denied</literal> errors. This issue was caused because of the restore order. The pod was created before the security context constraints (SCC) resource. As this pod violated the pod security standard, the pod was denied and subsequently failed. <link xlink:href="https://issues.redhat.com/browse/OADP-2420">OADP-2420</link></para>
</formalpara>
<formalpara>
<title>Restore partially failing for job resource</title>
<para>In previous releases of OADP, the restore of job resource was partially failing in OCP 4.14. This issue was not seen in older OCP versions. The issue was caused by an additional label being to the job resource, which was not present in older OCP versions. <link xlink:href="https://issues.redhat.com/browse/OADP-2530">OADP-2530</link></para>
</formalpara>
<simpara>For a complete list of all issues resolved in this release, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12420897">OADP 1.1.6 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues1.1.6_oadp-release-notes">
<title>Known issues</title>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-2688?filter=12421263">OADP 1.1.6 known issues</link> in Jira.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-1-5_oadp-release-notes">
<title>OADP 1.1.5 release notes</title>
<simpara>The OADP 1.1.5 release notes lists any new features, resolved issues and bugs, and known issues.</simpara>
<section xml:id="new-features1.1.5_oadp-release-notes">
<title>New features</title>
<simpara>This version of OADP is a service release. No new features are added to this version.</simpara>
</section>
<section xml:id="resolved-issues1.1.5_oadp-release-notes">
<title>Resolved issues</title>
<simpara>For a complete list of all issues resolved in this release, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12418875">OADP 1.1.5 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues1.1.5_oadp-release-notes">
<title>Known issues</title>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-1057?filter=12421178">OADP 1.1.5 known issues</link> in Jira.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-1-4_oadp-release-notes">
<title>OADP 1.1.4 release notes</title>
<simpara>The OADP 1.1.4 release notes lists any new features, resolved issues and bugs, and known issues.</simpara>
<section xml:id="new-features1.1.4_oadp-release-notes">
<title>New features</title>
<simpara>This version of OADP is a service release. No new features are added to this version.</simpara>
</section>
<section xml:id="resolved-issues1.1.4_oadp-release-notes">
<title>Resolved issues</title>
<formalpara>
<title>Add support for all the velero deployment server arguments</title>
<para>In previous releases of OADP, OADP did not facilitate the support of all the upstream Velero server arguments. This issue has been resolved in OADP 1.1.4 and all the upstream Velero server arguments are supported. <link xlink:href="https://issues.redhat.com/browse/OADP-1557">OADP-1557</link></para>
</formalpara>
<formalpara>
<title>Data Mover can restore from an incorrect snapshot when there was more than one VSR for the restore name and pvc name</title>
<para>In previous releases of OADP, OADP Data Mover could restore from an incorrect snapshot if there was more than one Volume Snapshot Restore (VSR) resource in the cluster for the same Velero <literal>restore</literal> name and PersistentVolumeClaim (pvc) name. <link xlink:href="https://issues.redhat.com/browse/OADP-1822">OADP-1822</link></para>
</formalpara>
<formalpara>
<title>Cloud Storage API BSLs need OwnerReference</title>
<para>In previous releases of OADP, ACM BackupSchedules failed validation because of a missing <literal>OwnerReference</literal> on Backup Storage Locations (BSLs) created with <literal>dpa.spec.backupLocations.bucket</literal>. <link xlink:href="https://issues.redhat.com/browse/OADP-1511">OADP-1511</link></para>
</formalpara>
<simpara>For a complete list of all issues resolved in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-1557?filter=12420906">OADP 1.1.4 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues1.1.4_oadp-release-notes">
<title>Known issues</title>
<simpara>This release has the following known issues:</simpara>
<formalpara>
<title>OADP backups might fail because a UID/GID range might have changed on the cluster</title>
<para>OADP backups might fail because a UID/GID range might have changed on the cluster where the application has been restored, with the result that OADP does not back up and restore OpenShift Container Platform UID/GID range metadata. To avoid the issue, if the backed application requires a specific UUID, ensure the range is available when restored. An additional workaround is to allow OADP to create the namespace in the restore operation.</para>
</formalpara>
<formalpara>
<title>A restoration might fail if ArgoCD is used during the process due to a label used by ArgoCD</title>
<para>A restoration might fail if ArgoCD is used during the process due to a label used by ArgoCD, <literal>app.kubernetes.io/instance</literal>. This label identifies which resources ArgoCD needs to manage, which can create a conflict with OADP&#8217;s procedure for managing resources on restoration. To work around this issue, set <literal>.spec.resourceTrackingMethod</literal> on the ArgoCD YAML to <literal>annotation+label</literal> or <literal>annotation</literal>. If the issue continues to persist, then disable ArgoCD before beginning to restore, and enable it again when restoration is finished.</para>
</formalpara>
<formalpara>
<title>OADP Velero plugins returning "received EOF, stopping recv loop" message</title>
<para>Velero plugins are started as separate processes. When the Velero operation has completed, either successfully or not, they exit. Therefore if you see a <literal>received EOF, stopping recv loop</literal> messages in debug logs, it does not mean an error occurred. The message indicates that a plugin operation has completed. <link xlink:href="https://issues.redhat.com/browse/OADP-2176">OADP-2176</link></para>
</formalpara>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-1057?filter=12420908">OADP 1.1.4 known issues</link> in Jira.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-1-3_oadp-release-notes">
<title>OADP 1.1.3 release notes</title>
<simpara>The OADP 1.1.3 release notes lists any new features, resolved issues and bugs, and known issues.</simpara>
<section xml:id="new-features1.1.3_oadp-release-notes">
<title>New features</title>
<simpara>This version of OADP is a service release. No new features are added to this version.</simpara>
</section>
<section xml:id="resolved-issues1.1.3_oadp-release-notes">
<title>Resolved issues</title>
<simpara>For a complete list of all issues resolved in this release, see the list of <link xlink:href="https://issues.redhat.com/issues/?filter=12418876">OADP 1.1.3 resolved issues</link> in Jira.</simpara>
</section>
<section xml:id="known-issues1.1.3_oadp-release-notes">
<title>Known issues</title>
<simpara>For a complete list of all known issues in this release, see the list of <link xlink:href="https://issues.redhat.com/browse/OADP-1057?filter=12421175">OADP 1.1.3 known issues</link> in Jira.</simpara>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-1-2_oadp-release-notes">
<title>OADP 1.1.2 release notes</title>
<simpara>The OADP 1.1.2 release notes include product recommendations, a list of fixed bugs and descriptions of known issues.</simpara>
<section xml:id="product-recommendations_oadp-release-notes">
<title>Product recommendations</title>
<formalpara>
<title>VolSync</title>
<para>To prepare for the upgrade from VolSync 0.5.1 to the latest version available from the VolSync <emphasis role="strong">stable</emphasis> channel,  you must add this annotation in the <literal>openshift-adp</literal> namespace by running the following command:</para>
</formalpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc annotate --overwrite namespace/openshift-adp volsync.backube/privileged-movers='true'</programlisting>
<formalpara>
<title>Velero</title>
<para>In this release, Velero has been upgraded from version 1.9.2 to version <link xlink:href="https://github.com/vmware-tanzu/velero/releases/tag/v1.9.5">1.9.5</link>.</para>
</formalpara>
<formalpara>
<title>Restic</title>
<para>In this release, Restic has been upgraded from version 0.13.1 to version <link xlink:href="https://github.com/restic/restic/releases/tag/v0.14.0">0.14.0</link>.</para>
</formalpara>
</section>
<section xml:id="fixed-bugs_oadp-release-notes">
<title>Resolved issues</title>
<simpara>The following issues have been resolved in this release:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-1150">OADP-1150</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-290">OADP-290</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/OADP-1056">OADP-1056</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="known-issues_oadp-release-notes">
<title>Known issues</title>
<simpara>This release has the following known issues:</simpara>
<itemizedlist>
<listitem>
<simpara>OADP currently does not support backup and restore of AWS EFS volumes using restic in Velero (<link xlink:href="https://issues.redhat.com/browse/OADP-778"><emphasis role="strong">OADP-778</emphasis></link>).</simpara>
</listitem>
<listitem>
<simpara>CSI backups might fail due to a Ceph limitation of <literal>VolumeSnapshotContent</literal> snapshots per PVC.</simpara>
<simpara>You can create many snapshots of the same persistent volume claim (PVC) but cannot schedule periodic creation of snapshots:</simpara>
<itemizedlist>
<listitem>
<simpara>For CephFS, you can create up to 100 snapshots per PVC. (<link xlink:href="https://issues.redhat.com/browse/OADP-804"><emphasis role="strong">OADP-804</emphasis></link>)</simpara>
</listitem>
<listitem>
<simpara>For RADOS Block Device (RBD), you can create up to 512 snapshots for each PVC. (<link xlink:href="https://issues.redhat.com/browse/OADP-975"><emphasis role="strong">OADP-975</emphasis></link>)</simpara>
</listitem>
</itemizedlist>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.11/html/managing_and_allocating_storage_resources/volume-snapshots_rhodf">Volume Snapshots</link>.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migration-oadp-release-notes-1-1-1_oadp-release-notes">
<title>OADP 1.1.1 release notes</title>
<simpara>The OADP 1.1.1 release notes include product recommendations and descriptions of known issues.</simpara>
<section xml:id="_product_recommendations">
<title>Product recommendations</title>
<simpara>Before you install OADP 1.1.1, it is recommended to either install VolSync 0.5.1 or to upgrade to it.</simpara>
</section>
<section xml:id="_known_issues">
<title>Known issues</title>
<simpara>This release has the following known issues:</simpara>
<itemizedlist>
<listitem>
<simpara>Multiple HTTP/2 enabled web servers are vulnerable to a DDoS attack (Rapid Reset Attack)</simpara>
<simpara>The HTTP/2 protocol is susceptible to a denial of service attack because request cancellation can reset multiple streams quickly. The server has to set up and tear down the streams while not hitting any server-side limit for the maximum number of active streams per connection. This results in a denial of service due to server resource consumption. For a list of all OADP issues associated with this CVE, see the following <link xlink:href="https://issues.redhat.com/browse/OADP-2868?filter=12421248">Jira list</link>.</simpara>
<simpara>It is advised to upgrade to OADP 1.1.7 or 1.2.3, which resolve this issue.</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/security/cve/cve-2023-39325">CVE-2023-39325 (Rapid Reset Attack)</link>.</simpara>
</listitem>
<listitem>
<simpara>OADP currently does not support backup and restore of AWS EFS volumes using restic in Velero (<link xlink:href="https://issues.redhat.com/browse/OADP-778"><emphasis role="strong">OADP-778</emphasis></link>).</simpara>
</listitem>
<listitem>
<simpara>CSI backups might fail due to a Ceph limitation of <literal>VolumeSnapshotContent</literal> snapshots per PVC.</simpara>
<simpara>You can create many snapshots of the same persistent volume claim (PVC) but cannot schedule periodic creation of snapshots:</simpara>
<itemizedlist>
<listitem>
<simpara>For CephFS, you can create up to 100 snapshots per PVC.</simpara>
</listitem>
<listitem>
<simpara>For RADOS Block Device (RBD), you can create up to 512 snapshots for each PVC. (<link xlink:href="https://issues.redhat.com/browse/OADP-804"><emphasis role="strong">OADP-804</emphasis></link>) and (<link xlink:href="https://issues.redhat.com/browse/OADP-975"><emphasis role="strong">OADP-975</emphasis></link>)</simpara>
<simpara>For more information, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.11/html/managing_and_allocating_storage_resources/volume-snapshots_rhodf">Volume Snapshots</link>.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</section>
<section xml:id="oadp-features-plugins">
<title>OADP features and plugins</title>

<simpara>OpenShift API for Data Protection (OADP) features provide options for backing up and restoring applications.</simpara>
<simpara>The default plugins enable Velero to integrate with certain cloud providers and to back up and restore OpenShift Container Platform resources.</simpara>
<section xml:id="oadp-features_oadp-features-plugins">
<title>OADP features</title>
<simpara>OpenShift API for Data Protection (OADP) supports the following features:</simpara>
<variablelist>
<varlistentry>
<term>Backup</term>
<listitem>
<simpara>You can use OADP to back up all applications on the OpenShift Platform, or you can filter the resources by type, namespace, or label.</simpara>
<simpara>OADP backs up Kubernetes objects and internal images by saving them as an archive file on object storage. OADP backs up persistent volumes (PVs) by creating snapshots with the native cloud snapshot API or with the Container Storage Interface (CSI). For cloud providers that do not support snapshots, OADP backs up resources and PV data with Restic.</simpara>
<note>
<simpara>You must exclude Operators from the backup of an application for backup and restore to succeed.</simpara>
</note>
</listitem>
</varlistentry>
<varlistentry>
<term>Restore</term>
<listitem>
<simpara>You can restore resources and PVs from a backup. You can restore all objects in a backup or filter the objects by namespace, PV, or label.</simpara>
<note>
<simpara>You must exclude Operators from the backup of an application for backup and restore to succeed.</simpara>
</note>
</listitem>
</varlistentry>
<varlistentry>
<term>Schedule</term>
<listitem>
<simpara>You can schedule backups at specified intervals.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Hooks</term>
<listitem>
<simpara>You can use hooks to run commands in a container on a pod, for example, <literal>fsfreeze</literal> to freeze a file system. You can configure a hook to run before or after a backup or restore. Restore hooks can run in an init container or in the application container.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="oadp-plugins_oadp-features-plugins">
<title>OADP plugins</title>
<simpara>The OpenShift API for Data Protection (OADP) provides default Velero plugins that are integrated with storage providers to support backup and snapshot operations. You can create <link xlink:href="https://velero.io/docs/v1.12/custom-plugins/">custom plugins</link> based on the Velero plugins.</simpara>
<simpara>OADP also provides plugins for OpenShift Container Platform resource backups, OpenShift Virtualization resource backups, and Container Storage Interface (CSI) snapshots.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>OADP plugins</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">OADP plugin</entry>
<entry align="left" valign="top">Function</entry>
<entry align="left" valign="top">Storage location</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top" morerows="1"><simpara><literal>aws</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Backs up and restores Kubernetes objects.</simpara></entry>
<entry align="left" valign="top"><simpara>AWS S3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Backs up and restores volumes with snapshots.</simpara></entry>
<entry align="left" valign="top"><simpara>AWS EBS</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara><literal>azure</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Backs up and restores Kubernetes objects.</simpara></entry>
<entry align="left" valign="top"><simpara>Microsoft Azure Blob storage</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Backs up and restores volumes with snapshots.</simpara></entry>
<entry align="left" valign="top"><simpara>Microsoft Azure Managed Disks</simpara></entry>
</row>
<row>
<entry align="left" valign="top" morerows="1"><simpara><literal>gcp</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Backs up and restores Kubernetes objects.</simpara></entry>
<entry align="left" valign="top"><simpara>Google Cloud Storage</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Backs up and restores volumes with snapshots.</simpara></entry>
<entry align="left" valign="top"><simpara>Google Compute Engine Disks</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>openshift</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Backs up and restores OpenShift Container Platform resources. <superscript>[1]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Object store</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kubevirt</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Backs up and restores OpenShift Virtualization resources. <superscript>[2]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Object store</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>csi</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Backs up and restores volumes with CSI snapshots. <superscript>[3]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Cloud storage that supports CSI snapshots</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>vsm</literal></simpara></entry>
<entry align="left" valign="top"><simpara>VolumeSnapshotMover relocates snapshots from the cluster into an object store to be used during a restore process to recover stateful applications, in situations such as cluster deletion. <superscript>[4]</superscript></simpara></entry>
<entry align="left" valign="top"><simpara>Object store</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>Mandatory.</simpara>
</listitem>
<listitem>
<simpara>Virtual machine disks are backed up with CSI snapshots or Restic.</simpara>
</listitem>
<listitem>
<simpara>The <literal>csi</literal> plugin uses the Kubernetes CSI snapshot API.</simpara>
<itemizedlist>
<listitem>
<simpara>OADP 1.1 or later uses <literal>snapshot.storage.k8s.io/v1</literal></simpara>
</listitem>
<listitem>
<simpara>OADP 1.0 uses <literal>snapshot.storage.k8s.io/v1beta1</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>OADP 1.2 only.</simpara>
</listitem>
</orderedlist>
</para>
</section>
<section xml:id="oadp-configuring-velero-plugins_oadp-features-plugins">
<title>About OADP Velero plugins</title>
<simpara>You can configure two types of plugins when you install Velero:</simpara>
<itemizedlist>
<listitem>
<simpara>Default cloud provider plugins</simpara>
</listitem>
<listitem>
<simpara>Custom plugins</simpara>
</listitem>
</itemizedlist>
<simpara>Both types of plugin are optional, but most users configure at least one cloud provider plugin.</simpara>
<section xml:id="_default_velero_cloud_provider_plugins">
<title>Default Velero cloud provider plugins</title>
<simpara>You can install any of the following default Velero cloud provider plugins when you configure the <literal>oadp_v1alpha1_dpa.yaml</literal> file during deployment:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>aws</literal> (Amazon Web Services)</simpara>
</listitem>
<listitem>
<simpara><literal>gcp</literal> (Google Cloud Platform)</simpara>
</listitem>
<listitem>
<simpara><literal>azure</literal> (Microsoft Azure)</simpara>
</listitem>
<listitem>
<simpara><literal>openshift</literal> (OpenShift Velero plugin)</simpara>
</listitem>
<listitem>
<simpara><literal>csi</literal> (Container Storage Interface)</simpara>
</listitem>
<listitem>
<simpara><literal>kubevirt</literal> (KubeVirt)</simpara>
</listitem>
</itemizedlist>
<simpara>You specify the desired default plugins in the <literal>oadp_v1alpha1_dpa.yaml</literal> file during deployment.</simpara>
<formalpara>
<title>Example file</title>
<para>The following <literal>.yaml</literal> file installs the <literal>openshift</literal>, <literal>aws</literal>, <literal>azure</literal>, and <literal>gcp</literal> plugins:</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered"> apiVersion: oadp.openshift.io/v1alpha1
 kind: DataProtectionApplication
 metadata:
   name: dpa-sample
 spec:
   configuration:
     velero:
       defaultPlugins:
       - openshift
       - aws
       - azure
       - gcp</programlisting>
</section>
<section xml:id="_custom_velero_plugins">
<title>Custom Velero plugins</title>
<simpara>You can install a custom Velero plugin by specifying the plugin <literal>image</literal> and <literal>name</literal> when you configure the <literal>oadp_v1alpha1_dpa.yaml</literal> file during deployment.</simpara>
<simpara>You specify the desired custom plugins in the <literal>oadp_v1alpha1_dpa.yaml</literal> file during deployment.</simpara>
<formalpara>
<title>Example file</title>
<para>The following <literal>.yaml</literal> file installs the default <literal>openshift</literal>, <literal>azure</literal>, and <literal>gcp</literal> plugins and a custom plugin that has the name <literal>custom-plugin-example</literal> and the image <literal>quay.io/example-repo/custom-velero-plugin</literal>:</para>
</formalpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: dpa-sample
spec:
 configuration:
   velero:
     defaultPlugins:
     - openshift
     - azure
     - gcp
     customPlugins:
     - name: custom-plugin-example
       image: quay.io/example-repo/custom-velero-plugin</programlisting>
</section>
<section xml:id="oadp-plugins-receiving-eof-message_oadp-features-plugins">
<title>Velero plugins returning "received EOF, stopping recv loop" message</title>
<note>
<simpara>Velero plugins are started as separate processes. After the Velero operation has completed, either successfully or not, they exit. Receiving a <literal>received EOF, stopping recv loop</literal> message in the debug logs indicates that a plugin operation has completed. It does not mean that an error has occurred.</simpara>
</note>
</section>
</section>
<section xml:id="oadp-supported-architecture_oadp-features-plugins">
<title>Supported architectures for OADP</title>
<simpara>OpenShift API for Data Protection (OADP) supports the following architectures:</simpara>
<itemizedlist>
<listitem>
<simpara>AMD64</simpara>
</listitem>
<listitem>
<simpara>ARM64</simpara>
</listitem>
<listitem>
<simpara>PPC64le</simpara>
</listitem>
<listitem>
<simpara>s390x</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>OADP 1.2.0 and later versions support the ARM64 architecture.</simpara>
</note>
</section>
<section xml:id="oadp-support-for-ibm-power-and-ibm-z">
<title>OADP support for IBM Power and IBM Z</title>
<simpara>OpenShift API for Data Protection (OADP) is platform neutral. The information that follows relates only to IBM Power&#174; and to IBM Z&#174;.</simpara>
<simpara>OADP 1.1.0 was tested successfully against OpenShift Container Platform 4.11 for both IBM Power&#174; and IBM Z&#174;. The sections that follow give testing and support information for OADP 1.1.0 in terms of backup locations for these systems.</simpara>
<section xml:id="oadp-ibm-power-test-matrix_oadp-features-plugins">
<title>OADP support for target backup locations using IBM Power</title>
<simpara>IBM Power&#174; running with OpenShift Container Platform 4.11 and 4.12, and OpenShift API for Data Protection (OADP) 1.1.2 was tested successfully against an AWS S3 backup location target. Although the test involved only an AWS S3 target, Red Hat supports running IBM Power&#174; with OpenShift Container Platform 4.11 and 4.12, and OADP 1.1.2 against all non-AWS S3 backup location targets as well.</simpara>
</section>
<section xml:id="oadp-ibm-z-test-support_oadp-features-plugins">
<title>OADP testing and support for target backup locations using IBM Z</title>
<simpara>IBM Z&#174; running with OpenShift Container Platform 4.11 and 4.12, and OpenShift API for Data Protection (OADP) 1.1.2 was tested successfully against an AWS S3 backup location target. Although the test involved only an AWS S3 target, Red Hat supports running IBM Z&#174; with OpenShift Container Platform 4.11 and 4.12, and OADP 1.1.2 against all non-AWS S3 backup location targets as well.</simpara>
</section>
</section>
<section xml:id="oadp-fips_oadp-features-plugins">
<title>OADP and FIPS</title>
<simpara>Federal Information Processing Standards (FIPS) are a set of computer security standards developed by the United States federal government in line with the Federal Information Security Management Act (FISMA).</simpara>
<simpara>OpenShift API for Data Protection (OADP) has been tested and works on FIPS-enabled OpenShift Container Platform clusters.</simpara>
</section>
</section>
<section xml:id="_installing_and_configuring_oadp">
<title>Installing and configuring OADP</title>
<section xml:id="about-installing-oadp">
<title>About installing OADP</title>

<simpara>As a cluster administrator, you install the OpenShift API for Data Protection (OADP) by installing the OADP Operator. The OADP Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<note>
<simpara>Starting from OADP 1.0.4, all OADP 1.0.<emphasis>z</emphasis> versions can only be used as a dependency of the MTC Operator and are not available as a standalone Operator.</simpara>
</note>
<simpara>To back up Kubernetes resources and internal images, you must have object storage as a backup location, such as one of the following storage types:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.xml#installing-oadp-aws">Amazon Web Services</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-azure.xml#installing-oadp-azure">Microsoft Azure</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-gcp.xml#installing-oadp-gcp">Google Cloud Platform</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-mcg.xml#installing-oadp-mcg">Multicloud Object Gateway</link></simpara>
</listitem>
<listitem>
<simpara>AWS S3 compatible object storage, such as Multicloud Object Gateway or MinIO</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The <literal>CloudStorage</literal> API, which automates the creation of a bucket for object storage, is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<note>
<simpara>The <literal>CloudStorage</literal> API is a Technology Preview feature when you use a <literal>CloudStorage</literal> object and want OADP to use the <literal>CloudStorage</literal> API to automatically create an S3 bucket for use as a <literal>BackupStorageLocation</literal>.</simpara>
<simpara>The <literal>CloudStorage</literal> API supports manually creating a <literal>BackupStorageLocation</literal> object by specifying an existing S3 bucket. The <literal>CloudStorage</literal> API that creates an S3 bucket automatically is currently only enabled for AWS S3 storage.</simpara>
</note>
<simpara>You can back up persistent volumes (PVs) by using snapshots or a File System Backup (FSB).</simpara>
<simpara>To back up PVs with snapshots, you must have a cloud provider that supports either a native snapshot API or Container Storage Interface (CSI) snapshots, such as one of the following cloud providers:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.xml#installing-oadp-aws">Amazon Web Services</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-azure.xml#installing-oadp-azure">Microsoft Azure</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-gcp.xml#installing-oadp-gcp">Google Cloud Platform</link></simpara>
</listitem>
<listitem>
<simpara>CSI snapshot-enabled cloud provider, such as <link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-ocs.xml#installing-oadp-ocs">OpenShift Data Foundation</link></simpara>
</listitem>
</itemizedlist>
<note>
<simpara>If you want to use CSI backup on OCP 4.11 and later, install OADP 1.1.<emphasis>x</emphasis>.</simpara>
<simpara>OADP 1.0.<emphasis>x</emphasis> does not support CSI backup on OCP 4.11 and later. OADP 1.0.<emphasis>x</emphasis> includes Velero 1.7.<emphasis>x</emphasis> and expects the API group <literal>snapshot.storage.k8s.io/v1beta1</literal>, which is not present on OCP 4.11 and later.</simpara>
</note>
<simpara>If your cloud provider does not support snapshots or if your storage is NFS, you can back up applications with <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-backing-up-applications-restic-doc.xml#backing-up-applications">Backing up applications with File System Backup: Kopia or Restic</link> on object storage.</simpara>
<simpara>You create a default <literal>Secret</literal> and then you install the Data Protection Application.</simpara>
<section xml:id="oadp-s3-compatible-backup-storage-providers_about-installing-oadp">
<title>AWS S3 compatible backup storage providers</title>
<simpara>OADP is compatible with many object storage providers for use with different backup and snapshot operations. Several object storage providers are fully supported, several are unsupported but known to work, and some have known  limitations.</simpara>
<section xml:id="oadp-s3-compatible-backup-storage-providers-supported">
<title>Supported backup storage providers</title>
<simpara>The following AWS S3 compatible object storage providers are fully supported by OADP through the AWS plugin for use as backup storage locations:</simpara>
<itemizedlist>
<listitem>
<simpara>MinIO</simpara>
</listitem>
<listitem>
<simpara>Multicloud Object Gateway (MCG)</simpara>
</listitem>
<listitem>
<simpara>Amazon Web Services (AWS) S3</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>The following compatible object storage providers are supported and have their own Velero object store plugins:</simpara>
<itemizedlist>
<listitem>
<simpara>Google Cloud Platform (GCP)</simpara>
</listitem>
<listitem>
<simpara>Microsoft Azure</simpara>
</listitem>
</itemizedlist>
</note>
</section>
<section xml:id="oadp-s3-compatible-backup-storage-providers-unsupported">
<title>Unsupported backup storage providers</title>
<simpara>The following AWS S3 compatible object storage providers, are known to work with Velero through the AWS plugin, for use as backup storage locations, however, they are unsupported and have not been tested by Red Hat:</simpara>
<itemizedlist>
<listitem>
<simpara>IBM Cloud&#174;</simpara>
</listitem>
<listitem>
<simpara>Oracle Cloud</simpara>
</listitem>
<listitem>
<simpara>DigitalOcean</simpara>
</listitem>
<listitem>
<simpara>NooBaa, unless installed using Multicloud Object Gateway (MCG)</simpara>
</listitem>
<listitem>
<simpara>Tencent Cloud</simpara>
</listitem>
<listitem>
<simpara>Ceph RADOS v12.2.7</simpara>
</listitem>
<listitem>
<simpara>Quobyte</simpara>
</listitem>
<listitem>
<simpara>Cloudian HyperStore</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Unless specified otherwise, "NooBaa" refers to the open source project that provides lightweight object storage, while "Multicloud Object Gateway (MCG)" refers to the Red Hat distribution of NooBaa.</simpara>
<simpara>For more information on the MCG, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.13/html-single/managing_hybrid_and_multicloud_resources/index#accessing-the-multicloud-object-gateway-with-your-applications_rhodf">Accessing the Multicloud Object Gateway with your applications</link>.</simpara>
</note>
</section>
<section xml:id="oadp-s3-compatible-backup-storage-providers-known-limitations">
<title>Backup storage providers with known limitations</title>
<simpara>The following AWS S3 compatible object storage providers are known to work with Velero through the AWS plugin with a limited feature set:</simpara>
<itemizedlist>
<listitem>
<simpara>Swift - It works for use as a backup storage location for backup storage, but is not compatible with Restic for filesystem-based volume backup and restore.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-configuring-noobaa-for-dr_about-installing-oadp">
<title>Configuring Multicloud Object Gateway (MCG) for disaster recovery on OpenShift Data Foundation</title>
<simpara>If you use cluster storage for your MCG bucket <literal>backupStorageLocation</literal> on OpenShift Data Foundation, configure MCG as an external object store.</simpara>
<warning>
<simpara>Failure to configure MCG as an external object store might lead to backups not being available.</simpara>
</warning>
<note>
<simpara>Unless specified otherwise, "NooBaa" refers to the open source project that provides lightweight object storage, while "Multicloud Object Gateway (MCG)" refers to the Red Hat distribution of NooBaa.</simpara>
<simpara>For more information on the MCG, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.13/html-single/managing_hybrid_and_multicloud_resources/index#accessing-the-multicloud-object-gateway-with-your-applications_rhodf">Accessing the Multicloud Object Gateway with your applications</link>.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Configure MCG as an external object store as described in <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.13/html/managing_hybrid_and_multicloud_resources/adding-storage-resources-for-hybrid-or-multicloud_rhodf#doc-wrapper">Adding storage resources for hybrid or Multicloud</link>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources" mark="discrete">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://velero.io/docs/v1.12/locations/">Overview of backup and snapshot locations in the Velero documentation</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="about-oadp-update-channels_about-installing-oadp">
<title>About OADP update channels</title>
<simpara>When you install an OADP Operator, you choose an <emphasis>update channel</emphasis>. This channel determines which upgrades to the OADP Operator and to Velero you receive. You can switch channels at any time.</simpara>
<simpara>The following update channels are available:</simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">stable</emphasis> channel is now deprecated. The <emphasis role="strong">stable</emphasis> channel contains the patches (z-stream updates) of OADP <literal>ClusterServiceVersion</literal> for <literal>oadp.v1.1.z</literal> and older versions from <literal>oadp.v1.0.z</literal>.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">stable-1.0</emphasis> channel contains <literal>oadp.v1.0.<emphasis>z</emphasis></literal>, the most recent OADP 1.0 <literal>ClusterServiceVersion</literal>.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">stable-1.1</emphasis> channel contains <literal>oadp.v1.1.<emphasis>z</emphasis></literal>, the most recent OADP 1.1 <literal>ClusterServiceVersion</literal>.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">stable-1.2</emphasis> channel contains <literal>oadp.v1.2.<emphasis>z</emphasis></literal>, the most recent OADP 1.2 <literal>ClusterServiceVersion</literal>.</simpara>
</listitem>
<listitem>
<simpara>The <emphasis role="strong">stable-1.3</emphasis> channel contains <literal>oadp.v1.3.<emphasis>z</emphasis></literal>, the most recent OADP 1.3 <literal>ClusterServiceVersion</literal>.</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">Which update channel is right for you?</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>The <emphasis role="strong">stable</emphasis> channel is now deprecated.  If you are already using the stable channel, you will continue to get updates from <literal>oadp.v1.1.<emphasis>z</emphasis></literal>.</simpara>
</listitem>
<listitem>
<simpara>Choose the <emphasis role="strong">stable-1.<emphasis>y</emphasis></emphasis> update channel to install OADP 1.<emphasis>y</emphasis> and to continue receiving patches for it. If you choose this channel, you will receive all z-stream patches for version 1.<emphasis>y</emphasis>.<emphasis>z</emphasis>.</simpara>
</listitem>
</itemizedlist>
<simpara><emphasis role="strong">When must you switch update channels?</emphasis></simpara>
<itemizedlist>
<listitem>
<simpara>If you have OADP 1.<emphasis>y</emphasis> installed, and you want to receive patches only for that y-stream, you must switch from the <emphasis role="strong">stable</emphasis> update channel to the <emphasis role="strong">stable-1.<emphasis>y</emphasis></emphasis> update channel. You will then receive all z-stream patches for version 1.<emphasis>y</emphasis>.<emphasis>z</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you have OADP 1.0 installed, want to upgrade to OADP 1.1, and then receive patches only for OADP 1.1, you must switch from the <emphasis role="strong">stable-1.0</emphasis> update channel to the <emphasis role="strong">stable-1.1</emphasis> update channel. You will then receive all z-stream patches for version 1.1.<emphasis>z</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>If you have OADP 1.<emphasis>y</emphasis> installed, with <emphasis>y</emphasis> greater than 0, and want to switch to OADP 1.0, you must <emphasis>uninstall</emphasis> your OADP Operator and then reinstall it using the <emphasis role="strong">stable-1.0</emphasis> update channel. You will then receive all z-stream patches for version 1.0.<emphasis>z</emphasis>.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>You cannot switch from OADP 1.<emphasis>y</emphasis> to OADP 1.0 by switching update channels. You must uninstall the Operator and then reinstall it.</simpara>
</note>
</section>
<section xml:id="about-installing-oadp-on-multiple-namespaces_about-installing-oadp">
<title>Installation of OADP on multiple namespaces</title>
<simpara>You can install OpenShift API for Data Protection (OADP) into multiple namespaces on the same cluster so that multiple project owners can manage their own OADP instance. This use case has been validated with File System Backup (FSB) and Container Storage Interface (CSI).</simpara>
<simpara>You install each instance of OADP as specified by the per-platform procedures contained in this document with the following additional requirements:</simpara>
<itemizedlist>
<listitem>
<simpara>All deployments of OADP on the same cluster must be the same version, for example, 1.1.4. Installing different versions of OADP on the same cluster is <emphasis role="strong">not</emphasis> supported.</simpara>
</listitem>
<listitem>
<simpara>Each individual deployment of OADP must have a unique set of credentials and a unique <literal>BackupStorageLocation</literal> configuration.</simpara>
</listitem>
<listitem>
<simpara>By default, each OADP deployment has cluster-level access across namespaces. OpenShift Container Platform administrators need to review security and RBAC settings carefully and make any necessary changes to them to ensure that each OADP instance has the correct permissions.</simpara>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../../operators/understanding/olm/olm-understanding-olm.xml#olm-csv_olm-understanding-olm">Cluster service version</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-velero-cpu-memory-requirements_about-installing-oadp">
<title>Velero CPU and memory requirements based on collected data</title>
<simpara>The following recommendations are based on observations of performance made in the scale and performance lab. The backup and restore resources can be impacted by the type of plugin, the amount of resources required by that backup or restore, and the respective data contained in the persistent volumes (PVs) related to those resources.</simpara>
<section xml:id="_cpu_and_memory_requirement_for_configurations">
<title>CPU and memory requirement for configurations</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="4">
<colspec colname="col_1" colwidth="25*"/>
<colspec colname="col_2" colwidth="25*"/>
<colspec colname="col_3" colwidth="25*"/>
<colspec colname="col_4" colwidth="25*"/>
<thead>
<row>
<entry align="left" valign="top">Configuration types</entry>
<entry align="left" valign="top"><superscript>[1]</superscript> Average usage</entry>
<entry align="left" valign="top"><superscript>[2]</superscript> Large usage</entry>
<entry align="left" valign="top">resourceTimeouts</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>CSI</simpara></entry>
<entry align="left" valign="top"><simpara>Velero:</simpara><simpara>CPU- Request 200m, Limits 1000m</simpara><simpara>Memory - Request 256Mi, Limits 1024Mi</simpara></entry>
<entry align="left" valign="top"><simpara>Velero:</simpara><simpara>CPU- Request 200m, Limits 2000m</simpara><simpara>Memory- Request  256Mi, Limits 2048Mi</simpara></entry>
<entry align="left" valign="top"><simpara>N/A</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Restic</simpara></entry>
<entry align="left" valign="top"><simpara><superscript>[3]</superscript> Restic:</simpara><simpara>CPU- Request 1000m, Limits 2000m</simpara><simpara>Memory - Request 16Gi, Limits 32Gi</simpara></entry>
<entry align="left" valign="top"><simpara><superscript>[4]</superscript> Restic:</simpara><simpara>CPU - Request 2000m, Limits 8000m</simpara><simpara>Memory - Request 16Gi, Limits 40Gi</simpara></entry>
<entry align="left" valign="top"><simpara>900m</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><superscript>[5]</superscript> Data Mover</simpara></entry>
<entry align="left" valign="top"><simpara>N/A</simpara></entry>
<entry align="left" valign="top"><simpara>N/A</simpara></entry>
<entry align="left" valign="top"><simpara>10m - average usage</simpara><simpara>60m - large usage</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
<para role="small">
<orderedlist numeration="arabic">
<listitem>
<simpara>Average usage - use these settings for most usage situations.</simpara>
</listitem>
<listitem>
<simpara>Large usage - use these settings for large usage situations, such as a large PV (500GB Usage), multiple namespaces (100+), or many pods within a single namespace (2000 pods+), and for optimal performance for backup and restore involving large datasets.</simpara>
</listitem>
<listitem>
<simpara>Restic resource usage corresponds to the amount of data, and type of data. For example, many small files or large amounts of data can cause Restic to use large amounts of resources. The <link xlink:href="https://velero.io/docs/v1.11/customize-installation/#customize-resource-requests-and-limits/">Velero</link> documentation references 500m as a supplied default, for most of our testing we found a 200m request suitable with 1000m limit.  As cited in the Velero documentation, exact CPU and memory usage is dependent on the scale of files and directories, in addition to environmental limitations.</simpara>
</listitem>
<listitem>
<simpara>Increasing the CPU has a significant impact on improving backup and restore times.</simpara>
</listitem>
<listitem>
<simpara>Data Mover - Data Mover default resourceTimeout is 10m. Our tests show that for restoring a large PV (500GB usage), it is required to increase the resourceTimeout to 60m.</simpara>
</listitem>
</orderedlist>
</para>
<note>
<simpara>The resource requirements listed throughout the guide are for average usage only. For large usage, adjust the settings as described in the table above.</simpara>
</note>
</section>
<section xml:id="oadp-backup-restore-for-large-usage_about-installing-oadp">
<title>NodeAgent CPU for large usage</title>
<simpara>Testing shows that increasing <literal>NodeAgent</literal> CPU can significantly improve backup and restore times when using OpenShift API for Data Protection (OADP).</simpara>
<important>
<simpara>It is not recommended to use Kopia without limits in production environments on nodes running production workloads due to Kopia’s aggressive consumption of resources. However, running Kopia with limits that are too low results in CPU limiting and slow backups and restore situations. Testing showed that running Kopia with 20 cores and 32 Gi memory supported backup and restore operations of over 100 GB of data, multiple namespaces, or over 2000 pods in a single namespace.</simpara>
</important>
<simpara>Testing detected no CPU limiting or memory saturation with these resource specifications.</simpara>
<simpara>You can set these limits in Ceph MDS pods by following the procedure in <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.14/html/troubleshooting_openshift_data_foundation/changing-resources-for-the-openshift-data-foundation-components_rhodf#changing_the_cpu_and_memory_resources_on_the_rook_ceph_pods">Changing the CPU and memory resources on the rook-ceph pods</link>.</simpara>
<simpara>You need to add the following lines to the storage cluster Custom Resource (CR) to set the limits:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">   resources:
     mds:
       limits:
         cpu: "3"
         memory: 128Gi
       requests:
         cpu: "3"
         memory: 8Gi</programlisting>
</section>
</section>
</section>
<section xml:id="oadp-installing-operator-doc">
<title>Installing the OADP Operator</title>

<simpara>You can install the OpenShift API for Data Protection (OADP) Operator on OpenShift Container Platform 4.14 by using Operator Lifecycle Manager (OLM).</simpara>
<simpara>The OADP Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in as a user with <literal>cluster-admin</literal> privileges.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>In the OpenShift Container Platform web console, click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Use the <emphasis role="strong">Filter by keyword</emphasis> field to find the <emphasis role="strong">OADP Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Select the <emphasis role="strong">OADP Operator</emphasis> and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Install</emphasis> to install the Operator in the <literal>openshift-adp</literal> project.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> to verify the installation.</simpara>
</listitem>
</orderedlist>
<section xml:id="velero-oadp-version-relationship_installing-oadp-operator">
<title>OADP-Velero-OpenShift Container Platform version relationship</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">OADP version</entry>
<entry align="left" valign="top">Velero version</entry>
<entry align="left" valign="top">OpenShift Container Platform version</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>1.1.0</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.1</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.2</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.3</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.4</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.5</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.6</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.7</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.0</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.1</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.2</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.3</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.3.0</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.12/">1.12</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.12 and later</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="installing-oadp-aws">
<title>Configuring the OpenShift API for Data Protection with Amazon Web Services</title>

<simpara>You install the OpenShift API for Data Protection (OADP) with Amazon Web Services (AWS) by installing the OADP Operator. The Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<note>
<simpara>Starting from OADP 1.0.4, all OADP 1.0.<emphasis>z</emphasis> versions can only be used as a dependency of the MTC Operator and are not available as a standalone Operator.</simpara>
</note>
<simpara>You configure AWS for Velero, create a default <literal>Secret</literal>, and then install the Data Protection Application. For more details, see <link xlink:href="../../..//backup_and_restore/application_backup_and_restore/installing/oadp-installing-operator.xml#oadp-installing-operator-doc">Installing the OADP Operator</link>.</simpara>
<simpara>To install the OADP Operator in a restricted network environment, you must first disable the default OperatorHub sources and mirror the Operator catalog. See <link xlink:href="../../../operators/admin/olm-restricted-networks.xml#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link> for details.</simpara>
<section xml:id="migration-configuring-aws-s3_installing-oadp-aws">
<title>Configuring Amazon Web Services</title>
<simpara>You configure Amazon Web Services (AWS) for the OpenShift API for Data Protection (OADP).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <link xlink:href="https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html">AWS CLI</link> installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Set the <literal>BUCKET</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BUCKET=&lt;your_bucket&gt;</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>REGION</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ REGION=&lt;your_region&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create an AWS S3 bucket:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws s3api create-bucket \
    --bucket $BUCKET \
    --region $REGION \
    --create-bucket-configuration LocationConstraint=$REGION <co xml:id="CO4-1"/></programlisting>
<calloutlist>
<callout arearefs="CO4-1">
<para><literal>us-east-1</literal> does not support a <literal>LocationConstraint</literal>. If your region is <literal>us-east-1</literal>, omit <literal>--create-bucket-configuration LocationConstraint=$REGION</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create an IAM user:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam create-user --user-name velero <co xml:id="CO5-1"/></programlisting>
<calloutlist>
<callout arearefs="CO5-1">
<para>If you want to use Velero to back up multiple clusters with multiple S3 buckets, create a unique user name for each cluster.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create a <literal>velero-policy.json</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &gt; velero-policy.json &lt;&lt;EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation",
                "s3:ListBucketMultipartUploads"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}"
            ]
        }
    ]
}
EOF</programlisting>
</listitem>
<listitem>
<simpara>Attach the policies to give the <literal>velero</literal> user the minimum necessary permissions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam put-user-policy \
  --user-name velero \
  --policy-name velero \
  --policy-document file://velero-policy.json</programlisting>
</listitem>
<listitem>
<simpara>Create an access key for the <literal>velero</literal> user:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam create-access-key --user-name velero</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">{
  "AccessKey": {
        "UserName": "velero",
        "Status": "Active",
        "CreateDate": "2017-07-31T22:24:41.576Z",
        "SecretAccessKey": &lt;AWS_SECRET_ACCESS_KEY&gt;,
        "AccessKeyId": &lt;AWS_ACCESS_KEY_ID&gt;
  }
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; ./credentials-velero
[default]
aws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;
EOF</programlisting>
<simpara>You use the <literal>credentials-velero</literal> file to create a <literal>Secret</literal> object for AWS before you install the Data Protection Application.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-about-backup-snapshot-locations_installing-oadp-aws">
<title>About backup and snapshot locations and their secrets</title>
<simpara>You specify backup and snapshot locations and their secrets in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<bridgehead xml:id="backup-locations_installing-oadp-aws" renderas="sect5">Backup locations</bridgehead>
<simpara>You specify AWS S3-compatible object storage, such as Multicloud Object Gateway or MinIO, as a backup location.</simpara>
<simpara>Velero backs up OpenShift Container Platform resources, Kubernetes objects, and internal images as an archive file on object storage.</simpara>
<bridgehead xml:id="snapshot-locations_installing-oadp-aws" renderas="sect5">Snapshot locations</bridgehead>
<simpara>If you use your cloud provider&#8217;s native snapshot API to back up persistent volumes, you must specify the cloud provider as the snapshot location.</simpara>
<simpara>If you use Container Storage Interface (CSI) snapshots, you do not need to specify a snapshot location because you will create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
<simpara>If you use File System Backup (FSB), you do not need to specify a snapshot location because FSB backs up the file system on object storage.</simpara>
<bridgehead xml:id="secrets_installing-oadp-aws" renderas="sect5">Secrets</bridgehead>
<simpara>If the backup and snapshot locations use the same credentials or if you do not require a snapshot location, you create a default <literal>Secret</literal>.</simpara>
<simpara>If the backup and snapshot locations use different credentials, you create two secret objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Custom <literal>Secret</literal> for the backup location, which you specify in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Default <literal>Secret</literal> for the snapshot location, which is not referenced in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The Data Protection Application requires a default <literal>Secret</literal>. Otherwise, the installation will fail.</simpara>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file.</simpara>
</important>
<section xml:id="oadp-creating-default-secret_installing-oadp-aws">
<title>Creating a default Secret</title>
<simpara>You create a default <literal>Secret</literal> if your backup and snapshot locations use the same credentials or if you do not require a snapshot location.</simpara>
<simpara>The default name of the <literal>Secret</literal> is <literal>cloud-credentials</literal>.</simpara>
<note>
<simpara>The <literal>DataProtectionApplication</literal> custom resource (CR) requires a default <literal>Secret</literal>.  Otherwise, the installation will fail. If the name of the backup location <literal>Secret</literal> is not specified, the default name is used.</simpara>
<simpara>If you do not want to use the backup location credentials during the installation, you can create a <literal>Secret</literal> with the default name by using an empty <literal>credentials-velero</literal> file.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your object storage and cloud storage, if any, must use the same credentials.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must create a <literal>credentials-velero</literal> file for the object storage in the appropriate format.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Secret</literal> with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
</itemizedlist>
<simpara>The <literal>Secret</literal> is referenced in the <literal>spec.backupLocations.credential</literal> block of the <literal>DataProtectionApplication</literal> CR when you install the Data Protection Application.</simpara>
</section>
<section xml:id="oadp-secrets-for-different-credentials_installing-oadp-aws">
<title>Creating profiles for different credentials</title>
<simpara>If your backup and snapshot locations use different credentials, you create separate profiles in the <literal>credentials-velero</literal> file.</simpara>
<simpara>Then, you create a <literal>Secret</literal> object and specify the profiles in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file with separate profiles for the backup and snapshot locations, as in the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">[backupStorage]
aws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;

[volumeSnapshot]
aws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> object with the <literal>credentials-velero</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials-velero <co xml:id="CO6-1"/></programlisting>
</listitem>
<listitem>
<simpara>Add the profiles to the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt;
          prefix: &lt;prefix&gt;
        config:
          region: us-east-1
          profile: "backupStorage"
        credential:
          key: cloud
          name: cloud-credentials
  snapshotLocations:
    - name: default
      velero:
        provider: aws
        config:
          region: us-west-2
          profile: "volumeSnapshot"</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-dpa-aws">
<title>Configuring the Data Protection Application</title>
<simpara>You can configure the Data Protection Application by setting Velero resource allocations or enabling self-signed CA certificates.</simpara>
<section xml:id="oadp-setting-resource-limits-and-requests_installing-oadp-aws">
<title>Setting Velero CPU and memory resource allocations</title>
<simpara>You set the CPU and memory resource allocations for the <literal>Velero</literal> pod by editing the  <literal>DataProtectionApplication</literal> custom resource (CR) manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.podConfig.ResourceAllocations</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  configuration:
    velero:
      podConfig:
        nodeSelector: &lt;node selector&gt; <co xml:id="CO6-2"/>
        resourceAllocations: <co xml:id="CO6-3"/>
          limits:
            cpu: "1"
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 256Mi</programlisting>
<calloutlist>
<callout arearefs="CO6-1 CO6-2">
<para>Specify the node selector to be supplied to Velero podSpec.</para>
</callout>
<callout arearefs="CO6-3">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>Kopia is an option in OADP 1.3 and later releases. You can use Kopia for file system backups, and Kopia is your only option for Data Mover cases with the built-in Data Mover.</simpara>
<simpara>Kopia is more resource intensive than Restic, and you might need to adjust the CPU and memory requirements accordingly.</simpara>
</note>
</section>
<section xml:id="oadp-self-signed-certificate_installing-oadp-aws">
<title>Enabling self-signed CA certificates</title>
<simpara>You must enable a self-signed CA certificate for object storage by editing the <literal>DataProtectionApplication</literal> custom resource (CR) manifest to prevent a <literal>certificate signed by unknown authority</literal> error.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.backupLocations.velero.objectStorage.caCert</literal> parameter and <literal>spec.backupLocations.velero.config</literal> parameters of the <literal>DataProtectionApplication</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket&gt;
          prefix: &lt;prefix&gt;
          caCert: &lt;base64_encoded_cert_string&gt; <co xml:id="CO7-1"/>
        config:
          insecureSkipTLSVerify: "false" <co xml:id="CO7-2"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO7-1">
<para>Specify the Base64-encoded CA certificate string.</para>
</callout>
<callout arearefs="CO7-2">
<para>The <literal>insecureSkipTLSVerify</literal> configuration can be set to either <literal>"true"</literal> or <literal>"false"</literal>. If set to <literal>"true"</literal>, SSL/TLS security is disabled. If set to <literal>"false"</literal>, SSL/TLS security is enabled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="oadp-using-ca-certificates-with-velero-command-aliased-for-velero-deployment_installing-oadp-aws">
<title>Using CA certificates with the velero command aliased for Velero deployment</title>
<simpara>You might want to use the Velero CLI without installing it locally on your system by creating an alias for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To use an aliased Velero command, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
<listitem>
<simpara>Check that the alias is working by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero version
Client:
	Version: v1.12.1-OADP
	Git commit: -
Server:
	Version: v1.12.1-OADP</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To use a CA certificate with this command, you can add a certificate to the Velero deployment by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CA_CERT=$(oc -n openshift-adp get dataprotectionapplications.oadp.openshift.io &lt;dpa-name&gt; -o jsonpath='{.spec.backupLocations[0].velero.objectStorage.caCert}')

$ [[ -n $CA_CERT ]] &amp;&amp; echo "$CA_CERT" | base64 -d | oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "cat &gt; /tmp/your-cacert.txt" || echo "DPA BSL has no caCert"</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ velero -n openshift-adp describe backup &lt;backup-name&gt; --details --cacert /tmp/your-cacert.txt</programlisting>
</listitem>
<listitem>
<simpara>If the Velero pod restarts, the <literal>/tmp/your-cacert.txt</literal> file disappears, and you must re-create the <literal>/tmp/your-cacert.txt</literal> file by re-running the commands from the previous step.</simpara>
</listitem>
<listitem>
<simpara>You can check if the <literal>/tmp/your-cacert.txt</literal> file still exists, in the file location where you stored it, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "ls /tmp/your-cacert.txt"
/tmp/your-cacert.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<simpara>In a future release of OpenShift API for Data Protection (OADP), we plan to mount the certificate to the Velero pod so that this step is not required.</simpara>
</section>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-2-and-earlier_installing-oadp-aws">
<title>Installing the Data Protection Application 1.2 and earlier</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>, which contains separate profiles for the backup and snapshot location credentials.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
<note>
<simpara>Velero creates a secret named <literal>velero-repo-credentials</literal> in the OADP namespace, which contains a default backup repository password.
You can update the secret with your own password encoded as base64 <emphasis role="strong">before</emphasis> you run your first backup targeted to the backup repository. The value of the key to update is <literal>Data[repository-password]</literal>.</simpara>
<simpara>After you create your DPA, the first time that you run a backup targeted to the backup repository, Velero creates a backup repository whose secret is <literal>velero-repo-credentials</literal>, which contains either the default password or the one you replaced it with.
If you update the secret password <emphasis role="strong">after</emphasis> the first backup, the new password will not match the password in <literal>velero-repo-credentials</literal>, and therefore, Velero will not be able to connect with the older backups.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
  configuration:
    velero:
      defaultPlugins:
        - openshift <co xml:id="CO8-1"/>
        - aws
      resourceTimeout: 10m <co xml:id="CO8-2"/>
    restic:
      enable: true <co xml:id="CO8-3"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO8-4"/>
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO8-5"/>
          prefix: &lt;prefix&gt; <co xml:id="CO8-6"/>
        config:
          region: &lt;region&gt;
          profile: "default"
        credential:
          key: cloud
          name: cloud-credentials <co xml:id="CO8-7"/>
  snapshotLocations: <co xml:id="CO8-8"/>
    - name: default
      velero:
        provider: aws
        config:
          region: &lt;region&gt; <co xml:id="CO8-9"/>
          profile: "default"</programlisting>
<calloutlist>
<callout arearefs="CO8-1">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO8-2">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO8-3">
<para>Set this value to <literal>false</literal> if you want to disable the Restic installation. Restic deploys a daemon set, which means that Restic pods run on each working node. In OADP version 1.2 and later, you can configure Restic for backups by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR. In OADP version 1.1, add <literal>spec.defaultVolumesToRestic: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO8-4">
<para>Specify on which nodes Restic is available. By default, Restic runs on all nodes.</para>
</callout>
<callout arearefs="CO8-5">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO8-6">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
<callout arearefs="CO8-7">
<para>Specify the name of the <literal>Secret</literal> object that you created. If you do not specify this value, the default name, <literal>cloud-credentials</literal>, is used. If you specify a custom name, the custom name is used for the backup location.</para>
</callout>
<callout arearefs="CO8-8">
<para>Specify a snapshot location, unless you use CSI snapshots or Restic to back up PVs.</para>
</callout>
<callout arearefs="CO8-9">
<para>The snapshot location must be in the same region as the PVs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-2_installing-oadp-aws">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-3_installing-oadp-aws">
<title>Installing the Data Protection Application 1.3</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>, which contains separate profiles for the backup and snapshot location credentials.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp <co xml:id="CO9-1"/>
spec:
  configuration:
    velero:
      defaultPlugins:
        - openshift <co xml:id="CO9-2"/>
        - aws
      resourceTimeout: 10m <co xml:id="CO9-3"/>
    nodeAgent: <co xml:id="CO9-4"/>
      enable: true <co xml:id="CO9-5"/>
      uploaderType: kopia <co xml:id="CO9-6"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO9-7"/>
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO9-8"/>
          prefix: &lt;prefix&gt; <co xml:id="CO9-9"/>
        config:
          region: &lt;region&gt;
          profile: "default"
        credential:
          key: cloud
          name: cloud-credentials <co xml:id="CO9-10"/>
  snapshotLocations: <co xml:id="CO9-11"/>
    - name: default
      velero:
        provider: aws
        config:
          region: &lt;region&gt; <co xml:id="CO9-12"/>
          profile: "default"</programlisting>
<calloutlist>
<callout arearefs="CO9-1">
<para>The default namespace for OADP is <literal>openshift-adp</literal>. The namespace is a variable and is configurable.</para>
</callout>
<callout arearefs="CO9-2">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO9-3">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO9-4">
<para>The administrative agent that routes the administrative requests to servers.</para>
</callout>
<callout arearefs="CO9-5">
<para>Set this value to <literal>true</literal> if you want to enable <literal>nodeAgent</literal> and perform File System Backup.</para>
</callout>
<callout arearefs="CO9-6">
<para>Enter <literal>kopia</literal> or <literal>restic</literal> as your uploader. You cannot change the selection after the installation. For the Built-in DataMover you must use Kopia. The <literal>nodeAgent</literal> deploys a daemon set, which means that the <literal>nodeAgent</literal> pods run on each working node. You can configure File System Backup by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO9-7">
<para>Specify the nodes on which Kopia or Restic are available. By default, Kopia or Restic run on all nodes.</para>
</callout>
<callout arearefs="CO9-8">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO9-9">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
<callout arearefs="CO9-10">
<para>Specify the name of the <literal>Secret</literal> object that you created. If you do not specify this value, the default name, <literal>cloud-credentials</literal>, is used. If you specify a custom name, the custom name is used for the backup location.</para>
</callout>
<callout arearefs="CO9-11">
<para>Specify a snapshot location, unless you use CSI snapshots or a File System Backup (FSB) to back up PVs.</para>
</callout>
<callout arearefs="CO9-12">
<para>The snapshot location must be in the same region as the PVs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-3_installing-oadp-aws">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-enabling-csi-dpa_installing-oadp-aws">
<title>Enabling CSI in the DataProtectionApplication CR</title>
<simpara>You enable the Container Storage Interface (CSI) in the <literal>DataProtectionApplication</literal> custom resource (CR) in order to back up persistent volumes with CSI snapshots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - csi <co xml:id="CO10-1"/></programlisting>
<calloutlist>
<callout arearefs="CO10-1">
<para>Add the <literal>csi</literal> default plugin.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installing-oadp-azure">
<title>Configuring the OpenShift API for Data Protection with Microsoft Azure</title>

<simpara>You install the OpenShift API for Data Protection (OADP) with Microsoft Azure by installing the OADP Operator. The Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<note>
<simpara>Starting from OADP 1.0.4, all OADP 1.0.<emphasis>z</emphasis> versions can only be used as a dependency of the MTC Operator and are not available as a standalone Operator.</simpara>
</note>
<simpara>You configure Azure for Velero, create a default <literal>Secret</literal>, and then install the Data Protection Application. For more details, see <link xlink:href="../../..//backup_and_restore/application_backup_and_restore/installing/oadp-installing-operator.xml#oadp-installing-operator-doc">Installing the OADP Operator</link>.</simpara>
<simpara>To install the OADP Operator in a restricted network environment, you must first disable the default OperatorHub sources and mirror the Operator catalog. See <link xlink:href="../../../operators/admin/olm-restricted-networks.xml#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link> for details.</simpara>
<section xml:id="migration-configuring-azure_installing-oadp-azure">
<title>Configuring Microsoft Azure</title>
<simpara>You configure Microsoft Azure for the OpenShift API for Data Protection (OADP).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <link xlink:href="https://docs.microsoft.com/en-us/cli/azure/install-azure-cli">Azure CLI</link> installed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to Azure:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az login</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>AZURE_RESOURCE_GROUP</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_RESOURCE_GROUP=Velero_Backups</programlisting>
</listitem>
<listitem>
<simpara>Create an Azure resource group:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az group create -n $AZURE_RESOURCE_GROUP --location CentralUS <co xml:id="CO11-1"/></programlisting>
<calloutlist>
<callout arearefs="CO11-1">
<para>Specify your location.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Set the <literal>AZURE_STORAGE_ACCOUNT_ID</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_STORAGE_ACCOUNT_ID="velero$(uuidgen | cut -d '-' -f5 | tr '[A-Z]' '[a-z]')"</programlisting>
</listitem>
<listitem>
<simpara>Create an Azure storage account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage account create \
    --name $AZURE_STORAGE_ACCOUNT_ID \
    --resource-group $AZURE_RESOURCE_GROUP \
    --sku Standard_GRS \
    --encryption-services blob \
    --https-only true \
    --kind BlobStorage \
    --access-tier Hot</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>BLOB_CONTAINER</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BLOB_CONTAINER=velero</programlisting>
</listitem>
<listitem>
<simpara>Create an Azure Blob storage container:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ az storage container create \
  -n $BLOB_CONTAINER \
  --public-access off \
  --account-name $AZURE_STORAGE_ACCOUNT_ID</programlisting>
</listitem>
<listitem>
<simpara>Obtain the storage account access key:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ AZURE_STORAGE_ACCOUNT_ACCESS_KEY=`az storage account keys list \
  --account-name $AZURE_STORAGE_ACCOUNT_ID \
  --query "[?keyName == 'key1'].value" -o tsv`</programlisting>
</listitem>
<listitem>
<simpara>Create a custom role that has the minimum required permissions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AZURE_ROLE=Velero
az role definition create --role-definition '{
   "Name": "'$AZURE_ROLE'",
   "Description": "Velero related permissions to perform backups, restores and deletions",
   "Actions": [
       "Microsoft.Compute/disks/read",
       "Microsoft.Compute/disks/write",
       "Microsoft.Compute/disks/endGetAccess/action",
       "Microsoft.Compute/disks/beginGetAccess/action",
       "Microsoft.Compute/snapshots/read",
       "Microsoft.Compute/snapshots/write",
       "Microsoft.Compute/snapshots/delete",
       "Microsoft.Storage/storageAccounts/listkeys/action",
       "Microsoft.Storage/storageAccounts/regeneratekey/action"
   ],
   "AssignableScopes": ["/subscriptions/'$AZURE_SUBSCRIPTION_ID'"]
   }'</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_TENANT_ID=${AZURE_TENANT_ID}
AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
AZURE_STORAGE_ACCOUNT_ACCESS_KEY=${AZURE_STORAGE_ACCOUNT_ACCESS_KEY} <co xml:id="CO12-1"/>
AZURE_CLOUD_NAME=AzurePublicCloud
EOF</programlisting>
<calloutlist>
<callout arearefs="CO12-1">
<para>Mandatory. You cannot back up internal images if the <literal>credentials-velero</literal> file contains only the service principal credentials.</para>
</callout>
</calloutlist>
<simpara>You use the <literal>credentials-velero</literal> file to create a <literal>Secret</literal> object for Azure before you install the Data Protection Application.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-about-backup-snapshot-locations_installing-oadp-azure">
<title>About backup and snapshot locations and their secrets</title>
<simpara>You specify backup and snapshot locations and their secrets in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<bridgehead xml:id="backup-locations_installing-oadp-azure" renderas="sect5">Backup locations</bridgehead>
<simpara>You specify AWS S3-compatible object storage, such as Multicloud Object Gateway or MinIO, as a backup location.</simpara>
<simpara>Velero backs up OpenShift Container Platform resources, Kubernetes objects, and internal images as an archive file on object storage.</simpara>
<bridgehead xml:id="snapshot-locations_installing-oadp-azure" renderas="sect5">Snapshot locations</bridgehead>
<simpara>If you use your cloud provider&#8217;s native snapshot API to back up persistent volumes, you must specify the cloud provider as the snapshot location.</simpara>
<simpara>If you use Container Storage Interface (CSI) snapshots, you do not need to specify a snapshot location because you will create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
<simpara>If you use File System Backup (FSB), you do not need to specify a snapshot location because FSB backs up the file system on object storage.</simpara>
<bridgehead xml:id="secrets_installing-oadp-azure" renderas="sect5">Secrets</bridgehead>
<simpara>If the backup and snapshot locations use the same credentials or if you do not require a snapshot location, you create a default <literal>Secret</literal>.</simpara>
<simpara>If the backup and snapshot locations use different credentials, you create two secret objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Custom <literal>Secret</literal> for the backup location, which you specify in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Default <literal>Secret</literal> for the snapshot location, which is not referenced in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The Data Protection Application requires a default <literal>Secret</literal>. Otherwise, the installation will fail.</simpara>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file.</simpara>
</important>
<section xml:id="oadp-creating-default-secret_installing-oadp-azure">
<title>Creating a default Secret</title>
<simpara>You create a default <literal>Secret</literal> if your backup and snapshot locations use the same credentials or if you do not require a snapshot location.</simpara>
<simpara>The default name of the <literal>Secret</literal> is <literal>cloud-credentials-azure</literal>.</simpara>
<note>
<simpara>The <literal>DataProtectionApplication</literal> custom resource (CR) requires a default <literal>Secret</literal>.  Otherwise, the installation will fail. If the name of the backup location <literal>Secret</literal> is not specified, the default name is used.</simpara>
<simpara>If you do not want to use the backup location credentials during the installation, you can create a <literal>Secret</literal> with the default name by using an empty <literal>credentials-velero</literal> file.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your object storage and cloud storage, if any, must use the same credentials.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must create a <literal>credentials-velero</literal> file for the object storage in the appropriate format.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Secret</literal> with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials-azure -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
</itemizedlist>
<simpara>The <literal>Secret</literal> is referenced in the <literal>spec.backupLocations.credential</literal> block of the <literal>DataProtectionApplication</literal> CR when you install the Data Protection Application.</simpara>
</section>
<section xml:id="oadp-secrets-for-different-credentials_installing-oadp-azure">
<title>Creating secrets for different credentials</title>
<simpara>If your backup and snapshot locations use different credentials, you must create two <literal>Secret</literal> objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Backup location <literal>Secret</literal> with a custom name. The custom name is specified in the <literal>spec.backupLocations</literal> block of the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Snapshot location <literal>Secret</literal> with the default name, <literal>cloud-credentials-azure</literal>. This <literal>Secret</literal> is not specified in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file for the snapshot location in the appropriate format for your cloud provider.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> for the snapshot location with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials-azure -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file for the backup location in the appropriate format for your object storage.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> for the backup location with a custom name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic &lt;custom_secret&gt; -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>Secret</literal> with the custom name to the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
...
  backupLocations:
    - velero:
        config:
          resourceGroup: &lt;azure_resource_group&gt;
          storageAccount: &lt;azure_storage_account_id&gt;
          subscriptionId: &lt;azure_subscription_id&gt;
          storageAccountKeyEnvVar: AZURE_STORAGE_ACCOUNT_ACCESS_KEY
        credential:
          key: cloud
          name: &lt;custom_secret&gt; <co xml:id="CO13-1"/>
        provider: azure
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt;
          prefix: &lt;prefix&gt;
  snapshotLocations:
    - velero:
        config:
          resourceGroup: &lt;azure_resource_group&gt;
          subscriptionId: &lt;azure_subscription_id&gt;
          incremental: "true"
        name: default
        provider: azure</programlisting>
<calloutlist>
<callout arearefs="CO13-1">
<para>Backup location <literal>Secret</literal> with custom name.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-dpa-azure">
<title>Configuring the Data Protection Application</title>
<simpara>You can configure the Data Protection Application by setting Velero resource allocations or enabling self-signed CA certificates.</simpara>
<section xml:id="oadp-setting-resource-limits-and-requests_installing-oadp-azure">
<title>Setting Velero CPU and memory resource allocations</title>
<simpara>You set the CPU and memory resource allocations for the <literal>Velero</literal> pod by editing the  <literal>DataProtectionApplication</literal> custom resource (CR) manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.podConfig.ResourceAllocations</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  configuration:
    velero:
      podConfig:
        nodeSelector: &lt;node selector&gt; <co xml:id="CO14-1"/>
        resourceAllocations: <co xml:id="CO14-2"/>
          limits:
            cpu: "1"
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 256Mi</programlisting>
<calloutlist>
<callout arearefs="CO14-1">
<para>Specify the node selector to be supplied to Velero podSpec.</para>
</callout>
<callout arearefs="CO14-2">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>Kopia is an option in OADP 1.3 and later releases. You can use Kopia for file system backups, and Kopia is your only option for Data Mover cases with the built-in Data Mover.</simpara>
<simpara>Kopia is more resource intensive than Restic, and you might need to adjust the CPU and memory requirements accordingly.</simpara>
</note>
</section>
<section xml:id="oadp-self-signed-certificate_installing-oadp-azure">
<title>Enabling self-signed CA certificates</title>
<simpara>You must enable a self-signed CA certificate for object storage by editing the <literal>DataProtectionApplication</literal> custom resource (CR) manifest to prevent a <literal>certificate signed by unknown authority</literal> error.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.backupLocations.velero.objectStorage.caCert</literal> parameter and <literal>spec.backupLocations.velero.config</literal> parameters of the <literal>DataProtectionApplication</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket&gt;
          prefix: &lt;prefix&gt;
          caCert: &lt;base64_encoded_cert_string&gt; <co xml:id="CO15-1"/>
        config:
          insecureSkipTLSVerify: "false" <co xml:id="CO15-2"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO15-1">
<para>Specify the Base64-encoded CA certificate string.</para>
</callout>
<callout arearefs="CO15-2">
<para>The <literal>insecureSkipTLSVerify</literal> configuration can be set to either <literal>"true"</literal> or <literal>"false"</literal>. If set to <literal>"true"</literal>, SSL/TLS security is disabled. If set to <literal>"false"</literal>, SSL/TLS security is enabled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="oadp-using-ca-certificates-with-velero-command-aliased-for-velero-deployment_installing-oadp-azure">
<title>Using CA certificates with the velero command aliased for Velero deployment</title>
<simpara>You might want to use the Velero CLI without installing it locally on your system by creating an alias for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To use an aliased Velero command, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
<listitem>
<simpara>Check that the alias is working by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero version
Client:
	Version: v1.12.1-OADP
	Git commit: -
Server:
	Version: v1.12.1-OADP</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To use a CA certificate with this command, you can add a certificate to the Velero deployment by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CA_CERT=$(oc -n openshift-adp get dataprotectionapplications.oadp.openshift.io &lt;dpa-name&gt; -o jsonpath='{.spec.backupLocations[0].velero.objectStorage.caCert}')

$ [[ -n $CA_CERT ]] &amp;&amp; echo "$CA_CERT" | base64 -d | oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "cat &gt; /tmp/your-cacert.txt" || echo "DPA BSL has no caCert"</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ velero -n openshift-adp describe backup &lt;backup-name&gt; --details --cacert /tmp/your-cacert.txt</programlisting>
</listitem>
<listitem>
<simpara>If the Velero pod restarts, the <literal>/tmp/your-cacert.txt</literal> file disappears, and you must re-create the <literal>/tmp/your-cacert.txt</literal> file by re-running the commands from the previous step.</simpara>
</listitem>
<listitem>
<simpara>You can check if the <literal>/tmp/your-cacert.txt</literal> file still exists, in the file location where you stored it, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "ls /tmp/your-cacert.txt"
/tmp/your-cacert.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<simpara>In a future release of OpenShift API for Data Protection (OADP), we plan to mount the certificate to the Velero pod so that this step is not required.</simpara>
</section>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-2-and-earlier_installing-oadp-azure">
<title>Installing the Data Protection Application 1.2 and earlier</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials-azure</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
<note>
<simpara>Velero creates a secret named <literal>velero-repo-credentials</literal> in the OADP namespace, which contains a default backup repository password.
You can update the secret with your own password encoded as base64 <emphasis role="strong">before</emphasis> you run your first backup targeted to the backup repository. The value of the key to update is <literal>Data[repository-password]</literal>.</simpara>
<simpara>After you create your DPA, the first time that you run a backup targeted to the backup repository, Velero creates a backup repository whose secret is <literal>velero-repo-credentials</literal>, which contains either the default password or the one you replaced it with.
If you update the secret password <emphasis role="strong">after</emphasis> the first backup, the new password will not match the password in <literal>velero-repo-credentials</literal>, and therefore, Velero will not be able to connect with the older backups.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
  configuration:
    velero:
      defaultPlugins:
        - azure
        - openshift <co xml:id="CO16-1"/>
      resourceTimeout: 10m <co xml:id="CO16-2"/>
    restic:
      enable: true <co xml:id="CO16-3"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO16-4"/>
  backupLocations:
    - velero:
        config:
          resourceGroup: &lt;azure_resource_group&gt; <co xml:id="CO16-5"/>
          storageAccount: &lt;azure_storage_account_id&gt; <co xml:id="CO16-6"/>
          subscriptionId: &lt;azure_subscription_id&gt; <co xml:id="CO16-7"/>
          storageAccountKeyEnvVar: AZURE_STORAGE_ACCOUNT_ACCESS_KEY
        credential:
          key: cloud
          name: cloud-credentials-azure  <co xml:id="CO16-8"/>
        provider: azure
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO16-9"/>
          prefix: &lt;prefix&gt; <co xml:id="CO16-10"/>
  snapshotLocations: <co xml:id="CO16-11"/>
    - velero:
        config:
          resourceGroup: &lt;azure_resource_group&gt;
          subscriptionId: &lt;azure_subscription_id&gt;
          incremental: "true"
        name: default
        provider: azure</programlisting>
<calloutlist>
<callout arearefs="CO16-1">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO16-2">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO16-3">
<para>Set this value to <literal>false</literal> if you want to disable the Restic installation. Restic deploys a daemon set, which means that Restic pods run on each working node. In OADP version 1.2 and later, you can configure Restic for backups by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR. In OADP version 1.1, add <literal>spec.defaultVolumesToRestic: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO16-4">
<para>Specify on which nodes Restic is available. By default, Restic runs on all nodes.</para>
</callout>
<callout arearefs="CO16-5">
<para>Specify the Azure resource group.</para>
</callout>
<callout arearefs="CO16-6">
<para>Specify the Azure storage account ID.</para>
</callout>
<callout arearefs="CO16-7">
<para>Specify the Azure subscription ID.</para>
</callout>
<callout arearefs="CO16-8">
<para>If you do not specify this value, the default name, <literal>cloud-credentials-azure</literal>, is used. If you specify a custom name, the custom name is used for the backup location.</para>
</callout>
<callout arearefs="CO16-9">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO16-10">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
<callout arearefs="CO16-11">
<para>You do not need to specify a snapshot location if you use CSI snapshots or Restic to back up PVs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-2_installing-oadp-azure">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-3_installing-oadp-azure">
<title>Installing the Data Protection Application 1.3</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials-azure</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp <co xml:id="CO17-1"/>
spec:
  configuration:
    velero:
      defaultPlugins:
        - azure
        - openshift <co xml:id="CO17-2"/>
      resourceTimeout: 10m <co xml:id="CO17-3"/>
    nodeAgent: <co xml:id="CO17-4"/>
      enable: true <co xml:id="CO17-5"/>
      uploaderType: kopia <co xml:id="CO17-6"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO17-7"/>
  backupLocations:
    - velero:
        config:
          resourceGroup: &lt;azure_resource_group&gt; <co xml:id="CO17-8"/>
          storageAccount: &lt;azure_storage_account_id&gt; <co xml:id="CO17-9"/>
          subscriptionId: &lt;azure_subscription_id&gt; <co xml:id="CO17-10"/>
          storageAccountKeyEnvVar: AZURE_STORAGE_ACCOUNT_ACCESS_KEY
        credential:
          key: cloud
          name: cloud-credentials-azure  <co xml:id="CO17-11"/>
        provider: azure
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO17-12"/>
          prefix: &lt;prefix&gt; <co xml:id="CO17-13"/>
  snapshotLocations: <co xml:id="CO17-14"/>
    - velero:
        config:
          resourceGroup: &lt;azure_resource_group&gt;
          subscriptionId: &lt;azure_subscription_id&gt;
          incremental: "true"
        name: default
        provider: azure</programlisting>
<calloutlist>
<callout arearefs="CO17-1">
<para>The default namespace for OADP is <literal>openshift-adp</literal>. The namespace is a variable and is configurable.</para>
</callout>
<callout arearefs="CO17-2">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO17-3">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO17-4">
<para>The administrative agent that routes the administrative requests to servers.</para>
</callout>
<callout arearefs="CO17-5">
<para>Set this value to <literal>true</literal> if you want to enable <literal>nodeAgent</literal> and perform File System Backup.</para>
</callout>
<callout arearefs="CO17-6">
<para>Enter <literal>kopia</literal> or <literal>restic</literal> as your uploader. You cannot change the selection after the installation. For the Built-in DataMover you must use Kopia. The <literal>nodeAgent</literal> deploys a daemon set, which means that the <literal>nodeAgent</literal> pods run on each working node. You can configure File System Backup by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO17-7">
<para>Specify the nodes on which Kopia or Restic are available. By default, Kopia or Restic run on all nodes.</para>
</callout>
<callout arearefs="CO17-8">
<para>Specify the Azure resource group.</para>
</callout>
<callout arearefs="CO17-9">
<para>Specify the Azure storage account ID.</para>
</callout>
<callout arearefs="CO17-10">
<para>Specify the Azure subscription ID.</para>
</callout>
<callout arearefs="CO17-11">
<para>If you do not specify this value, the default name, <literal>cloud-credentials-azure</literal>, is used. If you specify a custom name, the custom name is used for the backup location.</para>
</callout>
<callout arearefs="CO17-12">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO17-13">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
<callout arearefs="CO17-14">
<para>You do not need to specify a snapshot location if you use CSI snapshots or Restic to back up PVs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-3_installing-oadp-azure">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-enabling-csi-dpa_installing-oadp-azure">
<title>Enabling CSI in the DataProtectionApplication CR</title>
<simpara>You enable the Container Storage Interface (CSI) in the <literal>DataProtectionApplication</literal> custom resource (CR) in order to back up persistent volumes with CSI snapshots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - csi <co xml:id="CO18-1"/></programlisting>
<calloutlist>
<callout arearefs="CO18-1">
<para>Add the <literal>csi</literal> default plugin.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installing-oadp-gcp">
<title>Configuring the OpenShift API for Data Protection with Google Cloud Platform</title>

<simpara>You install the OpenShift API for Data Protection (OADP) with Google Cloud Platform (GCP) by installing the OADP Operator. The Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<note>
<simpara>Starting from OADP 1.0.4, all OADP 1.0.<emphasis>z</emphasis> versions can only be used as a dependency of the MTC Operator and are not available as a standalone Operator.</simpara>
</note>
<simpara>You configure GCP for Velero, create a default <literal>Secret</literal>, and then install the Data Protection Application. For more details, see <link xlink:href="../../..//backup_and_restore/application_backup_and_restore/installing/oadp-installing-operator.xml#oadp-installing-operator-doc">Installing the OADP Operator</link>.</simpara>
<simpara>To install the OADP Operator in a restricted network environment, you must first disable the default OperatorHub sources and mirror the Operator catalog. See <link xlink:href="../../../operators/admin/olm-restricted-networks.xml#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link> for details.</simpara>
<section xml:id="migration-configuring-gcp_installing-oadp-gcp">
<title>Configuring Google Cloud Platform</title>
<simpara>You configure Google Cloud Platform (GCP) for the OpenShift API for Data Protection (OADP).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the <literal>gcloud</literal> and <literal>gsutil</literal> CLI tools installed. See the <link xlink:href="https://cloud.google.com/sdk/docs/">Google cloud documentation</link> for details.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Log in to GCP:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud auth login</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>BUCKET</literal> variable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ BUCKET=&lt;bucket&gt; <co xml:id="CO19-1"/></programlisting>
<calloutlist>
<callout arearefs="CO19-1">
<para>Specify your bucket name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Create the storage bucket:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gsutil mb gs://$BUCKET/</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>PROJECT_ID</literal> variable to your active project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ PROJECT_ID=$(gcloud config get-value project)</programlisting>
</listitem>
<listitem>
<simpara>Create a service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam service-accounts create velero \
    --display-name "Velero service account"</programlisting>
</listitem>
<listitem>
<simpara>List your service accounts:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam service-accounts list</programlisting>
</listitem>
<listitem>
<simpara>Set the <literal>SERVICE_ACCOUNT_EMAIL</literal> variable to match its <literal>email</literal> value:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list \
    --filter="displayName:Velero service account" \
    --format 'value(email)')</programlisting>
</listitem>
<listitem>
<simpara>Attach the policies to give the <literal>velero</literal> user the minimum necessary permissions:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ROLE_PERMISSIONS=(
    compute.disks.get
    compute.disks.create
    compute.disks.createSnapshot
    compute.snapshots.get
    compute.snapshots.create
    compute.snapshots.useReadOnly
    compute.snapshots.delete
    compute.zones.get
    storage.objects.create
    storage.objects.delete
    storage.objects.get
    storage.objects.list
    iam.serviceAccounts.signBlob
)</programlisting>
</listitem>
<listitem>
<simpara>Create the <literal>velero.server</literal> custom role:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam roles create velero.server \
    --project $PROJECT_ID \
    --title "Velero Server" \
    --permissions "$(IFS=","; echo "${ROLE_PERMISSIONS[*]}")"</programlisting>
</listitem>
<listitem>
<simpara>Add IAM policy binding to the project:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member serviceAccount:$SERVICE_ACCOUNT_EMAIL \
    --role projects/$PROJECT_ID/roles/velero.server</programlisting>
</listitem>
<listitem>
<simpara>Update the IAM service account:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_EMAIL:objectAdmin gs://${BUCKET}</programlisting>
</listitem>
<listitem>
<simpara>Save the IAM service account keys to the <literal>credentials-velero</literal> file in the current directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ gcloud iam service-accounts keys create credentials-velero \
    --iam-account $SERVICE_ACCOUNT_EMAIL</programlisting>
<simpara>You use the <literal>credentials-velero</literal> file to create a <literal>Secret</literal> object for GCP before you install the Data Protection Application.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-about-backup-snapshot-locations_installing-oadp-gcp">
<title>About backup and snapshot locations and their secrets</title>
<simpara>You specify backup and snapshot locations and their secrets in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<bridgehead xml:id="backup-locations_installing-oadp-gcp" renderas="sect5">Backup locations</bridgehead>
<simpara>You specify AWS S3-compatible object storage, such as Multicloud Object Gateway or MinIO, as a backup location.</simpara>
<simpara>Velero backs up OpenShift Container Platform resources, Kubernetes objects, and internal images as an archive file on object storage.</simpara>
<bridgehead xml:id="snapshot-locations_installing-oadp-gcp" renderas="sect5">Snapshot locations</bridgehead>
<simpara>If you use your cloud provider&#8217;s native snapshot API to back up persistent volumes, you must specify the cloud provider as the snapshot location.</simpara>
<simpara>If you use Container Storage Interface (CSI) snapshots, you do not need to specify a snapshot location because you will create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
<simpara>If you use File System Backup (FSB), you do not need to specify a snapshot location because FSB backs up the file system on object storage.</simpara>
<bridgehead xml:id="secrets_installing-oadp-gcp" renderas="sect5">Secrets</bridgehead>
<simpara>If the backup and snapshot locations use the same credentials or if you do not require a snapshot location, you create a default <literal>Secret</literal>.</simpara>
<simpara>If the backup and snapshot locations use different credentials, you create two secret objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Custom <literal>Secret</literal> for the backup location, which you specify in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Default <literal>Secret</literal> for the snapshot location, which is not referenced in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The Data Protection Application requires a default <literal>Secret</literal>. Otherwise, the installation will fail.</simpara>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file.</simpara>
</important>
<section xml:id="oadp-creating-default-secret_installing-oadp-gcp">
<title>Creating a default Secret</title>
<simpara>You create a default <literal>Secret</literal> if your backup and snapshot locations use the same credentials or if you do not require a snapshot location.</simpara>
<simpara>The default name of the <literal>Secret</literal> is <literal>cloud-credentials-gcp</literal>.</simpara>
<note>
<simpara>The <literal>DataProtectionApplication</literal> custom resource (CR) requires a default <literal>Secret</literal>.  Otherwise, the installation will fail. If the name of the backup location <literal>Secret</literal> is not specified, the default name is used.</simpara>
<simpara>If you do not want to use the backup location credentials during the installation, you can create a <literal>Secret</literal> with the default name by using an empty <literal>credentials-velero</literal> file.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your object storage and cloud storage, if any, must use the same credentials.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must create a <literal>credentials-velero</literal> file for the object storage in the appropriate format.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Secret</literal> with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials-gcp -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
</itemizedlist>
<simpara>The <literal>Secret</literal> is referenced in the <literal>spec.backupLocations.credential</literal> block of the <literal>DataProtectionApplication</literal> CR when you install the Data Protection Application.</simpara>
</section>
<section xml:id="oadp-secrets-for-different-credentials_installing-oadp-gcp">
<title>Creating secrets for different credentials</title>
<simpara>If your backup and snapshot locations use different credentials, you must create two <literal>Secret</literal> objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Backup location <literal>Secret</literal> with a custom name. The custom name is specified in the <literal>spec.backupLocations</literal> block of the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Snapshot location <literal>Secret</literal> with the default name, <literal>cloud-credentials-gcp</literal>. This <literal>Secret</literal> is not specified in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file for the snapshot location in the appropriate format for your cloud provider.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> for the snapshot location with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials-gcp -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file for the backup location in the appropriate format for your object storage.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> for the backup location with a custom name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic &lt;custom_secret&gt; -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>Secret</literal> with the custom name to the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
...
  backupLocations:
    - velero:
        provider: gcp
        default: true
        credential:
          key: cloud
          name: &lt;custom_secret&gt; <co xml:id="CO20-1"/>
        objectStorage:
          bucket: &lt;bucket_name&gt;
          prefix: &lt;prefix&gt;
  snapshotLocations:
    - velero:
        provider: gcp
        default: true
        config:
          project: &lt;project&gt;
          snapshotLocation: us-west1</programlisting>
<calloutlist>
<callout arearefs="CO20-1">
<para>Backup location <literal>Secret</literal> with custom name.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-dpa-gcp">
<title>Configuring the Data Protection Application</title>
<simpara>You can configure the Data Protection Application by setting Velero resource allocations or enabling self-signed CA certificates.</simpara>
<section xml:id="oadp-setting-resource-limits-and-requests_installing-oadp-gcp">
<title>Setting Velero CPU and memory resource allocations</title>
<simpara>You set the CPU and memory resource allocations for the <literal>Velero</literal> pod by editing the  <literal>DataProtectionApplication</literal> custom resource (CR) manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.podConfig.ResourceAllocations</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  configuration:
    velero:
      podConfig:
        nodeSelector: &lt;node selector&gt; <co xml:id="CO21-1"/>
        resourceAllocations: <co xml:id="CO21-2"/>
          limits:
            cpu: "1"
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 256Mi</programlisting>
<calloutlist>
<callout arearefs="CO21-1">
<para>Specify the node selector to be supplied to Velero podSpec.</para>
</callout>
<callout arearefs="CO21-2">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>Kopia is an option in OADP 1.3 and later releases. You can use Kopia for file system backups, and Kopia is your only option for Data Mover cases with the built-in Data Mover.</simpara>
<simpara>Kopia is more resource intensive than Restic, and you might need to adjust the CPU and memory requirements accordingly.</simpara>
</note>
</section>
<section xml:id="oadp-self-signed-certificate_installing-oadp-gcp">
<title>Enabling self-signed CA certificates</title>
<simpara>You must enable a self-signed CA certificate for object storage by editing the <literal>DataProtectionApplication</literal> custom resource (CR) manifest to prevent a <literal>certificate signed by unknown authority</literal> error.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.backupLocations.velero.objectStorage.caCert</literal> parameter and <literal>spec.backupLocations.velero.config</literal> parameters of the <literal>DataProtectionApplication</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket&gt;
          prefix: &lt;prefix&gt;
          caCert: &lt;base64_encoded_cert_string&gt; <co xml:id="CO22-1"/>
        config:
          insecureSkipTLSVerify: "false" <co xml:id="CO22-2"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO22-1">
<para>Specify the Base64-encoded CA certificate string.</para>
</callout>
<callout arearefs="CO22-2">
<para>The <literal>insecureSkipTLSVerify</literal> configuration can be set to either <literal>"true"</literal> or <literal>"false"</literal>. If set to <literal>"true"</literal>, SSL/TLS security is disabled. If set to <literal>"false"</literal>, SSL/TLS security is enabled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="oadp-using-ca-certificates-with-velero-command-aliased-for-velero-deployment_installing-oadp-gcp">
<title>Using CA certificates with the velero command aliased for Velero deployment</title>
<simpara>You might want to use the Velero CLI without installing it locally on your system by creating an alias for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To use an aliased Velero command, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
<listitem>
<simpara>Check that the alias is working by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero version
Client:
	Version: v1.12.1-OADP
	Git commit: -
Server:
	Version: v1.12.1-OADP</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To use a CA certificate with this command, you can add a certificate to the Velero deployment by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CA_CERT=$(oc -n openshift-adp get dataprotectionapplications.oadp.openshift.io &lt;dpa-name&gt; -o jsonpath='{.spec.backupLocations[0].velero.objectStorage.caCert}')

$ [[ -n $CA_CERT ]] &amp;&amp; echo "$CA_CERT" | base64 -d | oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "cat &gt; /tmp/your-cacert.txt" || echo "DPA BSL has no caCert"</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ velero -n openshift-adp describe backup &lt;backup-name&gt; --details --cacert /tmp/your-cacert.txt</programlisting>
</listitem>
<listitem>
<simpara>If the Velero pod restarts, the <literal>/tmp/your-cacert.txt</literal> file disappears, and you must re-create the <literal>/tmp/your-cacert.txt</literal> file by re-running the commands from the previous step.</simpara>
</listitem>
<listitem>
<simpara>You can check if the <literal>/tmp/your-cacert.txt</literal> file still exists, in the file location where you stored it, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "ls /tmp/your-cacert.txt"
/tmp/your-cacert.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<simpara>In a future release of OpenShift API for Data Protection (OADP), we plan to mount the certificate to the Velero pod so that this step is not required.</simpara>
</section>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-2-and-earlier_installing-oadp-gcp">
<title>Installing the Data Protection Application 1.2 and earlier</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials-gcp</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
<note>
<simpara>Velero creates a secret named <literal>velero-repo-credentials</literal> in the OADP namespace, which contains a default backup repository password.
You can update the secret with your own password encoded as base64 <emphasis role="strong">before</emphasis> you run your first backup targeted to the backup repository. The value of the key to update is <literal>Data[repository-password]</literal>.</simpara>
<simpara>After you create your DPA, the first time that you run a backup targeted to the backup repository, Velero creates a backup repository whose secret is <literal>velero-repo-credentials</literal>, which contains either the default password or the one you replaced it with.
If you update the secret password <emphasis role="strong">after</emphasis> the first backup, the new password will not match the password in <literal>velero-repo-credentials</literal>, and therefore, Velero will not be able to connect with the older backups.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
  configuration:
    velero:
      defaultPlugins:
        - gcp
        - openshift <co xml:id="CO23-1"/>
      resourceTimeout: 10m <co xml:id="CO23-2"/>
    restic:
      enable: true <co xml:id="CO23-3"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO23-4"/>
  backupLocations:
    - velero:
        provider: gcp
        default: true
        credential:
          key: cloud <co xml:id="CO23-5"/>
          name: cloud-credentials-gcp <co xml:id="CO23-6"/>
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO23-7"/>
          prefix: &lt;prefix&gt; <co xml:id="CO23-8"/>
  snapshotLocations: <co xml:id="CO23-9"/>
    - velero:
        provider: gcp
        default: true
        config:
          project: &lt;project&gt;
          snapshotLocation: us-west1 <co xml:id="CO23-10"/></programlisting>
<calloutlist>
<callout arearefs="CO23-1">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO23-2">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO23-3">
<para>Set this value to <literal>false</literal> if you want to disable the Restic installation. Restic deploys a daemon set, which means that Restic pods run on each working node. In OADP version 1.2 and later, you can configure Restic for backups by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR. In OADP version 1.1, add <literal>spec.defaultVolumesToRestic: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO23-4">
<para>Specify on which nodes Restic is available. By default, Restic runs on all nodes.</para>
</callout>
<callout arearefs="CO23-5">
<para>Secret key that contains credentials. For Google workload identity federation cloud authentication use <literal>service_account.json</literal>.</para>
</callout>
<callout arearefs="CO23-6">
<para>Secret name that contains credentials. If you do not specify this value, the default name, <literal>cloud-credentials-gcp</literal>, is used.</para>
</callout>
<callout arearefs="CO23-7">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO23-8">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
<callout arearefs="CO23-9">
<para>Specify a snapshot location, unless you use CSI snapshots or Restic to back up PVs.</para>
</callout>
<callout arearefs="CO23-10">
<para>The snapshot location must be in the same region as the PVs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-2_installing-oadp-gcp">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-gcp-wif-cloud-authentication_installing-oadp-gcp">
<title>Google workload identity federation cloud authentication</title>
<simpara>Applications running outside Google Cloud use service account keys, such as usernames and passwords, to gain access to Google Cloud resources. These service account keys might become a security risk if they are not properly managed.</simpara>
<simpara>With Google&#8217;s workload identity federation, you can use Identity and Access Management (IAM) to offer IAM roles, including the ability to impersonate service accounts, to external identities. This eliminates the maintenance and security risks associated with service account keys.</simpara>
<simpara>Workload identity federation handles encrypting and decrypting certificates, extracting user attributes, and validation. Identity federation externalizes authentication, passing it over to Security Token Services (STS), and reduces the demands on individual developers. Authorization and controlling access to resources remain the responsibility of the application.</simpara>
<note>
<simpara>Google workload identity federation is available for OADP 1.3.x and later.</simpara>
</note>
<note>
<simpara>For backing up volumes, OADP on GCP with Google workload identity federation authentication supports only CSI snapshots.</simpara>
</note>
<simpara>If you do not use Google workload identity federation cloud authentication, continue to <emphasis>Installing the Data Protection Application</emphasis>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have installed a cluster in manual mode with <link xlink:href="https://docs.openshift.com/container-platform/4.15/installing/installing_gcp/installing-gcp-customizations.html#installing-gcp-with-short-term-creds_installing-gcp-customizations">GCP Workload Identity configured</link>.</simpara>
</listitem>
<listitem>
<simpara>You have access to the Cloud Credential Operator utility (<literal>ccoctl</literal>) and to the associated workload identity pool.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an <literal>oadp-credrequest</literal> directory by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ mkdir -p oadp-credrequest</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>CredentialsRequest.yaml</literal> file as following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">echo 'apiVersion: cloudcredential.openshift.io/v1
kind: CredentialsRequest
metadata:
  name: oadp-operator-credentials
  namespace: openshift-cloud-credential-operator
spec:
  providerSpec:
    apiVersion: cloudcredential.openshift.io/v1
    kind: GCPProviderSpec
    permissions:
    - compute.disks.get
    - compute.disks.create
    - compute.disks.createSnapshot
    - compute.snapshots.get
    - compute.snapshots.create
    - compute.snapshots.useReadOnly
    - compute.snapshots.delete
    - compute.zones.get
    - storage.objects.create
    - storage.objects.delete
    - storage.objects.get
    - storage.objects.list
    - iam.serviceAccounts.signBlob
    skipServiceCheck: true
  secretRef:
    name: cloud-credentials-gcp
    namespace: &lt;OPERATOR_INSTALL_NS&gt;
  serviceAccountNames:
  - velero
' &gt; oadp-credrequest/credrequest.yaml</programlisting>
</listitem>
<listitem>
<simpara>Use the <literal>ccoctl</literal> utility to process the <literal>CredentialsRequest</literal> objects in the <literal>oadp-credrequest</literal> directory by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ccoctl gcp create-service-accounts \
    --name=&lt;name&gt; \
    --project=&lt;gcp_project_id&gt; \
    --credentials-requests-dir=oadp-credrequest \
    --workload-identity-pool=&lt;pool_id&gt; \
    --workload-identity-provider=&lt;provider_id&gt;</programlisting>
<simpara>The <literal>manifests/openshift-adp-cloud-credentials-gcp-credentials.yaml</literal> file is now available to use in the following steps.</simpara>
</listitem>
<listitem>
<simpara>Create a namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace &lt;OPERATOR_INSTALL_NS&gt;</programlisting>
</listitem>
<listitem>
<simpara>Apply the credentials to the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f manifests/openshift-adp-cloud-credentials-gcp-credentials.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-installing-dpa-1-3_installing-oadp-gcp">
<title>Installing the Data Protection Application 1.3</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials-gcp</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: &lt;OPERATOR_INSTALL_NS&gt; <co xml:id="CO24-1"/>
spec:
  configuration:
    velero:
      defaultPlugins:
        - gcp
        - openshift <co xml:id="CO24-2"/>
      resourceTimeout: 10m <co xml:id="CO24-3"/>
    nodeAgent: <co xml:id="CO24-4"/>
      enable: true <co xml:id="CO24-5"/>
      uploaderType: kopia <co xml:id="CO24-6"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO24-7"/>
  backupLocations:
    - velero:
        provider: gcp
        default: true
        credential:
          key: cloud <co xml:id="CO24-8"/>
          name: cloud-credentials-gcp <co xml:id="CO24-9"/>
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO24-10"/>
          prefix: &lt;prefix&gt; <co xml:id="CO24-11"/>
  snapshotLocations: <co xml:id="CO24-12"/>
    - velero:
        provider: gcp
        default: true
        config:
          project: &lt;project&gt;
          snapshotLocation: us-west1 <co xml:id="CO24-13"/></programlisting>
<calloutlist>
<callout arearefs="CO24-1">
<para>The default namespace for OADP is <literal>openshift-adp</literal>. The namespace is a variable and is configurable.</para>
</callout>
<callout arearefs="CO24-2">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO24-3">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO24-4">
<para>The administrative agent that routes the administrative requests to servers.</para>
</callout>
<callout arearefs="CO24-5">
<para>Set this value to <literal>true</literal> if you want to enable <literal>nodeAgent</literal> and perform File System Backup.</para>
</callout>
<callout arearefs="CO24-6">
<para>Enter <literal>kopia</literal> or <literal>restic</literal> as your uploader. You cannot change the selection after the installation. For the Built-in DataMover you must use Kopia. The <literal>nodeAgent</literal> deploys a daemon set, which means that the <literal>nodeAgent</literal> pods run on each working node. You can configure File System Backup by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO24-7">
<para>Specify the nodes on which Kopia or Restic are available. By default, Kopia or Restic run on all nodes.</para>
</callout>
<callout arearefs="CO24-8">
<para>Secret key that contains credentials. For Google workload identity federation cloud authentication use <literal>service_account.json</literal>.</para>
</callout>
<callout arearefs="CO24-9">
<para>Secret name that contains credentials. If you do not specify this value, the default name, <literal>cloud-credentials-gcp</literal>, is used.</para>
</callout>
<callout arearefs="CO24-10">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO24-11">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
<callout arearefs="CO24-12">
<para>Specify a snapshot location, unless you use CSI snapshots or Restic to back up PVs.</para>
</callout>
<callout arearefs="CO24-13">
<para>The snapshot location must be in the same region as the PVs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-3_installing-oadp-gcp">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-enabling-csi-dpa_installing-oadp-gcp">
<title>Enabling CSI in the DataProtectionApplication CR</title>
<simpara>You enable the Container Storage Interface (CSI) in the <literal>DataProtectionApplication</literal> custom resource (CR) in order to back up persistent volumes with CSI snapshots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - csi <co xml:id="CO25-1"/></programlisting>
<calloutlist>
<callout arearefs="CO25-1">
<para>Add the <literal>csi</literal> default plugin.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installing-oadp-mcg">
<title>Configuring the OpenShift API for Data Protection with Multicloud Object Gateway</title>

<simpara>You install the OpenShift API for Data Protection (OADP) with Multicloud Object Gateway (MCG) by installing the OADP Operator. The Operator installs <link xlink:href="https://velero.io/docs/v1.12/">Velero 1.12</link>.</simpara>
<note>
<simpara>Starting from OADP 1.0.4, all OADP 1.0.<emphasis>z</emphasis> versions can only be used as a dependency of the MTC Operator and are not available as a standalone Operator.</simpara>
</note>
<simpara>You configure <link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-mcg.xml#installing-oadp-mcg">Multicloud Object Gateway</link> as a backup location.
MCG is a component of OpenShift Data Foundation. You configure MCG as a backup location in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<important>
<simpara>The <literal>CloudStorage</literal> API, which automates the creation of a bucket for object storage, is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You create a <literal>Secret</literal> for the backup location and then you install the Data Protection Application. For more details, see <link xlink:href="../../..//backup_and_restore/application_backup_and_restore/installing/oadp-installing-operator.xml#oadp-installing-operator-doc">Installing the OADP Operator</link>.</simpara>
<simpara>To install the OADP Operator in a restricted network environment, you must first disable the default OperatorHub sources and mirror the Operator catalog. For details, see <link xlink:href="../../../operators/admin/olm-restricted-networks.xml#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
<section xml:id="migration-configuring-mcg_installing-oadp-mcg">
<title>Retrieving Multicloud Object Gateway credentials</title>
<simpara>You must retrieve the Multicloud Object Gateway (MCG) credentials in order to create a <literal>Secret</literal> custom resource (CR) for the OpenShift API for Data Protection (OADP).</simpara>
<simpara>MCG is a component of OpenShift Data Foundation.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must deploy OpenShift Data Foundation by using the appropriate <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9">OpenShift Data Foundation deployment guide</link>.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Obtain the S3 endpoint, <literal>AWS_ACCESS_KEY_ID</literal>, and <literal>AWS_SECRET_ACCESS_KEY</literal> by running the <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/managing_hybrid_and_multicloud_resources/accessing-the-multicloud-object-gateway-with-your-applications_rhodf#accessing-the-Multicloud-object-gateway-from-the-terminal_rhodf"><literal>describe</literal> command</link> on the <literal>NooBaa</literal> custom resource.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF &gt; ./credentials-velero
[default]
aws_access_key_id=&lt;AWS_ACCESS_KEY_ID&gt;
aws_secret_access_key=&lt;AWS_SECRET_ACCESS_KEY&gt;
EOF</programlisting>
<simpara>You use the <literal>credentials-velero</literal> file to create a <literal>Secret</literal> object when you install the Data Protection Application.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-about-backup-snapshot-locations_installing-oadp-mcg">
<title>About backup and snapshot locations and their secrets</title>
<simpara>You specify backup and snapshot locations and their secrets in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<bridgehead xml:id="backup-locations_installing-oadp-mcg" renderas="sect5">Backup locations</bridgehead>
<simpara>You specify AWS S3-compatible object storage, such as Multicloud Object Gateway or MinIO, as a backup location.</simpara>
<simpara>Velero backs up OpenShift Container Platform resources, Kubernetes objects, and internal images as an archive file on object storage.</simpara>
<bridgehead xml:id="snapshot-locations_installing-oadp-mcg" renderas="sect5">Snapshot locations</bridgehead>
<simpara>If you use your cloud provider&#8217;s native snapshot API to back up persistent volumes, you must specify the cloud provider as the snapshot location.</simpara>
<simpara>If you use Container Storage Interface (CSI) snapshots, you do not need to specify a snapshot location because you will create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
<simpara>If you use File System Backup (FSB), you do not need to specify a snapshot location because FSB backs up the file system on object storage.</simpara>
<bridgehead xml:id="secrets_installing-oadp-mcg" renderas="sect5">Secrets</bridgehead>
<simpara>If the backup and snapshot locations use the same credentials or if you do not require a snapshot location, you create a default <literal>Secret</literal>.</simpara>
<simpara>If the backup and snapshot locations use different credentials, you create two secret objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Custom <literal>Secret</literal> for the backup location, which you specify in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Default <literal>Secret</literal> for the snapshot location, which is not referenced in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The Data Protection Application requires a default <literal>Secret</literal>. Otherwise, the installation will fail.</simpara>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file.</simpara>
</important>
<section xml:id="oadp-creating-default-secret_installing-oadp-mcg">
<title>Creating a default Secret</title>
<simpara>You create a default <literal>Secret</literal> if your backup and snapshot locations use the same credentials or if you do not require a snapshot location.</simpara>
<simpara>The default name of the <literal>Secret</literal> is <literal>cloud-credentials</literal>.</simpara>
<note>
<simpara>The <literal>DataProtectionApplication</literal> custom resource (CR) requires a default <literal>Secret</literal>.  Otherwise, the installation will fail. If the name of the backup location <literal>Secret</literal> is not specified, the default name is used.</simpara>
<simpara>If you do not want to use the backup location credentials during the installation, you can create a <literal>Secret</literal> with the default name by using an empty <literal>credentials-velero</literal> file.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your object storage and cloud storage, if any, must use the same credentials.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must create a <literal>credentials-velero</literal> file for the object storage in the appropriate format.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Secret</literal> with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
</itemizedlist>
<simpara>The <literal>Secret</literal> is referenced in the <literal>spec.backupLocations.credential</literal> block of the <literal>DataProtectionApplication</literal> CR when you install the Data Protection Application.</simpara>
</section>
<section xml:id="oadp-secrets-for-different-credentials_installing-oadp-mcg">
<title>Creating secrets for different credentials</title>
<simpara>If your backup and snapshot locations use different credentials, you must create two <literal>Secret</literal> objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Backup location <literal>Secret</literal> with a custom name. The custom name is specified in the <literal>spec.backupLocations</literal> block of the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>Snapshot location <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>. This <literal>Secret</literal> is not specified in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file for the snapshot location in the appropriate format for your cloud provider.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> for the snapshot location with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>credentials-velero</literal> file for the backup location in the appropriate format for your object storage.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Secret</literal> for the backup location with a custom name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic &lt;custom_secret&gt; -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
<listitem>
<simpara>Add the <literal>Secret</literal> with the custom name to the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
...
  backupLocations:
    - velero:
        config:
          profile: "default"
          region: minio
          s3Url: &lt;url&gt;
          insecureSkipTLSVerify: "true"
          s3ForcePathStyle: "true"
        provider: aws
        default: true
        credential:
          key: cloud
          name:  &lt;custom_secret&gt; <co xml:id="CO26-1"/>
        objectStorage:
          bucket: &lt;bucket_name&gt;
          prefix: &lt;prefix&gt;</programlisting>
<calloutlist>
<callout arearefs="CO26-1">
<para>Backup location <literal>Secret</literal> with custom name.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="configuring-dpa-mcg">
<title>Configuring the Data Protection Application</title>
<simpara>You can configure the Data Protection Application by setting Velero resource allocations or enabling self-signed CA certificates.</simpara>
<section xml:id="oadp-setting-resource-limits-and-requests_installing-oadp-mcg">
<title>Setting Velero CPU and memory resource allocations</title>
<simpara>You set the CPU and memory resource allocations for the <literal>Velero</literal> pod by editing the  <literal>DataProtectionApplication</literal> custom resource (CR) manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.podConfig.ResourceAllocations</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  configuration:
    velero:
      podConfig:
        nodeSelector: &lt;node selector&gt; <co xml:id="CO27-1"/>
        resourceAllocations: <co xml:id="CO27-2"/>
          limits:
            cpu: "1"
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 256Mi</programlisting>
<calloutlist>
<callout arearefs="CO27-1">
<para>Specify the node selector to be supplied to Velero podSpec.</para>
</callout>
<callout arearefs="CO27-2">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>Kopia is an option in OADP 1.3 and later releases. You can use Kopia for file system backups, and Kopia is your only option for Data Mover cases with the built-in Data Mover.</simpara>
<simpara>Kopia is more resource intensive than Restic, and you might need to adjust the CPU and memory requirements accordingly.</simpara>
</note>
</section>
<section xml:id="oadp-self-signed-certificate_installing-oadp-mcg">
<title>Enabling self-signed CA certificates</title>
<simpara>You must enable a self-signed CA certificate for object storage by editing the <literal>DataProtectionApplication</literal> custom resource (CR) manifest to prevent a <literal>certificate signed by unknown authority</literal> error.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.backupLocations.velero.objectStorage.caCert</literal> parameter and <literal>spec.backupLocations.velero.config</literal> parameters of the <literal>DataProtectionApplication</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket&gt;
          prefix: &lt;prefix&gt;
          caCert: &lt;base64_encoded_cert_string&gt; <co xml:id="CO28-1"/>
        config:
          insecureSkipTLSVerify: "false" <co xml:id="CO28-2"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO28-1">
<para>Specify the Base64-encoded CA certificate string.</para>
</callout>
<callout arearefs="CO28-2">
<para>The <literal>insecureSkipTLSVerify</literal> configuration can be set to either <literal>"true"</literal> or <literal>"false"</literal>. If set to <literal>"true"</literal>, SSL/TLS security is disabled. If set to <literal>"false"</literal>, SSL/TLS security is enabled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="oadp-using-ca-certificates-with-velero-command-aliased-for-velero-deployment_installing-oadp-mcg">
<title>Using CA certificates with the velero command aliased for Velero deployment</title>
<simpara>You might want to use the Velero CLI without installing it locally on your system by creating an alias for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To use an aliased Velero command, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
<listitem>
<simpara>Check that the alias is working by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero version
Client:
	Version: v1.12.1-OADP
	Git commit: -
Server:
	Version: v1.12.1-OADP</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To use a CA certificate with this command, you can add a certificate to the Velero deployment by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CA_CERT=$(oc -n openshift-adp get dataprotectionapplications.oadp.openshift.io &lt;dpa-name&gt; -o jsonpath='{.spec.backupLocations[0].velero.objectStorage.caCert}')

$ [[ -n $CA_CERT ]] &amp;&amp; echo "$CA_CERT" | base64 -d | oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "cat &gt; /tmp/your-cacert.txt" || echo "DPA BSL has no caCert"</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ velero -n openshift-adp describe backup &lt;backup-name&gt; --details --cacert /tmp/your-cacert.txt</programlisting>
</listitem>
<listitem>
<simpara>If the Velero pod restarts, the <literal>/tmp/your-cacert.txt</literal> file disappears, and you must re-create the <literal>/tmp/your-cacert.txt</literal> file by re-running the commands from the previous step.</simpara>
</listitem>
<listitem>
<simpara>You can check if the <literal>/tmp/your-cacert.txt</literal> file still exists, in the file location where you stored it, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "ls /tmp/your-cacert.txt"
/tmp/your-cacert.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<simpara>In a future release of OpenShift API for Data Protection (OADP), we plan to mount the certificate to the Velero pod so that this step is not required.</simpara>
</section>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-2-and-earlier_installing-oadp-mcg">
<title>Installing the Data Protection Application 1.2 and earlier</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
<note>
<simpara>Velero creates a secret named <literal>velero-repo-credentials</literal> in the OADP namespace, which contains a default backup repository password.
You can update the secret with your own password encoded as base64 <emphasis role="strong">before</emphasis> you run your first backup targeted to the backup repository. The value of the key to update is <literal>Data[repository-password]</literal>.</simpara>
<simpara>After you create your DPA, the first time that you run a backup targeted to the backup repository, Velero creates a backup repository whose secret is <literal>velero-repo-credentials</literal>, which contains either the default password or the one you replaced it with.
If you update the secret password <emphasis role="strong">after</emphasis> the first backup, the new password will not match the password in <literal>velero-repo-credentials</literal>, and therefore, Velero will not be able to connect with the older backups.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp
spec:
  configuration:
    velero:
      defaultPlugins:
        - aws
        - openshift <co xml:id="CO29-1"/>
      resourceTimeout: 10m <co xml:id="CO29-2"/>
    restic:
      enable: true <co xml:id="CO29-3"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO29-4"/>
  backupLocations:
    - velero:
        config:
          profile: "default"
          region: minio
          s3Url: &lt;url&gt; <co xml:id="CO29-5"/>
          insecureSkipTLSVerify: "true"
          s3ForcePathStyle: "true"
        provider: aws
        default: true
        credential:
          key: cloud
          name: cloud-credentials <co xml:id="CO29-6"/>
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO29-7"/>
          prefix: &lt;prefix&gt; <co xml:id="CO29-8"/></programlisting>
<calloutlist>
<callout arearefs="CO29-1">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO29-2">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO29-3">
<para>Set this value to <literal>false</literal> if you want to disable the Restic installation. Restic deploys a daemon set, which means that Restic pods run on each working node. In OADP version 1.2 and later, you can configure Restic for backups by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR. In OADP version 1.1, add <literal>spec.defaultVolumesToRestic: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO29-4">
<para>Specify on which nodes Restic is available. By default, Restic runs on all nodes.</para>
</callout>
<callout arearefs="CO29-5">
<para>Specify the URL of the S3 endpoint.</para>
</callout>
<callout arearefs="CO29-6">
<para>If you do not specify this value, the default name, <literal>cloud-credentials</literal>, is used. If you specify a custom name, the custom name is used for the backup location.</para>
</callout>
<callout arearefs="CO29-7">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO29-8">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-2_installing-oadp-mcg">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-3_installing-oadp-mcg">
<title>Installing the Data Protection Application 1.3</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use different credentials, you must create two <literal>Secrets</literal>:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>Secret</literal> with a custom name for the backup location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara><literal>Secret</literal> with another custom name for the snapshot location. You add this <literal>Secret</literal> to the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
  namespace: openshift-adp <co xml:id="CO30-1"/>
spec:
  configuration:
    velero:
      defaultPlugins:
        - aws
        - openshift <co xml:id="CO30-2"/>
      resourceTimeout: 10m <co xml:id="CO30-3"/>
    nodeAgent: <co xml:id="CO30-4"/>
      enable: true <co xml:id="CO30-5"/>
      uploaderType: kopia <co xml:id="CO30-6"/>
      podConfig:
        nodeSelector: &lt;node_selector&gt; <co xml:id="CO30-7"/>
  backupLocations:
    - velero:
        config:
          profile: "default"
          region: minio
          s3Url: &lt;url&gt; <co xml:id="CO30-8"/>
          insecureSkipTLSVerify: "true"
          s3ForcePathStyle: "true"
        provider: aws
        default: true
        credential:
          key: cloud
          name: cloud-credentials <co xml:id="CO30-9"/>
        objectStorage:
          bucket: &lt;bucket_name&gt; <co xml:id="CO30-10"/>
          prefix: &lt;prefix&gt; <co xml:id="CO30-11"/></programlisting>
<calloutlist>
<callout arearefs="CO30-1">
<para>The default namespace for OADP is <literal>openshift-adp</literal>. The namespace is a variable and is configurable.</para>
</callout>
<callout arearefs="CO30-2">
<para>The <literal>openshift</literal> plugin is mandatory.</para>
</callout>
<callout arearefs="CO30-3">
<para>Specify how many minutes to wait for several Velero resources before timeout occurs, such as Velero CRD availability, volumeSnapshot deletion, and backup repository availability. The default is 10m.</para>
</callout>
<callout arearefs="CO30-4">
<para>The administrative agent that routes the administrative requests to servers.</para>
</callout>
<callout arearefs="CO30-5">
<para>Set this value to <literal>true</literal> if you want to enable <literal>nodeAgent</literal> and perform File System Backup.</para>
</callout>
<callout arearefs="CO30-6">
<para>Enter <literal>kopia</literal> or <literal>restic</literal> as your uploader. You cannot change the selection after the installation. For the Built-in DataMover you must use Kopia. The <literal>nodeAgent</literal> deploys a daemon set, which means that the <literal>nodeAgent</literal> pods run on each working node. You can configure File System Backup by adding <literal>spec.defaultVolumesToFsBackup: true</literal> to the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO30-7">
<para>Specify the nodes on which Kopia or Restic are available. By default, Kopia or Restic run on all nodes.</para>
</callout>
<callout arearefs="CO30-8">
<para>Specify the URL of the S3 endpoint.</para>
</callout>
<callout arearefs="CO30-9">
<para>If you do not specify this value, the default name, <literal>cloud-credentials</literal>, is used. If you specify a custom name, the custom name is used for the backup location.</para>
</callout>
<callout arearefs="CO30-10">
<para>Specify a bucket as the backup storage location. If the bucket is not a dedicated bucket for Velero backups, you must specify a prefix.</para>
</callout>
<callout arearefs="CO30-11">
<para>Specify a prefix for Velero backups, for example, <literal>velero</literal>, if the bucket is used for multiple purposes.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-3_installing-oadp-mcg">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-enabling-csi-dpa_installing-oadp-mcg">
<title>Enabling CSI in the DataProtectionApplication CR</title>
<simpara>You enable the Container Storage Interface (CSI) in the <literal>DataProtectionApplication</literal> custom resource (CR) in order to back up persistent volumes with CSI snapshots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - csi <co xml:id="CO31-1"/></programlisting>
<calloutlist>
<callout arearefs="CO31-1">
<para>Add the <literal>csi</literal> default plugin.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="installing-oadp-ocs">
<title>Configuring the OpenShift API for Data Protection with OpenShift Data Foundation</title>

<simpara>You install the OpenShift API for Data Protection (OADP) with OpenShift Data Foundation by installing the OADP Operator and configuring a backup location and a snapshot location. Then, you install the Data Protection Application.</simpara>
<note>
<simpara>Starting from OADP 1.0.4, all OADP 1.0.<emphasis>z</emphasis> versions can only be used as a dependency of the MTC Operator and are not available as a standalone Operator.</simpara>
</note>
<simpara>You can configure <link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-mcg.xml#installing-oadp-mcg">Multicloud Object Gateway</link> or any AWS S3-compatible object storage as a backup location.</simpara>
<important>
<simpara>The <literal>CloudStorage</literal> API, which automates the creation of a bucket for object storage, is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara>You create a <literal>Secret</literal> for the backup location and then you install the Data Protection Application. For more details, see <link xlink:href="../../..//backup_and_restore/application_backup_and_restore/installing/oadp-installing-operator.xml#oadp-installing-operator-doc">Installing the OADP Operator</link>.</simpara>
<simpara>To install the OADP Operator in a restricted network environment, you must first disable the default OperatorHub sources and mirror the Operator catalog. For details, see <link xlink:href="../../../operators/admin/olm-restricted-networks.xml#olm-restricted-networks">Using Operator Lifecycle Manager on restricted networks</link>.</simpara>
<section xml:id="oadp-about-backup-snapshot-locations_installing-oadp-ocs">
<title>About backup and snapshot locations and their secrets</title>
<simpara>You specify backup and snapshot locations and their secrets in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
<bridgehead xml:id="backup-locations_installing-oadp-ocs" renderas="sect5">Backup locations</bridgehead>
<simpara>You specify AWS S3-compatible object storage, such as Multicloud Object Gateway or MinIO, as a backup location.</simpara>
<simpara>Velero backs up OpenShift Container Platform resources, Kubernetes objects, and internal images as an archive file on object storage.</simpara>
<bridgehead xml:id="snapshot-locations_installing-oadp-ocs" renderas="sect5">Snapshot locations</bridgehead>
<simpara>If you use your cloud provider&#8217;s native snapshot API to back up persistent volumes, you must specify the cloud provider as the snapshot location.</simpara>
<simpara>If you use Container Storage Interface (CSI) snapshots, you do not need to specify a snapshot location because you will create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
<simpara>If you use File System Backup (FSB), you do not need to specify a snapshot location because FSB backs up the file system on object storage.</simpara>
<bridgehead xml:id="secrets_installing-oadp-ocs" renderas="sect5">Secrets</bridgehead>
<simpara>If the backup and snapshot locations use the same credentials or if you do not require a snapshot location, you create a default <literal>Secret</literal>.</simpara>
<simpara>If the backup and snapshot locations use different credentials, you create two secret objects:</simpara>
<itemizedlist>
<listitem>
<simpara>Custom <literal>Secret</literal> for the backup location, which you specify in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>Default <literal>Secret</literal> for the snapshot location, which is not referenced in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The Data Protection Application requires a default <literal>Secret</literal>. Otherwise, the installation will fail.</simpara>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file.</simpara>
</important>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.13/html/managing_hybrid_and_multicloud_resources/object-bucket-claim#creating-an-object-bucket-claim-using-the-openshift-web-console_rhodf">Creating an Object Bucket Claim using the OpenShift Web Console</link>.</simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-creating-default-secret_installing-oadp-ocs">
<title>Creating a default Secret</title>
<simpara>You create a default <literal>Secret</literal> if your backup and snapshot locations use the same credentials or if you do not require a snapshot location.</simpara>
<note>
<simpara>The <literal>DataProtectionApplication</literal> custom resource (CR) requires a default <literal>Secret</literal>.  Otherwise, the installation will fail. If the name of the backup location <literal>Secret</literal> is not specified, the default name is used.</simpara>
<simpara>If you do not want to use the backup location credentials during the installation, you can create a <literal>Secret</literal> with the default name by using an empty <literal>credentials-velero</literal> file.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your object storage and cloud storage, if any, must use the same credentials.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must create a <literal>credentials-velero</literal> file for the object storage in the appropriate format.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Secret</literal> with the default name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create secret generic cloud-credentials -n openshift-adp --from-file cloud=credentials-velero</programlisting>
</listitem>
</itemizedlist>
<simpara>The <literal>Secret</literal> is referenced in the <literal>spec.backupLocations.credential</literal> block of the <literal>DataProtectionApplication</literal> CR when you install the Data Protection Application.</simpara>
</section>
</section>
<section xml:id="configuring-dpa-ocs">
<title>Configuring the Data Protection Application</title>
<simpara>You can configure the Data Protection Application by setting Velero resource allocations or enabling self-signed CA certificates.</simpara>
<section xml:id="oadp-setting-resource-limits-and-requests_installing-oadp-ocs">
<title>Setting Velero CPU and memory resource allocations</title>
<simpara>You set the CPU and memory resource allocations for the <literal>Velero</literal> pod by editing the  <literal>DataProtectionApplication</literal> custom resource (CR) manifest.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.podConfig.ResourceAllocations</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  configuration:
    velero:
      podConfig:
        nodeSelector: &lt;node selector&gt; <co xml:id="CO32-1"/>
        resourceAllocations: <co xml:id="CO32-2"/>
          limits:
            cpu: "1"
            memory: 1024Mi
          requests:
            cpu: 200m
            memory: 256Mi</programlisting>
<calloutlist>
<callout arearefs="CO32-1">
<para>Specify the node selector to be supplied to Velero podSpec.</para>
</callout>
<callout arearefs="CO32-2">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<note>
<simpara>Kopia is an option in OADP 1.3 and later releases. You can use Kopia for file system backups, and Kopia is your only option for Data Mover cases with the built-in Data Mover.</simpara>
<simpara>Kopia is more resource intensive than Restic, and you might need to adjust the CPU and memory requirements accordingly.</simpara>
</note>
<section xml:id="oadp-odf-cpu-memory-requirements_installing-oadp-ocs">
<title>Adjusting Ceph CPU and memory requirements based on collected data</title>
<simpara>The following recommendations are based on observations of performance made in the scale and performance lab. The changes are specifically related to Red Hat OpenShift Data Foundation (ODF). If working with ODF, consult the appropriate tuning guides for official recommendations.</simpara>
<section xml:id="oadp-odf-config-cpu-memory-requirements_installing-oadp-ocs">
<title>CPU and memory requirement for configurations</title>
<simpara>Backup and restore operations require large amounts of CephFS <literal>PersistentVolumes</literal> (PVs). To avoid Ceph MDS pods restarting with an <literal>out-of-memory</literal> (OOM) error, the following configuration is suggested:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Configuration types</entry>
<entry align="left" valign="top">Request</entry>
<entry align="left" valign="top">Max limit</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>CPU</simpara></entry>
<entry align="left" valign="top"><simpara>Request changed to 3</simpara></entry>
<entry align="left" valign="top"><simpara>Max limit to 3</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Memory</simpara></entry>
<entry align="left" valign="top"><simpara>Request changed to 8 Gi</simpara></entry>
<entry align="left" valign="top"><simpara>Max limit to 128 Gi</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
</section>
<section xml:id="oadp-self-signed-certificate_installing-oadp-ocs">
<title>Enabling self-signed CA certificates</title>
<simpara>You must enable a self-signed CA certificate for object storage by editing the <literal>DataProtectionApplication</literal> custom resource (CR) manifest to prevent a <literal>certificate signed by unknown authority</literal> error.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must have the OpenShift API for Data Protection (OADP) Operator installed.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>spec.backupLocations.velero.objectStorage.caCert</literal> parameter and <literal>spec.backupLocations.velero.config</literal> parameters of the <literal>DataProtectionApplication</literal> CR manifest:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: &lt;dpa_sample&gt;
spec:
...
  backupLocations:
    - name: default
      velero:
        provider: aws
        default: true
        objectStorage:
          bucket: &lt;bucket&gt;
          prefix: &lt;prefix&gt;
          caCert: &lt;base64_encoded_cert_string&gt; <co xml:id="CO33-1"/>
        config:
          insecureSkipTLSVerify: "false" <co xml:id="CO33-2"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO33-1">
<para>Specify the Base64-encoded CA certificate string.</para>
</callout>
<callout arearefs="CO33-2">
<para>The <literal>insecureSkipTLSVerify</literal> configuration can be set to either <literal>"true"</literal> or <literal>"false"</literal>. If set to <literal>"true"</literal>, SSL/TLS security is disabled. If set to <literal>"false"</literal>, SSL/TLS security is enabled.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<section xml:id="oadp-using-ca-certificates-with-velero-command-aliased-for-velero-deployment_installing-oadp-ocs">
<title>Using CA certificates with the velero command aliased for Velero deployment</title>
<simpara>You might want to use the Velero CLI without installing it locally on your system by creating an alias for it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>To use an aliased Velero command, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
<listitem>
<simpara>Check that the alias is working by running the following command:</simpara>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ velero version
Client:
	Version: v1.12.1-OADP
	Git commit: -
Server:
	Version: v1.12.1-OADP</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>To use a CA certificate with this command, you can add a certificate to the Velero deployment by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ CA_CERT=$(oc -n openshift-adp get dataprotectionapplications.oadp.openshift.io &lt;dpa-name&gt; -o jsonpath='{.spec.backupLocations[0].velero.objectStorage.caCert}')

$ [[ -n $CA_CERT ]] &amp;&amp; echo "$CA_CERT" | base64 -d | oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "cat &gt; /tmp/your-cacert.txt" || echo "DPA BSL has no caCert"</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ velero -n openshift-adp describe backup &lt;backup-name&gt; --details --cacert /tmp/your-cacert.txt</programlisting>
</listitem>
<listitem>
<simpara>If the Velero pod restarts, the <literal>/tmp/your-cacert.txt</literal> file disappears, and you must re-create the <literal>/tmp/your-cacert.txt</literal> file by re-running the commands from the previous step.</simpara>
</listitem>
<listitem>
<simpara>You can check if the <literal>/tmp/your-cacert.txt</literal> file still exists, in the file location where you stored it, by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc exec -n openshift-adp -i deploy/velero -c velero -- bash -c "ls /tmp/your-cacert.txt"
/tmp/your-cacert.txt</programlisting>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
<simpara>In a future release of OpenShift API for Data Protection (OADP), we plan to mount the certificate to the Velero pod so that this step is not required.</simpara>
</section>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-2-and-earlier_installing-oadp-ocs">
<title>Installing the Data Protection Application 1.2 and earlier</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
<note>
<simpara>Velero creates a secret named <literal>velero-repo-credentials</literal> in the OADP namespace, which contains a default backup repository password.
You can update the secret with your own password encoded as base64 <emphasis role="strong">before</emphasis> you run your first backup targeted to the backup repository. The value of the key to update is <literal>Data[repository-password]</literal>.</simpara>
<simpara>After you create your DPA, the first time that you run a backup targeted to the backup repository, Velero creates a backup repository whose secret is <literal>velero-repo-credentials</literal>, which contains either the default password or the one you replaced it with.
If you update the secret password <emphasis role="strong">after</emphasis> the first backup, the new password will not match the password in <literal>velero-repo-credentials</literal>, and therefore, Velero will not be able to connect with the older backups.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-2_installing-oadp-ocs">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/restic-9cq4q                                         1/1     Running   0          94s
pod/restic-m4lts                                         1/1     Running   0          94s
pod/restic-pv4kr                                         1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s

NAME                    DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/restic   3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-installing-dpa-1-3_installing-oadp-ocs">
<title>Installing the Data Protection Application 1.3</title>
<simpara>You install the Data Protection Application (DPA) by creating an instance of the <literal>DataProtectionApplication</literal> API.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You must configure object storage as a backup location.</simpara>
</listitem>
<listitem>
<simpara>If you use snapshots to back up PVs, your cloud provider must support either a native snapshot API or Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>If the backup and snapshot locations use the same credentials, you must create a <literal>Secret</literal> with the default name, <literal>cloud-credentials</literal>.</simpara>
<note>
<simpara>If you do not want to specify backup or snapshot locations during the installation, you can create a default <literal>Secret</literal> with an empty <literal>credentials-velero</literal> file. If there is no default <literal>Secret</literal>, the installation will fail.</simpara>
</note>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Click <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">Installed Operators</emphasis> and select the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>Under <emphasis role="strong">Provided APIs</emphasis>, click <emphasis role="strong">Create instance</emphasis> in the <emphasis role="strong">DataProtectionApplication</emphasis> box.</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">YAML View</emphasis> and update the parameters of the <literal>DataProtectionApplication</literal> manifest:</simpara>
</listitem>
<listitem>
<simpara>Click <emphasis role="strong">Create</emphasis>.</simpara>
</listitem>
</orderedlist>
<section xml:id="verifying-oadp-installation-1-3_installing-oadp-ocs">
<title>Verifying the installation</title>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify the installation by viewing the OpenShift API for Data Protection (OADP) resources by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME                                                     READY   STATUS    RESTARTS   AGE
pod/oadp-operator-controller-manager-67d9494d47-6l8z8    2/2     Running   0          2m8s
pod/node-agent-9cq4q                                     1/1     Running   0          94s
pod/node-agent-m4lts                                     1/1     Running   0          94s
pod/node-agent-pv4kr                                     1/1     Running   0          95s
pod/velero-588db7f655-n842v                              1/1     Running   0          95s

NAME                                                       TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
service/oadp-operator-controller-manager-metrics-service   ClusterIP   172.30.70.140    &lt;none&gt;        8443/TCP   2m8s
service/openshift-adp-velero-metrics-svc                   ClusterIP   172.30.10.0      &lt;none&gt;        8085/TCP   8h

NAME                        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/node-agent    3         3         3       3            3           &lt;none&gt;          96s

NAME                                                READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/oadp-operator-controller-manager    1/1     1            1           2m9s
deployment.apps/velero                              1/1     1            1           96s

NAME                                                           DESIRED   CURRENT   READY   AGE
replicaset.apps/oadp-operator-controller-manager-67d9494d47    1         1         1       2m9s
replicaset.apps/velero-588db7f655                              1         1         1       96s</screen>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the <literal>DataProtectionApplication</literal> (DPA) is reconciled by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa dpa-sample -n openshift-adp -o jsonpath='{.status}'</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">{"conditions":[{"lastTransitionTime":"2023-10-27T01:23:57Z","message":"Reconcile complete","reason":"Complete","status":"True","type":"Reconciled"}]}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the <literal>type</literal> is set to <literal>Reconciled</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify the backup storage location and confirm that the <literal>PHASE</literal> is <literal>Available</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocation -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">NAME           PHASE       LAST VALIDATED   AGE     DEFAULT
dpa-sample-1   Available   1s               3d16h   true</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-creating-object-bucket-claim_installing-oadp-ocs">
<title>Creating an Object Bucket Claim for disaster recovery on OpenShift Data Foundation</title>
<simpara>If you use cluster storage for your Multicloud Object Gateway (MCG) bucket <literal>backupStorageLocation</literal> on OpenShift Data Foundation, create an Object Bucket Claim (OBC) using the OpenShift Web Console.</simpara>
<warning>
<simpara>Failure to configure an Object Bucket Claim (OBC) might lead to backups not being available.</simpara>
</warning>
<note>
<simpara>Unless specified otherwise, "NooBaa" refers to the open source project that provides lightweight object storage, while "Multicloud Object Gateway (MCG)" refers to the Red Hat distribution of NooBaa.</simpara>
<simpara>For more information on the MCG, see <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.13/html-single/managing_hybrid_and_multicloud_resources/index#accessing-the-multicloud-object-gateway-with-your-applications_rhodf">Accessing the Multicloud Object Gateway with your applications</link>.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create an Object Bucket Claim (OBC) using the OpenShift web console as described in <link xlink:href="https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.13/html/managing_hybrid_and_multicloud_resources/object-bucket-claim#creating-an-object-bucket-claim-using-the-openshift-web-console_rhodf">Creating an Object Bucket Claim using the OpenShift Web Console</link>.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-enabling-csi-dpa_installing-oadp-ocs">
<title>Enabling CSI in the DataProtectionApplication CR</title>
<simpara>You enable the Container Storage Interface (CSI) in the <literal>DataProtectionApplication</literal> custom resource (CR) in order to back up persistent volumes with CSI snapshots.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>DataProtectionApplication</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      defaultPlugins:
      - openshift
      - csi <co xml:id="CO34-1"/></programlisting>
<calloutlist>
<callout arearefs="CO34-1">
<para>Add the <literal>csi</literal> default plugin.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
</section>
<section xml:id="_uninstalling_oadp">
<title>Uninstalling OADP</title>
<section xml:id="uninstalling-oadp">
<title>Uninstalling the OpenShift API for Data Protection</title>

<simpara>You uninstall the OpenShift API for Data Protection (OADP) by deleting the OADP Operator. See <link xlink:href="../../../operators/admin/olm-deleting-operators-from-cluster.xml#olm-deleting-operators-from-cluster">Deleting Operators from a cluster</link> for details.</simpara>
</section>
</section>
<section xml:id="_oadp_backing_up">
<title>OADP backing up</title>
<section xml:id="backing-up-applications">
<title>Backing up applications</title>

<simpara>You back up applications by creating a <literal>Backup</literal> custom resource (CR). See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-creating-backup-cr.xml#oadp-creating-backup-cr-doc">Creating a Backup CR</link>.</simpara>
<itemizedlist>
<listitem>
<simpara>The <literal>Backup</literal> CR creates backup files for Kubernetes resources and internal images on S3 object storage.</simpara>
</listitem>
<listitem>
<simpara>If your cloud provider has a native snapshot API or supports CSI snapshots, the <literal>Backup</literal> CR backs up persistent volumes (PVs) by creating snapshots. For more information about working with CSI snapshots, see <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-backing-up-pvs-csi-doc.xml#oadp-backing-up-pvs-csi-doc">Backing up persistent volumes with CSI snapshots</link>.</simpara>
</listitem>
</itemizedlist>
<simpara>For more information about CSI volume snapshots, see <link xlink:href="../../../storage/container_storage_interface/persistent-storage-csi-snapshots.xml#persistent-storage-csi-snapshots">CSI volume snapshots</link>.</simpara>
<important>
<simpara>The <literal>CloudStorage</literal> API, which automates the creation of a bucket for object storage, is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<note>
<simpara>The <literal>CloudStorage</literal> API is a Technology Preview feature when you use a <literal>CloudStorage</literal> object and want OADP to use the <literal>CloudStorage</literal> API to automatically create an S3 bucket for use as a <literal>BackupStorageLocation</literal>.</simpara>
<simpara>The <literal>CloudStorage</literal> API supports manually creating a <literal>BackupStorageLocation</literal> object by specifying an existing S3 bucket. The <literal>CloudStorage</literal> API that creates an S3 bucket automatically is currently only enabled for AWS S3 storage.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>If your cloud provider does not support snapshots or if your applications are on NFS data volumes, you can create backups by using Kopia or Restic. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-backing-up-applications-restic-doc.xml#oadp-backing-up-applications-restic-doc">Backing up applications with File System Backup: Kopia or Restic</link>.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The OpenShift API for Data Protection (OADP) does not support backing up volume snapshots that were created by other software.</simpara>
</important>
<simpara>You can create backup hooks to run commands before or after the backup operation. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-creating-backup-hooks-doc.xml#oadp-creating-backup-hooks-doc">Creating backup hooks</link>.</simpara>
<simpara>You can schedule backups by creating a <literal>Schedule</literal> CR instead of a <literal>Backup</literal> CR. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-scheduling-backups-doc.xml#oadp-scheduling-backups-doc">Scheduling backups using Schedule CR</link>].</simpara>
<section xml:id="known-issues-backing-up-applications">
<title>Known issues</title>
<simpara>OpenShift Container Platform 4.14 enforces a pod security admission (PSA) policy that can hinder the readiness of pods during a Restic restore process. </simpara>
<simpara>This issue has been resolved in the OADP 1.1.6 and OADP 1.2.2 releases, therefore it is recommended that users upgrade to these releases.</simpara>
<simpara>For more information, see <link xlink:href="../../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#oadp-restic-restore-failing-psa-policy_oadp-troubleshooting">Restic restore partially failing on OCP 4.15 due to changed PSA policy</link>.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../../operators/admin/olm-adding-operators-to-cluster.xml#olm-installing-operators-from-operatorhub_olm-adding-operators-to-a-cluster">Installing Operators on clusters for administrators</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../operators/user/olm-installing-operators-in-namespace.xml#olm-installing-operators-in-namespace">Installing Operators in namespaces for non-administrators</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-creating-backup-cr-doc">
<title>Creating a Backup CR</title>

<simpara>You back up Kubernetes images, internal images, and persistent volumes (PVs) by creating a <literal>Backup</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
<listitem>
<simpara>Backup location prerequisites:</simpara>
<itemizedlist>
<listitem>
<simpara>You must have S3 object storage configured for Velero.</simpara>
</listitem>
<listitem>
<simpara>You must have a backup location configured in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Snapshot location prerequisites:</simpara>
<itemizedlist>
<listitem>
<simpara>Your cloud provider must have a native snapshot API or support Container Storage Interface (CSI) snapshots.</simpara>
</listitem>
<listitem>
<simpara>For CSI snapshots, you must create a <literal>VolumeSnapshotClass</literal> CR to register the CSI driver.</simpara>
</listitem>
<listitem>
<simpara>You must have a volume location configured in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the <literal>backupStorageLocations</literal> CRs by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocations -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE       NAME              PHASE       LAST VALIDATED   AGE   DEFAULT
openshift-adp   velero-sample-1   Available   11s              31m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>Backup</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup&gt;
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  hooks: {}
  includedNamespaces:
  - &lt;namespace&gt; <co xml:id="CO35-1"/>
  includedResources: [] <co xml:id="CO35-2"/>
  excludedResources: [] <co xml:id="CO35-3"/>
  storageLocation: &lt;velero-sample-1&gt; <co xml:id="CO35-4"/>
  ttl: 720h0m0s
  labelSelector: <co xml:id="CO35-5"/>
    matchLabels:
      app=&lt;label_1&gt;
      app=&lt;label_2&gt;
      app=&lt;label_3&gt;
  orLabelSelectors: <co xml:id="CO35-6"/>
  - matchLabels:
      app=&lt;label_1&gt;
      app=&lt;label_2&gt;
      app=&lt;label_3&gt;</programlisting>
<calloutlist>
<callout arearefs="CO35-1">
<para>Specify an array of namespaces to back up.</para>
</callout>
<callout arearefs="CO35-2">
<para>Optional: Specify an array of resources to include in the backup. Resources might be shortcuts (for example, 'po' for 'pods') or fully-qualified. If unspecified, all resources are included.</para>
</callout>
<callout arearefs="CO35-3">
<para>Optional: Specify an array of resources to exclude from the backup. Resources might be shortcuts (for example, 'po' for 'pods') or fully-qualified.</para>
</callout>
<callout arearefs="CO35-4">
<para>Specify the name of the <literal>backupStorageLocations</literal> CR.</para>
</callout>
<callout arearefs="CO35-5">
<para>Map of {key,value} pairs of backup resources that have <emphasis role="strong">all</emphasis> the specified labels.</para>
</callout>
<callout arearefs="CO35-6">
<para>Map of {key,value} pairs of backup resources that have <emphasis role="strong">one or more</emphasis> of the specified labels.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the status of the <literal>Backup</literal> CR is <literal>Completed</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backup -n openshift-adp &lt;backup&gt; -o jsonpath='{.status.phase}'</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-backing-up-pvs-csi-doc">
<title>Backing up persistent volumes with CSI snapshots</title>

<simpara>You back up persistent volumes with Container Storage Interface (CSI) snapshots by editing the <literal>VolumeSnapshotClass</literal> custom resource (CR) of the cloud storage before you create the <literal>Backup</literal> CR, see <link xlink:href="../../../storage/container_storage_interface/persistent-storage-csi-snapshots.xml#persistent-storage-csi-snapshots-overview_persistent-storage-csi-snapshots">CSI volume snapshots</link>.</simpara>
<simpara>For more information, see <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-creating-backup-cr.xml#oadp-creating-backup-cr-doc">Creating a Backup CR</link>.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>The cloud provider must support CSI snapshots.</simpara>
</listitem>
<listitem>
<simpara>You must enable CSI in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add the <literal>metadata.labels.velero.io/csi-volumesnapshot-class: "true"</literal> key-value pair to the <literal>VolumeSnapshotClass</literal> CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.storage.k8s.io/v1
kind: VolumeSnapshotClass
metadata:
  name: &lt;volume_snapshot_class_name&gt;
  labels:
    velero.io/csi-volumesnapshot-class: "true"
driver: &lt;csi_driver&gt;
deletionPolicy: Retain</programlisting>
</listitem>
</itemizedlist>
<simpara>You can now create a <literal>Backup</literal> CR.</simpara>
</section>
<section xml:id="oadp-backing-up-applications-restic-doc">
<title>Backing up applications with File System Backup: Kopia or Restic</title>

<simpara>You can use OADP to back up and restore Kubernetes volumes attached to pods from the file system of the volumes. This process is called File System Backup (FSB) or Pod Volume Backup (PVB). It is accomplished by using modules from the open source backup tools Restic or Kopia.</simpara>
<simpara>If your cloud provider does not support snapshots or if your applications are on NFS data volumes, you can create backups by using FSB.</simpara>
<note>
<simpara><link xlink:href="https://restic.net/">Restic</link> is installed by the OADP Operator by default. If you prefer, you can install <link xlink:href="https://kopia.io/">Kopia</link> instead.</simpara>
</note>
<simpara>FSB integration with OADP provides a solution for backing up and restoring almost any type of Kubernetes volumes. This integration is an additional capability of OADP and is not a replacement for existing functionality.</simpara>
<simpara>You back up Kubernetes resources, internal images, and persistent volumes with Kopia or Restic by editing the <literal>Backup</literal> custom resource (CR).</simpara>
<simpara>You do not need to specify a snapshot location in the <literal>DataProtectionApplication</literal> CR.</simpara>
<note>
<simpara>In OADP version 1.3 and later, you can use either Kopia or Restic for backing up applications.</simpara>
<simpara>For the Built-in DataMover, you must use Kopia.</simpara>
<simpara>In OADP version 1.2 and earlier, you can only use Restic for backing up applications.</simpara>
</note>
<important>
<simpara>FSB does not support backing up <literal>hostPath</literal> volumes. For more information, see <link xlink:href="https://velero.io/docs/v1.12/file-system-backup/#limitations">FSB limitations</link>.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>You must not disable the default <literal>nodeAgent</literal> installation by setting <literal>spec.configuration.nodeAgent.enable</literal> to <literal>false</literal> in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>You must select Kopia or Restic as the uploader by setting <literal>spec.configuration.nodeAgent.uploaderType</literal> to <literal>kopia</literal> or <literal>restic</literal> in the <literal>DataProtectionApplication</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create the <literal>Backup</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup&gt;
  labels:
    velero.io/storage-location: default
  namespace: openshift-adp
spec:
  defaultVolumesToFsBackup: true <co xml:id="CO36-1"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO36-1">
<para>In OADP version 1.2 and later, add the <literal>defaultVolumesToFsBackup: true</literal> setting within the <literal>spec</literal> block. In OADP  version 1.1, add <literal>defaultVolumesToRestic: true</literal>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-creating-backup-hooks-doc">
<title>Creating backup hooks</title>

<simpara>When performing a backup, it is possible to specify one or more commands to execute in a container within a pod, based on the pod being backed up.</simpara>
<simpara>The commands can be configured to performed before any custom action processing (<emphasis>Pre</emphasis> hooks), or after all custom actions have been completed and any additional items specified by the custom action have been backed up (<emphasis>Post</emphasis> hooks).</simpara>
<simpara>You create backup hooks to run commands in a container in a pod by editing the <literal>Backup</literal> custom resource (CR).</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add a hook to the <literal>spec.hooks</literal> block of the <literal>Backup</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup&gt;
  namespace: openshift-adp
spec:
  hooks:
    resources:
      - name: &lt;hook_name&gt;
        includedNamespaces:
        - &lt;namespace&gt; <co xml:id="CO37-1"/>
        excludedNamespaces: <co xml:id="CO37-2"/>
        - &lt;namespace&gt;
        includedResources: []
        - pods <co xml:id="CO37-3"/>
        excludedResources: [] <co xml:id="CO37-4"/>
        labelSelector: <co xml:id="CO37-5"/>
          matchLabels:
            app: velero
            component: server
        pre: <co xml:id="CO37-6"/>
          - exec:
              container: &lt;container&gt; <co xml:id="CO37-7"/>
              command:
              - /bin/uname <co xml:id="CO37-8"/>
              - -a
              onError: Fail <co xml:id="CO37-9"/>
              timeout: 30s <co xml:id="CO37-10"/>
        post: <co xml:id="CO37-11"/>
...</programlisting>
<calloutlist>
<callout arearefs="CO37-1">
<para>Optional: You can specify namespaces to which the hook applies. If this value is not specified, the hook applies to all namespaces.</para>
</callout>
<callout arearefs="CO37-2">
<para>Optional: You can specify namespaces to which the hook does not apply.</para>
</callout>
<callout arearefs="CO37-3">
<para>Currently, pods are the only supported resource that hooks can apply to.</para>
</callout>
<callout arearefs="CO37-4">
<para>Optional: You can specify resources to which the hook does not apply.</para>
</callout>
<callout arearefs="CO37-5">
<para>Optional: This hook only applies to objects matching the label. If this value is not specified, the hook applies to all objects.</para>
</callout>
<callout arearefs="CO37-6">
<para>Array of hooks to run before the backup.</para>
</callout>
<callout arearefs="CO37-7">
<para>Optional: If the container is not specified, the command runs in the first container in the pod.</para>
</callout>
<callout arearefs="CO37-8">
<para>This is the entry point for the <literal>init</literal> container being added.</para>
</callout>
<callout arearefs="CO37-9">
<para>Allowed values for error handling are <literal>Fail</literal> and <literal>Continue</literal>. The default is <literal>Fail</literal>.</para>
</callout>
<callout arearefs="CO37-10">
<para>Optional: How long to wait for the commands to run. The default is <literal>30s</literal>.</para>
</callout>
<callout arearefs="CO37-11">
<para>This block defines an array of hooks to run after the backup, with the same parameters as the pre-backup hooks.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-scheduling-backups-doc">
<title>Scheduling backups using Schedule CR</title>

<simpara>The schedule operation allows you to create a backup of your data at a particular time, specified by a Cron expression.</simpara>
<simpara>You schedule backups by creating a <literal>Schedule</literal> custom resource (CR) instead of a <literal>Backup</literal> CR.</simpara>
<warning>
<simpara>Leave enough time in your backup schedule for a backup to finish before another backup is created.</simpara>
<simpara>For example, if a backup of a namespace typically takes 10 minutes, do not schedule backups more frequently than every 15 minutes.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Retrieve the <literal>backupStorageLocations</literal> CRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupStorageLocations -n openshift-adp</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE       NAME              PHASE       LAST VALIDATED   AGE   DEFAULT
openshift-adp   velero-sample-1   Available   11s              31m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>Schedule</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc apply -f -
apiVersion: velero.io/v1
kind: Schedule
metadata:
  name: &lt;schedule&gt;
  namespace: openshift-adp
spec:
  schedule: 0 7 * * * <co xml:id="CO38-1"/>
  template:
    hooks: {}
    includedNamespaces:
    - &lt;namespace&gt; <co xml:id="CO38-2"/>
    storageLocation: &lt;velero-sample-1&gt; <co xml:id="CO38-3"/>
    defaultVolumesToFsBackup: true <co xml:id="CO38-4"/>
    ttl: 720h0m0s
EOF</programlisting>
</listitem>
</orderedlist>
<calloutlist>
<callout arearefs="CO38-1">
<para><literal>cron</literal> expression to schedule the backup, for example, <literal>0 7 * * *</literal> to perform a backup every day at 7:00.</para>
<note>
<simpara>To schedule a backup at specific intervals, enter the <literal>&lt;duration_in_minutes&gt;</literal> in the following format:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">  schedule: "*/10 * * * *"</programlisting>
<simpara>Enter the minutes value between quotation marks (<literal>" "</literal>).</simpara>
</note>
</callout>
<callout arearefs="CO38-2">
<para>Array of namespaces to back up.</para>
</callout>
<callout arearefs="CO38-3">
<para>Name of the <literal>backupStorageLocations</literal> CR.</para>
</callout>
<callout arearefs="CO38-4">
<para>Optional: In OADP version 1.2 and later, add the <literal>defaultVolumesToFsBackup: true</literal> key-value pair to your configuration when performing backups of volumes with Restic. In OADP version 1.1, add the <literal>defaultVolumesToRestic: true</literal> key-value pair when you back up volumes with Restic.</para>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that the status of the <literal>Schedule</literal> CR is <literal>Completed</literal> after the scheduled backup runs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get schedule -n openshift-adp &lt;schedule&gt; -o jsonpath='{.status.phase}'</programlisting>
</listitem>
</orderedlist>
</callout>
</calloutlist>
</section>
<section xml:id="oadp-deleting-backups-doc">
<title>Deleting backups</title>

<simpara>You can remove backup files by deleting the <literal>Backup</literal> custom resource (CR).</simpara>
<warning>
<simpara>After you delete the <literal>Backup</literal> CR and the associated object storage data, you cannot recover the deleted data.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You created a <literal>Backup</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>You know the name of the <literal>Backup</literal> CR and the namespace that contains it.</simpara>
</listitem>
<listitem>
<simpara>You downloaded the Velero CLI tool.</simpara>
</listitem>
<listitem>
<simpara>You can access the Velero binary in your cluster.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Choose one of the following actions to delete the <literal>Backup</literal> CR:</simpara>
<itemizedlist>
<listitem>
<simpara>To delete the <literal>Backup</literal> CR and keep the associated object storage data, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete backup &lt;backup_CR_name&gt; -n &lt;velero_namespace&gt;</programlisting>
</listitem>
<listitem>
<simpara>To delete the <literal>Backup</literal> CR and delete the associated object storage data, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ velero backup delete &lt;backup_CR_name&gt; -n &lt;velero_namespace&gt;</programlisting>
<simpara>Where:</simpara>
<variablelist>
<varlistentry>
<term>&lt;backup_CR_name&gt;</term>
<listitem>
<simpara>The name of the <literal>Backup</literal> custom resource.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>&lt;velero_namespace&gt;</term>
<listitem>
<simpara>The namespace that contains the <literal>Backup</literal> custom resource.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-about-kopia">
<title>About Kopia</title>

<simpara>Kopia is a fast and secure open-source backup and restore tool that allows you to create encrypted snapshots of your data and save the snapshots to remote or cloud storage of your choice.</simpara>
<simpara>Kopia supports network and local storage locations, and many cloud or remote storage locations, including:</simpara>
<itemizedlist>
<listitem>
<simpara>Amazon S3 and any cloud storage that is compatible with S3</simpara>
</listitem>
<listitem>
<simpara>Azure Blob Storage</simpara>
</listitem>
<listitem>
<simpara>Google Cloud Storage platform</simpara>
</listitem>
</itemizedlist>
<simpara>Kopia uses content-addressable storage for snapshots:</simpara>
<itemizedlist>
<listitem>
<simpara>Snapshots are always incremental; data that is already included in previous snapshots is not re-uploaded to the repository. A file is only uploaded to the repository again if it is modified.</simpara>
</listitem>
<listitem>
<simpara>Stored data is deduplicated; if multiple copies of the same file exist, only one of them is stored.</simpara>
</listitem>
<listitem>
<simpara>If files are moved or renamed, Kopia can recognize that they have the same content and does not upload them again.</simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-kopia-integration">
<title>OADP integration with Kopia</title>
<simpara>OADP 1.3 supports Kopia as the backup mechanism for pod volume backup in addition to Restic. You must choose one or the other at installation by setting the <literal>uploaderType</literal> field in the <literal>DataProtectionApplication</literal> custom resource (CR). The possible values are <literal>restic</literal> or <literal>kopia</literal>. If you do not specify an  <literal>uploaderType</literal>, OADP 1.3 defaults to using Kopia as the backup mechanism. The data is written to and read from a unified repository.</simpara>
<simpara>The following example shows a <literal>DataProtectionApplication</literal> CR configured for using Kopia:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: dpa-sample
spec:
  configuration:
    nodeAgent:
      enable: true
      uploaderType: kopia
# ...</programlisting>
</section>
</section>
</section>
<section xml:id="_oadp_restoring">
<title>OADP restoring</title>
<section xml:id="restoring-applications">
<title>Restoring applications</title>

<simpara>You restore application backups by creating a <literal>Restore</literal> custom resource (CR). See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/restoring-applications.xml#oadp-creating-restore-cr_restoring-applications">Creating a Restore CR</link>.</simpara>
<simpara>You can create restore hooks to run commands in a container in a pod by editing the <literal>Restore</literal> CR. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/restoring-applications.xml#oadp-creating-restore-hooks_restoring-applications">Creating restore hooks</link>.</simpara>
<section xml:id="oadp-creating-restore-cr_restoring-applications">
<title>Creating a Restore CR</title>
<simpara>You restore a <literal>Backup</literal> custom resource (CR) by creating a <literal>Restore</literal> CR.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must install the OpenShift API for Data Protection (OADP) Operator.</simpara>
</listitem>
<listitem>
<simpara>The <literal>DataProtectionApplication</literal> CR must be in a <literal>Ready</literal> state.</simpara>
</listitem>
<listitem>
<simpara>You must have a Velero <literal>Backup</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>The persistent volume (PV) capacity must match the requested size at backup time. Adjust the requested size if needed.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Restore</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore&gt;
  namespace: openshift-adp
spec:
  backupName: &lt;backup&gt; <co xml:id="CO39-1"/>
  includedResources: [] <co xml:id="CO39-2"/>
  excludedResources:
  - nodes
  - events
  - events.events.k8s.io
  - backups.velero.io
  - restores.velero.io
  - resticrepositories.velero.io
  restorePVs: true <co xml:id="CO39-3"/></programlisting>
<calloutlist>
<callout arearefs="CO39-1">
<para>Name of the <literal>Backup</literal> CR.</para>
</callout>
<callout arearefs="CO39-2">
<para>Optional: Specify an array of resources to include in the restore process. Resources might be shortcuts (for example, <literal>po</literal> for <literal>pods</literal>) or fully-qualified. If unspecified, all resources are included.</para>
</callout>
<callout arearefs="CO39-3">
<para>Optional: The <literal>restorePVs</literal> parameter can be set to <literal>false</literal> to turn off restore of <literal>PersistentVolumes</literal> from <literal>VolumeSnapshot</literal> of Container Storage Interface (CSI) snapshots or from native snapshots when <literal>VolumeSnapshotLocation</literal> is configured.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the status of the <literal>Restore</literal> CR is <literal>Completed</literal> by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get restore -n openshift-adp &lt;restore&gt; -o jsonpath='{.status.phase}'</programlisting>
</listitem>
<listitem>
<simpara>Verify that the backup resources have been restored by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get all -n &lt;namespace&gt; <co xml:id="CO40-1"/></programlisting>
<calloutlist>
<callout arearefs="CO40-1">
<para>Namespace that you backed up.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If you use Restic to restore <literal>DeploymentConfig</literal> objects or if you use post-restore hooks, run the <literal>dc-restic-post-restore.sh</literal> cleanup script by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ bash dc-restic-post-restore.sh &lt;restore-name&gt;</programlisting>
<note>
<simpara>During the restore process, the OADP Velero plug-ins scale down the <literal>DeploymentConfig</literal> objects and restore the pods as standalone pods. This is done to prevent the cluster from deleting the restored <literal>DeploymentConfig</literal> pods immediately on restore and to allow Restic and post-restore hooks to complete their actions on the restored pods. The cleanup script shown below removes these disconnected pods and scales any <literal>DeploymentConfig</literal> objects back up to the appropriate number of replicas.</simpara>
</note>
<example>
<title><literal>dc-restic-post-restore.sh</literal> cleanup script</title>
<programlisting language="bash" linenumbering="unnumbered">#!/bin/bash
set -e

# if sha256sum exists, use it to check the integrity of the file
if command -v sha256sum &gt;/dev/null 2&gt;&amp;1; then
  CHECKSUM_CMD="sha256sum"
else
  CHECKSUM_CMD="shasum -a 256"
fi

label_name () {
    if [ "${#1}" -le "63" ]; then
	echo $1
	return
    fi
    sha=$(echo -n $1|$CHECKSUM_CMD)
    echo "${1:0:57}${sha:0:6}"
}

OADP_NAMESPACE=${OADP_NAMESPACE:=openshift-adp}

if [[ $# -ne 1 ]]; then
    echo "usage: ${BASH_SOURCE} restore-name"
    exit 1
fi

echo using OADP Namespace $OADP_NAMESPACE
echo restore: $1

label=$(label_name $1)
echo label: $label

echo Deleting disconnected restore pods
oc delete pods -l oadp.openshift.io/disconnected-from-dc=$label

for dc in $(oc get dc --all-namespaces -l oadp.openshift.io/replicas-modified=$label -o jsonpath='{range .items[*]}{.metadata.namespace}{","}{.metadata.name}{","}{.metadata.annotations.oadp\.openshift\.io/original-replicas}{","}{.metadata.annotations.oadp\.openshift\.io/original-paused}{"\n"}')
do
    IFS=',' read -ra dc_arr &lt;&lt;&lt; "$dc"
    if [ ${#dc_arr[0]} -gt 0 ]; then
	echo Found deployment ${dc_arr[0]}/${dc_arr[1]}, setting replicas: ${dc_arr[2]}, paused: ${dc_arr[3]}
	cat &lt;&lt;EOF | oc patch dc  -n ${dc_arr[0]} ${dc_arr[1]} --patch-file /dev/stdin
spec:
  replicas: ${dc_arr[2]}
  paused: ${dc_arr[3]}
EOF
    fi
done</programlisting>
</example>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-creating-restore-hooks_restoring-applications">
<title>Creating restore hooks</title>
<simpara>You create restore hooks to run commands in a container in a pod by editing the <literal>Restore</literal> custom resource (CR).</simpara>
<simpara>You can create two types of restore hooks:</simpara>
<itemizedlist>
<listitem>
<simpara>An <literal>init</literal> hook adds an init container to a pod to perform setup tasks before the application container starts.</simpara>
<simpara>If you restore a Restic backup, the <literal>restic-wait</literal> init container is added before the restore hook init container.</simpara>
</listitem>
<listitem>
<simpara>An <literal>exec</literal> hook runs commands or scripts in a container of a restored pod.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Add a hook to the <literal>spec.hooks</literal> block of the <literal>Restore</literal> CR, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore&gt;
  namespace: openshift-adp
spec:
  hooks:
    resources:
      - name: &lt;hook_name&gt;
        includedNamespaces:
        - &lt;namespace&gt; <co xml:id="CO41-1"/>
        excludedNamespaces:
        - &lt;namespace&gt;
        includedResources:
        - pods <co xml:id="CO41-2"/>
        excludedResources: []
        labelSelector: <co xml:id="CO41-3"/>
          matchLabels:
            app: velero
            component: server
        postHooks:
        - init:
            initContainers:
            - name: restore-hook-init
              image: alpine:latest
              volumeMounts:
              - mountPath: /restores/pvc1-vm
                name: pvc1-vm
              command:
              - /bin/ash
              - -c
            timeout: <co xml:id="CO41-4"/>
        - exec:
            container: &lt;container&gt; <co xml:id="CO41-5"/>
            command:
            - /bin/bash <co xml:id="CO41-6"/>
            - -c
            - "psql &lt; /backup/backup.sql"
            waitTimeout: 5m <co xml:id="CO41-7"/>
            execTimeout: 1m <co xml:id="CO41-8"/>
            onError: Continue <co xml:id="CO41-9"/></programlisting>
<calloutlist>
<callout arearefs="CO41-1">
<para>Optional: Array of namespaces to which the hook applies. If this value is not specified, the hook applies to all namespaces.</para>
</callout>
<callout arearefs="CO41-2">
<para>Currently, pods are the only supported resource that hooks can apply to.</para>
</callout>
<callout arearefs="CO41-3">
<para>Optional: This hook only applies to objects matching the label selector.</para>
</callout>
<callout arearefs="CO41-4">
<para>Optional: Timeout specifies the maximum length of time Velero waits for <literal>initContainers</literal> to complete.</para>
</callout>
<callout arearefs="CO41-5">
<para>Optional: If the container is not specified, the command runs in the first container in the pod.</para>
</callout>
<callout arearefs="CO41-6">
<para>This is the entrypoint for the init container being added.</para>
</callout>
<callout arearefs="CO41-7">
<para>Optional: How long to wait for a container to become ready. This should be long enough for the container to start and for any preceding hooks in the same container to complete. If not set, the restore process waits indefinitely.</para>
</callout>
<callout arearefs="CO41-8">
<para>Optional: How long to wait for the commands to run. The default is <literal>30s</literal>.</para>
</callout>
<callout arearefs="CO41-9">
<para>Allowed values for error handling are <literal>Fail</literal> and <literal>Continue</literal>:</para>
<itemizedlist>
<listitem>
<simpara><literal>Continue</literal>: Only command failures are logged.</simpara>
</listitem>
<listitem>
<simpara><literal>Fail</literal>: No more restore hooks run in any container in any pod. The status of the <literal>Restore</literal> CR will be <literal>PartiallyFailed</literal>.</simpara>
</listitem>
</itemizedlist>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="_oadp_and_rosa">
<title>OADP and ROSA</title>
<section xml:id="oadp-rosa-backing-up-applications">
<title>Backing up applications on ROSA clusters using OADP</title>

<simpara>You can use OpenShift API for Data Protection (OADP) with Red Hat OpenShift Service on AWS (ROSA) clusters to back up and restore application data.</simpara>
<simpara>ROSA is a fully-managed, turnkey application platform that allows you to deliver value to your customers by building and deploying applications.</simpara>
<simpara>ROSA provides seamless integration with a wide range of Amazon Web Services compute, database, analytics, machine learning, networking, mobile, and other services to speed up the building and delivery of differentiating experiences to your customers.</simpara>
<simpara>You can subscribe to the service directly from your AWS account.</simpara>
<simpara>After you create your clusters, you can operate your clusters with the OpenShift Container Platform web console or through <link xlink:href="https://docs.openshift.com/dedicated/ocm/ocm-overview.html">Red Hat OpenShift Cluster Manager</link>. You can also use ROSA with OpenShift APIs and command-line interface (CLI) tools.</simpara>
<simpara>For additional information about ROSA installation, see <link xlink:href="https://www.redhat.com/en/products/interactive-walkthrough/install-rosa">Installing Red Hat Openshift Service on AWS (ROSA) interactive walkthrough</link>.</simpara>
<simpara>Before installing OpenShift API for Data Protection (OADP), you must set up role and policy credentials for OADP so that it can use the Amazon Web Services (AWS) API.</simpara>
<simpara>This process is performed in the following two stages:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Prepare AWS credentials</simpara>
</listitem>
<listitem>
<simpara>Install the OADP Operator and give it an IAM role</simpara>
</listitem>
</orderedlist>
<section xml:id="preparing-aws-credentials-for-oadp_oadp-rosa-backing-up-applications">
<title>Preparing AWS credentials for OADP</title>
<simpara>An Amazon Web Services (AWS) account must be prepared and configured to accept an OpenShift API for Data Protection (OADP) installation.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create the following environment variables by running the following commands:</simpara>
<important>
<simpara>Change the cluster name to match your ROSA cluster, and ensure you are logged into the cluster as an administrator. Ensure that all fields are outputted correctly before continuing.</simpara>
</important>
<programlisting language="terminal" linenumbering="unnumbered">$ export CLUSTER_NAME=my-cluster <co xml:id="CO42-1"/>
  export ROSA_CLUSTER_ID=$(rosa describe cluster -c ${CLUSTER_NAME} --output json | jq -r .id)
  export REGION=$(rosa describe cluster -c ${CLUSTER_NAME} --output json | jq -r .region.id)
  export OIDC_ENDPOINT=$(oc get authentication.config.openshift.io cluster -o jsonpath='{.spec.serviceAccountIssuer}' | sed 's|^https://||')
  export AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
  export CLUSTER_VERSION=$(rosa describe cluster -c ${CLUSTER_NAME} -o json | jq -r .version.raw_id | cut -f -2 -d '.')
  export ROLE_NAME="${CLUSTER_NAME}-openshift-oadp-aws-cloud-credentials"
  export SCRATCH="/tmp/${CLUSTER_NAME}/oadp"
  mkdir -p ${SCRATCH}
  echo "Cluster ID: ${ROSA_CLUSTER_ID}, Region: ${REGION}, OIDC Endpoint:
  ${OIDC_ENDPOINT}, AWS Account ID: ${AWS_ACCOUNT_ID}"</programlisting>
<calloutlist>
<callout arearefs="CO42-1">
<para>Replace <literal>my-cluster</literal> with your ROSA cluster name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>On the AWS account, create an IAM policy to allow access to AWS S3:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Check to see if the policy exists by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ POLICY_ARN=$(aws iam list-policies --query "Policies[?PolicyName=='RosaOadpVer1'].{ARN:Arn}" --output text) <co xml:id="CO43-1"/></programlisting>
<calloutlist>
<callout arearefs="CO43-1">
<para>Replace <literal>RosaOadp</literal> with your policy name.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Enter the following command to create the policy JSON file and then create the policy in ROSA:</simpara>
<note>
<simpara>If the policy ARN is not found, the command creates the policy. If the policy ARN already exists, the <literal>if</literal> statement intentionally skips the policy creation.</simpara>
</note>
<programlisting language="terminal" linenumbering="unnumbered">$ if [[ -z "${POLICY_ARN}" ]]; then
  cat &lt;&lt; EOF &gt; ${SCRATCH}/policy.json <co xml:id="CO44-1"/>
  {
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "s3:CreateBucket",
        "s3:DeleteBucket",
        "s3:PutBucketTagging",
        "s3:GetBucketTagging",
        "s3:PutEncryptionConfiguration",
        "s3:GetEncryptionConfiguration",
        "s3:PutLifecycleConfiguration",
        "s3:GetLifecycleConfiguration",
        "s3:GetBucketLocation",
        "s3:ListBucket",
        "s3:GetObject",
        "s3:PutObject",
        "s3:DeleteObject",
        "s3:ListBucketMultipartUploads",
        "s3:AbortMultipartUploads",
        "s3:ListMultipartUploadParts",
        "s3:DescribeSnapshots",
        "ec2:DescribeVolumes",
        "ec2:DescribeVolumeAttribute",
        "ec2:DescribeVolumesModifications",
        "ec2:DescribeVolumeStatus",
        "ec2:CreateTags",
        "ec2:CreateVolume",
        "ec2:CreateSnapshot",
        "ec2:DeleteSnapshot"
      ],
      "Resource": "*"
    }
   ]}
EOF

  POLICY_ARN=$(aws iam create-policy --policy-name "RosaOadpVer1" \
  --policy-document file:///${SCRATCH}/policy.json --query Policy.Arn \
  --tags Key=rosa_openshift_version,Value=${CLUSTER_VERSION} Key=rosa_role_prefix,Value=ManagedOpenShift Key=operator_namespace,Value=openshift-oadp Key=operator_name,Value=openshift-oadp \
  --output text)
  fi</programlisting>
<calloutlist>
<callout arearefs="CO44-1">
<para><literal>SCRATCH</literal> is a name for a temporary directory created for the environment variables.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>View the policy ARN by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo ${POLICY_ARN}</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create an IAM role trust policy for the cluster:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the trust policy file by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF &gt; ${SCRATCH}/trust-policy.json
  {
      "Version":2012-10-17",
      "Statement": [{
        "Effect": "Allow",
        "Principal": {
          "Federated": "arn:aws:iam::${AWS_ACCOUNT_ID}:oidc-provider/${OIDC_ENDPOINT}"
        },
        "Action": "sts:AssumeRoleWithWebIdentity",
        "Condition": {
          "StringEquals": {
            "${OIDC_ENDPOINT}:sub": [
              "system:serviceaccount:openshift-adp:openshift-adp-controller-manager",
              "system:serviceaccount:openshift-adp:velero"]
          }
        }
      }]
  }
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create the role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ROLE_ARN=$(aws iam create-role --role-name \
  "${ROLE_NAME}" \
  --assume-role-policy-document file://${SCRATCH}/trust-policy.json \
--tags Key=rosa_cluster_id,Value=${ROSA_CLUSTER_ID} Key=rosa_openshift_version,Value=${CLUSTER_VERSION} Key=rosa_role_prefix,Value=ManagedOpenShift Key=operator_namespace,Value=openshift-adp Key=operator_name,Value=openshift-oadp \
   --query Role.Arn --output text)</programlisting>
</listitem>
<listitem>
<simpara>View the role ARN by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ echo ${ROLE_ARN}</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Attach the IAM policy to the IAM role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam attach-role-policy --role-name "${ROLE_NAME}" \
  --policy-arn ${POLICY_ARN}</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="installing-oadp-rosa-sts_oadp-rosa-backing-up-applications">
<title>Installing the OADP Operator and providing the IAM role</title>
<simpara>AWS Security Token Service (AWS STS) is a global web service that provides short-term credentials for IAM or federated users. OpenShift Container Platform (ROSA) with STS is the recommended credential mode for ROSA clusters. This document describes how to install OpenShift API for Data Protection (OADP) on ROSA with AWS STS.</simpara>
<important>
<simpara>Restic and Kopia are not supported in the OADP on ROSA with AWS STS environment. Verify that the Restic and Kopia node agent is disabled.
For backing up volumes, OADP on ROSA with AWS STS supports only native snapshots and Container Storage Interface (CSI) snapshots.</simpara>
</important>
<important>
<simpara>In an Amazon ROSA cluster that uses STS authentication, restoring backed-up data in a different AWS region is not supported.</simpara>
<simpara>The Data Mover feature is not currently supported in ROSA clusters. You can use native AWS S3 tools for moving data.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>An OpenShift Container Platform ROSA cluster with the required access and tokens. For instructions, see the previous procedure <emphasis>Preparing AWS credentials for OADP</emphasis>. If you plan to use two different clusters for backing up and restoring, you must prepare AWS credentials, including <literal>ROLE_ARN</literal>, for each cluster.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create an OpenShift Container Platform secret from your AWS token file by entering the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the credentials file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF &gt; ${SCRATCH}/credentials
  [default]
  role_arn = ${ROLE_ARN}
  web_identity_token_file = /var/run/secrets/openshift/serviceaccount/token
EOF</programlisting>
</listitem>
<listitem>
<simpara>Create a namespace for OADP:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace openshift-adp</programlisting>
</listitem>
<listitem>
<simpara>Create the OpenShift Container Platform secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp create secret generic cloud-credentials \
  --from-file=${SCRATCH}/credentials</programlisting>
<note>
<simpara>In OpenShift Container Platform versions 4.14 and later, the OADP Operator supports a new standardized STS workflow through the Operator Lifecycle Manager (OLM)
and Cloud Credentials Operator (CCO). In this workflow, you do not need to create the above
secret, you only need to supply the role ARN during the installation of OLM-managed operators using the OpenShift Container Platform web console, for more information see <emphasis>Installing from OperatorHub using the web console</emphasis>.</simpara>
<simpara>The preceding secret is created automatically by CCO.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Install the OADP Operator:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>In the OpenShift Container Platform web console, browse to <emphasis role="strong">Operators</emphasis> &#8594; <emphasis role="strong">OperatorHub</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>Search for the <emphasis role="strong">OADP Operator</emphasis>.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">role_ARN</emphasis> field, paste the role_arn that you created previously and click <emphasis role="strong">Install</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create AWS cloud storage using your AWS credentials by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc create -f -
  apiVersion: oadp.openshift.io/v1alpha1
  kind: CloudStorage
  metadata:
    name: ${CLUSTER_NAME}-oadp
    namespace: openshift-adp
  spec:
    creationSecret:
      key: credentials
      name: cloud-credentials
    enableSharedConfig: true
    name: ${CLUSTER_NAME}-oadp
    provider: aws
    region: $REGION
EOF</programlisting>
</listitem>
<listitem>
<simpara>Check your application&#8217;s storage default storage class by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pvc -n &lt;namespace&gt;</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME     STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
applog   Bound    pvc-351791ae-b6ab-4e8b-88a4-30f73caf5ef8   1Gi        RWO            gp3-csi        4d19h
mysql    Bound    pvc-16b8e009-a20a-4379-accc-bc81fedd0621   1Gi        RWO            gp3-csi        4d19h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Get the storage class by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageclass</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                PROVISIONER             RECLAIMPOLICY   VOLUMEBINDINGMODE      ALLOWVOLUMEEXPANSION   AGE
gp2                 kubernetes.io/aws-ebs   Delete          WaitForFirstConsumer   true                   4d21h
gp2-csi             ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   4d21h
gp3                 ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   4d21h
gp3-csi (default)   ebs.csi.aws.com         Delete          WaitForFirstConsumer   true                   4d21h</programlisting>
</para>
</formalpara>
<note>
<simpara>The following storage classes will work:</simpara>
<itemizedlist>
<listitem>
<simpara>gp3-csi</simpara>
</listitem>
<listitem>
<simpara>gp2-csi</simpara>
</listitem>
<listitem>
<simpara>gp3</simpara>
</listitem>
<listitem>
<simpara>gp2</simpara>
</listitem>
</itemizedlist>
</note>
<simpara>If the application or applications that are being backed up are all using persistent volumes (PVs) with Container Storage Interface (CSI), it is advisable to include the CSI plugin in the OADP DPA configuration.</simpara>
</listitem>
<listitem>
<simpara>Create the <literal>DataProtectionApplication</literal> resource to configure the connection to the storage where the backups and volume snapshots are stored:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>If you are using only CSI volumes, deploy a Data Protection Application by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc create -f -
  apiVersion: oadp.openshift.io/v1alpha1
  kind: DataProtectionApplication
  metadata:
    name: ${CLUSTER_NAME}-dpa
    namespace: openshift-adp
  spec:
    backupImages: false
    features:
      dataMover:
        enable: false
    backupLocations:
    - bucket:
        cloudStorageRef:
          name: ${CLUSTER_NAME}-oadp
        credential:
          key: credentials
          name: cloud-credentials
        default: true
        config:
          region: ${REGION}
    configuration:
      velero:
        defaultPlugins:
        - openshift
        - aws
        - csi
      restic:
        enable: false
EOF</programlisting>
</listitem>
<listitem>
<simpara>If you are using CSI or non-CSI volumes, deploy a Data Protection Application by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc create -f -
  apiVersion: oadp.openshift.io/v1alpha1
  kind: DataProtectionApplication
  metadata:
    name: ${CLUSTER_NAME}-dpa
    namespace: openshift-adp
  spec:
    backupLocations:
    - bucket:
        cloudStorageRef:
          name: ${CLUSTER_NAME}-oadp
        credential:
          key: credentials
          name: cloud-credentials
        default: true
        config:
          region: ${REGION}
    configuration:
      velero:
        defaultPlugins:
        - openshift
        - aws
      nodeAgent: <co xml:id="CO45-1"/>
        enable: false
        uploaderType: restic
    snapshotLocations:
      - velero:
          config:
            credentialsFile: /tmp/credentials/openshift-adp/cloud-credentials-credentials <co xml:id="CO45-2"/>
            enableSharedConfig: "true" <co xml:id="CO45-3"/>
            profile: default <co xml:id="CO45-4"/>
            region: ${REGION} <co xml:id="CO45-5"/>
          provider: aws
EOF</programlisting>
<calloutlist>
<callout arearefs="CO45-1">
<para>See the following note.</para>
</callout>
<callout arearefs="CO45-2">
<para>The <literal>credentialsFile</literal> field is the mounted location of the bucket credential on the pod.</para>
</callout>
<callout arearefs="CO45-3">
<para>The <literal>enableSharedConfig</literal> field allows the <literal>snapshotLocations</literal> to share or reuse the credential defined for the bucket.</para>
</callout>
<callout arearefs="CO45-4">
<para>Use the profile name set in the AWS credentials file.</para>
</callout>
<callout arearefs="CO45-5">
<para>Specify <literal>region</literal> as your AWS region. This must be the same as the cluster region.</para>
</callout>
</calloutlist>
<simpara>You are now ready to back up and restore OpenShift Container Platform applications, as described in <emphasis>Backing up applications</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<note>
<simpara>The <literal>enable</literal> parameter of <literal>restic</literal> is set to <literal>false</literal> in this configuration, because OADP does not support Restic in ROSA environments.</simpara>
<simpara>If you use OADP 1.2, replace this configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">nodeAgent:
  enable: false
  uploaderType: restic</programlisting>
<simpara>with the following configuration:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">restic:
  enable: false</programlisting>
</note>
<note>
<simpara>If you want to use two different clusters for backing up and restoring, the two clusters must have the same AWS S3 storage names in both the cloud storage CR and the OADP <literal>DataProtectionApplication</literal> configuration.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/documentation/en-us/openshift_container_platform/4.14/html/operators/user-tasks#olm-installing-from-operatorhub-using-web-console_olm-installing-operators-in-namespace">Installing from OperatorHub using the web console</link>.</simpara>
</listitem>
<listitem>
<simpara><link xlink:href="https://docs.openshift.com/container-platform/4.14/backup_and_restore/application_backup_and_restore/backing_up_and_restoring/backing-up-applications.html">Backing up applications</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-rosa-backing-up-and-cleaning">
<title>Example: Backing up workload on OADP ROSA STS, with an optional cleanup</title>
<section xml:id="performing-a-backup-oadp-rosa-sts_oadp-rosa-backing-up-applications">
<title>Performing a backup with OADP and ROSA STS</title>
<simpara>The following example <literal>hello-world</literal> application has no persistent volumes (PVs) attached. Perform a backup with OpenShift API for Data Protection (OADP) with Red Hat OpenShift Service on AWS (ROSA) STS.</simpara>
<simpara>Either Data Protection Application (DPA) configuration will work.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a workload to back up by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create namespace hello-world</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc new-app -n hello-world --image=docker.io/openshift/hello-openshift</programlisting>
</listitem>
<listitem>
<simpara>Expose the route by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc expose service/hello-openshift -n hello-world</programlisting>
</listitem>
<listitem>
<simpara>Check that the application is working by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl `oc get route/hello-openshift -n hello-world -o jsonpath='{.spec.host}'`</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Hello OpenShift!</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Back up the workload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc create -f -
  apiVersion: velero.io/v1
  kind: Backup
  metadata:
    name: hello-world
    namespace: openshift-adp
  spec:
    includedNamespaces:
    - hello-world
    storageLocation: ${CLUSTER_NAME}-dpa-1
    ttl: 720h0m0s
EOF</programlisting>
</listitem>
<listitem>
<simpara>Wait until the backup is completed and then run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc -n openshift-adp get backup hello-world -o json | jq .status"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "completionTimestamp": "2022-09-07T22:20:44Z",
  "expiration": "2022-10-07T22:20:22Z",
  "formatVersion": "1.1.0",
  "phase": "Completed",
  "progress": {
    "itemsBackedUp": 58,
    "totalItems": 58
  },
  "startTimestamp": "2022-09-07T22:20:22Z",
  "version": 1
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the demo workload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete ns hello-world</programlisting>
</listitem>
<listitem>
<simpara>Restore the workload from the backup by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt; EOF | oc create -f -
  apiVersion: velero.io/v1
  kind: Restore
  metadata:
    name: hello-world
    namespace: openshift-adp
  spec:
    backupName: hello-world
EOF</programlisting>
</listitem>
<listitem>
<simpara>Wait for the Restore to finish by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ watch "oc -n openshift-adp get restore hello-world -o json | jq .status"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="json" linenumbering="unnumbered">{
  "completionTimestamp": "2022-09-07T22:25:47Z",
  "phase": "Completed",
  "progress": {
    "itemsRestored": 38,
    "totalItems": 38
  },
  "startTimestamp": "2022-09-07T22:25:28Z",
  "warnings": 9
}</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check that the workload is restored by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n hello-world get pods</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                              READY   STATUS    RESTARTS   AGE
hello-openshift-9f885f7c6-kdjpj   1/1     Running   0          90s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the JSONPath by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ curl `oc get route/hello-openshift -n hello-world -o jsonpath='{.spec.host}'`</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Hello OpenShift!</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<note>
<simpara>For troubleshooting tips, see the OADP team’s <link xlink:href="https://access.redhat.com/articles/5456281">troubleshooting documentation</link>.</simpara>
</note>
</section>
<section xml:id="cleanup-a-backup-oadp-rosa-sts_oadp-rosa-backing-up-applications">
<title>Cleaning up a cluster after a backup with OADP and ROSA STS</title>
<simpara>If you need to uninstall the OpenShift API for Data Protection (OADP) Operator together with the backups and the S3 bucket from this example, follow these instructions.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the workload by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete ns hello-world</programlisting>
</listitem>
<listitem>
<simpara>Delete the Data Protection Application (DPA) by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp delete dpa ${CLUSTER_NAME}-dpa</programlisting>
</listitem>
<listitem>
<simpara>Delete the cloud storage by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp delete cloudstorage ${CLUSTER_NAME}-oadp</programlisting>
<warning>
<simpara>If this command hangs, you might need to delete the finalizer by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp patch cloudstorage ${CLUSTER_NAME}-oadp -p '{"metadata":{"finalizers":null}}' --type=merge</programlisting>
</warning>
</listitem>
<listitem>
<simpara>If the Operator is no longer required, remove it by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp delete subscription oadp-operator</programlisting>
</listitem>
<listitem>
<simpara>Remove the namespace from the Operator:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete ns openshift-adp</programlisting>
</listitem>
<listitem>
<simpara>If the backup and restore resources are no longer required, remove them from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete backup hello-world</programlisting>
</listitem>
<listitem>
<simpara>To delete backup, restore and remote objects in AWS S3 run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ velero backup delete hello-world</programlisting>
</listitem>
<listitem>
<simpara>If you no longer need the Custom Resource Definitions (CRD), remove them from the cluster by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ for CRD in `oc get crds | grep velero | awk '{print $1}'`; do oc delete crd $CRD; done</programlisting>
</listitem>
<listitem>
<simpara>Delete the AWS S3 bucket by running the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws s3 rm s3://${CLUSTER_NAME}-oadp --recursive</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ aws s3api delete-bucket --bucket ${CLUSTER_NAME}-oadp</programlisting>
</listitem>
<listitem>
<simpara>Detach the policy from the role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam detach-role-policy --role-name "${ROLE_NAME}" \
  --policy-arn "${POLICY_ARN}"</programlisting>
</listitem>
<listitem>
<simpara>Delete the role by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ aws iam delete-role --role-name "${ROLE_NAME}"</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
<section xml:id="_oadp_data_mover">
<title>OADP Data Mover</title>
<section xml:id="oadp-data-mover-intro">
<title>OADP Data Mover Introduction</title>

<simpara>OADP Data Mover allows you to restore stateful applications from the store if a failure, accidental deletion, or corruption of the cluster occurs.</simpara>
<note>
<simpara>The OADP 1.1 Data Mover is a Technology Preview feature.</simpara>
<simpara>The OADP 1.2 Data Mover has significantly improved features and performances, but is still a Technology Preview feature.</simpara>
</note>
<important>
<simpara>The OADP Data Mover is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<itemizedlist>
<listitem>
<simpara>You can use OADP Data Mover to back up Container Storage Interface (CSI) volume snapshots to a remote object store. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/oadp-using-data-mover-for-csi-snapshots-doc.xml#oadp-using-data-mover-for-csi-snapshots-doc">Using Data Mover for CSI snapshots</link>.</simpara>
</listitem>
<listitem>
<simpara>You can use OADP 1.2 Data Mover to backup and restore application data for clusters that use CephFS, CephRBD, or both. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/oadp-using-data-mover-for-csi-snapshots-doc.xml#oadp-using-data-mover-for-csi-snapshots-doc">Using OADP 1.2 Data Mover with Ceph storage</link>.</simpara>
</listitem>
<listitem>
<simpara>You must perform a data cleanup after you perform a backup, if you are using OADP 1.1 Data Mover. See <link xlink:href="../../../backup_and_restore/application_backup_and_restore/installing/oadp-cleaning-up-after-data-mover-1-1-backup-doc.xml#oadp-cleaning-up-after-data-mover-1-1-backup-doc">Cleaning up after a backup using OADP 1.1 Data Mover</link>.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Post-migration hooks are not likely to work well with the OADP 1.3 Data Mover.</simpara>
<simpara>The OADP 1.1 and OADP 1.2 Data Movers use synchronous processes to back up and restore application data. Because the processes are synchronous, users can be sure that any post-restore hooks start only after the persistent volumes (PVs) of the related pods are released by the persistent volume claim (PVC) of the Data Mover.</simpara>
<simpara>However, the OADP 1.3 Data Mover uses an asynchronous process. As a result of this difference in sequencing, a post-restore hook might be called before the related PVs were released by the PVC of the Data Mover. If this happens, the pod remains in <literal>Pending</literal> status and cannot run the hook. The hook attempt might time out before the pod is released, leading to a <literal>PartiallyFailed</literal> restore operation.</simpara>
</note>
<section xml:id="oadp-data-mover-prerequisites">
<title>OADP Data Mover prerequisites</title>
<itemizedlist>
<listitem>
<simpara>You have a stateful application running in a separate namespace.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OADP Operator by using Operator Lifecycle Manager (OLM).</simpara>
</listitem>
<listitem>
<simpara>You have created an appropriate <literal>VolumeSnapshotClass</literal> and <literal>StorageClass</literal>.</simpara>
</listitem>
<listitem>
<simpara>You have installed the VolSync operator using OLM.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-using-data-mover-for-csi-snapshots-doc">
<title>Using Data Mover for CSI snapshots</title>

<simpara>The OADP Data Mover enables customers to back up Container Storage Interface (CSI) volume snapshots to a remote object store. When Data Mover is enabled, you can restore stateful applications, using CSI volume snapshots pulled from the object store if a failure, accidental deletion, or corruption of the cluster occurs.</simpara>
<simpara>The Data Mover solution uses the Restic option of VolSync.</simpara>
<simpara>Data Mover supports backup and restore of CSI volume snapshots only.</simpara>
<simpara>In OADP 1.2 Data Mover <literal>VolumeSnapshotBackups</literal> (VSBs) and <literal>VolumeSnapshotRestores</literal> (VSRs) are queued using the VolumeSnapshotMover (VSM). The VSM&#8217;s performance is improved by specifying a concurrent number of VSBs and VSRs simultaneously <literal>InProgress</literal>. After all async plugin operations are complete, the backup is marked as complete.</simpara>
<note>
<simpara>The OADP 1.1 Data Mover is a Technology Preview feature.</simpara>
<simpara>The OADP 1.2 Data Mover has significantly improved features and performances, but is still a Technology Preview feature.</simpara>
</note>
<important>
<simpara>The OADP Data Mover is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<note>
<simpara>Red Hat recommends that customers who use OADP 1.2 Data Mover in order to back up and restore ODF CephFS volumes, upgrade or install OpenShift Container Platform version 4.12 or later for improved performance. OADP Data Mover can leverage CephFS shallow volumes in OpenShift Container Platform version 4.12 or later, which based on our testing, can improve the performance of backup times.</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="https://issues.redhat.com/browse/RHSTOR-4287">CephFS ROX details</link></simpara>
</listitem>
</itemizedlist>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have verified that the <literal>StorageClass</literal> and <literal>VolumeSnapshotClass</literal> custom resources (CRs) support CSI.</simpara>
</listitem>
<listitem>
<simpara>You have verified that only one <literal>VolumeSnapshotClass</literal> CR has the annotation <literal>snapshot.storage.kubernetes.io/is-default-class: "true"</literal>.</simpara>
<note>
<simpara>In OpenShift Container Platform version 4.12 or later, verify that this is the only default <literal>VolumeSnapshotClass</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>You have verified that <literal>deletionPolicy</literal> of the <literal>VolumeSnapshotClass</literal> CR is set to <literal>Retain</literal>.</simpara>
</listitem>
<listitem>
<simpara>You have verified that only one <literal>StorageClass</literal> CR has the annotation <literal>storageclass.kubernetes.io/is-default-class: "true"</literal>.</simpara>
</listitem>
<listitem>
<simpara>You have included the label <literal>velero.io/csi-volumesnapshot-class: "true"</literal> in your <literal>VolumeSnapshotClass</literal> CR.</simpara>
</listitem>
<listitem>
<simpara>You have verified that the <literal>OADP namespace</literal> has the annotation <literal>oc annotate --overwrite namespace/openshift-adp volsync.backube/privileged-movers="true"</literal>.</simpara>
<note>
<simpara>In OADP 1.1 the above setting is mandatory.</simpara>
<simpara>In OADP 1.2 the <literal>privileged-movers</literal> setting is not required in most scenarios. The restoring container permissions should be adequate for the Volsync copy. In some user scenarios, there may be permission errors that the <literal>privileged-mover</literal>= <literal>true</literal> setting should resolve.</simpara>
</note>
</listitem>
<listitem>
<simpara>You have installed the VolSync Operator by using the Operator Lifecycle Manager (OLM).</simpara>
<note>
<simpara>The VolSync Operator is required for using OADP Data Mover.</simpara>
</note>
</listitem>
<listitem>
<simpara>You have installed the OADP operator by using OLM.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Configure a Restic secret by creating a <literal>.yaml</literal> file as following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: &lt;secret_name&gt;
  namespace: openshift-adp
type: Opaque
stringData:
  RESTIC_PASSWORD: &lt;secure_restic_password&gt;</programlisting>
<note>
<simpara>By default, the Operator looks for a secret named <literal>dm-credential</literal>. If you are using a different name, you need to specify the name through a Data Protection Application (DPA) CR using <literal>dpa.spec.features.dataMover.credentialName</literal>.</simpara>
</note>
</listitem>
<listitem>
<simpara>Create a DPA CR similar to the following example. The default plugins include CSI.</simpara>
<formalpara>
<title>Example Data Protection Application (DPA) CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-sample
  namespace: openshift-adp
spec:
  backupLocations:
    - velero:
        config:
          profile: default
          region: us-east-1
        credential:
          key: cloud
          name: cloud-credentials
        default: true
        objectStorage:
          bucket: &lt;bucket_name&gt;
          prefix: &lt;bucket-prefix&gt;
        provider: aws
  configuration:
    restic:
      enable: &lt;true_or_false&gt;
    velero:
       itemOperationSyncFrequency: "10s"
       defaultPlugins:
        - openshift
        - aws
        - csi
        - vsm <co xml:id="CO46-1"/>
  features:
    dataMover:
      credentialName: restic-secret
      enable: true
      maxConcurrentBackupVolumes: "3" <co xml:id="CO46-2"/>
      maxConcurrentRestoreVolumes: "3" <co xml:id="CO46-3"/>
      pruneInterval: "14" <co xml:id="CO46-4"/>
      volumeOptions: <co xml:id="CO46-5"/>
      sourceVolumeOptions:
          accessMode: ReadOnlyMany
          cacheAccessMode: ReadWriteOnce
          cacheCapacity: 2Gi
      destinationVolumeOptions:
          storageClass: other-storageclass-name
          cacheAccessMode: ReadWriteMany
  snapshotLocations:
    - velero:
        config:
          profile: default
          region: us-west-2
        provider: aws</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO46-1">
<para>OADP 1.2 only.</para>
</callout>
<callout arearefs="CO46-2">
<para>OADP 1.2 only. Optional: Specify the upper limit of the number of snapshots allowed to be queued for backup. The default value is 10.</para>
</callout>
<callout arearefs="CO46-3">
<para>OADP 1.2 only. Optional: Specify the upper limit of the number of snapshots allowed to be queued for restore. The default value is 10.</para>
</callout>
<callout arearefs="CO46-4">
<para>OADP 1.2 only. Optional: Specify the number of days, between running Restic pruning on the repository. The prune operation repacks the data to free space, but it can also generate significant I/O traffic as a part of the process. Setting this option allows a trade-off between storage consumption, from no longer referenced data, and access costs.</para>
</callout>
<callout arearefs="CO46-5">
<para>OADP 1.2 only. Optional: Specify VolumeSync volume options for backup and restore.</para>
</callout>
</calloutlist>
<simpara>The OADP Operator installs two custom resource definitions (CRDs), <literal>VolumeSnapshotBackup</literal> and <literal>VolumeSnapshotRestore</literal>.</simpara>
<formalpara>
<title>Example <literal>VolumeSnapshotBackup</literal> CRD</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: datamover.oadp.openshift.io/v1alpha1
kind: VolumeSnapshotBackup
metadata:
  name: &lt;vsb_name&gt;
  namespace: &lt;namespace_name&gt; <co xml:id="CO47-1"/>
spec:
  volumeSnapshotContent:
    name: &lt;snapcontent_name&gt;
  protectedNamespace: &lt;adp_namespace&gt; <co xml:id="CO47-2"/>
  resticSecretRef:
    name: &lt;restic_secret_name&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO47-1">
<para>Specify the namespace where the volume snapshot exists.</para>
</callout>
<callout arearefs="CO47-2">
<para>Specify the namespace where the OADP Operator is installed. The default is <literal>openshift-adp</literal>.</para>
</callout>
</calloutlist>
<formalpara>
<title>Example <literal>VolumeSnapshotRestore</literal> CRD</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: datamover.oadp.openshift.io/v1alpha1
kind: VolumeSnapshotRestore
metadata:
  name: &lt;vsr_name&gt;
  namespace: &lt;namespace_name&gt; <co xml:id="CO48-1"/>
spec:
  protectedNamespace: &lt;protected_ns&gt; <co xml:id="CO48-2"/>
  resticSecretRef:
    name: &lt;restic_secret_name&gt;
  volumeSnapshotMoverBackupRef:
    sourcePVCData:
      name: &lt;source_pvc_name&gt;
      size: &lt;source_pvc_size&gt;
    resticrepository: &lt;your_restic_repo&gt;
    volumeSnapshotClassName: &lt;vsclass_name&gt;</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO48-1">
<para>Specify the namespace where the volume snapshot exists.</para>
</callout>
<callout arearefs="CO48-2">
<para>Specify the namespace where the OADP Operator is installed. The default is <literal>openshift-adp</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>You can back up a volume snapshot by performing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a backup CR:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup_name&gt;
  namespace: &lt;protected_ns&gt; <co xml:id="CO49-1"/>
spec:
  includedNamespaces:
  - &lt;app_ns&gt; <co xml:id="CO49-2"/>
  storageLocation: velero-sample-1</programlisting>
<calloutlist>
<callout arearefs="CO49-1">
<para>Specify the namespace where the Operator is installed. The default namespace is <literal>openshift-adp</literal>.</para>
</callout>
<callout arearefs="CO49-2">
<para>Specify the application namespace or namespaces to be backed up.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Wait up to 10 minutes and check whether the <literal>VolumeSnapshotBackup</literal> CR status is <literal>Completed</literal> by entering the following commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsb -n &lt;app_ns&gt;</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsb &lt;vsb_name&gt; -n &lt;app_ns&gt; -o jsonpath="{.status.phase}"</programlisting>
<simpara>A snapshot is created in the object store was configured in the DPA.</simpara>
<note>
<simpara>If the status of the <literal>VolumeSnapshotBackup</literal> CR becomes <literal>Failed</literal>, refer to the Velero logs for troubleshooting.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>You can restore a volume snapshot by performing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Delete the application namespace and the <literal>VolumeSnapshotContent</literal> that was created by the Velero CSI plugin.</simpara>
</listitem>
<listitem>
<simpara>Create a <literal>Restore</literal> CR and set <literal>restorePVs</literal> to <literal>true</literal>.</simpara>
<formalpara>
<title>Example <literal>Restore</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore_name&gt;
  namespace: &lt;protected_ns&gt;
spec:
  backupName: &lt;previous_backup_name&gt;
  restorePVs: true</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Wait up to 10 minutes and check whether the <literal>VolumeSnapshotRestore</literal> CR status is <literal>Completed</literal> by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsr -n &lt;app_ns&gt;</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsr &lt;vsr_name&gt; -n &lt;app_ns&gt; -o jsonpath="{.status.phase}"</programlisting>
</listitem>
<listitem>
<simpara>Check whether your application data and resources have been restored.</simpara>
<note>
<simpara>If the status of the <literal>VolumeSnapshotRestore</literal> CR becomes 'Failed', refer to the Velero logs for troubleshooting.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-12-data-mover-ceph-doc">
<title>Using OADP 1.2 Data Mover with Ceph storage</title>

<simpara>You can use OADP 1.2 Data Mover to backup and restore application data for clusters that use CephFS, CephRBD, or both.</simpara>
<simpara>OADP 1.2 Data Mover leverages Ceph features that support large-scale environments. One of these is the shallow copy method, which is available for OpenShift Container Platform 4.12 and later. This feature supports backing up and restoring <literal>StorageClass</literal> and <literal>AccessMode</literal> resources other than what is found on the source persistent volume claim (PVC).</simpara>
<important>
<simpara>The CephFS shallow copy feature is a back up feature. It is not part of restore operations.</simpara>
</important>
<section xml:id="oadp-ceph-prerequisites_backing-up-applications">
<title>Prerequisites for using OADP 1.2 Data Mover with Ceph storage</title>
<simpara>The following prerequisites apply to all back up and restore operations of data using OpenShift API for Data Protection (OADP) 1.2 Data Mover in a cluster that uses Ceph storage:</simpara>
<itemizedlist>
<listitem>
<simpara>You have installed OpenShift Container Platform 4.12 or later.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You have created a secret <literal>cloud-credentials</literal> in the namespace <literal>openshift-adp.</literal></simpara>
</listitem>
<listitem>
<simpara>You have installed Red Hat OpenShift Data Foundation.</simpara>
</listitem>
<listitem>
<simpara>You have installed the latest VolSync Operator by using Operator Lifecycle Manager.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="defining-crs-for-12-data-mover">
<title>Defining custom resources for use with OADP 1.2 Data Mover</title>
<simpara>When you install Red Hat OpenShift Data Foundation, it automatically creates default CephFS and a CephRBD <literal>StorageClass</literal> and <literal>VolumeSnapshotClass</literal> custom resources (CRs). You must define these CRs for use with OpenShift API for Data Protection (OADP) 1.2 Data Mover.</simpara>
<simpara>After you define the CRs, you must make several other changes to your environment before you can perform your back up and restore operations.</simpara>
<section xml:id="oadp-ceph-preparing-cephfs-crs_backing-up-applications">
<title>Defining CephFS custom resources for use with OADP 1.2 Data Mover</title>
<simpara>When you install Red Hat OpenShift Data Foundation, it automatically creates a default CephFS <literal>StorageClass</literal> custom resource (CR) and a default CephFS <literal>VolumeSnapshotClass</literal> CR. You can define these CRs for use with OpenShift API for Data Protection (OADP) 1.2 Data Mover.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Define the <literal>VolumeSnapshotClass</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>VolumeSnapshotClass</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.storage.k8s.io/v1
deletionPolicy: Retain <co xml:id="CO50-1"/>
driver: openshift-storage.cephfs.csi.ceph.com
kind: VolumeSnapshotClass
metadata:
  annotations:
    snapshot.storage.kubernetes.io/is-default-class: true <co xml:id="CO50-2"/>
  labels:
    velero.io/csi-volumesnapshot-class: true <co xml:id="CO50-3"/>
  name: ocs-storagecluster-cephfsplugin-snapclass
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/snapshotter-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/snapshotter-secret-namespace: openshift-storage</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO50-1">
<para>Must be set to <literal>Retain</literal>.</para>
</callout>
<callout arearefs="CO50-2">
<para>Must be set to <literal>true</literal>.</para>
</callout>
<callout arearefs="CO50-3">
<para>Must be set to <literal>true</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Define the <literal>StorageClass</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>StorageClass</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ocs-storagecluster-cephfs
  annotations:
    description: Provides RWO and RWX Filesystem volumes
    storageclass.kubernetes.io/is-default-class: true <co xml:id="CO51-1"/>
provisioner: openshift-storage.cephfs.csi.ceph.com
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  fsName: ocs-storagecluster-cephfilesystem
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO51-1">
<para>Must be set to <literal>true</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-ceph-preparing-cephrbd-crs_backing-up-applications">
<title>Defining CephRBD custom resources for use with OADP 1.2 Data Mover</title>
<simpara>When you install Red Hat OpenShift Data Foundation, it automatically creates a default CephRBD <literal>StorageClass</literal> custom resource (CR) and a default CephRBD <literal>VolumeSnapshotClass</literal> CR. You can define these CRs for use with OpenShift API for Data Protection (OADP) 1.2 Data Mover.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Define the <literal>VolumeSnapshotClass</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>VolumeSnapshotClass</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: snapshot.storage.k8s.io/v1
deletionPolicy: Retain <co xml:id="CO52-1"/>
driver: openshift-storage.rbd.csi.ceph.com
kind: VolumeSnapshotClass
metadata:
  labels:
    velero.io/csi-volumesnapshot-class: true <co xml:id="CO52-2"/>
  name: ocs-storagecluster-rbdplugin-snapclass
parameters:
  clusterID: openshift-storage
  csi.storage.k8s.io/snapshotter-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/snapshotter-secret-namespace: openshift-storage</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO52-1">
<para>Must be set to <literal>Retain</literal>.</para>
</callout>
<callout arearefs="CO52-2">
<para>Must be set to <literal>true</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Define the <literal>StorageClass</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>StorageClass</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ocs-storagecluster-ceph-rbd
  annotations:
    description: 'Provides RWO Filesystem volumes, and RWO and RWX Block volumes'
provisioner: openshift-storage.rbd.csi.ceph.com
parameters:
  csi.storage.k8s.io/fstype: ext4
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
  imageFormat: '2'
  clusterID: openshift-storage
  imageFeatures: layering
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  pool: ocs-storagecluster-cephblockpool
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-ceph-preparing-crs-additional_backing-up-applications">
<title>Defining additional custom resources for use with OADP 1.2 Data Mover</title>
<simpara>After you redefine the default <literal>StorageClass</literal> and CephRBD <literal>VolumeSnapshotClass</literal> custom resources (CRs), you must create the following CRs:</simpara>
<itemizedlist>
<listitem>
<simpara>A CephFS <literal>StorageClass</literal> CR defined to use the shallow copy feature</simpara>
</listitem>
<listitem>
<simpara>A Restic <literal>Secret</literal> CR</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a CephFS <literal>StorageClass</literal> CR and set the <literal>backingSnapshot</literal> parameter set to <literal>true</literal> as in the following example:</simpara>
<formalpara>
<title>Example CephFS <literal>StorageClass</literal> CR with <literal>backingSnapshot</literal> set to <literal>true</literal></title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: ocs-storagecluster-cephfs-shallow
  annotations:
    description: Provides RWO and RWX Filesystem volumes
    storageclass.kubernetes.io/is-default-class: false
provisioner: openshift-storage.cephfs.csi.ceph.com
parameters:
  csi.storage.k8s.io/provisioner-secret-namespace: openshift-storage
  csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
  csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
  csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
  clusterID: openshift-storage
  fsName: ocs-storagecluster-cephfilesystem
  csi.storage.k8s.io/controller-expand-secret-namespace: openshift-storage
  backingSnapshot: true <co xml:id="CO53-1"/>
  csi.storage.k8s.io/node-stage-secret-namespace: openshift-storage
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO53-1">
<para>Must be set to <literal>true</literal>.</para>
</callout>
</calloutlist>
<important>
<simpara>Ensure that the CephFS <literal>VolumeSnapshotClass</literal> and <literal>StorageClass</literal> CRs have the same value for <literal>provisioner</literal>.</simpara>
</important>
</listitem>
<listitem>
<simpara>Configure a Restic <literal>Secret</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example Restic <literal>Secret</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: Secret
metadata:
  name: &lt;secret_name&gt;
  namespace: &lt;namespace&gt;
type: Opaque
stringData:
  RESTIC_PASSWORD: &lt;restic_password&gt;</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-ceph-back-up-restore-cephfs">
<title>Backing up and restoring data using OADP 1.2 Data Mover and CephFS storage</title>
<simpara>You can use OpenShift API for Data Protection (OADP) 1.2 Data Mover to back up and restore data using CephFS storage by enabling the shallow copy feature of CephFS.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A stateful application is running in a separate namespace with persistent volume claims (PVCs) using CephFS as the provisioner.</simpara>
</listitem>
<listitem>
<simpara>The <literal>StorageClass</literal> and <literal>VolumeSnapshotClass</literal> custom resources (CRs) are defined for CephFS and OADP 1.2 Data Mover.</simpara>
</listitem>
<listitem>
<simpara>There is a secret <literal>cloud-credentials</literal> in the <literal>openshift-adp</literal> namespace.</simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-ceph-cephfs-back-up-dba_cephfs">
<title>Creating a DPA for use with CephFS storage</title>
<simpara>You must create a Data Protection Application (DPA) CR before you use the OpenShift API for Data Protection (OADP) 1.2 Data Mover to back up and restore data using CephFS storage.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify that the <literal>deletionPolicy</literal> field of the <literal>VolumeSnapshotClass</literal> CR is set to <literal>Retain</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get volumesnapshotclass -A  -o jsonpath='{range .items[*]}{"Name: "}{.metadata.name}{"  "}{"Retention Policy: "}{.deletionPolicy}{"\n"}{end}'</programlisting>
</listitem>
<listitem>
<simpara>Verify that the labels of the <literal>VolumeSnapshotClass</literal> CR are set to <literal>true</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get volumesnapshotclass -A  -o jsonpath='{range .items[*]}{"Name: "}{.metadata.name}{"  "}{"labels: "}{.metadata.labels}{"\n"}{end}'</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>storageclass.kubernetes.io/is-default-class</literal> annotation of the <literal>StorageClass</literal> CR is set to <literal>true</literal> by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get storageClass -A  -o jsonpath='{range .items[*]}{"Name: "}{.metadata.name}{"  "}{"annotations: "}{.metadata.annotations}{"\n"}{end}'</programlisting>
</listitem>
<listitem>
<simpara>Create a Data Protection Application (DPA) CR similar to the following example:</simpara>
<formalpara>
<title>Example DPA CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-sample
  namespace: openshift-adp
spec:
  backupLocations:
    - velero:
        config:
          profile: default
          region: us-east-1
        credential:
          key: cloud
          name: cloud-credentials
        default: true
        objectStorage:
          bucket: &lt;my_bucket&gt;
          prefix: velero
       provider: aws
    configuration:
      restic:
        enable: false  <co xml:id="CO54-1"/>
      velero:
        defaultPlugins:
          - openshift
          - aws
          - csi
          - vsm
    features:
      dataMover:
        credentialName: &lt;restic_secret_name&gt; <co xml:id="CO54-2"/>
        enable: true <co xml:id="CO54-3"/>
        volumeOptionsForStorageClasses:
          ocs-storagecluster-cephfs:
            sourceVolumeOptions:
              accessMode: ReadOnlyMany
              cacheAccessMode: ReadWriteMany
              cacheStorageClassName: ocs-storagecluster-cephfs
              storageClassName: ocs-storagecluster-cephfs-shallow</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO54-1">
<para>There is no default value for the <literal>enable</literal> field. Valid values are <literal>true</literal> or <literal>false</literal>.</para>
</callout>
<callout arearefs="CO54-2">
<para>Use the Restic <literal>Secret</literal> that you created when you prepared your environment for working with OADP 1.2 Data Mover and Ceph. If you do not use your Restic <literal>Secret</literal>, the CR uses the default value <literal>dm-credential</literal> for this parameter.</para>
</callout>
<callout arearefs="CO54-3">
<para>There is no default value for the <literal>enable</literal> field. Valid values are <literal>true</literal> or <literal>false</literal>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-ceph-cephfs-back-up_cephfs">
<title>Backing up data using OADP 1.2 Data Mover and CephFS storage</title>
<simpara>You can use OpenShift API for Data Protection (OADP) 1.2 Data Mover to back up data using CephFS storage by enabling the shallow copy feature of CephFS storage.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Backup</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>Backup</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup_name&gt;
  namespace: &lt;protected_ns&gt;
spec:
  includedNamespaces:
  - &lt;app_ns&gt;
  storageLocation: velero-sample-1</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the progress of the <literal>VolumeSnapshotBackup</literal> CRs by completing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To check the progress of all the <literal>VolumeSnapshotBackup</literal> CRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsb -n &lt;app_ns&gt;</programlisting>
</listitem>
<listitem>
<simpara>To check the progress of a specific <literal>VolumeSnapshotBackup</literal> CR, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsb &lt;vsb_name&gt; -n &lt;app_ns&gt; -ojsonpath="{.status.phase}`</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Wait several minutes until the <literal>VolumeSnapshotBackup</literal> CR has the status <literal>Completed</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify that there is at least one snapshot in the object store that is given in the Restic <literal>Secret</literal>. You can check for this snapshot in your targeted <literal>BackupStorageLocation</literal> storage provider that has a prefix of <literal>/&lt;OADP_namespace&gt;</literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-ceph-cephfs-restore_cephfs">
<title>Restoring data using OADP 1.2 Data Mover and CephFS storage</title>
<simpara>You can use OpenShift API for Data Protection (OADP) 1.2 Data Mover to restore data using CephFS storage if the shallow copy feature of CephFS storage was enabled for the back up procedure. The shallow copy feature is not used in the restore procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the application namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vsb -n &lt;app_namespace&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>Delete any <literal>VolumeSnapshotContent</literal> CRs that were created during backup by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshotcontent --all</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Restore</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>Restore</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore_name&gt;
  namespace: &lt;protected_ns&gt;
spec:
  backupName: &lt;previous_backup_name&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the progress of the <literal>VolumeSnapshotRestore</literal> CRs by doing the following:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To check the progress of all the <literal>VolumeSnapshotRestore</literal> CRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsr -n &lt;app_ns&gt;</programlisting>
</listitem>
<listitem>
<simpara>To check the progress of a specific <literal>VolumeSnapshotRestore</literal> CR, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsr &lt;vsr_name&gt; -n &lt;app_ns&gt; -ojsonpath="{.status.phase}</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that your application data has been restored by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route &lt;route_name&gt; -n &lt;app_ns&gt; -ojsonpath="{.spec.host}"</programlisting>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-ceph-split">
<title>Backing up and restoring data using OADP 1.2 Data Mover and split volumes (CephFS and Ceph RBD)</title>
<simpara>You can use OpenShift API for Data Protection (OADP) 1.2 Data Mover to back up and restore data in an environment that has <emphasis>split volumes</emphasis>, that is, an environment that uses both CephFS and CephRBD.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>A stateful application is running in a separate namespace with persistent volume claims (PVCs) using CephFS as the provisioner.</simpara>
</listitem>
<listitem>
<simpara>The <literal>StorageClass</literal> and <literal>VolumeSnapshotClass</literal> custom resources (CRs) are defined for CephFS and OADP 1.2 Data Mover.</simpara>
</listitem>
<listitem>
<simpara>There is a secret <literal>cloud-credentials</literal> in the <literal>openshift-adp</literal> namespace.</simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-ceph-split-back-up-dba_split">
<title>Creating a DPA for use with split volumes</title>
<simpara>You must create a Data Protection Application (DPA) CR before you use the OpenShift API for Data Protection (OADP) 1.2 Data Mover to back up and restore data using split volumes.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Create a Data Protection Application (DPA) CR as in the following example:</simpara>
<formalpara>
<title>Example DPA CR for environment with split volumes</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-sample
  namespace: openshift-adp
spec:
  backupLocations:
    - velero:
        config:
          profile: default
          region: us-east-1
        credential:
          key: cloud
          name: cloud-credentials
        default: true
        objectStorage:
          bucket: &lt;my-bucket&gt;
          prefix: velero
        provider: aws
  configuration:
    restic:
      enable: false
    velero:
      defaultPlugins:
        - openshift
        - aws
        - csi
        - vsm
  features:
    dataMover:
      credentialName: &lt;restic_secret_name&gt; <co xml:id="CO55-1"/>
      enable: true
      volumeOptionsForStorageClasses: <co xml:id="CO55-2"/>
        ocs-storagecluster-cephfs:
          sourceVolumeOptions:
            accessMode: ReadOnlyMany
            cacheAccessMode: ReadWriteMany
            cacheStorageClassName: ocs-storagecluster-cephfs
            storageClassName: ocs-storagecluster-cephfs-shallow
        ocs-storagecluster-ceph-rbd:
          sourceVolumeOptions:
            storageClassName: ocs-storagecluster-ceph-rbd
            cacheStorageClassName: ocs-storagecluster-ceph-rbd
        destinationVolumeOptions:
            storageClassName: ocs-storagecluster-ceph-rbd
            cacheStorageClassName: ocs-storagecluster-ceph-rbd</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO55-1">
<para>Use the Restic <literal>Secret</literal> that you created when you prepared your environment for working with OADP 1.2 Data Mover and Ceph. If you do not, then the CR will use the default value <literal>dm-credential</literal> for this parameter.</para>
</callout>
<callout arearefs="CO55-2">
<para>A different set of <literal>VolumeOptionsForStorageClass</literal> labels can be defined for each <literal>storageClass</literal> volume, thus allowing a backup to volumes with different providers.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-ceph-cephfs-back-up_split">
<title>Backing up data using OADP 1.2 Data Mover and split volumes</title>
<simpara>You can use OpenShift API for Data Protection (OADP) 1.2 Data Mover to back up data in an environment that has split volumes.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>Backup</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>Backup</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
  name: &lt;backup_name&gt;
  namespace: &lt;protected_ns&gt;
spec:
  includedNamespaces:
  - &lt;app_ns&gt;
  storageLocation: velero-sample-1</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the progress of the <literal>VolumeSnapshotBackup</literal> CRs by completing the following steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To check the progress of all the <literal>VolumeSnapshotBackup</literal> CRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsb -n &lt;app_ns&gt;</programlisting>
</listitem>
<listitem>
<simpara>To check the progress of a specific <literal>VolumeSnapshotBackup</literal> CR, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsb &lt;vsb_name&gt; -n &lt;app_ns&gt; -ojsonpath="{.status.phase}`</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Wait several minutes until the <literal>VolumeSnapshotBackup</literal> CR has the status <literal>Completed</literal>.</simpara>
</listitem>
<listitem>
<simpara>Verify that there is at least one snapshot in the object store that is given in the Restic <literal>Secret</literal>. You can check for this snapshot in your targeted <literal>BackupStorageLocation</literal> storage provider that has a prefix of <literal>/&lt;OADP_namespace&gt;</literal>.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-ceph-cephfs-restore_split">
<title>Restoring data using OADP 1.2 Data Mover and split volumes</title>
<simpara>You can use OpenShift API for Data Protection (OADP) 1.2 Data Mover to restore data in an environment that has split volumes, if the shallow copy feature of CephFS storage was enabled for the back up procedure. The shallow copy feature is not used in the restore procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete the application namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vsb -n &lt;app_namespace&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>Delete any <literal>VolumeSnapshotContent</literal> CRs that were created during backup by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshotcontent --all</programlisting>
</listitem>
<listitem>
<simpara>Create a <literal>Restore</literal> CR as in the following example:</simpara>
<formalpara>
<title>Example <literal>Restore</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: &lt;restore_name&gt;
  namespace: &lt;protected_ns&gt;
spec:
  backupName: &lt;previous_backup_name&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Monitor the progress of the <literal>VolumeSnapshotRestore</literal> CRs by doing the following:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>To check the progress of all the <literal>VolumeSnapshotRestore</literal> CRs, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsr -n &lt;app_ns&gt;</programlisting>
</listitem>
<listitem>
<simpara>To check the progress of a specific <literal>VolumeSnapshotRestore</literal> CR, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get vsr &lt;vsr_name&gt; -n &lt;app_ns&gt; -ojsonpath="{.status.phase}</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that your application data has been restored by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get route &lt;route_name&gt; -n &lt;app_ns&gt; -ojsonpath="{.spec.host}"</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="oadp-cleaning-up-after-data-mover-1-1-backup-doc">
<title>Cleaning up after a backup using OADP 1.1 Data Mover</title>

<simpara>For OADP 1.1 Data Mover, you must perform a data cleanup after you perform a backup.</simpara>
<simpara>The cleanup consists of deleting the following resources:</simpara>
<itemizedlist>
<listitem>
<simpara>Snapshots in a bucket</simpara>
</listitem>
<listitem>
<simpara>Cluster resources</simpara>
</listitem>
<listitem>
<simpara>Volume snapshot backups (VSBs) after a backup procedure that is either run by a schedule or is run repetitively</simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-cleaning-up-after-data-mover-snapshots_datamover11">
<title>Deleting snapshots in a bucket</title>
<simpara>OADP 1.1 Data Mover might leave one or more snapshots in a bucket after a backup. You can either delete all the snapshots or delete individual snapshots.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>To delete all snapshots in your bucket, delete the <literal>/&lt;protected_namespace&gt;</literal> folder that is specified in the Data Protection Application (DPA) <literal>.spec.backupLocation.objectStorage.bucket</literal> resource.</simpara>
</listitem>
<listitem>
<simpara>To delete an individual snapshot:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Browse to the <literal>/&lt;protected_namespace&gt;</literal> folder that is specified in the DPA <literal>.spec.backupLocation.objectStorage.bucket</literal> resource.</simpara>
</listitem>
<listitem>
<simpara>Delete the appropriate folders that are prefixed with <literal>/&lt;volumeSnapshotContent name&gt;-pvc</literal> where <literal>&lt;VolumeSnapshotContent_name&gt;</literal> is the <literal>VolumeSnapshotContent</literal> created by Data Mover per PVC.</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="deleting-cluster-resources-data-mover">
<title>Deleting cluster resources</title>
<simpara>OADP 1.1 Data Mover might leave cluster resources whether or not it successfully backs up your container storage interface (CSI) volume snapshots to a remote object store.</simpara>
<section xml:id="oadp-deleting-cluster-resources-following-success_datamover11">
<title>Deleting cluster resources following a successful backup and restore that used Data Mover</title>
<simpara>You can delete any <literal>VolumeSnapshotBackup</literal> or <literal>VolumeSnapshotRestore</literal> CRs that remain in your application namespace after a successful backup and restore where you used Data Mover.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Delete cluster resources that remain on the application namespace, the namespace with the application PVCs to backup and restore, after a backup where you use Data Mover:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vsb -n &lt;app_namespace&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>Delete cluster resources that remain after a restore where you use Data Mover:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vsr -n &lt;app_namespace&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>If needed, delete any <literal>VolumeSnapshotContent</literal> resources that remain after a backup and restore where you use Data Mover:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshotcontent --all</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-deleting-cluster-resources-following-failure_datamover11">
<title>Deleting cluster resources following a partially successful or a failed backup and restore that used Data Mover</title>
<simpara>If your backup and restore operation that uses Data Mover either fails or only partially succeeds, you must clean up any <literal>VolumeSnapshotBackup</literal> (VSB) or <literal>VolumeSnapshotRestore</literal> custom resource definitions (CRDs) that exist in the application namespace, and clean up any extra resources created by these controllers.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Clean up cluster resources that remain after a backup operation where you used Data Mover by entering the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Delete VSB CRDs on the application namespace, the namespace with the application PVCs to backup and restore:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vsb -n &lt;app_namespace&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>Delete <literal>VolumeSnapshot</literal> CRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshot -A --all</programlisting>
</listitem>
<listitem>
<simpara>Delete <literal>VolumeSnapshotContent</literal> CRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshotcontent --all</programlisting>
</listitem>
<listitem>
<simpara>Delete any PVCs on the protected namespace, the namespace the Operator is installed on.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete pvc -n &lt;protected_namespace&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>Delete any <literal>ReplicationSource</literal> resources on the namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete replicationsource -n &lt;protected_namespace&gt; --all</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Clean up cluster resources that remain after a restore operation using Data Mover by entering the following commands:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Delete VSR CRDs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete vsr -n &lt;app-ns&gt; --all</programlisting>
</listitem>
<listitem>
<simpara>Delete <literal>VolumeSnapshot</literal> CRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshot -A --all</programlisting>
</listitem>
<listitem>
<simpara>Delete <literal>VolumeSnapshotContent</literal> CRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete volumesnapshotcontent --all</programlisting>
</listitem>
<listitem>
<simpara>Delete any <literal>ReplicationDestination</literal> resources on the namespace.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete replicationdestination -n &lt;protected_namespace&gt; --all</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</section>
</section>
</section>
</section>
<section xml:id="_oadp_1_3_data_mover">
<title>OADP 1.3 Data Mover</title>
<section xml:id="about-oadp-1-3-data-mover">
<title>About the OADP 1.3 Data Mover</title>

<simpara>OADP 1.3 includes a built-in Data Mover that you can use to move Container Storage Interface (CSI) volume snapshots to a remote object store. The built-in Data Mover allows you to restore stateful applications from the remote object store if a failure, accidental deletion, or corruption of the cluster occurs. It uses <link xlink:href="../../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-about-kopia.xml#oadp-about-kopia">Kopia</link> as the uploader mechanism to read the snapshot data and write to the unified repository.</simpara>
<simpara>OADP supports CSI snapshots on the following:</simpara>
<itemizedlist>
<listitem>
<simpara>Red Hat OpenShift Data Foundation</simpara>
</listitem>
<listitem>
<simpara>Any other cloud storage provider with the Container Storage Interface (CSI) driver that supports the Kubernetes Volume Snapshot API</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>The OADP built-in Data Mover is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="enabling-oadp-1-3-data-mover">
<title>Enabling the built-in Data Mover</title>
<simpara>To enable the built-in Data Mover, you must include the CSI plugin and enable the node agent in the <literal>DataProtectionApplication</literal> custom resource (CR). The node agent is a Kubernetes daemonset that hosts data movement modules. These include the Data Mover controller, uploader, and the repository.</simpara>
<formalpara>
<title>Example <literal>DataProtectionApplication</literal> manifest</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: dpa-sample
spec:
  configuration:
    nodeAgent:
      enable: true <co xml:id="CO56-1"/>
      uploaderType: kopia <co xml:id="CO56-2"/>
    velero:
      defaultPlugins:
      - openshift
      - aws
      - csi <co xml:id="CO56-3"/>
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO56-1">
<para>The flag to enable the node agent.</para>
</callout>
<callout arearefs="CO56-2">
<para>The type of uploader. The possible values are <literal>restic</literal> or <literal>kopia</literal>. The built-in Data Mover uses Kopia as the default uploader mechanism regardless of the value of the <literal>uploaderType</literal> field.</para>
</callout>
<callout arearefs="CO56-3">
<para>The CSI plugin included in the list of default plugins.</para>
</callout>
</calloutlist>
</section>
<section xml:id="built-in-data-mover-crs">
<title>Built-in Data Mover controller and custom resource definitions (CRDs)</title>
<simpara>The built-in Data Mover feature introduces three new API objects defined as CRDs for managing backup and restore:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>DataDownload</literal>: Represents a data download of a volume snapshot. The CSI plugin creates one <literal>DataDownload</literal> object per volume to be restored. The <literal>DataDownload</literal> CR includes information about the target volume, the specified Data Mover, the progress of the current data download, the specified backup repository, and the result of the current data download after the process is complete.</simpara>
</listitem>
<listitem>
<simpara><literal>DataUpload</literal>: Represents a data upload of a volume snapshot. The CSI plugin creates one <literal>DataUpload</literal> object per CSI snapshot. The <literal>DataUpload</literal> CR includes information about the specified snapshot, the specified Data Mover, the specified backup repository, the progress of the current data upload, and the result of the current data upload after the process is complete.</simpara>
</listitem>
<listitem>
<simpara><literal>BackupRepository</literal>: Represents and manages the lifecycle of the backup repositories. OADP creates a backup repository per namespace when the first CSI snapshot backup or restore for a namespace is requested.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-backup-restore-csi-snapshots">
<title>Backing up and restoring CSI snapshots</title>

<simpara>You can back up and restore persistent volumes by using the OADP 1.3 Data Mover.</simpara>
<section xml:id="oadp-1-3-backing-csi-snapshots_oadp-backup-restore-csi-snapshots">
<title>Backing up persistent volumes with CSI snapshots</title>
<simpara>You can use the OADP Data Mover to back up Container Storage Interface (CSI) volume snapshots to a remote object store.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have installed the OADP Operator.</simpara>
</listitem>
<listitem>
<simpara>You have included the CSI plugin and enabled the node agent in the <literal>DataProtectionApplication</literal> custom resource (CR).</simpara>
</listitem>
<listitem>
<simpara>You have an application with persistent volumes running in a separate namespace.</simpara>
</listitem>
<listitem>
<simpara>You have added the <literal>metadata.labels.velero.io/csi-volumesnapshot-class: "true"</literal> key-value pair to the <literal>VolumeSnapshotClass</literal> CR.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the <literal>Backup</literal> object, as in the following example:</simpara>
<formalpara>
<title>Example <literal>Backup</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">kind: Backup
apiVersion: velero.io/v1
metadata:
  name: backup
  namespace: openshift-adp
spec:
  csiSnapshotTimeout: 10m0s
  defaultVolumesToFsBackup: false
  includedNamespaces:
  - mysql-persistent
  itemOperationTimeout: 4h0m0s
  snapshotMoveData: true <co xml:id="CO57-1"/>
  storageLocation: default
  ttl: 720h0m0s
  volumeSnapshotLocations:
  - dpa-sample-1
# ...</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO57-1">
<para>Set to <literal>true</literal> to enable movement of CSI snapshots to remote object storage.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f backup.yaml</programlisting>
<simpara>A <literal>DataUpload</literal> CR is created after the snapshot creation is complete.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the snapshot data is successfully transferred to the remote object store by monitoring the <literal>status.phase</literal> field of the <literal>DataUpload</literal> CR.  Possible values are <literal>In Progress</literal>, <literal>Completed</literal>, <literal>Failed</literal>, or <literal>Canceled</literal>. The object store is configured in the <literal>backupLocations</literal> stanza of the <literal>DataProtectionApplication</literal> CR.</simpara>
<itemizedlist>
<listitem>
<simpara>Run the following command to get a list of all <literal>DataUpload</literal> objects:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get datauploads -A</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE       NAME                  STATUS      STARTED   BYTES DONE   TOTAL BYTES   STORAGE LOCATION   AGE     NODE
openshift-adp   backup-test-1-sw76b   Completed   9m47s     108104082    108104082     dpa-sample-1       9m47s   ip-10-0-150-57.us-west-2.compute.internal
openshift-adp   mongo-block-7dtpf     Completed   14m       1073741824   1073741824    dpa-sample-1       14m     ip-10-0-150-57.us-west-2.compute.internal</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check the value of the <literal>status.phase</literal> field of the specific <literal>DataUpload</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get datauploads &lt;dataupload_name&gt; -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v2alpha1
kind: DataUpload
metadata:
  name: backup-test-1-sw76b
  namespace: openshift-adp
spec:
  backupStorageLocation: dpa-sample-1
  csiSnapshot:
    snapshotClass: ""
    storageClass: gp3-csi
    volumeSnapshot: velero-mysql-fq8sl
  operationTimeout: 10m0s
  snapshotType: CSI
  sourceNamespace: mysql-persistent
  sourcePVC: mysql
status:
  completionTimestamp: "2023-11-02T16:57:02Z"
  node: ip-10-0-150-57.us-west-2.compute.internal
  path: /host_pods/15116bac-cc01-4d9b-8ee7-609c3bef6bde/volumes/kubernetes.io~csi/pvc-eead8167-556b-461a-b3ec-441749e291c4/mount
  phase: Completed <co xml:id="CO58-1"/>
  progress:
    bytesDone: 108104082
    totalBytes: 108104082
  snapshotID: 8da1c5febf25225f4577ada2aeb9f899
  startTimestamp: "2023-11-02T16:56:22Z"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO58-1">
<para>Indicates that snapshot data is successfully transferred to the remote object store.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-1-3-restoring-csi-snapshots_oadp-backup-restore-csi-snapshots">
<title>Restoring CSI volume snapshots</title>
<simpara>You can restore a volume snapshot by creating a <literal>Restore</literal> CR.</simpara>
<note>
<simpara>You cannot restore Volsync backups from OADP 1.2 with the OAPD 1.3 built-in Data Mover. It is recommended to do a file system backup of all of your workloads with Restic prior to upgrading to OADP 1.3.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have an OADP <literal>Backup</literal> CR from which to restore the data.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a YAML file for the <literal>Restore</literal>  CR, as in the following example:</simpara>
<formalpara>
<title>Example <literal>Restore</literal> CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
  name: restore
  namespace: openshift-adp
spec:
  backupName: &lt;backup&gt;
# ...</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the manifest:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f restore.yaml</programlisting>
<simpara>A <literal>DataDownload</literal> CR is created when the restore starts.</simpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>You can monitor the status of the restore process by checking the <literal>status.phase</literal> field of the <literal>DataDownload</literal> CR. Possible values are <literal>In Progress</literal>, <literal>Completed</literal>, <literal>Failed</literal>, or <literal>Canceled</literal>.</simpara>
<itemizedlist>
<listitem>
<simpara>To get a list of all <literal>DataDownload</literal> objects, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get datadownloads -A</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAMESPACE       NAME                   STATUS      STARTED   BYTES DONE   TOTAL BYTES   STORAGE LOCATION   AGE     NODE
openshift-adp   restore-test-1-sk7lg   Completed   7m11s     108104082    108104082     dpa-sample-1       7m11s   ip-10-0-150-57.us-west-2.compute.internal</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Enter the following command to check the value of the <literal>status.phase</literal> field of the specific <literal>DataDownload</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get datadownloads &lt;datadownload_name&gt; -o yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v2alpha1
kind: DataDownload
metadata:
  name: restore-test-1-sk7lg
  namespace: openshift-adp
spec:
  backupStorageLocation: dpa-sample-1
  operationTimeout: 10m0s
  snapshotID: 8da1c5febf25225f4577ada2aeb9f899
  sourceNamespace: mysql-persistent
  targetVolume:
    namespace: mysql-persistent
    pv: ""
    pvc: mysql
status:
  completionTimestamp: "2023-11-02T17:01:24Z"
  node: ip-10-0-150-57.us-west-2.compute.internal
  phase: Completed <co xml:id="CO59-1"/>
  progress:
    bytesDone: 108104082
    totalBytes: 108104082
  startTimestamp: "2023-11-02T17:00:52Z"</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO59-1">
<para>Indicates that the CSI snapshot data is successfully restored.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="troubleshooting">
<title>Troubleshooting</title>

<simpara>You can debug Velero custom resources (CRs) by using the <link xlink:href="../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#oadp-debugging-oc-cli_oadp-troubleshooting">OpenShift CLI tool</link> or the <link xlink:href="../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#migration-debugging-velero-resources_oadp-troubleshooting">Velero CLI tool</link>. The Velero CLI tool provides more detailed logs and information.</simpara>
<simpara>You can check <link xlink:href="../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#oadp-installation-issues_oadp-troubleshooting">installation issues</link>, <link xlink:href="../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#oadp-backup-restore-cr-issues_oadp-troubleshooting">backup and restore CR issues</link>, and <link xlink:href="../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#oadp-restic-issues_oadp-troubleshooting">Restic issues</link>.</simpara>
<simpara>You can collect logs and CR information by using the <link xlink:href="../../backup_and_restore/application_backup_and_restore/troubleshooting.xml#migration-using-must-gather_oadp-troubleshooting"><literal>must-gather</literal> tool</link>.</simpara>
<simpara>You can obtain the Velero CLI tool by:</simpara>
<itemizedlist>
<listitem>
<simpara>Downloading the Velero CLI tool</simpara>
</listitem>
<listitem>
<simpara>Accessing the Velero binary in the Velero deployment in the cluster</simpara>
</listitem>
</itemizedlist>
<section xml:id="velero-obtaining-by-downloading_oadp-troubleshooting">
<title>Downloading the Velero CLI tool</title>
<simpara>You can download and install the Velero CLI tool by following the instructions on the <link xlink:href="https://velero.io/docs/v1.12/basic-install/#install-the-cli">Velero documentation page</link>.</simpara>
<simpara>The page includes instructions for:</simpara>
<itemizedlist>
<listitem>
<simpara>macOS by using Homebrew</simpara>
</listitem>
<listitem>
<simpara>GitHub</simpara>
</listitem>
<listitem>
<simpara>Windows by using Chocolatey</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to a Kubernetes cluster, v1.16 or later, with DNS and container networking enabled.</simpara>
</listitem>
<listitem>
<simpara>You have installed <literal>kubectl</literal> locally.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Open a browser and navigate to <link xlink:href="https://velero.io/docs/v1.12/basic-install/#install-the-cli">"Install the CLI" on the Velero website</link>.</simpara>
</listitem>
<listitem>
<simpara>Follow the appropriate procedure for macOS, GitHub, or Windows.</simpara>
</listitem>
<listitem>
<simpara>Download the Velero version appropriate for your version of OADP and OpenShift Container Platform.</simpara>
</listitem>
</orderedlist>
<section xml:id="velero-oadp-version-relationship_oadp-troubleshooting">
<title>OADP-Velero-OpenShift Container Platform version relationship</title>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">OADP version</entry>
<entry align="left" valign="top">Velero version</entry>
<entry align="left" valign="top">OpenShift Container Platform version</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>1.1.0</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.1</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.2</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.3</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.4</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.5</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.9 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.6</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.1.7</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.9]/">1.9</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.0</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.1</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.2</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.2.3</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.11/">1.11</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.11 and later</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>1.3.0</simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://velero.io/docs/v1.12/">1.12</link></simpara></entry>
<entry align="left" valign="top"><simpara>4.12 and later</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
</section>
<section xml:id="velero-obtaining-by-accessing-binary_oadp-troubleshooting">
<title>Accessing the Velero binary in the Velero deployment in the cluster</title>
<simpara>You can use a shell command to access the Velero binary in the Velero deployment in the cluster.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Your <literal>DataProtectionApplication</literal> custom resource has a status of <literal>Reconcile complete</literal>.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enter the following command to set the needed alias:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-debugging-oc-cli_oadp-troubleshooting">
<title>Debugging Velero resources with the OpenShift CLI tool</title>
<simpara>You can debug a failed backup or restore by checking Velero custom resources (CRs) and the <literal>Velero</literal> pod log with the OpenShift CLI tool.</simpara>
<bridgehead xml:id="oc-velero-cr_oadp-troubleshooting" renderas="sect4">Velero CRs</bridgehead>
<simpara>Use the <literal>oc describe</literal> command to retrieve a summary of warnings and errors associated with a <literal>Backup</literal> or <literal>Restore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe &lt;velero_cr&gt; &lt;cr_name&gt;</programlisting>
<bridgehead xml:id="oc-velero-pod-logs_oadp-troubleshooting" renderas="sect4">Velero pod logs</bridgehead>
<simpara>Use the <literal>oc logs</literal> command to retrieve the <literal>Velero</literal> pod logs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc logs pod/&lt;velero&gt;</programlisting>
<bridgehead xml:id="oc-velero-debug-logs_oadp-troubleshooting" renderas="sect4">Velero pod debug logs</bridgehead>
<simpara>You can specify the Velero log level in the <literal>DataProtectionApplication</literal> resource as shown in the following example.</simpara>
<note>
<simpara>This option is available starting from OADP 1.0.3.</simpara>
</note>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-sample
spec:
  configuration:
    velero:
      logLevel: warning</programlisting>
<simpara>The following <literal>logLevel</literal> values are available:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>trace</literal></simpara>
</listitem>
<listitem>
<simpara><literal>debug</literal></simpara>
</listitem>
<listitem>
<simpara><literal>info</literal></simpara>
</listitem>
<listitem>
<simpara><literal>warning</literal></simpara>
</listitem>
<listitem>
<simpara><literal>error</literal></simpara>
</listitem>
<listitem>
<simpara><literal>fatal</literal></simpara>
</listitem>
<listitem>
<simpara><literal>panic</literal></simpara>
</listitem>
</itemizedlist>
<simpara>It is recommended to use <literal>debug</literal> for most logs.</simpara>
</section>
<section xml:id="migration-debugging-velero-resources_oadp-troubleshooting">
<title>Debugging Velero resources with the Velero CLI tool</title>
<simpara>You can debug <literal>Backup</literal> and <literal>Restore</literal> custom resources (CRs) and retrieve logs with the Velero CLI tool.</simpara>
<simpara>The Velero CLI tool provides more detailed information than the OpenShift CLI tool.</simpara>
<bridgehead xml:id="velero-command-syntax_oadp-troubleshooting" renderas="sect4">Syntax</bridgehead>
<simpara>Use the <literal>oc exec</literal> command to run a Velero CLI command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  &lt;backup_restore_cr&gt; &lt;command&gt; &lt;cr_name&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  backup describe 0e44ae00-5dc3-11eb-9ca8-df7e5254778b-2d8ql</programlisting>
</para>
</formalpara>
<bridgehead xml:id="velero-help-option_oadp-troubleshooting" renderas="sect4">Help option</bridgehead>
<simpara>Use the <literal>velero --help</literal> option to list all Velero CLI commands:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  --help</programlisting>
<bridgehead xml:id="velero-describe-command_oadp-troubleshooting" renderas="sect4">Describe command</bridgehead>
<simpara>Use the <literal>velero describe</literal> command to retrieve a summary of warnings and errors associated with a <literal>Backup</literal> or <literal>Restore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  &lt;backup_restore_cr&gt; describe &lt;cr_name&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  backup describe 0e44ae00-5dc3-11eb-9ca8-df7e5254778b-2d8ql</programlisting>
</para>
</formalpara>
<bridgehead xml:id="velero-logs-command_oadp-troubleshooting" renderas="sect4">Logs command</bridgehead>
<simpara>Use the <literal>velero logs</literal> command to retrieve the logs of a <literal>Backup</literal> or <literal>Restore</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  &lt;backup_restore_cr&gt; logs &lt;cr_name&gt;</programlisting>
<formalpara>
<title>Example</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-adp exec deployment/velero -c velero -- ./velero \
  restore logs ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf</programlisting>
</para>
</formalpara>
</section>
<section xml:id="oadp-pod-crash-resource-request">
<title>Pods crash or restart due to lack of memory or CPU</title>
<simpara>If a Velero or Restic pod crashes due to a lack of memory or CPU, you can set specific resource requests for either of those resources.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/application_backup_and_restore/installing/about-installing-oadp.xml#oadp-velero-cpu-memory-requirements_about-installing-oadp">CPU and memory requirements</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-pod-crash-resource-request-velero_oadp-troubleshooting">
<title>Setting resource requests for a Velero pod</title>
<simpara>You can use the <literal>configuration.velero.podConfig.resourceAllocations</literal> specification field in the <literal>oadp_v1alpha1_dpa.yaml</literal> file to set specific resource requests for a <literal>Velero</literal> pod.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the <literal>cpu</literal> and <literal>memory</literal> resource requests in the YAML file:</simpara>
<formalpara>
<title>Example Velero file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
configuration:
  velero:
    podConfig:
      resourceAllocations: <co xml:id="CO60-1"/>
        requests:
          cpu: 200m
          memory: 256Mi</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO60-1">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-pod-crash-resource-request-retics_oadp-troubleshooting">
<title>Setting resource requests for a Restic pod</title>
<simpara>You can use the <literal>configuration.restic.podConfig.resourceAllocations</literal> specification field to set specific resource requests for a <literal>Restic</literal> pod.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Set the <literal>cpu</literal> and <literal>memory</literal> resource requests in the YAML file:</simpara>
<formalpara>
<title>Example Restic file</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
configuration:
  restic:
    podConfig:
      resourceAllocations: <co xml:id="CO61-1"/>
        requests:
          cpu: 1000m
          memory: 16Gi</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO61-1">
<para>The <literal>resourceAllocations</literal> listed are for average usage.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<important>
<simpara>The values for the resource request fields must follow the same format as Kubernetes resource requirements.
Also, if you do not specify <literal>configuration.velero.podConfig.resourceAllocations</literal> or <literal>configuration.restic.podConfig.resourceAllocations</literal>, the default <literal>resources</literal> specification for a Velero pod or a Restic pod is as follows:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">requests:
  cpu: 500m
  memory: 128Mi</programlisting>
</important>
</section>
</section>
<section xml:id="issues-with-velero-and-admission-workbooks">
<title>Issues with Velero and admission webhooks</title>
<simpara>Velero has limited abilities to resolve admission webhook issues during a restore. If you have workloads with admission webhooks, you might need to use an additional Velero plugin or make changes to how you restore the workload.</simpara>
<simpara>Typically, workloads with admission webhooks require you to create a resource of a specific kind first. This is especially true if your workload has child resources because admission webhooks typically block child resources.</simpara>
<simpara>For example, creating or restoring a top-level object such as <literal>service.serving.knative.dev</literal> typically creates child resources automatically. If you do this first, you will not need to use Velero to create and restore these resources. This avoids the problem of child resources being blocked by an admission webhook that Velero might use.</simpara>
<section xml:id="velero-restore-workarounds-for-workloads-with-admission-webhooks">
<title>Restoring workarounds for Velero backups that use admission webhooks</title>
<simpara>This section describes the additional steps required to restore resources for several types of Velero backups that use admission webhooks.</simpara>
<section xml:id="migration-debugging-velero-admission-webhooks-knative_oadp-troubleshooting">
<title>Restoring Knative resources</title>
<simpara>You might encounter problems using Velero to back up Knative resources that use admission webhooks.</simpara>
<simpara>You can avoid such problems by restoring the top level <literal>Service</literal> resource first whenever you back up and restore Knative resources that use admission webhooks.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Restore the top level <literal>service.serving.knavtive.dev Service</literal> resource:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ velero restore &lt;restore_name&gt; \
  --from-backup=&lt;backup_name&gt; --include-resources \
  service.serving.knavtive.dev</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="migration-debugging-velero-admission-webhooks-ibm-appconnect_oadp-troubleshooting">
<title>Restoring IBM AppConnect resources</title>
<simpara>If you experience issues when you use Velero to a restore an IBM&#174; AppConnect resource that has an admission webhook, you can run the checks in this procedure.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check if you have any mutating admission plugins of <literal>kind: MutatingWebhookConfiguration</literal> in the cluster:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get mutatingwebhookconfigurations</programlisting>
</listitem>
<listitem>
<simpara>Examine the YAML file of each <literal>kind: MutatingWebhookConfiguration</literal> to ensure that none of its rules block creation of the objects that are experiencing issues. For more information, see <link xlink:href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#rulewithoperations-v1-admissionregistration-k8s-io">the official Kubernetes documentation</link>.</simpara>
</listitem>
<listitem>
<simpara>Check that any <literal>spec.version</literal> in <literal>type: Configuration.appconnect.ibm.com/v1beta1</literal> used at backup time is supported by the installed Operator.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-plugins-receiving-eof-message_oadp-troubleshooting">
<title>Velero plugins returning "received EOF, stopping recv loop" message</title>
<note>
<simpara>Velero plugins are started as separate processes. After the Velero operation has completed, either successfully or not, they exit. Receiving a <literal>received EOF, stopping recv loop</literal> message in the debug logs indicates that a plugin operation has completed. It does not mean that an error has occurred.</simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../architecture/admission-plug-ins.xml">Admission plugins</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../architecture/admission-plug-ins.xml#admission-webhooks-about_admission-plug-ins">Webhook admission plugins</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../architecture/admission-plug-ins.xml#admission-webhook-types_admission-plug-ins">Types of webhook admission plugins</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-installation-issues_oadp-troubleshooting">
<title>Installation issues</title>
<simpara>You might encounter issues caused by using invalid directories or incorrect credentials when you install the Data Protection Application.</simpara>
<section xml:id="oadp-backup-location-contains-invalid-directories_oadp-troubleshooting">
<title>Backup storage contains invalid directories</title>
<simpara>The <literal>Velero</literal> pod log displays the error message, <literal>Backup storage contains invalid top-level directories</literal>.</simpara>
<formalpara>
<title>Cause</title>
<para>The object storage contains top-level directories that are not Velero directories.</para>
</formalpara>
<formalpara>
<title>Solution</title>
<para>If the object storage is not dedicated to Velero, you must specify a prefix for the bucket by setting the <literal>spec.backupLocations.velero.objectStorage.prefix</literal> parameter in the <literal>DataProtectionApplication</literal> manifest.</para>
</formalpara>
</section>
<section xml:id="oadp-incorrect-aws-credentials_oadp-troubleshooting">
<title>Incorrect AWS credentials</title>
<simpara>The <literal>oadp-aws-registry</literal> pod log displays the error message, <literal>InvalidAccessKeyId: The AWS Access Key Id you provided does not exist in our records.</literal></simpara>
<simpara>The <literal>Velero</literal> pod log displays the error message, <literal>NoCredentialProviders: no valid providers in chain</literal>.</simpara>
<formalpara>
<title>Cause</title>
<para>The <literal>credentials-velero</literal> file used to create the <literal>Secret</literal> object is incorrectly formatted.</para>
</formalpara>
<formalpara>
<title>Solution</title>
<para>Ensure that the <literal>credentials-velero</literal> file is correctly formatted, as in the following example:</para>
</formalpara>
<formalpara>
<title>Example <literal>credentials-velero</literal> file</title>
<para>
<screen>[default] <co xml:id="CO62-1"/>
aws_access_key_id=AKIAIOSFODNN7EXAMPLE <co xml:id="CO62-2"/>
aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO62-1">
<para>AWS default profile.</para>
</callout>
<callout arearefs="CO62-2">
<para>Do not enclose the values with quotation marks (<literal>"</literal>, <literal>'</literal>).</para>
</callout>
</calloutlist>
</section>
</section>
<section xml:id="oadp-operator-issues_oadp-troubleshooting">
<title>OADP Operator issues</title>
<simpara>The OpenShift API for Data Protection (OADP) Operator might encounter issues caused by problems it is not able to resolve.</simpara>
<section xml:id="oadp-operator-fails-silently_oadp-troubleshooting">
<title>OADP Operator fails silently</title>
<simpara>The S3 buckets of an OADP Operator might be empty, but when you run the command <literal>oc get po -n &lt;OADP_Operator_namespace&gt;</literal>, you see that the Operator has a status of <literal>Running</literal>.  In such a case, the Operator is said to have <emphasis>failed silently</emphasis> because it incorrectly reports that it is running.</simpara>
<formalpara>
<title>Cause</title>
<para>The problem is caused when cloud credentials provide insufficient permissions.</para>
</formalpara>
<formalpara>
<title>Solution</title>
<para>Retrieve a list of backup storage locations (BSLs) and check the manifest of each BSL for credential issues.</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Run one of the following commands to retrieve a list of BSLs:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Using the OpenShift CLI:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupstoragelocation -A</programlisting>
</listitem>
<listitem>
<simpara>Using the Velero CLI:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ velero backup-location get -n &lt;OADP_Operator_namespace&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Using the list of BSLs, run the following command to display the manifest of each BSL, and examine each manifest for an error.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get backupstoragelocation -n &lt;namespace&gt; -o yaml</programlisting>
</listitem>
</orderedlist>
<formalpara>
<title>Example result</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
items:
- apiVersion: velero.io/v1
  kind: BackupStorageLocation
  metadata:
    creationTimestamp: "2023-11-03T19:49:04Z"
    generation: 9703
    name: example-dpa-1
    namespace: openshift-adp-operator
    ownerReferences:
    - apiVersion: oadp.openshift.io/v1alpha1
      blockOwnerDeletion: true
      controller: true
      kind: DataProtectionApplication
      name: example-dpa
      uid: 0beeeaff-0287-4f32-bcb1-2e3c921b6e82
    resourceVersion: "24273698"
    uid: ba37cd15-cf17-4f7d-bf03-8af8655cea83
  spec:
    config:
      enableSharedConfig: "true"
      region: us-west-2
    credential:
      key: credentials
      name: cloud-credentials
    default: true
    objectStorage:
      bucket: example-oadp-operator
      prefix: example
    provider: aws
  status:
    lastValidationTime: "2023-11-10T22:06:46Z"
    message: "BackupStorageLocation \"example-dpa-1\" is unavailable: rpc
      error: code = Unknown desc = WebIdentityErr: failed to retrieve credentials\ncaused
      by: AccessDenied: Not authorized to perform sts:AssumeRoleWithWebIdentity\n\tstatus
      code: 403, request id: d3f2e099-70a0-467b-997e-ff62345e3b54"
    phase: Unavailable
kind: List
metadata:
  resourceVersion: ""</programlisting>
</para>
</formalpara>
</section>
</section>
<section xml:id="oadp-timeouts_oadp-troubleshooting">
<title>OADP timeouts</title>
<simpara>Extending a timeout allows complex or resource-intensive processes to complete successfully without premature termination. This configuration can reduce the likelihood of errors, retries, or failures.</simpara>
<simpara>Ensure that you balance timeout extensions in a logical manner so that you do not configure excessively long timeouts that might hide underlying issues in the process. Carefully consider and monitor an appropriate timeout value that meets the needs of the process and the overall system performance.</simpara>
<simpara>The following are various OADP timeouts, with instructions of how and when to implement these parameters:</simpara>
<section xml:id="restic-timeout_oadp-troubleshooting">
<title>Restic timeout</title>
<simpara><literal>timeout</literal> defines the Restic timeout. The default value is <literal>1h</literal>.</simpara>
<simpara>Use the Restic <literal>timeout</literal> for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>For Restic backups with total PV data usage that is greater than 500GB.</simpara>
</listitem>
<listitem>
<simpara>If backups are timing out with the following error:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">level=error msg="Error backing up item" backup=velero/monitoring error="timed out waiting for all PodVolumeBackups to complete"</programlisting>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.restic.timeout</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: &lt;dpa_name&gt;
spec:
  configuration:
    restic:
      timeout: 1h
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="velero-timeout_oadp-troubleshooting">
<title>Velero resource timeout</title>
<simpara><literal>resourceTimeout</literal> defines how long to wait for several Velero resources before timeout occurs, such as Velero custom resource definition (CRD) availability, <literal>volumeSnapshot</literal> deletion, and repository availability. The default is <literal>10m</literal>.</simpara>
<simpara>Use the <literal>resourceTimeout</literal> for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>For backups with total PV data usage that is greater than 1TB. This parameter is used as a timeout value when Velero tries to clean up or delete the Container Storage Interface (CSI) snapshots, before marking the backup as complete.</simpara>
<itemizedlist>
<listitem>
<simpara>A sub-task of this cleanup tries to patch VSC and this timeout can be used for that task.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>To create or ensure a backup repository is ready for filesystem based backups for Restic or Kopia.</simpara>
</listitem>
<listitem>
<simpara>To check if the Velero CRD is available in the cluster before restoring the custom resource (CR) or resource from the backup.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.resourceTimeout</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: &lt;dpa_name&gt;
spec:
  configuration:
    velero:
      resourceTimeout: 10m
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="datamover-timeout_oadp-troubleshooting">
<title>Data Mover timeout</title>
<simpara><literal>timeout</literal> is a user-supplied timeout to complete <literal>VolumeSnapshotBackup</literal> and <literal>VolumeSnapshotRestore</literal>. The default value is <literal>10m</literal>.</simpara>
<simpara>Use the Data Mover <literal>timeout</literal> for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>If creation of <literal>VolumeSnapshotBackups</literal> (VSBs) and <literal>VolumeSnapshotRestores</literal> (VSRs), times out after 10 minutes.</simpara>
</listitem>
<listitem>
<simpara>For large scale environments with total PV data usage that is greater than 500GB. Set the timeout for <literal>1h</literal>.</simpara>
</listitem>
<listitem>
<simpara>With the <literal>VolumeSnapshotMover</literal> (VSM) plugin.</simpara>
</listitem>
<listitem>
<simpara>Only with OADP 1.1.x.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.features.dataMover.timeout</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: &lt;dpa_name&gt;
spec:
  features:
    dataMover:
      timeout: 10m
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="csisnapshot-timeout_oadp-troubleshooting">
<title>CSI snapshot timeout</title>
<simpara><literal>CSISnapshotTimeout</literal> specifies the time during creation to wait until  the <literal>CSI VolumeSnapshot</literal> status becomes <literal>ReadyToUse</literal>, before returning error as timeout. The default value is <literal>10m</literal>.</simpara>
<simpara>Use the <literal>CSISnapshotTimeout</literal>  for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>With the CSI plugin.</simpara>
</listitem>
<listitem>
<simpara>For very large storage volumes that may take longer than 10 minutes to snapshot. Adjust this timeout if timeouts are found in the logs.</simpara>
</listitem>
</itemizedlist>
<note>
<simpara>Typically, the default value for <literal>CSISnapshotTimeout</literal> does not require adjustment, because the default setting can accommodate large storage volumes.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.csiSnapshotTimeout</literal> block of the <literal>Backup</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
 name: &lt;backup_name&gt;
spec:
 csiSnapshotTimeout: 10m
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="velero-default-item-operation-timeout_oadp-troubleshooting">
<title>Velero default item operation timeout</title>
<simpara><literal>defaultItemOperationTimeout</literal> defines how long to wait on asynchronous <literal>BackupItemActions</literal> and <literal>RestoreItemActions</literal> to complete before timing out. The default value is <literal>1h</literal>.</simpara>
<simpara>Use the <literal>defaultItemOperationTimeout</literal> for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>Only with Data Mover 1.2.x.</simpara>
</listitem>
<listitem>
<simpara>To specify the amount of time a particular backup or restore should wait for the Asynchronous actions to complete. In the context of OADP features, this value is used for the Asynchronous actions involved in the Container Storage Interface (CSI) Data Mover feature.</simpara>
</listitem>
<listitem>
<simpara>When <literal>defaultItemOperationTimeout</literal> is defined in the Data Protection Application (DPA)  using the <literal>defaultItemOperationTimeout</literal>, it applies to both backup and restore operations. You can use <literal>itemOperationTimeout</literal> to define only the backup or only the restore of those CRs, as described in the following "Item operation timeout - restore", and "Item operation timeout - backup" sections.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>spec.configuration.velero.defaultItemOperationTimeout</literal> block of the <literal>DataProtectionApplication</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: &lt;dpa_name&gt;
spec:
  configuration:
    velero:
      defaultItemOperationTimeout: 1h
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="item-operation-timeout-restore_oadp-troubleshooting">
<title>Item operation timeout - restore</title>
<simpara><literal>ItemOperationTimeout</literal> specifies the time that is used to wait for <literal>RestoreItemAction</literal> operations. The default value is <literal>1h</literal>.</simpara>
<simpara>Use the restore <literal>ItemOperationTimeout</literal> for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>Only with Data Mover 1.2.x.</simpara>
</listitem>
<listitem>
<simpara>For Data Mover uploads and downloads to or from the <literal>BackupStorageLocation</literal>. If the restore action is not completed when the timeout is reached, it will be marked as failed. If Data Mover operations are failing due to timeout issues, because of large storage volume sizes, then this timeout setting may need to be increased.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>Restore.spec.itemOperationTimeout</literal> block of the <literal>Restore</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Restore
metadata:
 name: &lt;restore_name&gt;
spec:
 itemOperationTimeout: 1h
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="item-operation-timeout-backup_oadp-troubleshooting">
<title>Item operation timeout - backup</title>
<simpara><literal>ItemOperationTimeout</literal> specifies the time used to wait for asynchronous
<literal>BackupItemAction</literal> operations. The default value is <literal>1h</literal>.</simpara>
<simpara>Use the backup <literal>ItemOperationTimeout</literal> for the following scenarios:</simpara>
<itemizedlist>
<listitem>
<simpara>Only with Data Mover 1.2.x.</simpara>
</listitem>
<listitem>
<simpara>For Data Mover uploads and downloads to or from the <literal>BackupStorageLocation</literal>. If the backup action is not completed when the timeout is reached, it will be marked as failed. If Data Mover operations are failing due to timeout issues, because of large storage volume sizes, then this timeout setting may need to be increased.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Edit the values in the <literal>Backup.spec.itemOperationTimeout</literal> block of the <literal>Backup</literal> CR manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: velero.io/v1
kind: Backup
metadata:
 name: &lt;backup_name&gt;
spec:
 itemOperationTimeout: 1h
# ...</programlisting>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-backup-restore-cr-issues_oadp-troubleshooting">
<title>Backup and Restore CR issues</title>
<simpara>You might encounter these common issues with <literal>Backup</literal> and <literal>Restore</literal> custom resources (CRs).</simpara>
<section xml:id="backup-cannot-retrieve-volume_oadp-troubleshooting">
<title>Backup CR cannot retrieve volume</title>
<simpara>The <literal>Backup</literal> CR displays the error message, <literal>InvalidVolume.NotFound: The volume ‘vol-xxxx’ does not exist</literal>.</simpara>
<formalpara>
<title>Cause</title>
<para>The persistent volume (PV) and the snapshot locations are in different regions.</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Solution</title>
<listitem>
<simpara>Edit the value of the <literal>spec.snapshotLocations.velero.config.region</literal> key in the <literal>DataProtectionApplication</literal> manifest so that the snapshot location is in the same region as the PV.</simpara>
</listitem>
<listitem>
<simpara>Create a new <literal>Backup</literal> CR.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="backup-cr-remains-in-progress_oadp-troubleshooting">
<title>Backup CR status remains in progress</title>
<simpara>The status of a <literal>Backup</literal> CR remains in the <literal>InProgress</literal> phase and does not complete.</simpara>
<formalpara>
<title>Cause</title>
<para>If a backup is interrupted, it cannot be resumed.</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Solution</title>
<listitem>
<simpara>Retrieve the details of the <literal>Backup</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  backup describe &lt;backup&gt;</programlisting>
</listitem>
<listitem>
<simpara>Delete the <literal>Backup</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete backup &lt;backup&gt; -n openshift-adp</programlisting>
<simpara>You do not need to clean up the backup location because a <literal>Backup</literal> CR in progress has not uploaded  files to object storage.</simpara>
</listitem>
<listitem>
<simpara>Create a new <literal>Backup</literal> CR.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="backup-cr-remains-partiallyfailed_oadp-troubleshooting">
<title>Backup CR status remains in PartiallyFailed</title>
<simpara>The status of a <literal>Backup</literal> CR without Restic in use remains in the <literal>PartiallyFailed</literal> phase and does not complete. A snapshot of the affiliated PVC is not created.</simpara>
<formalpara>
<title>Cause</title>
<para>If the backup is created based on the CSI snapshot class, but the label is missing, CSI snapshot plugin fails to create a snapshot. As a result, the <literal>Velero</literal> pod logs an error similar to the following:</para>
</formalpara>
<programlisting language="text" linenumbering="unnumbered">time="2023-02-17T16:33:13Z" level=error msg="Error backing up item" backup=openshift-adp/user1-backup-check5 error="error executing custom action (groupResource=persistentvolumeclaims, namespace=busy1, name=pvc1-user1): rpc error: code = Unknown desc = failed to get volumesnapshotclass for storageclass ocs-storagecluster-ceph-rbd: failed to get volumesnapshotclass for provisioner openshift-storage.rbd.csi.ceph.com, ensure that the desired volumesnapshot class has the velero.io/csi-volumesnapshot-class label" logSource="/remote-source/velero/app/pkg/backup/backup.go:417" name=busybox-79799557b5-vprq</programlisting>
<orderedlist numeration="arabic">
<title>Solution</title>
<listitem>
<simpara>Delete the <literal>Backup</literal> CR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete backup &lt;backup&gt; -n openshift-adp</programlisting>
</listitem>
<listitem>
<simpara>If required, clean up the stored data on the <literal>BackupStorageLocation</literal> to free up space.</simpara>
</listitem>
<listitem>
<simpara>Apply label <literal>velero.io/csi-volumesnapshot-class=true</literal> to the <literal>VolumeSnapshotClass</literal> object:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc label volumesnapshotclass/&lt;snapclass_name&gt; velero.io/csi-volumesnapshot-class=true</programlisting>
</listitem>
<listitem>
<simpara>Create a new <literal>Backup</literal> CR.</simpara>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="oadp-restic-issues_oadp-troubleshooting">
<title>Restic issues</title>
<simpara>You might encounter these issues when you back up applications with Restic.</simpara>
<section xml:id="restic-permission-error-nfs-root-squash-enabled_oadp-troubleshooting">
<title>Restic permission error for NFS data volumes with root_squash enabled</title>
<simpara>The <literal>Restic</literal> pod log displays the error message: <literal>controller=pod-volume-backup error="fork/exec/usr/bin/restic: permission denied"</literal>.</simpara>
<formalpara>
<title>Cause</title>
<para>If your NFS data volumes have <literal>root_squash</literal> enabled, <literal>Restic</literal> maps to <literal>nfsnobody</literal> and does not have permission to create backups.</para>
</formalpara>
<formalpara>
<title>Solution</title>
<para>You can resolve this issue by creating a supplemental group for <literal>Restic</literal> and adding the group ID to the <literal>DataProtectionApplication</literal> manifest:</para>
</formalpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Create a supplemental group for <literal>Restic</literal> on the NFS data volume.</simpara>
</listitem>
<listitem>
<simpara>Set the <literal>setgid</literal> bit on the NFS directories so that group ownership is inherited.</simpara>
</listitem>
<listitem>
<simpara>Add the <literal>spec.configuration.restic.supplementalGroups</literal> parameter and the group ID to the <literal>DataProtectionApplication</literal> manifest, as in the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  configuration:
    restic:
      enable: true
      supplementalGroups:
      - &lt;group_id&gt; <co xml:id="CO63-1"/></programlisting>
<calloutlist>
<callout arearefs="CO63-1">
<para>Specify the supplemental group ID.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Wait for the <literal>Restic</literal> pods to restart so that the changes are applied.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="restic-backup-cannot-be-recreated-after-s3-bucket-emptied_oadp-troubleshooting">
<title>Restic Backup CR cannot be recreated after bucket is emptied</title>
<simpara>If you create a Restic <literal>Backup</literal> CR for a namespace, empty the object storage bucket, and then recreate the <literal>Backup</literal> CR for the same namespace, the recreated <literal>Backup</literal> CR fails.</simpara>
<simpara>The <literal>velero</literal> pod log displays the following error message: <literal>stderr=Fatal: unable to open config file: Stat: The specified key does not exist.\nIs there a repository at the following location?</literal>.</simpara>
<formalpara>
<title>Cause</title>
<para>Velero does not recreate or update the Restic repository from the <literal>ResticRepository</literal> manifest if the Restic directories are deleted from object storage. See <link xlink:href="https://github.com/vmware-tanzu/velero/issues/4421">Velero issue 4421</link> for more information.</para>
</formalpara>
<itemizedlist>
<title>Solution</title>
<listitem>
<simpara>Remove the related Restic repository from the namespace by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete resticrepository openshift-adp &lt;name_of_the_restic_repository&gt;</programlisting>
<simpara>In the following error log, <literal>mysql-persistent</literal> is the problematic Restic repository. The name of the repository appears in italics for clarity.</simpara>
<programlisting language="text" linenumbering="unnumbered"> time="2021-12-29T18:29:14Z" level=info msg="1 errors
 encountered backup up item" backup=velero/backup65
 logSource="pkg/backup/backup.go:431" name=mysql-7d99fc949-qbkds
 time="2021-12-29T18:29:14Z" level=error msg="Error backing up item"
 backup=velero/backup65 error="pod volume backup failed: error running
 restic backup, stderr=Fatal: unable to open config file: Stat: The
 specified key does not exist.\nIs there a repository at the following
 location?\ns3:http://minio-minio.apps.mayap-oadp-
 veleo-1234.qe.devcluster.openshift.com/mayapvelerooadp2/velero1/
 restic/<emphasis>mysql-persistent</emphasis>\n: exit status 1" error.file="/remote-source/
 src/github.com/vmware-tanzu/velero/pkg/restic/backupper.go:184"
 error.function="github.com/vmware-tanzu/velero/
 pkg/restic.(*backupper).BackupPodVolumes"
 logSource="pkg/backup/backup.go:435" name=mysql-7d99fc949-qbkds</programlisting>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-restic-restore-failing-psa-policy_oadp-troubleshooting">
<title>Restic restore partially failing on OCP 4.14 due to changed PSA policy</title>
<simpara>OpenShift Container Platform 4.14 enforces a Pod Security Admission (PSA) policy that can hinder the readiness of pods during a Restic restore process. </simpara>
<simpara>If a <literal>SecurityContextConstraints</literal> (SCC) resource is not found when a pod is created, and the PSA policy on the pod is not set up to meet the required standards, pod admission is denied. </simpara>
<simpara>This issue arises due to the resource restore order of Velero.</simpara>
<formalpara>
<title>Sample error</title>
<para>
<programlisting language="text" linenumbering="unnumbered">\"level=error\" in line#2273: time=\"2023-06-12T06:50:04Z\"
level=error msg=\"error restoring mysql-869f9f44f6-tp5lv: pods\\\
"mysql-869f9f44f6-tp5lv\\\" is forbidden: violates PodSecurity\\\
"restricted:v1.24\\\": privil eged (container \\\"mysql\\\
" must not set securityContext.privileged=true),
allowPrivilegeEscalation != false (containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.capabilities.drop=[\\\"ALL\\\"]), seccompProfile (pod or containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.seccompProfile.type to \\\
"RuntimeDefault\\\" or \\\"Localhost\\\")\" logSource=\"/remote-source/velero/app/pkg/restore/restore.go:1388\" restore=openshift-adp/todolist-backup-0780518c-08ed-11ee-805c-0a580a80e92c\n
velero container contains \"level=error\" in line#2447: time=\"2023-06-12T06:50:05Z\"
level=error msg=\"Namespace todolist-mariadb,
resource restore error: error restoring pods/todolist-mariadb/mysql-869f9f44f6-tp5lv: pods \\\
"mysql-869f9f44f6-tp5lv\\\" is forbidden: violates PodSecurity \\\"restricted:v1.24\\\": privileged (container \\\
"mysql\\\" must not set securityContext.privileged=true),
allowPrivilegeEscalation != false (containers \\\
"restic-wait\\\",\\\"mysql\\\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.capabilities.drop=[\\\"ALL\\\"]), seccompProfile (pod or containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.seccompProfile.type to \\\
"RuntimeDefault\\\" or \\\"Localhost\\\")\"
logSource=\"/remote-source/velero/app/pkg/controller/restore_controller.go:510\"
restore=openshift-adp/todolist-backup-0780518c-08ed-11ee-805c-0a580a80e92c\n]",</programlisting>
</para>
</formalpara>
<orderedlist numeration="arabic">
<title>Solution</title>
<listitem>
<simpara>In your DPA custom resource (CR), check or set the <literal>restore-resource-priorities</literal> field on the Velero server to ensure that <literal>securitycontextconstraints</literal> is listed in order before <literal>pods</literal> in the list of resources:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get dpa -o yaml</programlisting>
<formalpara>
<title>Example DPA CR</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered"># ...
configuration:
  restic:
    enable: true
  velero:
    args:
      restore-resource-priorities: 'securitycontextconstraints,customresourcedefinitions,namespaces,storageclasses,volumesnapshotclass.snapshot.storage.k8s.io,volumesnapshotcontents.snapshot.storage.k8s.io,volumesnapshots.snapshot.storage.k8s.io,datauploads.velero.io,persistentvolumes,persistentvolumeclaims,serviceaccounts,secrets,configmaps,limitranges,pods,replicasets.apps,clusterclasses.cluster.x-k8s.io,endpoints,services,-,clusterbootstraps.run.tanzu.vmware.com,clusters.cluster.x-k8s.io,clusterresourcesets.addons.cluster.x-k8s.io' <co xml:id="CO64-1"/>
    defaultPlugins:
    - gcp
    - openshift</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO64-1">
<para>If you have an existing restore resource priority list, ensure you combine that existing list with the complete list.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Ensure that the security standards for the application pods are aligned, as provided in <link xlink:href="https://access.redhat.com/solutions/7002730">Fixing PodSecurity Admission warnings for deployments</link>, to prevent deployment warnings. If the application is not aligned with security standards, an error can occur regardless of the SCC. </simpara>
</listitem>
</orderedlist>
<note>
<simpara>This solution is temporary, and ongoing discussions are in progress to address it. </simpara>
</note>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://access.redhat.com/solutions/7002730">Fixing PodSecurity Admission warnings for deployments</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="migration-using-must-gather_oadp-troubleshooting">
<title>Using the must-gather tool</title>
<simpara>You can collect logs, metrics, and information about OADP custom resources by using the <literal>must-gather</literal> tool.</simpara>
<simpara>The <literal>must-gather</literal> data must be attached to all customer cases.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You must be logged in to the OpenShift Container Platform cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You must have the OpenShift CLI (<literal>oc</literal>) installed.</simpara>
</listitem>
<listitem>
<simpara>You must use Red Hat Enterprise Linux (RHEL) 8.x with OADP 1.2.</simpara>
</listitem>
<listitem>
<simpara>You must use Red Hat Enterprise Linux (RHEL) 9.x with OADP 1.3.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Navigate to the directory where you want to store the <literal>must-gather</literal> data.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal>oc adm must-gather</literal> command for one of the following data collection options:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>For OADP 1.2, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel8:v1.2</programlisting>
</listitem>
<listitem>
<simpara>For OADP 1.3, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3</programlisting>
<simpara>The data is saved as <literal>must-gather/must-gather.tar.gz</literal>. You can upload this file to a support case on the <link xlink:href="https://access.redhat.com/">Red Hat Customer Portal</link>.</simpara>
</listitem>
<listitem>
<simpara>For OADP 1.2, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel8:v1.2 -- /usr/bin/gather_metrics_dump</programlisting>
</listitem>
<listitem>
<simpara>For OADP 1.3, use the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3 -- /usr/bin/gather_metrics_dump</programlisting>
<simpara>This operation can take a long time. The data is saved as <literal>must-gather/metrics/prom_data.tar.gz</literal>.</simpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<section xml:id="migration-combining-must-gather_oadp-troubleshooting">
<title>Combining options when using the must-gather tool</title>
<simpara>Currently, it is not possible to combine must-gather scripts, for example specifying a timeout threshold while permitting insecure TLS connections. In some situations, you can get around this limitation by setting up internal variables on the must-gather command line, such as the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3 -- skip_tls=true /usr/bin/gather_with_timeout &lt;timeout_value_in_seconds&gt;</programlisting>
<simpara>In this example, set the <literal>skip_tls</literal> variable before running the <literal>gather_with_timeout</literal> script. The result is a combination of <literal>gather_with_timeout</literal> and <literal>gather_without_tls</literal>.</simpara>
<simpara>The only other variables that you can specify this way are the following:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>logs_since</literal>, with a default value of <literal>72h</literal></simpara>
</listitem>
<listitem>
<simpara><literal>request_timeout</literal>, with a default value of <literal>0s</literal></simpara>
</listitem>
</itemizedlist>
<simpara>If <literal>DataProtectionApplication</literal> custom resource (CR) is configured with <literal>s3Url</literal> and <literal>insecureSkipTLS: true</literal>, the CR does not collect the necessary logs because of a missing CA certificate. To collect those logs, run the <literal>must-gather</literal> command with the following option:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm must-gather --image=registry.redhat.io/oadp/oadp-mustgather-rhel9:v1.3 -- /usr/bin/gather_without_tls true</programlisting>
</section>
</section>
<section xml:id="oadp-monitoring_oadp-troubleshooting">
<title>OADP Monitoring</title>
<simpara>The OpenShift Container Platform provides a monitoring stack that allows users and administrators to effectively monitor and manage their clusters, as well as monitor and analyze the workload performance of user applications and services running on the clusters, including receiving alerts if an event occurs.</simpara>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../monitoring/monitoring-overview.xml#about-openshift-monitoring">Monitoring stack</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-monitoring-setup-monitor_oadp-troubleshooting">
<title>OADP monitoring setup</title>
<simpara>The OADP Operator leverages an OpenShift User Workload Monitoring provided by the OpenShift Monitoring Stack for retrieving metrics from the Velero service endpoint. The monitoring stack allows creating user-defined Alerting Rules or querying metrics by using the OpenShift Metrics query front end.</simpara>
<simpara>With enabled User Workload Monitoring, it is possible to configure and use any Prometheus-compatible third-party UI, such as Grafana, to visualize Velero metrics.</simpara>
<simpara>Monitoring metrics requires enabling monitoring for the user-defined projects and creating a <literal>ServiceMonitor</literal> resource to scrape those metrics from the already enabled OADP service endpoint that resides in the <literal>openshift-adp</literal> namespace.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to an OpenShift Container Platform cluster using an account with <literal>cluster-admin</literal> permissions.</simpara>
</listitem>
<listitem>
<simpara>You have created a cluster monitoring config map.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Edit the <literal>cluster-monitoring-config</literal> <literal>ConfigMap</literal> object in the <literal>openshift-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit configmap cluster-monitoring-config -n openshift-monitoring</programlisting>
</listitem>
<listitem>
<simpara>Add or enable the <literal>enableUserWorkload</literal> option in the <literal>data</literal> section&#8217;s <literal>config.yaml</literal> field:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true <co xml:id="CO65-1"/>
kind: ConfigMap
metadata:
# ...</programlisting>
<calloutlist>
<callout arearefs="CO65-1">
<para>Add this option or set to <literal>true</literal></para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Wait a short period of time to verify the User Workload Monitoring Setup by checking if the following components are up and running in the <literal>openshift-user-workload-monitoring</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pods -n openshift-user-workload-monitoring</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-6844b4b99c-b57j9   2/2     Running   0          43s
prometheus-user-workload-0             5/5     Running   0          32s
prometheus-user-workload-1             5/5     Running   0          32s
thanos-ruler-user-workload-0           3/3     Running   0          32s
thanos-ruler-user-workload-1           3/3     Running   0          32s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify the existence of the <literal>user-workload-monitoring-config</literal> ConfigMap in the <literal>openshift-user-workload-monitoring</literal>. If it exists, skip the remaining steps in this procedure.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get configmap user-workload-monitoring-config -n openshift-user-workload-monitoring</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Error from server (NotFound): configmaps "user-workload-monitoring-config" not found</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>user-workload-monitoring-config</literal> <literal>ConfigMap</literal> object for the User Workload Monitoring, and save it under the <literal>2_configure_user_workload_monitoring.yaml</literal> file name:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the <literal>2_configure_user_workload_monitoring.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f 2_configure_user_workload_monitoring.yaml
configmap/user-workload-monitoring-config created</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="oadp-creating-service-monitor_oadp-troubleshooting">
<title>Creating OADP service monitor</title>
<simpara>OADP provides an <literal>openshift-adp-velero-metrics-svc</literal> service which is created when the DPA is configured. The service monitor used by the user workload monitoring must point to the defined service.</simpara>
<simpara>Get details about the service by running the following commands:</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Ensure the <literal>openshift-adp-velero-metrics-svc</literal> service exists. It should contain <literal>app.kubernetes.io/name=velero</literal> label, which will be used as selector for the <literal>ServiceMonitor</literal> object.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get svc -n openshift-adp -l app.kubernetes.io/name=velero</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
openshift-adp-velero-metrics-svc   ClusterIP   172.30.38.244   &lt;none&gt;        8085/TCP   1h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a <literal>ServiceMonitor</literal> YAML file that matches the existing service label, and save the file as <literal>3_create_oadp_service_monitor.yaml</literal>. The service monitor is created in the <literal>openshift-adp</literal> namespace where the <literal>openshift-adp-velero-metrics-svc</literal> service resides.</simpara>
<formalpara>
<title>Example <literal>ServiceMonitor</literal> object</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: oadp-service-monitor
  name: oadp-service-monitor
  namespace: openshift-adp
spec:
  endpoints:
  - interval: 30s
    path: /metrics
    targetPort: 8085
    scheme: http
  selector:
    matchLabels:
      app.kubernetes.io/name: "velero"</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Apply the <literal>3_create_oadp_service_monitor.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f 3_create_oadp_service_monitor.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">servicemonitor.monitoring.coreos.com/oadp-service-monitor created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Confirm that the new service monitor is in an <emphasis role="strong">Up</emphasis> state by using the <emphasis role="strong">Administrator</emphasis> perspective of the OpenShift Container Platform web console:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Navigate to the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Targets</emphasis> page.</simpara>
</listitem>
<listitem>
<simpara>Ensure the <emphasis role="strong">Filter</emphasis> is unselected or that the <emphasis role="strong">User</emphasis> source is selected and type <literal>openshift-adp</literal> in the <literal>Text</literal> search field.</simpara>
</listitem>
<listitem>
<simpara>Verify that the status for the <emphasis role="strong">Status</emphasis> for the service monitor is <emphasis role="strong">Up</emphasis>.</simpara>
<figure>
<title>OADP metrics targets</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/oadp-metrics-targets.png"/>
</imageobject>
<textobject><phrase>OADP metrics targets</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-alerting-rules_oadp-troubleshooting">
<title>Creating an alerting rule</title>
<simpara>The OpenShift Container Platform monitoring stack allows to receive Alerts configured using Alerting Rules. To create an Alerting rule for the OADP project, use one of the Metrics which are scraped with the user workload monitoring.</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>PrometheusRule</literal> YAML file with the sample <literal>OADPBackupFailing</literal> alert and save it as <literal>4_create_oadp_alert_rule.yaml</literal>.</simpara>
<formalpara>
<title>Sample <literal>OADPBackupFailing</literal> alert</title>
<para>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sample-oadp-alert
  namespace: openshift-adp
spec:
  groups:
  - name: sample-oadp-backup-alert
    rules:
    - alert: OADPBackupFailing
      annotations:
        description: 'OADP had {{$value | humanize}} backup failures over the last 2 hours.'
        summary: OADP has issues creating backups
      expr: |
        increase(velero_backup_failure_total{job="openshift-adp-velero-metrics-svc"}[2h]) &gt; 0
      for: 5m
      labels:
        severity: warning</programlisting>
</para>
</formalpara>
<simpara>In this sample, the Alert displays under the following conditions:</simpara>
<itemizedlist>
<listitem>
<simpara>There is an increase of new failing backups during the 2 last hours that is greater than 0 and the state persists for at least 5 minutes.</simpara>
</listitem>
<listitem>
<simpara>If the time of the first increase is less than 5 minutes, the Alert will be in a <literal>Pending</literal> state, after which it will turn into a <literal>Firing</literal> state.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Apply the <literal>4_create_oadp_alert_rule.yaml</literal> file, which creates the <literal>PrometheusRule</literal> object in the <literal>openshift-adp</literal> namespace:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f 4_create_oadp_alert_rule.yaml</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">prometheusrule.monitoring.coreos.com/sample-oadp-alert created</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>After the Alert is triggered, you can view it in the following ways:</simpara>
<itemizedlist>
<listitem>
<simpara>In the <emphasis role="strong">Developer</emphasis> perspective, select the <emphasis role="strong">Observe</emphasis> menu.</simpara>
</listitem>
<listitem>
<simpara>In the <emphasis role="strong">Administrator</emphasis> perspective under the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Alerting</emphasis> menu, select <emphasis role="strong">User</emphasis> in the <emphasis role="strong">Filter</emphasis> box. Otherwise, by default only the <emphasis role="strong">Platform</emphasis> Alerts are displayed.</simpara>
<figure>
<title>OADP backup failing alert</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/oadp-backup-failing-alert.png"/>
</imageobject>
<textobject><phrase>OADP backup failing alert</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../monitoring/managing-alerts.xml#managing-alerts">Managing alerts</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="list-of-metrics_oadp-troubleshooting">
<title>List of available metrics</title>
<simpara>These are the list of metrics provided by the OADP together with their <link xlink:href="https://prometheus.io/docs/concepts/metric_types/">Types</link>.</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Metric name</entry>
<entry align="left" valign="top">Description</entry>
<entry align="left" valign="top">Type</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_hit_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of bytes retrieved from the cache</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_hit_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times content was retrieved from the cache</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_malformed</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times malformed content was read from the cache</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_miss_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times content was not found in the cache and fetched</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_missed_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of bytes retrieved from the underlying storage</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_miss_error_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times content could not be found in the underlying storage</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_cache_store_error_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times content could not be saved in the cache</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_get_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of bytes retrieved using <literal>GetContent()</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_get_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times <literal>GetContent()</literal> was called</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_get_error_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times <literal>GetContent()</literal> was called and the result was an error</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_get_not_found_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times <literal>GetContent()</literal> was called and the result was not found</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_write_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of bytes passed to <literal>WriteContent()</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>kopia_content_write_count</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Number of times <literal>WriteContent()</literal> was called</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_attempt_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of attempted backups</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_deletion_attempt_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of attempted backup deletions</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_deletion_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of failed backup deletions</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_deletion_success_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of successful backup deletions</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_duration_seconds</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Time taken to complete backup, in seconds</simpara></entry>
<entry align="left" valign="top"><simpara>Histogram</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of failed backups</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_items_errors</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of errors encountered during backup</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_items_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of items backed up</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_last_status</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Last status of the backup. A value of 1 is success, 0.</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_last_successful_timestamp</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Last time a backup ran successfully, Unix timestamp in seconds</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_partial_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of partially failed backups</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_success_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of successful backups</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_tarball_size_bytes</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Size, in bytes, of a backup</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Current number of existent backups</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_validation_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of validation failed backups</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_backup_warning_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of warned backups</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_csi_snapshot_attempt_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of CSI attempted volume snapshots</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_csi_snapshot_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of CSI failed volume snapshots</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_csi_snapshot_success_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of CSI successful volume snapshots</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_restore_attempt_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of attempted restores</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_restore_failed_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of failed restores</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_restore_partial_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of partially failed restores</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_restore_success_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of successful restores</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_restore_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Current number of existent restores</simpara></entry>
<entry align="left" valign="top"><simpara>Gauge</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_restore_validation_failed_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of failed restores failing validations</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_volume_snapshot_attempt_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of attempted volume snapshots</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_volume_snapshot_failure_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of failed volume snapshots</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>velero_volume_snapshot_success_total</literal></simpara></entry>
<entry align="left" valign="top"><simpara>Total number of successful volume snapshots</simpara></entry>
<entry align="left" valign="top"><simpara>Counter</simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</section>
<section xml:id="viewing-metrics-observe-ui_oadp-troubleshooting">
<title>Viewing metrics using the Observe UI</title>
<simpara>You can view metrics in the OpenShift Container Platform web console from the <emphasis role="strong">Administrator</emphasis> or <emphasis role="strong">Developer</emphasis> perspective, which must have access to the <literal>openshift-adp</literal> project.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Navigate to the <emphasis role="strong">Observe</emphasis> &#8594; <emphasis role="strong">Metrics</emphasis> page:</simpara>
<itemizedlist>
<listitem>
<simpara>If you are using the <emphasis role="strong">Developer</emphasis> perspective, follow these steps:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Select <emphasis role="strong">Custom query</emphasis>, or click on the <emphasis role="strong">Show PromQL</emphasis> link.</simpara>
</listitem>
<listitem>
<simpara>Type the query and click <emphasis role="strong">Enter</emphasis>.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>If you are using the <emphasis role="strong">Administrator</emphasis> perspective, type the expression in the text field and select <emphasis role="strong">Run Queries</emphasis>.</simpara>
<figure>
<title>OADP metrics query</title>
<mediaobject>
<imageobject>
<imagedata fileref="images/oadp-metrics-query.png"/>
</imageobject>
<textobject><phrase>OADP metrics query</phrase></textobject>
</mediaobject>
</figure>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
</section>
</section>
</section>
<section xml:id="oadp-api">
<title>APIs used with OADP</title>

<simpara>The document provides information about the following APIs that you can use with OADP:</simpara>
<itemizedlist>
<listitem>
<simpara>Velero API</simpara>
</listitem>
<listitem>
<simpara>OADP API</simpara>
</listitem>
</itemizedlist>
<section xml:id="velero-api">
<title>Velero API</title>
<simpara>Velero API documentation is maintained by Velero, not by Red Hat. It can be found at <link xlink:href="https://velero.io/docs/main/api-types/">Velero API types</link>.</simpara>
</section>
<section xml:id="oadp-api-tables">
<title>OADP API</title>
<simpara>The following tables provide the structure of the OADP API:</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>DataProtectionApplicationSpec</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>backupLocations</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[] <link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#BackupLocation"><literal>BackupLocation</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the list of configurations to use for <literal>BackupStorageLocations</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>snapshotLocations</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[] <link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#SnapshotLocation"><literal>SnapshotLocation</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the list of configurations to use for <literal>VolumeSnapshotLocations</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>unsupportedOverrides</literal></simpara></entry>
<entry align="left" valign="top"><simpara>map [ <link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#UnsupportedImageKey">UnsupportedImageKey</link> ]  <link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Can be used to override the deployed dependent images for development. Options are <literal>veleroImageFqin</literal>, <literal>awsPluginImageFqin</literal>, <literal>openshiftPluginImageFqin</literal>, <literal>azurePluginImageFqin</literal>, <literal>gcpPluginImageFqin</literal>, <literal>csiPluginImageFqin</literal>, <literal>dataMoverImageFqin</literal>, <literal>resticRestoreImageFqin</literal>, <literal>kubevirtPluginImageFqin</literal>, and <literal>operator-type</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>podAnnotations</literal></simpara></entry>
<entry align="left" valign="top"><simpara>map [ <link xlink:href="https://pkg.go.dev/builtin#string">string</link> ] <link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Used to add annotations to pods deployed by Operators.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>podDnsPolicy</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/k8s.io/api/core/v1#DNSPolicy"><literal>DNSPolicy</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the DNS of a pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>podDnsConfig</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/k8s.io/api/core/v1#PodDNSConfig"><literal>PodDNSConfig</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the DNS parameters of a pod in addition to those generated from <literal>DNSPolicy</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>backupImages</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/builtin#bool">bool</link></simpara></entry>
<entry align="left" valign="top"><simpara>Used to specify whether or not you want to deploy a registry for enabling backup and restore of images.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>configuration</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#ApplicationConfig"><literal>ApplicationConfig</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Used to define the data protection application&#8217;s server configuration.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>features</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#Features"><literal>Features</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration for the DPA to enable the Technology Preview features.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#DataProtectionApplicationSpec">Complete schema definitions for the OADP API</link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>BackupLocation</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>velero</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/vmware-tanzu/velero/pkg/apis/velero/v1#BackupStorageLocationSpec">velero.BackupStorageLocationSpec</link></simpara></entry>
<entry align="left" valign="top"><simpara>Location to store volume snapshots, as described in <link xlink:href="https://pkg.go.dev/github.com/vmware-tanzu/velero/pkg/apis/velero/v1#BackupStorageLocation">Backup Storage Location</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>bucket</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#CloudStorageLocation">CloudStorageLocation</link></simpara></entry>
<entry align="left" valign="top"><simpara>[Technology Preview] Automates creation of a bucket at some cloud storage providers for use as a backup storage location.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<important>
<simpara>The <literal>bucket</literal> parameter is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#BackupLocation">Complete schema definitions for the type <literal>BackupLocation</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>SnapshotLocation</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>velero</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/vmware-tanzu/velero/pkg/apis/velero/v1#VolumeSnapshotLocationSpec">VolumeSnapshotLocationSpec</link></simpara></entry>
<entry align="left" valign="top"><simpara>Location to store volume snapshots, as described in <link xlink:href="https://pkg.go.dev/github.com/vmware-tanzu/velero/pkg/apis/velero/v1#VolumeSnapshotLocation">Volume Snapshot Location</link>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#SnapshotLocation">Complete schema definitions for the type <literal>SnapshotLocation</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>ApplicationConfig</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>velero</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#VeleroConfig">VeleroConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration for the Velero server.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>restic</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#ResticConfig">ResticConfig</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration for the Restic server.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#ApplicationConfig">Complete schema definitions for the type <literal>ApplicationConfig</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>VeleroConfig</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>featureFlags</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[] <link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the list of features to enable for the Velero instance.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>defaultPlugins</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[] <link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>The following types of default Velero plugins can be installed: <literal>aws</literal>,<literal>azure</literal>, <literal>csi</literal>, <literal>gcp</literal>, <literal>kubevirt</literal>, and <literal>openshift</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>customPlugins</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#CustomPlugin">CustomPlugin</link></simpara></entry>
<entry align="left" valign="top"><simpara>Used for installation of custom Velero plugins.</simpara><simpara>Default and custom plugins are described in <link xlink:href="../../backup_and_restore/application_backup_and_restore/oadp-features-plugins.xml#oadp-features-plugins">OADP plugins</link></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>restoreResourcesVersionPriority</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Represents a config map that is created if defined for use in conjunction with the <literal>EnableAPIGroupVersions</literal> feature flag. Defining this field automatically adds <literal>EnableAPIGroupVersions</literal> to the Velero server feature flag.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>noDefaultBackupLocation</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#bool">bool</link></simpara></entry>
<entry align="left" valign="top"><simpara>To install Velero without a default backup storage location, you must set the <literal>noDefaultBackupLocation</literal> flag in order to confirm installation.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>podConfig</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#PodConfig"><literal>PodConfig</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>Velero</literal> pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>logLevel</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Velero server’s log level (use <literal>debug</literal> for the most granular logging, leave unset for Velero default). Valid options are <literal>trace</literal>, <literal>debug</literal>, <literal>info</literal>, <literal>warning</literal>, <literal>error</literal>, <literal>fatal</literal>, and <literal>panic</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#VeleroConfig">Complete schema definitions for the type <literal>VeleroConfig</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>CustomPlugin</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>name</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Name of custom plugin.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>image</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Image of custom plugin.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#CustomPlugin">Complete schema definitions for the type <literal>CustomPlugin</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>ResticConfig</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>enable</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/builtin#bool">bool</link></simpara></entry>
<entry align="left" valign="top"><simpara>If set to <literal>true</literal>, enables backup and restore using Restic. If set to <literal>false</literal>, snapshots are needed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>supplementalGroups</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xlink:href="https://pkg.go.dev/builtin#int64">int64</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the Linux groups to be applied to the <literal>Restic</literal> pod.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>A user-supplied duration string that defines the Restic timeout. Default value is <literal>1hr</literal> (1 hour). A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as <literal>300ms</literal>, -1.5h` or <literal>2h45m</literal>. Valid time units are <literal>ns</literal>, <literal>us</literal> (or <literal>µs</literal>), <literal>ms</literal>, <literal>s</literal>, <literal>m</literal>, and <literal>h</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>podConfig</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#PodConfig"><literal>PodConfig</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the <literal>Restic</literal> pod.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#ResticConfig">Complete schema definitions for the type <literal>ResticConfig</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>PodConfig</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>nodeSelector</literal></simpara></entry>
<entry align="left" valign="top"><simpara>map [ <link xlink:href="https://pkg.go.dev/builtin#string">string</link> ] <link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the <literal>nodeSelector</literal> to be supplied to a <literal>Velero</literal> <literal>podSpec</literal> or a <literal>Restic</literal> <literal>podSpec</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>tolerations</literal></simpara></entry>
<entry align="left" valign="top"><simpara>[]<link xlink:href="https://pkg.go.dev/k8s.io/api/core/v1#Toleration">Toleration</link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the list of tolerations to be applied to a Velero deployment or a Restic <literal>daemonset</literal>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>resourceAllocations</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/k8s.io/api/core/v1#ResourceRequirements">ResourceRequirements</link></simpara></entry>
<entry align="left" valign="top"><simpara>Set specific resource <literal>limits</literal> and <literal>requests</literal> for a <literal>Velero</literal> pod or a <literal>Restic</literal> pod as described in <link xlink:href="../../backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.xml#oadp-setting-resource-limits-and-requests_installing-oadp-aws">Setting Velero CPU and memory resource allocations</link>.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>labels</literal></simpara></entry>
<entry align="left" valign="top"><simpara>map [ <link xlink:href="https://pkg.go.dev/builtin#string">string</link> ] <link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>Labels to add to pods.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#PodConfig">Complete schema definitions for the type <literal>PodConfig</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>Features</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>dataMover</literal></simpara></entry>
<entry align="left" valign="top"><simpara>*<link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#DataMover"><literal>DataMover</literal></link></simpara></entry>
<entry align="left" valign="top"><simpara>Defines the configuration of the Data Mover.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara><link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator/api/v1alpha1#Features">Complete schema definitions for the type <literal>Features</literal></link>.</simpara>
<table frame="all" rowsep="1" colsep="1">
<title>DataMover</title>
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Property</entry>
<entry align="left" valign="top">Type</entry>
<entry align="left" valign="top">Description</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara><literal>enable</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#bool">bool</link></simpara></entry>
<entry align="left" valign="top"><simpara>If set to <literal>true</literal>, deploys the volume snapshot mover controller and a modified CSI Data Mover plugin. If set to <literal>false</literal>, these are not deployed.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>credentialName</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>User-supplied Restic <literal>Secret</literal> name for Data Mover.</simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara><literal>timeout</literal></simpara></entry>
<entry align="left" valign="top"><simpara><link xlink:href="https://pkg.go.dev/builtin#string">string</link></simpara></entry>
<entry align="left" valign="top"><simpara>A user-supplied duration string for <literal>VolumeSnapshotBackup</literal> and <literal>VolumeSnapshotRestore</literal> to complete. Default is <literal>10m</literal> (10 minutes). A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as <literal>300ms</literal>, -1.5h` or <literal>2h45m</literal>. Valid time units are <literal>ns</literal>, <literal>us</literal> (or <literal>µs</literal>), <literal>ms</literal>, <literal>s</literal>, <literal>m</literal>, and <literal>h</literal>.</simpara></entry>
</row>
</tbody>
</tgroup>
</table>
<simpara>The OADP API is more fully detailed in <link xlink:href="https://pkg.go.dev/github.com/openshift/oadp-operator">OADP Operator</link>.</simpara>
</section>
</section>
<section xml:id="oadp-advanced-topics">
<title>Advanced OADP features and functionalities</title>

<simpara>This document provides information about advanced features and functionalities of OpenShift API for Data Protection (OADP).</simpara>
<section xml:id="oadp-different-kubernetes-api-versions">
<title>Working with different Kubernetes API versions on the same cluster</title>
<section xml:id="oadp-checking-api-group-versions_oadp-advanced-topics">
<title>Listing the Kubernetes API group versions on a cluster</title>
<simpara>A source cluster might offer multiple versions of an API, where one of these versions is the preferred API version. For example, a source cluster with an API named <literal>Example</literal> might be available in the <literal>example.com/v1</literal> and <literal>example.com/v1beta2</literal> API groups.</simpara>
<simpara>If you use Velero to back up and restore such a source cluster, Velero backs up only the version of that resource that uses the preferred version of its Kubernetes API.</simpara>
<simpara>To return to the above example, if <literal>example.com/v1</literal> is the preferred API, then Velero only backs up the version of a resource that uses <literal>example.com/v1</literal>. Moreover, the target cluster needs to have <literal>example.com/v1</literal> registered in its set of available API resources in order for Velero to restore the resource on the target cluster.</simpara>
<simpara>Therefore, you need to generate a list of the Kubernetes API group versions on your target cluster to be sure the preferred API version is registered in its set of available API resources.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Enter the following command:</simpara>
</listitem>
</itemizedlist>
<programlisting language="terminal" linenumbering="unnumbered">$ oc api-resources</programlisting>
</section>
<section xml:id="oadp-about-enable-api-group-versions_oadp-advanced-topics">
<title>About Enable API Group Versions</title>
<simpara>By default, Velero only backs up resources that use the preferred version of the Kubernetes API. However, Velero also includes a feature, <link xlink:href="https://velero.io/docs/v1.9/enable-api-group-versions-feature/">Enable API Group Versions</link>, that overcomes this limitation. When enabled on the source cluster, this feature causes Velero to back up <emphasis>all</emphasis> Kubernetes API group versions that are supported on the cluster, not only the preferred one. After the versions are stored in the backup .tar file, they are available to be restored on the destination cluster.</simpara>
<simpara>For example, a source cluster with an API named <literal>Example</literal> might be available in the <literal>example.com/v1</literal> and <literal>example.com/v1beta2</literal> API groups, with <literal>example.com/v1</literal> being the preferred API.</simpara>
<simpara>Without the Enable API Group Versions feature enabled, Velero backs up only the preferred API group version for <literal>Example</literal>, which is <literal>example.com/v1</literal>. With the feature enabled, Velero also backs up <literal>example.com/v1beta2</literal>.</simpara>
<simpara>When the Enable API Group Versions feature is enabled on the destination cluster, Velero selects the version to restore on the basis of the order of priority of API group versions.</simpara>
<note>
<simpara>Enable API Group Versions is still in beta.</simpara>
</note>
<simpara>Velero uses the following algorithm to assign priorities to API versions, with <literal>1</literal> as the top priority:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Preferred version of the <emphasis>destination</emphasis> cluster</simpara>
</listitem>
<listitem>
<simpara>Preferred version of the source_ cluster</simpara>
</listitem>
<listitem>
<simpara>Common non-preferred supported version with the highest Kubernetes version priority</simpara>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://velero.io/docs/v1.9/enable-api-group-versions-feature/">Enable API Group Versions Feature</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-using-enable-api-group-versions_oadp-advanced-topics">
<title>Using Enable API Group Versions</title>
<simpara>You can use Velero&#8217;s Enable API Group Versions feature to back up <emphasis>all</emphasis> Kubernetes API group versions that are supported on a cluster, not only the preferred one.</simpara>
<note>
<simpara>Enable API Group Versions is still in beta.</simpara>
</note>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Configure the <literal>EnableAPIGroupVersions</literal> feature flag:</simpara>
</listitem>
</itemizedlist>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: oadp.openshift.io/vialpha1
kind: DataProtectionApplication
...
spec:
  configuration:
    velero:
      featureFlags:
      - EnableAPIGroupVersions</programlisting>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="https://velero.io/docs/v1.9/enable-api-group-versions-feature/">Enable API Group Versions Feature</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="backing-up-data-one-cluster-restoring-another-cluster">
<title>Backing up data from one cluster and restoring it to another cluster</title>
<section xml:id="oadp-about-backing-and-restoring-from-cluster-to-cluster_oadp-advanced-topics">
<title>About backing up data from one cluster and restoring it on another cluster</title>
<simpara>OpenShift API for Data Protection (OADP) is designed to back up and restore application data in the same OpenShift Container Platform cluster. Migration Toolkit for Containers (MTC) is designed to migrate containers, including application data, from one OpenShift Container Platform cluster to another cluster.</simpara>
<simpara>You can use OADP to back up application data from one OpenShift Container Platform cluster and restore it on another cluster. However, doing so is more complicated than using MTC or using OADP to back up and restore on the same cluster.</simpara>
<simpara>To successfully use OADP to back up data from one cluster and restore it to another cluster, you must take into account the following factors, in addition to the prerequisites and procedures that apply to using OADP to back up and restore data on the same cluster:</simpara>
<itemizedlist>
<listitem>
<simpara>Operators</simpara>
</listitem>
<listitem>
<simpara>Use of Velero</simpara>
</listitem>
<listitem>
<simpara>UID and GID ranges</simpara>
</listitem>
</itemizedlist>
<section xml:id="oadp-cluster-to-cluster-operators_oadp-advanced-topics">
<title>Operators</title>
<simpara>You must exclude Operators from the backup of an application for backup and restore to succeed.</simpara>
</section>
<section xml:id="oadp-cluster-to-cluster-velero_oadp-advanced-topics">
<title>Use of Velero</title>
<simpara>Velero, which OADP is built upon, does not natively support migrating persistent volume snapshots across cloud providers. To migrate volume snapshot data between cloud platforms, you must <emphasis>either</emphasis> enable the Velero Restic file system backup option, which backs up volume contents at the file system level, <emphasis>or</emphasis> use the OADP Data Mover for CSI snapshots.</simpara>
<note>
<simpara>In OADP 1.1 and earlier, the Velero Restic file system backup option is called <literal>restic</literal>.
In OADP 1.2 and later, the Velero Restic file system backup option is called <literal>file-system-backup</literal>.</simpara>
</note>
<itemizedlist>
<listitem>
<simpara>You must also use Velero&#8217;s <link xlink:href="https://velero.io/docs/main/file-system-backup/">File System Backup</link> to migrate data between AWS regions or between Microsoft Azure regions.</simpara>
</listitem>
<listitem>
<simpara>Velero does not support restoring data to a cluster with an <emphasis>earlier</emphasis> Kubernetes version than the source cluster.</simpara>
</listitem>
<listitem>
<simpara>It is theoretically possible to migrate workloads to a destination with a <emphasis>later</emphasis> Kubernetes version than the source, but you must consider the compatibility of API groups between clusters for each custom resource. If a Kubernetes version upgrade breaks the compatibility of core or native API groups, you must first update the impacted custom resources.</simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="oadp-pod-volume-backup_oadp-advanced-topics">
<title>About determining which pod volumes to back up</title>
<simpara>Before you start a backup operation by using File System Backup (FSB), you must specify which pods contain a volume that you want to back up. Velero refers to this process as "discovering" the appropriate pod volumes.</simpara>
<simpara>Velero supports two approaches for determining pod volumes:</simpara>
<itemizedlist>
<listitem>
<simpara><emphasis role="strong">Opt-in approach</emphasis>: The opt-in approach requires that you actively indicate that you want to include - <emphasis>opt-in</emphasis> - a volume in a backup. You do this by labelling each pod that contains a volume to be backed up with the name of the volume. When Velero finds a persistent volume (PV), it checks the pod that mounted the volume. If the pod is labelled with the name of the volume, Velero backs up the pod.</simpara>
</listitem>
<listitem>
<simpara><emphasis role="strong">Opt-out approach</emphasis>: With the opt-out approach, you must actively specify that you want to exclude a volume from a backup. You do this by labelling each pod that contains a volume you do not want to back up with the name of the volume. When Velero finds a PV, it checks the pod that mounted the volume. If the pod is labelled with the volume&#8217;s name, Velero does not back up the pod.</simpara>
</listitem>
</itemizedlist>
<section xml:id="pod-volume-limitations_oadp-advanced-topics">
<title>Limitations</title>
<itemizedlist>
<listitem>
<simpara>FSB does not support backing up and restoring <literal>hostpath</literal> volumes. However, FSB does support backing up and restoring local volumes.</simpara>
</listitem>
<listitem>
<simpara>Velero uses a static, common encryption key for all backup repositories it creates. <emphasis role="strong">This static key means that anyone who can access your backup storage can also decrypt your backup data</emphasis>. It is essential that you limit access to backup storage.</simpara>
</listitem>
<listitem>
<simpara>For PVCs, every incremental backup chain is maintained across pod reschedules.</simpara>
<simpara>For pod volumes that are <emphasis>not</emphasis> PVCs, such as <literal>emptyDir</literal> volumes, if
a pod is deleted or recreated, for example, by a <literal>ReplicaSet</literal> or a deployment, the next backup of those volumes will be a full backup and not an incremental backup. It is assumed that the lifecycle of a pod volume is defined by its pod.</simpara>
</listitem>
<listitem>
<simpara>Even though backup data can be kept incrementally, backing up large files, such as a database, can take a long time. This is because FSB uses deduplication to find the difference that needs to be backed up.</simpara>
</listitem>
<listitem>
<simpara>FSB reads and writes data from volumes by accessing the file system of the node on which the pod is running. For this reason, FSB can only back up volumes that are mounted from a pod and not directly from a PVC. Some Velero users have overcome this limitation by running a staging pod, such as a BusyBox or Alpine container with an infinite sleep, to mount these PVC and PV pairs before performing a Velero backup..</simpara>
</listitem>
<listitem>
<simpara>FSB expects volumes to be mounted under <literal>&lt;hostPath&gt;/&lt;pod UID&gt;</literal>, with
<literal>&lt;hostPath&gt;</literal> being configurable. Some Kubernetes systems, for example,
vCluster, do not mount volumes under the <literal>&lt;pod UID&gt;</literal> subdirectory, and
VFSB does not work with them as expected.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-backing-up-opt-in_oadp-advanced-topics">
<title>Backing up pod volumes by using the opt-in method</title>
<simpara>You can use the opt-in method to specify which volumes need to be backed up by File System Backup (FSB). You can do this by using the <literal>backup.velero.io/backup-volumes</literal> command.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>On each pod that contains one or more volumes that you want to back up, enter the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;your_pod_namespace&gt; annotate pod/&lt;your_pod_name&gt; \
  backup.velero.io/backup-volumes=&lt;your_volume_name_1&gt;, \ &lt;your_volume_name_2&gt;&gt;,...,&lt;your_volume_name_n&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;your_volume_name_x&gt;</literal></term>
<listitem>
<simpara>specifies the name of the xth volume in the pod specification.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
</section>
<section xml:id="oadp-backing-up-opt-out_oadp-advanced-topics">
<title>Backing up pod volumes by using the opt-out method</title>
<simpara>When using the opt-out approach, all pod volumes are backed up by using File System Backup (FSB), although there are some exceptions:</simpara>
<itemizedlist>
<listitem>
<simpara>Volumes that mount the default service account token, secrets, and configuration maps.</simpara>
</listitem>
<listitem>
<simpara><literal>hostPath</literal> volumes</simpara>
</listitem>
</itemizedlist>
<simpara>You can use the opt-out method to specify which volumes <emphasis role="strong">not</emphasis> to back up. You can do this by using the <literal>backup.velero.io/backup-volumes-excludes</literal> command.</simpara>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>On each pod that contains one or more volumes that you do not want to back up, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n &lt;your_pod_namespace&gt; annotate pod/&lt;your_pod_name&gt; \
  backup.velero.io/backup-volumes-excludes=&lt;your_volume_name_1&gt;, \ &lt;your_volume_name_2&gt;&gt;,...,&lt;your_volume_name_n&gt;</programlisting>
<simpara>where:</simpara>
<variablelist>
<varlistentry>
<term><literal>&lt;your_volume_name_x&gt;</literal></term>
<listitem>
<simpara>specifies the name of the xth volume in the pod specification.</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
</itemizedlist>
<note>
<simpara>You can enable this behavior for all Velero backups by running the <literal>velero install</literal> command with the <literal>--default-volumes-to-fs-backup</literal> flag.</simpara>
</note>
</section>
</section>
<section xml:id="oadp-cluster-to-cluster-uid-and-gid-ranges_oadp-advanced-topics">
<title>UID and GID ranges</title>
<simpara>If you back up data from one cluster and restore it to another cluster,  problems might occur with UID (User ID) and GID (Group ID) ranges. The following section explains these potential issues and mitigations:</simpara>
<variablelist>
<varlistentry>
<term>Summary of the issues</term>
<listitem>
<simpara>The namespace UID and GID ranges might change depending on the destination cluster. OADP does not back up and restore OpenShift UID range metadata. If the backed up application requires a specific UID, ensure the range is availableupon restore. For more information about OpenShift&#8217;s UID and GID ranges, see <link xlink:href="https://cloud.redhat.com/blog/a-guide-to-openshift-and-uids">A Guide to OpenShift and UIDs</link>.</simpara>
</listitem>
</varlistentry>
<varlistentry>
<term>Detailed description of the issues</term>
<listitem>
<simpara>When you create a namespace in OpenShift Container Platform by using the shell command <literal>oc create namespace</literal>, OpenShift Container Platform assigns the namespace a unique User ID (UID) range from its available pool of UIDs, a Supplemental Group (GID) range, and unique SELinux MCS labels. This information is stored in the <literal>metadata.annotations</literal> field of the cluster. This information is part of the Security Context Constraints (SCC) annotations, which comprise of the following components:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>openshift.io/sa.scc.mcs</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift.io/sa.scc.supplemental-groups</literal></simpara>
</listitem>
<listitem>
<simpara><literal>openshift.io/sa.scc.uid-range</literal></simpara>
</listitem>
</itemizedlist>
</listitem>
</varlistentry>
</variablelist>
<simpara>When you use OADP to restore the namespace, it automatically uses the information in <literal>metadata.annotations</literal> without resetting it for the destination cluster. As a result, the workload might not have access to the backed up data if any of the following is true:</simpara>
<itemizedlist>
<listitem>
<simpara>There is an existing namespace with other SCC annotations, for example, on another cluster. In this case, OADP uses the existing namespace during the backup instead of the namespace you want to restore.</simpara>
</listitem>
<listitem>
<simpara>A label selector was used during the backup, but the namespace in which the workloads are executed does not have the label. In this case, OADP does not back up the namespace, but creates a new namespace during the restore that does not contain the annotations of the backed up namespace. This results in a new UID range being assigned to the namespace.</simpara>
<simpara>This can be an issue for customer workloads if OpenShift Container Platform assigns a pod a <literal>securityContext</literal> UID to a pod based on namespace annotations that have changed since the persistent volume data was backed up.</simpara>
</listitem>
<listitem>
<simpara>The UID of the container no longer matches the UID of the file owner.</simpara>
</listitem>
<listitem>
<simpara>An error occurs because OpenShift Container Platform has not changed the UID range of the destination cluster to match the backup cluster data. As a result, the backup cluster has a different UID than the destination cluster, which means that the application cannot read or write data on the destination cluster.</simpara>
<variablelist>
<varlistentry>
<term>Mitigations</term>
<listitem>
<simpara>You can use one or more of the following mitigations to resolve the UID and GID range issues:</simpara>
</listitem>
</varlistentry>
</variablelist>
</listitem>
<listitem>
<simpara>Simple mitigations:</simpara>
<itemizedlist>
<listitem>
<simpara>If you use a label selector in the <literal>Backup</literal> CR to filter the objects to include in the backup, be sure to add this label selector to the namespace that contains the workspace.</simpara>
</listitem>
<listitem>
<simpara>Remove any pre-existing version of a namespace on the destination cluster before attempting to restore a namespace with the same name.</simpara>
</listitem>
</itemizedlist>
</listitem>
<listitem>
<simpara>Advanced mitigations:</simpara>
<itemizedlist>
<listitem>
<simpara>Fix UID ranges after migration by <link xlink:href="https://access.redhat.com/articles/6844071">Resolving overlapping UID ranges in OpenShift namespaces after migration</link>. Step 1 is optional.</simpara>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<simpara>For an in-depth discussion of UID and GID ranges in OpenShift Container Platform with an emphasis on overcoming issues in backing up data on one cluster and restoring it on another, see <link xlink:href="https://cloud.redhat.com/blog/a-guide-to-openshift-and-uids">A Guide to OpenShift and UIDs</link>.</simpara>
</section>
<section xml:id="oadp-backing-and-restoring-from-cluster-to-cluster_oadp-advanced-topics">
<title>Backing up data from one cluster and restoring it to another cluster</title>
<simpara>In general, you back up data from one OpenShift Container Platform cluster and restore it on another OpenShift Container Platform cluster in the same way that you back up and restore data to the same cluster. However, there are some additional prerequisites and differences in the procedure when backing up data from one OpenShift Container Platform cluster and restoring it on another.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>All relevant prerequisites for backing up and restoring on your platform (for example, AWS, Microsoft Azure, GCP, and so on), especially the prerequisites for the Data Protection Application (DPA), are described in the relevant sections of this guide.</simpara>
</listitem>
</itemizedlist>
<itemizedlist>
<title>Procedure</title>
<listitem>
<simpara>Make the following additions to the procedures given for your platform:</simpara>
<itemizedlist>
<listitem>
<simpara>Ensure that the backup store location (BSL) and volume snapshot location have the same names and paths to restore resources to another cluster.</simpara>
</listitem>
<listitem>
<simpara>Share the same object storage location credentials across the clusters.</simpara>
</listitem>
<listitem>
<simpara>For best results, use OADP to create the namespace on the destination cluster.</simpara>
</listitem>
<listitem>
<simpara>If you use the Velero <literal>file-system-backup</literal> option, enable the <literal>--default-volumes-to-fs-backup</literal> flag for use during backup by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ velero backup create &lt;backup_name&gt; --default-volumes-to-fs-backup &lt;any_other_options&gt;</programlisting>
</listitem>
</itemizedlist>
</listitem>
</itemizedlist>
<note>
<simpara>In OADP 1.2 and later, the Velero Restic option is called <literal>file-system-backup</literal>.</simpara>
</note>
</section>
</section>
<section xml:id="additional-resources_oadp-advanced-topics" role="_additional-resources">
<title>Additional resources</title>
<simpara>For more information about API group versions, see <link xlink:href="../../backup_and_restore/application_backup_and_restore/oadp-advanced-topics.xml#oadp-different-kubernetes-api-versions">Working with different Kubernetes API versions on the same cluster</link>.</simpara>
<simpara>For more information about OADP Data Mover, see <link xlink:href="../../backup_and_restore/application_backup_and_restore/installing/oadp-using-data-mover-for-csi-snapshots-doc.xml#backing-up-applications">Using Data Mover for CSI snapshots</link>.</simpara>
<simpara>For more information about using Restic with OADP, see <link xlink:href="../../backup_and_restore/application_backup_and_restore/backing_up_and_restoring/oadp-backing-up-applications-restic-doc.xml#backing-up-applications">Backing up applications with File System Backup: Kopia or Restic</link>.</simpara>
</section>
</section>
</chapter>
<chapter xml:id="_control_plane_backup_and_restore">
<title>Control plane backup and restore</title>
<section xml:id="backup-etcd">
<title>Backing up etcd</title>

<simpara>etcd is the key-value store for OpenShift Container Platform, which persists the state of all resource objects.</simpara>
<simpara>Back up your cluster&#8217;s etcd data regularly and store in a secure location ideally outside the OpenShift Container Platform environment. Do not take an etcd backup before the first certificate rotation completes, which occurs 24 hours after installation, otherwise the backup will contain expired certificates. It is also recommended to take etcd backups during non-peak usage hours because the etcd snapshot has a high I/O cost.</simpara>
<simpara>Be sure to take an etcd backup after you upgrade your cluster. This is important because when you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.y.z cluster must use an etcd backup that was taken from 4.y.z.</simpara>
<important>
<simpara>Back up your cluster&#8217;s etcd data by performing a single invocation of the backup script on a control plane host. Do not take a backup for each control plane host.</simpara>
</important>
<simpara>After you have an etcd backup, you can <link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">restore to a previous cluster state</link>.</simpara>
<section xml:id="backing-up-etcd-data_backup-etcd">
<title>Backing up etcd data</title>
<simpara>Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd.</simpara>
<important>
<simpara>Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have checked whether the cluster-wide proxy is enabled.</simpara>
<tip>
<simpara>You can check whether the proxy is enabled by reviewing the output of <literal>oc get proxy cluster -o yaml</literal>. The proxy is enabled if the <literal>httpProxy</literal>, <literal>httpsProxy</literal>, and <literal>noProxy</literal> fields have values set.</simpara>
</tip>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Start a debug session as root for a control plane node:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug --as-root node/&lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Change your root directory to <literal>/host</literal> in the debug shell:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>If the cluster-wide proxy is enabled, be sure that you have exported the <literal>NO_PROXY</literal>, <literal>HTTP_PROXY</literal>, and <literal>HTTPS_PROXY</literal> environment variables.</simpara>
</listitem>
<listitem>
<simpara>Run the <literal>cluster-backup.sh</literal> script in the debug shell and pass in the location to save the backup to.</simpara>
<tip>
<simpara>The <literal>cluster-backup.sh</literal> script is maintained as a component of the etcd Cluster Operator and is a wrapper around the <literal>etcdctl snapshot save</literal> command.</simpara>
</tip>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup</programlisting>
<formalpara>
<title>Example script output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6
found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7
found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6
found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3
ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1
etcdctl version: 3.4.14
API version: 3.4
{"level":"info","ts":1624647639.0188997,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part"}
{"level":"info","ts":"2021-06-25T19:00:39.030Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1624647639.0301006,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://10.0.0.5:2379"}
{"level":"info","ts":"2021-06-25T19:00:40.215Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}
{"level":"info","ts":1624647640.6032252,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://10.0.0.5:2379","size":"114 MB","took":1.584090459}
{"level":"info","ts":1624647640.6047094,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db"}
Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db
{"hash":3866667823,"revision":31407,"totalKey":12828,"totalSize":114446336}
snapshot db and kube resources are successfully saved to /home/core/assets/backup</programlisting>
</para>
</formalpara>
<simpara>In this example, two files are created in the <literal>/home/core/assets/backup/</literal> directory on the control plane host:</simpara>
<itemizedlist>
<listitem>
<simpara><literal>snapshot_&lt;datetimestamp&gt;.db</literal>: This file is the etcd snapshot. The <literal>cluster-backup.sh</literal> script confirms its validity.</simpara>
</listitem>
<listitem>
<simpara><literal>static_kuberesources_&lt;datetimestamp&gt;.tar.gz</literal>: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot.</simpara>
<note>
<simpara>If etcd encryption is enabled, it is recommended to store this second file separately from the etcd snapshot for security reasons. However, this file is required to restore from the etcd snapshot.</simpara>
<simpara>Keep in mind that etcd encryption only encrypts values, not keys. This means that resource types, namespaces, and object names are unencrypted.</simpara>
</note>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</section>
<section xml:id="additional-resources_backup-etcd" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../hosted_control_planes/hcp-backup-restore-dr.xml#hcp-backup-restore">Backing up and restoring etcd on a hosted cluster</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="creating-automated-etcd-backups_backup-etcd">
<title>Creating automated etcd backups</title>
<simpara>The automated backup feature for etcd supports both recurring and single backups. Recurring backups create a cron job that starts a single backup each time the job triggers.</simpara>
<important>
<simpara>Automating etcd backups is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.</simpara>
<simpara>For more information about the support scope of Red Hat Technology Preview features, see <link xlink:href="https://access.redhat.com/support/offerings/techpreview/">Technology Preview Features Support Scope</link>.</simpara>
</important>
<section xml:id="enabling-automated-etcd-backups_backup-etcd">
<title>Enabling automated etcd backups</title>
<simpara>Follow these steps to enable automated backups for etcd.</simpara>
<warning>
<simpara>Enabling the <literal>TechPreviewNoUpgrade</literal> feature set on your cluster prevents minor version updates. The <literal>TechPreviewNoUpgrade</literal> feature set cannot be disabled. Do not enable this feature set on production clusters.</simpara>
</warning>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a <literal>FeatureGate</literal> custom resource (CR) file named <literal>enable-tech-preview-no-upgrade.yaml</literal> with the following contents:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster
spec:
  featureSet: TechPreviewNoUpgrade</programlisting>
</listitem>
<listitem>
<simpara>Apply the CR and enable automated backups:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f enable-tech-preview-no-upgrade.yaml</programlisting>
</listitem>
<listitem>
<simpara>It takes time to enable the related APIs. Verify the creation of the custom resource definition (CRD) by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get crd | grep backup</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">backups.config.openshift.io 2023-10-25T13:32:43Z
etcdbackups.operator.openshift.io 2023-10-25T13:32:04Z</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-single-etcd-backup_backup-etcd">
<title>Creating a single etcd backup</title>
<simpara>Follow these steps to create a single etcd backup by creating and applying a custom resource (CR).</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
<listitem>
<simpara>You have a PVC to save backup data to.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Create a CR file named <literal>etcd-single-backup.yaml</literal> with contents such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: operator.openshift.io/v1alpha1
kind: EtcdBackup
metadata:
  name: etcd-single-backup
  namespace: openshift-etcd
spec:
  pvcName: etcd-backup-pvc <co xml:id="CO66-1"/></programlisting>
<calloutlist>
<callout arearefs="CO66-1">
<para>The name of the persistent volume claim (PVC) to save the backup to. Adjust this value according to your environment.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the CR to start a single backup:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f etcd-single-backup.yaml</programlisting>
</listitem>
</orderedlist>
</section>
<section xml:id="creating-recurring-etcd-backups_backup-etcd">
<title>Creating recurring etcd backups</title>
<simpara>Follow these steps to create automated recurring backups of etcd.</simpara>
<simpara>Use dynamically-provisioned storage to keep the created etcd backup data in a safe, external location if possible. If dynamically-provisioned storage is not available, consider storing the backup data on an NFS share to make backup recovery more accessible.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have access to the OpenShift CLI (<literal>oc</literal>).</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>If dynamically-provisioned storage is available, complete the following steps to create automated recurring backups:</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a persistent volume claim (PVC) named <literal>etcd-backup-pvc.yaml</literal> with contents such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: etcd-backup-pvc
  namespace: openshift-etcd
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 200Gi <co xml:id="CO67-1"/>
  storageClassName: standard-csi <co xml:id="CO67-2"/>
  volumeMode: Filesystem</programlisting>
<calloutlist>
<callout arearefs="CO67-1">
<para>The amount of storage available to the PVC. Adjust this value for your requirements.</para>
</callout>
<callout arearefs="CO67-2">
<para>The name of the <literal>StorageClass</literal> required by the claim. Adjust this value according to your environment.</para>
</callout>
</calloutlist>
<note>
<simpara>Each of the following providers require changes to the <literal>accessModes</literal> and <literal>storageClassName</literal> keys:</simpara>
<informaltable frame="all" rowsep="1" colsep="1">
<tgroup cols="3">
<colspec colname="col_1" colwidth="33.3333*"/>
<colspec colname="col_2" colwidth="33.3333*"/>
<colspec colname="col_3" colwidth="33.3334*"/>
<thead>
<row>
<entry align="left" valign="top">Provider</entry>
<entry align="left" valign="top"><literal>accessModes</literal> value</entry>
<entry align="left" valign="top"><literal>storageClassName</literal> value</entry>
</row>
</thead>
<tbody>
<row>
<entry align="left" valign="top"><simpara>AWS with the <literal>versioned-installer-efc_operator-ci</literal> profile</simpara></entry>
<entry align="left" valign="top"><simpara><literal>- ReadWriteMany</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>efs-sc</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Google Cloud Platform</simpara></entry>
<entry align="left" valign="top"><simpara><literal>- ReadWriteMany</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>filestore-csi</literal></simpara></entry>
</row>
<row>
<entry align="left" valign="top"><simpara>Microsoft Azure</simpara></entry>
<entry align="left" valign="top"><simpara><literal>- ReadWriteMany</literal></simpara></entry>
<entry align="left" valign="top"><simpara><literal>azurefile-csi</literal></simpara></entry>
</row>
</tbody>
</tgroup>
</informaltable>
</note>
</listitem>
<listitem>
<simpara>Apply the PVC by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f etcd-backup-pvc.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify the creation of the PVC by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pvc</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME              STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
etcd-backup-pvc   Pending                                      standard-csi   51s</programlisting>
</para>
</formalpara>
<note>
<simpara>Dynamic PVCs stay in the <literal>Pending</literal> state until they are mounted.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>If dynamically-provisioned storage is unavailable, create a local storage PVC by completing the following steps:</simpara>
<warning>
<simpara>If you delete or otherwise lose access to the node that contains the stored backup data, you can lose data.</simpara>
</warning>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create a <literal>StorageClass</literal> CR file named <literal>etcd-backup-local-storage.yaml</literal> with the following contents:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: etcd-backup-local-storage
provisioner: kubernetes.io/no-provisioner
volumeBindingMode: WaitForFirstConsumer</programlisting>
</listitem>
<listitem>
<simpara>Apply the <literal>StorageClass</literal> CR by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f etcd-backup-local-storage.yaml</programlisting>
</listitem>
<listitem>
<simpara>Create a PV named <literal>etcd-backup-pv-fs.yaml</literal> from the applied <literal>StorageClass</literal> with content such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: v1
kind: PersistentVolume
metadata:
  name: etcd-backup-pv-fs
spec:
  capacity:
    storage: 100Gi <co xml:id="CO68-1"/>
  volumeMode: Filesystem
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage
  local:
    path: /mnt/
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - &lt;example-master-node&gt; <co xml:id="CO68-2"/></programlisting>
<calloutlist>
<callout arearefs="CO68-1">
<para>The amount of storage available to the PV. Adjust this value for your requirements.</para>
</callout>
<callout arearefs="CO68-2">
<para>Replace this value with the node to attach this PV to.</para>
</callout>
</calloutlist>
<tip>
<simpara>Run the following command to list the available nodes:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
</tip>
</listitem>
<listitem>
<simpara>Verify the creation of the PV by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get pv</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS    REASON   AGE
etcd-backup-pv-fs       100Gi      RWX            Delete           Available           local-storage            10s</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create a PVC named <literal>etcd-backup-pvc.yaml</literal> with contents such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: etcd-backup-pvc
spec:
  accessModes:
  - ReadWriteMany
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Gi <co xml:id="CO69-1"/>
  storageClassName: local-storage</programlisting>
<calloutlist>
<callout arearefs="CO69-1">
<para>The amount of storage available to the PVC. Adjust this value for your requirements.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Apply the PVC by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f etcd-backup-pvc.yaml</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Create a custom resource definition (CRD) file named <literal>etcd-recurring-backups.yaml</literal>. The contents of the created CRD define the schedule and retention type of automated backups.</simpara>
<simpara>For the default retention type of <literal>RetentionNumber</literal> with 15 retained backups, use contents such as the following example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: config.openshift.io/v1alpha1
kind: Backup
metadata:
  name: etcd-recurring-backup
spec:
  etcd:
    schedule: "20 4 * * *" <co xml:id="CO70-1"/>
    timeZone: "UTC"
    pvcName: etcd-backup-pvc</programlisting>
<calloutlist>
<callout arearefs="CO70-1">
<para>The <literal>CronTab</literal> schedule for recurring backups. Adjust this value for your needs.</para>
</callout>
</calloutlist>
<simpara>To use retention based on the maximum number of backups, add the following key-value pairs to the <literal>etcd</literal> key:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  etcd:
    retentionPolicy:
      retentionType: RetentionNumber <co xml:id="CO71-1"/>
      retentionNumber:
        maxNumberOfBackups: 5 <co xml:id="CO71-2"/></programlisting>
<calloutlist>
<callout arearefs="CO71-1">
<para>The retention type. Defaults to <literal>RetentionNumber</literal> if unspecified.</para>
</callout>
<callout arearefs="CO71-2">
<para>The maximum number of backups to retain. Adjust this value for your needs. Defaults to 15 backups if unspecified.</para>
</callout>
</calloutlist>
<warning>
<simpara>A known issue causes the number of retained backups to be one greater than the configured value.</simpara>
</warning>
<simpara>For retention based on the file size of backups, use the following:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">spec:
  etcd:
    retentionPolicy:
      retentionType: RetentionSize
      retentionSize:
        maxSizeOfBackupsGb: 20 <co xml:id="CO72-1"/></programlisting>
<calloutlist>
<callout arearefs="CO72-1">
<para>The maximum file size of the retained backups in gigabytes. Adjust this value for your needs. Defaults to 10 GB if unspecified.</para>
</callout>
</calloutlist>
<warning>
<simpara>A known issue causes the maximum size of retained backups to be up to 10 GB greater than the configured value.</simpara>
</warning>
</listitem>
<listitem>
<simpara>Create the cron job defined by the CRD by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc create -f etcd-recurring-backup.yaml</programlisting>
</listitem>
<listitem>
<simpara>To find the created cron job, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get cronjob -n openshift-etcd</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
<section xml:id="replacing-unhealthy-etcd-member">
<title>Replacing an unhealthy etcd member</title>

<simpara>This document describes the process to replace a single unhealthy etcd member.</simpara>
<simpara>This process depends on whether the etcd member is unhealthy because the machine is not running or the node is not ready, or whether it is unhealthy because the etcd pod is crashlooping.</simpara>
<note>
<simpara>If you have lost the majority of your control plane hosts, follow the disaster recovery procedure to <link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">restore to a previous cluster state</link> instead of this procedure.</simpara>
<simpara>If the control plane certificates are not valid on the member being replaced, then you must follow the procedure to <link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-3-expired-certs.xml#dr-recovering-expired-certs">recover from expired control plane certificates</link> instead of this procedure.</simpara>
<simpara>If a control plane node is lost and a new one is created, the etcd cluster Operator handles generating the new TLS certificates and adding the node as an etcd member.</simpara>
</note>
<section xml:id="_prerequisites_3">
<title>Prerequisites</title>
<itemizedlist>
<listitem>
<simpara>Take an <link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.xml#backing-up-etcd-data_backup-etcd">etcd backup</link> prior to replacing an unhealthy etcd member.</simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="restore-identify-unhealthy-etcd-member_replacing-unhealthy-etcd-member">
<title>Identifying an unhealthy etcd member</title>
<simpara>You can identify if your cluster has an unhealthy etcd member.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Check the status of the <literal>EtcdMembersAvailable</literal> status condition using the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="EtcdMembersAvailable")]}{.message}{"\n"}'</programlisting>
</listitem>
<listitem>
<simpara>Review the output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">2 of 3 members are available, ip-10-0-131-183.ec2.internal is unhealthy</programlisting>
<simpara>This example output shows that the <literal>ip-10-0-131-183.ec2.internal</literal> etcd member is unhealthy.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="restore-determine-state-etcd-member_replacing-unhealthy-etcd-member">
<title>Determining the state of the unhealthy etcd member</title>
<simpara>The steps to replace an unhealthy etcd member depend on which of the following states your etcd member is in:</simpara>
<itemizedlist>
<listitem>
<simpara>The machine is not running or the node is not ready</simpara>
</listitem>
<listitem>
<simpara>The etcd pod is crashlooping</simpara>
</listitem>
</itemizedlist>
<simpara>This procedure determines which state your etcd member is in. This enables you to know which procedure to follow to replace the unhealthy etcd member.</simpara>
<note>
<simpara>If you are aware that the machine is not running or the node is not ready, but you expect it to return to a healthy state soon, then you do not need to perform a procedure to replace the etcd member. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have identified an unhealthy etcd member.</simpara>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Determine if the <emphasis role="strong">machine is not running</emphasis>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -A -ojsonpath='{range .items[*]}{@.status.nodeRef.name}{"\t"}{@.status.providerStatus.instanceState}{"\n"}' | grep -v running</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ip-10-0-131-183.ec2.internal  stopped <co xml:id="CO73-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO73-1">
<para>This output lists the node and the status of the node&#8217;s machine. If the status is anything other than <literal>running</literal>, then the <emphasis role="strong">machine is not running</emphasis>.</para>
</callout>
</calloutlist>
<simpara>If the <emphasis role="strong">machine is not running</emphasis>, then follow the <emphasis>Replacing an unhealthy etcd member whose machine is not running or whose node is not ready</emphasis> procedure.</simpara>
</listitem>
<listitem>
<simpara>Determine if the <emphasis role="strong">node is not ready</emphasis>.</simpara>
<simpara>If either of the following scenarios are true, then the <emphasis role="strong">node is not ready</emphasis>.</simpara>
<itemizedlist>
<listitem>
<simpara>If the machine is running, then check whether the node is unreachable:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -o jsonpath='{range .items[*]}{"\n"}{.metadata.name}{"\t"}{range .spec.taints[*]}{.key}{" "}' | grep unreachable</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ip-10-0-131-183.ec2.internal	node-role.kubernetes.io/master node.kubernetes.io/unreachable node.kubernetes.io/unreachable <co xml:id="CO74-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO74-1">
<para>If the node is listed with an <literal>unreachable</literal> taint, then the <emphasis role="strong">node is not ready</emphasis>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>If the node is still reachable, then check whether the node is listed as <literal>NotReady</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/master | grep "NotReady"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">ip-10-0-131-183.ec2.internal   NotReady   master   122m   v1.28.5 <co xml:id="CO75-1"/></programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO75-1">
<para>If the node is listed as <literal>NotReady</literal>, then the <emphasis role="strong">node is not ready</emphasis>.</para>
</callout>
</calloutlist>
</listitem>
</itemizedlist>
<simpara>If the <emphasis role="strong">node is not ready</emphasis>, then follow the <emphasis>Replacing an unhealthy etcd member whose machine is not running or whose node is not ready</emphasis> procedure.</simpara>
</listitem>
<listitem>
<simpara>Determine if the <emphasis role="strong">etcd pod is crashlooping</emphasis>.</simpara>
<simpara>If the machine is running and the node is ready, then check whether the etcd pod is crashlooping.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Verify that all control plane nodes are listed as <literal>Ready</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -l node-role.kubernetes.io/master</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                           STATUS   ROLES    AGE     VERSION
ip-10-0-131-183.ec2.internal   Ready    master   6h13m   v1.28.5
ip-10-0-164-97.ec2.internal    Ready    master   6h13m   v1.28.5
ip-10-0-154-204.ec2.internal   Ready    master   6h13m   v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Check whether the status of an etcd pod is either <literal>Error</literal> or <literal>CrashloopBackoff</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-131-183.ec2.internal                2/3     Error       7          6h9m <co xml:id="CO76-1"/>
etcd-ip-10-0-164-97.ec2.internal                 3/3     Running     0          6h6m
etcd-ip-10-0-154-204.ec2.internal                3/3     Running     0          6h6m</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO76-1">
<para>Since this status of this pod is <literal>Error</literal>, then the <emphasis role="strong">etcd pod is crashlooping</emphasis>.</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
<simpara>If the <emphasis role="strong">etcd pod is crashlooping</emphasis>, then follow the <emphasis>Replacing an unhealthy etcd member whose etcd pod is crashlooping</emphasis> procedure.</simpara>
</listitem>
</orderedlist>
</section>
<section xml:id="_replacing_the_unhealthy_etcd_member">
<title>Replacing the unhealthy etcd member</title>
<simpara>Depending on the state of your unhealthy etcd member, use one of the following procedures:</simpara>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/replacing-unhealthy-etcd-member.xml#restore-replace-stopped-etcd-member_replacing-unhealthy-etcd-member">Replacing an unhealthy etcd member whose machine is not running or whose node is not ready</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/replacing-unhealthy-etcd-member.xml#restore-replace-crashlooping-etcd-member_replacing-unhealthy-etcd-member">Replacing an unhealthy etcd member whose etcd pod is crashlooping</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../backup_and_restore/control_plane_backup_and_restore/replacing-unhealthy-etcd-member.xml#restore-replace-stopped-baremetal-etcd-member_replacing-unhealthy-etcd-member">Replacing an unhealthy stopped baremetal etcd member</link></simpara>
</listitem>
</itemizedlist>
<section xml:id="restore-replace-stopped-etcd-member_replacing-unhealthy-etcd-member">
<title>Replacing an unhealthy etcd member whose machine is not running or whose node is not ready</title>
<simpara>This procedure details the steps to replace an etcd member that is unhealthy either because the machine is not running or because the node is not ready.</simpara>
<note>
<simpara>If your cluster uses a control plane machine set, see "Recovering a degraded etcd Operator" in "Troubleshooting the control plane machine set" for a more simple etcd recovery procedure.</simpara>
</note>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have identified the unhealthy etcd member.</simpara>
</listitem>
<listitem>
<simpara>You have verified that either the machine is not running or the node is not ready.</simpara>
<important>
<simpara>You must wait if the other control plane nodes are powered off. The control plane nodes must remain powered off until the replacement of an unhealthy etcd member is complete.</simpara>
</important>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have taken an etcd backup.</simpara>
<important>
<simpara>It is important to take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues.</simpara>
</important>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Remove the unhealthy member.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Choose a pod that is <emphasis>not</emphasis> on the affected node:</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-131-183.ec2.internal                3/3     Running     0          123m
etcd-ip-10-0-164-97.ec2.internal                 3/3     Running     0          123m
etcd-ip-10-0-154-204.ec2.internal                3/3     Running     0          124m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Connect to the running etcd container, passing in the name of a pod that is not on the affected node:</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-ip-10-0-154-204.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>View the member list:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 6fc1e7c9db35841d | started | ip-10-0-131-183.ec2.internal | https://10.0.131.183:2380 | https://10.0.131.183:2379 |
| 757b6793e2408b6c | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| ca8c2990a0aa29d1 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+</programlisting>
</para>
</formalpara>
<simpara>Take note of the ID and the name of the unhealthy etcd member, because these values are needed later in the procedure. The <literal>$ etcdctl endpoint health</literal> command will list the removed member until the procedure of replacement is finished and a new member is added.</simpara>
</listitem>
<listitem>
<simpara>Remove the unhealthy etcd member by providing the ID to the <literal>etcdctl member remove</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member remove 6fc1e7c9db35841d</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Member 6fc1e7c9db35841d removed from cluster ead669ce1fbfb346</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the member list again and verify that the member was removed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 757b6793e2408b6c | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| ca8c2990a0aa29d1 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+</programlisting>
</para>
</formalpara>
<simpara>You can now exit the node shell.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Turn off the quorum guard by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'</programlisting>
<simpara>This command ensures that you can successfully re-create secrets and roll out the static pods.</simpara>
<important>
<simpara>After you turn off the quorum guard, the cluster might be unreachable for a short time while the remaining etcd instances reboot to reflect the configuration change.</simpara>
</important>
<note>
<simpara>etcd cannot tolerate any additional member failure when running with two members. Restarting either remaining member breaks the quorum and causes downtime in your cluster. The quorum guard protects etcd from restarts due to configuration changes that could cause downtime, so it must be disabled to complete this procedure.</simpara>
</note>
</listitem>
<listitem>
<simpara>Delete the affected node by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete node &lt;node_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>Remove the old secrets for the unhealthy etcd member that was removed.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>List the secrets for the unhealthy etcd member that was removed.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secrets -n openshift-etcd | grep ip-10-0-131-183.ec2.internal <co xml:id="CO77-1"/></programlisting>
<calloutlist>
<callout arearefs="CO77-1">
<para>Pass in the name of the unhealthy etcd member that you took note of earlier in this procedure.</para>
</callout>
</calloutlist>
<simpara>There is a peer, serving, and metrics secret as shown in the following output:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-peer-ip-10-0-131-183.ec2.internal              kubernetes.io/tls                     2      47m
etcd-serving-ip-10-0-131-183.ec2.internal           kubernetes.io/tls                     2      47m
etcd-serving-metrics-ip-10-0-131-183.ec2.internal   kubernetes.io/tls                     2      47m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the secrets for the unhealthy etcd member that was removed.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Delete the peer secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret -n openshift-etcd etcd-peer-ip-10-0-131-183.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>Delete the serving secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret -n openshift-etcd etcd-serving-ip-10-0-131-183.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>Delete the metrics secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret -n openshift-etcd etcd-serving-metrics-ip-10-0-131-183.ec2.internal</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete and re-create the control plane machine. After this machine is re-created, a new revision is forced and etcd scales up automatically.</simpara>
<simpara>If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps. Otherwise, you must create the new master using the same method that was used to originally create it.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Obtain the machine for the unhealthy member.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-0                  Running   m4.xlarge   us-east-1   us-east-1a   3h37m   ip-10-0-131-183.ec2.internal   aws:///us-east-1a/i-0ec2782f8287dfb7e   stopped <co xml:id="CO78-1"/>
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-154-204.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-164-97.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba   running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO78-1">
<para>This is the control plane machine for the unhealthy node, <literal>ip-10-0-131-183.ec2.internal</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the machine configuration to a file on your file system:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine clustername-8qw5l-master-0 \ <co xml:id="CO79-1"/>
    -n openshift-machine-api \
    -o yaml \
    &gt; new-master-machine.yaml</programlisting>
<calloutlist>
<callout arearefs="CO79-1">
<para>Specify the name of the control plane machine for the unhealthy node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>new-master-machine.yaml</literal> file that was created in the previous step to assign a new name and remove unnecessary fields.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Remove the entire <literal>status</literal> section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">status:
  addresses:
  - address: 10.0.131.183
    type: InternalIP
  - address: ip-10-0-131-183.ec2.internal
    type: InternalDNS
  - address: ip-10-0-131-183.ec2.internal
    type: Hostname
  lastUpdated: "2020-04-20T17:44:29Z"
  nodeRef:
    kind: Node
    name: ip-10-0-131-183.ec2.internal
    uid: acca4411-af0d-4387-b73e-52b2484295ad
  phase: Running
  providerStatus:
    apiVersion: awsproviderconfig.openshift.io/v1beta1
    conditions:
    - lastProbeTime: "2020-04-20T16:53:50Z"
      lastTransitionTime: "2020-04-20T16:53:50Z"
      message: machine successfully created
      reason: MachineCreationSucceeded
      status: "True"
      type: MachineCreation
    instanceId: i-0fdb85790d76d0c3f
    instanceState: stopped
    kind: AWSMachineProviderStatus</programlisting>
</listitem>
<listitem>
<simpara>Change the <literal>metadata.name</literal> field to a new name.</simpara>
<simpara>It is recommended to keep the same base name as the old machine and change the ending number to the next available number. In this example, <literal>clustername-8qw5l-master-0</literal> is changed to <literal>clustername-8qw5l-master-3</literal>.</simpara>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
  name: clustername-8qw5l-master-3
  ...</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>spec.providerID</literal> field:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  providerID: aws:///us-east-1a/i-0fdb85790d76d0c3f</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete the machine of the unhealthy member:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete machine -n openshift-machine-api clustername-8qw5l-master-0 <co xml:id="CO80-1"/></programlisting>
<calloutlist>
<callout arearefs="CO80-1">
<para>Specify the name of the control plane machine for the unhealthy node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the machine was deleted:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-154-204.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-164-97.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba   running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Create the new machine using the <literal>new-master-machine.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f new-master-machine.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the new machine has been created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE          TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running        m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-154-204.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running        m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-164-97.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba   running
clustername-8qw5l-master-3                  Provisioning   m4.xlarge   us-east-1   us-east-1a   85s     ip-10-0-133-53.ec2.internal    aws:///us-east-1a/i-015b0888fe17bc2c8   running <co xml:id="CO81-1"/>
clustername-8qw5l-worker-us-east-1a-wbtgd   Running        m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running        m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running        m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO81-1">
<para>The new machine, <literal>clustername-8qw5l-master-3</literal> is being created and is ready once the phase changes from <literal>Provisioning</literal> to <literal>Running</literal>.</para>
</callout>
</calloutlist>
<simpara>It might take a few minutes for the new machine to be created. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Turn the quorum guard back on by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'</programlisting>
</listitem>
<listitem>
<simpara>You can verify that the <literal>unsupportedConfigOverrides</literal> section is removed from the object by entering this command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd/cluster -oyaml</programlisting>
</listitem>
<listitem>
<simpara>If you are using single-node OpenShift, restart the node. Otherwise, you might encounter the following error in the etcd cluster Operator:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">EtcdCertSignerControllerDegraded: [Operation cannot be fulfilled on secrets "etcd-peer-sno-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on secrets "etcd-serving-sno-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on secrets "etcd-serving-metrics-sno-0": the object has been modified; please apply your changes to the latest version and try again]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that all etcd pods are running properly.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-133-53.ec2.internal                 3/3     Running     0          7m49s
etcd-ip-10-0-164-97.ec2.internal                 3/3     Running     0          123m
etcd-ip-10-0-154-204.ec2.internal                3/3     Running     0          124m</programlisting>
</para>
</formalpara>
<simpara>If the output from the previous command only lists two pods, you can manually force an etcd redeployment. In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <co xml:id="CO82-1"/></programlisting>
<calloutlist>
<callout arearefs="CO82-1">
<para>The <literal>forceRedeploymentReason</literal> value must be unique, which is why a timestamp is appended.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that there are exactly three etcd members.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Connect to the running etcd container, passing in the name of a pod that was not on the affected node:</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-ip-10-0-154-204.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>View the member list:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 5eb0d6b8ca24730c | started |  ip-10-0-133-53.ec2.internal |  https://10.0.133.53:2380 |  https://10.0.133.53:2379 |
| 757b6793e2408b6c | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| ca8c2990a0aa29d1 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+</programlisting>
</para>
</formalpara>
<simpara>If the output from the previous command lists more than three etcd members, you must carefully remove the unwanted member.</simpara>
<warning>
<simpara>Be sure to remove the correct etcd member; removing a good etcd member might lead to quorum loss.</simpara>
</warning>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<itemizedlist role="_additional-resources">
<title>Additional resources</title>
<listitem>
<simpara><link xlink:href="../../machine_management/control_plane_machine_management/cpmso-troubleshooting.xml#cpmso-ts-etcd-degraded_cpmso-troubleshooting">Recovering a degraded etcd Operator</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="restore-replace-crashlooping-etcd-member_replacing-unhealthy-etcd-member">
<title>Replacing an unhealthy etcd member whose etcd pod is crashlooping</title>
<simpara>This procedure details the steps to replace an etcd member that is unhealthy because the etcd pod is crashlooping.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have identified the unhealthy etcd member.</simpara>
</listitem>
<listitem>
<simpara>You have verified that the etcd pod is crashlooping.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have taken an etcd backup.</simpara>
<important>
<simpara>It is important to take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues.</simpara>
</important>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Stop the crashlooping etcd pod.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Debug the node that is crashlooping.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc debug node/ip-10-0-131-183.ec2.internal <co xml:id="CO83-1"/></programlisting>
<calloutlist>
<callout arearefs="CO83-1">
<para>Replace this with the name of the unhealthy node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Change your root directory to <literal>/host</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# chroot /host</programlisting>
</listitem>
<listitem>
<simpara>Move the existing etcd pod file out of the kubelet manifest directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# mkdir /var/lib/etcd-backup</programlisting>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# mv /etc/kubernetes/manifests/etcd-pod.yaml /var/lib/etcd-backup/</programlisting>
</listitem>
<listitem>
<simpara>Move the etcd data directory to a different location:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# mv /var/lib/etcd/ /tmp</programlisting>
<simpara>You can now exit the node shell.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Remove the unhealthy member.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Choose a pod that is <emphasis>not</emphasis> on the affected node.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-131-183.ec2.internal                2/3     Error       7          6h9m
etcd-ip-10-0-164-97.ec2.internal                 3/3     Running     0          6h6m
etcd-ip-10-0-154-204.ec2.internal                3/3     Running     0          6h6m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Connect to the running etcd container, passing in the name of a pod that is not on the affected node.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-ip-10-0-154-204.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>View the member list:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| 62bcf33650a7170a | started | ip-10-0-131-183.ec2.internal | https://10.0.131.183:2380 | https://10.0.131.183:2379 |
| b78e2856655bc2eb | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| d022e10b498760d5 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+</programlisting>
</para>
</formalpara>
<simpara>Take note of the ID and the name of the unhealthy etcd member, because these values are needed later in the procedure.</simpara>
</listitem>
<listitem>
<simpara>Remove the unhealthy etcd member by providing the ID to the <literal>etcdctl member remove</literal> command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member remove 62bcf33650a7170a</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Member 62bcf33650a7170a removed from cluster ead669ce1fbfb346</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the member list again and verify that the member was removed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+------------------------------+---------------------------+---------------------------+
|        ID        | STATUS  |             NAME             |        PEER ADDRS         |       CLIENT ADDRS        |
+------------------+---------+------------------------------+---------------------------+---------------------------+
| b78e2856655bc2eb | started |  ip-10-0-164-97.ec2.internal |  https://10.0.164.97:2380 |  https://10.0.164.97:2379 |
| d022e10b498760d5 | started | ip-10-0-154-204.ec2.internal | https://10.0.154.204:2380 | https://10.0.154.204:2379 |
+------------------+---------+------------------------------+---------------------------+---------------------------+</programlisting>
</para>
</formalpara>
<simpara>You can now exit the node shell.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Turn off the quorum guard by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'</programlisting>
<simpara>This command ensures that you can successfully re-create secrets and roll out the static pods.</simpara>
</listitem>
<listitem>
<simpara>Remove the old secrets for the unhealthy etcd member that was removed.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>List the secrets for the unhealthy etcd member that was removed.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secrets -n openshift-etcd | grep ip-10-0-131-183.ec2.internal <co xml:id="CO84-1"/></programlisting>
<calloutlist>
<callout arearefs="CO84-1">
<para>Pass in the name of the unhealthy etcd member that you took note of earlier in this procedure.</para>
</callout>
</calloutlist>
<simpara>There is a peer, serving, and metrics secret as shown in the following output:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-peer-ip-10-0-131-183.ec2.internal              kubernetes.io/tls                     2      47m
etcd-serving-ip-10-0-131-183.ec2.internal           kubernetes.io/tls                     2      47m
etcd-serving-metrics-ip-10-0-131-183.ec2.internal   kubernetes.io/tls                     2      47m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the secrets for the unhealthy etcd member that was removed.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Delete the peer secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret -n openshift-etcd etcd-peer-ip-10-0-131-183.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>Delete the serving secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret -n openshift-etcd etcd-serving-ip-10-0-131-183.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>Delete the metrics secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret -n openshift-etcd etcd-serving-metrics-ip-10-0-131-183.ec2.internal</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Force etcd redeployment.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "single-master-recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <co xml:id="CO85-1"/></programlisting>
<calloutlist>
<callout arearefs="CO85-1">
<para>The <literal>forceRedeploymentReason</literal> value must be unique, which is why a timestamp is appended.</para>
</callout>
</calloutlist>
<simpara>When the etcd cluster Operator performs a redeployment, it ensures that all control plane nodes have a functioning etcd pod.</simpara>
</listitem>
<listitem>
<simpara>Turn the quorum guard back on by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'</programlisting>
</listitem>
<listitem>
<simpara>You can verify that the <literal>unsupportedConfigOverrides</literal> section is removed from the object by entering this command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd/cluster -oyaml</programlisting>
</listitem>
<listitem>
<simpara>If you are using single-node OpenShift, restart the node. Otherwise, you might encounter the following error in the etcd cluster Operator:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">EtcdCertSignerControllerDegraded: [Operation cannot be fulfilled on secrets "etcd-peer-sno-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on secrets "etcd-serving-sno-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on secrets "etcd-serving-metrics-sno-0": the object has been modified; please apply your changes to the latest version and try again]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<itemizedlist>
<title>Verification</title>
<listitem>
<simpara>Verify that the new member is available and healthy.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Connect to the running etcd container again.</simpara>
<simpara>In a terminal that has access to the cluster as a cluster-admin user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-ip-10-0-154-204.ec2.internal</programlisting>
</listitem>
<listitem>
<simpara>Verify that all members are healthy:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl endpoint health</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">https://10.0.131.183:2379 is healthy: successfully committed proposal: took = 16.671434ms
https://10.0.154.204:2379 is healthy: successfully committed proposal: took = 16.698331ms
https://10.0.164.97:2379 is healthy: successfully committed proposal: took = 16.621645ms</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
<section xml:id="restore-replace-stopped-baremetal-etcd-member_replacing-unhealthy-etcd-member">
<title>Replacing an unhealthy bare metal etcd member whose machine is not running or whose node is not ready</title>
<simpara>This procedure details the steps to replace a bare metal etcd member that is unhealthy either because the machine is not running or because the node is not ready.</simpara>
<simpara>If you are running installer-provisioned infrastructure or you used the Machine API to create your machines, follow these steps. Otherwise you must create the new control plane node using the same method that was used to originally create it.</simpara>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>You have identified the unhealthy bare metal etcd member.</simpara>
</listitem>
<listitem>
<simpara>You have verified that either the machine is not running or the node is not ready.</simpara>
</listitem>
<listitem>
<simpara>You have access to the cluster as a user with the <literal>cluster-admin</literal> role.</simpara>
</listitem>
<listitem>
<simpara>You have taken an etcd backup.</simpara>
<important>
<simpara>You must take an etcd backup before performing this procedure so that your cluster can be restored if you encounter any issues.</simpara>
</important>
</listitem>
</itemizedlist>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Verify and remove the unhealthy member.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Choose a pod that is <emphasis>not</emphasis> on the affected node:</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-openshift-control-plane-0   5/5   Running   11   3h56m   192.168.10.9   openshift-control-plane-0  &lt;none&gt;           &lt;none&gt;
etcd-openshift-control-plane-1   5/5   Running   0    3h54m   192.168.10.10   openshift-control-plane-1   &lt;none&gt;           &lt;none&gt;
etcd-openshift-control-plane-2   5/5   Running   0    3h58m   192.168.10.11   openshift-control-plane-2   &lt;none&gt;           &lt;none&gt;</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Connect to the running etcd container, passing in the name of a pod that is not on the affected node:</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-openshift-control-plane-0</programlisting>
</listitem>
<listitem>
<simpara>View the member list:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+--------------------+---------------------------+---------------------------+---------------------+
| ID               | STATUS  | NAME                      | PEER ADDRS                  | CLIENT ADDRS                | IS LEARNER |
+------------------+---------+--------------------+---------------------------+---------------------------+---------------------+
| 7a8197040a5126c8 | started | openshift-control-plane-2 | https://192.168.10.11:2380/ | https://192.168.10.11:2379/ | false |
| 8d5abe9669a39192 | started | openshift-control-plane-1 | https://192.168.10.10:2380/ | https://192.168.10.10:2379/ | false |
| cc3830a72fc357f9 | started | openshift-control-plane-0 | https://192.168.10.9:2380/ | https://192.168.10.9:2379/   | false |
+------------------+---------+--------------------+---------------------------+---------------------------+---------------------+</programlisting>
</para>
</formalpara>
<simpara>Take note of the ID and the name of the unhealthy etcd member, because these values are required later in the procedure. The <literal>etcdctl endpoint health</literal> command will list the removed member until the replacement procedure is completed and the new member is added.</simpara>
</listitem>
<listitem>
<simpara>Remove the unhealthy etcd member by providing the ID to the <literal>etcdctl member remove</literal> command:</simpara>
<warning>
<simpara>Be sure to remove the correct etcd member; removing a good etcd member might lead to quorum loss.</simpara>
</warning>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member remove 7a8197040a5126c8</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">Member 7a8197040a5126c8 removed from cluster b23536c33f2cdd1b</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>View the member list again and verify that the member was removed:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+--------------------+---------------------------+---------------------------+-------------------------+
| ID               | STATUS  | NAME                      | PEER ADDRS                  | CLIENT ADDRS                | IS LEARNER |
+------------------+---------+--------------------+---------------------------+---------------------------+-------------------------+
| 7a8197040a5126c8 | started | openshift-control-plane-2 | https://192.168.10.11:2380/ | https://192.168.10.11:2379/ | false |
| 8d5abe9669a39192 | started | openshift-control-plane-1 | https://192.168.10.10:2380/ | https://192.168.10.10:2379/ | false |
+------------------+---------+--------------------+---------------------------+---------------------------+-------------------------+</programlisting>
</para>
</formalpara>
<simpara>You can now exit the node shell.</simpara>
<important>
<simpara>After you remove the member, the cluster might be unreachable for a short time while the remaining etcd instances reboot.</simpara>
</important>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Turn off the quorum guard by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'</programlisting>
<simpara>This command ensures that you can successfully re-create secrets and roll out the static pods.</simpara>
</listitem>
<listitem>
<simpara>Remove the old secrets for the unhealthy etcd member that was removed by running the following commands.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>List the secrets for the unhealthy etcd member that was removed.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get secrets -n openshift-etcd | grep openshift-control-plane-2</programlisting>
<simpara>Pass in the name of the unhealthy etcd member that you took note of earlier in this procedure.</simpara>
<simpara>There is a peer, serving, and metrics secret as shown in the following output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">etcd-peer-openshift-control-plane-2             kubernetes.io/tls   2   134m
etcd-serving-metrics-openshift-control-plane-2  kubernetes.io/tls   2   134m
etcd-serving-openshift-control-plane-2          kubernetes.io/tls   2   134m</programlisting>
</listitem>
<listitem>
<simpara>Delete the secrets for the unhealthy etcd member that was removed.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Delete the peer secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret etcd-peer-openshift-control-plane-2 -n openshift-etcd

secret "etcd-peer-openshift-control-plane-2" deleted</programlisting>
</listitem>
<listitem>
<simpara>Delete the serving secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret etcd-serving-metrics-openshift-control-plane-2 -n openshift-etcd

secret "etcd-serving-metrics-openshift-control-plane-2" deleted</programlisting>
</listitem>
<listitem>
<simpara>Delete the metrics secret:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete secret etcd-serving-openshift-control-plane-2 -n openshift-etcd

secret "etcd-serving-openshift-control-plane-2" deleted</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete the control plane machine.</simpara>
<simpara>If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps. Otherwise, you must create the new control plane node using the same method that was used to originally create it.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Obtain the machine for the unhealthy member.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                              PHASE     TYPE   REGION   ZONE   AGE     NODE                               PROVIDERID                                                                                              STATE
examplecluster-control-plane-0    Running                          3h11m   openshift-control-plane-0   baremetalhost:///openshift-machine-api/openshift-control-plane-0/da1ebe11-3ff2-41c5-b099-0aa41222964e   externally provisioned <co xml:id="CO86-1"/>
examplecluster-control-plane-1    Running                          3h11m   openshift-control-plane-1   baremetalhost:///openshift-machine-api/openshift-control-plane-1/d9f9acbc-329c-475e-8d81-03b20280a3e1   externally provisioned
examplecluster-control-plane-2    Running                          3h11m   openshift-control-plane-2   baremetalhost:///openshift-machine-api/openshift-control-plane-2/3354bdac-61d8-410f-be5b-6a395b056135   externally provisioned
examplecluster-compute-0          Running                          165m    openshift-compute-0         baremetalhost:///openshift-machine-api/openshift-compute-0/3d685b81-7410-4bb3-80ec-13a31858241f         provisioned
examplecluster-compute-1          Running                          165m    openshift-compute-1         baremetalhost:///openshift-machine-api/openshift-compute-1/0fdae6eb-2066-4241-91dc-e7ea72ab13b9         provisioned</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO86-1">
<para>This is the control plane machine for the unhealthy node, <literal>examplecluster-control-plane-2</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the machine configuration to a file on your file system:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine examplecluster-control-plane-2 \ <co xml:id="CO87-1"/>
    -n openshift-machine-api \
    -o yaml \
    &gt; new-master-machine.yaml</programlisting>
<calloutlist>
<callout arearefs="CO87-1">
<para>Specify the name of the control plane machine for the unhealthy node.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>new-master-machine.yaml</literal> file that was created in the previous step to assign a new name and remove unnecessary fields.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Remove the entire <literal>status</literal> section:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">status:
  addresses:
  - address: ""
    type: InternalIP
  - address: fe80::4adf:37ff:feb0:8aa1%ens1f1.373
    type: InternalDNS
  - address: fe80::4adf:37ff:feb0:8aa1%ens1f1.371
    type: Hostname
  lastUpdated: "2020-04-20T17:44:29Z"
  nodeRef:
    kind: Machine
    name: fe80::4adf:37ff:feb0:8aa1%ens1f1.372
    uid: acca4411-af0d-4387-b73e-52b2484295ad
  phase: Running
  providerStatus:
    apiVersion: machine.openshift.io/v1beta1
    conditions:
    - lastProbeTime: "2020-04-20T16:53:50Z"
      lastTransitionTime: "2020-04-20T16:53:50Z"
      message: machine successfully created
      reason: MachineCreationSucceeded
      status: "True"
      type: MachineCreation
    instanceId: i-0fdb85790d76d0c3f
    instanceState: stopped
    kind: Machine</programlisting>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Change the <literal>metadata.name</literal> field to a new name.</simpara>
<simpara>It is recommended to keep the same base name as the old machine and change the ending number to the next available number. In this example, <literal>examplecluster-control-plane-2</literal> is changed to <literal>examplecluster-control-plane-3</literal>.</simpara>
<simpara>For example:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
  name: examplecluster-control-plane-3
  ...</programlisting>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Remove the <literal>spec.providerID</literal> field:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  providerID: baremetalhost:///openshift-machine-api/openshift-control-plane-2/3354bdac-61d8-410f-be5b-6a395b056135</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>metadata.annotations</literal> and <literal>metadata.generation</literal> fields:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  annotations:
    machine.openshift.io/instance-state: externally provisioned
  ...
  generation: 2</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>spec.conditions</literal>, <literal>spec.lastUpdated</literal>, <literal>spec.nodeRef</literal> and <literal>spec.phase</literal> fields:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">  lastTransitionTime: "2022-08-03T08:40:36Z"
message: 'Drain operation currently blocked by: [{Name:EtcdQuorumOperator Owner:clusteroperator/etcd}]'
reason: HookPresent
severity: Warning
status: "False"

type: Drainable
lastTransitionTime: "2022-08-03T08:39:55Z"
status: "True"
type: InstanceExists

lastTransitionTime: "2022-08-03T08:36:37Z"
status: "True"
type: Terminable
lastUpdated: "2022-08-03T08:40:36Z"
nodeRef:
kind: Node
name: openshift-control-plane-2
uid: 788df282-6507-4ea2-9a43-24f237ccbc3c
phase: Running</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Ensure that the Bare Metal Operator is available by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get clusteroperator baremetal</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME        VERSION   AVAILABLE   PROGRESSING   DEGRADED   SINCE   MESSAGE
baremetal   4.14.0    True        False         False      3d15h</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Remove the old <literal>BareMetalHost</literal> object by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete bmh openshift-control-plane-2 -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">baremetalhost.metal3.io "openshift-control-plane-2" deleted</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Delete the machine of the unhealthy member by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete machine -n openshift-machine-api examplecluster-control-plane-2</programlisting>
<simpara>After you remove the <literal>BareMetalHost</literal> and <literal>Machine</literal> objects, then the <literal>Machine</literal> controller automatically deletes the <literal>Node</literal> object.</simpara>
<simpara>If deletion of the machine is delayed for any reason or the command is obstructed and delayed, you can force deletion by removing the machine object finalizer field.</simpara>
<important>
<simpara>Do not interrupt machine deletion by pressing <literal>Ctrl+c</literal>. You must allow the command to proceed to completion. Open a new terminal window to edit and delete the finalizer fields.</simpara>
</important>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Edit the machine configuration by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc edit machine -n openshift-machine-api examplecluster-control-plane-2</programlisting>
</listitem>
<listitem>
<simpara>Delete the following fields in the <literal>Machine</literal> custom resource, and then save the updated file:</simpara>
<programlisting language="yaml" linenumbering="unnumbered">finalizers:
- machine.machine.openshift.io</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">machine.machine.openshift.io/examplecluster-control-plane-2 edited</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Verify that the machine was deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                              PHASE     TYPE   REGION   ZONE   AGE     NODE                                 PROVIDERID                                                                                       STATE
examplecluster-control-plane-0    Running                          3h11m   openshift-control-plane-0   baremetalhost:///openshift-machine-api/openshift-control-plane-0/da1ebe11-3ff2-41c5-b099-0aa41222964e   externally provisioned
examplecluster-control-plane-1    Running                          3h11m   openshift-control-plane-1   baremetalhost:///openshift-machine-api/openshift-control-plane-1/d9f9acbc-329c-475e-8d81-03b20280a3e1   externally provisioned
examplecluster-compute-0          Running                          165m    openshift-compute-0         baremetalhost:///openshift-machine-api/openshift-compute-0/3d685b81-7410-4bb3-80ec-13a31858241f         provisioned
examplecluster-compute-1          Running                          165m    openshift-compute-1         baremetalhost:///openshift-machine-api/openshift-compute-1/0fdae6eb-2066-4241-91dc-e7ea72ab13b9         provisioned</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the node has been deleted by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes

NAME                     STATUS ROLES   AGE   VERSION
openshift-control-plane-0 Ready master 3h24m v1.28.5
openshift-control-plane-1 Ready master 3h24m v1.28.5
openshift-compute-0       Ready worker 176m v1.28.5
openshift-compute-1       Ready worker 176m v1.28.5</programlisting>
</listitem>
<listitem>
<simpara>Create the new <literal>BareMetalHost</literal> object and the secret to store the BMC credentials:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ cat &lt;&lt;EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: openshift-control-plane-2-bmc-secret
  namespace: openshift-machine-api
data:
  password: &lt;password&gt;
  username: &lt;username&gt;
type: Opaque
---
apiVersion: metal3.io/v1alpha1
kind: BareMetalHost
metadata:
  name: openshift-control-plane-2
  namespace: openshift-machine-api
spec:
  automatedCleaningMode: disabled
  bmc:
    address: redfish://10.46.61.18:443/redfish/v1/Systems/1
    credentialsName: openshift-control-plane-2-bmc-secret
    disableCertificateVerification: true
  bootMACAddress: 48:df:37:b0:8a:a0
  bootMode: UEFI
  externallyProvisioned: false
  online: true
  rootDeviceHints:
    deviceName: /dev/disk/by-id/scsi-&lt;serial_number&gt;
  userData:
    name: master-user-data-managed
    namespace: openshift-machine-api
EOF</programlisting>
<note>
<simpara>The username and password can be found from the other bare metal host&#8217;s secrets. The protocol to use in <literal>bmc:address</literal> can be taken from other bmh objects.</simpara>
</note>
<important>
<simpara>If you reuse the <literal>BareMetalHost</literal> object definition from an existing control plane host, do not leave the <literal>externallyProvisioned</literal> field set to <literal>true</literal>.</simpara>
<simpara>Existing control plane <literal>BareMetalHost</literal> objects may have the <literal>externallyProvisioned</literal> flag set to <literal>true</literal> if they were provisioned by the OpenShift Container Platform installation program.</simpara>
</important>
<simpara>After the inspection is complete, the <literal>BareMetalHost</literal> object is created and available to be provisioned.</simpara>
</listitem>
<listitem>
<simpara>Verify the creation process using available <literal>BareMetalHost</literal> objects:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get bmh -n openshift-machine-api

NAME                      STATE                  CONSUMER                      ONLINE ERROR   AGE
openshift-control-plane-0 externally provisioned examplecluster-control-plane-0 true         4h48m
openshift-control-plane-1 externally provisioned examplecluster-control-plane-1 true         4h48m
openshift-control-plane-2 available              examplecluster-control-plane-3 true         47m
openshift-compute-0       provisioned            examplecluster-compute-0       true         4h48m
openshift-compute-1       provisioned            examplecluster-compute-1       true         4h48m</programlisting>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Create the new control plane machine using the <literal>new-master-machine.yaml</literal> file:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f new-master-machine.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the new machine has been created:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                   PHASE     TYPE   REGION   ZONE   AGE     NODE                              PROVIDERID                                                                                            STATE
examplecluster-control-plane-0         Running                          3h11m   openshift-control-plane-0   baremetalhost:///openshift-machine-api/openshift-control-plane-0/da1ebe11-3ff2-41c5-b099-0aa41222964e   externally provisioned <co xml:id="CO88-1"/>
examplecluster-control-plane-1         Running                          3h11m   openshift-control-plane-1   baremetalhost:///openshift-machine-api/openshift-control-plane-1/d9f9acbc-329c-475e-8d81-03b20280a3e1   externally provisioned
examplecluster-control-plane-2         Running                          3h11m   openshift-control-plane-2   baremetalhost:///openshift-machine-api/openshift-control-plane-2/3354bdac-61d8-410f-be5b-6a395b056135   externally provisioned
examplecluster-compute-0               Running                          165m    openshift-compute-0         baremetalhost:///openshift-machine-api/openshift-compute-0/3d685b81-7410-4bb3-80ec-13a31858241f         provisioned
examplecluster-compute-1               Running                          165m    openshift-compute-1         baremetalhost:///openshift-machine-api/openshift-compute-1/0fdae6eb-2066-4241-91dc-e7ea72ab13b9         provisioned</programlisting>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO88-1">
<para>The new machine, <literal>clustername-8qw5l-master-3</literal> is being created and is ready after the phase changes from <literal>Provisioning</literal> to <literal>Running</literal>.</para>
</callout>
</calloutlist>
<simpara>It should take a few minutes for the new machine to be created. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.</simpara>
</listitem>
<listitem>
<simpara>Verify that the bare metal host becomes provisioned and no error reported by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get bmh -n openshift-machine-api</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get bmh -n openshift-machine-api
NAME                      STATE                  CONSUMER                       ONLINE ERROR AGE
openshift-control-plane-0 externally provisioned examplecluster-control-plane-0 true         4h48m
openshift-control-plane-1 externally provisioned examplecluster-control-plane-1 true         4h48m
openshift-control-plane-2 provisioned            examplecluster-control-plane-3 true          47m
openshift-compute-0       provisioned            examplecluster-compute-0       true         4h48m
openshift-compute-1       provisioned            examplecluster-compute-1       true         4h48m</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Verify that the new node is added and in a ready state by running this command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes
NAME                     STATUS ROLES   AGE   VERSION
openshift-control-plane-0 Ready master 4h26m v1.28.5
openshift-control-plane-1 Ready master 4h26m v1.28.5
openshift-control-plane-2 Ready master 12m   v1.28.5
openshift-compute-0       Ready worker 3h58m v1.28.5
openshift-compute-1       Ready worker 3h58m v1.28.5</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Turn the quorum guard back on by entering the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'</programlisting>
</listitem>
<listitem>
<simpara>You can verify that the <literal>unsupportedConfigOverrides</literal> section is removed from the object by entering this command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd/cluster -oyaml</programlisting>
</listitem>
<listitem>
<simpara>If you are using single-node OpenShift, restart the node. Otherwise, you might encounter the following error in the etcd cluster Operator:</simpara>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">EtcdCertSignerControllerDegraded: [Operation cannot be fulfilled on secrets "etcd-peer-sno-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on secrets "etcd-serving-sno-0": the object has been modified; please apply your changes to the latest version and try again, Operation cannot be fulfilled on secrets "etcd-serving-metrics-sno-0": the object has been modified; please apply your changes to the latest version and try again]</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
<orderedlist numeration="arabic">
<title>Verification</title>
<listitem>
<simpara>Verify that all etcd pods are running properly.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-openshift-control-plane-0      5/5     Running     0     105m
etcd-openshift-control-plane-1      5/5     Running     0     107m
etcd-openshift-control-plane-2      5/5     Running     0     103m</programlisting>
</para>
</formalpara>
<simpara>If the output from the previous command only lists two pods, you can manually force an etcd redeployment. In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <co xml:id="CO89-1"/></programlisting>
<calloutlist>
<callout arearefs="CO89-1">
<para>The <literal>forceRedeploymentReason</literal> value must be unique, which is why a timestamp is appended.</para>
</callout>
</calloutlist>
<simpara>To verify there are exactly three etcd members, connect to the running etcd container, passing in the name of a pod that was not on the affected node. In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc rsh -n openshift-etcd etcd-openshift-control-plane-0</programlisting>
</listitem>
<listitem>
<simpara>View the member list:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.2# etcdctl member list -w table</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">+------------------+---------+--------------------+---------------------------+---------------------------+-----------------+
|        ID        | STATUS  |        NAME        |        PEER ADDRS         |       CLIENT ADDRS        |    IS LEARNER    |
+------------------+---------+--------------------+---------------------------+---------------------------+-----------------+
| 7a8197040a5126c8 | started | openshift-control-plane-2 | https://192.168.10.11:2380 | https://192.168.10.11:2379 |   false |
| 8d5abe9669a39192 | started | openshift-control-plane-1 | https://192.168.10.10:2380 | https://192.168.10.10:2379 |   false |
| cc3830a72fc357f9 | started | openshift-control-plane-0 | https://192.168.10.9:2380 | https://192.168.10.9:2379 |     false |
+------------------+---------+--------------------+---------------------------+---------------------------+-----------------+</programlisting>
</para>
</formalpara>
<note>
<simpara>If the output from the previous command lists more than three etcd members, you must carefully remove the unwanted member.</simpara>
</note>
</listitem>
<listitem>
<simpara>Verify that all etcd members are healthy by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered"># etcdctl endpoint health --cluster</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">https://192.168.10.10:2379 is healthy: successfully committed proposal: took = 8.973065ms
https://192.168.10.9:2379 is healthy: successfully committed proposal: took = 11.559829ms
https://192.168.10.11:2379 is healthy: successfully committed proposal: took = 11.665203ms</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>Validate that all nodes are at the latest revision by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd -o=jsonpath='{range.items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<screen>AllNodesAtLatestRevision</screen>
</listitem>
</orderedlist>
</section>
</section>
<section xml:id="additional-resources_replacing-unhealthy-etcd-member" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../machine_management/deleting-machine.xml#machine-lifecycle-hook-deletion-etcd_deleting-machine">Quorum protection with machine lifecycle hooks</link></simpara>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="_disaster_recovery">
<title>Disaster recovery</title>
<section xml:id="about-dr">
<title>About disaster recovery</title>

<simpara>The disaster recovery documentation provides information for administrators on
how to recover from several disaster situations that might occur with their
OpenShift Container Platform cluster. As an administrator, you might need to follow one or
more of the following procedures to return your cluster to a working
state.</simpara>
<important>
<simpara>Disaster recovery requires you to have at least one healthy control plane host.</simpara>
</important>
<variablelist>
<varlistentry>
<term><link xlink:href="../../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-restoring-cluster-state">Restoring to a previous cluster state</link></term>
<listitem>
<simpara>This solution handles situations where you want to restore your cluster to
a previous state, for example, if an administrator deletes something critical.
This also includes situations where you have lost the majority of your control plane hosts, leading to etcd quorum loss and the cluster going offline. As long as you have taken an etcd backup, you can follow this procedure to restore your cluster to a previous state.</simpara>
<simpara>If applicable, you might also need to <link xlink:href="../../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-3-expired-certs.xml#dr-recovering-expired-certs">recover from expired control plane certificates</link>.</simpara>
<warning>
<simpara>Restoring to a previous cluster state is a destructive and destablizing action to take on a running cluster. This procedure should only be used as a last resort.</simpara>
<simpara>Prior to performing a restore, see <link xlink:href="../../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.xml#dr-scenario-2-restoring-cluster-state-about_dr-restoring-cluster-state">About restoring cluster state</link> for more information on the impact to the cluster.</simpara>
</warning>
<note>
<simpara>If you have a majority of your masters still available and have an etcd quorum, then follow the procedure to <link xlink:href="../../../backup_and_restore/control_plane_backup_and_restore/replacing-unhealthy-etcd-member.xml#replacing-unhealthy-etcd-member">replace a single unhealthy etcd member</link>.</simpara>
</note>
</listitem>
</varlistentry>
<varlistentry>
<term><link xlink:href="../../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-3-expired-certs.xml#dr-recovering-expired-certs">Recovering from expired control plane certificates</link></term>
<listitem>
<simpara>This solution handles situations where your control plane certificates have
expired. For example, if you shut down your cluster before the first certificate
rotation, which occurs 24 hours after installation, your certificates will not
be rotated and will expire. You can follow this procedure to recover from
expired control plane certificates.</simpara>
</listitem>
</varlistentry>
</variablelist>
</section>
<section xml:id="dr-restoring-cluster-state">
<title>Restoring to a previous cluster state</title>

<simpara>To restore the cluster to a previous state, you must have previously <link xlink:href="../../../backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.xml#backing-up-etcd-data_backup-etcd">backed up etcd data</link> by creating a snapshot. You will use this snapshot to restore the cluster state.</simpara>
<section xml:id="dr-scenario-2-restoring-cluster-state-about_dr-restoring-cluster-state">
<title>About restoring cluster state</title>
<simpara>You can use an etcd backup to restore your cluster to a previous state. This can be used to recover from the following situations:</simpara>
<itemizedlist>
<listitem>
<simpara>The cluster has lost the majority of control plane hosts (quorum loss).</simpara>
</listitem>
<listitem>
<simpara>An administrator has deleted something critical and must restore to recover the cluster.</simpara>
</listitem>
</itemizedlist>
<warning>
<simpara>Restoring to a previous cluster state is a destructive and destablizing action to take on a running cluster. This should only be used as a last resort.</simpara>
<simpara>If you are able to retrieve data using the Kubernetes API server, then etcd is available and you should not restore using an etcd backup.</simpara>
</warning>
<simpara>Restoring etcd effectively takes a cluster back in time and all clients will experience a conflicting, parallel history. This can impact the behavior of watching components like kubelets, Kubernetes controller managers, SDN controllers, and persistent volume controllers.</simpara>
<simpara>It can cause Operator churn when the content in etcd does not match the actual content on disk, causing Operators for the Kubernetes API server, Kubernetes controller manager, Kubernetes scheduler, and etcd to get stuck when files on disk conflict with content in etcd. This can require manual actions to resolve the issues.</simpara>
<simpara>In extreme cases, the cluster can lose track of persistent volumes, delete critical workloads that no longer exist, reimage machines, and rewrite CA bundles with expired certificates.</simpara>
</section>
<section xml:id="dr-scenario-2-restoring-cluster-state_dr-restoring-cluster-state">
<title>Restoring to a previous cluster state</title>
<simpara>You can use a saved <literal>etcd</literal> backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.</simpara>
<note>
<simpara>If your cluster uses a control plane machine set, see "Troubleshooting the control plane machine set" for a more simple <literal>etcd</literal> recovery procedure.</simpara>
</note>
<important>
<simpara>When you restore your cluster, you must use an <literal>etcd</literal> backup that was taken from the same z-stream release. For example, an OpenShift Container Platform 4.7.2 cluster must use an <literal>etcd</literal> backup that was taken from 4.7.2.</simpara>
</important>
<itemizedlist>
<title>Prerequisites</title>
<listitem>
<simpara>Access to the cluster as a user with the <literal>cluster-admin</literal> role through a certificate-based <literal>kubeconfig</literal> file, like the one that was used during installation.</simpara>
</listitem>
<listitem>
<simpara>A healthy control plane host to use as the recovery host.</simpara>
</listitem>
<listitem>
<simpara>SSH access to control plane hosts.</simpara>
</listitem>
<listitem>
<simpara>A backup directory containing both the <literal>etcd</literal> snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: <literal>snapshot_&lt;datetimestamp&gt;.db</literal> and <literal>static_kuberesources_&lt;datetimestamp&gt;.tar.gz</literal>.</simpara>
</listitem>
</itemizedlist>
<important>
<simpara>For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and recreate other non-recovery, control plane machines, one by one.</simpara>
</important>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on.</simpara>
</listitem>
<listitem>
<simpara>Establish SSH connectivity to each of the control plane nodes, including the recovery host.</simpara>
<simpara><literal>kube-apiserver</literal> becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal.</simpara>
<important>
<simpara>If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.</simpara>
</important>
</listitem>
<listitem>
<simpara>Copy the <literal>etcd</literal> backup directory to the recovery control plane host.</simpara>
<simpara>This procedure assumes that you copied the <literal>backup</literal> directory containing the <literal>etcd</literal> snapshot and the resources for the static pods to the <literal>/home/core/</literal> directory of your recovery control plane host.</simpara>
</listitem>
<listitem>
<simpara>Stop the static pods on any other control plane nodes.</simpara>
<note>
<simpara>You do not need to stop the static pods on the recovery host.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Access a control plane host that is not the recovery host.</simpara>
</listitem>
<listitem>
<simpara>Move the existing etcd pod file out of the kubelet manifest directory by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/etcd-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>etcd</literal> pods are stopped by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the existing <literal>kube-apiserver</literal> file out of the kubelet manifest directory by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kube-apiserver</literal> containers are stopped by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep kube-apiserver | egrep -v "operator|guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the existing <literal>kube-controller-manager</literal> file out of the kubelet manifest directory by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/kube-controller-manager-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kube-controller-manager</literal> containers are stopped by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep kube-controller-manager | egrep -v "operator|guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the existing <literal>kube-scheduler</literal> file out of the kubelet manifest directory by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/kube-scheduler-pod.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>kube-scheduler</literal> containers are stopped by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep kube-scheduler | egrep -v "operator|guard"</programlisting>
<simpara>If the output of this command is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Move the <literal>etcd</literal> data directory to a different location with the following example:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /var/lib/etcd/ /tmp</programlisting>
</listitem>
<listitem>
<simpara>If the <literal>/etc/kubernetes/manifests/keepalived.yaml</literal> file exists, follow these steps:</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Move the <literal>/etc/kubernetes/manifests/keepalived.yaml</literal> file out of the kubelet manifest directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo mv /etc/kubernetes/manifests/keepalived.yaml /tmp</programlisting>
</listitem>
<listitem>
<simpara>Verify that any containers managed by the <literal>keepalived</literal> daemon are stopped:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps --name keepalived</programlisting>
<simpara>The output of this command should be empty. If it is not empty, wait a few minutes and check again.</simpara>
</listitem>
<listitem>
<simpara>Check if the control plane has any Virtual IPs (VIPs) assigned to it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip -o address | egrep '&lt;api_vip&gt;|&lt;ingress_vip&gt;'</programlisting>
</listitem>
<listitem>
<simpara>For each reported VIP, run the following command to remove it:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo ip address del &lt;reported_vip&gt; dev &lt;reported_vip_device&gt;</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Repeat this step on each of the other control plane hosts that is not the recovery host.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Access the recovery control plane host.</simpara>
</listitem>
<listitem>
<simpara>If the <literal>keepalived</literal> daemon is in use, verify that the recovery control plane node owns the VIP:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ ip -o address | grep &lt;api_vip&gt;</programlisting>
<simpara>The address of the VIP is highlighted in the output if it exists. This command returns an empty string if the VIP is not set or configured incorrectly.</simpara>
</listitem>
<listitem>
<simpara>If the cluster-wide proxy is enabled, be sure that you have exported the <literal>NO_PROXY</literal>, <literal>HTTP_PROXY</literal>, and <literal>HTTPS_PROXY</literal> environment variables.</simpara>
<tip>
<simpara>You can check whether the proxy is enabled by reviewing the output of <literal>oc get proxy cluster -o yaml</literal>. The proxy is enabled if the <literal>httpProxy</literal>, <literal>httpsProxy</literal>, and <literal>noProxy</literal> fields have values set.</simpara>
</tip>
</listitem>
<listitem>
<simpara>Run the restore script on the recovery control plane host and pass in the path to the <literal>etcd</literal> backup directory:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo -E /usr/local/bin/cluster-restore.sh /home/core/backup</programlisting>
<formalpara>
<title>Example script output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">...stopping kube-scheduler-pod.yaml
...stopping kube-controller-manager-pod.yaml
...stopping etcd-pod.yaml
...stopping kube-apiserver-pod.yaml
Waiting for container etcd to stop
.complete
Waiting for container etcdctl to stop
.............................complete
Waiting for container etcd-metrics to stop
complete
Waiting for container kube-controller-manager to stop
complete
Waiting for container kube-apiserver to stop
..........................................................................................complete
Waiting for container kube-scheduler to stop
complete
Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup
starting restore-etcd static pod
starting kube-apiserver-pod.yaml
static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml
starting kube-controller-manager-pod.yaml
static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml
starting kube-scheduler-pod.yaml
static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml</programlisting>
</para>
</formalpara>
<simpara>The cluster-restore.sh script must show that <literal>etcd</literal>, <literal>kube-apiserver</literal>, <literal>kube-controller-manager</literal>, and <literal>kube-scheduler</literal> pods are stopped and then started at the end of the restore process.</simpara>
<note>
<simpara>The restore process can cause nodes to enter the <literal>NotReady</literal> state if the node certificates were updated after the last <literal>etcd</literal> backup.</simpara>
</note>
</listitem>
<listitem>
<simpara>Check the nodes to ensure they are in the <literal>Ready</literal> state.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get nodes -w</programlisting>
<formalpara>
<title>Sample output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                STATUS  ROLES          AGE     VERSION
host-172-25-75-28   Ready   master         3d20h   v1.28.5
host-172-25-75-38   Ready   infra,worker   3d20h   v1.28.5
host-172-25-75-40   Ready   master         3d20h   v1.28.5
host-172-25-75-65   Ready   master         3d20h   v1.28.5
host-172-25-75-74   Ready   infra,worker   3d20h   v1.28.5
host-172-25-75-79   Ready   worker         3d20h   v1.28.5
host-172-25-75-86   Ready   worker         3d20h   v1.28.5
host-172-25-75-98   Ready   infra,worker   3d20h   v1.28.5</programlisting>
</para>
</formalpara>
<simpara>It can take several minutes for all nodes to report their state.</simpara>
</listitem>
<listitem>
<simpara>If any nodes are in the <literal>NotReady</literal> state, log in to the nodes and remove all of the PEM files from the <literal>/var/lib/kubelet/pki</literal> directory on each node. You can SSH into the nodes or use the terminal window in the web console.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$  ssh -i &lt;ssh-key-path&gt; core@&lt;master-hostname&gt;</programlisting>
<formalpara>
<title>Sample <literal>pki</literal> directory</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">sh-4.4# pwd
/var/lib/kubelet/pki
sh-4.4# ls
kubelet-client-2022-04-28-11-24-09.pem  kubelet-server-2022-04-28-11-24-15.pem
kubelet-client-current.pem              kubelet-server-current.pem</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Restart the kubelet service on all control plane hosts.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>From the recovery host, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo systemctl restart kubelet.service</programlisting>
</listitem>
<listitem>
<simpara>Repeat this step on all other control plane hosts.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Approve the pending Certificate Signing Requests (CSRs):</simpara>
<note>
<simpara>Clusters with no worker nodes, such as single-node clusters or clusters consisting of three schedulable control plane nodes, will not have any pending CSRs to approve. You can skip all the commands listed in this step.</simpara>
</note>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Get the list of current CSRs by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME        AGE    SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-2s94x   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <co xml:id="CO90-1"/>
csr-4bd6t   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <co xml:id="CO90-2"/>
csr-4hl85   13m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <co xml:id="CO90-3"/>
csr-zhhhp   3m8s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <co xml:id="CO90-4"/>
...</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO90-1 CO90-2">
<para>A pending kubelet service</para>
</callout>
</calloutlist>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>CSR (for user-provisioned installations).
&lt;2&gt; A pending <literal>node-bootstrapper</literal> CSR.</simpara>
<orderedlist numeration="loweralpha">
<listitem>
<simpara>Review the details of a CSR to verify that it is valid by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe csr &lt;csr_name&gt; <co xml:id="CO91-1"/></programlisting>
<calloutlist>
<callout arearefs="CO91-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Approve each valid <literal>node-bootstrapper</literal> CSR by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>For user-provisioned installations, approve each valid kubelet service CSR by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that the single member control plane has started successfully.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>From the recovery host, verify that the <literal>etcd</literal> container is running by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">3ad41b7908e32       36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009                                                         About a minute ago   Running             etcd                                          0                   7c05f8af362f0</programlisting>
</para>
</formalpara>
</listitem>
<listitem>
<simpara>From the recovery host, verify that the <literal>etcd</literal> pod is running by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                             READY   STATUS      RESTARTS   AGE
etcd-ip-10-0-143-125.ec2.internal                1/1     Running     1          2m47s</programlisting>
</para>
</formalpara>
<simpara>If the status is <literal>Pending</literal>, or the output lists more than one running <literal>etcd</literal> pod, wait a few minutes and check again.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>If you are using the <literal>OVNKubernetes</literal> network plugin, you must restart <literal>ovnkube-controlplane</literal> pods.</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete all of the <literal>ovnkube-controlplane</literal> pods by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-control-plane</programlisting>
</listitem>
<listitem>
<simpara>Verify that all of the <literal>ovnkube-controlplane</literal> pods were redeployed by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane</programlisting>
<orderedlist numeration="arabic">
<listitem>
<simpara>If you are using the OVN-Kubernetes network plugin, restart the Open Virtual Network (OVN) Kubernetes pods on all the nodes one by one. Use the following steps to restart OVN-Kubernetes pods on each node:</simpara>
<important>
<orderedlist numeration="arabic">
<title>Restart OVN-Kubernetes pods in the following order:</title>
<listitem>
<simpara>The recovery control plane host</simpara>
</listitem>
<listitem>
<simpara>The other control plane hosts (if available)</simpara>
</listitem>
<listitem>
<simpara>The other nodes</simpara>
</listitem>
</orderedlist>
</important>
<note>
<simpara>Validating and mutating admission webhooks can reject pods. If you add any additional webhooks with the <literal>failurePolicy</literal> set to <literal>Fail</literal>, then they can reject pods and the restoration process can fail. You can avoid this by saving and deleting webhooks while restoring the cluster state. After the cluster state is restored successfully, you can enable the webhooks again.</simpara>
<simpara>Alternatively, you can temporarily set the <literal>failurePolicy</literal> to <literal>Ignore</literal> while restoring the cluster state. After the cluster state is restored successfully, you can set the <literal>failurePolicy</literal> to <literal>Fail</literal>.</simpara>
</note>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Remove the northbound database (nbdb) and southbound database (sbdb). Access the recovery host and the remaining control plane nodes by using Secure Shell (SSH) and run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo rm -f /var/lib/ovn-ic/etc/*.db</programlisting>
</listitem>
<listitem>
<simpara>Restart the OpenVSwitch services. Access the node by using Secure Shell (SSH) and run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ sudo systemctl restart ovs-vswitchd ovsdb-server</programlisting>
</listitem>
<listitem>
<simpara>Delete the <literal>ovnkube-node</literal> pod on the node by running the following command, replacing <literal>&lt;node&gt;</literal> with the name of the node that you are restarting:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-node --field-selector=spec.nodeName==&lt;node&gt;</programlisting>
</listitem>
<listitem>
<simpara>Verify that the <literal>ovnkube-node</literal> pod is running again with:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-node --field-selector=spec.nodeName==&lt;node&gt;</programlisting>
<note>
<simpara>It might take several minutes for the pods to restart.</simpara>
</note>
<orderedlist numeration="arabic">
<listitem>
<simpara>Delete and re-create other non-recovery, control plane machines, one by one. After the machines are re-created, a new revision is forced and <literal>etcd</literal> automatically scales up.</simpara>
<itemizedlist>
<listitem>
<simpara>If you use a user-provisioned bare metal installation, you can re-create a control plane machine by using the same method that you used to originally create it. For more information, see "Installing a user-provisioned cluster on bare metal".</simpara>
<warning>
<simpara>Do not delete and re-create the machine for the recovery host.</simpara>
</warning>
</listitem>
<listitem>
<simpara>If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps:</simpara>
<warning>
<simpara>Do not delete and re-create the machine for the recovery host.</simpara>
<simpara>For bare metal installations on installer-provisioned infrastructure, control plane machines are not re-created. For more information, see "Replacing a bare-metal control plane node".</simpara>
</warning>
</listitem>
</itemizedlist>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Obtain the machine for one of the lost control plane hosts.</simpara>
<simpara>In a terminal that has access to the cluster as a cluster-admin user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<simpara>Example output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-0                  Running   m4.xlarge   us-east-1   us-east-1a   3h37m   ip-10-0-131-183.ec2.internal   aws:///us-east-1a/i-0ec2782f8287dfb7e   stopped <co xml:id="CO92-1"/>
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
<calloutlist>
<callout arearefs="CO92-1">
<para>This is the control plane machine for the lost control plane host, <literal>ip-10-0-131-183.ec2.internal</literal>.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Save the machine configuration to a file on your file system by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machine clustername-8qw5l-master-0 \ <co xml:id="CO93-1"/>
    -n openshift-machine-api \
    -o yaml \
    &gt; new-master-machine.yaml</programlisting>
<calloutlist>
<callout arearefs="CO93-1">
<para>Specify the name of the control plane machine for the lost control plane host.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Edit the <literal>new-master-machine.yaml</literal> file that was created in the previous step to assign a new name and remove unnecessary fields.</simpara>
<orderedlist numeration="lowerroman">
<listitem>
<simpara>Remove the entire <literal>status</literal> section by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">status:
  addresses:
  - address: 10.0.131.183
    type: InternalIP
  - address: ip-10-0-131-183.ec2.internal
    type: InternalDNS
  - address: ip-10-0-131-183.ec2.internal
    type: Hostname
  lastUpdated: "2020-04-20T17:44:29Z"
  nodeRef:
    kind: Node
    name: ip-10-0-131-183.ec2.internal
    uid: acca4411-af0d-4387-b73e-52b2484295ad
  phase: Running
  providerStatus:
    apiVersion: awsproviderconfig.openshift.io/v1beta1
    conditions:
    - lastProbeTime: "2020-04-20T16:53:50Z"
      lastTransitionTime: "2020-04-20T16:53:50Z"
      message: machine successfully created
      reason: MachineCreationSucceeded
      status: "True"
      type: MachineCreation
    instanceId: i-0fdb85790d76d0c3f
    instanceState: stopped
    kind: AWSMachineProviderStatus</programlisting>
</listitem>
<listitem>
<simpara>Change the <literal>metadata.name</literal> field to a new name by running:</simpara>
<simpara>It is recommended to keep the same base name as the old machine and change the ending number to the next available number. In this example, <literal>clustername-8qw5l-master-0</literal> is changed to <literal>clustername-8qw5l-master-3</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
  name: clustername-8qw5l-master-3
  ...</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>spec.providerID</literal> field by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">providerID: aws:///us-east-1a/i-0fdb85790d76d0c3f</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>metadata.annotations</literal> and <literal>metadata.generation</literal> fields by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">annotations:
  machine.openshift.io/instance-state: running
...
generation: 2</programlisting>
</listitem>
<listitem>
<simpara>Remove the <literal>metadata.resourceVersion</literal> and <literal>metadata.uid</literal> fields by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">resourceVersion: "13291"
uid: a282eb70-40a2-4e89-8009-d05dd420d31a</programlisting>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Delete the machine of the lost control plane host by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc delete machine -n openshift-machine-api clustername-8qw5l-master-0 <co xml:id="CO94-1"/></programlisting>
<calloutlist>
<callout arearefs="CO94-1">
<para>Specify the name of the control plane machine for the lost control plane host.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Verify that the machine was deleted by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<simpara>Example output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal   aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
</listitem>
<listitem>
<simpara>Create a machine by using the <literal>new-master-machine.yaml</literal> file by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc apply -f new-master-machine.yaml</programlisting>
</listitem>
<listitem>
<simpara>Verify that the new machine has been created by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get machines -n openshift-machine-api -o wide</programlisting>
<simpara>Example output:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">NAME                                        PHASE          TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running        m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running        m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-master-3                  Provisioning   m4.xlarge   us-east-1   us-east-1a   85s     ip-10-0-173-171.ec2.internal    aws:///us-east-1a/i-015b0888fe17bc2c8  running <co xml:id="CO95-1"/>
clustername-8qw5l-worker-us-east-1a-wbtgd   Running        m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running        m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running        m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running</programlisting>
<calloutlist>
<callout arearefs="CO95-1">
<para>The new machine, <literal>clustername-8qw5l-master-3</literal> is being created and is ready after the phase changes from <literal>Provisioning</literal> to <literal>Running</literal>.</para>
</callout>
</calloutlist>
<simpara>It might take a few minutes for the new machine to be created. The <literal>etcd</literal> cluster Operator will automatically sync when the machine or node returns to a healthy state.</simpara>
</listitem>
<listitem>
<simpara>Repeat these steps for each lost control plane host that is not the recovery host.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Turn off the quorum guard by entering:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'</programlisting>
<simpara>This command ensures that you can successfully re-create secrets and roll out the static pods.</simpara>
</listitem>
<listitem>
<simpara>In a separate terminal window within the recovery host, export the recovery <literal>kubeconfig</literal> file by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfig</programlisting>
</listitem>
<listitem>
<simpara>Force <literal>etcd</literal> redeployment.</simpara>
<simpara>In the same terminal window where you exported the recovery <literal>kubeconfig</literal> file, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <co xml:id="CO96-1"/></programlisting>
<calloutlist>
<callout arearefs="CO96-1">
<para>The <literal>forceRedeploymentReason</literal> value must be unique, which is why a timestamp is appended.</para>
</callout>
</calloutlist>
<simpara>When the <literal>etcd</literal> cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up.</simpara>
</listitem>
<listitem>
<simpara>Turn the quorum guard back on by entering:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'</programlisting>
</listitem>
<listitem>
<simpara>You can verify that the <literal>unsupportedConfigOverrides</literal> section is removed from the object by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd/cluster -oyaml</programlisting>
</listitem>
<listitem>
<simpara>Verify all nodes are updated to the latest revision.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition for <literal>etcd</literal> to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO97-1"/></programlisting>
<calloutlist>
<callout arearefs="CO97-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>After <literal>etcd</literal> is redeployed, force new rollouts for the control plane. <literal>kube-apiserver</literal> will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run:</simpara>
</listitem>
</orderedlist>
</listitem>
<listitem>
<simpara>Force a new rollout for <literal>kube-apiserver</literal>:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubeapiserver cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</programlisting>
<simpara>Verify all nodes are updated to the latest revision.</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO98-1"/></programlisting>
<calloutlist>
<callout arearefs="CO98-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Force a new rollout for the Kubernetes controller manager by running the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubecontrollermanager cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</programlisting>
<simpara>Verify all nodes are updated to the latest revision by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO99-1"/></programlisting>
<calloutlist>
<callout arearefs="CO99-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
</listitem>
<listitem>
<simpara>Force a new rollout for the <literal>kube-scheduler</literal> by running:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc patch kubescheduler cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge</programlisting>
<simpara>Verify all nodes are updated to the latest revision by using:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'</programlisting>
<simpara>Review the <literal>NodeInstallerProgressing</literal> status condition to verify that all nodes are at the latest revision. The output shows <literal>AllNodesAtLatestRevision</literal> upon successful update:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">AllNodesAtLatestRevision
3 nodes are at revision 7 <co xml:id="CO100-1"/></programlisting>
<calloutlist>
<callout arearefs="CO100-1">
<para>In this example, the latest revision number is <literal>7</literal>.</para>
</callout>
</calloutlist>
<simpara>If the output includes multiple revision numbers, such as <literal>2 nodes are at revision 6; 1 nodes are at revision 7</literal>, this means that the update is still in progress. Wait a few minutes and try again.</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Verify that all control plane hosts have started and joined the cluster.</simpara>
<simpara>In a terminal that has access to the cluster as a <literal>cluster-admin</literal> user, run the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc -n openshift-etcd get pods -l k8s-app=etcd</programlisting>
<formalpara>
<title>Example output</title>
<para>
<programlisting language="terminal" linenumbering="unnumbered">etcd-ip-10-0-143-125.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-154-194.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-173-171.ec2.internal                2/2     Running     0          9h</programlisting>
</para>
</formalpara>
</listitem>
</orderedlist>
</listitem>
</orderedlist>
<simpara>To ensure that all workloads return to normal operation following a recovery procedure, restart each pod that stores <literal>kube-apiserver</literal> information. This includes OpenShift Container Platform components such as routers, Operators, and third-party components.</simpara>
<note>
<simpara>On completion of the previous procedural steps, you might need to wait a few minutes for all services to return to their restored state. For example, authentication by using <literal>oc login</literal> might not immediately work until the OAuth server pods are restarted.</simpara>
<simpara>Consider using the <literal>system:admin</literal> <literal>kubeconfig</literal> file for immediate authentication. This method basis its authentication on SSL/TLS client certificates as against OAuth tokens. You can authenticate with this file by issuing the following command:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ export KUBECONFIG=&lt;installation_directory&gt;/auth/kubeconfig</programlisting>
<simpara>Issue the following command to display your authenticated user name:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc whoami</programlisting>
</note>
</section>
<section xml:id="additional-resources_dr-restoring-cluster-state" role="_additional-resources">
<title>Additional resources</title>
<itemizedlist>
<listitem>
<simpara><link xlink:href="../../../installing/installing_bare_metal/installing-bare-metal.xml#installing-bare-metal">Installing a user-provisioned cluster on bare metal</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../networking/accessing-hosts.xml#accessing-hosts">Creating a bastion host to access OpenShift Container Platform instances and the control plane nodes with SSH</link></simpara>
</listitem>
<listitem>
<simpara><link xlink:href="../../../installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.xml#replacing-a-bare-metal-control-plane-node_ipi-install-expanding">Replacing a bare-metal control plane node</link></simpara>
</listitem>
</itemizedlist>
</section>
<section xml:id="dr-scenario-cluster-state-issues_dr-restoring-cluster-state">
<title>Issues and workarounds for restoring a persistent storage state</title>
<simpara>If your OpenShift Container Platform cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a <literal>StatefulSet</literal> object. When you restore from an etcd backup, the status of the workloads in OpenShift Container Platform is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.</simpara>
<important>
<simpara>The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an OpenShift Container Platform cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.</simpara>
</important>
<simpara>The following are some example scenarios that produce an out-of-date status:</simpara>
<itemizedlist>
<listitem>
<simpara>MySQL database is running in a pod backed up by a PV object. Restoring OpenShift Container Platform from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.</simpara>
</listitem>
<listitem>
<simpara>Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. OpenShift Container Platform is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.</simpara>
</listitem>
<listitem>
<simpara>Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.</simpara>
</listitem>
<listitem>
<simpara>A device is removed or renamed from OpenShift Container Platform nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from <literal>/dev/disk/by-id</literal> or <literal>/dev</literal> directories. This situation might cause the local PVs to refer to devices that no longer exist.</simpara>
<simpara>To fix this problem, an administrator must:</simpara>
<orderedlist numeration="arabic">
<listitem>
<simpara>Manually remove the PVs with invalid devices.</simpara>
</listitem>
<listitem>
<simpara>Remove symlinks from respective nodes.</simpara>
</listitem>
<listitem>
<simpara>Delete <literal>LocalVolume</literal> or <literal>LocalVolumeSet</literal> objects (see <emphasis>Storage</emphasis> &#8594; <emphasis>Configuring persistent storage</emphasis> &#8594; <emphasis>Persistent storage using local volumes</emphasis> &#8594; <emphasis>Deleting the Local Storage Operator Resources</emphasis>).</simpara>
</listitem>
</orderedlist>
</listitem>
</itemizedlist>
</section>
</section>
<section xml:id="dr-recovering-expired-certs">
<title>Recovering from expired control plane certificates</title>

<section xml:id="dr-scenario-3-recovering-expired-certs_dr-recovering-expired-certs">
<title>Recovering from expired control plane certificates</title>
<simpara>The cluster can automatically recover from expired control plane certificates.</simpara>
<simpara>However, you must manually approve the pending <literal>node-bootstrapper</literal> certificate signing requests (CSRs) to recover kubelet certificates. For user-provisioned installations, you might also need to approve pending kubelet serving CSRs.</simpara>
<simpara>Use the following steps to approve the pending CSRs:</simpara>
<orderedlist numeration="arabic">
<title>Procedure</title>
<listitem>
<simpara>Get the list of current CSRs:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc get csr</programlisting>
<formalpara>
<title>Example output</title>
<para>
<screen>NAME        AGE    SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-2s94x   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending <co xml:id="CO101-1"/>
csr-4bd6t   8m3s   kubernetes.io/kubelet-serving                 system:node:&lt;node_name&gt;                                                     Pending
csr-4hl85   13m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <co xml:id="CO101-2"/>
csr-zhhhp   3m8s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending
...</screen>
</para>
</formalpara>
<calloutlist>
<callout arearefs="CO101-1">
<para>A pending kubelet service CSR (for user-provisioned installations).</para>
</callout>
<callout arearefs="CO101-2">
<para>A pending <literal>node-bootstrapper</literal> CSR.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Review the details of a CSR to verify that it is valid:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc describe csr &lt;csr_name&gt; <co xml:id="CO102-1"/></programlisting>
<calloutlist>
<callout arearefs="CO102-1">
<para><literal>&lt;csr_name&gt;</literal> is the name of a CSR from the list of current CSRs.</para>
</callout>
</calloutlist>
</listitem>
<listitem>
<simpara>Approve each valid <literal>node-bootstrapper</literal> CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
</listitem>
<listitem>
<simpara>For user-provisioned installations, approve each valid kubelet serving CSR:</simpara>
<programlisting language="terminal" linenumbering="unnumbered">$ oc adm certificate approve &lt;csr_name&gt;</programlisting>
</listitem>
</orderedlist>
</section>
</section>
</section>
</chapter>
</book>
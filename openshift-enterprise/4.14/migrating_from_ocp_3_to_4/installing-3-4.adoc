:_mod-docs-content-type: ASSEMBLY
[id="installing-3-4"]
= Installing the Migration Toolkit for Containers
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: installing-3-4
:installing-3-4:

toc::[]

You can install the {mtc-full} ({mtc-short}) on {product-title} 3 and 4.

After you install the {mtc-full} Operator on {product-title} {product-version} by using the Operator Lifecycle Manager, you manually install the legacy {mtc-full} Operator on {product-title} 3.

By default, the {mtc-short} web console and the `Migration Controller` pod run on the target cluster. You can configure the `Migration Controller` custom resource manifest to run the {mtc-short} web console and the `Migration Controller` pod on a link:https://access.redhat.com/articles/5064151[source cluster or on a remote cluster].

After you have installed {mtc-short}, you must configure an object storage to use as a replication repository.

To uninstall {mtc-short}, see xref:../migrating_from_ocp_3_to_4/installing-3-4.adoc#migration-uninstalling-mtc-clean-up_installing-3-4[Uninstalling {mtc-short} and deleting resources].

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: CONCEPT
[id="migration-compatibility-guidelines_{context}"]
= Compatibility guidelines

You must install the {mtc-full} ({mtc-short}) Operator that is compatible with your {product-title} version.

.Definitions

legacy platform:: {product-title} 4.5 and earlier.
modern platform:: {product-title} 4.6 and later.
legacy operator:: The {mtc-short} Operator designed for legacy platforms.
modern operator:: The {mtc-short} Operator designed for modern platforms.
control cluster:: The cluster that runs the {mtc-short} controller and GUI.
remote cluster:: A source or destination cluster for a migration that runs Velero. The Control Cluster communicates with Remote clusters via the Velero API to drive migrations.


[cols="1,2,2", options="header"]
.{mtc-short} compatibility: Migrating from a legacy platform
|===
||{product-title} 4.5 or earlier |{product-title} 4.6 or later
|Stable {mtc-short} version a|{mtc-short} {mtc-version}._z_

Legacy {mtc-version} operator: Install manually with the `operator.yml` file.
[IMPORTANT]
====
This cluster cannot be the control cluster.
====

|{mtc-short} {mtc-version}._z_

Install with OLM, release channel `release-v1.7`
|===

[NOTE]
====
Edge cases exist in which network restrictions prevent modern clusters from connecting to other clusters involved in the migration. For example, when migrating from an {product-title} 3.11 cluster on premises to a modern {product-title} cluster in the cloud, where the modern cluster cannot connect to the {product-title} 3.11 cluster.

With {mtc-short} {mtc-version}, if one of the remote clusters is unable to communicate with the control cluster because of network restrictions, use the `crane tunnel-api` command.

With the stable {mtc-short} release, although you should always designate the most modern cluster as the control cluster, in this specific case it is possible to designate the legacy cluster as the control cluster and push workloads to the remote cluster.
====

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-installing-legacy-operator_{context}"]
= Installing the legacy {mtc-full} Operator on {product-title} 3

You can install the legacy {mtc-full} Operator manually on {product-title} 3.

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.
* You must have access to `registry.redhat.io`.
* You must have `podman` installed.
* You must create an link:https://access.redhat.com/solutions/3772061[image stream secret] and copy it to each node in the cluster.

.Procedure

. Log in to `registry.redhat.io` with your Red Hat Customer Portal credentials:
+
[source,terminal]
----
$ podman login registry.redhat.io
----

. Download the `operator.yml` file by entering the following command:
+
[source,terminal,subs="attributes+"]
----
$ podman cp $(podman create \
  registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v{mtc-version}):/operator.yml ./
----

. Download the `controller.yml` file by entering the following command:
+
[source,terminal,subs="attributes+"]
----
$ podman cp $(podman create \
  registry.redhat.io/rhmtc/openshift-migration-legacy-rhel8-operator:v{mtc-version}):/controller.yml ./
----


. Log in to your {product-title} source cluster.

. Verify that the cluster can authenticate with `registry.redhat.io`:
+
[source,terminal]
----
$ oc run test --image registry.redhat.io/ubi9 --command sleep infinity
----

. Create the {mtc-full} Operator object:
+
[source,terminal]
----
$ oc create -f operator.yml
----
+
.Example output
[source,terminal]
----
namespace/openshift-migration created
rolebinding.rbac.authorization.k8s.io/system:deployers created
serviceaccount/migration-operator created
customresourcedefinition.apiextensions.k8s.io/migrationcontrollers.migration.openshift.io created
role.rbac.authorization.k8s.io/migration-operator created
rolebinding.rbac.authorization.k8s.io/migration-operator created
clusterrolebinding.rbac.authorization.k8s.io/migration-operator created
deployment.apps/migration-operator created
Error from server (AlreadyExists): error when creating "./operator.yml":
rolebindings.rbac.authorization.k8s.io "system:image-builders" already exists <1>
Error from server (AlreadyExists): error when creating "./operator.yml":
rolebindings.rbac.authorization.k8s.io "system:image-pullers" already exists
----
<1> You can ignore `Error from server (AlreadyExists)` messages. They are caused by the {mtc-full} Operator creating resources for earlier versions of {product-title} 4 that are provided in later releases.

. Create the `MigrationController` object:
+
[source,terminal]
----
$ oc create -f controller.yml
----

. Verify that the {mtc-short} pods are running:
+
[source,terminal]
----
$ oc get pods -n openshift-migration
----

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-installing-mtc-on-ocp-4_{context}"]
= Installing the {mtc-full} Operator on {product-title} {product-version}

You install the {mtc-full} Operator on {product-title} {product-version} by using the Operator Lifecycle Manager.

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.

.Procedure

. In the {product-title} web console, click *Operators* -> *OperatorHub*.
. Use the *Filter by keyword* field to find the *{mtc-full} Operator*.
. Select the *{mtc-full} Operator* and click *Install*.
. Click *Install*.
+
On the *Installed Operators* page, the *{mtc-full} Operator* appears in the *openshift-migration* project with the status *Succeeded*.

. Click *{mtc-full} Operator*.
. Under *Provided APIs*, locate the *Migration Controller* tile, and click *Create Instance*.
. Click *Create*.
. Click *Workloads* -> *Pods* to verify that the {mtc-short} pods are running.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: CONCEPT
[id="migration-about-configuring-proxies_{context}"]
= Proxy configuration

For {product-title} 4.1 and earlier versions, you must configure proxies in the `MigrationController` custom resource (CR) manifest after you install the {mtc-full} Operator because these versions do not support a cluster-wide `proxy` object.

For {product-title} 4.2 to {product-version}, the {mtc-full} ({mtc-short}) inherits the cluster-wide proxy settings. You can change the proxy parameters if you want to override the cluster-wide proxy settings.

[id="direct-volume-migration_{context}"]
== Direct volume migration

Direct Volume Migration (DVM) was introduced in MTC 1.4.2. DVM supports only one proxy. The source cluster cannot access the route of the target cluster if the target cluster is also behind a proxy.

If you want to perform a DVM from a source cluster behind a proxy, you must configure a TCP proxy that works at the transport layer and forwards the SSL connections transparently without decrypting and re-encrypting them with their own SSL certificates. A Stunnel proxy is an example of such a proxy.

[id="tcp-proxy-setup-for-dvm_{context}"]
=== TCP proxy setup for DVM

You can set up a direct connection between the source and the target cluster through a TCP proxy and configure the `stunnel_tcp_proxy` variable in the `MigrationController` CR to use the proxy:

[source, yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  stunnel_tcp_proxy: http://username:password@ip:port
----

Direct volume migration (DVM) supports only basic authentication for the proxy. Moreover, DVM works only from behind proxies that can tunnel a TCP connection transparently. HTTP/HTTPS proxies in man-in-the-middle mode do not work. The existing cluster-wide proxies might not support this behavior. As a result, the proxy settings for DVM are intentionally kept different from the usual proxy configuration in {mtc-short}.

[id="why-tcp-proxy-instead-of-an-http-https-proxy_{context}"]
=== Why use a TCP proxy instead of an HTTP/HTTPS proxy?

You can enable DVM by running Rsync between the source and the target cluster over an OpenShift route.  Traffic is encrypted using Stunnel, a TCP proxy. The Stunnel running on the source cluster initiates a TLS connection with the target Stunnel and transfers data over an encrypted channel.

Cluster-wide HTTP/HTTPS proxies in OpenShift are usually configured in man-in-the-middle mode where they negotiate their own TLS session with the outside servers. However, this does not work with Stunnel. Stunnel requires that its TLS session be untouched by the proxy, essentially making the proxy a transparent tunnel which simply forwards the TCP connection as-is. Therefore, you must use a TCP proxy.

[id="dvm-known-issues_{context}"]
=== Known issue

.Migration fails with error `Upgrade request required`

The migration Controller uses the SPDY protocol to execute commands within remote pods. If the remote cluster is behind a proxy or a firewall that does not support the SPDY protocol, the migration controller fails to execute remote commands. The migration fails with the error message `Upgrade request required`.
Workaround: Use a proxy that supports the SPDY protocol.

In addition to supporting the SPDY protocol, the proxy or firewall also must pass the `Upgrade` HTTP header to the API server. The client uses this header to open a websocket connection with the API server. If the `Upgrade` header is blocked by the proxy or firewall, the migration fails with the error message `Upgrade request required`.
Workaround: Ensure that the proxy forwards the `Upgrade` header.

[id="tuning-network-policies-for-migrations_{context}"]
== Tuning network policies for migrations

OpenShift supports restricting traffic to or from pods using _NetworkPolicy_ or _EgressFirewalls_ based on the network plugin used by the cluster. If any of the source namespaces involved in a migration use such mechanisms to restrict network traffic to pods, the restrictions might inadvertently stop traffic to Rsync pods during migration.

Rsync pods running on both the source and the target clusters must connect to each other over an OpenShift Route. Existing _NetworkPolicy_ or _EgressNetworkPolicy_ objects can be configured to automatically exempt Rsync pods from these traffic restrictions.

[id="dvm-network-policy-configuration_{context}"]
=== NetworkPolicy configuration

[id="egress-traffic-from-rsync-pods_{context}"]
==== Egress traffic from Rsync pods

You can use the unique labels of Rsync pods to allow egress traffic to pass from them if the `NetworkPolicy` configuration in the source or destination namespaces blocks this type of traffic. The following policy allows *all* egress traffic from Rsync pods in the namespace:

[source, yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  egress:
  - {}
  policyTypes:
  - Egress
----

[id="ingress-traffic-to-rsync-pods_{context}"]
==== Ingress traffic to Rsync pods

[source, yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  ingress:
  - {}
  policyTypes:
  - Ingress
----

[id="egressnetworkpolicy-config_{context}"]
=== EgressNetworkPolicy configuration

The `EgressNetworkPolicy` object or _Egress Firewalls_ are OpenShift constructs designed to block egress traffic leaving the cluster.

Unlike the `NetworkPolicy` object, the Egress Firewall works at a project level because it applies to all pods in the namespace. Therefore, the unique labels of Rsync pods do not exempt only Rsync pods from the restrictions. However, you can add the CIDR ranges of the source or target cluster to the _Allow_ rule of the policy so that a direct connection can be setup between two clusters.

Based on which cluster the Egress Firewall is present in, you can add the CIDR range of the other cluster to allow egress traffic between the two:

[source, yaml]
----
apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: test-egress-policy
  namespace: <namespace>
spec:
  egress:
  - to:
      cidrSelector: <cidr_of_source_or_target_cluster>
    type: Deny
----

[id="choosing-alternate-endpoints-for-data-transfer_{context}"]
=== Choosing alternate endpoints for data transfer

By default, DVM uses an {product-title} route as an endpoint to transfer PV data to destination clusters. You can choose another type of supported endpoint, if cluster topologies allow.

For each cluster, you can configure an endpoint by setting the `rsync_endpoint_type` variable on the appropriate *destination* cluster in your `MigrationController` CR:

[source, yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  rsync_endpoint_type: [NodePort|ClusterIP|Route]
----

[id="configuring-supplemental-groups-for-rsync-pods_{context}"]
=== Configuring supplemental groups for Rsync pods
When your PVCs use a shared storage, you can configure the access to that storage by adding supplemental groups to Rsync pod definitions in order for the pods to allow access:

.Supplementary groups for Rsync pods
[option="header"]
|===
|Variable|Type|Default|Description

|`src_supplemental_groups`
|string
|Not set
|Comma-separated list of supplemental groups for source Rsync pods

|`target_supplemental_groups`
|string
|Not set
|Comma-separated list of supplemental groups for target Rsync pods
|===

.Example usage

The `MigrationController` CR can be updated to set values for these supplemental groups:

[source, yaml]
----
spec:
  src_supplemental_groups: "1000,2000"
  target_supplemental_groups: "2000,3000"
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-configuring-proxies_{context}"]
= Configuring proxies

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.

.Procedure

. Get the `MigrationController` CR manifest:
+
[source,terminal]
----
$ oc get migrationcontroller <migration_controller> -n openshift-migration
----

. Update the proxy parameters:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: <migration_controller>
  namespace: openshift-migration
...
spec:
  stunnel_tcp_proxy: http://<username>:<password>@<ip>:<port> <1>
  noProxy: example.com <2>
----
<1> Stunnel proxy URL for direct volume migration.
<2> Comma-separated list of destination domain names, domains, IP addresses, or other network CIDRs to exclude proxying.
+
Preface a domain with `.` to match subdomains only. For example, `.y.com` matches `x.y.com`, but not `y.com`. Use `*` to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the `networking.machineNetwork[].cidr` field from the installation configuration, you must add them to this list to prevent connection issues.
+
This field is ignored if neither the `httpProxy` nor the `httpsProxy` field is set.

. Save the manifest as `migration-controller.yaml`.
. Apply the updated manifest:
+
[source,terminal]
----
$ oc replace -f migration-controller.yaml -n openshift-migration
----

:leveloffset!:

For more information, see xref:../networking/enable-cluster-wide-proxy.adoc#nw-proxy-configure-object_config-cluster-wide-proxy[Configuring the cluster-wide proxy].

[id="configuring-replication-repository_{context}"]
== Configuring a replication repository

You must configure an object storage to use as a replication repository. The {mtc-full} ({mtc-short}) copies data from the source cluster to the replication repository, and then from the replication repository to the target cluster.

{mtc-short} supports the xref:../migrating_from_ocp_3_to_4/about-mtc-3-4.adoc#migration-understanding-data-copy-methods_about-mtc-3-4[file system and snapshot data copy methods] for migrating data from the source cluster to the target cluster. You can select a method that is suited for your environment and is supported by your storage provider.

The following storage providers are supported:

* xref:../migrating_from_ocp_3_to_4/installing-3-4.adoc#migration-configuring-mcg_installing-3-4[Multicloud Object Gateway]
* xref:../migrating_from_ocp_3_to_4/installing-3-4.adoc#migration-configuring-aws-s3_installing-3-4[Amazon Web Services S3]
* xref:../migrating_from_ocp_3_to_4/installing-3-4.adoc#migration-configuring-gcp_installing-3-4[Google Cloud Platform]
* xref:../migrating_from_ocp_3_to_4/installing-3-4.adoc#migration-configuring-azure_installing-3-4[Microsoft Azure Blob]
* Generic S3 object storage, for example, Minio or Ceph S3

[id="replication-repository-prerequisites_{context}"]
=== Prerequisites

* All clusters must have uninterrupted network access to the replication repository.
* If you use a proxy server with an internally hosted replication repository, you must ensure that the proxy allows access to the replication repository.

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc
// * backup_and_restore/application_backup_and_restore/installing/installing-oadp-mcg.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-configuring-mcg_{context}"]
= Retrieving Multicloud Object Gateway credentials

You must retrieve the Multicloud Object Gateway (MCG) credentials and S3 endpoint in order to configure MCG as a replication repository for the {mtc-full} ({mtc-short}).
You must retrieve the Multicloud Object Gateway (MCG) credentials in order to create a `Secret` custom resource (CR) for the OpenShift API for Data Protection (OADP).
//ifdef::installing-oadp-mcg[]
//endif::[]

MCG is a component of {rh-storage}.

.Prerequisites
* You must deploy {rh-storage} by using the appropriate link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9[OpenShift Data Foundation deployment guide].

.Procedure

. Obtain the S3 endpoint, `AWS_ACCESS_KEY_ID`, and `AWS_SECRET_ACCESS_KEY` by running the link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/managing_hybrid_and_multicloud_resources/accessing-the-multicloud-object-gateway-with-your-applications_rhodf#accessing-the-Multicloud-object-gateway-from-the-terminal_rhodf[`describe` command] on the `NooBaa` custom resource.
+
You use these credentials to add MCG as a replication repository.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * backup_and_restore/application_backup_and_restore/installing/installing-oadp-aws.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-configuring-aws-s3_{context}"]
= Configuring Amazon Web Services

You configure Amazon Web Services (AWS) S3 object storage as a replication repository for the {mtc-full} ({mtc-short}).

.Prerequisites

* You must have the link:https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html[AWS CLI] installed.
* The AWS S3 storage bucket must be accessible to the source and target clusters.
* If you are using the snapshot copy method:
** You must have access to EC2 Elastic Block Storage (EBS).
** The source and target clusters must be in the same region.
** The source and target clusters must have the same storage class.
** The storage class must be compatible with snapshots.

.Procedure

. Set the `BUCKET` variable:
+
[source,terminal]
----
$ BUCKET=<your_bucket>
----

. Set the `REGION` variable:
+
[source,terminal]
----
$ REGION=<your_region>
----

. Create an AWS S3 bucket:
+
[source,terminal]
----
$ aws s3api create-bucket \
    --bucket $BUCKET \
    --region $REGION \
    --create-bucket-configuration LocationConstraint=$REGION <1>
----
<1> `us-east-1` does not support a `LocationConstraint`. If your region is `us-east-1`, omit `--create-bucket-configuration LocationConstraint=$REGION`.

. Create an IAM user:
+
[source,terminal]
----
$ aws iam create-user --user-name velero <1>
----
<1> If you want to use Velero to back up multiple clusters with multiple S3 buckets, create a unique user name for each cluster.

. Create a `velero-policy.json` file:
+
[source,terminal]
----
$ cat > velero-policy.json <<EOF
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "ec2:DescribeVolumes",
                "ec2:DescribeSnapshots",
                "ec2:CreateTags",
                "ec2:CreateVolume",
                "ec2:CreateSnapshot",
                "ec2:DeleteSnapshot"
            ],
            "Resource": "*"
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:DeleteObject",
                "s3:PutObject",
                "s3:AbortMultipartUpload",
                "s3:ListMultipartUploadParts"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}/*"
            ]
        },
        {
            "Effect": "Allow",
            "Action": [
                "s3:ListBucket",
                "s3:GetBucketLocation",
                "s3:ListBucketMultipartUploads"
            ],
            "Resource": [
                "arn:aws:s3:::${BUCKET}"
            ]
        }
    ]
}
EOF
----

. Attach the policies to give the `velero` user the minimum necessary permissions:
+
[source,terminal]
----
$ aws iam put-user-policy \
  --user-name velero \
  --policy-name velero \
  --policy-document file://velero-policy.json
----

. Create an access key for the `velero` user:
+
[source,terminal]
----
$ aws iam create-access-key --user-name velero
----
+
.Example output
+
[source,terminal]
----
{
  "AccessKey": {
        "UserName": "velero",
        "Status": "Active",
        "CreateDate": "2017-07-31T22:24:41.576Z",
        "SecretAccessKey": <AWS_SECRET_ACCESS_KEY>,
        "AccessKeyId": <AWS_ACCESS_KEY_ID>
  }
}
----
+
Record the `AWS_SECRET_ACCESS_KEY` and the `AWS_ACCESS_KEY_ID`. You use the credentials to add AWS as a replication repository.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * backup_and_restore/application_backup_and_restore/installing/installing-oadp-gcp.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-configuring-gcp_{context}"]
= Configuring Google Cloud Platform

You configure a Google Cloud Platform (GCP) storage bucket as a replication repository for the {mtc-full} ({mtc-short}).

.Prerequisites

* You must have the `gcloud` and `gsutil` CLI tools installed. See the link:https://cloud.google.com/sdk/docs/[Google cloud documentation] for details.

* The GCP storage bucket must be accessible to the source and target clusters.
* If you are using the snapshot copy method:
** The source and target clusters must be in the same region.
** The source and target clusters must have the same storage class.
** The storage class must be compatible with snapshots.

.Procedure

. Log in to GCP:
+
[source,terminal]
----
$ gcloud auth login
----

. Set the `BUCKET` variable:
+
[source,terminal]
----
$ BUCKET=<bucket> <1>
----
<1> Specify your bucket name.

. Create the storage bucket:
+
[source,terminal]
----
$ gsutil mb gs://$BUCKET/
----

. Set the `PROJECT_ID` variable to your active project:
+
[source,terminal]
----
$ PROJECT_ID=$(gcloud config get-value project)
----

. Create a service account:
+
[source,terminal]
----
$ gcloud iam service-accounts create velero \
    --display-name "Velero service account"
----

. List your service accounts:
+
[source,terminal]
----
$ gcloud iam service-accounts list
----

. Set the `SERVICE_ACCOUNT_EMAIL` variable to match its `email` value:
+
[source,terminal]
----
$ SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list \
    --filter="displayName:Velero service account" \
    --format 'value(email)')
----

. Attach the policies to give the `velero` user the minimum necessary permissions:
+
[source,terminal]
----
$ ROLE_PERMISSIONS=(
    compute.disks.get
    compute.disks.create
    compute.disks.createSnapshot
    compute.snapshots.get
    compute.snapshots.create
    compute.snapshots.useReadOnly
    compute.snapshots.delete
    compute.zones.get
    storage.objects.create
    storage.objects.delete
    storage.objects.get
    storage.objects.list
    iam.serviceAccounts.signBlob
)
----

. Create the `velero.server` custom role:
+
[source,terminal]
----
$ gcloud iam roles create velero.server \
    --project $PROJECT_ID \
    --title "Velero Server" \
    --permissions "$(IFS=","; echo "${ROLE_PERMISSIONS[*]}")"
----

. Add IAM policy binding to the project:
+
[source,terminal]
----
$ gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member serviceAccount:$SERVICE_ACCOUNT_EMAIL \
    --role projects/$PROJECT_ID/roles/velero.server
----

. Update the IAM service account:
+
[source,terminal]
----
$ gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_EMAIL:objectAdmin gs://${BUCKET}
----

. Save the IAM service account keys to the `credentials-velero` file in the current directory:
+
[source,terminal]
----
$ gcloud iam service-accounts keys create credentials-velero \
    --iam-account $SERVICE_ACCOUNT_EMAIL
----
+
You use the `credentials-velero` file to add GCP as a replication repository.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * backup_and_restore/application_backup_and_restore/installing/installing-oadp-azure.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-configuring-azure_{context}"]
= Configuring Microsoft Azure

You configure a Microsoft Azure Blob storage container as a replication repository for the {mtc-full} ({mtc-short}).

.Prerequisites

* You must have the link:https://docs.microsoft.com/en-us/cli/azure/install-azure-cli[Azure CLI] installed.
* The Azure Blob storage container must be accessible to the source and target clusters.
* If you are using the snapshot copy method:
** The source and target clusters must be in the same region.
** The source and target clusters must have the same storage class.
** The storage class must be compatible with snapshots.

.Procedure

. Log in to Azure:
+
[source,terminal]
----
$ az login
----

. Set the `AZURE_RESOURCE_GROUP` variable:
+
[source,terminal]
----
$ AZURE_RESOURCE_GROUP=Velero_Backups
----

. Create an Azure resource group:
+
[source,terminal]
----
$ az group create -n $AZURE_RESOURCE_GROUP --location CentralUS <1>
----
<1> Specify your location.

. Set the `AZURE_STORAGE_ACCOUNT_ID` variable:
+
[source,terminal]
----
$ AZURE_STORAGE_ACCOUNT_ID="velero$(uuidgen | cut -d '-' -f5 | tr '[A-Z]' '[a-z]')"
----

. Create an Azure storage account:
+
[source,terminal]
----
$ az storage account create \
    --name $AZURE_STORAGE_ACCOUNT_ID \
    --resource-group $AZURE_RESOURCE_GROUP \
    --sku Standard_GRS \
    --encryption-services blob \
    --https-only true \
    --kind BlobStorage \
    --access-tier Hot
----

. Set the `BLOB_CONTAINER` variable:
+
[source,terminal]
----
$ BLOB_CONTAINER=velero
----

. Create an Azure Blob storage container:
+
[source,terminal]
----
$ az storage container create \
  -n $BLOB_CONTAINER \
  --public-access off \
  --account-name $AZURE_STORAGE_ACCOUNT_ID
----

. Create a service principal and credentials for `velero`:
+
[source,terminal]
----
$ AZURE_SUBSCRIPTION_ID=`az account list --query '[?isDefault].id' -o tsv` \
  AZURE_TENANT_ID=`az account list --query '[?isDefault].tenantId' -o tsv` \
  AZURE_CLIENT_SECRET=`az ad sp create-for-rbac --name "velero" \
  --role "Contributor" --query 'password' -o tsv` \
  AZURE_CLIENT_ID=`az ad sp list --display-name "velero" \
  --query '[0].appId' -o tsv`
----

. Save the service principal credentials in the `credentials-velero` file:
+
[source,terminal]
----
$ cat << EOF > ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_TENANT_ID=${AZURE_TENANT_ID}
AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
AZURE_CLOUD_NAME=AzurePublicCloud
EOF
----
+
You use the `credentials-velero` file to add Azure as a replication repository.

:leveloffset!:

[role="_additional-resources"]
[id="{context}_configuring-replication-repository-additional-resources"]
=== Additional resources

* xref:../migrating_from_ocp_3_to_4/about-mtc-3-4.adoc#migration-mtc-workflow_about-mtc-3-4[{mtc-short} workflow]
* xref:../migrating_from_ocp_3_to_4/about-mtc-3-4.adoc#migration-understanding-data-copy-methods_about-mtc-3-4[About data copy methods]
* xref:../migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc#migration-adding-replication-repository-to-cam_migrating-applications-3-4[Adding a replication repository to the {mtc-short} web console]

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/troubleshooting-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-uninstalling-mtc-clean-up_{context}"]
= Uninstalling {mtc-short} and deleting resources

You can uninstall the {mtc-full} ({mtc-short}) and delete its resources to clean up the cluster.

[NOTE]
====
Deleting the `velero` CRDs removes Velero from the cluster.
====

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges.

.Procedure

. Delete the `MigrationController` custom resource (CR) on all clusters:
+
[source,terminal]
----
$ oc delete migrationcontroller <migration_controller>
----

. Uninstall the {mtc-full} Operator on {product-title} 4 by using the Operator Lifecycle Manager.

. Delete cluster-scoped resources on all clusters by running the following commands:

* `migration` custom resource definitions (CRDs):
+
[source,terminal]
----
$ oc delete $(oc get crds -o name | grep 'migration.openshift.io')
----

* `velero` CRDs:
+
[source,terminal]
----
$ oc delete $(oc get crds -o name | grep 'velero')
----

* `migration` cluster roles:
+
[source,terminal]
----
$ oc delete $(oc get clusterroles -o name | grep 'migration.openshift.io')
----

* `migration-operator` cluster role:
+
[source,terminal]
----
$ oc delete clusterrole migration-operator
----

* `velero` cluster roles:
+
[source,terminal]
----
$ oc delete $(oc get clusterroles -o name | grep 'velero')
----

* `migration` cluster role bindings:
+
[source,terminal]
----
$ oc delete $(oc get clusterrolebindings -o name | grep 'migration.openshift.io')
----

* `migration-operator` cluster role bindings:
+
[source,terminal]
----
$ oc delete clusterrolebindings migration-operator
----

* `velero` cluster role bindings:
+
[source,terminal]
----
$ oc delete $(oc get clusterrolebindings -o name | grep 'velero')
----

:leveloffset!:
:installing-3-4!:

//# includes=_attributes/common-attributes,modules/migration-compatibility-guidelines,modules/migration-installing-legacy-operator,modules/migration-installing-mtc-on-ocp-4,modules/migration-about-configuring-proxies,modules/migration-configuring-proxies,modules/migration-configuring-mcg,modules/migration-configuring-aws-s3,modules/migration-configuring-gcp,modules/migration-configuring-azure,modules/migration-uninstalling-mtc-clean-up

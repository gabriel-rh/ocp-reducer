:_mod-docs-content-type: ASSEMBLY
[id="advanced-migration-options-3-4"]
= Advanced migration options
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: advanced-migration-options-3-4
:advanced-migration-options-3-4:

toc::[]

You can automate your migrations and modify the `MigPlan` and `MigrationController` custom resources in order to perform large-scale migrations and to improve performance.

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/about-mtc-3-4.adoc
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/about-mtc.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

[id="migration-terminology_{context}"]
= Terminology

[cols="1,3a", options="header"]
.{mtc-short} terminology
|===
|Term |Definition
|Source cluster |Cluster from which the applications are migrated.
|Destination cluster^[1]^ |Cluster to which the applications are migrated.
|Replication repository |Object storage used for copying images, volumes, and Kubernetes objects during indirect migration or for Kubernetes objects during direct volume migration or direct image migration.

The replication repository must be accessible to all clusters.

|Host cluster |Cluster on which the `migration-controller` pod and the web console are running. The host cluster is usually the destination cluster but this is not required.

The host cluster does not require an exposed registry route for direct image migration.
|Remote cluster |A remote cluster is usually the source cluster but this is not required.

A remote cluster requires a `Secret` custom resource that contains the `migration-controller` service account token.

A remote cluster requires an exposed secure registry route for direct image migration.

|Indirect migration |Images, volumes, and Kubernetes objects are copied from the source cluster to the replication repository and then from the replication repository to the destination cluster.
|Direct volume migration |Persistent volumes are copied directly from the source cluster to the destination cluster.
|Direct image migration |Images are copied directly from the source cluster to the destination cluster.
|Stage migration |Data is copied to the destination cluster without stopping the application.

Running a stage migration multiple times reduces the duration of the cutover migration.
|Cutover migration |The application is stopped on the source cluster and its resources are migrated to the destination cluster.
|State migration |Application state is migrated by copying specific persistent volume claims to the destination cluster.
|Rollback migration |Rollback migration rolls back a completed migration.
|===
^1^  Called the _target_ cluster in the {mtc-short} web console.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc
:_mod-docs-content-type: PROCEDURE
[id="migration-migrating-applications-on-prem-to-cloud_{context}"]
= Migrating an application from on-premises to a cloud-based cluster

You can migrate from a source cluster that is behind a firewall to a cloud-based destination cluster by establishing a network tunnel between the two clusters. The `crane tunnel-api` command establishes such a tunnel by creating a VPN tunnel on the source cluster and then connecting to a VPN server running on the destination cluster. The VPN server is exposed to the client using a load balancer address on the destination cluster.

A service created on the destination cluster exposes the source cluster's API to {mtc-short}, which is running on the destination cluster.

.Prerequisites

* The system that creates the VPN tunnel must have access and be logged in to both clusters.
* It must be possible to create a load balancer on the destination cluster. Refer to your cloud provider to ensure this is possible.
* Have names prepared to assign to namespaces, on both the source cluster and the destination cluster, in which to run the VPN tunnel. These namespaces should not be created in advance. For information about namespace rules, see \https://kubernetes.io/docs/concepts/overview/working-with-objects/names/#dns-subdomain-names.
* When connecting multiple firewall-protected source clusters to the cloud cluster, each source cluster requires its own namespace.
* OpenVPN server is installed on the destination cluster.
* OpenVPN client is installed on the source cluster.
* When configuring the source cluster in {mtc-short}, the API URL takes the form of `\https://proxied-cluster.<namespace>.svc.cluster.local:8443`.
** If you use the API, see _Create a MigCluster CR manifest for each remote cluster_.
** If you use the {mtc-short} web console, see _Migrating your applications using the {mtc-short} web console_.
* The {mtc-short} web console and Migration Controller must be installed on the target cluster.

.Procedure

. Install the `crane` utility:
+
[source,terminal,subs=attributes+]
----
$ podman cp $(podman create registry.redhat.io/rhmtc/openshift-migration-controller-rhel8:v{mtc-version}):/crane ./
----
. Log in remotely to a node on the source cluster and a node on the destination cluster.

. Obtain the cluster context for both clusters after logging in:
+
[source,terminal,subs="+quotes"]
----
$ oc config view
----

. Establish a tunnel by entering the following command on the command system:
+
[source,terminal,sub="+quotes"]
----
$ crane tunnel-api [--namespace <namespace>] \
      --destination-context <destination-cluster> \
      --source-context <source-cluster>
----
+
If you don't specify a namespace, the command uses the default value `openvpn`.
+
For example:
+
[source,terminal,subs="+quotes"]
----
$ crane tunnel-api --namespace my_tunnel \
      --destination-context openshift-migration/c131-e-us-east-containers-cloud-ibm-com/admin \
      --source-context default/192-168-122-171-nip-io:8443/admin
----
+
[TIP]
====
See all available parameters for the `crane tunnel-api` command by entering `crane tunnel-api --help`.
====
+
The command generates TSL/SSL Certificates. This process might take several minutes. A message appears when the process completes.
+
The OpenVPN server starts on the destination cluster and the OpenVPN client starts on the source cluster.
+
After a few minutes, the load balancer resolves on the source node.
+
[TIP]
====
You can view the log for the OpenVPN pods to check the status of this process by entering the following commands with root privileges:

[source,terminal,subs="+quotes"]
----
# oc get po -n <namespace>
----

.Example output
[source,terminal]
----
NAME            READY     STATUS      RESTARTS    AGE
<pod_name>    2/2       Running     0           44s
----

[source,terminal,subs="+quotes"]
----
# oc logs -f -n <namespace> <pod_name> -c openvpn
----
When the address of the load balancer is resolved, the message `Initialization Sequence Completed` appears at the end of the log.
====

. On the OpenVPN server, which is on a destination control node, verify that the `openvpn` service and the `proxied-cluster` service are running:
+
[source,terminal,subs="+quotes"]
----
$ oc get service -n <namespace>
----

. On the source node, get the service account (SA) token for the migration controller:
+
[source,terminal]
----
# oc sa get-token -n openshift-migration migration-controller
----

. Open the {mtc-short} web console and add the source cluster, using the following values:
+
* *Cluster name*: The source cluster name.
* *URL*: `proxied-cluster.<namespace>.svc.cluster.local:8443`. If you did not define a value for `<namespace>`, use `openvpn`.
* *Service account token*: The token of the migration controller service account.
* *Exposed route host to image registry*: `proxied-cluster.<namespace>.svc.cluster.local:5000`. If you did not define a value for `<namespace>`, use `openvpn`.

After {mtc-short} has successfully validated the connection, you can proceed to create and run a migration plan. The namespace for the source cluster should appear in the list of namespaces.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* For information about creating a MigCluster CR manifest for each remote cluster, see xref:../migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc#migration-migrating-applications-api_advanced-migration-options-3-4[Migrating an application by using the {mtc-short} API].
* For information about adding a cluster using the web console, see xref:../migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc#migrating-applications-mtc-web-console_migrating-applications-3-4[Migrating your applications by using the {mtc-short} web console]

[id="migrating-applications-cli_{context}"]
== Migrating applications by using the command line

You can migrate applications with the {mtc-short} API by using the command line interface (CLI) in order to automate the migration.

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

[id="migration-prerequisites_{context}"]
= Migration prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.

.Direct image migration

* You must ensure that the secure {product-registry} of the source cluster is exposed.
* You must create a route to the exposed registry.

.Direct volume migration

* If your clusters use proxies, you must configure an Stunnel TCP proxy.

.Internal images

* If your application uses internal images from the `openshift` namespace, you must ensure that the required versions of the images are present on the target cluster.
+
You can manually update an image stream tag in order to use a deprecated {product-title} 3 image on an {product-title} {product-version} cluster.

.Clusters

* The source cluster must be upgraded to the latest {mtc-short} z-stream release.
* The {mtc-short} version must be the same on all clusters.

.Network

* The clusters have unrestricted network access to each other and to the replication repository.
* If you copy the persistent volumes with `move`, the clusters must have unrestricted network access to the remote volumes.
* You must enable the following ports on an {product-title} 3 cluster:
** `8443` (API server)
** `443` (routes)
** `53` (DNS)
* You must enable the following ports on an {product-title} 4 cluster:
** `6443` (API server)
** `443` (routes)
** `53` (DNS)
* You must enable port `443` on the replication repository if you are using TLS.

.Persistent volumes (PVs)

* The PVs must be valid.
* The PVs must be bound to persistent volume claims.
* If you use snapshots to copy the PVs, the following additional prerequisites apply:
** The cloud provider must support snapshots.
** The PVs must have the same cloud provider.
** The PVs must be located in the same geographic region.
** The PVs must have the same storage class.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-creating-registry-route-for-dim_{context}"]
= Creating a registry route for direct image migration

For direct image migration, you must create a route to the exposed {product-registry} on all remote clusters.

.Prerequisites

* The {product-registry} must be exposed to external traffic on all remote clusters.
+
The {product-title} 4 registry is exposed by default.
+
The {product-title} 3 registry must be link:https://docs.openshift.com/container-platform/3.11/install_config/registry/securing_and_exposing_registry.html#exposing-the-registry[exposed manually].

.Procedure

* To create a route to an {product-title} 3 registry, run the following command:
+
[source,terminal]
----
$ oc create route passthrough --service=docker-registry -n default
----

* To create a route to an {product-title} 4 registry, run the following command:
+
[source,terminal]
----
$ oc create route passthrough --service=image-registry -n openshift-image-registry
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: CONCEPT
[id="migration-about-configuring-proxies_{context}"]
= Proxy configuration

For {product-title} 4.1 and earlier versions, you must configure proxies in the `MigrationController` custom resource (CR) manifest after you install the {mtc-full} Operator because these versions do not support a cluster-wide `proxy` object.

For {product-title} 4.2 to {product-version}, the {mtc-full} ({mtc-short}) inherits the cluster-wide proxy settings. You can change the proxy parameters if you want to override the cluster-wide proxy settings.

[id="direct-volume-migration_{context}"]
== Direct volume migration

Direct Volume Migration (DVM) was introduced in MTC 1.4.2. DVM supports only one proxy. The source cluster cannot access the route of the target cluster if the target cluster is also behind a proxy.

If you want to perform a DVM from a source cluster behind a proxy, you must configure a TCP proxy that works at the transport layer and forwards the SSL connections transparently without decrypting and re-encrypting them with their own SSL certificates. A Stunnel proxy is an example of such a proxy.

[id="tcp-proxy-setup-for-dvm_{context}"]
=== TCP proxy setup for DVM

You can set up a direct connection between the source and the target cluster through a TCP proxy and configure the `stunnel_tcp_proxy` variable in the `MigrationController` CR to use the proxy:

[source, yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  stunnel_tcp_proxy: http://username:password@ip:port
----

Direct volume migration (DVM) supports only basic authentication for the proxy. Moreover, DVM works only from behind proxies that can tunnel a TCP connection transparently. HTTP/HTTPS proxies in man-in-the-middle mode do not work. The existing cluster-wide proxies might not support this behavior. As a result, the proxy settings for DVM are intentionally kept different from the usual proxy configuration in {mtc-short}.

[id="why-tcp-proxy-instead-of-an-http-https-proxy_{context}"]
=== Why use a TCP proxy instead of an HTTP/HTTPS proxy?

You can enable DVM by running Rsync between the source and the target cluster over an OpenShift route.  Traffic is encrypted using Stunnel, a TCP proxy. The Stunnel running on the source cluster initiates a TLS connection with the target Stunnel and transfers data over an encrypted channel.

Cluster-wide HTTP/HTTPS proxies in OpenShift are usually configured in man-in-the-middle mode where they negotiate their own TLS session with the outside servers. However, this does not work with Stunnel. Stunnel requires that its TLS session be untouched by the proxy, essentially making the proxy a transparent tunnel which simply forwards the TCP connection as-is. Therefore, you must use a TCP proxy.

[id="dvm-known-issues_{context}"]
=== Known issue

.Migration fails with error `Upgrade request required`

The migration Controller uses the SPDY protocol to execute commands within remote pods. If the remote cluster is behind a proxy or a firewall that does not support the SPDY protocol, the migration controller fails to execute remote commands. The migration fails with the error message `Upgrade request required`.
Workaround: Use a proxy that supports the SPDY protocol.

In addition to supporting the SPDY protocol, the proxy or firewall also must pass the `Upgrade` HTTP header to the API server. The client uses this header to open a websocket connection with the API server. If the `Upgrade` header is blocked by the proxy or firewall, the migration fails with the error message `Upgrade request required`.
Workaround: Ensure that the proxy forwards the `Upgrade` header.

[id="tuning-network-policies-for-migrations_{context}"]
== Tuning network policies for migrations

OpenShift supports restricting traffic to or from pods using _NetworkPolicy_ or _EgressFirewalls_ based on the network plugin used by the cluster. If any of the source namespaces involved in a migration use such mechanisms to restrict network traffic to pods, the restrictions might inadvertently stop traffic to Rsync pods during migration.

Rsync pods running on both the source and the target clusters must connect to each other over an OpenShift Route. Existing _NetworkPolicy_ or _EgressNetworkPolicy_ objects can be configured to automatically exempt Rsync pods from these traffic restrictions.

[id="dvm-network-policy-configuration_{context}"]
=== NetworkPolicy configuration

[id="egress-traffic-from-rsync-pods_{context}"]
==== Egress traffic from Rsync pods

You can use the unique labels of Rsync pods to allow egress traffic to pass from them if the `NetworkPolicy` configuration in the source or destination namespaces blocks this type of traffic. The following policy allows *all* egress traffic from Rsync pods in the namespace:

[source, yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  egress:
  - {}
  policyTypes:
  - Egress
----

[id="ingress-traffic-to-rsync-pods_{context}"]
==== Ingress traffic to Rsync pods

[source, yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-all-egress-from-rsync-pods
spec:
  podSelector:
    matchLabels:
      owner: directvolumemigration
      app: directvolumemigration-rsync-transfer
  ingress:
  - {}
  policyTypes:
  - Ingress
----

[id="egressnetworkpolicy-config_{context}"]
=== EgressNetworkPolicy configuration

The `EgressNetworkPolicy` object or _Egress Firewalls_ are OpenShift constructs designed to block egress traffic leaving the cluster.

Unlike the `NetworkPolicy` object, the Egress Firewall works at a project level because it applies to all pods in the namespace. Therefore, the unique labels of Rsync pods do not exempt only Rsync pods from the restrictions. However, you can add the CIDR ranges of the source or target cluster to the _Allow_ rule of the policy so that a direct connection can be setup between two clusters.

Based on which cluster the Egress Firewall is present in, you can add the CIDR range of the other cluster to allow egress traffic between the two:

[source, yaml]
----
apiVersion: network.openshift.io/v1
kind: EgressNetworkPolicy
metadata:
  name: test-egress-policy
  namespace: <namespace>
spec:
  egress:
  - to:
      cidrSelector: <cidr_of_source_or_target_cluster>
    type: Deny
----

[id="choosing-alternate-endpoints-for-data-transfer_{context}"]
=== Choosing alternate endpoints for data transfer

By default, DVM uses an {product-title} route as an endpoint to transfer PV data to destination clusters. You can choose another type of supported endpoint, if cluster topologies allow.

For each cluster, you can configure an endpoint by setting the `rsync_endpoint_type` variable on the appropriate *destination* cluster in your `MigrationController` CR:

[source, yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  [...]
  rsync_endpoint_type: [NodePort|ClusterIP|Route]
----

[id="configuring-supplemental-groups-for-rsync-pods_{context}"]
=== Configuring supplemental groups for Rsync pods
When your PVCs use a shared storage, you can configure the access to that storage by adding supplemental groups to Rsync pod definitions in order for the pods to allow access:

.Supplementary groups for Rsync pods
[option="header"]
|===
|Variable|Type|Default|Description

|`src_supplemental_groups`
|string
|Not set
|Comma-separated list of supplemental groups for source Rsync pods

|`target_supplemental_groups`
|string
|Not set
|Comma-separated list of supplemental groups for target Rsync pods
|===

.Example usage

The `MigrationController` CR can be updated to set values for these supplemental groups:

[source, yaml]
----
spec:
  src_supplemental_groups: "1000,2000"
  target_supplemental_groups: "2000,3000"
----

:leveloffset!:
:leveloffset: +3

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/installing-3-4.adoc
// * migrating_from_ocp_3_to_4/installing-restricted-3-4.adoc
// * migration_toolkit_for_containers/installing-mtc.adoc
// * migration_toolkit_for_containers/installing-mtc-restricted.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-configuring-proxies_{context}"]
= Configuring proxies

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.

.Procedure

. Get the `MigrationController` CR manifest:
+
[source,terminal]
----
$ oc get migrationcontroller <migration_controller> -n openshift-migration
----

. Update the proxy parameters:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: <migration_controller>
  namespace: openshift-migration
...
spec:
  stunnel_tcp_proxy: http://<username>:<password>@<ip>:<port> <1>
  noProxy: example.com <2>
----
<1> Stunnel proxy URL for direct volume migration.
<2> Comma-separated list of destination domain names, domains, IP addresses, or other network CIDRs to exclude proxying.
+
Preface a domain with `.` to match subdomains only. For example, `.y.com` matches `x.y.com`, but not `y.com`. Use `*` to bypass proxy for all destinations.
If you scale up workers that are not included in the network defined by the `networking.machineNetwork[].cidr` field from the installation configuration, you must add them to this list to prevent connection issues.
+
This field is ignored if neither the `httpProxy` nor the `httpsProxy` field is set.

. Save the manifest as `migration-controller.yaml`.
. Apply the updated manifest:
+
[source,terminal]
----
$ oc replace -f migration-controller.yaml -n openshift-migration
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-migrating-applications-api_{context}"]
= Migrating an application by using the {mtc-short} API

You can migrate an application from the command line by using the {mtc-full} ({mtc-short}) API.

.Procedure

. Create a `MigCluster` CR manifest for the host cluster:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  name: <host_cluster>
  namespace: openshift-migration
spec:
  isHostCluster: true
EOF
----

. Create a `Secret` object manifest for each remote cluster:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  name: <cluster_secret>
  namespace: openshift-config
type: Opaque
data:
  saToken: <sa_token> <1>
EOF
----
<1> Specify the base64-encoded `migration-controller` service account (SA) token of the remote cluster. You can obtain the token by running the following command:
+
[source,terminal]
----
$ oc sa get-token migration-controller -n openshift-migration | base64 -w 0
----

. Create a `MigCluster` CR manifest for each remote cluster:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigCluster
metadata:
  name: <remote_cluster> <1>
  namespace: openshift-migration
spec:
  exposedRegistryPath: <exposed_registry_route> <2>
  insecure: false <3>
  isHostCluster: false
  serviceAccountSecretRef:
    name: <remote_cluster_secret> <4>
    namespace: openshift-config
  url: <remote_cluster_url> <5>
EOF
----
<1> Specify the `Cluster` CR of the remote cluster.
<2> Optional: For direct image migration, specify the exposed registry route.
<3> SSL verification is enabled if `false`. CA certificates are not required or checked if `true`.
<4> Specify the `Secret` object of the remote cluster.
<5> Specify the URL of the remote cluster.

. Verify that all clusters are in a `Ready` state:
+
[source,terminal]
----
$ oc describe cluster <cluster>
----

. Create a `Secret` object manifest for the replication repository:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: v1
kind: Secret
metadata:
  namespace: openshift-config
  name: <migstorage_creds>
type: Opaque
data:
  aws-access-key-id: <key_id_base64> <1>
  aws-secret-access-key: <secret_key_base64> <2>
EOF
----
<1> Specify the key ID in base64 format.
<2> Specify the secret key in base64 format.
+
AWS credentials are base64-encoded by default. For other storage providers, you must encode your credentials by running the following command with each key:
+
[source,terminal]
----
$ echo -n "<key>" | base64 -w 0 <1>
----
<1> Specify the key ID or the secret key. Both keys must be base64-encoded.

. Create a `MigStorage` CR manifest for the replication repository:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigStorage
metadata:
  name: <migstorage>
  namespace: openshift-migration
spec:
  backupStorageConfig:
    awsBucketName: <bucket> <1>
    credsSecretRef:
      name: <storage_secret> <2>
      namespace: openshift-config
  backupStorageProvider: <storage_provider> <3>
  volumeSnapshotConfig:
    credsSecretRef:
      name: <storage_secret> <4>
      namespace: openshift-config
  volumeSnapshotProvider: <storage_provider> <5>
EOF
----
<1> Specify the bucket name.
<2> Specify the `Secrets` CR of the object storage. You must ensure that the credentials stored in the `Secrets` CR of the object storage are correct.
<3> Specify the storage provider.
<4> Optional: If you are copying data by using snapshots, specify the `Secrets` CR of the object storage. You must ensure that the credentials stored in the `Secrets` CR of the object storage are correct.
<5> Optional: If you are copying data by using snapshots, specify the storage provider.

. Verify that the `MigStorage` CR is in a `Ready` state:
+
[source,terminal]
----
$ oc describe migstorage <migstorage>
----

. Create a `MigPlan` CR manifest:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migplan>
  namespace: openshift-migration
spec:
  destMigClusterRef:
    name: <host_cluster>
    namespace: openshift-migration
  indirectImageMigration: true <1>
  indirectVolumeMigration: true <2>
  migStorageRef:
    name: <migstorage> <3>
    namespace: openshift-migration
  namespaces:
    - <source_namespace_1> <4>
    - <source_namespace_2>
    - <source_namespace_3>:<destination_namespace> <5>
  srcMigClusterRef:
    name: <remote_cluster> <6>
    namespace: openshift-migration
EOF
----
<1> Direct image migration is enabled if `false`.
<2> Direct volume migration is enabled if `false`.
<3> Specify the name of the `MigStorage` CR instance.
<4> Specify one or more source namespaces. By default, the destination namespace has the same name.
<5> Specify a destination namespace if it is different from the source namespace.
<6> Specify the name of the source cluster `MigCluster` instance.

. Verify that the `MigPlan` instance is in a `Ready` state:
+
[source,terminal]
----
$ oc describe migplan <migplan> -n openshift-migration
----

. Create a `MigMigration` CR manifest to start the migration defined in the `MigPlan` instance:
+
[source,yaml]
----
$ cat << EOF | oc apply -f -
apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  name: <migmigration>
  namespace: openshift-migration
spec:
  migPlanRef:
    name: <migplan> <1>
    namespace: openshift-migration
  quiescePods: true <2>
  stage: false <3>
  rollback: false <4>
EOF
----
<1> Specify the `MigPlan` CR name.
<2> The pods on the source cluster are stopped before migration if `true`.
<3> A stage migration, which copies most of the data without stopping the application, is performed if `true`.
<4> A completed migration is rolled back if `true`.

. Verify the migration by watching the `MigMigration` CR progress:
+
[source,terminal]
----
$ oc watch migmigration <migmigration> -n openshift-migration
----
+
The output resembles the following:
+
.Example output
+
[source,text]
----
Name:         c8b034c0-6567-11eb-9a4f-0bc004db0fbc
Namespace:    openshift-migration
Labels:       migration.openshift.io/migplan-name=django
Annotations:  openshift.io/touch: e99f9083-6567-11eb-8420-0a580a81020c
API Version:  migration.openshift.io/v1alpha1
Kind:         MigMigration
...
Spec:
  Mig Plan Ref:
    Name:       migplan
    Namespace:  openshift-migration
  Stage:        false
Status:
  Conditions:
    Category:              Advisory
    Last Transition Time:  2021-02-02T15:04:09Z
    Message:               Step: 19/47
    Reason:                InitialBackupCreated
    Status:                True
    Type:                  Running
    Category:              Required
    Last Transition Time:  2021-02-02T15:03:19Z
    Message:               The migration is ready.
    Status:                True
    Type:                  Ready
    Category:              Required
    Durable:               true
    Last Transition Time:  2021-02-02T15:04:05Z
    Message:               The migration registries are healthy.
    Status:                True
    Type:                  RegistriesHealthy
  Itinerary:               Final
  Observed Digest:         7fae9d21f15979c71ddc7dd075cb97061895caac5b936d92fae967019ab616d5
  Phase:                   InitialBackupCreated
  Pipeline:
    Completed:  2021-02-02T15:04:07Z
    Message:    Completed
    Name:       Prepare
    Started:    2021-02-02T15:03:18Z
    Message:    Waiting for initial Velero backup to complete.
    Name:       Backup
    Phase:      InitialBackupCreated
    Progress:
      Backup openshift-migration/c8b034c0-6567-11eb-9a4f-0bc004db0fbc-wpc44: 0 out of estimated total of 0 objects backed up (5s)
    Started:        2021-02-02T15:04:07Z
    Message:        Not started
    Name:           StageBackup
    Message:        Not started
    Name:           StageRestore
    Message:        Not started
    Name:           DirectImage
    Message:        Not started
    Name:           DirectVolume
    Message:        Not started
    Name:           Restore
    Message:        Not started
    Name:           Cleanup
  Start Timestamp:  2021-02-02T15:03:18Z
Events:
  Type    Reason   Age                 From                     Message
  ----    ------   ----                ----                     -------
  Normal  Running  57s                 migmigration_controller  Step: 2/47
  Normal  Running  57s                 migmigration_controller  Step: 3/47
  Normal  Running  57s (x3 over 57s)   migmigration_controller  Step: 4/47
  Normal  Running  54s                 migmigration_controller  Step: 5/47
  Normal  Running  54s                 migmigration_controller  Step: 6/47
  Normal  Running  52s (x2 over 53s)   migmigration_controller  Step: 7/47
  Normal  Running  51s (x2 over 51s)   migmigration_controller  Step: 8/47
  Normal  Ready    50s (x12 over 57s)  migmigration_controller  The migration is ready.
  Normal  Running  50s                 migmigration_controller  Step: 9/47
  Normal  Running  50s                 migmigration_controller  Step: 10/47
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-state-migration-cli_{context}"]
= State migration

You can perform repeatable, state-only migrations by using {mtc-full} ({mtc-short}) to migrate persistent volume claims (PVCs) that constitute an application's state. You migrate specified PVCs by excluding other PVCs from the migration plan. You can map the PVCs to ensure that the source and the target PVCs are synchronized. Persistent volume (PV) data is copied to the target cluster. The PV references are not moved, and the application pods continue to run on the source cluster.

State migration is specifically designed to be used in conjunction with external CD mechanisms, such as OpenShift Gitops. You can migrate application manifests using GitOps while migrating the state using {mtc-short}.

If you have a CI/CD pipeline, you can migrate stateless components by deploying them on the target cluster. Then you can migrate stateful components by using {mtc-short}.

You can perform a state migration between clusters or within the same cluster.

[IMPORTANT]
====
State migration migrates only the components that constitute an application's state. If you want to migrate an entire namespace, use stage or cutover migration.
====

.Prerequisites

* The state of the application on the source cluster is persisted in `PersistentVolumes` provisioned through `PersistentVolumeClaims`.

* The manifests of the application are available in a central repository that is accessible from both the source and the target clusters.


.Procedure

. Migrate persistent volume data from the source to the target cluster.
+
You can perform this step as many times as needed. The source application continues running.

. Quiesce the source application.
+
You can do this by setting the replicas of workload resources to `0`, either directly on the source cluster or by updating the manifests in GitHub and re-syncing the Argo CD application.

. Clone application manifests to the target cluster.
+
You can use Argo CD to clone the application manifests to the target cluster.

. Migrate the remaining volume data from the source to the target cluster.
+
Migrate any new data created by the application during the state migration process by performing a final data migration.

. If the cloned application is in a quiesced state, unquiesce it.

. Switch the DNS record to the target cluster to re-direct user traffic to the migrated application.

[NOTE]
====
{mtc-short} 1.6 cannot quiesce applications automatically when performing state migration. It can only migrate PV data. Therefore, you must use your CD mechanisms for quiescing or unquiescing applications.

{mtc-short} 1.7 introduces explicit Stage and Cutover flows. You can use staging to perform initial data transfers as many times as needed. Then you can perform a cutover, in which the source applications are quiesced automatically.
====

:leveloffset!:

[role="_additional-resources"]
[id="additional-resources-for-state-migration_{context}"]
[discrete]
=== Additional resources

* See xref:../migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc#migration-excluding-pvcs_advanced-migration-options-3-4[Excluding PVCs from migration] to select PVCs for state migration.
* See xref:../migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc#migration-mapping-pvcs_advanced-migration-options-3-4[Mapping PVCs] to migrate source PV data to provisioned PVCs on the destination cluster.
* See xref:../migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc#migration-kubernetes-objects_advanced-migration-options-3-4[Migrating Kubernetes objects] to migrate the Kubernetes objects that constitute an application's state.

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

[id="migration-hooks_{context}"]
= Migration hooks

You can add up to four migration hooks to a single migration plan, with each hook running at a different phase of the migration. Migration hooks perform tasks such as customizing application quiescence, manually migrating unsupported data types, and updating applications after migration.

A migration hook runs on a source or a target cluster at one of the following migration steps:

* `PreBackup`: Before resources are backed up on the source cluster.
* `PostBackup`: After resources are backed up on the source cluster.
* `PreRestore`: Before resources are restored on the target cluster.
* `PostRestore`: After resources are restored on the target cluster.

You can create a hook by creating an Ansible playbook that runs with the default Ansible image or with a custom hook container.

.Ansible playbook

The Ansible playbook is mounted on a hook container as a config map. The hook container runs as a job, using the cluster, service account, and namespace specified in the `MigPlan` custom resource. The job continues to run until it reaches the default limit of 6 retries or a successful completion. This continues even if the initial pod is evicted or killed.

The default Ansible runtime image is `registry.redhat.io/rhmtc/openshift-migration-hook-runner-rhel7:{mtc-version}`. This image is based on the Ansible Runner image and includes `python-openshift` for Ansible Kubernetes resources and an updated `oc` binary.

.Custom hook container

You can use a custom hook container instead of the default Ansible image.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

[id="migration-writing-ansible-playbook-hook_{context}"]
= Writing an Ansible playbook for a migration hook

You can write an Ansible playbook to use as a migration hook. The hook is added to a migration plan by using the {mtc-short} web console or by specifying values for the `spec.hooks` parameters in the `MigPlan` custom resource (CR) manifest.

The Ansible playbook is mounted onto a hook container as a config map. The hook container runs as a job, using the cluster, service account, and namespace specified in the `MigPlan` CR. The hook container uses a specified service account token so that the tasks do not require authentication before they run in the cluster.

[id="migration-writing-ansible-playbook-hook-ansible-modules_{context}"]
== Ansible modules

You can use the Ansible `shell` module to run `oc` commands.

.Example `shell` module
[source,yaml]
----
- hosts: localhost
  gather_facts: false
  tasks:
  - name: get pod name
    shell: oc get po --all-namespaces
----

You can use `kubernetes.core` modules, such as `k8s_info`, to interact with Kubernetes resources.

.Example `k8s_facts` module
[source,yaml]
----
- hosts: localhost
  gather_facts: false
  tasks:
  - name: Get pod
    k8s_info:
      kind: pods
      api: v1
      namespace: openshift-migration
      name: "{{ lookup( 'env', 'HOSTNAME') }}"
    register: pods

  - name: Print pod name
    debug:
      msg: "{{ pods.resources[0].metadata.name }}"
----

You can use the `fail` module to produce a non-zero exit status in cases where a non-zero exit status would not normally be produced, ensuring that the success or failure of a hook is detected. Hooks run as jobs and the success or failure status of a hook is based on the exit status of the job container.

.Example `fail` module
[source,yaml]
----
- hosts: localhost
  gather_facts: false
  tasks:
  - name: Set a boolean
    set_fact:
      do_fail: true

  - name: "fail"
    fail:
      msg: "Cause a failure"
    when: do_fail
----

[id="migration-writing-ansible-playbook-hook-environment-variables_{context}"]
== Environment variables

The `MigPlan` CR name and migration namespaces are passed as environment variables to the hook container. These variables are accessed by using the `lookup` plugin.

.Example environment variables
[source,yaml]
----
- hosts: localhost
  gather_facts: false
  tasks:
  - set_fact:
      namespaces: "{{ (lookup( 'env', 'MIGRATION_NAMESPACES')).split(',') }}"

  - debug:
      msg: "{{ item }}"
    with_items: "{{ namespaces }}"

  - debug:
      msg: "{{ lookup( 'env', 'MIGRATION_PLAN_NAME') }}"
----

:leveloffset!:

[id="migration-plan-options_{context}"]
== Migration plan options

You can exclude, edit, and map components in the `MigPlan` custom resource (CR).

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-excluding-resources_{context}"]
= Excluding resources

You can exclude resources, for example, image streams, persistent volumes (PVs), or subscriptions, from a {mtc-full} ({mtc-short}) migration plan to reduce the resource load for migration or to migrate images or PVs with a different tool.

By default, the {mtc-short} excludes service catalog resources and Operator Lifecycle Manager (OLM) resources from migration. These resources are parts of the service catalog API group and the OLM API group, neither of which is supported for migration at this time.

.Procedure

. Edit the `MigrationController` custom resource manifest:
+
[source,terminal]
----
$ oc edit migrationcontroller <migration_controller> -n openshift-migration
----

. Update the `spec` section by adding parameters to exclude specific resources. For those resources that do not have their own exclusion parameters, add the `additional_excluded_resources` parameter:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigrationController
metadata:
  name: migration-controller
  namespace: openshift-migration
spec:
  disable_image_migration: true <1>
  disable_pv_migration: true <2>
  additional_excluded_resources: <3>
  - resource1
  - resource2
  ...
----
<1> Add `disable_image_migration: true` to exclude image streams from the migration. `imagestreams` is added to the `excluded_resources` list in `main.yml` when the `MigrationController` pod restarts.
<2> Add `disable_pv_migration: true` to exclude PVs from the migration plan. `persistentvolumes` and `persistentvolumeclaims` are added to the `excluded_resources` list in `main.yml` when the `MigrationController` pod restarts. Disabling PV migration also disables PV discovery when you create the migration plan.
<3> You can add {product-title} resources that you want to exclude to the `additional_excluded_resources` list.


. Wait two minutes for the `MigrationController` pod to restart so that the changes are applied.

. Verify that the resource is excluded:
+
[source,terminal]
----
$ oc get deployment -n openshift-migration migration-controller -o yaml | grep EXCLUDED_RESOURCES -A1
----
+
The output contains the excluded resources:
+
.Example output
[source,yaml]
----
name: EXCLUDED_RESOURCES
value:
resource1,resource2,imagetags,templateinstances,clusterserviceversions,packagemanifests,subscriptions,servicebrokers,servicebindings,serviceclasses,serviceinstances,serviceplans,imagestreams,persistentvolumes,persistentvolumeclaims
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc

[id="migration-mapping-destination-namespaces-in-the-migplan-cr_{context}"]
= Mapping namespaces

If you map namespaces in the `MigPlan` custom resource (CR), you must ensure that the namespaces are not duplicated on the source or the destination clusters because the UID and GID ranges of the namespaces are copied during migration.

.Two source namespaces mapped to the same destination namespace
[source,yaml]
----
spec:
  namespaces:
    - namespace_2
    - namespace_1:namespace_2
----

If you want the source namespace to be mapped to a namespace of the same name, you do not need to create a mapping. By default, a source namespace and a target namespace have the same name.

.Incorrect namespace mapping
[source,yaml]
----
spec:
  namespaces:
    - namespace_1:namespace_1
----

.Correct namespace reference
[source,yaml]
----
spec:
  namespaces:
    - namespace_1
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-excluding-pvcs_{context}"]
= Excluding persistent volume claims

You select persistent volume claims (PVCs) for state migration by excluding the PVCs that you do not want to migrate. You exclude PVCs by setting the `spec.persistentVolumes.pvc.selection.action` parameter of the `MigPlan` custom resource (CR) after the persistent volumes (PVs) have been discovered.

.Prerequisites

* `MigPlan` CR is in a `Ready` state.

.Procedure

* Add the `spec.persistentVolumes.pvc.selection.action` parameter to the `MigPlan` CR and set it to `skip`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migplan>
  namespace: openshift-migration
spec:
...
  persistentVolumes:
  - capacity: 10Gi
    name: <pv_name>
    pvc:
...
    selection:
      action: skip
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-mapping-pvcs_{context}"]
= Mapping persistent volume claims

You can migrate persistent volume (PV) data from the source cluster to persistent volume claims (PVCs) that are already provisioned in the destination cluster in the `MigPlan` CR by mapping the PVCs. This mapping ensures that the destination PVCs of migrated applications are synchronized with the source PVCs.

You map PVCs by updating the `spec.persistentVolumes.pvc.name` parameter in the `MigPlan` custom resource (CR) after the PVs have been discovered.

.Prerequisites

* `MigPlan` CR is in a `Ready` state.

.Procedure

* Update the `spec.persistentVolumes.pvc.name` parameter in the `MigPlan` CR:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migplan>
  namespace: openshift-migration
spec:
...
  persistentVolumes:
  - capacity: 10Gi
    name: <pv_name>
    pvc:
      name: <source_pvc>:<destination_pvc> <1>
----
<1> Specify the PVC on the source cluster and the PVC on the destination cluster. If the destination PVC does not exist, it will be created. You can use this mapping to change the PVC name during migration.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-editing-pvs-in-migplan_{context}"]
= Editing persistent volume attributes

After you create a `MigPlan` custom resource (CR), the `MigrationController` CR discovers the persistent volumes (PVs). The `spec.persistentVolumes` block and the `status.destStorageClasses` block are added to the `MigPlan` CR.

You can edit the values in the `spec.persistentVolumes.selection` block. If you change values outside the `spec.persistentVolumes.selection` block, the values are overwritten when the `MigPlan` CR is reconciled by the `MigrationController` CR.

[NOTE]
====
The default value for the `spec.persistentVolumes.selection.storageClass` parameter is determined by the following logic:

. If the source cluster PV is Gluster or NFS, the default is either `cephfs`, for `accessMode: ReadWriteMany`, or `cephrbd`, for `accessMode: ReadWriteOnce`.
. If the PV is neither Gluster nor NFS _or_ if `cephfs` or `cephrbd` are not available, the default is a storage class for the same provisioner.
. If a storage class for the same provisioner is not available, the default is the default storage class of the destination cluster.

You can change the `storageClass` value to the value of any `name` parameter in the `status.destStorageClasses` block of the `MigPlan` CR.

If the `storageClass` value is empty, the PV will have no storage class after migration. This option is appropriate if, for example, you want to move the PV to an NFS volume on the destination cluster.
====


.Prerequisites

* `MigPlan` CR is in a `Ready` state.

.Procedure

* Edit the `spec.persistentVolumes.selection` values in the `MigPlan` CR:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migplan>
  namespace: openshift-migration
spec:
  persistentVolumes:
  - capacity: 10Gi
    name: pvc-095a6559-b27f-11eb-b27f-021bddcaf6e4
    proposedCapacity: 10Gi
    pvc:
      accessModes:
      - ReadWriteMany
      hasReference: true
      name: mysql
      namespace: mysql-persistent
    selection:
      action: <copy> <1>
      copyMethod: <filesystem> <2>
      verify: true <3>
      storageClass: <gp2> <4>
      accessMode: <ReadWriteMany> <5>
    storageClass: cephfs
----
<1> Allowed values are `move`, `copy`, and `skip`. If only one action is supported, the default value is the supported action. If multiple actions are supported, the default value is `copy`.
<2> Allowed values are `snapshot` and `filesystem`. Default value is `filesystem`.
<3> The `verify` parameter is displayed if you select the verification option for file system copy in the {mtc-short} web console. You can set it to `false`.
<4> You can change the default value to the value of any `name` parameter in the `status.destStorageClasses` block of the `MigPlan` CR. If no value is specified, the PV will have no storage class after migration.
<5> Allowed values are `ReadWriteOnce` and `ReadWriteMany`. If this value is not specified, the default is the access mode of the source cluster PVC. You can only edit the access mode in the `MigPlan` CR. You cannot edit it by using the {mtc-short} web console.

:leveloffset!:

[role="_additional-resources"]
[id="additional-resources-for-editing-pv-attributes_{context}"]
[discrete]
==== Additional resources

* For details about the `move` and `copy` actions, see xref:../migrating_from_ocp_3_to_4/about-mtc-3-4.adoc#migration-mtc-workflow_about-mtc-3-4[MTC workflow].
* For details about the `skip` action, see xref:../migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc#migration-excluding-pvcs_advanced-migration-options-3-4[Excluding PVCs from migration].
* For details about the file system and snapshot copy methods, see xref:../migrating_from_ocp_3_to_4/about-mtc-3-4.adoc#migration-understanding-data-copy-methods_about-mtc-3-4[About data copy methods].

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-kubernetes-objects_{context}"]
= Performing a state migration of Kubernetes objects by using the {mtc-short} API

After you migrate all the PV data, you can use the Migration Toolkit for Containers (MTC) API to perform a one-time state migration of Kubernetes objects that constitute an application.

You do this by configuring `MigPlan` custom resource (CR) fields to provide a list of Kubernetes resources with an additional label selector to further filter those resources, and then performing a migration by creating a `MigMigration` CR. The `MigPlan` resource is closed after the migration.

[NOTE]
====
Selecting Kubernetes resources is an API-only feature. You must update the `MigPlan` CR and create a `MigMigration` CR for it by using the CLI. The {mtc-short} web console does not support migrating Kubernetes objects.
====

[NOTE]
====
After migration, the `closed` parameter of the `MigPlan` CR is set to `true`. You cannot create another `MigMigration` CR for this `MigPlan` CR.
====

You add Kubernetes objects to the `MigPlan` CR by using one of the following options:

* Adding the Kubernetes objects to the `includedResources` section. When the `includedResources` field is specified in the `MigPlan` CR, the plan takes a list of `group-kind` as input. Only resources present in the list are included in the migration.
* Adding the optional `labelSelector` parameter to filter the `includedResources` in the `MigPlan`. When this field is specified, only resources matching the label selector are included in the migration. For example, you can filter a list of `Secret` and `ConfigMap` resources by using the label `app: frontend` as a filter.

.Procedure

. Update the `MigPlan` CR to include Kubernetes resources and, optionally, to filter the included resources by adding the `labelSelector` parameter:

.. To update the `MigPlan` CR to include Kubernetes resources:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migplan>
  namespace: openshift-migration
spec:
  includedResources:
  - kind: <kind> <1>
    group: ""
  - kind: <kind>
    group: ""
----
<1> Specify the Kubernetes object, for example, `Secret` or `ConfigMap`.

.. Optional: To filter the included resources by adding the `labelSelector` parameter:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigPlan
metadata:
  name: <migplan>
  namespace: openshift-migration
spec:
  includedResources:
  - kind: <kind> <1>
    group: ""
  - kind: <kind>
    group: ""
...
  labelSelector:
    matchLabels:
      <label> <2>
----
<1> Specify the Kubernetes object, for example, `Secret` or `ConfigMap`.
<2> Specify the label of the resources to migrate, for example, `app: frontend`.

. Create a `MigMigration` CR to migrate the selected Kubernetes resources. Verify that the correct `MigPlan` is referenced in `migPlanRef`:
+
[source,yaml]
----
apiVersion: migration.openshift.io/v1alpha1
kind: MigMigration
metadata:
  generateName: <migplan>
  namespace: openshift-migration
spec:
  migPlanRef:
    name: <migplan>
    namespace: openshift-migration
  stage: false
----

:leveloffset!:

[id="migration-controller-options_{context}"]
== Migration controller options

You can edit migration plan limits, enable persistent volume resizing, or enable cached Kubernetes clients in the `MigrationController` custom resource (CR) for large migrations and improved performance.

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-changing-migration-plan-limits_{context}"]
= Increasing limits for large migrations

You can increase the limits on migration objects and container resources for large migrations with the {mtc-full} ({mtc-short}).

[IMPORTANT]
====
You must test these changes before you perform a migration in a production environment.
====

.Procedure

. Edit the `MigrationController` custom resource (CR) manifest:
+
[source,terminal]
----
$ oc edit migrationcontroller -n openshift-migration
----

. Update the following parameters:
+
[source,yaml]
----
...
mig_controller_limits_cpu: "1" <1>
mig_controller_limits_memory: "10Gi" <2>
...
mig_controller_requests_cpu: "100m" <3>
mig_controller_requests_memory: "350Mi" <4>
...
mig_pv_limit: 100 <5>
mig_pod_limit: 100 <6>
mig_namespace_limit: 10 <7>
...
----
<1> Specifies the number of CPUs available to the `MigrationController` CR.
<2> Specifies the amount of memory available to the `MigrationController` CR.
<3> Specifies the number of CPU units available for `MigrationController` CR requests. `100m` represents 0.1 CPU units (100 * 1e-3).
<4> Specifies the amount of memory available for `MigrationController` CR requests.
<5> Specifies the number of persistent volumes that can be migrated.
<6> Specifies the number of pods that can be migrated.
<7> Specifies the number of namespaces that can be migrated.

. Create a migration plan that uses the updated parameters to verify the changes.
+
If your migration plan exceeds the `MigrationController` CR limits, the {mtc-short} console displays a warning message when you save the migration plan.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-enabling-pv-resizing-dvm_{context}"]
= Enabling persistent volume resizing for direct volume migration

You can enable persistent volume (PV) resizing for direct volume migration to avoid running out of disk space on the destination cluster.

When the disk usage of a PV reaches a configured level, the `MigrationController` custom resource (CR) compares the requested storage capacity of a persistent volume claim (PVC) to its actual provisioned capacity. Then, it calculates the space required on the destination cluster.

A `pv_resizing_threshold` parameter determines when PV resizing is used. The default threshold is `3%`. This means that PV resizing occurs when the disk usage of a PV is more than `97%`. You can increase this threshold so that PV resizing occurs at a lower disk usage level.

PVC capacity is calculated according to the following criteria:

* If the requested storage capacity (`spec.resources.requests.storage`) of the PVC is not equal to its actual provisioned capacity (`status.capacity.storage`), the greater value is used.
* If a PV is provisioned through a PVC and then subsequently changed so that its PV and PVC capacities no longer match, the greater value is used.

.Prerequisites

* The PVCs must be attached to one or more running pods so that the `MigrationController` CR can execute commands.

.Procedure

. Log in to the host cluster.
. Enable PV resizing by patching the `MigrationController` CR:
+
[source,terminal]
----
$ oc patch migrationcontroller migration-controller -p '{"spec":{"enable_dvm_pv_resizing":true}}' \ <1>
  --type='merge' -n openshift-migration
----
<1> Set the value to `false` to disable PV resizing.

. Optional: Update the `pv_resizing_threshold` parameter to increase the threshold:
+
[source,terminal]
----
$ oc patch migrationcontroller migration-controller -p '{"spec":{"pv_resizing_threshold":41}}' \ <1>
  --type='merge' -n openshift-migration
----
<1> The default value is `3`.
+
When the threshold is exceeded, the following status message is displayed in the `MigPlan` CR status:
+
[source,yaml]
----
status:
  conditions:
...
  - category: Warn
    durable: true
    lastTransitionTime: "2021-06-17T08:57:01Z"
    message: 'Capacity of the following volumes will be automatically adjusted to avoid disk capacity issues in the target cluster:  [pvc-b800eb7b-cf3b-11eb-a3f7-0eae3e0555f3]'
    reason: Done
    status: "False"
    type: PvCapacityAdjustmentRequired
----
+
[NOTE]
====
For AWS gp2 storage, this message does not appear unless the `pv_resizing_threshold` is 42% or greater because of the way gp2 calculates volume usage and size. (link:https://bugzilla.redhat.com/show_bug.cgi?id=1973148[*BZ#1973148*])
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-enabling-cached-kubernetes-clients_{context}"]
= Enabling cached Kubernetes clients

You can enable cached Kubernetes clients in the `MigrationController` custom resource (CR) for improved performance during migration. The greatest performance benefit is displayed when migrating between clusters in different regions or with significant network latency.

[NOTE]
====
Delegated tasks, for example, Rsync backup for direct volume migration or Velero backup and restore, however, do not show improved performance with cached clients.
====

Cached clients require extra memory because the `MigrationController` CR caches all API resources that are required for interacting with `MigCluster` CRs. Requests that are normally sent to the API server are directed to the cache instead. The cache watches the API server for updates.

You can increase the memory limits and requests of the `MigrationController` CR if `OOMKilled` errors occur after you enable cached clients.

.Procedure

. Enable cached clients by running the following command:
+
[source,terminal]
----
$ oc -n openshift-migration patch migrationcontroller migration-controller --type=json --patch \
  '[{ "op": "replace", "path": "/spec/mig_controller_enable_cache", "value": true}]'
----

. Optional: Increase the `MigrationController` CR memory limits by running the following command:
+
[source,terminal]
----
$ oc -n openshift-migration patch migrationcontroller migration-controller --type=json --patch \
  '[{ "op": "replace", "path": "/spec/mig_controller_limits_memory", "value": <10Gi>}]'
----

. Optional: Increase the `MigrationController` CR memory requests by running the following command:
+
[source,terminal]
----
$ oc -n openshift-migration patch migrationcontroller migration-controller --type=json --patch \
  '[{ "op": "replace", "path": "/spec/mig_controller_requests_memory", "value": <350Mi>}]'
----

:leveloffset!:

:advanced-migration-options-3-4!:

//# includes=_attributes/common-attributes,modules/migration-terminology,modules/migration-migrating-on-prem-to-cloud,modules/migration-prerequisites,modules/migration-creating-registry-route-for-dim,modules/migration-about-configuring-proxies,modules/migration-configuring-proxies,modules/migration-migrating-applications-api,modules/migration-state-migration-cli,modules/migration-hooks,modules/migration-writing-ansible-playbook-hook,modules/migration-excluding-resources,modules/migration-mapping-destination-namespaces-in-the-migplan-cr,modules/migration-excluding-pvcs,modules/migration-mapping-pvcs,modules/migration-editing-pvs-in-migplan,modules/migration-kubernetes-objects,modules/migration-changing-migration-plan-limits,modules/migration-enabling-pv-resizing-dvm,modules/migration-enabling-cached-kubernetes-clients

:_mod-docs-content-type: ASSEMBLY
[id="cpmso-troubleshooting"]
= Troubleshooting the control plane machine set
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: cpmso-troubleshooting

toc::[]

Use the information in this section to understand and recover from issues you might encounter.

//Checking the control plane machine set custom resource status
:leveloffset: +1

// Module included in the following assemblies:
//
// * machine_management/cpmso-getting-started.adoc
// * machine_management/cpmso-troubleshooting.adoc
// * machine_management/cpmso-disabling.adoc


:_mod-docs-content-type: PROCEDURE
[id="cpmso-checking-status_{context}"]
= Checking the control plane machine set custom resource state

You can verify the existence and state of the `ControlPlaneMachineSet` custom resource (CR).

.Procedure

* Determine the state of the CR by running the following command:
+
[source,terminal]
----
$ oc get controlplanemachineset.machine.openshift.io cluster \
  --namespace openshift-machine-api
----

** A result of `Active` indicates that the `ControlPlaneMachineSet` CR exists and is activated. No administrator action is required.

** A result of `Inactive` indicates that a `ControlPlaneMachineSet` CR exists but is not activated.

** A result of `NotFound` indicates that there is no existing `ControlPlaneMachineSet` CR.

.Next steps

To use the control plane machine set, you must ensure that a `ControlPlaneMachineSet` CR with the correct settings for your cluster exists.

* If your cluster has an existing CR, you must verify that the configuration in the CR is correct for your cluster.

* If your cluster does not have an existing CR, you must create one with the correct configuration for your cluster.


:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-activating_cpmso-getting-started[Activating the control plane machine set custom resource]
* xref:../../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-creating-cr_cpmso-getting-started[Creating a control plane machine set custom resource]

//Adding a missing Azure internal load balancer
:leveloffset: +1

// Module included in the following assemblies:
//
// * machine_management/cpmso-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="cpmso-ts-ilb-missing_{context}"]
= Adding a missing Azure internal load balancer

The `internalLoadBalancer` parameter is required in both the `ControlPlaneMachineSet` and control plane `Machine` custom resources (CRs) for Azure. If this parameter is not preconfigured on your cluster, you must add it to both CRs.

For more information about where this parameter is located in the Azure provider specification, see the sample Azure provider specification. The placement in the control plane `Machine` CR is similar.

.Procedure

. List the control plane machines in your cluster by running the following command:
+
[source,terminal]
----
$ oc get machines \
  -l machine.openshift.io/cluster-api-machine-role==master \
  -n openshift-machine-api
----

. For each control plane machine, edit the CR by running the following command:
+
[source,terminal]
----
$ oc edit machine <control_plane_machine_name>
----

. Add the `internalLoadBalancer` parameter with the correct details for your cluster and save your changes.

. Edit your control plane machine set CR by running the following command:
+
[source,terminal]
----
$ oc edit controlplanemachineset.machine.openshift.io cluster \
  -n openshift-machine-api
----

. Add the `internalLoadBalancer` parameter with the correct details for your cluster and save your changes.

.Next steps

* For clusters that use the default `RollingUpdate` update strategy, the Operator automatically propagates the changes to your control plane configuration.

* For clusters that are configured to use the `OnDelete` update strategy, you must replace your control plane machines manually.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../../machine_management/control_plane_machine_management/cpmso-configuration.adoc#cpmso-yaml-provider-spec-azure_cpmso-configuration[Sample Azure provider specification]

//Recovering a degraded etcd Operator after a machine health check operation
:leveloffset: +1

// Module included in the following assemblies:
//
// * machine_management/cpmso-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="cpmso-ts-etcd-degraded_{context}"]
= Recovering a degraded etcd Operator

Certain situations can cause the etcd Operator to become degraded.

For example, while performing remediation, the machine health check might delete a control plane machine that is hosting etcd. If the etcd member is not reachable at that time, the etcd Operator becomes degraded.

When the etcd Operator is degraded, manual intervention is required to force the Operator to remove the failed member and restore the cluster state.

.Procedure

. List the control plane machines in your cluster by running the following command:
+
[source,terminal]
----
$ oc get machines \
  -l machine.openshift.io/cluster-api-machine-role==master \
  -n openshift-machine-api \
  -o wide
----
+
Any of the following conditions might indicate a failed control plane machine:
+
--
** The `STATE` value is `stopped`.
** The `PHASE` value is `Failed`.
** The `PHASE` value is `Deleting` for more than ten minutes.
--
+
[IMPORTANT]
====
Before continuing, ensure that your cluster has two healthy control plane machines. Performing the actions in this procedure on more than one control plane machine risks losing etcd quorum and can cause data loss.

If you have lost the majority of your control plane hosts, leading to etcd quorum loss, then you must follow the disaster recovery procedure "Restoring to a previous cluster state" instead of this procedure.
====

. Edit the machine CR for the failed control plane machine by running the following command:
+
[source,terminal]
----
$ oc edit machine <control_plane_machine_name>
----

. Remove the contents of the `lifecycleHooks` parameter from the failed control plane machine and save your changes.
+
The etcd Operator removes the failed machine from the cluster and can then safely add new etcd members.

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../../backup_and_restore/control_plane_backup_and_restore/disaster_recovery/scenario-2-restoring-cluster-state.adoc#dr-restoring-cluster-state[Restoring to a previous cluster state]

[id="cpmso-troubleshooting-shiftstack-upgrade_{context}"]
== Upgrading clusters that run on {rh-openstack}

For clusters that run on {rh-openstack-first} that you upgrade from {product-title} 4.13 to 4.14, you might have to perform post-upgrade tasks before you can use control plane machine sets.

// TODO: Rejigger
// Post-upgrade config for ShiftStack with machine AZs explicitly defined and rootVolumes w/out AZs
:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/control_plane_machine_management/cpmso-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="cpmso-openstack-ts-root-volume-azs_{context}"]
= Configuring {rh-openstack} clusters that have machines with root volume availability zones after an upgrade

For some clusters that run on {rh-openstack-first} that you upgrade, you must manually update machine resources before you can use control plane machine sets if the following configurations are true:

* You upgraded the cluster from {product-title} 4.13 to 4.14.

* The cluster infrastructure is installer-provisioned.

* Machines were distributed across multiple availability zones.

* Machines were configured to use root volumes for which block storage availability zones were not defined.

To understand why this procedure is necessary, see link:https://access.redhat.com/solutions/7013893[Solution #7024383].

.Procedure

. For all control plane machines, edit the provider spec for all control plane machines that match the environment. For example, to edit the machine `master-0`, enter the following command:
+
[source,terminal]
----
$ oc edit machine/<cluster_id>-master-0 -n openshift-machine-api
----
+
where:
+
`<cluster_id>`:: Specifies the ID of the upgraded cluster.

. In the provider spec, set the value of the property `rootVolume.availabilityZone` to the volume of the availability zone you want to use.
+
.An example {rh-openstack} provider spec
[source,yaml]
----
providerSpec:
  value:
    apiVersion: machine.openshift.io/v1alpha1
    availabilityZone: az0
      cloudName: openstack
    cloudsSecret:
      name: openstack-cloud-credentials
      namespace: openshift-machine-api
    flavor: m1.xlarge
    image: rhcos-4.14
    kind: OpenstackProviderSpec
    metadata:
      creationTimestamp: null
    networks:
    - filter: {}
      subnets:
      - filter:
          name: refarch-lv7q9-nodes
          tags: openshiftClusterID=refarch-lv7q9
    rootVolume:
        availabilityZone: nova <1>
        diskSize: 30
        sourceUUID: rhcos-4.12
        volumeType: fast-0
    securityGroups:
    - filter: {}
      name: refarch-lv7q9-master
    serverGroupName: refarch-lv7q9-master
    serverMetadata:
      Name: refarch-lv7q9-master
      openshiftClusterID: refarch-lv7q9
    tags:
    - openshiftClusterID=refarch-lv7q9
    trunk: true
    userDataSecret:
      name: master-user-data
----
<1> Set the zone name as this value.
+
[NOTE]
====
If you edited or recreated machine resources after your initial cluster deployment, you might have to adapt these steps for your configuration.

In your {rh-openstack} cluster, find the availability zone of the root volumes for your machines and use that as the value.
====

. Run the following command to retrieve information about the control plane machine set resource:
+
[source,terminal]
----
$ oc describe controlplanemachineset.machine.openshift.io/cluster --namespace openshift-machine-api
----

. Run the following command to edit the resource:
+
[source,terminal]
----
$ oc edit controlplanemachineset.machine.openshift.io/cluster --namespace openshift-machine-api
----

. For that resource, set the value of the `spec.state` property to `Active` to activate control plane machine sets for your cluster.

Your control plane is ready to be managed by the Cluster Control Plane Machine Set Operator.

:leveloffset!:

// Post-upgrade config for ShiftStack with control plane AZs explicitly defined
:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/control_plane_machine_management/cpmso-troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="cpmso-openstack-with-az-config_{context}"]
= Configuring {rh-openstack} clusters that have control plane machines with availability zones after an upgrade

For some clusters that run on {rh-openstack-first} that you upgrade, you must manually update machine resources before you can use control plane machine sets if the following configurations are true:

* You upgraded the cluster from {product-title} 4.13 to 4.14.

* The cluster infrastructure is installer-provisioned.

* Control plane machines were distributed across multiple compute availability zones.

To understand why this procedure is necessary, see link:https://access.redhat.com/solutions/7013893[Solution #7013893].

.Procedure

. For the `master-1` and `master-2` control plane machines, open the provider specs for editing. For example, to edit the first machine, enter the following command:
+
[source,terminal]
----
$ oc edit machine/<cluster_id>-master-1 -n openshift-machine-api
----
+
where:
+
`<cluster_id>`:: Specifies the ID of the upgraded cluster.

. For the `master-1` and `master-2` control plane machines, edit the value of the `serverGroupName` property in their provider specs to match that of the machine `master-0`.
+
.An example {rh-openstack} provider spec
[source,yaml]
----
providerSpec:
  value:
    apiVersion: machine.openshift.io/v1alpha1
    availabilityZone: az0
      cloudName: openstack
    cloudsSecret:
      name: openstack-cloud-credentials
      namespace: openshift-machine-api
    flavor: m1.xlarge
    image: rhcos-4.14
    kind: OpenstackProviderSpec
    metadata:
      creationTimestamp: null
    networks:
    - filter: {}
      subnets:
      - filter:
          name: refarch-lv7q9-nodes
          tags: openshiftClusterID=refarch-lv7q9
    securityGroups:
    - filter: {}
      name: refarch-lv7q9-master
    serverGroupName: refarch-lv7q9-master-az0 <1>
    serverMetadata:
      Name: refarch-lv7q9-master
      openshiftClusterID: refarch-lv7q9
    tags:
    - openshiftClusterID=refarch-lv7q9
    trunk: true
    userDataSecret:
      name: master-user-data
----
<1> This value must match for machines `master-0`, `master-1`, and `master-3`.
+
[NOTE]
====
If you edited or recreated machine resources after your initial cluster deployment, you might have to adapt these steps for your configuration.

In your {rh-openstack} cluster, find the server group that your control plane instances are in and use that as the value.
====

. Run the following command to retrieve information about the control plane machine set resource:
+
[source,terminal]
----
$ oc describe controlplanemachineset.machine.openshift.io/cluster --namespace openshift-machine-api
----

. Run the following command to edit the resource:
+
[source,terminal]
----
$ oc edit controlplanemachineset.machine.openshift.io/cluster --namespace openshift-machine-api
----

. For that resource, set the value of the `spec.state` property to `Active` to activate control plane machine sets for your cluster.

Your control plane is ready to be managed by the Cluster Control Plane Machine Set Operator.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/cpmso-checking-status,modules/cpmso-ts-ilb-missing,modules/cpmso-ts-mhc-etcd-degraded,modules/cpmso-openstack-ts-root-volume-azs,modules/cpmso-openstack-with-az-config

:_mod-docs-content-type: ASSEMBLY
[id="persistent-storage-using-nfs"]
= Persistent storage using NFS
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: persistent-storage-nfs

toc::[]

{product-title} clusters can be provisioned with persistent storage
using NFS. Persistent volumes (PVs) and persistent volume claims (PVCs)
provide a convenient method for sharing a volume across a project. While the
NFS-specific information contained in a PV definition could also be defined
directly in a `Pod` definition, doing so does not create the volume as a
distinct cluster resource, making the volume more susceptible to conflicts.

[role="_additional-resources"]
.Additional resources

* https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/7/html/storage_administration_guide/ch-nfs[Network File System (NFS)]

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

:_mod-docs-content-type: PROCEDURE
[id="persistent-storage-nfs-provisioning_{context}"]
= Provisioning

Storage must exist in the underlying infrastructure before it can be
mounted as a volume in {product-title}. To provision NFS volumes,
a list of NFS servers and export paths are all that is required.

.Procedure

. Create an object definition for the PV:
+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0001 <1>
spec:
  capacity:
    storage: 5Gi <2>
  accessModes:
  - ReadWriteOnce <3>
  nfs: <4>
    path: /tmp <5>
    server: 172.17.0.2 <6>
  persistentVolumeReclaimPolicy: Retain <7>
----
<1> The name of the volume. This is the PV identity in various `oc <command>
pod` commands.
<2> The amount of storage allocated to this volume.
<3> Though this appears to be related to controlling access to the volume,
it is actually used similarly to labels and used to match a PVC to a PV.
Currently, no access rules are enforced based on the `accessModes`.
<4> The volume type being used, in this case the `nfs` plugin.
<5> The path that is exported by the NFS server.
<6> The hostname or IP address of the NFS server.
<7> The reclaim policy for the PV. This defines what happens to a volume
when released.
+
[NOTE]
====
Each NFS volume must be mountable by all schedulable nodes in the cluster.
====

. Verify that the PV was created:
+
[source,terminal]
----
$ oc get pv
----
+
.Example output
[source,terminal]
----
NAME     LABELS    CAPACITY     ACCESSMODES   STATUS      CLAIM  REASON    AGE
pv0001   <none>    5Gi          RWO           Available                    31s
----

. Create a persistent volume claim that binds to the new PV:
+
[source,yaml]
----
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: nfs-claim1
spec:
  accessModes:
    - ReadWriteOnce <1>
  resources:
    requests:
      storage: 5Gi <2>
  volumeName: pv0001
  storageClassName: ""
----
<1> The access modes do not enforce security, but rather act as labels to match a PV to a PVC.
<2> This claim looks for PVs offering *5Gi* or greater capacity.

. Verify that the persistent volume claim was created:
+
[source,terminal]
----
$ oc get pvc
----
+
.Example output
[source,terminal]
----
NAME         STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
nfs-claim1   Bound    pv0001   5Gi        RWO                           2m
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

[id="nfs-enforcing-disk-quota_{context}"]
= Enforcing disk quotas

You can use disk partitions to enforce disk quotas and size constraints.
Each partition can be its own export. Each export is one PV.
{product-title} enforces unique names for PVs, but the uniqueness of the
NFS volume's server and path is up to the administrator.

Enforcing quotas in this way allows the developer to request persistent
storage by a specific amount, such as 10Gi, and be matched with a
corresponding volume of equal or greater capacity.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

[id="nfs-volume-security_{context}"]
= NFS volume security

This section covers NFS volume security, including matching permissions and
SELinux considerations. The user is expected to understand the basics of
POSIX permissions, process UIDs, supplemental groups, and SELinux.

Developers request NFS storage by referencing either a PVC by name or the
NFS volume plugin directly in the `volumes` section of their `Pod`
definition.

The `/etc/exports` file on the NFS server contains the accessible NFS
directories. The target NFS directory has POSIX owner and group IDs. The
{product-title} NFS plugin mounts the container's NFS directory with the
same POSIX ownership and permissions found on the exported NFS directory.
However, the container is not run with its effective UID equal to the
owner of the NFS mount, which is the desired behavior.

As an example, if the target NFS directory appears on the NFS server as:

[[nfs-export]]
[source,terminal]
----
$ ls -lZ /opt/nfs -d
----

.Example output
[source,terminal]
----
drwxrws---. nfsnobody 5555 unconfined_u:object_r:usr_t:s0   /opt/nfs
----
[source,terminal]
----
$ id nfsnobody
----
.Example output
[source,terminal]
----
uid=65534(nfsnobody) gid=65534(nfsnobody) groups=65534(nfsnobody)
----

Then the container must match SELinux labels, and either run with a UID of
`65534`, the `nfsnobody` owner, or with `5555` in its supplemental groups to access the directory.

[NOTE]
====
The owner ID of `65534` is used as an example. Even though NFS's
`root_squash` maps `root`, uid `0`, to `nfsnobody`, uid `65534`, NFS
exports can have arbitrary owner IDs. Owner `65534` is not required
for NFS exports.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc
// * storage/registry/configuring_registry_storage/configuring-registry-storage-baremetal.adoc

[id=storage-persistent-storage-nfs-group-ids_{context}]
= Group IDs

The recommended way to handle NFS access, assuming it is not an option to
change permissions on the NFS export, is to use supplemental groups.
Supplemental groups in {product-title} are used for shared storage, of
which NFS is an example. In contrast, block storage such as
iSCSI uses the `fsGroup` SCC strategy and the `fsGroup` value in the `securityContext` of the pod.

[NOTE]
====
To gain access to persistent storage, it is generally preferable to use supplemental group IDs versus user IDs.
====

Because the group ID on the example target NFS directory
is `5555`, the pod can define that group ID using `supplementalGroups`
under the `securityContext` definition of the pod. For example:

[source,yaml]
----
spec:
  containers:
    - name:
    ...
  securityContext: <1>
    supplementalGroups: [5555] <2>
----
<1> `securityContext` must be defined at the pod level, not under a
specific container.
<2> An array of GIDs defined for the pod. In this case, there is
one element in the array. Additional GIDs would be comma-separated.

Assuming there are no custom SCCs that might satisfy the pod
requirements, the pod likely matches the `restricted` SCC. This SCC has
the `supplementalGroups` strategy set to `RunAsAny`, meaning that any
supplied group ID is accepted without range checking.

As a result, the above pod passes admissions and is launched. However,
if group ID range checking is desired, a custom SCC is the preferred
solution. A custom SCC can be created such that minimum
and maximum group IDs are defined, group ID range checking is enforced,
and a group ID of `5555` is allowed.

[NOTE]
====
To use a custom SCC, you must first add it to the appropriate service
account. For example, use the `default` service account in the given project
unless another has been specified on the `Pod` specification.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.aodc

[id="nfs-user-id_{context}"]
= User IDs

User IDs can be defined in the container image or in the `Pod` definition.

[NOTE]
====
It is generally preferable to use supplemental group IDs to gain access to
persistent storage versus using user IDs.
====

In the example target NFS directory shown above, the container
needs its UID set to `65534`, ignoring group IDs for the moment, so the
following can be added to the `Pod` definition:

[source,yaml]
----
spec:
  containers: <1>
  - name:
  ...
    securityContext:
      runAsUser: 65534 <2>
----
<1> Pods contain a `securityContext` definition specific to each container and
a pod's `securityContext` which applies to all containers defined in
the pod.
<2> `65534` is the `nfsnobody` user.

Assuming that the project is `default` and the SCC is `restricted`, the user ID of `65534` as requested by the pod is not allowed. Therefore, the pod fails for the following reasons:

* It requests `65534` as its user ID.
* All SCCs available to the pod are examined to see which SCC allows a
user ID of `65534`. While all policies of the SCCs are checked, the focus
here is on user ID.
* Because all available SCCs use `MustRunAsRange` for their `runAsUser`
strategy, UID range checking is required.
* `65534` is not included in the SCC or project's user ID range.

It is generally considered a good practice not to modify the predefined
SCCs. The preferred way to fix this situation is to create a custom SCC
A custom SCC can be created such that minimum and maximum user IDs
are defined, UID range checking is still enforced, and the UID of `65534`
 is allowed.

[NOTE]
====
To use a custom SCC, you must first add it to the appropriate service
account. For example, use the `default` service account in the given project
unless another has been specified on the `Pod` specification.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

:_mod-docs-content-type: PROCEDURE
[id="nfs-selinux_{context}"]
= SELinux

{op-system-base-full} and {op-system-first} systems are configured to use SELinux on remote NFS servers by default.

For non-{op-system-base} and non-{op-system} systems, SELinux does not allow writing from a pod to a remote NFS server. The NFS volume mounts correctly but it is read-only. You will need to enable the correct SELinux permissions by using the following procedure.

.Prerequisites

* The `container-selinux` package must be installed. This package provides the `virt_use_nfs` SELinux boolean.

.Procedure

* Enable the `virt_use_nfs` boolean using the following command. The `-P` option makes this boolean persistent across reboots.
+
[source,terminal]
----
# setsebool -P virt_use_nfs 1
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

= Export settings

To enable arbitrary container users to read and write the volume,
each exported volume on the NFS server should conform to the following
conditions:

* Every export must be exported using the following format:
+
[source,terminal]
----
/<example_fs> *(rw,root_squash)
----

* The firewall must be configured to allow traffic to the mount point.
** For NFSv4, configure the default port `2049` (*nfs*).
+
.NFSv4
[source,terminal]
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
----

** For NFSv3, there are three ports to configure:
`2049` (*nfs*), `20048` (*mountd*), and `111` (*portmapper*).
+
.NFSv3
[source,terminal]
----
# iptables -I INPUT 1 -p tcp --dport 2049 -j ACCEPT
----
+
[source,terminal]
----
# iptables -I INPUT 1 -p tcp --dport 20048 -j ACCEPT
----
+
[source,terminal]
----
# iptables -I INPUT 1 -p tcp --dport 111 -j ACCEPT
----

* The NFS export and directory must be set up so that they are accessible
by the target pods. Either set the export to be owned by the container's
primary UID, or supply the pod group access using `supplementalGroups`,
as shown in the group IDs above.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

[id="nfs-reclaiming-resources_{context}"]
= Reclaiming resources
NFS implements the {product-title} `Recyclable` plugin interface. Automatic
processes handle reclamation tasks based on policies set on each persistent
volume.

By default, PVs are set to `Retain`.

Once claim to a PVC is deleted, and the PV is released, the PV object
should not be reused. Instead, a new PV should be created with the same
basic volume details as the original.

For example, the administrator creates a PV named `nfs1`:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs1
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 192.168.1.1
    path: "/"
----

The user creates `PVC1`, which binds to `nfs1`. The user then deletes
`PVC1`, releasing claim to `nfs1`. This results in `nfs1` being `Released`.
If the administrator wants to make the same NFS share available,
they should create a new PV with the same NFS server details, but a
different PV name:

[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: nfs2
spec:
  capacity:
    storage: 1Mi
  accessModes:
    - ReadWriteMany
  nfs:
    server: 192.168.1.1
    path: "/"
----

Deleting the original PV and re-creating it with the same name is
discouraged. Attempting to manually change the status of a PV
from `Released` to `Available` causes errors and potential data loss.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-nfs.adoc

= Additional configuration and troubleshooting

Depending on what version of NFS is being used and how it is configured,
there may be additional configuration steps needed for proper export and
security mapping. The following are some that may apply:

[cols="1,2"]
|===

|NFSv4 mount incorrectly shows all files with ownership of `nobody:nobody`
a|- Could be attributed to the ID mapping settings, found in `/etc/idmapd.conf` on your NFS.
- See https://access.redhat.com/solutions/33455[this Red Hat Solution].

|Disabling ID mapping on NFSv4
a|- On both the NFS client and server, run:
+
[source,terminal]
----
# echo 'Y' > /sys/module/nfsd/parameters/nfs4_disable_idmapping
----
|===

:leveloffset!:

//# includes=_attributes/common-attributes,modules/storage-persistent-storage-nfs-provisioning,modules/storage-persistent-storage-nfs-enforcing-disk-quota,modules/storage-persistent-storage-nfs-volume-security,modules/storage-persistent-storage-nfs-group-ids,modules/storage-persistent-storage-nfs-user-ids,modules/storage-persistent-storage-nfs-selinux,modules/storage-persistent-storage-nfs-export-settings,modules/storage-persistent-storage-nfs-reclaiming-resources,modules/storage-persistent-storage-nfs-additional-configuration

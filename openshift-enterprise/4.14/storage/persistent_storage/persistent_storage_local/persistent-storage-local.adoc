:_mod-docs-content-type: ASSEMBLY
[id="persistent-storage-using-local-volume"]
= Persistent storage using local volumes
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: persistent-storage-local

toc::[]

{product-title} can be provisioned with persistent storage by using
local volumes. Local persistent volumes allow you to access local storage
devices, such as a disk or partition, by using the standard
persistent volume claim interface.

Local volumes can be used without manually scheduling pods to nodes
because the system is aware of the volume node constraints. However,
local volumes are still subject to the availability of the underlying node
and are not suitable for all applications.

[NOTE]
====
Local volumes can only be used as a statically created persistent volume.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-storage-install_{context}"]
= Installing the Local Storage Operator

The Local Storage Operator is not installed in {product-title} by default. Use the following procedure to install and configure this Operator to enable local volumes in your cluster.

.Prerequisites

* Access to the {product-title} web console or command-line interface (CLI).

.Procedure

. Create the `openshift-local-storage` project:
+
[source,terminal]
----
$ oc adm new-project openshift-local-storage
----

. Optional: Allow local storage creation on infrastructure nodes.
+
You might want to use the Local Storage Operator to create volumes on infrastructure nodes in support of components such as logging and monitoring.
+
You must adjust the default node selector so that the Local Storage Operator includes the infrastructure nodes, and not just worker nodes.
+
To block the Local Storage Operator from inheriting the cluster-wide default selector, enter the following command:
+
[source,terminal]
----
$ oc annotate namespace openshift-local-storage openshift.io/node-selector=''
----

. Optional: Allow local storage to run on the management pool of CPUs in single-node deployment.
+
Use the Local Storage Operator in single-node deployments and allow the use of CPUs that belong to the `management` pool. Perform this step on single-node installations that use management workload partitioning.
+
To allow Local Storage Operator to run on the management CPU pool, run following commands:
+
[source,terminal]
----
$ oc annotate namespace openshift-local-storage workload.openshift.io/allowed='management'
----

.From the UI

To install the Local Storage Operator from the web console, follow these steps:

. Log in to the {product-title} web console.

. Navigate to *Operators* -> *OperatorHub*.

. Type *Local Storage* into the filter box to locate the Local Storage Operator.

. Click *Install*.

. On the *Install Operator* page, select *A specific namespace on the cluster*. Select *openshift-local-storage* from the drop-down menu.

. Adjust the values for *Update Channel* and *Approval Strategy* to the values that you want.

. Click *Install*.

Once finished, the Local Storage Operator will be listed in the *Installed Operators* section of the web console.

.From the CLI
. Install the Local Storage Operator from the CLI.

.. Create an object YAML file to define an Operator group and subscription for the Local Storage Operator,
such as `openshift-local-storage.yaml`:
+
.Example openshift-local-storage.yaml
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: local-operator-group
  namespace: openshift-local-storage
spec:
  targetNamespaces:
    - openshift-local-storage
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: local-storage-operator
  namespace: openshift-local-storage
spec:
  channel: stable
  installPlanApproval: Automatic <1>
  name: local-storage-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
----
<1> The user approval policy for an install plan.

. Create the Local Storage Operator object by entering the following command:
+
[source,terminal]
----
$ oc apply -f openshift-local-storage.yaml
----
+
At this point, the Operator Lifecycle Manager (OLM) is now aware of the Local Storage Operator. A ClusterServiceVersion (CSV) for the Operator should appear in the target namespace, and APIs provided by the Operator should be available for creation.
+
. Verify local storage installation by checking that all pods and the Local Storage Operator have been created:

.. Check that all the required pods have been created:
+
[source,terminal]
----
$ oc -n openshift-local-storage get pods
----
+
.Example output
[source,terminal]
----
NAME                                      READY   STATUS    RESTARTS   AGE
local-storage-operator-746bf599c9-vlt5t   1/1     Running   0          19m
----

.. Check the ClusterServiceVersion (CSV) YAML manifest to see that the Local Storage Operator is available in the `openshift-local-storage` project:
+
[source,terminal]
----
$ oc get csvs -n openshift-local-storage
----
+
.Example output
[source,terminal]
----
NAME                                         DISPLAY         VERSION               REPLACES   PHASE
local-storage-operator.4.2.26-202003230335   Local Storage   4.2.26-202003230335              Succeeded
----

After all checks have passed, the Local Storage Operator is installed successfully.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-volume-cr_{context}"]
= Provisioning local volumes by using the Local Storage Operator

Local volumes cannot be created by dynamic provisioning. Instead, persistent volumes can be created by the Local Storage Operator. The local volume provisioner looks for any file system or block volume devices at the paths specified in the defined resource.

.Prerequisites

* The Local Storage Operator is installed.
* You have a local disk that meets the following conditions:
** It is attached to a node.
** It is not mounted.
** It does not contain partitions.

.Procedure

. Create the local volume resource. This resource must define the nodes and paths to the local volumes.
+
[NOTE]
====
Do not use different storage class names for the same device. Doing so will create multiple persistent volumes (PVs).
====
+
--
.Example: Filesystem
[source,yaml]
----
apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage" <1>
spec:
  nodeSelector: <2>
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-140-183
          - ip-10-0-158-139
          - ip-10-0-164-33
  storageClassDevices:
    - storageClassName: "local-sc" <3>
      volumeMode: Filesystem <4>
      fsType: xfs <5>
      devicePaths: <6>
        - /path/to/device <7>
----
<1> The namespace where the Local Storage Operator is installed.
<2> Optional: A node selector containing a list of nodes where the local storage volumes are attached. This example uses the node hostnames, obtained from `oc get node`. If a value is not defined, then the Local Storage Operator will attempt to find matching disks on all available nodes.
<3> The name of the storage class to use when creating persistent volume objects. The Local Storage Operator automatically creates the storage class if it does not exist. Be sure to use a storage class that uniquely identifies this set of local volumes.
<4> The volume mode, either `Filesystem` or `Block`, that defines the type of local volumes.
+
[NOTE]
====
A raw block volume (`volumeMode: Block`) is not formatted with a file system. Use this mode only if any application running on the pod can use raw block devices.
====
<5> The file system that is created when the local volume is mounted for the first time.
<6> The path containing a list of local storage devices to choose from.
<7> Replace this value with your actual local disks filepath to the `LocalVolume` resource `by-id`, such as `/dev/disk/by-id/wwn`. PVs are created for these local disks when the provisioner is deployed successfully.
+
[NOTE]
====
If you are running {product-title} with {op-system-base} KVM, you must assign a serial number to your VM disk. Otherwise, the VM disk can not be identified after reboot. You can use the `virsh edit <VM>` command to add the `<serial>mydisk</serial>` definition.
====
--
+
.Example: Block
[source,yaml]
----
apiVersion: "local.storage.openshift.io/v1"
kind: "LocalVolume"
metadata:
  name: "local-disks"
  namespace: "openshift-local-storage" <1>
spec:
  nodeSelector: <2>
    nodeSelectorTerms:
    - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - ip-10-0-136-143
          - ip-10-0-140-255
          - ip-10-0-144-180
  storageClassDevices:
    - storageClassName: "localblock-sc" <3>
      volumeMode: Block <4>
      devicePaths: <5>
        - /path/to/device <6>
----
<1> The namespace where the Local Storage Operator is installed.
<2> Optional: A node selector containing a list of nodes where the local storage volumes are attached. This example uses the node hostnames, obtained from `oc get node`. If a value is not defined, then the Local Storage Operator will attempt to find matching disks on all available nodes.
<3> The name of the storage class to use when creating persistent volume objects.
<4> The volume mode, either `Filesystem` or `Block`, that defines the type of local volumes.
<5> The path containing a list of local storage devices to choose from.
<6> Replace this value with your actual local disks filepath to the `LocalVolume` resource `by-id`, such as `dev/disk/by-id/wwn`. PVs are created for these local disks when the provisioner is deployed successfully.
+
[NOTE]
====
If you are running {product-title} with {op-system-base} KVM, you must assign a serial number to your VM disk. Otherwise, the VM disk can not be identified after reboot. You can use the `virsh edit <VM>` command to add the `<serial>mydisk</serial>` definition.
====

. Create the local volume resource in your {product-title} cluster. Specify the file you just created:
+
[source,terminal]
----
$ oc create -f <local-volume>.yaml
----

. Verify that the provisioner was created and that the corresponding daemon sets were created:
+
[source,terminal]
----
$ oc get all -n openshift-local-storage
----
+
.Example output
[source,terminal]
----
NAME                                          READY   STATUS    RESTARTS   AGE
pod/diskmaker-manager-9wzms                   1/1     Running   0          5m43s
pod/diskmaker-manager-jgvjp                   1/1     Running   0          5m43s
pod/diskmaker-manager-tbdsj                   1/1     Running   0          5m43s
pod/local-storage-operator-7db4bd9f79-t6k87   1/1     Running   0          14m

NAME                                     TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)             AGE
service/local-storage-operator-metrics   ClusterIP   172.30.135.36   <none>        8383/TCP,8686/TCP   14m

NAME                               DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
daemonset.apps/diskmaker-manager   3         3         3       3            3           <none>          5m43s

NAME                                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/local-storage-operator   1/1     1            1           14m

NAME                                                DESIRED   CURRENT   READY   AGE
replicaset.apps/local-storage-operator-7db4bd9f79   1         1         1       14m
----
+
Note the desired and current number of daemon set processes. A desired count of `0` indicates that the label selectors were invalid.

. Verify that the persistent volumes were created:
+
[source,terminal]
----
$ oc get pv
----
+
.Example output
[source,terminal]
----
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
local-pv-1cec77cf   100Gi      RWO            Delete           Available           local-sc                88m
local-pv-2ef7cd2a   100Gi      RWO            Delete           Available           local-sc                82m
local-pv-3fa1c73    100Gi      RWO            Delete           Available           local-sc                48m
----

[IMPORTANT]
====
Editing the `LocalVolume` object does not change the `fsType` or `volumeMode` of existing persistent volumes because doing so might result in a destructive operation.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-create-cr-manual_{context}"]
= Provisioning local volumes without the Local Storage Operator

Local volumes cannot be created by dynamic provisioning. Instead, persistent volumes can be created by defining the persistent volume (PV) in an object definition. The local volume provisioner looks for any file system or block volume devices at the paths specified in the defined resource.

[IMPORTANT]
====
Manual provisioning of PVs includes the risk of potential data leaks across PV reuse when PVCs are deleted.
The Local Storage Operator is recommended for automating the life cycle of devices when provisioning local PVs.
====

.Prerequisites

* Local disks are attached to the {product-title} nodes.

.Procedure

. Define the PV. Create a file, such as `example-pv-filesystem.yaml` or `example-pv-block.yaml`, with the `PersistentVolume` object definition. This resource must define the nodes and paths to the local volumes.
+
[NOTE]
====
Do not use different storage class names for the same device. Doing so will create multiple PVs.
====
+
.example-pv-filesystem.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv-filesystem
spec:
  capacity:
    storage: 100Gi
  volumeMode: Filesystem <1>
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage <2>
  local:
    path: /dev/xvdf <3>
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
----
<1> The volume mode, either `Filesystem` or `Block`, that defines the type of PVs.
<2> The name of the storage class to use when creating PV resources. Use a storage class that uniquely identifies this set of PVs.
<3> The path containing a list of local storage devices to choose from, or a directory. You can only specify a directory with `Filesystem` `volumeMode`.
+
[NOTE]
====
A raw block volume (`volumeMode: block`) is not formatted with a file system. Use this mode only if any application running on the pod can use raw block devices.
====
+
.example-pv-block.yaml
[source,yaml]
----
apiVersion: v1
kind: PersistentVolume
metadata:
  name: example-pv-block
spec:
  capacity:
    storage: 100Gi
  volumeMode: Block <1>
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
  storageClassName: local-storage <2>
  local:
    path: /dev/xvdf <3>
  nodeAffinity:
    required:
      nodeSelectorTerms:
      - matchExpressions:
        - key: kubernetes.io/hostname
          operator: In
          values:
          - example-node
----
<1> The volume mode, either `Filesystem` or `Block`, that defines the type of PVs.
<2> The name of the storage class to use when creating PV resources. Be sure to use a storage class that uniquely identifies this set of PVs.
<3> The path containing a list of local storage devices to choose from.

. Create the PV resource in your {product-title} cluster. Specify the file you just created:
+
[source,terminal]
----
$ oc create -f <example-pv>.yaml
----

. Verify that the local PV was created:
+
[source,terminal]
----
$ oc get pv
----
+
.Example output
[source,terminal]
----
NAME                    CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM                STORAGECLASS    REASON   AGE
example-pv-filesystem   100Gi      RWO            Delete           Available                        local-storage            3m47s
example-pv1             1Gi        RWO            Delete           Bound       local-storage/pvc1   local-storage            12h
example-pv2             1Gi        RWO            Delete           Bound       local-storage/pvc2   local-storage            12h
example-pv3             1Gi        RWO            Delete           Bound       local-storage/pvc3   local-storage            12h
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="create-local-pvc_{context}"]
= Creating the local volume persistent volume claim

Local volumes must be statically created as a persistent volume claim (PVC)
to be accessed by the pod.

.Prerequisites

* Persistent volumes have been created using the local volume provisioner.

.Procedure

. Create the PVC using the corresponding storage class:
+
[source,yaml]
----
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: local-pvc-name <1>
spec:
  accessModes:
  - ReadWriteOnce
  volumeMode: Filesystem <2>
  resources:
    requests:
      storage: 100Gi <3>
  storageClassName: local-sc <4>
----
<1> Name of the PVC.
<2> The type of the PVC. Defaults to `Filesystem`.
<3> The amount of storage available to the PVC.
<4> Name of the storage class required by the claim.

. Create the PVC in the {product-title} cluster, specifying the file
you just created:
+
[source,terminal]
----
$ oc create -f <local-pvc>.yaml
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-pod_{context}"]
= Attach the local claim

After a local volume has been mapped to a persistent volume claim
it can be specified inside of a resource.

.Prerequisites

* A persistent volume claim exists in the same namespace.

.Procedure

. Include the defined claim in the resource spec. The following example
declares the persistent volume claim inside a pod:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
spec:
  ...
  containers:
    volumeMounts:
    - name: local-disks <1>
      mountPath: /data <2>
  volumes:
  - name: localpvc
    persistentVolumeClaim:
      claimName: local-pvc-name <3>
----
<1> The name of the volume to mount.
<2> The path inside the pod where the volume is mounted. Do not mount to the container root, `/`, or any path that is the same in the host and the container. This can corrupt your host system if the container is sufficiently privileged, such as the host `/dev/pts` files. It is safe to mount the host by using `/host`.
<3> The name of the existing persistent volume claim to use.

. Create the resource in the {product-title} cluster, specifying the file
you just created:
+
[source,terminal]
----
$ oc create -f <local-pod>.yaml
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-storage-discovery_{context}"]
= Automating discovery and provisioning for local storage devices

The Local Storage Operator automates local storage discovery and provisioning. With this feature, you can simplify installation when dynamic provisioning is not available during deployment, such as with bare metal, VMware, or AWS store instances with attached devices.

:FeatureName: Automatic discovery and provisioning
:leveloffset: +1

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset: 1

Use the following procedure to automatically discover local devices, and to automatically provision local volumes for selected devices.

[WARNING]
====
Use the `LocalVolumeSet` object with caution. When you automatically provision persistent volumes (PVs) from local disks, the local PVs might claim all devices that match. If you are using a `LocalVolumeSet` object, make sure the Local Storage Operator is the only entity managing local devices on the node. Creating multiple instances of a `LocalVolumeSet` that target a node more than once is not supported.
====

.Prerequisites
* You have cluster administrator permissions.

* You have installed the Local Storage Operator.

* You have attached local disks to {product-title} nodes.

* You have access to the {product-title} web console and the `oc` command-line interface (CLI).

.Procedure

. To enable automatic discovery of local devices from the web console:

.. In the _Administrator_ perspective, navigate to *Operators* -> *Installed Operators* and click on the *Local Volume Discovery* tab.

.. Click *Create Local Volume Discovery*.

.. Select either *All nodes* or *Select nodes*, depending on whether you want to discover available disks on all or specific nodes.
+
[NOTE]
====
Only worker nodes are available, regardless of whether you filter using *All nodes* or *Select nodes*.
====
+
.. Click *Create*.

A local volume discovery instance named `auto-discover-devices` is displayed.

. To display a continuous list of available devices on a node:

.. Log in to the {product-title} web console.

.. Navigate to *Compute* -> *Nodes*.

.. Click the node name that you want to open. The "Node Details" page is displayed.

.. Select the *Disks* tab to display the list of the selected devices.
+
The device list updates continuously as local disks are added or removed. You can filter the devices by name, status, type, model, capacity, and mode.

. To automatically provision local volumes for the discovered devices from the web console:

.. Navigate to *Operators* -> *Installed Operators* and select *Local Storage* from the list of Operators.

.. Select *Local Volume Set* -> *Create Local Volume Set*.

.. Enter a volume set name and a storage class name.

.. Choose *All nodes* or *Select nodes* to apply filters accordingly.
+
[NOTE]
====
Only worker nodes are available, regardless of whether you filter using *All nodes* or *Select nodes*.
====
+
.. Select the disk type, mode, size, and limit you want to apply to the local volume set, and click *Create*.
+
A message displays after several minutes, indicating that the "Operator reconciled successfully."

[start=3]
. Alternatively, to provision local volumes for the discovered devices from the CLI:

.. Create an object YAML file to define the local volume set, such as `local-volume-set.yaml`, as shown in the following example:
+
[source,yaml]
----
apiVersion: local.storage.openshift.io/v1alpha1
kind: LocalVolumeSet
metadata:
  name: example-autodetect
spec:
  nodeSelector:
    nodeSelectorTerms:
      - matchExpressions:
          - key: kubernetes.io/hostname
            operator: In
            values:
              - worker-0
              - worker-1
  storageClassName: example-storageclass <1>
  volumeMode: Filesystem
  fsType: ext4
  maxDeviceCount: 10
  deviceInclusionSpec:
    deviceTypes: <2>
      - disk
      - part
    deviceMechanicalProperties:
      - NonRotational
    minSize: 10G
    maxSize: 100G
    models:
      - SAMSUNG
      - Crucial_CT525MX3
    vendors:
      - ATA
      - ST2000LM
----
+
<1> Determines the storage class that is created for persistent volumes that are provisioned from discovered devices. The Local Storage Operator automatically creates the storage class if it does not exist. Be sure to use a storage class that uniquely identifies this set of local volumes.
+
<2> When using the local volume set feature, the Local Storage Operator does not support the use of logical volume management (LVM) devices.

.. Create the local volume set object:
+
[source,terminal]
----
$ oc apply -f local-volume-set.yaml
----

.. Verify that the local persistent volumes were dynamically provisioned based on the storage class:
+
[source,terminal]
----
$ oc get pv
----
+
.Example output
[source,terminal]
----
NAME                CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS           REASON   AGE
local-pv-1cec77cf   100Gi      RWO            Delete           Available           example-storageclass            88m
local-pv-2ef7cd2a   100Gi      RWO            Delete           Available           example-storageclass            82m
local-pv-3fa1c73    100Gi      RWO            Delete           Available           example-storageclass            48m
----

[NOTE]
====
Results are deleted after they are removed from the node. Symlinks must be manually removed.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-tolerations_{context}"]
= Using tolerations with Local Storage Operator pods

Taints can be applied to nodes to prevent them from running general workloads. To allow the Local Storage Operator to use tainted nodes, you must add tolerations to the `Pod` or `DaemonSet` definition. This allows the created resources to run on these tainted nodes.

You apply tolerations to the Local Storage Operator pod through the `LocalVolume` resource
and apply taints to a node through the node specification. A taint on a node instructs the node to repel all pods that do not tolerate the taint. Using a specific taint that is not on other pods ensures that the Local Storage Operator pod can also run on that node.

[IMPORTANT]
====
Taints and tolerations consist of a key, value, and effect. As an argument, it is expressed as `key=value:effect`. An operator allows you to leave one of these parameters empty.
====

.Prerequisites

* The Local Storage Operator is installed.

* Local disks are attached to {product-title} nodes with a taint.

* Tainted nodes are expected to provision local storage.

.Procedure
To configure local volumes for scheduling on tainted nodes:

. Modify the YAML file that defines the `Pod` and add the `LocalVolume` spec, as shown in the following example:
+
[source,yaml]
----
  apiVersion: "local.storage.openshift.io/v1"
  kind: "LocalVolume"
  metadata:
    name: "local-disks"
    namespace: "openshift-local-storage"
  spec:
    tolerations:
      - key: localstorage <1>
        operator: Equal <2>
        value: "localstorage" <3>
    storageClassDevices:
        - storageClassName: "localblock-sc"
          volumeMode: Block <4>
          devicePaths: <5>
            - /dev/xvdg
----
<1> Specify the key that you added to the node.
<2> Specify the `Equal` operator to require the `key`/`value` parameters to match. If operator is `Exists`, the system checks that the key exists and ignores the value. If operator is `Equal`, then the key and value must match.
<3> Specify the value `local` of the tainted node.
<4> The volume mode, either `Filesystem` or `Block`, defining the type of the local volumes.
<5> The path containing a list of local storage devices to choose from.

. Optional: To create local persistent volumes on only tainted nodes, modify the YAML file and add the `LocalVolume` spec, as shown in the following example:
+
[source,yaml]
----
spec:
  tolerations:
    - key: node-role.kubernetes.io/master
      operator: Exists
----

The defined tolerations will be passed to the resulting daemon sets, allowing the diskmaker and provisioner pods to be created for nodes that contain the specified taints.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

[id="local-storage-metrics_{context}"]
= Local Storage Operator Metrics

{product-title} provides the following metrics for the Local Storage Operator:

* `lso_discovery_disk_count`: total number of discovered devices on each node

* `lso_lvset_provisioned_PV_count`: total number of PVs created by `LocalVolumeSet` objects

* `lso_lvset_unmatched_disk_count`: total number of disks that Local Storage Operator did not select for provisioning because of mismatching criteria

* `lso_lvset_orphaned_symlink_count`: number of devices with PVs that no longer match `LocalVolumeSet` object criteria

* `lso_lv_orphaned_symlink_count`: number of devices with PVs that no longer match `LocalVolume` object criteria

* `lso_lv_provisioned_PV_count`: total number of provisioned PVs for `LocalVolume`

To use these metrics, be sure to:

* Enable support for monitoring when installing the Local Storage Operator.

* When upgrading to {product-title} 4.9 or later, enable metric support manually by adding the `operator-metering=true` label to the namespace.

:leveloffset!:

For more information about metrics, see xref:../../../monitoring/managing-metrics.adoc#managing-metric[Managing metrics].

== Deleting the Local Storage Operator resources

:leveloffset: +2

// Module included in the following assemblies:
//
// storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-removing-device_{context}"]
= Removing a local volume or local volume set

Occasionally, local volumes and local volume sets must be deleted. While removing the entry in the resource and deleting the persistent volume is typically enough, if you want to reuse the same device path or have it managed by a different storage class, then additional steps are needed.

[NOTE]
====
The following procedure outlines an example for removing a local volume. The same procedure can also be used to remove symlinks for a local volume set custom resource.
====

.Prerequisites

* The persistent volume must be in a `Released` or `Available` state.
+
[WARNING]
====
Deleting a persistent volume that is still in use can result in data loss or corruption.
====

.Procedure

. Edit the previously created local volume to remove any unwanted disks.

.. Edit the cluster resource:
+
[source,terminal]
----
$ oc edit localvolume <name> -n openshift-local-storage
----

.. Navigate to the lines under `devicePaths`, and delete any representing unwanted disks.

. Delete any persistent volumes created.
+
[source,terminal]
----
$ oc delete pv <pv-name>
----

. Delete any symlinks on the node.
+
[WARNING]
====
The following step involves accessing a node as the root user. Modifying the state of the node beyond the steps in this procedure could result in cluster instability.
====
+
.. Create a debug pod on the node:
+
[source,terminal]
----
$ oc debug node/<node-name>
----

.. Change your root directory to `/host`:
+
[source,terminal]
----
$ chroot /host
----

.. Navigate to the directory containing the local volume symlinks.
+
[source,terminal]
----
$ cd /mnt/openshift-local-storage/<sc-name> <1>
----
<1> The name of the storage class used to create the local volumes.

.. Delete the symlink belonging to the removed device.
+
[source,terminal]
----
$ rm <symlink>
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/persistent_storage/persistent-storage-local.adoc

:_mod-docs-content-type: PROCEDURE
[id="local-storage-uninstall_{context}"]
= Uninstalling the Local Storage Operator

To uninstall the Local Storage Operator, you must remove the Operator and all created resources in the `openshift-local-storage` project.

[WARNING]
====
Uninstalling the Local Storage Operator while local storage PVs are still in use is not recommended. While the PVs will remain after the Operator's removal,
there might be indeterminate behavior if the Operator is uninstalled and reinstalled without removing the PVs and local storage resources.
====

.Prerequisites

* Access to the {product-title} web console.

.Procedure

. Delete any local volume resources installed in the project, such as `localvolume`, `localvolumeset`, and `localvolumediscovery`:
+
[source,terminal]
----
$ oc delete localvolume --all --all-namespaces
$ oc delete localvolumeset --all --all-namespaces
$ oc delete localvolumediscovery --all --all-namespaces
----

. Uninstall the Local Storage Operator from the web console.

.. Log in to the {product-title} web console.

.. Navigate to *Operators* -> *Installed Operators*.

.. Type *Local Storage* into the filter box to locate the Local Storage Operator.

.. Click the Options menu {kebab} at the end of the Local Storage Operator.

.. Click *Uninstall Operator*.

.. Click *Remove* in the window that appears.

. The PVs created by the Local Storage Operator will remain in the cluster until deleted. After these volumes are no longer in use, delete them by running the following command:
+
[source,terminal]
----
$ oc delete pv <pv-name>
----

. Delete the `openshift-local-storage` project:
+
[source,terminal]
----
$ oc delete project openshift-local-storage
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/persistent-storage-local-install,modules/persistent-storage-local-create-cr,modules/persistent-storage-local-create-cr-manual,modules/persistent-storage-local-pvc,modules/persistent-storage-local-pod,modules/persistent-storage-local-discovery,modules/snippets/technology-preview,modules/persistent-storage-local-tolerations,modules/persistent-storage-local-metrics,modules/persistent-storage-local-removing-devices,modules/persistent-storage-local-uninstall-operator

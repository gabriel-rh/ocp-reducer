:_mod-docs-content-type: ASSEMBLY
[id="migrating-applications-with-mtc"]
= Migrating your applications
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: migrating-applications-with-mtc

toc::[]

You can migrate your applications by using the {mtc-full} ({mtc-short}) web console or the xref:../migration_toolkit_for_containers/advanced-migration-options-mtc.adoc#migrating-applications-cli_advanced-migration-options-mtc[command line].

Most cluster-scoped resources are not yet handled by {mtc-short}. If your applications require cluster-scoped resources, you might have to create them manually on the target cluster.

You can use stage migration and cutover migration to migrate an application between clusters:

* Stage migration copies data from the source cluster to the target cluster without stopping the application. You can run a stage migration multiple times to reduce the duration of the cutover migration.
* Cutover migration stops the transactions on the source cluster and moves the resources to the target cluster.

You can use state migration to migrate an application's state:

* State migration copies selected persistent volume claims (PVCs).
* You can use state migration to migrate a namespace within the same cluster.

During migration, the {mtc-full} ({mtc-short}) preserves the following namespace annotations:

* `openshift.io/sa.scc.mcs`
* `openshift.io/sa.scc.supplemental-groups`
* `openshift.io/sa.scc.uid-range`
+
These annotations preserve the UID range, ensuring that the containers retain their file system permissions on the target cluster. There is a risk that the migrated UIDs could duplicate UIDs within an existing or future namespace on the target cluster.

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migrating_from_ocp_3_to_4/advanced-migration-options-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc
// * migration_toolkit_for_containers/advanced-migration-options-mtc.adoc

[id="migration-prerequisites_{context}"]
= Migration prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.

.Direct image migration

* You must ensure that the secure {product-registry} of the source cluster is exposed.
* You must create a route to the exposed registry.

.Direct volume migration

* If your clusters use proxies, you must configure an Stunnel TCP proxy.


.Clusters

* The source cluster must be upgraded to the latest {mtc-short} z-stream release.
* The {mtc-short} version must be the same on all clusters.

.Network

* The clusters have unrestricted network access to each other and to the replication repository.
* If you copy the persistent volumes with `move`, the clusters must have unrestricted network access to the remote volumes.
* You must enable the following ports on an {product-title} 4 cluster:
** `6443` (API server)
** `443` (routes)
** `53` (DNS)
* You must enable port `443` on the replication repository if you are using TLS.

.Persistent volumes (PVs)

* The PVs must be valid.
* The PVs must be bound to persistent volume claims.
* If you use snapshots to copy the PVs, the following additional prerequisites apply:
** The cloud provider must support snapshots.
** The PVs must have the same cloud provider.
** The PVs must be located in the same geographic region.
** The PVs must have the same storage class.

:leveloffset!:

[id="migrating-applications-mtc-web-console_{context}"]
== Migrating your applications by using the {mtc-short} web console

You can configure clusters and a replication repository by using the {mtc-short} web console. Then, you can create and run a migration plan.

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc

:_mod-docs-content-type: PROCEDURE
[id="migration-launching-cam_{context}"]
= Launching the {mtc-short} web console

You can launch the {mtc-full} ({mtc-short}) web console in a browser.

.Prerequisites

* The {mtc-short} web console must have network access to the {product-title} web console.
* The {mtc-short} web console must have network access to the OAuth authorization server.

.Procedure

. Log in to the {product-title} cluster on which you have installed {mtc-short}.
. Obtain the {mtc-short} web console URL by entering the following command:
+
[source,terminal]
----
$ oc get -n openshift-migration route/migration -o go-template='https://{{ .spec.host }}'
----
+
The output resembles the following: `\https://migration-openshift-migration.apps.cluster.openshift.com`.

. Launch a browser and navigate to the {mtc-short} web console.
+
[NOTE]
====
If you try to access the {mtc-short} web console immediately after installing the {mtc-full} Operator, the console might not load because the Operator is still configuring the cluster. Wait a few minutes and retry.
====

. If you are using self-signed CA certificates, you will be prompted to accept the CA certificate of the source cluster API server. The web page guides you through the process of accepting the remaining certificates.

. Log in with your {product-title} *username* and *password*.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc

:_mod-docs-content-type: PROCEDURE
[id="migration-adding-cluster-to-cam_{context}"]
= Adding a cluster to the {mtc-short} web console

You can add a cluster to the {mtc-full} ({mtc-short}) web console.

.Prerequisites

* If you are using Azure snapshots to copy data:
** You must specify the Azure resource group name for the cluster.
** The clusters must be in the same Azure resource group.
** The clusters must be in the same geographic location.
* If you are using direct image migration, you must expose a route to the image registry of the source cluster.

.Procedure

. Log in to the cluster.
. Obtain the `migration-controller` service account token:
+
[source,terminal]
----
$ oc sa get-token migration-controller -n openshift-migration
----
+
.Example output
+
[source,terminal]
----
eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJtaWciLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlY3JldC5uYW1lIjoibWlnLXRva2VuLWs4dDJyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6Im1pZyIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImE1YjFiYWMwLWMxYmYtMTFlOS05Y2NiLTAyOWRmODYwYjMwOCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDptaWc6bWlnIn0.xqeeAINK7UXpdRqAtOj70qhBJPeMwmgLomV9iFxr5RoqUgKchZRG2J2rkqmPm6vr7K-cm7ibD1IBpdQJCcVDuoHYsFgV4mp9vgOfn9osSDp2TGikwNz4Az95e81xnjVUmzh-NjDsEpw71DH92iHV_xt2sTwtzftS49LpPW2LjrV0evtNBP_t_RfskdArt5VSv25eORl7zScqfe1CiMkcVbf2UqACQjo3LbkpfN26HAioO2oH0ECPiRzT0Xyh-KwFutJLS9Xgghyw-LD9kPKcE_xbbJ9Y4Rqajh7WdPYuB0Jd9DPVrslmzK-F6cgHHYoZEv0SvLQi-PO0rpDrcjOEQQ
----

. In the {mtc-short} web console, click *Clusters*.
. Click *Add cluster*.
. Fill in the following fields:

* *Cluster name*: The cluster name can contain lower-case letters (`a-z`) and numbers (`0-9`). It must not contain spaces or international characters.
* *URL*: Specify the API server URL, for example, `\https://<www.example.com>:8443`.
* *Service account token*: Paste the `migration-controller` service account token.
* *Exposed route host to image registry*: If you are using direct image migration, specify the exposed route to the image registry of the source cluster.
+
To create the route, run the following command:
+
** For {product-title} 3:
+
[source,terminal]
----
$ oc create route passthrough --service=docker-registry --port=5000 -n default
----
** For {product-title} 4:
+
[source,terminal]
----
$ oc create route passthrough --service=image-registry --port=5000 -n openshift-image-registry
----

* *Azure cluster*: You must select this option if you use Azure snapshots to copy your data.
* *Azure resource group*: This field is displayed if *Azure cluster* is selected. Specify the Azure resource group.
* *Require SSL verification*: Optional: Select this option to verify SSL connections to the cluster.
* *CA bundle file*: This field is displayed if *Require SSL verification* is selected. If you created a custom CA certificate bundle file for self-signed certificates, click *Browse*, select the CA bundle file, and upload it.

. Click *Add cluster*.
+
The cluster appears in the *Clusters* list.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc

:_mod-docs-content-type: PROCEDURE
[id="migration-adding-replication-repository-to-cam_{context}"]
= Adding a replication repository to the {mtc-short} web console

You can add an object storage as a replication repository to the {mtc-full} ({mtc-short}) web console.

{mtc-short} supports the following storage providers:

* Amazon Web Services (AWS) S3
* Multi-Cloud Object Gateway (MCG)
* Generic S3 object storage, for example, Minio or Ceph S3
* Google Cloud Provider (GCP)
* Microsoft Azure Blob

.Prerequisites

* You must configure the object storage as a replication repository.

.Procedure

. In the {mtc-short} web console, click *Replication repositories*.
. Click *Add repository*.
. Select a *Storage provider type* and fill in the following fields:

* *AWS* for S3 providers, including AWS and MCG:

** *Replication repository name*: Specify the replication repository name in the {mtc-short} web console.
** *S3 bucket name*: Specify the name of the S3 bucket.
** *S3 bucket region*: Specify the S3 bucket region. *Required* for AWS S3. *Optional* for some S3 providers. Check the product documentation of your S3 provider for expected values.
** *S3 endpoint*: Specify the URL of the S3 service, not the bucket, for example, `\https://<s3-storage.apps.cluster.com>`. *Required* for a generic S3 provider. You must use the `https://` prefix.
** *S3 provider access key*: Specify the `<AWS_SECRET_ACCESS_KEY>` for AWS or the S3 provider access key for MCG and other S3 providers.
** *S3 provider secret access key*: Specify the `<AWS_ACCESS_KEY_ID>` for AWS or the S3 provider secret access key for MCG and other S3 providers.
** *Require SSL verification*: Clear this checkbox if you are using a generic S3 provider.
** If you created a custom CA certificate bundle for self-signed certificates, click *Browse* and browse to the Base64-encoded file.

* *GCP*:

** *Replication repository name*: Specify the replication repository name in the {mtc-short} web console.
** *GCP bucket name*: Specify the name of the GCP bucket.
** *GCP credential JSON blob*: Specify the string in the `credentials-velero` file.

* *Azure*:

** *Replication repository name*: Specify the replication repository name in the {mtc-short} web console.
** *Azure resource group*: Specify the resource group of the Azure Blob storage.
** *Azure storage account name*: Specify the Azure Blob storage account name.
** *Azure credentials - INI file contents*: Specify the string in the `credentials-velero` file.

. Click *Add repository* and wait for connection validation.

. Click *Close*.
+
The new repository appears in the *Replication repositories* list.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc

:_mod-docs-content-type: PROCEDURE
[id="migration-creating-migration-plan-cam_{context}"]
= Creating a migration plan in the {mtc-short} web console

You can create a migration plan in the {mtc-full} ({mtc-short}) web console.

.Prerequisites

* You must be logged in as a user with `cluster-admin` privileges on all clusters.
* You must ensure that the same {mtc-short} version is installed on all clusters.
* You must add the clusters and the replication repository to the {mtc-short} web console.
* If you want to use the _move_ data copy method to migrate a persistent volume (PV), the source and target clusters must have uninterrupted network access to the remote volume.
* If you want to use direct image migration, you must specify the exposed route to the image registry of the source cluster. This can be done by using the {mtc-short} web console or by updating the `MigCluster` custom resource manifest.

.Procedure

. In the {mtc-short} web console, click *Migration plans*.
. Click *Add migration plan*.
. Enter the *Plan name*.
+
The migration plan name must not exceed 253 lower-case alphanumeric characters (`a-z, 0-9`) and must not contain spaces or underscores (`_`).

. Select a *Source cluster*, a *Target cluster*, and a *Repository*.
. Click *Next*.
. Select the projects for migration.
. Optional: Click the edit icon beside a project to change the target namespace.
. Click *Next*.
. Select a *Migration type* for each PV:

* The *Copy* option copies the data from the PV of a source cluster to the replication repository and then restores the data on a newly created PV, with similar characteristics, in the target cluster.
* The *Move* option unmounts a remote volume, for example, NFS, from the source cluster, creates a PV resource on the target cluster pointing to the remote volume, and then mounts the remote volume on the target cluster. Applications running on the target cluster use the same remote volume that the source cluster was using.

. Click *Next*.
. Select a *Copy method* for each PV:

* *Snapshot copy* backs up and restores data using the cloud provider's snapshot functionality. It is significantly faster than *Filesystem copy*.
* *Filesystem copy* backs up the files on the source cluster and restores them on the target cluster.
+
The file system copy method is required for direct volume migration.

. You can select *Verify copy* to verify data migrated with *Filesystem copy*. Data is verified by generating a checksum for each source file and checking the checksum after restoration. Data verification significantly reduces performance.

. Select a *Target storage class*.
+
If you selected *Filesystem copy*, you can change the target storage class.

. Click *Next*.
. On the *Migration options* page, the *Direct image migration* option is selected if you specified an exposed image registry route for the source cluster. The *Direct PV migration* option is selected if you are migrating data with *Filesystem copy*.
+
The direct migration options copy images and files directly from the source cluster to the target cluster. This option is much faster than copying images and files from the source cluster to the replication repository and then from the replication repository to the target cluster.

. Click *Next*.
. Optional: Click *Add Hook* to add a hook to the migration plan.
+
A hook runs custom code. You can add up to four hooks to a single migration plan. Each hook runs during a different migration step.

.. Enter the name of the hook to display in the web console.
.. If the hook is an Ansible playbook, select *Ansible playbook* and click *Browse* to upload the playbook or paste the contents of the playbook in the field.
.. Optional: Specify an Ansible runtime image if you are not using the default hook image.
.. If the hook is not an Ansible playbook, select *Custom container image* and specify the image name and path.
+
A custom container image can include Ansible playbooks.

.. Select *Source cluster* or *Target cluster*.
.. Enter the *Service account name* and the *Service account namespace*.
.. Select the migration step for the hook:

* *preBackup*: Before the application workload is backed up on the source cluster
* *postBackup*: After the application workload is backed up on the source cluster
* *preRestore*: Before the application workload is restored on the target cluster
* *postRestore*: After the application workload is restored on the target cluster

.. Click *Add*.

. Click *Finish*.
+
The migration plan is displayed in the *Migration plans* list.

:leveloffset!:

[discrete]
[id="additional-resources-for-persistent-volume-copy-methods_{context}"]
[role="_additional-resources"]
=== Additional resources for persistent volume copy methods

* xref:../migration_toolkit_for_containers/about-mtc.adoc#file-system-copy-method_about-mtc[{mtc-short} file system copy method]
* xref:../migration_toolkit_for_containers/about-mtc.adoc#snapshot-copy-method_about-mtc[{mtc-short} snapshot copy method]

:leveloffset: +2

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/migrating-applications-3-4.adoc
// * migration_toolkit_for_containers/migrating-applications-with-mtc

:_mod-docs-content-type: PROCEDURE
[id="migration-running-migration-plan-cam_{context}"]
= Running a migration plan in the {mtc-short} web console

You can migrate applications and data with the migration plan you created in the {mtc-full} ({mtc-short}) web console.

[NOTE]
====
During migration, {mtc-short} sets the reclaim policy of migrated persistent volumes (PVs) to `Retain` on the target cluster.

The `Backup` custom resource contains a `PVOriginalReclaimPolicy` annotation that indicates the original reclaim policy. You can manually restore the reclaim policy of the migrated PVs.
====

.Prerequisites

The {mtc-short} web console must contain the following:

* Source cluster in a `Ready` state
* Target cluster in a `Ready` state
* Replication repository
* Valid migration plan

.Procedure

. Log in to the {mtc-short} web console and click *Migration plans*.
. Click the Options menu {kebab} next to a migration plan and select one of the following options under *Migration*:

* *Stage* copies data from the source cluster to the target cluster without stopping the application.
* *Cutover* stops the transactions on the source cluster and moves the resources to the target cluster.
+
Optional: In the *Cutover migration* dialog, you can clear the *Halt transactions on the source cluster during migration* checkbox.

* *State* copies selected persistent volume claims (PVCs).
+
[IMPORTANT]
====
Do not use state migration to migrate a namespace between clusters. Use stage or cutover migration instead.
====

** Select one or more PVCs in the *State migration* dialog and click *Migrate*.

. When the migration is complete, verify that the application migrated successfully in the {product-title} web console:

.. Click *Home* -> *Projects*.
.. Click the migrated project to view its status.
.. In the *Routes* section, click *Location* to verify that the application is functioning, if applicable.
.. Click *Workloads* -> *Pods* to verify that the pods are running in the migrated namespace.
.. Click *Storage* -> *Persistent volumes* to verify that the migrated persistent volumes are correctly provisioned.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/migration-prerequisites,modules/migration-launching-cam,modules/migration-adding-cluster-to-cam,modules/migration-adding-replication-repository-to-cam,modules/migration-creating-migration-plan-cam,modules/migration-running-migration-plan-cam

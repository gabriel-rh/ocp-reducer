:_mod-docs-content-type: ASSEMBLY
[id="node-feature-discovery-operator"]
= Node Feature Discovery Operator
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: node-feature-discovery-operator

toc::[]

Learn about the Node Feature Discovery (NFD) Operator and how you can use it to expose node-level information by orchestrating Node Feature Discovery, a Kubernetes add-on for detecting hardware features and system configuration.

:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/psap-node-feature-discovery-operator.adoc

:perf:
:_mod-docs-content-type: CONCEPT
[id="about-node-feature-discovery-operator_{context}"]
= About the Node Feature Discovery Operator
The Node Feature Discovery Operator (NFD) manages the detection of hardware features and configuration in an {product-title} cluster by labeling the nodes with hardware-specific information. NFD labels the host with node-specific attributes, such as PCI cards, kernel, operating system version, and so on.

The NFD Operator can be found on the Operator Hub by searching for “Node Feature Discovery”.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/psap-node-feature-discovery-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-the-node-feature-discovery-operator_{context}"]
= Installing the Node Feature Discovery Operator

The Node Feature Discovery (NFD) Operator orchestrates all resources needed to run the NFD daemon set. As a cluster administrator, you can install the NFD Operator by using the {product-title} CLI or the web console.

[id="install-operator-cli_{context}"]
== Installing the NFD Operator using the CLI

As a cluster administrator, you can install the NFD Operator using the CLI.

.Prerequisites

* An {product-title} cluster
* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.

.Procedure

. Create a namespace for the NFD Operator.

.. Create the following `Namespace` custom resource (CR) that defines the `openshift-nfd` namespace, and then save the YAML in the `nfd-namespace.yaml` file:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-nfd
----

.. Create the namespace by running the following command:
+
[source,terminal]
----
$ oc create -f nfd-namespace.yaml
----

. Install the NFD Operator in the namespace you created in the previous step by creating the following objects:

.. Create the following `OperatorGroup` CR and save the YAML in the `nfd-operatorgroup.yaml` file:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  generateName: openshift-nfd-
  name: openshift-nfd
  namespace: openshift-nfd
spec:
  targetNamespaces:
  - openshift-nfd
----

.. Create the `OperatorGroup` CR by running the following command:
+
[source,terminal]
----
$ oc create -f nfd-operatorgroup.yaml
----

.. Create the following `Subscription` CR and save the YAML in the `nfd-sub.yaml` file:
+
.Example Subscription
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: nfd
  namespace: openshift-nfd
spec:
  channel: "stable"
  installPlanApproval: Automatic
  name: nfd
  source: redhat-operators
  sourceNamespace: openshift-marketplace
----

.. Create the subscription object by running the following command:
+
[source,terminal]
----
$ oc create -f nfd-sub.yaml
----

.. Change to the `openshift-nfd` project:
+
[source,terminal]
----
$ oc project openshift-nfd
----

.Verification

* To verify that the Operator deployment is successful, run:
+
[source,terminal]
----
$ oc get pods
----
+
.Example output
[source,terminal]
----
NAME                                      READY   STATUS    RESTARTS   AGE
nfd-controller-manager-7f86ccfb58-vgr4x   2/2     Running   0          10m
----
+
A successful deployment shows a `Running` status.

[id="install-operator-web-console_{context}"]
== Installing the NFD Operator using the web console

As a cluster administrator, you can install the NFD Operator using the web console.

.Procedure

. In the {product-title} web console, click *Operators* -> *OperatorHub*.

. Choose *Node Feature Discovery* from the list of available Operators, and then click *Install*.

. On the *Install Operator* page, select *A specific namespace on the cluster*, and then click *Install*. You do not need to create a namespace because it is created for you.

.Verification

To verify that the NFD Operator installed successfully:

. Navigate to the *Operators* -> *Installed Operators* page.
. Ensure that *Node Feature Discovery* is listed in the *openshift-nfd* project with a *Status* of *InstallSucceeded*.
+
[NOTE]
====
During installation an Operator might display a *Failed* status. If the installation later succeeds with an *InstallSucceeded* message, you can ignore the *Failed* message.
====

.Troubleshooting

If the Operator does not appear as installed, troubleshoot further:

. Navigate to the *Operators* -> *Installed Operators* page and inspect the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors under *Status*.
. Navigate to the *Workloads* -> *Pods* page and check the logs for pods in the `openshift-nfd` project.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/psap-node-feature-discovery-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="using-the-node-feature-discovery-operator_{context}"]
= Using the Node Feature Discovery Operator

The Node Feature Discovery (NFD) Operator orchestrates all resources needed to run the Node-Feature-Discovery daemon set by watching for a `NodeFeatureDiscovery` CR. Based on the `NodeFeatureDiscovery` CR, the Operator will create the operand (NFD) components in the desired namespace. You can edit the CR to choose another `namespace`, `image`, `imagePullPolicy`, and `nfd-worker-conf`, among other options.

As a cluster administrator, you can create a `NodeFeatureDiscovery` instance using the {product-title} CLI or the web console.

[id="create-cd-cli_{context}"]
== Create a NodeFeatureDiscovery instance using the CLI

As a cluster administrator, you can create a `NodeFeatureDiscovery` CR instance using the CLI.

.Prerequisites

* An {product-title} cluster
* Install the OpenShift CLI (`oc`).
* Log in as a user with `cluster-admin` privileges.
* Install the NFD Operator.

.Procedure

. Create the following `NodeFeatureDiscovery` Custom Resource (CR), and then save the YAML in the `NodeFeatureDiscovery.yaml` file:
+
[source,yaml,subs="attributes+"]
----
apiVersion: nfd.openshift.io/v1
kind: NodeFeatureDiscovery
metadata:
  name: nfd-instance
  namespace: openshift-nfd
spec:
  instance: "" # instance is empty by default
  topologyupdater: false # False by default
  operand:
    image: registry.redhat.io/openshift4/ose-node-feature-discovery:v{product-version}
    imagePullPolicy: Always
  workerConfig:
    configData: |
      core:
      #  labelWhiteList:
      #  noPublish: false
        sleepInterval: 60s
      #  sources: [all]
      #  klog:
      #    addDirHeader: false
      #    alsologtostderr: false
      #    logBacktraceAt:
      #    logtostderr: true
      #    skipHeaders: false
      #    stderrthreshold: 2
      #    v: 0
      #    vmodule:
      ##   NOTE: the following options are not dynamically run-time configurable
      ##         and require a nfd-worker restart to take effect after being changed
      #    logDir:
      #    logFile:
      #    logFileMaxSize: 1800
      #    skipLogHeaders: false
      sources:
        cpu:
          cpuid:
      #     NOTE: whitelist has priority over blacklist
            attributeBlacklist:
              - "BMI1"
              - "BMI2"
              - "CLMUL"
              - "CMOV"
              - "CX16"
              - "ERMS"
              - "F16C"
              - "HTT"
              - "LZCNT"
              - "MMX"
              - "MMXEXT"
              - "NX"
              - "POPCNT"
              - "RDRAND"
              - "RDSEED"
              - "RDTSCP"
              - "SGX"
              - "SSE"
              - "SSE2"
              - "SSE3"
              - "SSE4.1"
              - "SSE4.2"
              - "SSSE3"
            attributeWhitelist:
        kernel:
          kconfigFile: "/path/to/kconfig"
          configOpts:
            - "NO_HZ"
            - "X86"
            - "DMI"
        pci:
          deviceClassWhitelist:
            - "0200"
            - "03"
            - "12"
          deviceLabelFields:
            - "class"
  customConfig:
    configData: |
          - name: "more.kernel.features"
            matchOn:
            - loadedKMod: ["example_kmod3"]
----

For more details on how to customize NFD workers, refer to the link:https://kubernetes-sigs.github.io/node-feature-discovery/v0.10/advanced/worker-configuration-reference.html[Configuration file reference of nfd-worker].

. Create the `NodeFeatureDiscovery` CR instance by running the following command:
+
[source,terminal]
----
$ oc create -f NodeFeatureDiscovery.yaml
----

.Verification

* To verify that the instance is created, run:
+
[source,terminal]
----
$ oc get pods
----
+
.Example output
[source,terminal]
----
NAME                                      READY   STATUS    RESTARTS   AGE
nfd-controller-manager-7f86ccfb58-vgr4x   2/2     Running   0          11m
nfd-master-hcn64                          1/1     Running   0          60s
nfd-master-lnnxx                          1/1     Running   0          60s
nfd-master-mp6hr                          1/1     Running   0          60s
nfd-worker-vgcz9                          1/1     Running   0          60s
nfd-worker-xqbws                          1/1     Running   0          60s
----
+
A successful deployment shows a `Running` status.

[id="create-nfd-cr-web-console_{context}"]
== Create a NodeFeatureDiscovery CR using the web console

.Procedure

. Navigate to the *Operators* -> *Installed Operators* page.
. Find *Node Feature Discovery* and see a box under *Provided APIs*.
. Click *Create instance*.
. Edit the values of the `NodeFeatureDiscovery` CR.
. Click *Create*.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/psap-node-feature-discovery-operator.adoc

:_mod-docs-content-type: REFERENCE
[id="configuring-the-node-feature-discovery_{context}"]
= Configuring the Node Feature Discovery Operator

[id="configuring-node-feature-discovery-operator-core_{context}"]
== core

The `core` section contains common configuration settings that are not specific to any particular feature source.

[discrete]
[id="configuring-node-feature-discovery-operator-core-sleepInterval_{context}"]
=== core.sleepInterval

`core.sleepInterval` specifies the interval between consecutive passes of feature detection or re-detection, and thus also the interval between node re-labeling. A non-positive value implies infinite sleep interval; no re-detection or re-labeling is done.

This value is overridden by the deprecated `--sleep-interval` command line flag, if specified.

.Example usage
[source,yaml]
----
core:
  sleepInterval: 60s <1>
----
The default value is `60s`.

[discrete]
[id="configuring-node-feature-discovery-operator-core-sources_{context}"]
=== core.sources

`core.sources` specifies the list of enabled feature sources. A special value `all` enables all feature sources.

This value is overridden by the deprecated `--sources` command line flag, if specified.

Default: `[all]`

.Example usage
[source,yaml]
----
core:
  sources:
    - system
    - custom
----

[discrete]
[id="configuring-node-feature-discovery-operator-core-label-whitelist_{context}"]
=== core.labelWhiteList

`core.labelWhiteList` specifies a regular expression for filtering feature labels based on the label name. Non-matching labels are not published.

The regular expression is only matched against the basename part of the label, the part of the name after '/'. The label prefix,  or namespace, is omitted.

This value is overridden by the deprecated `--label-whitelist` command line flag, if specified.

Default: `null`

.Example usage
[source,yaml]
----
core:
  labelWhiteList: '^cpu-cpuid'
----

[discrete]
[id="configuring-node-feature-discovery-operator-core-no-publish_{context}"]
=== core.noPublish

Setting `core.noPublish` to `true` disables all communication with the `nfd-master`. It is effectively a dry run flag; `nfd-worker` runs feature detection normally, but no labeling requests are sent to `nfd-master`.

This value is overridden by the `--no-publish` command line flag, if specified.

Example:

.Example usage
[source,yaml]
----
core:
  noPublish: true <1>
----
The default value is `false`.

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog_{context}"]
== core.klog

The following options specify the logger configuration, most of which can be dynamically adjusted at run-time.

The logger options can also be specified using command line flags, which take precedence over any corresponding config file options.

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-adddirheader_{context}"]
=== core.klog.addDirHeader

If set to `true`, `core.klog.addDirHeader` adds the file directory to the header of the log messages.

Default: `false`

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-alsologtostderr_{context}"]
=== core.klog.alsologtostderr

Log to standard error as well as files.

Default: `false`

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-BacktraceAt_{context}"]
=== core.klog.logBacktraceAt

When logging hits line file:N, emit a stack trace.

Default: *empty*

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-logdir_{context}"]
=== core.klog.logDir

If non-empty, write log files in this directory.

Default: *empty*

Run-time configurable: no

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-logfile_{context}"]
=== core.klog.logFile

If not empty, use this log file.

Default: *empty*

Run-time configurable: no

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-logFileMaxSize_{context}"]
=== core.klog.logFileMaxSize

`core.klog.logFileMaxSize` defines the maximum size a log file can grow to. Unit is megabytes. If the value is `0`, the maximum file size is unlimited.

Default: `1800`

Run-time configurable: no

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-logtostderr_{context}"]
=== core.klog.logtostderr

Log to standard error instead of files

Default: `true`

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-skipHeaders_{context}"]
=== core.klog.skipHeaders

If `core.klog.skipHeaders` is set to `true`, avoid header prefixes in the log messages.

Default: `false`

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-skipLogHeaders_{context}"]
=== core.klog.skipLogHeaders

If `core.klog.skipLogHeaders` is set to `true`, avoid headers when opening log files.

Default: `false`

Run-time configurable: no

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-stderrthreshold_{context}"]
=== core.klog.stderrthreshold

Logs at or above this threshold go to stderr.

Default: `2`

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-v_{context}"]
=== core.klog.v

`core.klog.v` is the number for the log level verbosity.

Default: `0`

Run-time configurable: yes

[discrete]
[id="configuring-node-feature-discovery-operator-core-klog-vmodule_{context}"]
=== core.klog.vmodule

`core.klog.vmodule` is a comma-separated list of `pattern=N` settings for file-filtered logging.

Default: *empty*

Run-time configurable: yes

[id="configuring-node-feature-discovery-operator-sources_{context}"]
== sources

The `sources` section contains feature source specific configuration parameters.

[discrete]
[id="configuring-node-feature-discovery-operator-sources-cpu-cpuid-attributeBlacklist_{context}"]
=== sources.cpu.cpuid.attributeBlacklist

Prevent publishing `cpuid` features listed in this option.

This value is overridden by `sources.cpu.cpuid.attributeWhitelist`, if specified.

Default: `[BMI1, BMI2, CLMUL, CMOV, CX16, ERMS, F16C, HTT, LZCNT, MMX, MMXEXT, NX, POPCNT, RDRAND, RDSEED, RDTSCP, SGX, SGXLC, SSE, SSE2, SSE3, SSE4.1, SSE4.2, SSSE3]`

.Example usage
[source,yaml]
----
sources:
  cpu:
    cpuid:
      attributeBlacklist: [MMX, MMXEXT]
----

[discrete]
[id="configuring-node-feature-discovery-operator-sources-cpu-cpuid-attributeWhitelist_{context}"]
=== sources.cpu.cpuid.attributeWhitelist

Only publish the `cpuid` features listed in this option.

`sources.cpu.cpuid.attributeWhitelist` takes precedence over `sources.cpu.cpuid.attributeBlacklist`.

Default: *empty*

.Example usage
[source,yaml]
----
sources:
  cpu:
    cpuid:
      attributeWhitelist: [AVX512BW, AVX512CD, AVX512DQ, AVX512F, AVX512VL]
----

[discrete]
[id="configuring-node-feature-discovery-operator-sources-kernel-kconfigFilet_{context}"]
=== sources.kernel.kconfigFile

`sources.kernel.kconfigFile` is the path of the kernel config file. If empty, NFD runs a search in the well-known standard locations.

Default: *empty*

.Example usage
[source,yaml]
----
sources:
  kernel:
    kconfigFile: "/path/to/kconfig"
----

[discrete]
[id="configuring-node-feature-discovery-operator-sources-kernel-configOpts_{context}"]
=== sources.kernel.configOpts

`sources.kernel.configOpts` represents kernel configuration options to publish as feature labels.

Default: `[NO_HZ, NO_HZ_IDLE, NO_HZ_FULL, PREEMPT]`

.Example usage
[source,yaml]
----
sources:
  kernel:
    configOpts: [NO_HZ, X86, DMI]
----

[discrete]
[id="configuring-node-feature-discovery-operator-sources-pci-deviceClassWhitelist_{context}"]
=== sources.pci.deviceClassWhitelist

`sources.pci.deviceClassWhitelist` is a list of link:https://pci-ids.ucw.cz/read/PD[PCI device class IDs] for which to publish a label. It can be specified as a main class only (for example, `03`) or full class-subclass combination (for example `0300`). The former implies that all
subclasses are accepted.  The format of the labels can be further configured with `deviceLabelFields`.

Default: `["03", "0b40", "12"]`

.Example usage
[source,yaml]
----
sources:
  pci:
    deviceClassWhitelist: ["0200", "03"]
----

[discrete]
[id="configuring-node-feature-discovery-operator-sources-pci-deviceLabelFields_{context}"]
=== sources.pci.deviceLabelFields

`sources.pci.deviceLabelFields` is the set of PCI ID fields to use when constructing the name of the feature label. Valid fields are `class`, `vendor`, `device`, `subsystem_vendor` and `subsystem_device`.

Default: `[class, vendor]`

.Example usage
[source,yaml]
----
sources:
  pci:
    deviceLabelFields: [class, vendor, device]
----

With the example config above, NFD would publish labels such as `feature.node.kubernetes.io/pci-<class-id>_<vendor-id>_<device-id>.present=true`

[discrete]
[id="configuring-node-feature-discovery-operator-sources-usb-deviceClassWhitelist_{context}"]
=== sources.usb.deviceClassWhitelist

`sources.usb.deviceClassWhitelist` is a list of USB link:https://www.usb.org/defined-class-codes[device class] IDs for
which to publish a feature label. The format of the labels can be further
configured with `deviceLabelFields`.

Default: `["0e", "ef", "fe", "ff"]`

.Example usage
[source,yaml]
----
sources:
  usb:
    deviceClassWhitelist: ["ef", "ff"]
----

[discrete]
[id="configuring-node-feature-discovery-operator-sources-usb-deviceLabelFields_{context}"]
=== sources.usb.deviceLabelFields

`sources.usb.deviceLabelFields` is the set of USB ID fields from which to compose the name of the feature label. Valid fields are `class`, `vendor`, and `device`.

Default: `[class, vendor, device]`

.Example usage
[source,yaml]
----
sources:
  pci:
    deviceLabelFields: [class, vendor]
----

With the example config above, NFD would publish labels like: `feature.node.kubernetes.io/usb-<class-id>_<vendor-id>.present=true`.

[discrete]
[id="configuring-node-feature-discovery-operator-sources-custom_{context}"]
=== sources.custom

`sources.custom` is the list of rules to process in the custom feature source to create user-specific labels.

Default: *empty*

.Example usage
[source,yaml]
----
source:
  custom:
  - name: "my.custom.feature"
    matchOn:
    - loadedKMod: ["e1000e"]
    - pciId:
        class: ["0200"]
        vendor: ["8086"]
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/psap-node-feature-discovery-operator.adoc

:_mod-docs-content-type: PROCEDURE
[id="using-the-nfd-topology-updater_{context}"]
= Using the NFD Topology Updater

The Node Feature Discovery (NFD) Topology Updater is a daemon responsible for examining allocated resources on a worker node. It accounts for resources that are available to be allocated to new pod on a per-zone basis, where a zone can be a Non-Uniform Memory Access (NUMA) node. The NFD Topology Updater communicates the information to nfd-master, which creates a `NodeResourceTopology` custom resource (CR) corresponding to all of the worker nodes in the cluster. One instance of the NFD Topology Updater runs on each node of the cluster.

To enable the Topology Updater workers in NFD, set the `topologyupdater` variable to `true` in the `NodeFeatureDiscovery` CR, as described in the section *Using the Node Feature Discovery Operator*.

== NodeResourceTopology CR

When run with NFD Topology Updater, NFD creates custom resource instances corresponding to the node resource hardware topology, such as:

[source,yaml]
----
apiVersion: topology.node.k8s.io/v1alpha1
kind: NodeResourceTopology
metadata:
  name: node1
topologyPolicies: ["SingleNUMANodeContainerLevel"]
zones:
  - name: node-0
    type: Node
    resources:
      - name: cpu
        capacity: 20
        allocatable: 16
        available: 10
      - name: vendor/nic1
        capacity: 3
        allocatable: 3
        available: 3
  - name: node-1
    type: Node
    resources:
      - name: cpu
        capacity: 30
        allocatable: 30
        available: 15
      - name: vendor/nic2
        capacity: 6
        allocatable: 6
        available: 6
  - name: node-2
    type: Node
    resources:
      - name: cpu
        capacity: 30
        allocatable: 30
        available: 15
      - name: vendor/nic1
        capacity: 3
        allocatable: 3
        available: 3
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/psap-node-feature-discovery-operator.adoc

:_mod-docs-content-type: REFERENCE
[id="nfd-topology-updater-command-line-flags_{context}"]
= NFD Topology Updater command line flags

To view available command line flags, run the `nfd-topology-updater -help` command. For example, in a podman container, run the following command:

[source,terminal]
----
$ podman run gcr.io/k8s-staging-nfd/node-feature-discovery:master nfd-topology-updater -help
----

[discrete]
[id="nfd-topology-updater-ca-file_{context}"]
== -ca-file

The `-ca-file` flag is one of the three flags, together with the `-cert-file` and `-key-file`flags, that controls the mutual TLS authentication on the NFD Topology Updater. This flag specifies the TLS root certificate that is used for verifying the authenticity of nfd-master.

Default: empty

[IMPORTANT]
====
The `-ca-file` flag must be specified together with the `-cert-file` and `-key-file` flags.
====

.Example
[source,terminal]
----
$ nfd-topology-updater -ca-file=/opt/nfd/ca.crt -cert-file=/opt/nfd/updater.crt -key-file=/opt/nfd/updater.key
----

[discrete]
[id="nfd-topology-updater-cert-file_{context}"]
== -cert-file

The `-cert-file` flag is one of the three flags, together with the `-ca-file` and `-key-file flags`, that controls mutual TLS authentication on the NFD Topology Updater. This flag specifies the TLS certificate presented for authenticating outgoing requests.

Default: empty

[IMPORTANT]
====
The `-cert-file` flag must be specified together with the `-ca-file` and `-key-file` flags.
====

.Example
[source,terminal]
----
$ nfd-topology-updater -cert-file=/opt/nfd/updater.crt -key-file=/opt/nfd/updater.key -ca-file=/opt/nfd/ca.crt
----

[discrete]
[id="nfd-topology-updater-help_{context}"]
== -h, -help

Print usage and exit.

[discrete]
[id="nfd-topology-updater-key-file_{context}"]
== -key-file

The `-key-file` flag is one of the three flags, together with the `-ca-file` and `-cert-file` flags, that controls the mutual TLS authentication on the NFD Topology Updater. This flag specifies the private key corresponding the given certificate file, or `-cert-file`, that is used for authenticating outgoing requests.

Default: empty

[IMPORTANT]
====
The `-key-file` flag must be specified together with the `-ca-file` and `-cert-file` flags.
====

.Example
[source,terminal]
----
$ nfd-topology-updater -key-file=/opt/nfd/updater.key -cert-file=/opt/nfd/updater.crt -ca-file=/opt/nfd/ca.crt
----

[discrete]
[id="nfd-topology-updater-kubelet-config-file_{context}"]
== -kubelet-config-file

The `-kubelet-config-file` specifies the path to the Kubelet's configuration
file.

Default: `/host-var/lib/kubelet/config.yaml`

.Example
[source,terminal]
----
$ nfd-topology-updater -kubelet-config-file=/var/lib/kubelet/config.yaml
----

[discrete]
[id="nfd-topology-updater-no-publish_{context}"]
== -no-publish

The `-no-publish` flag disables all communication with the nfd-master, making it a dry run flag for nfd-topology-updater. NFD Topology Updater runs resource hardware topology detection normally, but no CR requests are sent to nfd-master.

Default: `false`

.Example
[source,terminal]
----
$ nfd-topology-updater -no-publish
----

[id="nfd-topology-updater-oneshot_{context}"]
== -oneshot

The `-oneshot` flag causes the NFD Topology Updater to exit after one pass of resource hardware topology detection.

Default: `false`

.Example
[source,terminal]
----
$ nfd-topology-updater -oneshot -no-publish
----

[discrete]
[id="nfd-topology-updater-podresources-socket_{context}"]
== -podresources-socket

The `-podresources-socket` flag specifies the path to the Unix socket where kubelet exports a gRPC service to enable discovery of in-use CPUs and devices, and to provide metadata for them.

Default: `/host-var/liblib/kubelet/pod-resources/kubelet.sock`

.Example
[source,terminal]
----
$ nfd-topology-updater -podresources-socket=/var/lib/kubelet/pod-resources/kubelet.sock
----

[discrete]
[id="nfd-topology-updater-server_{context}"]
== -server

The `-server` flag specifies the address of the nfd-master endpoint to connect to.

Default: `localhost:8080`

.Example
[source,terminal]
----
$ nfd-topology-updater -server=nfd-master.nfd.svc.cluster.local:443
----

[discrete]
[id="nfd-topology-updater-server-name-override_{context}"]
== -server-name-override

The `-server-name-override` flag specifies the common name (CN) which to expect from the nfd-master TLS certificate. This flag is mostly intended for development and debugging purposes.

Default: empty

.Example
[source,terminal]
----
$ nfd-topology-updater -server-name-override=localhost
----

[discrete]
[id="nfd-topology-updater-sleep-interval_{context}"]
== -sleep-interval

The `-sleep-interval` flag specifies the interval between resource hardware topology re-examination and custom resource updates. A non-positive value implies infinite sleep interval and no re-detection is done.

Default: `60s`

.Example
[source,terminal]
----
$ nfd-topology-updater -sleep-interval=1h
----

[discrete]
[id="nfd-topology-updater-version_{context}"]
== -version

Print version and exit.

[discrete]
[id="nfd-topology-updater-watch-namespace_{context}"]
== -watch-namespace

The `-watch-namespace` flag specifies the namespace to ensure that resource hardware topology examination only happens for the pods running in the
specified namespace. Pods that are not running in the specified namespace are not considered during resource accounting. This is particularly useful for testing and debugging purposes. A `*` value means that all of the pods across all namespaces are considered during the accounting process.

Default: `*`

.Example
[source,terminal]
----
$ nfd-topology-updater -watch-namespace=rte
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/psap-node-feature-discovery-operator,modules/psap-installing-node-feature-discovery-operator,modules/psap-using-node-feature-discovery-operator,modules/psap-configuring-node-feature-discovery,modules/psap-node-feature-discovery-using-topology-updater,modules/psap-node-feature-discovery-topology-updater-command-reference

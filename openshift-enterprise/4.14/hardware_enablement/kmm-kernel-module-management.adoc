:_mod-docs-content-type: ASSEMBLY
[id="kernel-module-management-operator"]
= Kernel Module Management Operator
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: kernel-module-management-operator

toc::[]

Learn about the Kernel Module Management (KMM) Operator and how you can use it to deploy out-of-tree kernel modules and device plugins on {product-title} clusters.

:FeatureName: Kernel Module Management Operator

:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="about-kmm_{context}"]
= About the Kernel Module Management Operator

The Kernel Module Management (KMM) Operator manages, builds, signs, and deploys out-of-tree kernel modules and device plugins on {product-title} clusters.

KMM adds a new `Module` CRD which describes an out-of-tree kernel module and its associated device plugin.
You can use `Module` resources to configure how to load the module, define `ModuleLoader` images for kernel versions, and include instructions for building and signing modules for specific kernel versions.

KMM is designed to accommodate multiple kernel versions at once for any kernel module, allowing for seamless node upgrades and reduced application downtime.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-install_{context}"]
= Installing the Kernel Module Management Operator

As a cluster administrator, you can install the Kernel Module Management (KMM) Operator by using the OpenShift CLI or the web console.

The KMM Operator is supported on {product-title} 4.12 and later.
Installing KMM on version 4.11 does not require specific additional steps.
For details on installing KMM on version 4.10 and earlier, see the section "Installing the Kernel Module Management Operator on earlier versions of {product-title}".

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-install-using-web-console_{context}"]
= Installing the Kernel Module Management Operator using the web console

As a cluster administrator, you can install the Kernel Module Management (KMM) Operator using the {product-title} web console.

.Procedure

. Log in to the {product-title} web console.
. Install the Kernel Module Management Operator:
.. In the {product-title} web console, click *Operators* -> *OperatorHub*.

.. Select *Kernel Module Management Operator* from the list of available Operators, and then click *Install*.

.. From the *Installed Namespace* list, select the `openshift-kmm` namespace.

..  Click *Install*.

.Verification

To verify that KMM Operator installed successfully:

. Navigate to the *Operators* -> *Installed Operators* page.
. Ensure that *Kernel Module Management Operator* is listed in the *openshift-kmm* project with a *Status* of *InstallSucceeded*.
+
[NOTE]
====
During installation, an Operator might display a *Failed* status. If the installation later succeeds with an *InstallSucceeded* message, you can ignore the *Failed* message.
====

.Troubleshooting
. To troubleshoot issues with Operator installation:
+
.. Navigate to the *Operators* -> *Installed Operators* page and inspect the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors under *Status*.
.. Navigate to the *Workloads* -> *Pods* page and check the logs for pods in the `openshift-kmm` project.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-install-using-cli_{context}"]
= Installing the Kernel Module Management Operator by using the CLI

As a cluster administrator, you can install the Kernel Module Management (KMM) Operator by using the OpenShift CLI.

.Prerequisites

* You have a running {product-title} cluster.
* You installed the OpenShift CLI (`oc`).
* You are logged into the OpenShift CLI as a user with `cluster-admin` privileges.

.Procedure

. Install KMM in the `openshift-kmm` namespace:

.. Create the following `Namespace` CR and save the YAML  file, for example, `kmm-namespace.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-kmm
----

.. Create the following `OperatorGroup` CR and save the YAML file, for example, `kmm-op-group.yaml`:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: kernel-module-management
  namespace: openshift-kmm
----

.. Create the following `Subscription` CR and save the YAML file, for example, `kmm-sub.yaml`:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: kernel-module-management
  namespace: openshift-kmm
spec:
  channel: release-1.0
  installPlanApproval: Automatic
  name: kernel-module-management
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: kernel-module-management.v1.0.0
----

.. Create the subscription object by running the following command:
+
[source,terminal]
----
$ oc create -f kmm-sub.yaml
----

.Verification

* To verify that the Operator deployment is successful, run the following command:
+
[source,terminal]
----
$ oc get -n openshift-kmm deployments.apps kmm-operator-controller-manager
----
+
.Example output
[source,terminal]
----
NAME                              READY UP-TO-DATE  AVAILABLE AGE
kmm-operator-controller-manager   1/1   1           1         97s
----
+
The Operator is available.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-install-older-version_{context}"]
= Installing the Kernel Module Management Operator on earlier versions of {product-title}

The KMM Operator is supported on {product-title} 4.12 and later.
For version 4.10 and earlier, you must create a new `SecurityContextConstraint` object and bind it to the Operator's `ServiceAccount`.
As a cluster administrator, you can install the Kernel Module Management (KMM) Operator by using the OpenShift CLI.

.Prerequisites

* You have a running {product-title} cluster.
* You installed the OpenShift CLI (`oc`).
* You are logged into the OpenShift CLI as a user with `cluster-admin` privileges.

.Procedure

. Install KMM in the `openshift-kmm` namespace:

.. Create the following `Namespace` CR and save the YAML file, for example, `kmm-namespace.yaml` file:
+
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-kmm
----

.. Create the following `SecurityContextConstraint` object and save the YAML file, for example, `kmm-security-constraint.yaml`:
+
[source,yaml]
----
allowHostDirVolumePlugin: false
allowHostIPC: false
allowHostNetwork: false
allowHostPID: false
allowHostPorts: false
allowPrivilegeEscalation: false
allowPrivilegedContainer: false
allowedCapabilities:
  - NET_BIND_SERVICE
apiVersion: security.openshift.io/v1
defaultAddCapabilities: null
fsGroup:
  type: MustRunAs
groups: []
kind: SecurityContextConstraints
metadata:
  name: restricted-v2
priority: null
readOnlyRootFilesystem: false
requiredDropCapabilities:
  - ALL
runAsUser:
  type: MustRunAsRange
seLinuxContext:
  type: MustRunAs
seccompProfiles:
  - runtime/default
supplementalGroups:
  type: RunAsAny
users: []
volumes:
  - configMap
  - downwardAPI
  - emptyDir
  - persistentVolumeClaim
  - projected
  - secret
----

.. Bind the `SecurityContextConstraint` object to the Operator's `ServiceAccount` by running the following commands:
+
[source,terminal]
----
$ oc apply -f kmm-security-constraint.yaml
----
+
[source,terminal]
----
$ oc adm policy add-scc-to-user kmm-security-constraint -z kmm-operator-controller-manager -n openshift-kmm
----

.. Create the following `OperatorGroup` CR and save the YAML file, for example, `kmm-op-group.yaml`:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: kernel-module-management
  namespace: openshift-kmm
----

.. Create the following `Subscription` CR and save the YAML file, for example, `kmm-sub.yaml`:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: kernel-module-management
  namespace: openshift-kmm
spec:
  channel: release-1.0
  installPlanApproval: Automatic
  name: kernel-module-management
  source: redhat-operators
  sourceNamespace: openshift-marketplace
  startingCSV: kernel-module-management.v1.0.0
----

.. Create the subscription object by running the following command:
+
[source,terminal]
----
$ oc create -f kmm-sub.yaml
----

.Verification

* To verify that the Operator deployment is successful, run the following command:
+
[source,terminal]
----
$ oc get -n openshift-kmm deployments.apps kmm-operator-controller-manager
----
+
.Example output
[source,terminal]
----
NAME                              READY UP-TO-DATE  AVAILABLE AGE
kmm-operator-controller-manager   1/1   1           1         97s
----
+
The Operator is available.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-deploy-kernel-modules_{context}"]
= Kernel module deployment

For each `Module` resource, Kernel Module Management (KMM) can create a number of `DaemonSet` resources:

* One ModuleLoader `DaemonSet` per compatible kernel version running in the cluster.
* One device plugin `DaemonSet`, if configured.

The module loader daemon set resources run ModuleLoader images to load kernel modules.
A module loader image is an OCI image that contains the `.ko` files and both the `modprobe` and `sleep` binaries.

When the module loader pod is created, the pod runs `modprobe` to insert the specified module into the kernel.
It then enters a sleep state until it is terminated.
When that happens, the `ExecPreStop` hook runs `modprobe -r` to unload the kernel module.

If the `.spec.devicePlugin` attribute is configured in a `Module` resource, then KMM creates a link:https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/[device plugin]
daemon set in the cluster.
That daemon set targets:

* Nodes that match the `.spec.selector` of the `Module` resource.
* Nodes with the kernel module loaded (where the module loader pod is in the `Ready` condition).

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-creating-module-cr_{context}"]

= The Module custom resource definition

The `Module` custom resource definition (CRD) represents a kernel module that can be loaded on all or select nodes in the cluster, through a module loader image.
A `Module` custom resource (CR) specifies one or more kernel versions with which it is compatible, and a node selector.

The compatible versions for a `Module` resource are listed under `.spec.moduleLoader.container.kernelMappings`.
A kernel mapping can either match a `literal` version, or use `regexp` to match many of them at the same time.

The reconciliation loop for the `Module` resource runs the following steps:

. List all nodes matching `.spec.selector`.
. Build a set of all kernel versions running on those nodes.
. For each kernel version:
 .. Go through `.spec.moduleLoader.container.kernelMappings` and find the appropriate container image name. If the kernel mapping has `build` or `sign` defined and the container image does not already exist, run the build, the signing job, or both, as needed.
.. Create a module loader daemon set with the container image determined in the previous step.
.. If `.spec.devicePlugin` is defined, create a device plugin daemon set using the configuration specified under `.spec.devicePlugin.container`.
. Run `garbage-collect` on:
 .. Existing daemon set resources targeting kernel versions that are not run by any node in the cluster.
 .. Successful build jobs.
 .. Successful signing jobs.

:leveloffset!:

// Added for TELCODOCS-1280
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-setting-soft-dependencies-between-kernel-modules_{context}"]
= Set soft dependencies between kernel modules

Some configurations require that several kernel modules be loaded in a specific order to work properly, even though the modules do not directly depend on each other through symbols.
These are called soft dependencies.
`depmod` is usually not aware of these dependencies, and they do not appear in the files it produces.
For example, if `mod_a` has a soft dependency on `mod_b`, `modprobe mod_a` will not load `mod_b`.

You can resolve these situations by declaring soft dependencies in the Module Custom Resource Definition (CRD) using the `modulesLoadingOrder` field.

[source,yaml]
----
# ...
spec:
  moduleLoader:
    container:
      modprobe:
        moduleName: mod_a
        dirName: /opt
        firmwarePath: /firmware
        parameters:
          - param=1
        modulesLoadingOrder:
          - mod_a
          - mod_b
----

In the configuration above:

* The loading order is `mod_b`, then `mod_a`.
* The unloading order is `mod_a`, then `mod_b`.

[NOTE]
====
The first value in the list, to be loaded last, must be equivalent to the `moduleName`.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: REFERENCE
[id="kmm-security_{context}"]
= Security and permissions

[IMPORTANT]
====
Loading kernel modules is a highly sensitive operation.
After they are loaded, kernel modules have all possible permissions to do any kind of operation on the node.
====

[id="serviceaccounts-and-securitycontextconstraint_{context}"]
== ServiceAccounts and SecurityContextConstraints

Kernel Module Management (KMM) creates a privileged workload to load the kernel modules on nodes.
That workload needs `ServiceAccounts` allowed to use the `privileged` `SecurityContextConstraint` (SCC) resource.

The authorization model for that workload depends on the namespace of the `Module` resource, as well as its spec.

* If the `.spec.moduleLoader.serviceAccountName` or `.spec.devicePlugin.serviceAccountName` fields are set, they are always used.
* If those fields are not set, then:
 ** If the `Module` resource is created in the operator's namespace (`openshift-kmm` by default), then KMM uses its default, powerful `ServiceAccounts` to run the daemon sets.
 ** If the `Module` resource is created in any other namespace, then KMM runs the daemon sets as the namespace's `default` `ServiceAccount`. The `Module` resource cannot run a privileged workload unless you manually enable it to use the `privileged` SCC.

[IMPORTANT]
====
`openshift-kmm` is a trusted namespace.

When setting up RBAC permissions, remember that any user or `ServiceAccount` creating a `Module` resource in the `openshift-kmm` namespace results in KMM automatically running privileged workloads on potentially all nodes in the cluster.
====

To allow any `ServiceAccount` to use the `privileged` SCC and therefore to run module loader or device plugin pods, use the following command:

[source,terminal]
----
$ oc adm policy add-scc-to-user privileged -z "${serviceAccountName}" [ -n "${namespace}" ]
----

[id="pod-security-standards_{context}"]
== Pod security standards

OpenShift runs a synchronization mechanism that sets the namespace Pod Security level automatically based on
the security contexts in use. No action is needed.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../authentication/understanding-and-managing-pod-security-admission.adoc#understanding-and-managing-pod-security-admission[Understanding and managing pod security admission].

// Added for TELCODOCS-1279
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-replacing-in-tree-modules-with-out-of-tree-modules_{context}"]
= Replacing in-tree modules with out-of-tree modules

You can use Kernel Module Management (KMM) to build kernel modules that can be loaded or unloaded into the kernel on demand. These modules extend the functionality of the kernel without the need to reboot the system. Modules can be configured as built-in or dynamically loaded.

Dynamically loaded modules include in-tree modules and out-of-tree (OOT) modules. In-tree modules are internal to the Linux kernel tree, that is, they are already part of the kernel. Out-of-tree modules are external to the Linux kernel tree. They are generally written for development and testing purposes, such as testing the new version of a kernel module that is shipped in-tree, or to deal with incompatibilities.

Some modules loaded by KMM could replace in-tree modules already loaded on the node. To unload an in-tree module before loading your module, set the `.spec.moduleLoader.container.inTreeModuleToRemove` field. The following is an example for module replacement for all kernel mappings:

[source,yaml]
----
# ...
spec:
  moduleLoader:
    container:
      modprobe:
        moduleName: mod_a

      inTreeModuleToRemove: mod_b
----

In this example, the `moduleLoader` pod uses `inTreeModuleToRemove` to unload the in-tree `mod_b` before loading `mod_a`
from the `moduleLoader` image.
When the `moduleLoader`pod is terminated and `mod_a` is unloaded, `mod_b` is not loaded again.

The following is an example for module replacement for specific kernel mappings:

[source,yaml]
----
# ...
spec:
  moduleLoader:
    container:
      kernelMappings:
        - literal: 6.0.15-300.fc37.x86_64
          containerImage: some.registry/org/my-kmod:6.0.15-300.fc37.x86_64
          inTreeModuleToRemove: <module_name>
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* link:https://fastbitlab.com/building-a-linux-kernel-module/[Building a linux kernel module]

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: REFERENCE
[id="kmm-example-cr_{context}"]

= Example Module CR

The following is an annotated `Module` example:

[source,yaml]
----
apiVersion: kmm.sigs.x-k8s.io/v1beta1
kind: Module
metadata:
  name: <my_kmod>
spec:
  moduleLoader:
    container:
      modprobe:
        moduleName: <my_kmod> <1>
        dirName: /opt <2>
        firmwarePath: /firmware <3>
        parameters:  <4>
          - param=1
      kernelMappings:  <5>
        - literal: 6.0.15-300.fc37.x86_64
          containerImage: some.registry/org/my-kmod:6.0.15-300.fc37.x86_64
        - regexp: '^.+\fc37\.x86_64$' <6>
          containerImage: "some.other.registry/org/<my_kmod>:${KERNEL_FULL_VERSION}"
        - regexp: '^.+$' <7>
          containerImage: "some.registry/org/<my_kmod>:${KERNEL_FULL_VERSION}"
          build:
            buildArgs:  <8>
              - name: ARG_NAME
                value: <some_value>
            secrets:
              - name: <some_kubernetes_secret>  <9>
            baseImageRegistryTLS: <10>
              insecure: false
              insecureSkipTLSVerify: false <11>
            dockerfileConfigMap:  <12>
              name: <my_kmod_dockerfile>
          sign:
            certSecret:
              name: <cert_secret>  <13>
            keySecret:
              name: <key_secret>  <14>
            filesToSign:
              - /opt/lib/modules/${KERNEL_FULL_VERSION}/<my_kmod>.ko
          registryTLS: <15>
            insecure: false <16>
            insecureSkipTLSVerify: false
    serviceAccountName: <sa_module_loader>  <17>
  devicePlugin:  <18>
    container:
      image: some.registry/org/device-plugin:latest  <19>
      env:
        - name: MY_DEVICE_PLUGIN_ENV_VAR
          value: SOME_VALUE
      volumeMounts:  <20>
        - mountPath: /some/mountPath
          name: <device_plugin_volume>
    volumes:  <21>
      - name: <device_plugin_volume>
        configMap:
          name: <some_configmap>
    serviceAccountName: <sa_device_plugin> <22>
  imageRepoSecret:  <23>
    name: <secret_name>
  selector:
    node-role.kubernetes.io/worker: ""
----
<1> Required.
<2> Optional.
<3> Optional: Copies `/firmware/*` into `/var/lib/firmware/` on the node.
<4> Optional.
<5> At least one kernel item is required.
<6> For each node running a kernel matching the regular expression, KMM creates a `DaemonSet` resource running the image specified in `containerImage` with `${KERNEL_FULL_VERSION}` replaced with the kernel version.
<7> For any other kernel, build the image using the Dockerfile in the `my-kmod` ConfigMap.
<8> Optional.
<9> Optional: A value for `some-kubernetes-secret` can be obtained from the build environment at `/run/secrets/some-kubernetes-secret`.
<10> Optional: Avoid using this parameter. If set to `true`, the build is allowed to pull the image in the Dockerfile `FROM` instruction using plain HTTP.
<11> Optional: Avoid using this parameter. If set to `true`, the build will skip any TLS server certificate validation when pulling the image in the Dockerfile `FROM` instruction using plain HTTP.
<12> Required.
<13> Required: A secret holding the public secureboot key with the key 'cert'.
<14> Required: A secret holding the private secureboot key with the key 'key'.
<15> Optional: Avoid using this parameter. If set to `true`, KMM will be allowed to check if the container image already exists using plain HTTP.
<16> Optional: Avoid using this parameter. If set to `true`, KMM will skip any TLS server certificate validation when checking if the container image already exists.
<17> Optional.
<18> Optional.
<19> Required: If the device plugin section is present.
<20> Optional.
<21> Optional.
<22> Optional.
<23> Optional: Used to pull module loader and device plugin images.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-creating-moduleloader-image_{context}"]
= Using a ModuleLoader image

Kernel Module Management (KMM) works with purpose-built module loader images.
These are standard OCI images that must satisfy the following requirements:

* `.ko` files must be located in `+/opt/lib/modules/${KERNEL_VERSION}+`.
* `modprobe` and `sleep` binaries must be defined in the `$PATH` variable.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-running-depmod_{context}"]

= Running depmod

If your module loader image contains several kernel modules and if one of the modules depends on another module, it is best practice to run `depmod` at the end of the build process to generate dependencies and map files.

[NOTE]
====
You must have a Red Hat subscription to download the `kernel-devel` package.
====

.Procedure

. To generate `modules.dep` and `.map` files for a specific kernel version, run `+depmod -b /opt ${KERNEL_VERSION}+`.

[id="example-dockerfile_{context}"]
== Example Dockerfile

If you are building your image on {product-title}, consider using the Driver Tool Kit (DTK).

For further information, see link:https://cloud.redhat.com/blog/how-to-use-entitled-image-builds-to-build-drivercontainers-with-ubi-on-openshift[using an entitled build].

[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: kmm-ci-dockerfile
data:
  dockerfile: |
    ARG DTK_AUTO
    FROM ${DTK_AUTO} as builder
    ARG KERNEL_VERSION
    WORKDIR /usr/src
    RUN ["git", "clone", "https://github.com/rh-ecosystem-edge/kernel-module-management.git"]
    WORKDIR /usr/src/kernel-module-management/ci/kmm-kmod
    RUN KERNEL_SRC_DIR=/lib/modules/${KERNEL_VERSION}/build make all
    FROM registry.redhat.io/ubi9/ubi-minimal
    ARG KERNEL_VERSION
    RUN microdnf install kmod
    COPY --from=builder /usr/src/kernel-module-management/ci/kmm-kmod/kmm_ci_a.ko /opt/lib/modules/${KERNEL_VERSION}/
    COPY --from=builder /usr/src/kernel-module-management/ci/kmm-kmod/kmm_ci_b.ko /opt/lib/modules/${KERNEL_VERSION}/
    RUN depmod -b /opt ${KERNEL_VERSION}
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../hardware_enablement/psap-driver-toolkit.adoc#driver-toolkit[Driver Toolkit].

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-building-in-cluster_{context}"]

= Building in the cluster

KMM can build module loader images in the cluster. Follow these guidelines:

* Provide build instructions using the `build` section of a kernel mapping.
* Copy the `Dockerfile` for your container image into a `ConfigMap` resource, under the `dockerfile` key.
* Ensure that the `ConfigMap` is located in the same namespace as the `Module`.

KMM checks if the image name specified in the `containerImage` field exists. If it does, the build is skipped.

Otherwise, KMM creates a `Build` resource to build your image. After the image is built, KMM proceeds with the `Module` reconciliation. See the following example.

[source,yaml]
----
# ...
- regexp: '^.+$'
  containerImage: "some.registry/org/<my_kmod>:${KERNEL_FULL_VERSION}"
  build:
    buildArgs:  <1>
      - name: ARG_NAME
        value: <some_value>
    secrets: <2>
      - name: <some_kubernetes_secret> <3>
    baseImageRegistryTLS:
      insecure: false <4>
      insecureSkipTLSVerify: false <5>
    dockerfileConfigMap:  <6>
      name: <my_kmod_dockerfile>
  registryTLS:
    insecure: false <7>
    insecureSkipTLSVerify: false <8>
----
<1> Optional.
<2> Optional.
<3> Will be mounted in the build pod as `/run/secrets/some-kubernetes-secret`.
<4> Optional: Avoid using this parameter. If set to `true`, the build will be allowed to pull the image in the Dockerfile `FROM` instruction using plain HTTP.
<5> Optional: Avoid using this parameter. If set to `true`, the build will skip any TLS server certificate validation when pulling the image in the Dockerfile `FROM` instruction using plain HTTP.
<6> Required.
<7> Optional: Avoid using this parameter. If set to `true`, KMM will be allowed to check if the container image already exists using plain HTTP.
<8> Optional: Avoid using this parameter. If set to `true`, KMM will skip any TLS server certificate validation when checking if the container image already exists.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../cicd/builds/build-configuration.adoc#build-configuration[Build configuration resources].

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-using-driver-toolkit_{context}"]

= Using the Driver Toolkit

The Driver Toolkit (DTK) is a convenient base image for building build module loader images.
It contains tools and libraries for the OpenShift version currently running in the cluster.

.Procedure

Use DTK as the first stage of a multi-stage `Dockerfile`.

. Build the kernel modules.

. Copy the `.ko` files into a smaller end-user image such as https://catalog.redhat.com/software/containers/ubi9/ubi-minimal[`ubi-minimal`].

. To leverage DTK in your in-cluster build, use the `DTK_AUTO` build argument.
The value is automatically set by KMM when creating the `Build` resource. See the following example.
+
[source,dockerfile]
----
ARG DTK_AUTO
FROM ${DTK_AUTO} as builder
ARG KERNEL_VERSION
WORKDIR /usr/src
RUN ["git", "clone", "https://github.com/rh-ecosystem-edge/kernel-module-management.git"]
WORKDIR /usr/src/kernel-module-management/ci/kmm-kmod
RUN KERNEL_SRC_DIR=/lib/modules/${KERNEL_VERSION}/build make all
FROM registry.redhat.io/ubi9/ubi-minimal
ARG KERNEL_VERSION
RUN microdnf install kmod
COPY --from=builder /usr/src/kernel-module-management/ci/kmm-kmod/kmm_ci_a.ko /opt/lib/modules/${KERNEL_VERSION}/
COPY --from=builder /usr/src/kernel-module-management/ci/kmm-kmod/kmm_ci_b.ko /opt/lib/modules/${KERNEL_VERSION}/
RUN depmod -b /opt ${KERNEL_VERSION}
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../hardware_enablement/psap-driver-toolkit.adoc#driver-toolkit[Driver Toolkit].

//Deploying kernel modules (Might just leave this short intro in the assembly and put further module below it)
//    * Running ModuleLoader images (CONCEPT, or could be included in the assembly with the intro)
//    * Using the device plugin (CONCEPT, or could be included in the assembly with the intro)
//  * Creating the Module Custom Resource (PROCEDURE? Seems like not a process the user does after reading it. Maybe a REFERENCE)
//  * Security and permissions (CONCEPT or REFERENCE)
//    * ServiceAccounts and SecurityContextConstraints (can include in Security and permissions)
//    * Pod Security Standards (can include in Security and permissions)
//  * Example Module CR (REFERENCE)

// Added for TELCODOCS-1065
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-using-signing-with-kmm_{context}"]
= Using signing with Kernel Module Management (KMM)

On a Secure Boot enabled system, all kernel modules (kmods) must be signed with a public/private key-pair enrolled into the Machine Owner's Key (MOK) database. Drivers distributed as part of a distribution should already be signed by the distribution's private key, but for kernel modules build out-of-tree, KMM supports signing kernel modules using the `sign` section of the kernel mapping.

For more details on using Secure Boot, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/signing-a-kernel-and-modules-for-secure-boot_managing-monitoring-and-updating-the-kernel#generating-a-public-and-private-key-pair_signing-a-kernel-and-modules-for-secure-boot[Generating a public and private key pair]

.Prerequisites

* A public private key pair in the correct (DER) format.
* At least one secure-boot enabled node with the public key enrolled in its MOK database.
* Either a pre-built driver container image, or the source code and `Dockerfile` needed to build one in-cluster.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-adding-the-keys-for-secureboot_{context}"]
= Adding the keys for secureboot

To use KMM Kernel Module Management (KMM) to sign kernel modules, a certificate and private key are required. For details on how to create these, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/signing-a-kernel-and-modules-for-secure-boot_managing-monitoring-and-updating-the-kernel#generating-a-public-and-private-key-pair_signing-a-kernel-and-modules-for-secure-boot[Generating a public and private key pair].

For details on how to extract the public and private key pair, see link:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/signing-a-kernel-and-modules-for-secure-boot_managing-monitoring-and-updating-the-kernel#signing-kernel-modules-with-the-private-key_signing-a-kernel-and-modules-for-secure-boot[Signing kernel modules with the private key]. Use steps 1 through 4 to extract the keys into files.

.Procedure

. Create the `sb_cert.cer` file that contains the certificate and the `sb_cert.priv` file that contains the private key:
+
[source,terminal]
----
$ openssl req -x509 -new -nodes -utf8 -sha256 -days 36500 -batch -config configuration_file.config -outform DER -out my_signing_key_pub.der -keyout my_signing_key.priv
----

. Add the files by using one of the following methods:
+
* Add the files as link:https://kubernetes.io/docs/concepts/configuration/secret/[secrets] directly:
+
[source,terminal]
----
$ oc create secret generic my-signing-key --from-file=key=<my_signing_key.priv>
----
+
[source,terminal]
----
$ oc create secret generic my-signing-key-pub --from-file=key=<my_signing_key_pub.der>
----
+
* Add the files by base64 encoding them:
+
[source,terminal]
----
$ cat sb_cert.priv | base64 -w 0 > my_signing_key2.base64
----
+
[source,terminal]
----
$ cat sb_cert.cer | base64 -w 0 > my_signing_key_pub.base64
----

. Add the encoded text to a YAML file:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: my-signing-key-pub
  namespace: default <1>
type: Opaque
data:
  cert: <base64_encoded_secureboot_public_key>

---
apiVersion: v1
kind: Secret
metadata:
  name: my-signing-key
  namespace: default <1>
type: Opaque
data:
  key: <base64_encoded_secureboot_private_key>
----
<1> `namespace` - Replace `default` with a valid namespace.

. Apply the YAML file:
+
[source,terminal]
----
$ oc apply -f <yaml_filename>
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-checking-the-keys_{context}"]
= Checking the keys

After you have added the keys, you must check them to ensure they are set correctly.

.Procedure

. Check to ensure the public key secret is set correctly:
+
[source,terminal]
----
$ oc get secret -o yaml <certificate secret name> | awk '/cert/{print $2; exit}' | base64 -d  | openssl x509 -inform der -text
----
+
This should display a certificate with a Serial Number, Issuer, Subject, and more.

. Check to ensure the private key secret is set correctly:
+
[source,terminal]
----
$ oc get secret -o yaml <private key secret name> | awk '/key/{print $2; exit}' | base64 -d
----
+
This should display the key enclosed in the `-----BEGIN PRIVATE KEY-----` and `-----END PRIVATE KEY-----` lines.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-signing-a-prebuilt-driver-container_{context}"]
= Signing a pre-built driver container

Use this procedure if you have a pre-built image, such as an image either distributed by a hardware vendor or built elsewhere.

The following YAML file adds the public/private key-pair as secrets with the required key names - `key` for the private key, `cert` for the public key. The cluster then pulls down the `unsignedImage` image, opens it, signs the kernel modules listed in `filesToSign`, adds them back, and pushes the resulting image as `containerImage`.


Kernel Module Management (KMM) should then deploy the DaemonSet that loads the signed kmods onto all the nodes that match the selector. The driver containers should run successfully on any nodes that have the public key in their MOK database, and any nodes that are not secure-boot enabled, which ignore the signature. They should fail to load on any that have secure-boot enabled but do not have that key in their MOK database.

.Prerequisites

* The `keySecret` and `certSecret` secrets have been created.

.Procedure

. Apply the YAML file:
+
[source,yaml]
----
---
apiVersion: kmm.sigs.x-k8s.io/v1beta1
kind: Module
metadata:
  name: example-module
spec:
  moduleLoader:
    serviceAccountName: default
    container:
      modprobe: <1>
        moduleName: '<your module name>'
      kernelMappings:
        # the kmods will be deployed on all nodes in the cluster with a kernel that matches the regexp
        - regexp: '^.*\.x86_64$'
          # the container to produce containing the signed kmods
          containerImage: <image name e.g. quay.io/myuser/my-driver:<kernelversion>-signed>
          sign:
            # the image containing the unsigned kmods (we need this because we are not building the kmods within the cluster)
            unsignedImage: <image name e.g. quay.io/myuser/my-driver:<kernelversion> >
            keySecret: # a secret holding the private secureboot key with the key 'key'
              name: <private key secret name>
            certSecret: # a secret holding the public secureboot key with the key 'cert'
              name: <certificate secret name>
            filesToSign: # full path within the unsignedImage container to the kmod(s) to sign
              - /opt/lib/modules/4.18.0-348.2.1.el8_5.x86_64/kmm_ci_a.ko
  imageRepoSecret:
    # the name of a secret containing credentials to pull unsignedImage and push containerImage to the registry
    name: repo-pull-secret
  selector:
    kubernetes.io/arch: amd64
----

<1> `modprobe` - The name of the kmod to load.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-building-and-signing-a-moduleloader-container-image_{context}"]
= Building and signing a ModuleLoader container image

Use this procedure if you have source code and must build your image first.

The following YAML file builds a new container image using the source code from the repository. The image produced is saved back in the registry with a temporary name, and this temporary image is then signed using the parameters in the `sign` section.

The temporary image name is based on the final image name and is set to be `<containerImage>:<tag>-<namespace>_<module name>_kmm_unsigned`.

For example, using the following YAML file, Kernel Module Management (KMM) builds an image named `example.org/repository/minimal-driver:final-default_example-module_kmm_unsigned` containing the build with unsigned kmods and push it to the registry. Then it creates a second image named `example.org/repository/minimal-driver:final` that contains the signed kmods. It is this second image that is loaded by the `DaemonSet` object and deploys the kmods to the cluster nodes.

After it is signed, the temporary image can be safely deleted from the registry. It will be rebuilt, if needed.

.Prerequisites

* The `keySecret` and `certSecret` secrets have been created.

.Procedure

. Apply the YAML file:
+
[source,yaml]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: example-module-dockerfile
  namespace: default <1>
data:
  Dockerfile: |
    ARG DTK_AUTO
    ARG KERNEL_VERSION
    FROM ${DTK_AUTO} as builder
    WORKDIR /build/
    RUN git clone -b main --single-branch https://github.com/rh-ecosystem-edge/kernel-module-management.git
    WORKDIR kernel-module-management/ci/kmm-kmod/
    RUN make
    FROM registry.access.redhat.com/ubi9/ubi:latest
    ARG KERNEL_VERSION
    RUN yum -y install kmod && yum clean all
    RUN mkdir -p /opt/lib/modules/${KERNEL_VERSION}
    COPY --from=builder /build/kernel-module-management/ci/kmm-kmod/*.ko /opt/lib/modules/${KERNEL_VERSION}/
    RUN /usr/sbin/depmod -b /opt
---
apiVersion: kmm.sigs.x-k8s.io/v1beta1
kind: Module
metadata:
  name: example-module
  namespace: default <1>
spec:
  moduleLoader:
    serviceAccountName: default <2>
    container:
      modprobe:
        moduleName: simple_kmod
      kernelMappings:
        - regexp: '^.*\.x86_64$'
          containerImage: < the name of the final driver container to produce>
          build:
            dockerfileConfigMap:
              name: example-module-dockerfile
          sign:
            keySecret:
              name: <private key secret name>
            certSecret:
              name: <certificate secret name>
            filesToSign:
              - /opt/lib/modules/4.18.0-348.2.1.el8_5.x86_64/kmm_ci_a.ko
  imageRepoSecret: <3>
    name: repo-pull-secret
  selector: # top-level selector
    kubernetes.io/arch: amd64
----

<1> `namespace` - Replace `default` with a valid namespace.

<2> `serviceAccountName` - The default `serviceAccountName` does not have the required permissions to run a module that is privileged. For information on creating a service account, see "Creating service accounts" in the "Additional resources" of this section.

<3> `imageRepoSecret` - Used as `imagePullSecrets` in the `DaemonSet` object and to pull and push for the build and sign features.

:leveloffset!:
[role="_additional-resources"]
.Additional resources

* xref:../authentication/understanding-and-creating-service-accounts.adoc#service-accounts-managing_understanding-service-accounts[Creating service accounts].

// Added for TELCODOCS-1277
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-customizing-upgrades-for-kernel-modules_{context}"]
= Customizing upgrades for kernel modules

Use this procedure to upgrade the kernel module while running maintenance operations on the node, including rebooting the node, if needed. To minimize the impact on the workloads running in the cluster, run the kernel upgrade process sequentially, one node at a time.

[NOTE]
====
This procedure requires knowledge of the workload utilizing the kernel module and must be managed by the cluster administrator.
====


.Prerequisites

* Before upgrading, set the `kmm.node.kubernetes.io/version-module.<module_namespace>.<module_name>=$moduleVersion` label on all the nodes that are used by the kernel module.

* Terminate all user application workloads on the node or move them to another node.

* Unload the currently loaded kernel module.

* Ensure that the user workload (the application running in the cluster that is accessing kernel module) is not running on the node prior to kernel module unloading and that the workload is back running on the node after the new kernel module version has been loaded.

.Procedure

. Ensure that the device plugin managed by KMM on the node is unloaded.

. Update the following fields in the `Module` custom resource (CR):
- `containerImage` (to the appropriate kernel version)
- `version`
+
The update should be atomic; that is, both the `containerImage` and `version` fields must be updated simultaneously.

. Terminate any workload using the kernel module on the node being upgraded.

. Remove the `kmm.node.kubernetes.io/version-module.<module_namespace>.<module_name>` label on the node.
Run the following command to unload the kernel module from the node:
+
[source,terminal]
----
$ oc label node/<node_name> kmm.node.kubernetes.io/version-module.<module_namespace>.<module_name>-
----

. If required, as the cluster administrator, perform any additional maintenance required on the node for the kernel module upgrade.
+
If no additional upgrading is needed, you can skip Steps 3 through 6 by updating the `kmm.node.kubernetes.io/version-module.<module-namespace>.<module-name>` label value to the new `$moduleVersion` as set in the `Module`.

. Run the following command to add the `kmm.node.kubernetes.io/version-module.<module_namespace>.<module_name>=$moduleVersion` label to the node. The `$moduleVersion` must be equal to the new value of the `version` field in the `Module` CR.
+
[source,terminal]
----
$ oc label node/<node_name> kmm.node.kubernetes.io/version-module.<module_namespace>.<module_name>=<desired_version>
----
+
[NOTE]
====
Because of Kubernetes limitations in label names, the combined length of `Module` name and namespace must not exceed 39 characters.
====

. Restore any workload that leverages the kernel module on the node.

. Reload the device plugin managed by KMM on the node.

:leveloffset!:

// Added for TELCODOCS-1278
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-day1-kernel-module-loading_{context}"]
= Day 1 kernel module loading

Kernel Module Management (KMM) is typically a Day 2 Operator. Kernel modules are loaded only after the complete initialization of a Linux (RHCOS) server. However, in some scenarios the kernel module must be loaded at an earlier stage. Day 1 functionality allows you to use the Machine Config Operator (MCO) to load kernel modules during the Linux `systemd` initialization stage.

:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../post_installation_configuration/machine-configuration-tasks.adoc#machine-config-operator_post-install-machine-configuration-tasks[Machine Config Operator]

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-day1-supported-use-cases_{context}"]
= Day 1 supported use cases

The Day 1 functionality supports a limited number of use cases. The main use case is to allow loading out-of-tree (OOT) kernel modules prior to NetworkManager service initialization. It does not support loading kernel module at the `initramfs` stage.

The following are the conditions needed for Day 1 functionality:

* The kernel module is not loaded in the kernel.

* The in-tree kernel module is loaded into the kernel, but can be unloaded and replaced by the OOT kernel module. This means that the in-tree module is not referenced by any other kernel modules.

* In order for Day 1 functionlity to work, the node must have a functional network interface, that is, an in-tree kernel driver for that interface. The OOT kernel module can be a network driver that will replace the functional network driver.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-day1-oot-kernel-module-loading-flow_{context}"]
= OOT kernel module loading flow

The loading of the out-of-tree (OOT) kernel module leverages the Machine Config Operator (MCO). The flow sequence is as follows:

.Procedure

. Apply a `MachineConfig` resource to the existing running cluster. In order to identify the necessary nodes that need to be updated,
you must create an appropriate `MachineConfigPool` resource.

. MCO applies the reboots node by node. On any rebooted node, two new `systemd` services are deployed: `pull` service and `load` service.

. The `load` service is configured to run prior to the `NetworkConfiguration` service. The service tries to pull a predefined kernel module image and then, using that image, to unload an in-tree module and load an OOT kernel module.

. The `pull` service is configured to run after NetworkManager service. The service checks if the preconfigured kernel module image is located on the node's filesystem. If it is, the service exists normally, and the server continues with the boot process. If not, it pulls the image onto the node and reboots the node afterwards.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-day1-kernel-module-image_{context}"]
= The kernel module image

The Day 1 functionality uses the same DTK based image leveraged by Day 2 KMM builds. The out-of-tree kernel module should be located under `/opt/lib/modules/${kernelVersion}`.

:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../hardware_enablement/psap-driver-toolkit.adoc#driver-toolkit[Driver Toolkit]

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-day1-in-tree-module-replacement_{context}"]
= In-tree module replacement

The Day 1 functionality always tries to replace the in-tree kernel module with the OOT version. If the in-tree kernel module is not loaded, the flow is not affected; the service proceeds and loads the OOT kernel module.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-day1-mco-yaml-creation_{context}"]
= MCO yaml creation

KMM provides an API to create an MCO YAML manifest for the Day 1 functionality:

[source,console]
----
ProduceMachineConfig(machineConfigName, machineConfigPoolRef, kernelModuleImage, kernelModuleName string) (string, error)
----

The returned output is a string representation of the MCO YAML manifest to be applied. It is up to the customer to apply this YAML.

The parameters are:

`machineConfigName`:: The name of the MCO YAML manifest. This parameter is set as the `name` parameter of the metadata of the MCO YAML manifest.

`machineConfigPoolRef`:: The `MachineConfigPool` name used to identify the targeted nodes.

`kernelModuleImage`:: The name of the container image that includes the OOT kernel module.

`kernelModuleName`:: The name of the OOT kernel module. This parameter is used both to unload the in-tree kernel module (if loaded into the kernel) and to load the OOT kernel module.

The API is located under `pkg/mcproducer` package of the KMM source code. The KMM operator does not need to be running to use the Day 1 functionality. You only need to import the `pkg/mcproducer` package into their operator/utility code, call the API, and apply the produced MCO YAML to the cluster.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-day1-machineconfigpool_{context}"]
= The MachineConfigPool

The `MachineConfigPool` identifies a collection of nodes that are affected by the applied MCO.

[source,yaml]
----
kind: MachineConfigPool
metadata:
  name: sfc
spec:
  machineConfigSelector: <1>
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker, sfc]}
  nodeSelector: <2>
    matchLabels:
      node-role.kubernetes.io/sfc: ""
  paused: false
  maxUnavailable: 1
----
<1> Matches the labels in the MachineConfig.
<2> Matches the labels on the node.

There are predefined `MachineConfigPools` in the OCP cluster:

* `worker`: Targets all worker nodes in the cluster

* `master`: Targets all master nodes in the cluster

Define the following `MachineConfig` to target the master `MachineConfigPool`:

[source,yaml]
----
metadata:
  labels:
    machineconfiguration.opensfhit.io/role: master
----


Define the following `MachineConfig` to target the worker `MachineConfigPool`:

[source,yaml]
----
metadata:
  labels:
    machineconfiguration.opensfhit.io/role: worker
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* link:https://www.redhat.com/en/blog/openshift-container-platform-4-how-does-machine-config-pool-work[About MachineConfigPool]

:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-debugging-and-troubleshooting_{context}"]
= Debugging and troubleshooting

If the kmods in your driver container are not signed or are signed with the wrong key, then the container can enter a `PostStartHookError` or `CrashLoopBackOff` status. You can verify by running the `oc describe` command on your container, which displays the following message in this scenario:

[source,terminal]
----
modprobe: ERROR: could not insert '<your_kmod_name>': Required key not available
----

:leveloffset!:

// Added for TELCODOCS-1067
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-firmware-support_{context}"]
= KMM firmware support

Kernel modules sometimes need to load firmware files from the file system. KMM supports copying firmware files from the ModuleLoader image to the node's file system.

The contents of `.spec.moduleLoader.container.modprobe.firmwarePath` are copied into the `/var/lib/firmware` path on the node before running the `modprobe` command to insert the kernel module.

All files and empty directories are removed from that location before running the `modprobe -r` command to unload the kernel module, when the pod is terminated.

:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../hardware_enablement/kmm-kernel-module-management.adoc#kmm-creating-moduleloader-image_kernel-module-management-operator[Creating a ModuleLoader image].

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-configuring-the-lookup-path-on-nodes_{context}"]
= Configuring the lookup path on nodes

On {product-title} nodes, the set of default lookup paths for firmwares does not include the `/var/lib/firmware` path.

.Procedure

. Use the Machine Config Operator to create a `MachineConfig` custom resource (CR) that contains the `/var/lib/firmware` path:
+
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker <1>
  name: 99-worker-kernel-args-firmware-path
spec:
  kernelArguments:
    - 'firmware_class.path=/var/lib/firmware'
----
<1> You can configure the label based on your needs. In the case of {sno}, use either `control-pane` or `master` objects.


. By applying the `MachineConfig` CR, the nodes are automatically rebooted.

:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../post_installation_configuration/machine-configuration-tasks.adoc#understanding-the-machine-config-operator[Machine Config Operator].

:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-building-a-moduleloader-image_{context}"]
= Building a ModuleLoader image

.Procedure

* In addition to building the kernel module itself, include the binary firmware in the builder image:
+
[source,dockerfile]
----
FROM registry.redhat.io/ubi9/ubi-minimal as builder

# Build the kmod

RUN ["mkdir", "/firmware"]
RUN ["curl", "-o", "/firmware/firmware.bin", "https://artifacts.example.com/firmware.bin"]

FROM registry.redhat.io/ubi9/ubi-minimal

# Copy the kmod, install modprobe, run depmod

COPY --from=builder /firmware /firmware
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-tuning-the-module-resource_{context}"]
= Tuning the Module resource

.Procedure

* Set `.spec.moduleLoader.container.modprobe.firmwarePath` in the `Module` custom resource (CR):
+
[source,yaml]
----
apiVersion: kmm.sigs.x-k8s.io/v1beta1
kind: Module
metadata:
  name: my-kmod
spec:
  moduleLoader:
    container:
      modprobe:
        moduleName: my-kmod  # Required

        firmwarePath: /firmware <1>
----
<1> Optional: Copies `/firmware/*` into `/var/lib/firmware/` on the node.

:leveloffset!:

// Added for TELCODOCS-1059
:leveloffset: +1

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: CONCEPT
[id="kmm-troubleshooting_{context}"]
= Troubleshooting KMM

When troubleshooting KMM installation issues, you can monitor logs to determine at which stage issues occur.
Then, retrieve diagnostic data relevant to that stage.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-must-gather-tool_{context}"]
= Using the must-gather tool

The `oc adm must-gather` command is the preferred way to collect a support bundle and provide debugging information to Red Hat
Support. Collect specific information by running the command with the appropriate arguments as described in the following sections.

:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../support/gathering-cluster-data.adoc#about-must-gather_gathering-cluster-data[About the must-gather tool]

:leveloffset: +3

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-gathering-data-for-kmm_{context}"]
= Gathering data for KMM

.Procedure

. Gather the data for the KMM Operator controller manager:

.. Set the `MUST_GATHER_IMAGE` variable:
+
[source,terminal]
----
$ export MUST_GATHER_IMAGE=$(oc get deployment -n openshift-kmm kmm-operator-controller-manager -ojsonpath='{.spec.template.spec.containers[?(@.name=="manager")].env[?(@.name=="RELATED_IMAGES_MUST_GATHER")].value}')
----
+
[NOTE]
====
Use the `-n <namespace>` switch to specify a namespace if you installed KMM in a custom namespace.
====

.. Run the `must-gather` tool:
+
[source,terminal]
----
$ oc adm must-gather --image="${MUST_GATHER_IMAGE}" -- /usr/bin/gather
----

. View the Operator logs:
+
[source,terminal]
----
$ oc logs -fn openshift-kmm deployments/kmm-operator-controller-manager
----
+
.Example output
[%collapsible]
====
[source,terminal]
----
I0228 09:36:37.352405       1 request.go:682] Waited for 1.001998746s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/machine.openshift.io/v1beta1?timeout=32s
I0228 09:36:40.767060       1 listener.go:44] kmm/controller-runtime/metrics "msg"="Metrics server is starting to listen" "addr"="127.0.0.1:8080"
I0228 09:36:40.769483       1 main.go:234] kmm/setup "msg"="starting manager"
I0228 09:36:40.769907       1 internal.go:366] kmm "msg"="Starting server" "addr"={"IP":"127.0.0.1","Port":8080,"Zone":""} "kind"="metrics" "path"="/metrics"
I0228 09:36:40.770025       1 internal.go:366] kmm "msg"="Starting server" "addr"={"IP":"::","Port":8081,"Zone":""} "kind"="health probe"
I0228 09:36:40.770128       1 leaderelection.go:248] attempting to acquire leader lease openshift-kmm/kmm.sigs.x-k8s.io...
I0228 09:36:40.784396       1 leaderelection.go:258] successfully acquired lease openshift-kmm/kmm.sigs.x-k8s.io
I0228 09:36:40.784876       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="Module" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="Module" "source"="kind source: *v1beta1.Module"
I0228 09:36:40.784925       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="Module" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="Module" "source"="kind source: *v1.DaemonSet"
I0228 09:36:40.784968       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="Module" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="Module" "source"="kind source: *v1.Build"
I0228 09:36:40.785001       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="Module" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="Module" "source"="kind source: *v1.Job"
I0228 09:36:40.785025       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="Module" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="Module" "source"="kind source: *v1.Node"
I0228 09:36:40.785039       1 controller.go:193] kmm "msg"="Starting Controller" "controller"="Module" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="Module"
I0228 09:36:40.785458       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="PodNodeModule" "controllerGroup"="" "controllerKind"="Pod" "source"="kind source: *v1.Pod"
I0228 09:36:40.786947       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="PreflightValidation" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidation" "source"="kind source: *v1beta1.PreflightValidation"
I0228 09:36:40.787406       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="PreflightValidation" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidation" "source"="kind source: *v1.Build"
I0228 09:36:40.787474       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="PreflightValidation" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidation" "source"="kind source: *v1.Job"
I0228 09:36:40.787488       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="PreflightValidation" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidation" "source"="kind source: *v1beta1.Module"
I0228 09:36:40.787603       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="NodeKernel" "controllerGroup"="" "controllerKind"="Node" "source"="kind source: *v1.Node"
I0228 09:36:40.787634       1 controller.go:193] kmm "msg"="Starting Controller" "controller"="NodeKernel" "controllerGroup"="" "controllerKind"="Node"
I0228 09:36:40.787680       1 controller.go:193] kmm "msg"="Starting Controller" "controller"="PreflightValidation" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidation"
I0228 09:36:40.785607       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="imagestream" "controllerGroup"="image.openshift.io" "controllerKind"="ImageStream" "source"="kind source: *v1.ImageStream"
I0228 09:36:40.787822       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="preflightvalidationocp" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidationOCP" "source"="kind source: *v1beta1.PreflightValidationOCP"
I0228 09:36:40.787853       1 controller.go:193] kmm "msg"="Starting Controller" "controller"="imagestream" "controllerGroup"="image.openshift.io" "controllerKind"="ImageStream"
I0228 09:36:40.787879       1 controller.go:185] kmm "msg"="Starting EventSource" "controller"="preflightvalidationocp" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidationOCP" "source"="kind source: *v1beta1.PreflightValidation"
I0228 09:36:40.787905       1 controller.go:193] kmm "msg"="Starting Controller" "controller"="preflightvalidationocp" "controllerGroup"="kmm.sigs.x-k8s.io" "controllerKind"="PreflightValidationOCP"
I0228 09:36:40.786489       1 controller.go:193] kmm "msg"="Starting Controller" "controller"="PodNodeModule" "controllerGroup"="" "controllerKind"="Pod"
----
====

:leveloffset!:
:leveloffset: +3

// Module included in the following assemblies:
//
// * hardware_enablement/kmm-kernel-module-management.adoc

:_mod-docs-content-type: PROCEDURE
[id="kmm-gathering-data-for-kmm-hub_{context}"]
= Gathering data for KMM-Hub

.Procedure

. Gather the data for the KMM Operator hub controller manager:

.. Set the `MUST_GATHER_IMAGE` variable:
+
[source,terminal]
----
$ export MUST_GATHER_IMAGE=$(oc get deployment -n openshift-kmm-hub kmm-operator-hub-controller-manager -ojsonpath='{.spec.template.spec.containers[?(@.name=="manager")].env[?(@.name=="RELATED_IMAGES_MUST_GATHER")].value}')
----
+
[NOTE]
====
Use the `-n <namespace>` switch to specify a namespace if you installed KMM in a custom namespace.
====

.. Run the `must-gather` tool:
+
[source,terminal]
----
$ oc adm must-gather --image="${MUST_GATHER_IMAGE}" -- /usr/bin/gather -u
----

. View the Operator logs:
+
[source,terminal]
----
$ oc logs -fn openshift-kmm-hub deployments/kmm-operator-hub-controller-manager
----
+
.Example output
[%collapsible]
====
[source,terminal]
----
I0417 11:34:08.807472       1 request.go:682] Waited for 1.023403273s due to client-side throttling, not priority and fairness, request: GET:https://172.30.0.1:443/apis/tuned.openshift.io/v1?timeout=32s
I0417 11:34:12.373413       1 listener.go:44] kmm-hub/controller-runtime/metrics "msg"="Metrics server is starting to listen" "addr"="127.0.0.1:8080"
I0417 11:34:12.376253       1 main.go:150] kmm-hub/setup "msg"="Adding controller" "name"="ManagedClusterModule"
I0417 11:34:12.376621       1 main.go:186] kmm-hub/setup "msg"="starting manager"
I0417 11:34:12.377690       1 leaderelection.go:248] attempting to acquire leader lease openshift-kmm-hub/kmm-hub.sigs.x-k8s.io...
I0417 11:34:12.378078       1 internal.go:366] kmm-hub "msg"="Starting server" "addr"={"IP":"127.0.0.1","Port":8080,"Zone":""} "kind"="metrics" "path"="/metrics"
I0417 11:34:12.378222       1 internal.go:366] kmm-hub "msg"="Starting server" "addr"={"IP":"::","Port":8081,"Zone":""} "kind"="health probe"
I0417 11:34:12.395703       1 leaderelection.go:258] successfully acquired lease openshift-kmm-hub/kmm-hub.sigs.x-k8s.io
I0417 11:34:12.396334       1 controller.go:185] kmm-hub "msg"="Starting EventSource" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule" "source"="kind source: *v1beta1.ManagedClusterModule"
I0417 11:34:12.396403       1 controller.go:185] kmm-hub "msg"="Starting EventSource" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule" "source"="kind source: *v1.ManifestWork"
I0417 11:34:12.396430       1 controller.go:185] kmm-hub "msg"="Starting EventSource" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule" "source"="kind source: *v1.Build"
I0417 11:34:12.396469       1 controller.go:185] kmm-hub "msg"="Starting EventSource" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule" "source"="kind source: *v1.Job"
I0417 11:34:12.396522       1 controller.go:185] kmm-hub "msg"="Starting EventSource" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule" "source"="kind source: *v1.ManagedCluster"
I0417 11:34:12.396543       1 controller.go:193] kmm-hub "msg"="Starting Controller" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule"
I0417 11:34:12.397175       1 controller.go:185] kmm-hub "msg"="Starting EventSource" "controller"="imagestream" "controllerGroup"="image.openshift.io" "controllerKind"="ImageStream" "source"="kind source: *v1.ImageStream"
I0417 11:34:12.397221       1 controller.go:193] kmm-hub "msg"="Starting Controller" "controller"="imagestream" "controllerGroup"="image.openshift.io" "controllerKind"="ImageStream"
I0417 11:34:12.498335       1 filter.go:196] kmm-hub "msg"="Listing all ManagedClusterModules" "managedcluster"="local-cluster"
I0417 11:34:12.498570       1 filter.go:205] kmm-hub "msg"="Listed ManagedClusterModules" "count"=0 "managedcluster"="local-cluster"
I0417 11:34:12.498629       1 filter.go:238] kmm-hub "msg"="Adding reconciliation requests" "count"=0 "managedcluster"="local-cluster"
I0417 11:34:12.498687       1 filter.go:196] kmm-hub "msg"="Listing all ManagedClusterModules" "managedcluster"="sno1-0"
I0417 11:34:12.498750       1 filter.go:205] kmm-hub "msg"="Listed ManagedClusterModules" "count"=0 "managedcluster"="sno1-0"
I0417 11:34:12.498801       1 filter.go:238] kmm-hub "msg"="Adding reconciliation requests" "count"=0 "managedcluster"="sno1-0"
I0417 11:34:12.501947       1 controller.go:227] kmm-hub "msg"="Starting workers" "controller"="imagestream" "controllerGroup"="image.openshift.io" "controllerKind"="ImageStream" "worker count"=1
I0417 11:34:12.501948       1 controller.go:227] kmm-hub "msg"="Starting workers" "controller"="ManagedClusterModule" "controllerGroup"="hub.kmm.sigs.x-k8s.io" "controllerKind"="ManagedClusterModule" "worker count"=1
I0417 11:34:12.502285       1 imagestream_reconciler.go:50] kmm-hub "msg"="registered imagestream info mapping" "ImageStream"={"name":"driver-toolkit","namespace":"openshift"} "controller"="imagestream" "controllerGroup"="image.openshift.io" "controllerKind"="ImageStream" "dtkImage"="quay.io/openshift-release-dev/ocp-v4.0-art-dev@sha256:df42b4785a7a662b30da53bdb0d206120cf4d24b45674227b16051ba4b7c3934" "name"="driver-toolkit" "namespace"="openshift" "osImageVersion"="412.86.202302211547-0" "reconcileID"="e709ff0a-5664-4007-8270-49b5dff8bae9"
----
====

:leveloffset!:

//# includes=_attributes/common-attributes,modules/kmm-about-kmm,modules/kmm-installation,modules/kmm-installing-using-web-console,modules/kmm-installing-using-cli,modules/kmm-installing-older-versions,modules/kmm-deploying-modules,modules/kmm-creating-module-cr,modules/kmm-setting-soft-dependencies-between-kernel-modules,modules/kmm-security,modules/kmm-replacing-in-tree-modules-with-out-of-tree-modules,modules/kmm-example-module-cr,modules/kmm-creating-moduleloader-image,modules/kmm-running-depmod,modules/kmm-building-in-cluster,modules/kmm-using-driver-toolkit,modules/kmm-using-signing-with-kmm,modules/kmm-adding-the-keys-for-secureboot,modules/kmm-checking-the-keys,modules/kmm-signing-a-prebuilt-driver-container,modules/kmm-building-and-signing-a-moduleloader-container-image,modules/kmm-customizing-upgrades-for-kernel-modules,modules/kmm-day1-kernel-module-loading,modules/kmm-day1-supported-use-cases,modules/kmm-day1-oot-kernel-module-loading-flow,modules/kmm-day1-kernel-module-image,modules/kmm-day1-in-tree-module-replacement,modules/kmm-day1-mco-yaml-creation,modules/kmm-day1-machineconfigpool,modules/kmm-debugging-and-troubleshooting,modules/kmm-firmware-support,modules/kmm-configuring-the-lookup-path-on-nodes,modules/kmm-building-a-moduleloader-image,modules/kmm-tuning-the-module-resource,modules/kmm-troubleshooting,modules/kmm-must-gather-tool,modules/kmm-gathering-data-for-kmm,modules/kmm-gathering-data-for-kmm-hub

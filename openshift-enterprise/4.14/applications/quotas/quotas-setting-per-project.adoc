:_mod-docs-content-type: ASSEMBLY
[id="quotas-setting-per-project"]
= Resource quotas per project
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: quotas-setting-per-project

toc::[]

A _resource quota_, defined by a `ResourceQuota` object, provides constraints that limit aggregate resource consumption per project. It can limit the quantity of objects that can be created in a project by type, as well as the total amount of compute resources and storage that might be consumed by resources in that project.

This guide describes how resource quotas work, how cluster administrators can set and manage resource quotas on a per project basis, and how developers and cluster administrators can view them.

:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

[id="quotas-resources-managed_{context}"]
= Resources managed by quotas

The following describes the set of compute resources and object types that can be managed by a quota.

[NOTE]
====
A pod is in a terminal state if `status.phase in (Failed, Succeeded)` is true.
====

.Compute resources managed by quota
[cols="3a,8a",options="header"]
|===

|Resource Name |Description

|`cpu`
|The sum of CPU requests across all pods in a non-terminal state cannot exceed this value. `cpu` and `requests.cpu` are the same value and can be used interchangeably.

|`memory`
|The sum of memory requests across all pods in a non-terminal state cannot exceed this value. `memory` and `requests.memory` are the same value and can be used interchangeably.

|`requests.cpu`
|The sum of CPU requests across all pods in a non-terminal state cannot exceed this value. `cpu` and `requests.cpu` are the same value and can be used interchangeably.

|`requests.memory`
|The sum of memory requests across all pods in a non-terminal state cannot exceed this value. `memory` and `requests.memory` are the same value and can be used interchangeably.

|`limits.cpu`
|The sum of CPU limits across all pods in a non-terminal state cannot exceed this value.

|`limits.memory`
|The sum of memory limits across all pods in a non-terminal state cannot exceed this value.

|===

.Storage resources managed by quota
[cols="3a,8a",options="header"]
|===

|Resource Name |Description

|`requests.storage`
|The sum of storage requests across all persistent volume claims in any state cannot exceed this value.

|`persistentvolumeclaims`
|The total number of persistent volume claims that can exist in the project.

|`<storage-class-name>.storageclass.storage.k8s.io/requests.storage`
|The sum of storage requests across all persistent volume claims in any state that have a matching storage class, cannot exceed this value.

|`<storage-class-name>.storageclass.storage.k8s.io/persistentvolumeclaims`
|The total number of persistent volume claims with a matching storage class that can exist in the project.

|`ephemeral-storage`
|The sum of local ephemeral storage requests across all pods in a non-terminal state cannot exceed this value. `ephemeral-storage` and `requests.ephemeral-storage` are the same value and can be used interchangeably.

|`requests.ephemeral-storage`
|The sum of ephemeral storage requests across all pods in a non-terminal state cannot exceed this value. `ephemeral-storage` and `requests.ephemeral-storage` are the same value and can be used interchangeably.

|`limits.ephemeral-storage`
|The sum of ephemeral storage limits across all pods in a non-terminal state cannot exceed this value.
|===

[id="quotas-object-counts-managed_{context}"]
.Object counts managed by quota
[cols="3a,8a",options="header"]
|===

|Resource Name |Description

|`pods`
|The total number of pods in a non-terminal state that can exist in the project.

|`replicationcontrollers`
|The total number of ReplicationControllers that can exist in the project.

|`resourcequotas`
|The total number of resource quotas that can exist in the project.

|`services`
|The total number of services that can exist in the project.

|`services.loadbalancers`
|The total number of services of type `LoadBalancer` that can exist in the project.

|`services.nodeports`
|The total number of services of type `NodePort` that can exist in the project.

|`secrets`
|The total number of secrets that can exist in the project.

|`configmaps`
|The total number of `ConfigMap` objects that can exist in the project.

|`persistentvolumeclaims`
|The total number of persistent volume claims that can exist in the project.

|`openshift.io/imagestreams`
|The total number of imagestreams that can exist in the project.
|===

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

[id="quotas-scopes_{context}"]
= Quota scopes

Each quota can have an associated set of _scopes_. A quota only measures usage
for a resource if it matches the intersection of enumerated scopes.

Adding a scope to a quota restricts the set of resources to which that quota can
apply. Specifying a resource outside of the allowed set results in a validation
error.

|===

|Scope |Description

|`BestEffort`
|Match pods that have best effort quality of service for either `cpu` or
`memory`.

|`NotBestEffort`
|Match pods that do not have best effort quality of service for `cpu` and
`memory`.
|===

A `BestEffort` scope restricts a quota to limiting the following resources:

- `pods`

A `NotBestEffort` scope restricts a quota to tracking the following resources:

- `pods`
- `memory`
- `requests.memory`
- `limits.memory`
- `cpu`
- `requests.cpu`
- `limits.cpu`

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

[id="quota-enforcement_{context}"]
= Quota enforcement

After a resource quota for a project is first created, the project restricts the
ability to create any new resources that may violate a quota constraint until it
has calculated updated usage statistics.

After a quota is created and usage statistics are updated, the project accepts
the creation of new content. When you create or modify resources, your quota
usage is incremented immediately upon the request to create or modify the
resource.

When you delete a resource, your quota use is decremented during the next full
recalculation of quota statistics for the project. A configurable amount of time
determines how long it takes to reduce quota usage statistics to their current
observed system value.

If project modifications exceed a quota usage limit, the server denies the
action, and an appropriate error message is returned to the user explaining the
quota constraint violated, and what their currently observed usage statistics
are in the system.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

[id="quotas-requests-vs-limits_{context}"]
= Requests versus limits

When allocating compute resources, each container might specify a request and a
limit value each for CPU, memory, and ephemeral storage. Quotas can restrict any
of these values.

If the quota has a value specified for `requests.cpu` or `requests.memory`,
then it requires that every incoming container make an explicit request for
those resources. If the quota has a value specified for `limits.cpu` or
`limits.memory`, then it requires that every incoming container specify an
explicit limit for those resources.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

[id="quotas-sample-resource-quota-definitions_{context}"]
= Sample resource quota definitions

.`core-object-counts.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: core-object-counts
spec:
  hard:
    configmaps: "10" <1>
    persistentvolumeclaims: "4" <2>
    replicationcontrollers: "20" <3>
    secrets: "10" <4>
    services: "10" <5>
    services.loadbalancers: "2" <6>
----
<1> The total number of `ConfigMap` objects that can exist in the project.
<2> The total number of persistent volume claims (PVCs) that can exist in the
project.
<3> The total number of replication controllers that can exist in the project.
<4> The total number of secrets that can exist in the project.
<5> The total number of services that can exist in the project.
<6> The total number of services of type `LoadBalancer` that can exist in the project.

.`openshift-object-counts.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: openshift-object-counts
spec:
  hard:
    openshift.io/imagestreams: "10" <1>
----
<1> The total number of image streams that can exist in the project.

.`compute-resources.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources
spec:
  hard:
    pods: "4" <1>
    requests.cpu: "1" <2>
    requests.memory: 1Gi <3>
    limits.cpu: "2" <4>
    limits.memory: 2Gi <5>

----
<1> The total number of pods in a non-terminal state that can exist in the project.
<2> Across all pods in a non-terminal state, the sum of CPU requests cannot exceed 1 core.
<3> Across all pods in a non-terminal state, the sum of memory requests cannot exceed 1Gi.
<4> Across all pods in a non-terminal state, the sum of CPU limits cannot exceed 2 cores.
<5> Across all pods in a non-terminal state, the sum of memory limits cannot exceed 2Gi.


.`besteffort.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: besteffort
spec:
  hard:
    pods: "1" <1>
  scopes:
  - BestEffort <2>
----
<1> The total number of pods in a non-terminal state with `BestEffort` quality of service that can exist in the project.
<2> Restricts the quota to only matching pods that have `BestEffort` quality of service for either memory or CPU.

.`compute-resources-long-running.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources-long-running
spec:
  hard:
    pods: "4" <1>
    limits.cpu: "4" <2>
    limits.memory: "2Gi" <3>
  scopes:
  - NotTerminating <4>
----
<1> The total number of pods in a non-terminal state.
<2> Across all pods in a non-terminal state, the sum of CPU limits cannot exceed this value.
<3> Across all pods in a non-terminal state, the sum of memory limits cannot exceed this value.
<4> Restricts the quota to only matching pods where `spec.activeDeadlineSeconds` is set to `nil`. Build pods fall under `NotTerminating` unless the `RestartNever` policy is applied.

.`compute-resources-time-bound.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: compute-resources-time-bound
spec:
  hard:
    pods: "2" <1>
    limits.cpu: "1" <2>
    limits.memory: "1Gi" <3>
  scopes:
  - Terminating <4>
----
<1> The total number of pods in a terminating state.
<2> Across all pods in a terminating state, the sum of CPU limits cannot exceed this value.
<3> Across all pods in a terminating state, the sum of memory limits cannot exceed this value.
<4> Restricts the quota to only matching pods where `spec.activeDeadlineSeconds >=0`. For example, this quota charges for build or deployer pods, but not long running pods like a web server or database.

.`storage-consumption.yaml`
[source,yaml]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: storage-consumption
spec:
  hard:
    persistentvolumeclaims: "10" <1>
    requests.storage: "50Gi" <2>
    gold.storageclass.storage.k8s.io/requests.storage: "10Gi" <3>
    silver.storageclass.storage.k8s.io/requests.storage: "20Gi" <4>
    silver.storageclass.storage.k8s.io/persistentvolumeclaims: "5" <5>
    bronze.storageclass.storage.k8s.io/requests.storage: "0" <6>
    bronze.storageclass.storage.k8s.io/persistentvolumeclaims: "0" <7>
    requests.ephemeral-storage: 2Gi <8>
    limits.ephemeral-storage: 4Gi <9>
----
<1> The total number of persistent volume claims in a project
<2> Across all persistent volume claims in a project, the sum of storage requested cannot exceed this value.
<3> Across all persistent volume claims in a project, the sum of storage requested in the gold storage class cannot exceed this value.
<4> Across all persistent volume claims in a project, the sum of storage requested in the silver storage class cannot exceed this value.
<5> Across all persistent volume claims in a project, the total number of claims in the silver storage class cannot exceed this value.
<6> Across all persistent volume claims in a project, the sum of storage requested in the bronze storage class cannot exceed this value. When this is set to `0`, it means bronze storage class cannot request storage.
<7> Across all persistent volume claims in a project, the sum of storage requested in the bronze storage class cannot exceed this value. When this is set to `0`, it means bronze storage class cannot create claims.
<8> Across all pods in a non-terminal state, the sum of ephemeral storage requests cannot exceed 2Gi.
<9> Across all pods in a non-terminal state, the sum of ephemeral storage limits cannot exceed 4Gi.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

:_mod-docs-content-type: PROCEDURE
[id="quotas-creating-a-quota_{context}"]
= Creating a quota

You can create a quota to constrain resource usage in a given project.

.Procedure

. Define the quota in a file.

. Use the file to create the quota and apply it to a project:
+
[source,terminal]
----
$ oc create -f <file> [-n <project_name>]
----
+
For example:
+
[source,terminal]
----
$ oc create -f core-object-counts.yaml -n demoproject
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

:_mod-docs-content-type: PROCEDURE
[id="quota-creating-object-count-quotas_{context}"]
= Creating object count quotas

You can create an object count quota for all standard namespaced resource types on {product-title}, such as `BuildConfig` and `DeploymentConfig` objects. An object quota count places a defined quota on all standard namespaced resource types.

When using a resource quota, an object is charged against the quota upon creation. These types of quotas are useful to protect against exhaustion of resources. The quota can only be created if there are enough spare resources within the project.

.Procedure

To configure an object count quota for a resource:

. Run the following command:
+
[source,terminal]
----
$ oc create quota <name> \
    --hard=count/<resource>.<group>=<quota>,count/<resource>.<group>=<quota> <1>
----
<1> The `<resource>` variable is the name of the resource, and `<group>` is the API group, if applicable. Use the `oc api-resources` command for a list of resources and their associated API groups.
+
For example:
+
[source,terminal]
----
$ oc create quota test \
    --hard=count/deployments.extensions=2,count/replicasets.extensions=4,count/pods=3,count/secrets=4
----
+
.Example output
[source,terminal]
----
resourcequota "test" created
----
+
This example limits the listed resources to the hard limit in each project in the cluster.

. Verify that the quota was created:
+
[source,terminal]
----
$ oc describe quota test
----
+
.Example output
[source,terminal]
----
Name:                         test
Namespace:                    quota
Resource                      Used  Hard
--------                      ----  ----
count/deployments.extensions  0     2
count/pods                    0     3
count/replicasets.extensions  0     4
count/secrets                 0     4
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * applications/quotas-setting-per-project.adoc

:_mod-docs-content-type: PROCEDURE
[id="setting-resource-quota-for-extended-resources_{context}"]
= Setting resource quota for extended resources

Overcommitment of resources is not allowed for extended resources, so you must specify `requests` and `limits` for the same extended resource in a quota. Currently, only quota items with the prefix `requests.` is allowed for extended resources. The following is an example scenario of how to set resource quota for the GPU resource `nvidia.com/gpu`.

.Procedure

. Determine how many GPUs are available on a node in your cluster. For example:
+
[source,terminal]
----
# oc describe node ip-172-31-27-209.us-west-2.compute.internal | egrep 'Capacity|Allocatable|gpu'
----
+
.Example output
[source,terminal]
----
                    openshift.com/gpu-accelerator=true
Capacity:
 nvidia.com/gpu:  2
Allocatable:
 nvidia.com/gpu:  2
  nvidia.com/gpu  0           0
----
+
In this example, 2 GPUs are available.

. Set a quota in the namespace `nvidia`. In this example, the quota is `1`:
+
[source,terminal]
----
# cat gpu-quota.yaml
----
+
.Example output
[source,terminal]
----
apiVersion: v1
kind: ResourceQuota
metadata:
  name: gpu-quota
  namespace: nvidia
spec:
  hard:
    requests.nvidia.com/gpu: 1
----

. Create the quota:
+
[source,terminal]
----
# oc create -f gpu-quota.yaml
----
+
.Example output
[source,terminal]
----
resourcequota/gpu-quota created
----

. Verify that the namespace has the correct quota set:
+
[source,terminal]
----
# oc describe quota gpu-quota -n nvidia
----
+
.Example output
[source,terminal]
----
Name:                    gpu-quota
Namespace:               nvidia
Resource                 Used  Hard
--------                 ----  ----
requests.nvidia.com/gpu  0     1
----

. Define a pod that asks for a single GPU. The following example definition file is called `gpu-pod.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  generateName: gpu-pod-
  namespace: nvidia
spec:
  restartPolicy: OnFailure
  containers:
  - name: rhel7-gpu-pod
    image: rhel7
    env:
      - name: NVIDIA_VISIBLE_DEVICES
        value: all
      - name: NVIDIA_DRIVER_CAPABILITIES
        value: "compute,utility"
      - name: NVIDIA_REQUIRE_CUDA
        value: "cuda>=5.0"
    command: ["sleep"]
    args: ["infinity"]
    resources:
      limits:
        nvidia.com/gpu: 1
----

. Create the pod:
+
[source,terminal]
----
# oc create -f gpu-pod.yaml
----

. Verify that the pod is running:
+
[source,terminal]
----
# oc get pods
----
+
.Example output
[source,terminal]
----
NAME              READY     STATUS      RESTARTS   AGE
gpu-pod-s46h7     1/1       Running     0          1m
----

. Verify that the quota `Used` counter is correct:
+
[source,terminal]
----
# oc describe quota gpu-quota -n nvidia
----
+
.Example output
[source,terminal]
----
Name:                    gpu-quota
Namespace:               nvidia
Resource                 Used  Hard
--------                 ----  ----
requests.nvidia.com/gpu  1     1
----

. Attempt to create a second GPU pod in the `nvidia` namespace. This is technically available on the node because it has 2 GPUs:
+
[source,terminal]
----
# oc create -f gpu-pod.yaml
----
+
.Example output
[source,terminal]
----
Error from server (Forbidden): error when creating "gpu-pod.yaml": pods "gpu-pod-f7z2w" is forbidden: exceeded quota: gpu-quota, requested: requests.nvidia.com/gpu=1, used: requests.nvidia.com/gpu=1, limited: requests.nvidia.com/gpu=1
----
+
This *Forbidden* error message is expected because you have a quota of 1 GPU and this pod tried to allocate a second GPU, which exceeds its quota.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

:_mod-docs-content-type: PROCEDURE
[id="quota-viewing-quotas_{context}"]
= Viewing a quota

You can view usage statistics related to any hard limits defined in a project's
quota by navigating in the web console to the project's *Quota* page.

You can also use the CLI to view quota details.

.Procedure

. Get the list of quotas defined in the project. For example, for a project called
`demoproject`:
+
[source,terminal]
----
$ oc get quota -n demoproject
----
+
.Example output
[source,terminal]
----
NAME                AGE
besteffort          11m
compute-resources   2m
core-object-counts  29m
----

. Describe the quota you are interested in, for example the `core-object-counts`
quota:
+
[source,terminal]
----
$ oc describe quota core-object-counts -n demoproject
----
+
.Example output
[source,terminal]
----
Name:			core-object-counts
Namespace:		demoproject
Resource		Used	Hard
--------		----	----
configmaps		3	10
persistentvolumeclaims	0	4
replicationcontrollers	3	20
secrets			9	10
services		2	10
----

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * applications/quotas/quotas-setting-per-project.adoc

// NOTE: This is currently not configurable in 4.1, removing from 4.1 docs.

:_mod-docs-content-type: PROCEDURE
[id="configuring-explicit-resource-quotas_{context}"]
= Configuring explicit resource quotas

Configure explicit resource quotas in a project request template to apply specific resource quotas in new projects.

.Prerequisites

* Access to the cluster as a user with the cluster-admin role.

* Install the OpenShift CLI (`oc`).

.Procedure

. Add a resource quota definition to a project request template:
+
** If a project request template does not exist in a cluster:
.. Create a bootstrap project template and output it to a file called `template.yaml`:
+
[source,terminal]
----
$ oc adm create-bootstrap-project-template -o yaml > template.yaml
----
+
.. Add a resource quota definition to `template.yaml`. The following example defines a resource quota named 'storage-consumption'. The definition must be added before the `parameters:` section in the template:
+
[source,yaml]
----
- apiVersion: v1
  kind: ResourceQuota
  metadata:
    name: storage-consumption
    namespace: ${PROJECT_NAME}
  spec:
    hard:
      persistentvolumeclaims: "10" <1>
      requests.storage: "50Gi" <2>
      gold.storageclass.storage.k8s.io/requests.storage: "10Gi" <3>
      silver.storageclass.storage.k8s.io/requests.storage: "20Gi" <4>
      silver.storageclass.storage.k8s.io/persistentvolumeclaims: "5" <5>
      bronze.storageclass.storage.k8s.io/requests.storage: "0" <6>
      bronze.storageclass.storage.k8s.io/persistentvolumeclaims: "0" <7>
----
<1> The total number of persistent volume claims in a project.
<2> Across all persistent volume claims in a project, the sum of storage requested cannot exceed this value.
<3> Across all persistent volume claims in a project, the sum of storage requested in the gold storage class cannot exceed this value.
<4> Across all persistent volume claims in a project, the sum of storage requested in the silver storage class cannot exceed this value.
<5> Across all persistent volume claims in a project, the total number of claims in the silver storage class cannot exceed this value.
<6> Across all persistent volume claims in a project, the sum of storage requested in the bronze storage class cannot exceed this value. When this value is set to `0`, the bronze storage class cannot request storage.
<7> Across all persistent volume claims in a project, the sum of storage requested in the bronze storage class cannot exceed this value. When this value is set to `0`, the bronze storage class cannot create claims.
+
.. Create a project request template from the modified `template.yaml` file in the `openshift-config` namespace:
+
[source,terminal]
----
$ oc create -f template.yaml -n openshift-config
----
+
[NOTE]
====
To include the configuration as a `kubectl.kubernetes.io/last-applied-configuration` annotation, add the `--save-config` option to the `oc create` command.
====
+
By default, the template is called `project-request`.
+
** If a project request template already exists within a cluster:
+
[NOTE]
====
If you declaratively or imperatively manage objects within your cluster by using configuration files, edit the existing project request template through those files instead.
====
+
.. List templates in the `openshift-config` namespace:
+
[source,terminal]
----
$ oc get templates -n openshift-config
----
+
.. Edit an existing project request template:
+
[source,terminal]
----
$ oc edit template <project_request_template> -n openshift-config
----
+
.. Add a resource quota definition, such as the preceding `storage-consumption` example, into the existing template. The definition must be added before the `parameters:` section in the template.

. If you created a project request template, reference it in the cluster's project configuration resource:
.. Access the project configuration resource for editing:
+
** By using the web console:
... Navigate to the *Administration* -> *Cluster Settings* page.
... Click *Configuration* to view all configuration resources.
... Find the entry for *Project* and click *Edit YAML*.
+
** By using the CLI:
... Edit the `project.config.openshift.io/cluster` resource:
+
[source,terminal]
----
$ oc edit project.config.openshift.io/cluster
----
+
.. Update the `spec` section of the project configuration resource to include the `projectRequestTemplate` and `name` parameters. The following example references the default project request template name `project-request`:
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Project
metadata:
  ...
spec:
  projectRequestTemplate:
    name: project-request
----

. Verify that the resource quota is applied when projects are created:
.. Create a project:
+
[source,terminal]
----
$ oc new-project <project_name>
----
+
.. List the project's resource quotas:
+
[source,terminal]
----
$ oc get resourcequotas
----
+
.. Describe the resource quota in detail:
+
[source,terminal]
----
$ oc describe resourcequotas <resource_quota_name>
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/quotas-resources-managed,modules/quotas-scopes,modules/quotas-enforcement,modules/quotas-requests-vs-limits,modules/quotas-sample-resource-quotas-def,modules/quotas-creating-a-quota,modules/quotas-creating-object-count-quotas,modules/setting-resource-quota-for-extended-resources,modules/quotas-viewing-quotas,modules/quotas-requiring-explicit-quota

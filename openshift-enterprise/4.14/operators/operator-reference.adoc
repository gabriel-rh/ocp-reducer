:_mod-docs-content-type: ASSEMBLY
[id="cluster-operators-ref"]
= Cluster Operators reference
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: cluster-operators-ref

toc::[]

This reference guide indexes the _cluster Operators_ shipped by Red Hat that serve as the architectural foundation for {product-title}. Cluster Operators are installed by default, unless otherwise noted, and are managed by the Cluster Version Operator (CVO). For more details on the control plane architecture, see xref:../architecture/control-plane.adoc#operators-overview_control-plane[Operators in {product-title}].

Cluster administrators can view cluster Operators in the {product-title} web console from the *Administration* -> *Cluster Settings* page.

[NOTE]
====
Cluster Operators are not managed by Operator Lifecycle Manager (OLM) and OperatorHub. OLM and OperatorHub are part of the link:https://operatorframework.io/[Operator Framework] used in {product-title} for installing and running optional xref:../architecture/control-plane.adoc#olm-operators_control-plane[add-on Operators].
====

Some of the following cluster Operators can be disabled prior to installation. For more information see xref:../installing/cluster-capabilities.adoc#cluster-capabilities[cluster capabilities].

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc
// *  installing/cluster-capabilities.adoc

:operator-ref:


:_mod-docs-content-type: REFERENCE
[id="cluster-bare-metal-operator_{context}"]
= Cluster Baremetal Operator
= Bare-metal capability


[NOTE]
====
The Cluster Baremetal Operator is an optional cluster capability that can be disabled by cluster administrators during installation. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Cluster Baremetal Operator (CBO) deploys all the components necessary to take a bare-metal server to a fully functioning worker node ready to run {product-title} compute nodes. The CBO ensures that the metal3 deployment, which consists of the Bare Metal Operator (BMO) and Ironic containers, runs on one of the control plane nodes within the {product-title} cluster. The CBO also listens for {product-title} updates to resources that it watches and takes appropriate action.



[discrete]
== Project

link:https://github.com/openshift/cluster-baremetal-operator[cluster-baremetal-operator]


:!operator-ref:


:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/cluster-capabilities.adoc#cluster-bare-metal-operator_cluster-capabilities[Bare-metal capability]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc
[id="baremetal-event-relay_{context}"]
= {redfish-operator}

[discrete]
== Purpose
The OpenShift {redfish-operator} manages the life-cycle of the Bare Metal Event Relay. The Bare Metal Event Relay enables you to configure the types of cluster event that are monitored using Redfish hardware events.

[discrete]
== Configuration objects
You can use this command to edit the configuration after installation: for example, the webhook port.
You can edit configuration objects with:

[source,terminal]
----
$ oc -n [namespace] edit cm hw-event-proxy-operator-manager-config
----

[source,terminal]
----
apiVersion: controller-runtime.sigs.k8s.io/v1alpha1
kind: ControllerManagerConfig
health:
  healthProbeBindAddress: :8081
metrics:
  bindAddress: 127.0.0.1:8080
webhook:
  port: 9443
leaderElection:
  leaderElect: true
  resourceName: 6e7a703c.redhat-cne.org
----

[discrete]
== Project
link:https://github.com/redhat-cne/hw-event-proxy-operator[hw-event-proxy-operator]

[discrete]
== CRD
The proxy enables applications running on bare-metal clusters to respond quickly to Redfish hardware changes and failures such as breaches of temperature thresholds, fan failure, disk loss, power outages, and memory failure, reported using the HardwareEvent CR.

`hardwareevents.event.redhat-cne.org`:

* Scope: Namespaced
* CR: HardwareEvent
* Validation: Yes

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cloud-credential-operator_{context}"]
= Cloud Credential Operator

[discrete]
== Purpose

The Cloud Credential Operator (CCO) manages cloud provider credentials as Kubernetes custom resource definitions (CRDs). The CCO syncs on `CredentialsRequest` custom resources (CRs) to allow {product-title} components to request cloud provider credentials with the specific permissions that are required for the cluster to run.

By setting different values for the `credentialsMode` parameter in the `install-config.yaml` file, the CCO can be configured to operate in several different modes. If no mode is specified, or the `credentialsMode` parameter is set to an empty string (`""`), the CCO operates in its default mode.

[discrete]
== Project

link:https://github.com/openshift/cloud-credential-operator[openshift-cloud-credential-operator]

[discrete]
== CRDs

* `credentialsrequests.cloudcredential.openshift.io`
** Scope: Namespaced
** CR: `CredentialsRequest`
** Validation: Yes

[discrete]
== Configuration objects

No configuration required.

:leveloffset!:

[role="_additional-resources"]
[discrete]
[id="additional-resources_cluster-op-ref-cco"]
=== Additional resources
* xref:../authentication/managing_cloud_provider_credentials/about-cloud-credential-operator.adoc#about-cloud-credential-operator[About the Cloud Credential Operator]
* xref:../rest_api/security_apis/credentialsrequest-cloudcredential-openshift-io-v1.adoc#credentialsrequest-cloudcredential-openshift-io-v1[`CredentialsRequest` custom resource]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-authentication-operator_{context}"]
= Cluster Authentication Operator

[discrete]
== Purpose

The Cluster Authentication Operator installs and maintains the `Authentication` custom resource in a cluster and can be viewed with:

[source,terminal]
----
$ oc get clusteroperator authentication -o yaml
----

[discrete]
== Project

link:https://github.com/openshift/cluster-authentication-operator[cluster-authentication-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-autoscaler-operator_{context}"]
= Cluster Autoscaler Operator

[discrete]
== Purpose

The Cluster Autoscaler Operator manages deployments of the OpenShift Cluster Autoscaler using the `cluster-api` provider.

[discrete]
== Project

link:https://github.com/openshift/cluster-autoscaler-operator[cluster-autoscaler-operator]

[discrete]
== CRDs

* `ClusterAutoscaler`: This is a singleton resource, which controls the configuration autoscaler instance for the cluster. The Operator only responds to the `ClusterAutoscaler` resource named `default` in the managed namespace, the value of the `WATCH_NAMESPACE` environment variable.
* `MachineAutoscaler`: This resource targets a node group and manages the annotations to enable and configure autoscaling for that group, the `min` and `max` size. Currently only `MachineSet` objects can be targeted.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-cloud-controller-manager-operator_{context}"]
= Cluster Cloud Controller Manager Operator

[discrete]
== Purpose

[NOTE]
====
The status of this Operator is General Availability for Amazon Web Services (AWS), IBM Cloud, global Microsoft Azure, Microsoft Azure Stack Hub, Nutanix, {rh-openstack-first}, and VMware vSphere.

The Operator is available as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] for Alibaba Cloud, Google Cloud Platform (GCP), and IBM Cloud Power VS.
====

The Cluster Cloud Controller Manager Operator manages and updates the cloud controller managers deployed on top of {product-title}. The Operator is based on the Kubebuilder framework and `controller-runtime` libraries. It is installed via the Cluster Version Operator (CVO).

It contains the following components:

* Operator
* Cloud configuration observer

By default, the Operator exposes Prometheus metrics through the `metrics` service.

[discrete]
== Project

link:https://github.com/openshift/cluster-cloud-controller-manager-operator[cluster-cloud-controller-manager-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-capi-operator_{context}"]
= Cluster CAPI Operator

[NOTE]
====
This Operator is available as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview] for Amazon Web Services (AWS) and Google Cloud Platform (GCP) clusters.
====

[discrete]
== Purpose

The Cluster CAPI Operator maintains the lifecycle of Cluster API resources. This Operator is responsible for all administrative tasks related to deploying the Cluster API project within an {product-title} cluster.

[discrete]
== Project

link:https://github.com/openshift/cluster-capi-operator[cluster-capi-operator]

[discrete]
== CRDs

* `awsmachines.infrastructure.cluster.x-k8s.io`
** Scope: Namespaced
** CR: `awsmachine`
** Validation: No

*  `gcpmachines.infrastructure.cluster.x-k8s.io`
** Scope: Namespaced
** CR: `gcpmachine`
** Validation: No

* `awsmachinetemplates.infrastructure.cluster.x-k8s.io`
** Scope: Namespaced
** CR: `awsmachinetemplate`
** Validation: No

*  `gcpmachinetemplates.infrastructure.cluster.x-k8s.io`
** Scope: Namespaced
** CR: `gcpmachinetemplate`
** Validation: No

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc

[id="cluster-config-operator_{context}"]
= Cluster Config Operator

[discrete]
== Purpose

The Cluster Config Operator performs the following tasks related to `config.openshift.io`:

* Creates CRDs.
* Renders the initial custom resources.
* Handles migrations.


[discrete]
== Project

link:https://github.com/openshift/cluster-config-operator[cluster-config-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc
// * installing/cluster-capabilities.adoc

:operator-ref:


:_mod-docs-content-type: REFERENCE
[id="cluster-csi-snapshot-controller-operator_{context}"]
= Cluster CSI Snapshot Controller Operator
= CSI snapshot controller capability


[NOTE]
====
The Cluster CSI Snapshot Controller Operator is an optional cluster capability that can be disabled by cluster administrators during installation. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Cluster CSI Snapshot Controller Operator installs and maintains the CSI Snapshot Controller. The CSI Snapshot Controller is responsible for watching the `VolumeSnapshot` CRD objects and manages the creation and deletion lifecycle of volume snapshots.


[discrete]
== Project

link:https://github.com/openshift/cluster-csi-snapshot-controller-operator[cluster-csi-snapshot-controller-operator]


:!operator-ref:


:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/cluster-capabilities.adoc#cluster-csi-snapshot-controller-operator_cluster-capabilities[CSI snapshot controller capability]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc
// * installing/cluster-capabilities.adoc

// operators/operator-reference.adoc
:operator-ref:

// installing/cluster-capabilities.adoc

:_mod-docs-content-type: REFERENCE
[id="cluster-image-registry-operator_{context}"]
= Cluster Image Registry Operator
= Cluster Image Registry capability

[discrete]
== Purpose


The Cluster Image Registry Operator manages a singleton instance of the {product-registry}. It manages all configuration of the registry, including creating storage.

On initial start up, the Operator creates a default `image-registry` resource instance based on the configuration detected in the cluster. This indicates what cloud storage type to use based on the cloud provider.

If insufficient information is available to define a complete `image-registry` resource, then an incomplete resource is defined and the Operator updates the resource status with information about what is missing.

The Cluster Image Registry Operator runs in the `openshift-image-registry` namespace and it also manages the registry instance in that location. All configuration and workload resources for the registry reside in that namespace.


[discrete]
== Project

link:https://github.com/openshift/cluster-image-registry-operator[cluster-image-registry-operator]

:!operator-ref:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc

[id="cluster-machine-approver-operator_{context}"]
= Cluster Machine Approver Operator

[discrete]
== Purpose

The Cluster Machine Approver Operator automatically approves the CSRs requested for a new worker node after cluster installation.

[NOTE]
====
For the control plane node, the `approve-csr` service on the bootstrap node automatically approves all CSRs during the cluster bootstrapping phase.
====

[discrete]
== Project

link:https://github.com/openshift/cluster-machine-approver[cluster-machine-approver-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-monitoring-operator_{context}"]
= Cluster Monitoring Operator

[discrete]
== Purpose

The Cluster Monitoring Operator manages and updates the Prometheus-based cluster monitoring stack deployed on top of {product-title}.

[discrete]
== Project

link:https://github.com/openshift/cluster-monitoring-operator[openshift-monitoring]

[discrete]
== CRDs

* `alertmanagers.monitoring.coreos.com`
** Scope: Namespaced
** CR: `alertmanager`
** Validation: Yes
* `prometheuses.monitoring.coreos.com`
** Scope: Namespaced
** CR: `prometheus`
** Validation: Yes
* `prometheusrules.monitoring.coreos.com`
** Scope: Namespaced
** CR: `prometheusrule`
** Validation: Yes
* `servicemonitors.monitoring.coreos.com`
** Scope: Namespaced
** CR: `servicemonitor`
** Validation: Yes

[discrete]
== Configuration objects

[source,terminal]
----
$ oc -n openshift-monitoring edit cm cluster-monitoring-config
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-network-operator_{context}"]
= Cluster Network Operator

[discrete]
== Purpose

The Cluster Network Operator installs and upgrades the networking components on an {product-title} cluster.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc
// *  installing/cluster-capabilities.adoc

// operators/operator-reference.adoc
:operator-ref:

// installing/cluster-capabilities.adoc

:_mod-docs-content-type: REFERENCE
[id="cluster-samples-operator_{context}"]
= Cluster Samples Operator
= OpenShift samples capability


[NOTE]
====
The Cluster Samples Operator is an optional cluster capability that can be disabled by cluster administrators during installation. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Cluster Samples Operator manages the sample image streams and templates stored in the `openshift` namespace.

On initial start up, the Operator creates the default samples configuration resource to initiate the creation of the image streams and templates. The configuration object is a cluster scoped object with the key `cluster` and type `configs.samples`.

The image streams are the {op-system-first}-based {product-title} image streams pointing to images on `registry.redhat.io`. Similarly, the templates are those categorized as {product-title} templates.


The Cluster Samples Operator deployment is contained within the `openshift-cluster-samples-operator` namespace. On start up, the install pull secret is used by the image stream import logic in the {product-registry} and API server to authenticate with `registry.redhat.io`. An administrator can create any additional secrets in the `openshift` namespace if they change the registry used for the sample image streams. If created, those secrets contain the content of a `config.json` for `docker` needed to facilitate image import.

The image for the Cluster Samples Operator contains image stream and template definitions for the associated {product-title} release. After the Cluster Samples Operator creates a sample, it adds an annotation that denotes the {product-title} version that it is compatible with. The Operator uses this annotation to ensure that each sample matches the compatible release version. Samples outside of its inventory are ignored, as are skipped samples.

Modifications to any samples that are managed by the Operator are allowed as long as the version annotation is not modified or deleted. However, on an upgrade, as the version annotation will change, those modifications can get replaced as the sample will be updated with the newer version. The Jenkins images are part of the image payload from the installation and are tagged into the image streams directly.

The samples resource includes a finalizer, which cleans up the following upon its deletion:

* Operator-managed image streams
* Operator-managed templates
* Operator-generated configuration resources
* Cluster status resources

Upon deletion of the samples resource, the Cluster Samples Operator recreates the resource using the default configuration.

[discrete]
== Project

link:https://github.com/openshift/cluster-samples-operator[cluster-samples-operator]

:!operator-ref:


:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../installing/cluster-capabilities.adoc#cluster-samples-operator_cluster-capabilities[OpenShift samples capability]

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc
// *  installing/cluster-capabilities.adoc

:operator-ref:


[id="cluster-storage-operator_{context}"]
= Cluster Storage Operator
= Cluster storage capability


[NOTE]
====
The Cluster Storage Operator is an optional cluster capability that can be disabled by cluster administrators during installation. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Cluster Storage Operator sets {product-title} cluster-wide storage defaults. It ensures a default `storageclass` exists for {product-title} clusters. It also installs Container Storage Interface (CSI) drivers which enable your cluster to use various storage backends.



[discrete]
== Project

link:https://github.com/openshift/cluster-storage-operator[cluster-storage-operator]

[discrete]
== Configuration

No configuration is required.


[discrete]
== Notes

* The storage class that the Operator creates can be made non-default by editing its annotation, but this storage class cannot be deleted as long as the Operator runs.

:!operator-ref:


:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/cluster-capabilities.adoc#cluster-storage-operator_cluster-capabilities[Storage capability]

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc

[id="cluster-version-operator_{context}"]
= Cluster Version Operator

[discrete]
== Purpose

Cluster Operators manage specific areas of cluster functionality. The Cluster Version Operator (CVO) manages the lifecycle of cluster Operators, many of which are installed in {product-title} by default.

The CVO also checks with the OpenShift Update Service to see the valid updates and update paths based on current component versions and information in the graph.

[discrete]
== Project

link:https://github.com/openshift/cluster-version-operator[cluster-version-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc
// *  installing/cluster-capabilities.adoc

// operators/operator-reference.adoc
:operator-ref:


:_mod-docs-content-type: REFERENCE
[id="console-operator_{context}"]
= Console Operator
= Console capability


[NOTE]
====
The Console Operator is an optional cluster capability that can be disabled by cluster administrators during installation. If you disable the Console Operator at installation, your cluster is still supported and upgradable. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Console Operator installs and maintains the {product-title} web console on a cluster. The Console Operator is installed by default and automatically maintains a console.


[discrete]
== Project

link:https://github.com/openshift/console-operator[console-operator]


:!operator-ref:

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/cluster-capabilities.adoc#console-operator_cluster-capabilities[Web console capability]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="control-plane-machine-set-operator_{context}"]
= Control Plane Machine Set Operator

[NOTE]
====
This Operator is available for Amazon Web Services (AWS), Google Cloud Platform (GCP), Microsoft Azure, Nutanix, and VMware vSphere.
====

[discrete]
== Purpose

The Control Plane Machine Set Operator automates the management of control plane machine resources within an {product-title} cluster.

[discrete]
== Project

link:https://github.com/openshift/cluster-control-plane-machine-set-operator[cluster-control-plane-machine-set-operator]

[discrete]
== CRDs

* `controlplanemachineset.machine.openshift.io`
** Scope: Namespaced
** CR: `ControlPlaneMachineSet`
** Validation: Yes

:leveloffset!:

[role="_additional-resources"]
[discrete]
[id="additional-resources_cluster-op-ref-cpmso"]
=== Additional resources

* xref:../machine_management/control_plane_machine_management/cpmso-about.adoc#cpmso-about[About control plane machine sets]
* xref:../rest_api/machine_apis/controlplanemachineset-machine-openshift-io-v1.adoc#controlplanemachineset-machine-openshift-io-v1[`ControlPlaneMachineSet` custom resource]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="dns-operator_{context}"]
= DNS Operator

[discrete]
== Purpose

The DNS Operator deploys and manages CoreDNS to provide a name resolution service to pods that enables DNS-based Kubernetes Service discovery in {product-title}.

The Operator creates a working default deployment based on the cluster's configuration.

* The default cluster domain is `cluster.local`.
* Configuration of the CoreDNS Corefile or Kubernetes plugin is not yet supported.

The DNS Operator manages CoreDNS as a Kubernetes daemon set exposed as a service with a static IP. CoreDNS runs on all nodes in the cluster.

[discrete]
== Project

link:https://github.com/openshift/cluster-dns-operator[cluster-dns-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="etcd-cluster-operator_{context}"]
= etcd cluster Operator

[discrete]
== Purpose

The etcd cluster Operator automates etcd cluster scaling, enables etcd monitoring and metrics, and simplifies disaster recovery procedures.
[discrete]
== Project

link:https://github.com/openshift/cluster-etcd-operator/[cluster-etcd-operator]

[discrete]
== CRDs

* `etcds.operator.openshift.io`
** Scope: Cluster
** CR: `etcd`
** Validation: Yes

[discrete]
== Configuration objects

[source,terminal]
----
$ oc edit etcd cluster
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="ingress-operator_{context}"]
= Ingress Operator

[discrete]
== Purpose

The Ingress Operator configures and manages the {product-title} router.

[discrete]
== Project

link:https://github.com/openshift/cluster-ingress-operator[openshift-ingress-operator]

[discrete]
== CRDs

* `clusteringresses.ingress.openshift.io`
** Scope: Namespaced
** CR: `clusteringresses`
** Validation: No

[discrete]
== Configuration objects

* Cluster config
** Type Name: `clusteringresses.ingress.openshift.io`
** Instance Name: `default`
** View Command:
+
[source,terminal]
----
$ oc get clusteringresses.ingress.openshift.io -n openshift-ingress-operator default -o yaml
----

[discrete]
== Notes

The Ingress Operator sets up the router in the `openshift-ingress` project and creates the deployment for the router:

[source,terminal]
----
$ oc get deployment -n openshift-ingress
----

The Ingress Operator uses the `clusterNetwork[].cidr` from the `network/cluster` status to determine what mode (IPv4, IPv6, or dual stack) the managed Ingress Controller (router) should operate in. For example, if `clusterNetwork` contains only a v6 `cidr`, then the Ingress Controller operates in IPv6-only mode.

In the following example, Ingress Controllers managed by the Ingress Operator will run in IPv4-only mode because only one cluster network exists and the network is an IPv4 `cidr`:

[source,terminal]
----
$ oc get network/cluster -o jsonpath='{.status.clusterNetwork[*]}'
----

.Example output
[source,terminal]
----
map[cidr:10.128.0.0/14 hostPrefix:23]
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc
// * installing/cluster-capabilities.adoc


:operator-ref:

:_mod-docs-content-type: REFERENCE
[id="insights-operator_{context}"]
= Insights Operator
= Insights capability


[NOTE]
====
The Insights Operator is an optional cluster capability that can be disabled by cluster administrators during installation. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Insights Operator gathers {product-title} configuration data and sends it to Red Hat. The data is used to produce proactive insights recommendations about potential issues that a cluster might be exposed to. These insights are communicated to cluster administrators through Insights Advisor on link:https://console.redhat.com/[console.redhat.com].


[discrete]
== Project

link:https://github.com/openshift/insights-operator[insights-operator]

[discrete]
== Configuration

No configuration is required.


[discrete]
== Notes

Insights Operator complements {product-title} Telemetry.

:!operator-ref:


:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/cluster-capabilities.adoc#insights-operator_cluster-capabilities[Insights capability]
* See xref:../support/remote_health_monitoring/about-remote-health-monitoring.adoc#about-remote-health-monitoring[About remote health monitoring] for details about Insights Operator and Telemetry.

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="kube-apiserver-operator_{context}"]
= Kubernetes API Server Operator

[discrete]
== Purpose

The Kubernetes API Server Operator manages and updates the Kubernetes API server deployed on top of {product-title}. The Operator is based on the {product-title} `library-go` framework and it is installed using the Cluster Version Operator (CVO).

[discrete]
== Project

link:https://github.com/openshift/cluster-kube-apiserver-operator[openshift-kube-apiserver-operator]

[discrete]
== CRDs

* `kubeapiservers.operator.openshift.io`
** Scope: Cluster
** CR: `kubeapiserver`
** Validation: Yes

[discrete]
== Configuration objects

[source,terminal]
----
$ oc edit kubeapiserver
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="kube-controller-manager-operator_{context}"]
= Kubernetes Controller Manager Operator

[discrete]
== Purpose

The Kubernetes Controller Manager Operator manages and updates the Kubernetes Controller Manager deployed on top of {product-title}. The Operator is based on {product-title} `library-go` framework and it is installed via the Cluster Version Operator (CVO).

It contains the following components:

* Operator
* Bootstrap manifest renderer
* Installer based on static pods
* Configuration observer

By default, the Operator exposes Prometheus metrics through the `metrics` service.

[discrete]
== Project

link:https://github.com/openshift/cluster-kube-controller-manager-operator[cluster-kube-controller-manager-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-kube-scheduler-operator_{context}"]
= Kubernetes Scheduler Operator

[discrete]
== Purpose

The Kubernetes Scheduler Operator manages and updates the Kubernetes Scheduler deployed on top of {product-title}. The Operator is based on the {product-title} `library-go` framework and it is installed with the Cluster Version Operator (CVO).

The Kubernetes Scheduler Operator contains the following components:

* Operator
* Bootstrap manifest renderer
* Installer based on static pods
* Configuration observer

By default, the Operator exposes Prometheus metrics through the metrics service.

[discrete]
== Project

link:https://github.com/openshift/cluster-kube-scheduler-operator[cluster-kube-scheduler-operator]

[discrete]
== Configuration

The configuration for the Kubernetes Scheduler is the result of merging:

* a default configuration.
* an observed configuration from the spec `schedulers.config.openshift.io`.

All of these are sparse configurations, invalidated JSON snippets which are merged to form a valid configuration at the end.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-kube-storage-version-migrator-operator_{context}"]
= Kubernetes Storage Version Migrator Operator

[discrete]
== Purpose

The Kubernetes Storage Version Migrator Operator detects changes of the default storage version, creates migration requests for resource types when the storage version changes, and processes migration requests.

[discrete]
== Project

link:https://github.com/openshift/cluster-kube-storage-version-migrator-operator[cluster-kube-storage-version-migrator-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="machine-api-operator_{context}"]
= Machine API Operator

[discrete]
== Purpose

The Machine API Operator manages the lifecycle of specific purpose custom resource definitions (CRD), controllers, and RBAC objects that extend the Kubernetes API. This declares the desired state of machines in a cluster.

[discrete]
== Project

link:https://github.com/openshift/machine-api-operator[machine-api-operator]

[discrete]
== CRDs

* `MachineSet`
* `Machine`
* `MachineHealthCheck`

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc
// * post_installation_configuration/machine-configuration-tasks.adoc

[id="machine-config-operator_{context}"]
= Machine Config Operator

[discrete]
== Purpose

The Machine Config Operator manages and applies configuration and updates of the base operating system and container runtime, including everything between the kernel and kubelet.

There are four components:

* `machine-config-server`: Provides Ignition configuration to new machines joining the cluster.
* `machine-config-controller`: Coordinates the upgrade of machines to the desired configurations defined by a `MachineConfig` object. Options are provided to control the upgrade for sets of machines individually.
* `machine-config-daemon`: Applies new machine configuration during update. Validates and verifies the state of the machine to the requested machine configuration.
* `machine-config`: Provides a complete source of machine configuration at installation, first start up, and updates for a machine.

// Text snippet included in the following modules:
//
// * modules/installation-about-custom-azure-vnet.adoc
// * modules/machine-config-operator.adoc
// * security/certificate_types_descriptions/machine-config-operator-certificates.adoc

:_mod-docs-content-type: SNIPPET

[IMPORTANT]
====
Currently, there is no supported way to block or restrict the machine config server endpoint. The machine config server must be exposed to the network so that newly-provisioned machines, which have no existing configuration or state, are able to fetch their configuration. In this model, the root of trust is the certificate signing requests (CSR) endpoint, which is where the kubelet sends its certificate signing request for approval to join the cluster. Because of this, machine configs should not be used to distribute sensitive information, such as secrets and certificates.

To ensure that the machine config server endpoints, ports 22623 and 22624, are secured in bare metal scenarios, customers must configure proper network policies.
====

.Additional resources

* xref:../networking/openshift_sdn/about-openshift-sdn.adoc#about-openshift-sdn[About the OpenShift SDN network plugin].

[discrete]
== Project

link:https://github.com/openshift/machine-config-operator[openshift-machine-config-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc
// * installing/cluster-capabilities.adoc

// operators/operator-reference.adoc
:operator-ref:

// installing/cluster-capabilities.adoc

:_mod-docs-content-type: REFERENCE
[id="marketplace-operator_{context}"]
= Marketplace Operator
= Marketplace capability


[NOTE]
====
The Marketplace Operator is an optional cluster capability that can be disabled by cluster administrators if it is not needed. For more information about optional cluster capabilities, see "Cluster capabilities" in _Installing_.
====


[discrete]
== Purpose


The Marketplace Operator simplifies the process for bringing off-cluster Operators to your cluster by using a set of default Operator Lifecycle Manager (OLM) catalogs on the cluster. When the Marketplace Operator is installed, it creates the `openshift-marketplace` namespace. OLM ensures catalog sources installed in the `openshift-marketplace` namespace are available for all namespaces on the cluster.


[discrete]
== Project

link:https://github.com/operator-framework/operator-marketplace[operator-marketplace]


:!operator-ref:


:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../installing/cluster-capabilities.adoc#marketplace-operator_cluster-capabilities[Marketplace capability]

:leveloffset: +1

// Module included in the following assemblies:
//
// * scalability_and_performance/using-node-tuning-operator.adoc
// * operators/operator-reference.adoc
// * post_installation_configuration/node-tasks.adoc

:operators:

:_mod-docs-content-type: CONCEPT
[id="about-node-tuning-operator_{context}"]
= Node Tuning Operator
= Node Tuning capability

[discrete]
== Purpose


The Node Tuning Operator helps you manage node-level tuning by orchestrating the TuneD daemon and achieves low latency performance by using the Performance Profile controller. The majority of high-performance applications require some level of kernel tuning. The Node Tuning Operator provides a unified management interface to users of node-level sysctls and more flexibility to add custom tuning specified by user needs.


The Operator manages the containerized TuneD daemon for {product-title} as a Kubernetes daemon set. It ensures the custom tuning specification is passed to all containerized TuneD daemons running in the cluster in the format that the daemons understand. The daemons run on all nodes in the cluster, one per node.

Node-level settings applied by the containerized TuneD daemon are rolled back on an event that triggers a profile change or when the containerized TuneD daemon is terminated gracefully by receiving and handling a termination signal.

The Node Tuning Operator uses the Performance Profile controller to implement automatic tuning to achieve low latency performance for {product-title} applications.

The cluster administrator configures a performance profile to define node-level settings such as the following:

* Updating the kernel to kernel-rt.
* Choosing CPUs for housekeeping.
* Choosing CPUs for running workloads.

[NOTE]
====
Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
====

The Node Tuning Operator is part of a standard {product-title} installation in version 4.1 and later.

[NOTE]
====
In earlier versions of {product-title}, the Performance Addon Operator was used to implement automatic tuning to achieve low latency performance for OpenShift applications. In {product-title} 4.11 and later, this functionality is part of the Node Tuning Operator.
====

[discrete]
== Project

link:https://github.com/openshift/cluster-node-tuning-operator[cluster-node-tuning-operator]

:leveloffset!:

[discrete]
[role="_additional-resources"]
[id="cluster-operators-ref-nto-addtl-resources"]
=== Additional resources
* xref:../scalability_and_performance/cnf-low-latency-tuning.adoc#cnf-understanding-low-latency_cnf-master[Low latency tuning of OCP nodes]

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="openshift-apiserver-operator_{context}"]
= OpenShift API Server Operator

[discrete]
== Purpose

The OpenShift API Server Operator installs and maintains the `openshift-apiserver` on a cluster.

[discrete]
== Project

link:https://github.com/openshift/cluster-openshift-apiserver-operator[openshift-apiserver-operator]

[discrete]
== CRDs

* `openshiftapiservers.operator.openshift.io`
** Scope: Cluster
** CR: `openshiftapiserver`
** Validation: Yes

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="cluster-openshift-controller-manager-operator_{context}"]
= OpenShift Controller Manager Operator

[discrete]
== Purpose

The OpenShift Controller Manager Operator installs and maintains the `OpenShiftControllerManager` custom resource in a cluster and can be viewed with:

[source,terminal]
----
$ oc get clusteroperator openshift-controller-manager -o yaml
----

The custom resource definition (CRD) `openshiftcontrollermanagers.operator.openshift.io` can be viewed in a cluster with:

[source,terminal]
----
$ oc get crd openshiftcontrollermanagers.operator.openshift.io -o yaml
----

[discrete]
== Project

link:https://github.com/openshift/cluster-openshift-controller-manager-operator[cluster-openshift-controller-manager-operator]

:leveloffset!:

[id="cluster-operators-ref-olm"]
== Operator Lifecycle Manager Operators
[discrete]
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/understanding/olm/olm-understanding-olm.adoc
// * operators/operator-reference.adoc

[id="olm-overview_{context}"]
= Purpose

_Operator Lifecycle Manager_ (OLM) helps users install, update, and manage the lifecycle of Kubernetes native applications (Operators) and their associated services running across their {product-title} clusters. It is part of the link:https://operatorframework.io/[Operator Framework], an open source toolkit designed to manage Operators in an effective, automated, and scalable way.

.Operator Lifecycle Manager workflow
image::olm-workflow.png[]

OLM runs by default in {product-title} {product-version}, which aids
cluster administrators
in installing, upgrading, and granting access to Operators running on their cluster. The {product-title} web console provides management screens for
cluster administrators
to install Operators, as well as grant specific projects access to use the catalog of Operators available on the cluster.

For developers, a self-service experience allows provisioning and configuring instances of databases, monitoring, and big data services without having to be subject matter experts, because the Operator has that knowledge baked into it.

:leveloffset!:
[discrete]
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/understanding/olm/olm-understanding-olm.adoc
// * operators/operator-reference.adoc

[id="olm-architecture_{context}"]
= CRDs

Operator Lifecycle Manager (OLM) is composed of two Operators: the OLM Operator and the Catalog Operator.

Each of these Operators is responsible for managing the custom resource definitions (CRDs) that are the basis for the OLM framework:

.CRDs managed by OLM and Catalog Operators
[cols="2a,1a,1a,8a",options="header"]
|===
|Resource |Short name |Owner |Description

|`ClusterServiceVersion` (CSV)
|`csv`
|OLM
|Application metadata: name, version, icon, required resources, installation, and so on.

|`InstallPlan`
|`ip`
|Catalog
|Calculated list of resources to be created to automatically install or upgrade a CSV.

|`CatalogSource`
|`catsrc`
|Catalog
|A repository of CSVs, CRDs, and packages that define an application.

|`Subscription`
|`sub`
|Catalog
|Used to keep CSVs up to date by tracking a channel in a package.

|`OperatorGroup`
|`og`
|OLM
|Configures all Operators deployed in the same namespace as the `OperatorGroup` object to watch for their custom resource (CR) in a list of namespaces or cluster-wide.
|===

Each of these Operators is also responsible for creating the following resources:

.Resources created by OLM and Catalog Operators
[options="header"]
|===
|Resource |Owner

|`Deployments`
.4+.^|OLM

|`ServiceAccounts`
|`(Cluster)Roles`
|`(Cluster)RoleBindings`

|`CustomResourceDefinitions` (CRDs)
.2+.^|Catalog
|`ClusterServiceVersions`
|===

:leveloffset!:
[discrete]
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/understanding/olm/olm-arch.adoc
// * operators/operator-reference.adoc

[id="olm-arch-olm-operator_{context}"]
= OLM Operator

The OLM Operator is responsible for deploying applications defined by CSV resources after the required resources specified in the CSV are present in the cluster.

The OLM Operator is not concerned with the creation of the required resources; you can choose to manually create these resources using the CLI or using the Catalog Operator. This separation of concern allows users incremental buy-in in terms of how much of the OLM framework they choose to leverage for their application.

The OLM Operator uses the following workflow:

. Watch for cluster service versions (CSVs) in a namespace and check that requirements are met.
. If requirements are met, run the install strategy for the CSV.
+
[NOTE]
====
A CSV must be an active member of an Operator group for the install strategy to run.
====

:leveloffset!:
[discrete]
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/understanding/olm/olm-arch.adoc
// * operators/operator-reference.adoc

[id="olm-arch-catalog-operator_{context}"]
= Catalog Operator

The Catalog Operator is responsible for resolving and installing cluster service versions (CSVs) and the required resources they specify. It is also responsible for watching catalog sources for updates to packages in channels and upgrading them, automatically if desired, to the latest available versions.

To track a package in a channel, you can create a `Subscription` object configuring the desired package, channel, and the `CatalogSource` object you want to use for pulling updates. When updates are found, an appropriate `InstallPlan` object is written into the namespace on behalf of the user.

The Catalog Operator uses the following workflow:

. Connect to each catalog source in the cluster.
. Watch for unresolved install plans created by a user, and if found:
.. Find the CSV matching the name requested and add the CSV as a resolved resource.
.. For each managed or required CRD, add the CRD as a resolved resource.
.. For each required CRD, find the CSV that manages it.
. Watch for resolved install plans and create all of the discovered resources for it, if approved by a user or automatically.
. Watch for catalog sources and subscriptions and create install plans based on them.

:leveloffset!:
[discrete]
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/understanding/olm/olm-arch.adoc
// * operators/operator-reference.adoc

[id="olm-arch-catalog-registry_{context}"]
= Catalog Registry

The Catalog Registry stores CSVs and CRDs for creation in a cluster and stores metadata about packages and channels.

A _package manifest_ is an entry in the Catalog Registry that associates a package identity with sets of CSVs. Within a package, channels point to a particular CSV. Because CSVs explicitly reference the CSV that they replace, a package manifest provides the Catalog Operator with all of the information that is required to update a CSV to the latest version in a channel, stepping through each intermediate version.

:leveloffset!:

[role="_additional-resources"]
[discrete]
[id="cluster-operators-ref-olm-addtl-resources"]
=== Additional resources
* For more information, see the sections on xref:../operators/understanding/olm/olm-understanding-olm.adoc#olm-understanding-olm[understanding Operator Lifecycle Manager (OLM)].

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/operator-reference.adoc

[id="openshift-service-ca-operator_{context}"]
= OpenShift Service CA Operator

[discrete]
== Purpose

The OpenShift Service CA Operator mints and manages serving certificates for Kubernetes services.

[discrete]
== Project

link:https://github.com/openshift/service-ca-operator[openshift-service-ca-operator]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// *  operators/operator-reference.adoc

:operator-name: vSphere Problem Detector Operator

[id="vsphere-problem-detector-operator_{context}"]
= {operator-name}

[discrete]
== Purpose

The {operator-name} checks clusters that are deployed on vSphere for common installation and misconfiguration issues that are related to storage.

[NOTE]
====
The {operator-name} is only started by the Cluster Storage Operator when the Cluster Storage Operator detects that the cluster is deployed on vSphere.
====

[discrete]
== Configuration

No configuration is required.

[discrete]
== Notes

* The Operator supports {product-title} installations on vSphere.
* The Operator uses the `vsphere-cloud-credentials` to communicate with vSphere.
* The Operator performs checks that are related to storage.

// Clear temporary attributes
:!operator-name:

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* For more details, see xref:../installing/installing_vsphere/using-vsphere-problem-detector-operator.adoc#using-vsphere-problem-detector-operator[Using the vSphere Problem Detector Operator].

//# includes=_attributes/common-attributes,modules/cluster-bare-metal-operator,modules/baremetal-event-relay,modules/cloud-credential-operator,modules/cluster-authentication-operator,modules/cluster-autoscaler-operator,modules/cluster-cloud-controller-manager-operator,modules/cluster-capi-operator,modules/cluster-config-operator,modules/cluster-csi-snapshot-controller-operator,modules/cluster-image-registry-operator,modules/cluster-machine-approver-operator,modules/cluster-monitoring-operator,modules/cluster-network-operator,modules/cluster-samples-operator,modules/cluster-storage-operator,modules/cluster-version-operator,modules/console-operator,modules/control-plane-machine-set-operator,modules/cluster-dns-operator,modules/etcd-operator,modules/ingress-operator,modules/insights-operator,modules/kube-apiserver-operator,modules/kube-controller-manager-operator,modules/cluster-kube-scheduler-operator,modules/cluster-kube-storage-version-migrator-operator,modules/machine-api-operator,modules/machine-config-operator,modules/snippets/mcs-endpoint-limitation,modules/operator-marketplace,modules/node-tuning-operator,modules/openshift-apiserver-operator,modules/cluster-openshift-controller-manager-operators,modules/olm-overview,modules/olm-architecture,modules/olm-arch-olm-operator,modules/olm-arch-catalog-operator,modules/olm-arch-catalog-registry,modules/openshift-service-ca-operator,modules/vsphere-problem-detector-operator

:_mod-docs-content-type: ASSEMBLY
[id="olm-managing-custom-catalogs"]
= Managing custom catalogs
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: olm-managing-custom-catalogs

toc::[]

Cluster administrators
and Operator catalog maintainers can create and manage custom catalogs packaged using the xref:../../operators/understanding/olm-packaging-format.adoc#olm-bundle-format_olm-packaging-format[bundle format] on Operator Lifecycle Manager (OLM) in {product-title}.

[IMPORTANT]
====
Kubernetes periodically deprecates certain APIs that are removed in subsequent releases. As a result, Operators are unable to use removed APIs starting with the version of {product-title} that uses the Kubernetes version that removed the API.

If your cluster is using custom catalogs, see xref:../../operators/operator_sdk/osdk-working-bundle-images#osdk-control-compat_osdk-working-bundle-images[Controlling Operator compatibility with {product-title} versions] for more details about how Operator authors can update their projects to help avoid workload issues and prevent incompatible upgrades.
====

[role="_additional-resources"]
.Additional resources

* xref:../../operators/understanding/olm-rh-catalogs.adoc#olm-rh-catalogs[Red Hat-provided Operator catalogs]

[id="olm-managing-custom-catalogs-bundle-format-prereqs"]
== Prerequisites

* You have installed the xref:../../cli_reference/opm/cli-opm-install.adoc#cli-opm-install[`opm` CLI].

[id="olm-managing-custom-catalogs-fb"]
== File-based catalogs

_File-based catalogs_ are the latest iteration of the catalog format in Operator Lifecycle Manager (OLM). It is a plain text-based (JSON or YAML) and declarative config evolution of the earlier SQLite database format, and it is fully backwards compatible.

[IMPORTANT]
====
As of {product-title} 4.11, the default Red Hat-provided Operator catalog releases in the file-based catalog format. The default Red Hat-provided Operator catalogs for {product-title} 4.6 through 4.10 released in the deprecated SQLite database format.

The `opm` subcommands, flags, and functionality related to the SQLite database format are also deprecated and will be removed in a future release. The features are still supported and must be used for catalogs that use the deprecated SQLite database format.

Many of the `opm` subcommands and flags for working with the SQLite database format, such as `opm index prune`, do not work with the file-based catalog format.
For more information about working with file-based catalogs, see xref:../../operators/understanding/olm-packaging-format.adoc#olm-file-based-catalogs_olm-packaging-format[Operator Framework packaging format] and xref:../../installing/disconnected_install/installing-mirroring-disconnected.adoc#installing-mirroring-disconnected[Mirroring images for a disconnected installation using the oc-mirror plugin].
====

:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:registry-image: registry.redhat.io/openshift4/ose-operator-registry:v{product-version}

:_mod-docs-content-type: PROCEDURE
[id="olm-creating-fb-catalog-image_{context}"]
= Creating a file-based catalog image

You can use the `opm` CLI to create a catalog image that uses the plain text _file-based catalog_ format (JSON or YAML), which replaces the deprecated SQLite database format.

.Prerequisites

* You have installed the `opm` CLI.
* You have `podman` version 1.9.3+.
* A bundle image is built and pushed to a registry that supports link:https://docs.docker.com/registry/spec/manifest-v2-2/[Docker v2-2].

.Procedure

. Initialize the catalog:

.. Create a directory for the catalog by running the following command:
+
[source,terminal]
----
$ mkdir <catalog_dir>
----

.. Generate a Dockerfile that can build a catalog image by running the `opm generate dockerfile` command:
+
[source,terminal,subs="attributes+"]
----
$ opm generate dockerfile <catalog_dir> \
    -i {registry-image} <1>
----
<1> Specify the official Red Hat base image by using the `-i` flag, otherwise the Dockerfile uses the default upstream image.
+
The Dockerfile must be in the same parent directory as the catalog directory that you created in the previous step:
+
.Example directory structure
[source,terminal]
----
. <1>
├── <catalog_dir> <2>
└── <catalog_dir>.Dockerfile <3>
----
<1> Parent directory
<2> Catalog directory
<3> Dockerfile generated by the `opm generate dockerfile` command

.. Populate the catalog with the package definition for your Operator by running the `opm init` command:
+
[source,terminal]
----
$ opm init <operator_name> \ <1>
    --default-channel=preview \ <2>
    --description=./README.md \ <3>
    --icon=./operator-icon.svg \ <4>
    --output yaml \ <5>
    > <catalog_dir>/index.yaml <6>
----
<1> Operator, or package, name
<2> Channel that subscriptions default to if unspecified
<3> Path to the Operator's `README.md` or other documentation
<4> Path to the Operator's icon
<5> Output format: JSON or YAML
<6> Path for creating the catalog configuration file
+
This command generates an `olm.package` declarative config blob in the specified catalog configuration file.

. Add a bundle to the catalog by running the `opm render` command:
+
[source,terminal]
----
$ opm render <registry>/<namespace>/<bundle_image_name>:<tag> \ <1>
    --output=yaml \
    >> <catalog_dir>/index.yaml <2>
----
<1> Pull spec for the bundle image
<2> Path to the catalog configuration file
+
[NOTE]
====
Channels must contain at least one bundle.
====

. Add a channel entry for the bundle. For example, modify the following example to your specifications, and add it to your `<catalog_dir>/index.yaml` file:
+
.Example channel entry
[source,yaml]
----
---
schema: olm.channel
package: <operator_name>
name: preview
entries:
  - name: <operator_name>.v0.1.0 <1>
----
<1> Ensure that you include the period (`.`) after `<operator_name>` but before the `v` in the version. Otherwise, the entry fails to pass the `opm validate` command.

. Validate the file-based catalog:

.. Run the `opm validate` command against the catalog directory:
+
[source,terminal]
----
$ opm validate <catalog_dir>
----

.. Check that the error code is `0`:
+
[source,terminal]
----
$ echo $?
----
+
.Example output
[source,terminal]
----
0
----

. Build the catalog image by running the `podman build` command:
+
[source,terminal]
----
$ podman build . \
    -f <catalog_dir>.Dockerfile \
    -t <registry>/<namespace>/<catalog_image_name>:<tag>
----

. Push the catalog image to a registry:

.. If required, authenticate with your target registry by running the `podman login` command:
+
[source,terminal]
----
$ podman login <registry>
----

.. Push the catalog image by running the `podman push` command:
+
[source,terminal]
----
$ podman push <registry>/<namespace>/<catalog_image_name>:<tag>
----

:!registry-image:

:leveloffset!:
[role="_additional-resources"]
.Additional resources

* xref:../../cli_reference/opm/cli-opm-ref.adoc#cli-opm-ref[`opm` CLI reference]

:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:registry-image: registry.redhat.io/openshift4/ose-operator-registry:v{product-version}

:_mod-docs-content-type: PROCEDURE
[id="olm-filtering-fbc_{context}"]
= Updating or filtering a file-based catalog image

You can use the `opm` CLI to update or filter (also known as prune) a catalog image that uses the file-based catalog format. By extracting and modifying the contents of an existing catalog image, you can update, add, or remove one or more Operator package entries from the catalog. You can then rebuild the image as an updated version of the catalog.

// This note points to a topic that's excluded from OSD and ROSA.
[NOTE]
====
Alternatively, if you already have a catalog image on a mirror registry, you can use the oc-mirror CLI plugin to automatically prune any removed images from an updated source version of that catalog image while mirroring it to the target registry.

For more information about the oc-mirror plugin and this use case, see the "Keeping your mirror registry content updated" section, and specifically the "Pruning images" subsection, of "Mirroring images for a disconnected installation using the oc-mirror plugin".
====

.Prerequisites
* You have the following on your workstation:
** The `opm` CLI.
** `podman` version 1.9.3+.
** A file-based catalog image.
** A catalog directory structure recently initialized on your workstation related to this catalog.
+
If you do not have an initialized catalog directory, create the directory and generate the Dockerfile. For more information, see the "Initialize the catalog" step from the "Creating a file-based catalog image" procedure.

.Procedure

. Extract the contents of the catalog image in YAML format to an `index.yaml` file in your catalog directory:
+
[source,terminal]
----
$ opm render <registry>/<namespace>/<catalog_image_name>:<tag> \
    -o yaml > <catalog_dir>/index.yaml
----
+
[NOTE]
====
Alternatively, you can use the `-o json` flag to output in JSON format.
====

. Modify the contents of the resulting `index.yaml` file to your specifications by updating, adding, or removing one or more Operator package entries.
+
[IMPORTANT]
====
After a bundle has been published in a catalog, assume that one of your users has installed it. Ensure that all previously published bundles in a catalog have an update path to the current or newer channel head to avoid stranding users that have that version installed.
====
+
For example, if you wanted to remove an Operator package, the following example lists a set of `olm.package`, `olm.channel`, and `olm.bundle` blobs which must be deleted to remove the package from the catalog:
+
.Example removed entries
[%collapsible]
====
[source,yaml]
----
---
defaultChannel: release-2.7
icon:
  base64data: <base64_string>
  mediatype: image/svg+xml
name: example-operator
schema: olm.package
---
entries:
- name: example-operator.v2.7.0
  skipRange: '>=2.6.0 <2.7.0'
- name: example-operator.v2.7.1
  replaces: example-operator.v2.7.0
  skipRange: '>=2.6.0 <2.7.1'
- name: example-operator.v2.7.2
  replaces: example-operator.v2.7.1
  skipRange: '>=2.6.0 <2.7.2'
- name: example-operator.v2.7.3
  replaces: example-operator.v2.7.2
  skipRange: '>=2.6.0 <2.7.3'
- name: example-operator.v2.7.4
  replaces: example-operator.v2.7.3
  skipRange: '>=2.6.0 <2.7.4'
name: release-2.7
package: example-operator
schema: olm.channel
---
image: example.com/example-inc/example-operator-bundle@sha256:<digest>
name: example-operator.v2.7.0
package: example-operator
properties:
- type: olm.gvk
  value:
    group: example-group.example.io
    kind: MyObject
    version: v1alpha1
- type: olm.gvk
  value:
    group: example-group.example.io
    kind: MyOtherObject
    version: v1beta1
- type: olm.package
  value:
    packageName: example-operator
    version: 2.7.0
- type: olm.bundle.object
  value:
    data: <base64_string>
- type: olm.bundle.object
  value:
    data: <base64_string>
relatedImages:
- image: example.com/example-inc/example-related-image@sha256:<digest>
  name: example-related-image
schema: olm.bundle
---
----
====

. Save your changes to the `index.yaml` file.

. Validate the catalog:
+
[source,terminal]
----
$ opm validate <catalog_dir>
----

. Rebuild the catalog:
+
[source,terminal]
----
$ podman build . \
    -f <catalog_dir>.Dockerfile \
    -t <registry>/<namespace>/<catalog_image_name>:<tag>
----

. Push the updated catalog image to a registry:
+
[source,terminal]
----
$ podman push <registry>/<namespace>/<catalog_image_name>:<tag>
----

.Verification

. In the web console, navigate to the OperatorHub configuration resource in the *Administration* -> *Cluster Settings* -> *Configuration* page.

. Add the catalog source or update the existing catalog source to use the pull spec for your updated catalog image.
+
For more information, see "Adding a catalog source to a cluster" in the "Additional resources" of this section.

. After the catalog source is in a *READY* state, navigate to the *Operators* -> *OperatorHub* page and check that the changes you made are reflected in the list of Operators.

:leveloffset!:
[role="_additional-resources"]
.Additional resources

* xref:../../installing/disconnected_install/installing-mirroring-disconnected.adoc#updating-mirror-registry-content[Mirroring images for a disconnected installation using the oc-mirror plugin -> Keeping your mirror registry content updated]
* xref:../../operators/admin/olm-restricted-networks.adoc#olm-creating-catalog-from-index_olm-restricted-networks[Adding a catalog source to a cluster]

[id="olm-managing-custom-catalogs-sqlite"]
== SQLite-based catalogs

:FeatureName: The SQLite database format for Operator catalogs
:leveloffset: +1

// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a deprecated feature. Deprecated functionality is still included in {product-title} and continues to be supported; however, it will be removed in a future release of this product and is not recommended for new deployments.

For the most recent list of major functionality that has been deprecated or removed within {product-title}, refer to the _Deprecated and removed features_ section of the {product-title} release notes.
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-creating-index-image_{context}"]
= Creating a SQLite-based index image

You can create an index image based on the SQLite database format by using the `opm` CLI.

.Prerequisites

* You have installed the `opm` CLI.
* You have `podman` version 1.9.3+.
* A bundle image is built and pushed to a registry that supports link:https://docs.docker.com/registry/spec/manifest-v2-2/[Docker v2-2].

.Procedure

. Start a new index:
+
[source,terminal]
----
$ opm index add \
    --bundles <registry>/<namespace>/<bundle_image_name>:<tag> \//<1>
    --tag <registry>/<namespace>/<index_image_name>:<tag> \//<2>
    [--binary-image <registry_base_image>] <3>
----
<1> Comma-separated list of bundle images to add to the index.
<2> The image tag that you want the index image to have.
<3> Optional: An alternative registry base image to use for serving the catalog.

. Push the index image to a registry.

.. If required, authenticate with your target registry:
+
[source,terminal]
----
$ podman login <registry>
----

.. Push the index image:
+
[source,terminal]
----
$ podman push <registry>/<namespace>/<index_image_name>:<tag>
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:index-image: redhat-operator-index

:_mod-docs-content-type: PROCEDURE
[id="olm-updating-index-image_{context}"]
= Updating a SQLite-based index image

After configuring OperatorHub to use a catalog source that references a custom index image,
cluster administrators
can keep the available Operators on their cluster up-to-date by adding bundle images to the index image.

You can update an existing index image using the `opm index add` command.

.Prerequisites

* You have installed the `opm` CLI.
* You have `podman` version 1.9.3+.
* An index image is built and pushed to a registry.
* You have an existing catalog source referencing the index image.

.Procedure

. Update the existing index by adding bundle images:
+
[source,terminal]
----
$ opm index add \
    --bundles <registry>/<namespace>/<new_bundle_image>@sha256:<digest> \//<1>
    --from-index <registry>/<namespace>/<existing_index_image>:<existing_tag> \//<2>
    --tag <registry>/<namespace>/<existing_index_image>:<updated_tag> \//<3>
    --pull-tool podman //<4>
----
<1> The `--bundles` flag specifies a comma-separated list of additional bundle images to add to the index.
<2> The `--from-index` flag specifies the previously pushed index.
<3> The `--tag` flag specifies the image tag to apply to the updated index image.
<4> The `--pull-tool` flag specifies the tool used to pull container images.
+
where:
+
--
`<registry>`:: Specifies the hostname of the registry, such as `quay.io` or `mirror.example.com`.
`<namespace>`:: Specifies the namespace of the registry, such as `ocs-dev` or `abc`.
`<new_bundle_image>`:: Specifies the new bundle image to add to the registry, such as `ocs-operator`.
`<digest>`:: Specifies the SHA image ID, or digest, of the bundle image, such as `c7f11097a628f092d8bad148406aa0e0951094a03445fd4bc0775431ef683a41`.
`<existing_index_image>`:: Specifies the previously pushed image, such as `abc-redhat-operator-index`.
`<existing_tag>`:: Specifies a previously pushed image tag, such as `pass:a[{product-version}]`.
`<updated_tag>`:: Specifies the image tag to apply to the updated index image, such as `pass:a[{product-version}].1`.
--
+
.Example command
[source,terminal,subs="attributes+"]
----
$ opm index add \
    --bundles quay.io/ocs-dev/ocs-operator@sha256:c7f11097a628f092d8bad148406aa0e0951094a03445fd4bc0775431ef683a41 \
    --from-index mirror.example.com/abc/abc-redhat-operator-index:{product-version} \
    --tag mirror.example.com/abc/abc-redhat-operator-index:{product-version}.1 \
    --pull-tool podman
----

. Push the updated index image:
+
[source,terminal]
----
$ podman push <registry>/<namespace>/<existing_index_image>:<updated_tag>
----

. After Operator Lifecycle Manager (OLM) automatically polls the index image referenced in the catalog source at its regular interval, verify that the new packages are successfully added:
+
[source,terminal]
----
$ oc get packagemanifests -n openshift-marketplace
----

:!index-image:

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-restricted-networks.adoc
// * operators/admin/olm-managing-custom-catalogs.adoc

:catalog-name: redhat-operators
:index-image-pullspec: registry.redhat.io/redhat/redhat-operator-index:v{product-version}
:index-image: redhat-operator-index:v{product-version}
:registry-image: registry.redhat.io/openshift4/ose-operator-registry:v4.9
:package1: advanced-cluster-management
:package2: jaeger-product
:package3: quay-operator

:_mod-docs-content-type: PROCEDURE
[id="olm-pruning-index-image_{context}"]
= Filtering a SQLite-based index image

An index image, based on the Operator bundle format, is a containerized snapshot of an Operator catalog. You can filter, or _prune_, an index of all but a specified list of packages, which creates a copy of the source index containing only the Operators that you want.


.Prerequisites

* You have `podman` version 1.9.3+.
* You have link:https://github.com/fullstorydev/grpcurl[`grpcurl`] (third-party command-line tool).
* You have installed the `opm` CLI.
* You have access to a registry that supports
link:https://docs.docker.com/registry/spec/manifest-v2-2/[Docker v2-2].

.Procedure


. Authenticate with your target registry:
+
[source,terminal]
----
$ podman login <target_registry>
----

. Determine the list of packages you want to include in your pruned index.

.. Run the source index image that you want to prune in a container. For example:
+
[source,terminal,subs="attributes+"]
----
$ podman run -p50051:50051 \
    -it {index-image-pullspec}
----
+
.Example output
[source,terminal,subs="attributes+"]
----
Trying to pull {index-image-pullspec}...
Getting image source signatures
Copying blob ae8a0c23f5b1 done
...
INFO[0000] serving registry                              database=/database/index.db port=50051
----

.. In a separate terminal session, use the `grpcurl` command to get a list of the packages provided by the index:
+
[source,terminal]
----
$ grpcurl -plaintext localhost:50051 api.Registry/ListPackages > packages.out
----

.. Inspect the `packages.out` file and identify which package names from this list you want to keep in your pruned index. For example:
+
.Example snippets of packages list
[source,text,subs="attributes+"]
----
...
{
  "name": "{package1}"
}
...
{
  "name": "{package2}"
}
...
{
{
  "name": "{package3}"
}
...
----

.. In the terminal session where you executed the `podman run` command, press kbd:[Ctrl] and kbd:[C] to stop the container process.

. Run the following command to prune the source index of all but the specified packages:
+
[source,text,subs="attributes+"]
----
$ opm index prune \
    -f {index-image-pullspec} \// <1>
    -p {package1},{package2},{package3} \// <2>
    [-i {registry-image}] \// <3>
    -t <target_registry>:<port>/<namespace>/{index-image} <4>
----
<1> Index to prune.
<2> Comma-separated list of packages to keep.
<3> Required only for {ibmpowerProductName} and {ibmzProductName} images: Operator Registry base image with the tag that matches the target {product-title} cluster major and minor version.
<4> Custom tag for new index image being built.

. Run the following command to push the new index image to your target registry:
+
[source,text,subs="attributes+"]
----
$ podman push <target_registry>:<port>/<namespace>/{index-image}
----
+
where `<namespace>` is any existing namespace on the registry.

:!catalog-name:
:!index-image-pullspec:
:!index-image:
:!registry-image:
:!package1:
:!package2:
:!package3:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:_mod-docs-content-type: CONCEPT
[id="olm-catalog-sources-and-psa_{context}"]
= Catalog sources and pod security admission

_Pod security admission_ was introduced in {product-title} 4.11 to ensure pod security standards. Catalog sources built using the SQLite-based catalog format and a version of the `opm` CLI tool released before {product-title} 4.11 cannot run under restricted pod security enforcement.

In {product-title} {product-version}, namespaces do not have restricted pod security enforcement by default and the default catalog source security mode is set to `legacy`.

Default restricted enforcement for all namespaces is planned for inclusion in a future {product-title} release. When restricted enforcement occurs, the security context of the pod specification for catalog source pods must match the restricted pod security standard. If your catalog source image requires a different pod security standard, the pod security admissions label for the namespace must be explicitly set.

[NOTE]
====
If you do not want to run your SQLite-based catalog source pods as restricted, you do not need to update your catalog source in {product-title} {product-version}.

However, it is recommended that you take action now to ensure your catalog sources run under restricted pod security enforcement. If you do not take action to ensure your catalog sources run under restricted pod security enforcement, your catalog sources might not run in future {product-title} releases.
====

As a catalog author, you can enable compatibility with restricted pod security enforcement by completing either of the following actions:

* Migrate your catalog to the file-based catalog format.
* Update your catalog image with a version of the `opm` CLI tool released with {product-title} 4.11 or later.

[NOTE]
====
The SQLite database catalog format is deprecated, but still supported by Red Hat. In a future release, the SQLite database format will not be supported, and catalogs will need to migrate to the file-based catalog format. As of {product-title} 4.11, the default Red Hat-provided Operator catalog is released in the file-based catalog format. File-based catalogs are compatible with restricted pod security enforcement.
====

If you do not want to update your SQLite database catalog image or migrate your catalog to the file-based catalog format, you can configure your catalog to run with elevated permissions.

:leveloffset!:

// This xref points to a topic that is not currently included in the OSD and ROSA docs.
[role="_additional-resources"]
.Additional resources

* xref:../../authentication/understanding-and-managing-pod-security-admission.adoc#understanding-and-managing-pod-security-admission[Understanding and managing pod security admission]

:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-migrating-sqlite-catalog-to-fbc_{context}"]
= Migrating SQLite database catalogs to the file-based catalog format

You can update your deprecated SQLite database format catalogs to the file-based catalog format.

.Prerequisites

* You have a SQLite database catalog source.
* You have access to the cluster as a user with the `cluster-admin` role.
* You have the latest version of the `opm` CLI tool released with {product-title} {product-version} on your workstation.

.Procedure

. Migrate your SQLite database catalog to a file-based catalog by running the following command:
+
[source,terminal]
----
$ opm migrate <registry_image> <fbc_directory>
----

. Generate a Dockerfile for your file-based catalog by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ opm generate dockerfile <fbc_directory> \
  --binary-image \
  registry.redhat.io/openshift4/ose-operator-registry:v{product-version}
----

.Next steps

* The generated Dockerfile can be built, tagged, and pushed to your registry.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../operators/admin/olm-managing-custom-catalogs.adoc#olm-creating-catalog-from-index_olm-managing-custom-catalogs[Adding a catalog source to a cluster]

:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-updating-sqlite-catalog-to-a-new-opm-version_{context}"]
= Rebuilding SQLite database catalog images

You can rebuild your SQLite database catalog image with the latest version of the `opm` CLI tool that is released with your version of {product-title}.

.Prerequisites

* You have a SQLite database catalog source.
* You have access to the cluster as a user with the `cluster-admin` role.
* You have the latest version of the `opm` CLI tool released with {product-title} {product-version} on your workstation.

.Procedure

* Run the following command to rebuild your catalog with a more recent version of the `opm` CLI tool:
+
[source,terminal,subs="attributes+"]
----
$ opm index add --binary-image \
  registry.redhat.io/openshift4/ose-operator-registry:v{product-version} \
  --from-index <your_registry_image> \
  --bundles "" -t \<your_registry_image>
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-sqlite-catalog-elevated-privileges_{context}"]
= Configuring catalogs to run with elevated permissions

If you do not want to update your SQLite database catalog image or migrate your catalog to the file-based catalog format, you can perform the following actions to ensure your catalog source runs when the default pod security enforcement changes to restricted:

* Manually set the catalog security mode to legacy in your catalog source definition. This action ensures your catalog runs with legacy permissions even if the default catalog security mode changes to restricted.
* Label the catalog source namespace for baseline or privileged pod security enforcement.

[NOTE]
====
The SQLite database catalog format is deprecated, but still supported by Red Hat. In a future release, the SQLite database format will not be supported, and catalogs will need to migrate to the file-based catalog format. File-based catalogs are compatible with restricted pod security enforcement.
====

.Prerequisites

* You have a SQLite database catalog source.
* You have access to the cluster as a user with the `cluster-admin` role.
* You have a target namespace that supports running pods with the elevated pod security admission standard of `baseline` or `privileged`.

.Procedure

. Edit the `CatalogSource` definition by setting the `spec.grpcPodConfig.securityContextConfig` label to `legacy`, as shown in the following example:
+
.Example `CatalogSource` definition
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-catsrc
  namespace: my-ns
spec:
  sourceType: grpc
  grpcPodConfig:
    securityContextConfig: legacy
  image: my-image:latest
----
+
[TIP]
====
In {product-title} {product-version}, the `spec.grpcPodConfig.securityContextConfig` field is set to `legacy` by default. In a future release of {product-title}, it is planned that the default setting will change to `restricted`. If your catalog cannot run under restricted enforcement, it is recommended that you manually set this field to `legacy`.
====

. Edit your `<namespace>.yaml` file to add elevated pod security admission standards to your catalog source namespace, as shown in the following example:
+
.Example `<namespace>.yaml` file
[source,yaml]
----
apiVersion: v1
kind: Namespace
metadata:
...
  labels:
    security.openshift.io/scc.podSecurityLabelSync: "false" <1>
    openshift.io/cluster-monitoring: "true"
    pod-security.kubernetes.io/enforce: baseline <2>
  name: "<namespace_name>"
----
<1> Turn off pod security label synchronization by adding the `security.openshift.io/scc.podSecurityLabelSync=false` label to the namespace.
<2> Apply the pod security admission `pod-security.kubernetes.io/enforce` label. Set the label to `baseline` or `privileged`. Use the `baseline` pod security profile unless other workloads in the namespace require a `privileged` profile.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * post_installation_configuration/preparing-for-users.adoc
// * operators/admin/olm-restricted-networks.adoc
// * operators/admin/managing-custom-catalogs.adoc

:index-image: redhat-operator-index
:tag: v{product-version}
:namespace: openshift-marketplace

:_mod-docs-content-type: PROCEDURE
[id="olm-creating-catalog-from-index_{context}"]
= Adding a catalog source to a cluster

Adding a catalog source to an {product-title} cluster enables the discovery and installation of Operators for users.
Cluster administrators
can create a `CatalogSource` object that references an index image. OperatorHub uses catalog sources to populate the user interface.

// In OSD/ROSA, a dedicated-admin can see catalog sources here, but can't add, edit, or delete them.
[TIP]
====
Alternatively, you can use the web console to manage catalog sources. From the *Administration* -> *Cluster Settings* -> *Configuration* -> *OperatorHub* page, click the *Sources* tab, where you can create, update, delete, disable, and enable individual sources.
====

// In OSD/ROSA, a dedicated-admin can update catalog sources in the console by searching for them.

.Prerequisites

* You built and pushed an index image to a registry.
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Create a `CatalogSource` object that references your index image.

.. Modify the following to your specifications and save it as a `catalogSource.yaml` file:
+
[source,yaml,subs="attributes+"]
----
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-operator-catalog
  namespace: {namespace} <1>
  annotations:
    olm.catalogImageTemplate: <2>
      "<registry>/<namespace>/<index_image_name>:v{kube_major_version}.{kube_minor_version}.{kube_patch_version}"
spec:
  sourceType: grpc
  grpcPodConfig:
    securityContextConfig: <security_mode> <3>
  image: <registry>/<namespace>/<index_image_name>:<tag> <4>
  displayName: My Operator Catalog
  publisher: <publisher_name> <5>
  updateStrategy:
    registryPoll: <6>
      interval: 30m
----
<1> If you want the catalog source to be available globally to users in all namespaces, specify the `{namespace}` namespace. Otherwise, you can specify a different namespace for the catalog to be scoped and available only for that namespace.
<2> Optional: Set the `olm.catalogImageTemplate` annotation to your index image name and use one or more of the Kubernetes cluster version variables as shown when constructing the template for the image tag.
<3> Specify the value of `legacy` or `restricted`. If the field is not set, the default value is `legacy`. In a future {product-title} release, it is planned that the default value will be `restricted`. If your catalog cannot run with `restricted` permissions, it is recommended that you manually set this field to `legacy`.
<4> Specify your index image. If you specify a tag after the image name, for example `:{tag}`, the catalog source pod uses an image pull policy of `Always`, meaning the pod always pulls the image prior to starting the container. If you specify a digest, for example `@sha256:<id>`, the image pull policy is `IfNotPresent`, meaning the pod pulls the image only if it does not already exist on the node.
<5> Specify your name or an organization name publishing the catalog.
<6> Catalog sources can automatically check for new versions to keep up to date.

.. Use the file to create the `CatalogSource` object:
+
[source,terminal]
----
$ oc apply -f catalogSource.yaml
----

. Verify the following resources are created successfully.

.. Check the pods:
+
[source,terminal,subs="attributes+"]
----
$ oc get pods -n {namespace}
----
+
.Example output
[source,terminal]
----
NAME                                    READY   STATUS    RESTARTS  AGE
my-operator-catalog-6njx6               1/1     Running   0         28s
marketplace-operator-d9f549946-96sgr    1/1     Running   0         26h
----

.. Check the catalog source:
+
[source,terminal,subs="attributes+"]
----
$ oc get catalogsource -n {namespace}
----
+
.Example output
[source,terminal]
----
NAME                  DISPLAY               TYPE PUBLISHER  AGE
my-operator-catalog   My Operator Catalog   grpc            5s
----

.. Check the package manifest:
+
[source,terminal,subs="attributes+"]
----
$ oc get packagemanifest -n {namespace}
----
+
.Example output
[source,terminal]
----
NAME                          CATALOG               AGE
jaeger-product                My Operator Catalog   93s
----

You can now install the Operators from the *OperatorHub* page on your {product-title} web console.

:!index-image:
:!tag:
:!namespace:

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../operators/understanding/olm/olm-understanding-olm.adoc#olm-catalogsource_olm-understanding-olm[Operator Lifecycle Manager concepts and resources -> Catalog source]
* xref:../../operators/admin/olm-managing-custom-catalogs.adoc#olm-accessing-images-private-registries_olm-managing-custom-catalogs[Accessing images for Operators from private registries]
// This xref may be relevant to OSD/ROSA, but the topic is not currently included in the OSD and ROSA docs.
* xref:../../openshift_images/managing_images/image-pull-policy.adoc#image-pull-policy[Image pull policy]

// Exclude from OSD/ROSA - dedicated-admins can't create the necessary secrets to do this procedure.
:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/admin/managing-custom-catalogs.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-accessing-images-private-registries_{context}"]
= Accessing images for Operators from private registries

If certain images relevant to Operators managed by Operator Lifecycle Manager (OLM) are hosted in an authenticated container image registry, also known as a private registry, OLM and OperatorHub are unable to pull the images by default. To enable access, you can create a pull secret that contains the authentication credentials for the registry. By referencing one or more pull secrets in a catalog source, OLM can handle placing the secrets in the Operator and catalog namespace to allow installation.

Other images required by an Operator or its Operands might require access to private registries as well. OLM does not handle placing the secrets in target tenant namespaces for this scenario, but authentication credentials can be added to the global cluster pull secret or individual namespace service accounts to enable the required access.

The following types of images should be considered when determining whether Operators managed by OLM have appropriate pull access:

Index images:: A `CatalogSource` object can reference an index image, which use the Operator bundle format and are catalog sources packaged as container images hosted in images registries. If an index image is hosted in a private registry, a secret can be used to enable pull access.

Bundle images:: Operator bundle images are metadata and manifests packaged as container images that represent a unique version of an Operator. If any bundle images referenced in a catalog source are hosted in one or more private registries, a secret can be used to enable pull access.

Operator and Operand images:: If an Operator installed from a catalog source uses a private image, either for the Operator image itself or one of the Operand images it watches, the Operator will fail to install because the deployment will not have access to the required registry authentication. Referencing secrets in a catalog source does not enable OLM to place the secrets in target tenant namespaces in which Operands are installed.
+
Instead, the authentication details can be added to the global cluster pull secret in the `openshift-config` namespace, which provides access to all namespaces on the cluster. Alternatively, if providing access to the entire cluster is not permissible, the pull secret can be added to the `default` service accounts of the target tenant namespaces.

.Prerequisites

* You have at least one of the following hosted in a private registry:
** An index image or catalog image.
** An Operator bundle image.
** An Operator or Operand image.
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Create a secret for each required private registry.

.. Log in to the private registry to create or update your registry credentials file:
+
[source,terminal]
----
$ podman login <registry>:<port>
----
+
[NOTE]
====
The file path of your registry credentials can be different depending on the container tool used to log in to the registry. For the `podman` CLI, the default location is `${XDG_RUNTIME_DIR}/containers/auth.json`. For the `docker` CLI, the default location is `/root/.docker/config.json`.
====

.. It is recommended to include credentials for only one registry per secret, and manage credentials for multiple registries in separate secrets. Multiple secrets can be included in a `CatalogSource` object in later steps, and {product-title} will merge the secrets into a single virtual credentials file for use during an image pull.
+
A registry credentials file can, by default, store details for more than one registry or for multiple repositories in one registry. Verify the current contents of your file. For example:
+
.File storing credentials for multiple registries
[source,json]
----
{
    "auths": {
        "registry.redhat.io": {
            "auth": "FrNHNydQXdzclNqdg=="
        },
        "quay.io": {
            "auth": "fegdsRib21iMQ=="
        },
        "https://quay.io/my-namespace/my-user/my-image": {
            "auth": "eWfjwsDdfsa221=="
        },
        "https://quay.io/my-namespace/my-user": {
            "auth": "feFweDdscw34rR=="
        },
        "https://quay.io/my-namespace": {
            "auth": "frwEews4fescyq=="
        }
    }
}
----
+
Because this file is used to create secrets in later steps, ensure that you are storing details for only one registry per file. This can be accomplished by using either of the following methods:
+
--
* Use the `podman logout <registry>` command to remove credentials for additional registries until only the one registry you want remains.
* Edit your registry credentials file and separate the registry details to be stored in multiple files. For example:
+
.File storing credentials for one registry
[source,json]
----
{
        "auths": {
                "registry.redhat.io": {
                        "auth": "FrNHNydQXdzclNqdg=="
                }
        }
}
----
+
.File storing credentials for another registry
[source,json]
----
{
        "auths": {
                "quay.io": {
                        "auth": "Xd2lhdsbnRib21iMQ=="
                }
        }
}
----
--

.. Create a secret in the `openshift-marketplace` namespace that contains the authentication credentials for a private registry:
+
[source,terminal]
----
$ oc create secret generic <secret_name> \
    -n openshift-marketplace \
    --from-file=.dockerconfigjson=<path/to/registry/credentials> \
    --type=kubernetes.io/dockerconfigjson
----
+
Repeat this step to create additional secrets for any other required private registries, updating the `--from-file` flag to specify another registry credentials file path.

. Create or update an existing `CatalogSource` object to reference one or more secrets:
+
[source,yaml]
----
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: my-operator-catalog
  namespace: openshift-marketplace
spec:
  sourceType: grpc
  secrets: <1>
  - "<secret_name_1>"
  - "<secret_name_2>"
  grpcPodConfig:
    securityContextConfig: <security_mode> <2>
  image: <registry>:<port>/<namespace>/<image>:<tag>
  displayName: My Operator Catalog
  publisher: <publisher_name>
  updateStrategy:
    registryPoll:
      interval: 30m
----
<1> Add a `spec.secrets` section and specify any required secrets.
<2> Specify the value of `legacy` or `restricted`. If the field is not set, the default value is `legacy`. In a future {product-title} release, it is planned that the default value will be `restricted`. If your catalog cannot run with `restricted` permissions, it is recommended that you manually set this field to `legacy`.

. If any Operator or Operand images that are referenced by a subscribed Operator require access to a private registry, you can either provide access to all namespaces in the cluster, or individual target tenant namespaces.

* To provide access to all namespaces in the cluster, add authentication details to the global cluster pull secret in the `openshift-config` namespace.
+
[WARNING]
====
Cluster resources must adjust to the new global pull secret, which can temporarily limit the usability of the cluster.
====

.. Extract the `.dockerconfigjson` file from the global pull secret:
+
[source,terminal]
----
$ oc extract secret/pull-secret -n openshift-config --confirm
----

.. Update the `.dockerconfigjson` file with your authentication credentials for the required private registry or registries and save it as a new file:
+
[source,terminal]
----
$ cat .dockerconfigjson | \
    jq --compact-output '.auths["<registry>:<port>/<namespace>/"] |= . + {"auth":"<token>"}' \//<1>
    > new_dockerconfigjson
----
<1> Replace `<registry>:<port>/<namespace>` with the private registry details and `<token>` with your authentication credentials.

.. Update the global pull secret with the new file:
+
[source,terminal]
----
$ oc set data secret/pull-secret -n openshift-config \
    --from-file=.dockerconfigjson=new_dockerconfigjson
----

* To update an individual namespace, add a pull secret to the service account for the Operator that requires access in the target tenant namespace.

.. Recreate the secret that you created for the `openshift-marketplace` in the tenant namespace:
+
[source,terminal]
----
$ oc create secret generic <secret_name> \
    -n <tenant_namespace> \
    --from-file=.dockerconfigjson=<path/to/registry/credentials> \
    --type=kubernetes.io/dockerconfigjson
----

.. Verify the name of the service account for the Operator by searching the tenant namespace:
+
[source,terminal]
----
$ oc get sa -n <tenant_namespace> <1>
----
<1> If the Operator was installed in an individual namespace, search that namespace. If the Operator was installed for all namespaces, search the `openshift-operators` namespace.
+
.Example output
[source,terminal]
----
NAME            SECRETS   AGE
builder         2         6m1s
default         2         6m1s
deployer        2         6m1s
etcd-operator   2         5m18s <1>
----
<1> Service account for an installed etcd Operator.

.. Link the secret to the service account for the Operator:
+
[source,terminal]
----
$ oc secrets link <operator_sa> \
    -n <tenant_namespace> \
     <secret_name> \
    --for=pull
----

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* See xref:../../cicd/builds/creating-build-inputs.adoc#builds-secrets-overview_creating-build-inputs[What is a secret?] for more information on the types of secrets, including those used for registry credentials.
* See xref:../../openshift_images/managing_images/using-image-pull-secrets.adoc#images-update-global-pull-secret_using-image-pull-secrets[Updating the global cluster pull secret] for more details on the impact of changing this secret.
* See xref:../../openshift_images/managing_images/using-image-pull-secrets.adoc#images-allow-pods-to-reference-images-from-secure-registries_using-image-pull-secrets[Allowing pods to reference images from other secured registries] for more details on linking pull secrets to service accounts per namespace.

// Exclude from OSD/ROSA - dedicated-admins can't do this procedure.
:leveloffset: +1

// Module included in the following assemblies:
//
// * installing/installing_aws/installing-restricted-networks-aws-installer-provisioned.adoc
// * installing/installing_aws/installing-restricted-networks-aws.adoc
// * installing/installing_bare_metal/installing-restricted-networks-bare-metal.adoc
// * installing/installing_gcp/installing-restricted-networks-gcp-installer-provisioned.adoc
// * installing/installing_gcp/installing-restricted-networks-gcp.adoc
// * installing/installing_ibm_power/installing-restricted-networks-ibm-power.adoc
// * installing/installing_ibm_powervs/installing-restricted-networks-ibm-power-vs.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z-kvm.adoc
// * installing/installing_ibm_z/installing-restricted-networks-ibm-z.adoc
// * installing/installing_openstack/installing-openstack-installer-restricted.adoc
// * installing/installing_platform_agnostic/installing-platform-agnostic.adoc
// * installing/installing_vmc/installing-restricted-networks-vmc-user-infra.adoc
// * installing/installing_vmc/installing-restricted-networks-vmc.adoc
// * installing/installing_vsphere/installing-restricted-networks-installer-provisioned-vsphere.adoc
// * installing/installing_vsphere/installing-restricted-networks-vsphere.adoc
// * operators/admin/olm-restricted-networks.adoc
// * operators/admin/olm-managing-custom-catalogs.adoc
// * installing/installing-restricted-networks-nutanix-installer-provisioned.adoc

:olm-managing-custom-catalogs:

:_mod-docs-content-type: PROCEDURE
[id="olm-restricted-networks-operatorhub_{context}"]
= Disabling the default OperatorHub catalog sources

Operator catalogs that source content provided by Red Hat and community projects are configured for OperatorHub by default during an {product-title} installation.
As a cluster administrator, you can disable the set of default catalogs.

.Procedure

* Disable the sources for the default catalogs by adding `disableAllDefaultSources: true` to the `OperatorHub` object:
+
[source,terminal]
----
$ oc patch OperatorHub cluster --type json \
    -p '[{"op": "add", "path": "/spec/disableAllDefaultSources", "value": true}]'
----

[TIP]
====
Alternatively, you can use the web console to manage catalog sources. From the *Administration* -> *Cluster Settings* -> *Configuration* -> *OperatorHub* page, click the *Sources* tab, where you can create, update, delete, disable, and enable individual sources.
====

:!olm-managing-custom-catalogs:

:leveloffset!:

// Removing custom catalogs can be done as a dedicated-admin, but the steps are different.
:leveloffset: +1

// Module included in the following assemblies:
//
// * operators/admin/olm-managing-custom-catalogs.adoc

// The OSD/ROSA version of this procedure is sd-olm-removing-catalogs.adoc.

:_mod-docs-content-type: PROCEDURE
[id="olm-removing-catalogs_{context}"]
= Removing custom catalogs

As a cluster administrator, you can remove custom Operator catalogs that have been previously added to your cluster by deleting the related catalog source.

.Prerequisites
* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. In the *Administrator* perspective of the web console, navigate to *Administration* -> *Cluster Settings*.

. Click the *Configuration* tab, and then click *OperatorHub*.

. Click the *Sources* tab.

. Select the *Options* menu {kebab} for the catalog that you want to remove, and then click *Delete CatalogSource*.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/olm-creating-fb-catalog-image,modules/olm-filtering-fbc,modules/deprecated-feature,modules/olm-creating-index-image,modules/olm-updating-index-image,modules/olm-pruning-index-image,modules/olm-catalog-source-and-psa,modules/olm-migrating-sqlite-catalog-to-fbc,modules/olm-updating-sqlite-catalog-to-a-new-opm-version,modules/olm-sqlite-catalog-configuring-elevated-permissions,modules/olm-creating-catalog-from-index,modules/olm-accessing-images-private-registries,modules/olm-restricted-networks-configuring-operatorhub,modules/olm-removing-catalogs

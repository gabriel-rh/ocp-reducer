:_mod-docs-content-type: ASSEMBLY
[id="admission-plug-ins"]
= Admission plugins
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: admission-plug-ins

toc::[]

// Sentence taken from Architecture -> Index.
Admission plugins are used to help regulate how {product-title} functions.

// Concept modules
:leveloffset: +1

// Module included in the following assemblies:
//
// * architecture/admission-plug-ins.adoc

:_mod-docs-content-type: CONCEPT
[id="admission-plug-ins-about_{context}"]
= About admission plugins

Admission plugins intercept requests to the master API to validate resource requests. After a request is authenticated and authorized, the admission plugins ensure that any associated policies are followed. For example, they are commonly used to enforce security policy, resource limitations or configuration requirements.

Admission plugins run in sequence as an admission chain. If any admission plugin in the sequence rejects a request, the whole chain is aborted and an error is returned.

{product-title} has a default set of admission plugins enabled for each resource type. These are required for proper functioning of the cluster. Admission plugins ignore resources that they are not responsible for.

In addition to the defaults, the admission chain can be extended dynamically through webhook admission plugins that call out to custom webhook servers. There are two types of webhook admission plugins: a mutating admission plugin and a validating admission plugin. The mutating admission plugin runs first and can both modify resources and validate requests. The validating admission plugin validates requests and runs after the mutating admission plugin so that modifications triggered by the mutating admission plugin can also be validated.

Calling webhook servers through a mutating admission plugin can produce side effects on resources related to the target object. In such situations, you must take steps to validate that the end result is as expected.

[WARNING]
====
Dynamic admission should be used cautiously because it impacts cluster control plane operations. When calling webhook servers through webhook admission plugins in {product-title} {product-version}, ensure that you have read the documentation fully and tested for side effects of mutations. Include steps to restore resources back to their original state prior to mutation, in the event that a request does not pass through the entire admission chain.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * architecture/admission-plug-ins.adoc

[id="admission-plug-ins-default_{context}"]
= Default admission plugins

//Future xref - A set of default admission plugins is enabled in {product-title} {product-version}. These default plugins contribute to fundamental control plane functionality, such as ingress policy, xref:../nodes/clusters/nodes-cluster-overcommit.adoc#nodes-cluster-resource-override_nodes-cluster-overcommit[cluster resource limit override] and quota policy.
Default validating and admission plugins are enabled in {product-title} {product-version}. These default plugins contribute to fundamental control plane functionality, such as ingress policy, cluster resource limit override and quota policy.

// Text snippet included in the following assemblies:
//
// * applications/projects/working-with-projects.adoc
// * applications/quotas/quotas-setting-across-multiple-projects.adoc
// * openshift_images/image-streams-manage.adoc
//
// Text snippet included in the following modules:
//
// * modules/admission-plug-ins-about.adoc
// * modules/creating-a-project-using-the-CLI.adoc
// * modules/creating-a-project-using-the-web-console.adoc
// * modules/images-managing-images-enabling-imagestreams-kube.adoc
// * modules/odc-creating-projects-using-developer-perspective.adoc
// * modules/rbac-default-projects.adoc
// * modules/security-context-constraints-psa-about.adoc
// * modules/security-context-constraints-rbac.adoc

:_mod-docs-content-type: SNIPPET

[IMPORTANT]
====
Do not run workloads in or share access to default projects. Default projects are reserved for running core cluster components.

The following default projects are considered highly privileged: `default`, `kube-public`, `kube-system`, `openshift`, `openshift-infra`, `openshift-node`, and other system-created projects that have the `openshift.io/run-level` label set to `0` or `1`. Functionality that relies on admission plugins, such as pod security admission, security context constraints, cluster resource quotas, and image reference resolution, does not work in highly privileged projects.
====

The following lists contain the default admission plugins:

.Validating admission plugins
[%collapsible]
====
* `LimitRanger`
* `ServiceAccount`
* `PodNodeSelector`
* `Priority`
* `PodTolerationRestriction`
* `OwnerReferencesPermissionEnforcement`
* `PersistentVolumeClaimResize`
* `RuntimeClass`
* `CertificateApproval`
* `CertificateSigning`
* `CertificateSubjectRestriction`
* `autoscaling.openshift.io/ManagementCPUsOverride`
* `authorization.openshift.io/RestrictSubjectBindings`
* `scheduling.openshift.io/OriginPodNodeEnvironment`
* `network.openshift.io/ExternalIPRanger`
* `network.openshift.io/RestrictedEndpointsAdmission`
* `image.openshift.io/ImagePolicy`
* `security.openshift.io/SecurityContextConstraint`
* `security.openshift.io/SCCExecRestrictions`
* `route.openshift.io/IngressAdmission`
* `config.openshift.io/ValidateAPIServer`
* `config.openshift.io/ValidateAuthentication`
* `config.openshift.io/ValidateFeatureGate`
* `config.openshift.io/ValidateConsole`
* `operator.openshift.io/ValidateDNS`
* `config.openshift.io/ValidateImage`
* `config.openshift.io/ValidateOAuth`
* `config.openshift.io/ValidateProject`
* `config.openshift.io/DenyDeleteClusterConfiguration`
* `config.openshift.io/ValidateScheduler`
* `quota.openshift.io/ValidateClusterResourceQuota`
* `security.openshift.io/ValidateSecurityContextConstraints`
* `authorization.openshift.io/ValidateRoleBindingRestriction`
* `config.openshift.io/ValidateNetwork`
* `operator.openshift.io/ValidateKubeControllerManager`
* `ValidatingAdmissionWebhook`
* `ResourceQuota`
* `quota.openshift.io/ClusterResourceQuota`
====


.Mutating admission plugins
[%collapsible]
====
* `NamespaceLifecycle`
* `LimitRanger`
* `ServiceAccount`
* `NodeRestriction`
* `TaintNodesByCondition`
* `PodNodeSelector`
* `Priority`
* `DefaultTolerationSeconds`
* `PodTolerationRestriction`
* `DefaultStorageClass`
* `StorageObjectInUseProtection`
* `RuntimeClass`
* `DefaultIngressClass`
* `autoscaling.openshift.io/ManagementCPUsOverride`
* `scheduling.openshift.io/OriginPodNodeEnvironment`
* `image.openshift.io/ImagePolicy`
* `security.openshift.io/SecurityContextConstraint`
* `security.openshift.io/DefaultSecurityContextConstraints`
* `MutatingAdmissionWebhook`
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * architecture/admission-plug-ins.adoc

[id="admission-webhooks-about_{context}"]
= Webhook admission plugins

In addition to {product-title} default admission plugins, dynamic admission can be implemented through webhook admission plugins that call webhook servers, to extend the functionality of the admission chain. Webhook servers are called over HTTP at defined endpoints.

There are two types of webhook admission plugins in {product-title}:

//Future xref - * During the admission process, xref:../architecture/admission-plug-ins.adoc#mutating-admission-plug-in[the mutating admission plugin] can perform tasks, such as injecting affinity labels.
* During the admission process, the _mutating admission plugin_ can perform tasks, such as injecting affinity labels.

//Future xref - * At the end of the admission process, xref:../architecture/admission-plug-ins.adoc#validating-admission-plug-in[the validating admission plugin] makes sure an object is configured properly, for example ensuring affinity labels are as expected. If the validation passes, {product-title} schedules the object as configured.
* At the end of the admission process, the _validating admission plugin_ can be used to make sure an object is configured properly, for example ensuring affinity labels are as expected. If the validation passes, {product-title} schedules the object as configured.

When an API request comes in, mutating or validating admission plugins use the list of external webhooks in the configuration and call them in parallel:

* If all of the webhooks approve the request, the admission chain continues.

* If any of the webhooks deny the request, the admission request is denied and the reason for doing so is based on the first denial.

* If more than one webhook denies the admission request, only the first denial reason is returned to the user.

* If an error is encountered when calling a webhook, the request is either denied or the webhook is ignored depending on the error policy set. If the error policy is set to `Ignore`, the request is unconditionally accepted in the event of a failure. If the policy is set to `Fail`, failed requests are denied. Using `Ignore` can result in unpredictable behavior for all clients.

//Future xrefs - Communication between the webhook admission plugin and the webhook server must use TLS. Generate a certificate authority (CA) certificate and use the certificate to sign the server certificate that is used by your webhook server. The PEM-encoded CA certificate is supplied to the webhook admission plugin using a mechanism, such as xref:../security/certificates/service-serving-certificate.adoc#service-serving-certificate[service serving certificate secrets].
Communication between the webhook admission plugin and the webhook server must use TLS. Generate a CA certificate and use the certificate to sign the server certificate that is used by your webhook admission server. The PEM-encoded CA certificate is supplied to the webhook admission plugin using a mechanism, such as service serving certificate secrets.

The following diagram illustrates the sequential admission chain process within which multiple webhook servers are called.

.API admission chain with mutating and validating admission plugins
image::api-admission-chain.png["API admission stage", align="center"]

An example webhook admission plugin use case is where all pods must have a common set of labels. In this example, the mutating admission plugin can inject labels and the validating admission plugin can check that labels are as expected. {product-title} would subsequently schedule pods that include required labels and reject those that do not.

Some common webhook admission plugin use cases include:

//Future xref - * Namespace reservation.
* Namespace reservation.
//Future xrefs - * :../networking/hardware_networks/configuring-sriov-operator.adoc#configuring-sriov-operator[Limiting custom network resources managed by the SR-IOV network device plugin].
* Limiting custom network resources managed by the SR-IOV network device plugin.
//Future xref - * xref:../nodes/scheduling/nodes-scheduler-taints-tolerations.adoc#nodes-scheduler-taints-tolerations_dedicating_nodes-scheduler-taints-tolerations[Defining tolerations that enable taints to qualify which pods should be scheduled on a node].
* Defining tolerations that enable taints to qualify which pods should be scheduled on a node.
//Future xref - * xref:../nodes/pods/nodes-pods-priority.adoc#admin-guide-priority-preemption-names_nodes-pods-priority[Pod priority class validation].
* Pod priority class validation.

[NOTE]
====
The maximum default webhook timeout value in {product-title} is 13 seconds, and it cannot be changed.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * architecture/admission-plug-ins.adoc

[id="admission-webhook-types_{context}"]
= Types of webhook admission plugins

Cluster administrators can call out to webhook servers through the mutating admission plugin or the validating admission plugin in the API server admission chain.

[id="mutating-admission-plug-in_{context}"]
== Mutating admission plugin

The mutating admission plugin is invoked during the mutation phase of the admission process, which allows modification of resource content before it is persisted. One example webhook that can be called through the mutating admission plugin is the Pod Node Selector feature, which uses an annotation on a namespace to find a label selector and add it to the pod specification.

[id="mutating-admission-plug-in-config_{context}"]
.Sample mutating admission plugin configuration

[source,yaml]
----
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration <1>
metadata:
  name: <webhook_name> <2>
webhooks:
- name: <webhook_name> <3>
  clientConfig: <4>
    service:
      namespace: default <5>
      name: kubernetes <6>
      path: <webhook_url> <7>
    caBundle: <ca_signing_certificate> <8>
  rules: <9>
  - operations: <10>
    - <operation>
    apiGroups:
    - ""
    apiVersions:
    - "*"
    resources:
    - <resource>
  failurePolicy: <policy> <11>
  sideEffects: None
----

<1> Specifies a mutating admission plugin configuration.
<2> The name for the `MutatingWebhookConfiguration` object. Replace `<webhook_name>` with the appropriate value.
<3> The name of the webhook to call. Replace `<webhook_name>` with the appropriate value.
<4> Information about how to connect to, trust, and send data to the webhook server.
<5> The namespace where the front-end service is created.
<6> The name of the front-end service.
<7> The webhook URL used for admission requests. Replace `<webhook_url>` with the appropriate value.
<8> A PEM-encoded CA certificate that signs the server certificate that is used by the webhook server.  Replace `<ca_signing_certificate>` with the appropriate certificate in base64 format.
<9> Rules that define when the API server should use this webhook admission plugin.
<10> One or more operations that trigger the API server to call this webhook admission plugin. Possible values are `create`, `update`, `delete` or `connect`. Replace `<operation>` and `<resource>` with the appropriate values.
<11> Specifies how the policy should proceed if the webhook server is unavailable.
Replace `<policy>` with either `Ignore` (to unconditionally accept the request in the event of a failure) or `Fail` (to deny the failed request). Using `Ignore` can result in unpredictable behavior for all clients.

[IMPORTANT]
====
In {product-title} {product-version}, objects created by users or control loops through a mutating admission plugin might return unexpected results, especially if values set in an initial request are overwritten, which is not recommended.
====

[id="validating-admission-plug-in_{context}"]
== Validating admission plugin

A validating admission plugin is invoked during the validation phase of the admission process. This phase allows the enforcement of invariants on particular API resources to ensure that the resource does not change again. The Pod Node Selector is also an example of a webhook which is called by the validating admission plugin, to ensure that all `nodeSelector` fields are constrained by the node selector restrictions on the namespace.

[id="validating-admission-plug-in-config_{context}"]
//http://blog.kubernetes.io/2018/01/extensible-admission-is-beta.html
.Sample validating admission plugin configuration

[source,yaml]
----
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingWebhookConfiguration <1>
metadata:
  name: <webhook_name> <2>
webhooks:
- name: <webhook_name> <3>
  clientConfig: <4>
    service:
      namespace: default  <5>
      name: kubernetes <6>
      path: <webhook_url> <7>
    caBundle: <ca_signing_certificate> <8>
  rules: <9>
  - operations: <10>
    - <operation>
    apiGroups:
    - ""
    apiVersions:
    - "*"
    resources:
    - <resource>
  failurePolicy: <policy> <11>
  sideEffects: Unknown
----

<1> Specifies a validating admission plugin configuration.
<2> The name for the `ValidatingWebhookConfiguration` object. Replace `<webhook_name>` with the appropriate value.
<3> The name of the webhook to call. Replace `<webhook_name>` with the appropriate value.
<4> Information about how to connect to, trust, and send data to the webhook server.
<5> The namespace where the front-end service is created.
<6> The name of the front-end service.
<7> The webhook URL used for admission requests. Replace `<webhook_url>` with the appropriate value.
<8> A PEM-encoded CA certificate that signs the server certificate that is used by the webhook server.  Replace `<ca_signing_certificate>` with the appropriate certificate in base64 format.
<9> Rules that define when the API server should use this webhook admission plugin.
<10> One or more operations that trigger the API server to call this webhook admission plugin. Possible values are `create`, `update`, `delete` or `connect`. Replace `<operation>` and `<resource>` with the appropriate values.
<11> Specifies how the policy should proceed if the webhook server is unavailable.
Replace `<policy>` with either `Ignore` (to unconditionally accept the request in the event of a failure) or `Fail` (to deny the failed request). Using `Ignore` can result in unpredictable behavior for all clients.

:leveloffset!:

// Procedure module
:leveloffset: +1

// Module included in the following assemblies:
//
// * architecture/admission-plug-ins.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-dynamic-admission_{context}"]
= Configuring dynamic admission

This procedure outlines high-level steps to configure dynamic admission. The functionality of the admission chain is extended by configuring a webhook admission plugin to call out to a webhook server.

The webhook server is also configured as an aggregated API server. This allows other {product-title} components to communicate with the webhook using internal credentials and facilitates testing using the `oc` command. Additionally, this enables role based access control (RBAC) into the webhook and prevents token information from other API servers from being disclosed to the webhook.

.Prerequisites

* An {product-title} account with cluster administrator access.
* The {product-title} CLI (`oc`) installed.
* A published webhook server container image.

.Procedure

. Build a webhook server container image and make it available to the cluster using an image registry.

. Create a local CA key and certificate and use them to sign the webhook server's certificate signing request (CSR).

. Create a new project for webhook resources:
+
[source,terminal]
----
$ oc new-project my-webhook-namespace  <1>
----
<1> Note that the webhook server might expect a specific name.

. Define RBAC rules for the aggregated API service in a file called `rbac.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: List
items:

- apiVersion: rbac.authorization.k8s.io/v1  <1>
  kind: ClusterRoleBinding
  metadata:
    name: auth-delegator-my-webhook-namespace
  roleRef:
    kind: ClusterRole
    apiGroup: rbac.authorization.k8s.io
    name: system:auth-delegator
  subjects:
  - kind: ServiceAccount
    namespace: my-webhook-namespace
    name: server

- apiVersion: rbac.authorization.k8s.io/v1  <2>
  kind: ClusterRole
  metadata:
    annotations:
    name: system:openshift:online:my-webhook-server
  rules:
  - apiGroups:
    - online.openshift.io
    resources:
    - namespacereservations  <3>
    verbs:
    - get
    - list
    - watch

- apiVersion: rbac.authorization.k8s.io/v1  <4>
  kind: ClusterRole
  metadata:
    name: system:openshift:online:my-webhook-requester
  rules:
  - apiGroups:
    - admission.online.openshift.io
    resources:
    - namespacereservations <5>
    verbs:
    - create

- apiVersion: rbac.authorization.k8s.io/v1  <6>
  kind: ClusterRoleBinding
  metadata:
    name: my-webhook-server-my-webhook-namespace
  roleRef:
    kind: ClusterRole
    apiGroup: rbac.authorization.k8s.io
    name: system:openshift:online:my-webhook-server
  subjects:
  - kind: ServiceAccount
    namespace: my-webhook-namespace
    name: server

- apiVersion: rbac.authorization.k8s.io/v1  <7>
  kind: RoleBinding
  metadata:
    namespace: kube-system
    name: extension-server-authentication-reader-my-webhook-namespace
  roleRef:
    kind: Role
    apiGroup: rbac.authorization.k8s.io
    name: extension-apiserver-authentication-reader
  subjects:
  - kind: ServiceAccount
    namespace: my-webhook-namespace
    name: server

- apiVersion: rbac.authorization.k8s.io/v1  <8>
  kind: ClusterRole
  metadata:
    name: my-cluster-role
  rules:
  - apiGroups:
    - admissionregistration.k8s.io
    resources:
    - validatingwebhookconfigurations
    - mutatingwebhookconfigurations
    verbs:
    - get
    - list
    - watch
  - apiGroups:
    - ""
    resources:
    - namespaces
    verbs:
    - get
    - list
    - watch

- apiVersion: rbac.authorization.k8s.io/v1
  kind: ClusterRoleBinding
  metadata:
    name: my-cluster-role
  roleRef:
    kind: ClusterRole
    apiGroup: rbac.authorization.k8s.io
    name: my-cluster-role
  subjects:
  - kind: ServiceAccount
    namespace: my-webhook-namespace
    name: server
----
<1> Delegates authentication and authorization to the webhook server API.
<2> Allows the webhook server to access cluster resources.
<3> Points to resources. This example points to the `namespacereservations` resource.
<4> Enables the aggregated API server to create admission reviews.
<5> Points to resources. This example points to the `namespacereservations` resource.
<6> Enables the webhook server to access cluster resources.
<7> Role binding to read the configuration for terminating authentication.
<8> Default cluster role and cluster role bindings for an aggregated API server.

. Apply those RBAC rules to the cluster:
+
[source,terminal]
----
$ oc auth reconcile -f rbac.yaml
----

. Create a YAML file called `webhook-daemonset.yaml` that is used to deploy a webhook as a daemon set server in a namespace:
+
[source,yaml]
----
apiVersion: apps/v1
kind: DaemonSet
metadata:
  namespace: my-webhook-namespace
  name: server
  labels:
    server: "true"
spec:
  selector:
    matchLabels:
      server: "true"
  template:
    metadata:
      name: server
      labels:
        server: "true"
    spec:
      serviceAccountName: server
      containers:
      - name: my-webhook-container  <1>
        image: <image_registry_username>/<image_path>:<tag>  <2>
        imagePullPolicy: IfNotPresent
        command:
        - <container_commands>  <3>
        ports:
        - containerPort: 8443 <4>
        volumeMounts:
        - mountPath: /var/serving-cert
          name: serving-cert
        readinessProbe:
          httpGet:
            path: /healthz
            port: 8443 <5>
            scheme: HTTPS
      volumes:
      - name: serving-cert
        secret:
          defaultMode: 420
          secretName: server-serving-cert
----
<1> Note that the webhook server might expect a specific container name.
<2> Points to a webhook server container image. Replace `<image_registry_username>/<image_path>:<tag>` with the appropriate value.
<3> Specifies webhook container run commands. Replace `<container_commands>` with the appropriate value.
<4> Defines the target port within pods. This example uses port 8443.
<5> Specifies the port used by the readiness probe. This example uses port 8443.

. Deploy the daemon set:
+
[source,terminal]
----
$ oc apply -f webhook-daemonset.yaml
----

. Define a secret for the service serving certificate signer, within a YAML file called `webhook-secret.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  namespace: my-webhook-namespace
  name: server-serving-cert
type: kubernetes.io/tls
data:
  tls.crt: <server_certificate>  <1>
  tls.key: <server_key>  <2>
----
<1> References the signed webhook server certificate. Replace `<server_certificate>` with the appropriate certificate in base64 format.
<2> References the signed webhook server key. Replace `<server_key>` with the appropriate key in base64 format.

. Create the secret:
+
[source,terminal]
----
$ oc apply -f webhook-secret.yaml
----

. Define a service account and service, within a YAML file called `webhook-service.yaml`:
+
[source,yaml]
----
apiVersion: v1
kind: List
items:

- apiVersion: v1
  kind: ServiceAccount
  metadata:
    namespace: my-webhook-namespace
    name: server

- apiVersion: v1
  kind: Service
  metadata:
    namespace: my-webhook-namespace
    name: server
    annotations:
      service.beta.openshift.io/serving-cert-secret-name: server-serving-cert
  spec:
    selector:
      server: "true"
    ports:
    - port: 443  <1>
      targetPort: 8443  <2>
----
<1> Defines the port that the service listens on. This example uses port 443.
<2> Defines the target port within pods that the service forwards connections to. This example uses port 8443.

. Expose the webhook server within the cluster:
+
[source,terminal]
----
$ oc apply -f webhook-service.yaml
----

. Define a custom resource definition for the webhook server, in a file called `webhook-crd.yaml`:
+
[source,yaml]
----
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: namespacereservations.online.openshift.io  <1>
spec:
  group: online.openshift.io  <2>
  version: v1alpha1  <3>
  scope: Cluster  <4>
  names:
    plural: namespacereservations  <5>
    singular: namespacereservation  <6>
    kind: NamespaceReservation  <7>
----
<1> Reflects `CustomResourceDefinition` `spec` values and is in the format `<plural>.<group>`. This example uses the `namespacereservations` resource.
<2> REST API group name.
<3> REST API version name.
<4> Accepted values are `Namespaced` or `Cluster`.
<5> Plural name to be included in URL.
<6> Alias seen in `oc` output.
<7> The reference for resource manifests.

. Apply the custom resource definition:
+
[source,terminal]
----
$ oc apply -f webhook-crd.yaml
----

. Configure the webhook server also as an aggregated API server, within a file called `webhook-api-service.yaml`:
+
[source,yaml]
----
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v1beta1.admission.online.openshift.io
spec:
  caBundle: <ca_signing_certificate>  <1>
  group: admission.online.openshift.io
  groupPriorityMinimum: 1000
  versionPriority: 15
  service:
    name: server
    namespace: my-webhook-namespace
  version: v1beta1
----
<1> A PEM-encoded CA certificate that signs the server certificate that is used by the webhook server. Replace `<ca_signing_certificate>` with the appropriate certificate in base64 format.

. Deploy the aggregated API service:
+
[source,terminal]
----
$ oc apply -f webhook-api-service.yaml
----

. Define the webhook admission plugin configuration within a file called `webhook-config.yaml`. This example uses the validating admission plugin:
+
[source,yaml]
----
apiVersion: admissionregistration.k8s.io/v1beta1
kind: ValidatingWebhookConfiguration
metadata:
  name: namespacereservations.admission.online.openshift.io  <1>
webhooks:
- name: namespacereservations.admission.online.openshift.io  <2>
  clientConfig:
    service:  <3>
      namespace: default
      name: kubernetes
      path: /apis/admission.online.openshift.io/v1beta1/namespacereservations  <4>
    caBundle: <ca_signing_certificate>  <5>
  rules:
  - operations:
    - CREATE
    apiGroups:
    - project.openshift.io
    apiVersions:
    - "*"
    resources:
    - projectrequests
  - operations:
    - CREATE
    apiGroups:
    - ""
    apiVersions:
    - "*"
    resources:
    - namespaces
  failurePolicy: Fail
----
<1> Name for the `ValidatingWebhookConfiguration` object. This example uses the `namespacereservations` resource.
<2> Name of the webhook to call. This example uses the `namespacereservations` resource.
<3> Enables access to the webhook server through the aggregated API.
<4> The webhook URL used for admission requests. This example uses the `namespacereservation` resource.
<5> A PEM-encoded CA certificate that signs the server certificate that is used by the webhook server. Replace `<ca_signing_certificate>` with the appropriate certificate in base64 format.

. Deploy the webhook:
+
[source,terminal]
----
$ oc apply -f webhook-config.yaml
----

. Verify that the webhook is functioning as expected. For example, if you have configured dynamic admission to reserve specific namespaces, confirm that requests to create those namespaces are rejected and that requests to create non-reserved namespaces succeed.

:leveloffset!:

[role="_additional-resources"]
[id="admission-plug-ins-additional-resources"]
== Additional resources


* xref:../nodes/scheduling/nodes-scheduler-taints-tolerations.adoc#nodes-scheduler-taints-tolerations_dedicating_nodes-scheduler-taints-tolerations[Defining tolerations that enable taints to qualify which pods should be scheduled on a node]

* xref:../nodes/pods/nodes-pods-priority.adoc#admin-guide-priority-preemption-names_nodes-pods-priority[Pod priority class validation]

//# includes=_attributes/common-attributes,modules/admission-plug-ins-about,modules/admission-plug-ins-default,modules/snippets/default-projects,modules/admission-webhooks-about,modules/admission-webhook-types,modules/configuring-dynamic-admission

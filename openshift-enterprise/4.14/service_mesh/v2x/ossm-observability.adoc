:_mod-docs-content-type: ASSEMBLY
[id="ossm-observability"]
= Metrics, logs, and traces
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: observability

toc::[]

Once you have added your application to the mesh, you can observe the data flow through your application. If you do not have your own application installed, you can see how observability works in {SMProductName} by installing the xref:../../service_mesh/v2x/prepare-to-deploy-applications-ossm.adoc#ossm-tutorial-bookinfo-overview_ossm-create-mesh[Bookinfo sample application].

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-observability-addresses_{context}"]
= Discovering console addresses

{SMProductName} provides the following consoles to view your service mesh data:

* *Kiali console* - Kiali is the management console for {SMProductName}.
* *Jaeger console* - Jaeger is the management console for {DTProductName}.
* *Grafana console* - Grafana provides mesh administrators with advanced query and metrics analysis and dashboards for Istio data. Optionally, Grafana can be used to analyze service mesh metrics.
* *Prometheus console* - {SMProductName} uses Prometheus to store telemetry information from services.

When you install the {SMProductShortName} control plane, it automatically generates routes for each of the installed components. Once you have the route address, you can access the Kiali, Jaeger, Prometheus, or Grafana console to view and manage your service mesh data.

.Prerequisite

* The component must be enabled and installed.  For example, if you did not install distributed tracing, you will not be able to access the Jaeger console.

.Procedure from OpenShift console

. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Networking* -> *Routes*.

. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+
. If necessary, use the filter to find the component console whose route you want to access.  Click the route *Location* to launch the console.

. Click *Log In With OpenShift*.

.Procedure from the CLI
. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. Switch to the {SMProductShortName} control plane project. In this example, `istio-system` is the {SMProductShortName} control plane project.  Run the following command:
+
[source,terminal]
----
$ oc project istio-system
----
+
. To get the routes for the various {SMProductName} consoles, run the folowing command:
+
[source,terminal]
----
$ oc get routes
----
+
This command returns the URLs for the Kiali, Jaeger, Prometheus, and Grafana web consoles, and any other routes in your service mesh. You should see output similar to the following:
+

[source,terminal]
----
NAME                    HOST/PORT                         SERVICES              PORT    TERMINATION
bookinfo-gateway        bookinfo-gateway-yourcompany.com  istio-ingressgateway          http2
grafana                 grafana-yourcompany.com           grafana               <all>   reencrypt/Redirect
istio-ingressgateway    istio-ingress-yourcompany.com     istio-ingressgateway  8080
jaeger                  jaeger-yourcompany.com            jaeger-query          <all>   reencrypt
kiali                   kiali-yourcompany.com             kiali                 20001   reencrypt/Redirect
prometheus              prometheus-yourcompany.com        prometheus            <all>   reencrypt/Redirect
----

. Copy the URL for the console you want to access from the `HOST/PORT` column into a browser to open the console.

. Click *Log In With OpenShift*.

:leveloffset!:

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
* service_mesh/v2x/ossm-troubleshooting-istio.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-accessing-kiali-console_{context}"]
= Accessing the Kiali console

You can view your application's topology, health, and metrics in the Kiali console. If your service is experiencing problems, the Kiali console lets you view the data flow through your service. You can view insights about the mesh components at different levels, including abstract applications, services, and workloads. Kiali also provides an interactive graph view of your namespace in real time.

To access the Kiali console you must have {SMProductName} installed, Kiali installed and configured.

The installation process creates a route to access the Kiali console.

If you know the URL for the Kiali console, you can access it directly.  If you do not know the URL, use the following directions.

.Procedure for administrators

. Log in to the {product-title} web console with an administrator role.

. Click *Home* -> *Projects*.

. On the *Projects* page, if necessary, use the filter to find the name of your project.

. Click the name of your project, for example, `bookinfo`.

. On the *Project details* page, in the *Launcher* section, click the *Kiali* link.

. Log in to the Kiali console with the same user name and password that you use to access the {product-title} console.
+
When you first log in to the Kiali Console, you see the *Overview* page which displays all the namespaces in your service mesh that you have permission to view.
+
If you are validating the console installation and namespaces have not yet been added to the mesh, there might not be any data to display other than `istio-system`.

.Procedure for developers

. Log in to the {product-title} web console with a developer role.

. Click *Project*.

. On the *Project Details* page, if necessary, use the filter to find the name of your project.

. Click the name of your project, for example, `bookinfo`.

. On the *Project* page, in the *Launcher* section, click the *Kiali* link.

. Click *Log In With OpenShift*.

:leveloffset!:

:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v1x/ossm-observability.adoc
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-observability-visual_{context}"]
= Viewing service mesh data in the Kiali console

The Kiali Graph offers a powerful visualization of your mesh traffic. The topology combines real-time request traffic with your Istio configuration information to present immediate insight into the behavior of your service mesh, letting you quickly pinpoint issues. Multiple Graph Types let you visualize traffic as a high-level service topology, a low-level workload topology, or as an application-level topology.

There are several graphs to choose from:

* The *App graph* shows an aggregate workload for all applications that are labeled the same.

* The *Service graph* shows a node for each service in your mesh but excludes all applications and workloads from the graph. It provides a high level view and aggregates all traffic for defined services.

* The *Versioned App graph* shows a node for each version of an application. All versions of an application are grouped together.

* The *Workload graph* shows a node for each workload in your service mesh. This graph does not require you to use the application and version labels. If your application does not use version labels, use this the graph.

Graph nodes are decorated with a variety of information, pointing out various route routing options like virtual services and service entries, as well as special configuration like fault-injection and circuit breakers. It can identify mTLS issues, latency issues, error traffic and more. The Graph is highly configurable, can show traffic animation, and has powerful Find and Hide abilities.

Click the *Legend* button to view information about the shapes, colors, arrows, and badges displayed in the graph.

To view a summary of metrics, select any node or edge in the graph to display its metric details in the summary details panel.

[id="ossm-observability-topology_{context}"]
== Changing graph layouts in Kiali

The layout for the Kiali graph can render differently depending on your application architecture and the data to display. For example, the number of graph nodes and their interactions can determine how the Kiali graph is rendered. Because it is not possible to create a single layout that renders nicely for every situation, Kiali offers a choice of several different layouts.

.Prerequisites

*  If you do not have your own application installed, install the Bookinfo sample application.  Then generate traffic for the Bookinfo application by entering the following command several times.
+
[source,terminal]
----
$ curl "http://$GATEWAY_URL/productpage"
----
+
This command simulates a user visiting the `productpage` microservice of the application.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.

. In Kiali console, click *Graph* to view a namespace graph.

. From the *Namespace* menu, select your application namespace, for example, `bookinfo`.

. To choose a different graph layout, do either or both of the following:

* Select different graph data groupings from the menu at the top of the graph.

** App graph
** Service graph
** Versioned App graph (default)
** Workload graph

* Select a different graph layout from the Legend at the bottom of the graph.
** Layout default dagre
** Layout 1 cose-bilkent
** Layout 2 cola

:leveloffset!:

:leveloffset: +2

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-viewing-logs_{context}"]
= Viewing logs in the Kiali console

You can view logs for your workloads in the Kiali console.  The *Workload Detail* page includes a *Logs* tab which displays a unified logs view that displays both application and proxy logs. You can select how often you want the log display in Kiali to be refreshed.

To change the logging level on the logs displayed in Kiali, you change the logging configuration for the workload or the proxy.

.Prerequisites

* Service Mesh installed and configured.
* Kiali installed and configured.
* The address for the Kiali console.
* Application or Bookinfo sample application added to the mesh.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.
+
The Kiali Overview page displays namespaces that have been added to the mesh that you have permissions to view.
+
. Click *Workloads*.

. On the *Workloads* page, select the project from the *Namespace* menu.

. If necessary, use the filter to find the workload whose logs you want to view.  Click the workload *Name*.  For example, click *ratings-v1*.

. On the *Workload Details* page, click the *Logs* tab to view the logs for the workload.

[TIP]
====
If you do not see any log entries, you may need to adjust either the Time Range or the Refresh interval.
====

:leveloffset!:

:leveloffset: +2

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-viewing-metrics_{context}"]
= Viewing metrics in the Kiali console

You can view inbound and outbound metrics for your applications, workloads, and services in the Kiali console.  The Detail pages include the following tabs:

* inbound Application metrics
* outbound Application metrics
* inbound Workload metrics
* outbound Workload metrics
* inbound Service metrics

These tabs display predefined metrics dashboards, tailored to the relevant application, workload or service level. The application and workload detail views show request and response metrics such as volume, duration, size, or TCP traffic. The service detail view shows request and response metrics for inbound traffic only.

Kiali lets you customize the charts by choosing the charted dimensions. Kiali can also present metrics reported by either source or destination proxy metrics. And for troubleshooting, Kiali can overlay trace spans on the metrics.

.Prerequisites

* Service Mesh installed and configured.
* Kiali installed and configured.
* The address for the Kiali console.
* (Optional) Distributed tracing installed and configured.

.Procedure

. Launch the Kiali console.

. Click *Log In With OpenShift*.
+
The Kiali Overview page displays namespaces that have been added to the mesh that you have permissions to view.
+
. Click either *Applications*, *Workloads*, or *Services*.

. On the *Applications*, *Workloads*, or *Services* page, select the project from the *Namespace* menu.

. If necessary, use the filter to find the application, workload, or service whose logs you want to view.  Click the *Name*.

. On the *Application Detail*, *Workload Details*, or *Service Details* page, click either the *Inbound Metrics* or *Outbound Metrics* tab to view the metrics.

:leveloffset!:

:leveloffset: +1

////
This module is included in the following assemblies:
* service_mesh/v1x/ossm-config.adoc
* service_mesh/v2x/ossm-observability.adoc
////

[id="ossm-overview-distr-tracing_{context}"]
= Distributed tracing

Distributed tracing is the process of tracking the performance of individual services in an application by tracing the path of the service calls in the application. Each time a user takes action in an application, a request is executed that might require many services to interact to produce a response. The path of this request is called a distributed transaction.

{SMProductName} uses {DTProductName} to allow developers to view call flows in a microservice application.

:leveloffset!:

:leveloffset: +2

////
This module is included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-config-external-jaeger_{context}"]
= Connecting an existing distributed tracing instance

If you already have an existing {JaegerName} instance in {product-title}, you can configure your `ServiceMeshControlPlane` resource to use that instance for {DTShortName}.

.Prerequisites

* {DTProductName} instance installed and configured.

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane` resource, for example `basic`.

. Add the name of your {JaegerShortName} instance to the `ServiceMeshControlPlane`.
+
.. Click the *YAML* tab.
+
.. Add the name of your {JaegerShortName} instance to `spec.addons.jaeger.name` in your `ServiceMeshControlPlane` resource. In the following example, `distr-tracing-production` is the name of the {JaegerShortName} instance.
+
.Example distributed tracing configuration
[source,yaml]
----
spec:
  addons:
    jaeger:
      name: distr-tracing-production
----
+
.. Click *Save*.

. Click *Reload* to verify the `ServiceMeshControlPlane` resource was configured correctly.

:leveloffset!:

:leveloffset: +2

////
This module is included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////
:_mod-docs-content-type: PROCEDURE
[id="ossm-config-sampling_{context}"]
= Adjusting the sampling rate

A trace is an execution path between services in the service mesh. A trace is comprised of one or more spans. A span is a logical unit of work that has a name, start time, and duration. The sampling rate determines how often a trace is persisted.

The Envoy proxy sampling rate is set to sample 100% of traces in your service mesh by default. A high sampling rate consumes cluster resources and performance but is useful when debugging issues. Before you deploy {SMProductName} in production, set the value to a smaller proportion of traces. For example, set `spec.tracing.sampling` to `100` to sample 1% of traces.

Configure the Envoy proxy sampling rate as a scaled integer representing 0.01% increments.

In a basic installation, `spec.tracing.sampling` is set to `10000`, which samples 100% of traces. For example:

* Setting the value to 10 samples 0.1% of traces.
* Setting the value to 500 samples 5% of traces.

[NOTE]
====
The Envoy proxy sampling rate applies for applications that are available to a Service Mesh, and use the Envoy proxy. This sampling rate determines how much data the Envoy proxy collects and tracks.

The Jaeger remote sampling rate applies to applications that are external to the Service Mesh, and do not use the Envoy proxy, such as a database. This sampling rate determines how much data the distributed tracing system collects and stores.
====

.Procedure

. In the {product-title} web console, click *Operators* -> *Installed Operators*.

. Click the *Project* menu and select the project where you installed the control plane, for example *istio-system*.

. Click the {SMProductName} Operator. In the *Istio Service Mesh Control Plane* column, click the name of your `ServiceMeshControlPlane` resource, for example `basic`.

. To adjust the sampling rate, set a different value for `spec.tracing.sampling`.
+
.. Click the *YAML* tab.
+
.. Set the value for `spec.tracing.sampling` in your `ServiceMeshControlPlane` resource. In the following example, set it to `100`.
+
.Jaeger sampling example
[source,yaml]
----
spec:
  tracing:
    sampling: 100
----
+
.. Click *Save*.

. Click *Reload* to verify the `ServiceMeshControlPlane` resource was configured correctly.

:leveloffset!:

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
* service_mesh/v2x/ossm-troubleshooting-istio.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-accessing-jaeger-console_{context}"]
= Accessing the Jaeger console

To access the Jaeger console you must have {SMProductName} installed, {JaegerName} installed and configured.

The installation process creates a route to access the Jaeger console.

If you know the URL for the Jaeger console, you can access it directly.  If you do not know the URL, use the following directions.

.Procedure from OpenShift console
. Log in to the {product-title} web console as a user with cluster-admin rights. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.

. Navigate to *Networking* -> *Routes*.

. On the *Routes* page, select the {SMProductShortName} control plane project, for example `istio-system`, from the *Namespace* menu.
+
The *Location* column displays the linked address for each route.
+
. If necessary, use the filter to find the `jaeger` route.  Click the route *Location* to launch the console.

. Click *Log In With OpenShift*.


.Procedure from Kiali console

. Launch the Kiali console.

. Click *Distributed Tracing* in the left navigation pane.

. Click *Log In With OpenShift*.


.Procedure from the CLI

. Log in to the {product-title} CLI as a user with the `cluster-admin` role. If you use {product-dedicated}, you must have an account with the `dedicated-admin` role.
+
[source,terminal]
----
$ oc login --username=<NAMEOFUSER> https://<HOSTNAME>:6443
----
+
. To query for details of the route using the command line, enter the following command. In this example, `istio-system` is the {SMProductShortName} control plane namespace.
+
[source,terminal]
----
$ export JAEGER_URL=$(oc get route -n istio-system jaeger -o jsonpath='{.spec.host}')
----
+
. Launch a browser and navigate to ``\https://<JAEGER_URL>``, where `<JAEGER_URL>` is the route that you discovered in the previous step.

. Log in using the same user name and password that you use to access the {Product-title} console.

. If you have added services to the service mesh and have generated traces, you can use the filters and *Find Traces* button to search your trace data.
+
If you are validating the console installation, there is no trace data to display.

:leveloffset!:


:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-access-grafana_{context}"]
= Accessing the Grafana console

Grafana is an analytics tool you can use to view, query, and analyze your service mesh metrics. In this example, `istio-system` is the {SMProductShortName} control plane namespace. To access Grafana, do the following:

.Procedure

. Log in to the {product-title} web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click *Routes*.

. Click the link in the *Location* column for the *Grafana* row.

. Log in to the Grafana console with your {product-title} credentials.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * service_mesh/v2x/ossm-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="ossm-access-prometheus_{context}"]
= Accessing the Prometheus console

Prometheus is a monitoring and alerting tool that you can use to collect multi-dimensional data about your microservices. In this example, `istio-system` is the {SMProductShortName} control plane namespace.

.Procedure

. Log in to the {product-title} web console.

. Click the *Project* menu and select the project where you installed the {SMProductShortName} control plane, for example *istio-system*.

. Click *Routes*.

. Click the link in the *Location* column for the *Prometheus* row.

. Log in to the Prometheus console with your {product-title} credentials.

:leveloffset!:

:leveloffset: +1

////
Module included in the following assemblies:
* service_mesh/v2x/ossm-observability.adoc
////

:_mod-docs-content-type: PROCEDURE
[id="ossm-integrating-with-user-workload-monitoring_{context}"]
= Integrating with user-workload monitoring

By default, {SMProductName} (OSSM) installs the Service Mesh control plane (SMCP) with a dedicated instance of Prometheus for collecting metrics from a mesh. However, production systems need more advanced monitoring systems, like {product-title} monitoring for user-defined projects.

The following steps show how to integrate Service Mesh with user-workload monitoring.

.Prerequisites

* User-workload monitoring is enabled.
* {SMProductName} Operator 2.4 is installed.
* Kiali Operator 1.65 is installed.

.Procedure

. Create a token to Thanos for Kiali by running the following commands:
+
.. Set the `SECRET` environment variable by running the following command:
+
[source,terminal]
----
$ SECRET=`oc get secret -n openshift-user-workload-monitoring |
 grep  prometheus-user-workload-token | head -n 1 | awk '{print $1 }'`
----
+
.. Set the `TOKEN` environment variable by running the following command:
+
[source,terminal]
----
$ TOKEN=`oc get secret $SECRET -n openshift-user-workload-monitoring -o jsonpath='{.data.token}' | base64 -d`
----
+
.. Create a token to Thanos for Kiali by running the following command:
+
[source,terminal]
----
$ oc create secret generic thanos-querier-web-token -n istio-system --from-literal=token=$TOKEN
----

. Configure Kiali for user-workload monitoring:
+
[source,yaml]
----
apiVersion: kiali.io/v1alpha1
kind: Kiali
metadata:
  name: kiali-user-workload-monitoring
  namespace: istio-system
spec:
  external_services:
    istio:
      url_service_version: 'http://istiod-basic.istio-system:15014/version'
    prometheus:
      auth:
        token: secret:thanos-querier-web-token:token
        type: bearer
        use_kiali_token: false
      query_scope:
        mesh_id: "basic-istio-system"
      thanos_proxy:
        enabled: true
      url: https://thanos-querier.openshift-monitoring.svc.cluster.local:9091
  version: v1.65
----

. Configure the SMCP for external Prometheus:
+
[source,yaml]
----
apiVersion: maistra.io/v2
kind: ServiceMeshControlPlane
metadata:
  name: basic
  namespace: istio-system
spec:
  addons:
    prometheus:
      enabled: false # <1>
    grafana:
      enabled: false # <2>
    kiali:
      name: kiali-user-workload-monitoring
  meshConfig:
    extensionProviders:
    - name: prometheus
      prometheus: {}
----
<1> Disable the default Prometheus instance provided by OSSM.
<2> Disable Grafana. It is not supported with an external Prometheus instance.

. Apply a custom network policy to allow ingress traffic from the monitoring namespace:
+
[source,yaml]
----
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: user-workload-access
  namespace: bookinfo # <1>
spec:
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          network.openshift.io/policy-group: monitoring
  podSelector: {}
  policyTypes:
  - Ingress
----
<1> The custom network policy must be applied to all namespaces.

. Apply a `Telemetry` object to enable traffic metrics in Istio proxies:
+
[source,yaml]
----
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: enable-prometheus-metrics
  namespace: istio-system # <1>
spec:
  selector: # <2>
    matchLabels:
      app: bookinfo
  metrics:
  - providers:
    - name: prometheus
----
<1> A `Telemetry` object created in the control plane namespace applies to all workloads in a mesh. To apply telemetry to only one namespace, create the object in the target namespace.
<2> Optional: Setting the `selector.matchLabels` spec applies the `Telemetry` object to specific workloads in the target namespace.

. Apply a `ServiceMonitor` object to monitor the Istio control plane:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: istiod-monitor
  namespace: istio-system # <1>
spec:
  targetLabels:
  - app
  selector:
    matchLabels:
      istio: pilot
  endpoints:
  - port: http-monitoring
    interval: 30s
    relabelings:
    - action: replace
      replacement: "basic-istio-system" # <2>
      targetLabel: mesh_id
----
<1> Since {product-title} monitoring ignores the `namespaceSelector` spec in `ServiceMonitor` and `PodMonitor` objects, you must apply the `PodMonitor` object in all mesh namespaces, including the control plane namespace.
<2> The string `"basic-istio-system"` is a combination of the SMCP name and its namespace, but any label can be used as long as it is unique for every mesh using user workload monitoring in the cluster. The `spec.prometheus.query_scope` of the Kiali resource configured in Step 2 needs to match this value.
+
[NOTE]
====
If there is only one mesh using user-workload monitoring, then both the `mesh_id` relabeling and the `spec.prometheus.query_scope` field in the Kiali resource are optional (but the `query_scope` field given here should be removed if the `mesh_id` label is removed).

If there might be multiple meshes using user-workload monitoring, then both the `mesh_id` relabelings and the `spec.prometheus.query_scope` field in the Kiali resource are required so that Kiali only sees metrics from its associated mesh. If Kiali is not being deployed, applying the `mesh_id` relabeling is still recommended so that metrics from different meshes can be distinguished from one another.
====

. Apply a `PodMonitor` object to collect metrics from Istio proxies:
+
[source,yaml]
----
apiVersion: monitoring.coreos.com/v1
kind: PodMonitor
metadata:
  name: istio-proxies-monitor
  namespace: istio-system # <1>
spec:
  selector:
    matchExpressions:
    - key: istio-prometheus-ignore
      operator: DoesNotExist
  podMetricsEndpoints:
  - path: /stats/prometheus
    interval: 30s
    relabelings:
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_container_name]
      regex: "istio-proxy"
    - action: keep
      sourceLabels: [__meta_kubernetes_pod_annotationpresent_prometheus_io_scrape]
    - action: replace
      regex: (\d+);(([A-Fa-f0-9]{1,4}::?){1,7}[A-Fa-f0-9]{1,4})
      replacement: '[$2]:$1'
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: replace
      regex: (\d+);((([0-9]+?)(\.|$)){4})
      replacement: $2:$1
      sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_port,
      __meta_kubernetes_pod_ip]
      targetLabel: __address__
    - action: labeldrop
      regex: "__meta_kubernetes_pod_label_(.+)"
    - sourceLabels: [__meta_kubernetes_namespace]
      action: replace
      targetLabel: namespace
    - sourceLabels: [__meta_kubernetes_pod_name]
      action: replace
      targetLabel: pod_name
    - action: replace
      replacement: "basic-istio-system" # <2>
      targetLabel: mesh_id
----
<1> Since {product-title} monitoring ignores the `namespaceSelector` spec in `ServiceMonitor` and `PodMonitor` objects, you must apply the `PodMonitor` object in all mesh namespaces, including the control plane namespace.
<2> The string `"basic-istio-system"` is a combination of the SMCP name and its namespace, but any label can be used as long as it is unique for every mesh using user workload monitoring in the cluster. The `spec.prometheus.query_scope` of the Kiali resource configured in Step 2 needs to match this value.
+
[NOTE]
====
If there is only one mesh using user-workload monitoring, then both the `mesh_id` relabeling and the `spec.prometheus.query_scope` field in the Kiali resource are optional (but the `query_scope` field given here should be removed if the `mesh_id` label is removed).

If there might be multiple meshes using user-workload monitoring, then both the `mesh_id` relabelings and the `spec.prometheus.query_scope` field in the Kiali resource are required so that Kiali only sees metrics from its associated mesh. If Kiali is not being deployed, applying the `mesh_id` relabeling is still recommended so that metrics from different meshes can be distinguished from one another.
====

. Open the {product-title} web console, and check that metrics are visible.

:leveloffset!:

[role="_additional-resources"]
[id="additional-resources_user-workload-monitoring"]
== Additional resources

* xref:../../monitoring/enabling-monitoring-for-user-defined-projects.adoc[Enabling monitoring for user-defined projects]

//# includes=_attributes/common-attributes,modules/ossm-observability-addresses,modules/ossm-kiali-accessing-console,modules/ossm-observability-visual,modules/ossm-kiali-viewing-logs,modules/ossm-kiali-viewing-metrics,modules/ossm-distr-tracing,modules/ossm-config-external-jaeger,modules/ossm-config-sampling,modules/ossm-jaeger-accessing-console,modules/ossm-access-grafana,modules/ossm-access-prometheus,modules/ossm-integrating-with-user-workload-monitoring

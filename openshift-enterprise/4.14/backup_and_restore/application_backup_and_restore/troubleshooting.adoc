:_mod-docs-content-type: ASSEMBLY
[id="troubleshooting"]
= Troubleshooting
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
// common attributes
:product-short-name: OpenShift Dedicated
:toc:
:toc-title:
:experimental:
:imagesdir: images
:OCP: OpenShift Container Platform
:ocp-version: 4.14
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:AWS: Amazon Web Services (AWS)
:GCP: Google Cloud Platform (GCP)
:product-registry: OpenShift image registry
:kebab: image:kebab.png[title="Options menu"]
:rhq-short: Red Hat Quay
:SMProductName: Red Hat OpenShift Service Mesh
:pipelines-title: Red Hat OpenShift Pipelines
:logging-sd: Red Hat OpenShift Logging
:ServerlessProductName: OpenShift Serverless
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:rhoda: Red Hat OpenShift Database Access
:rhoda-short: RHODA
:rhods: Red Hat OpenShift Data Science
:osd: OpenShift Dedicated
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
:hcp: hosted control planes
:hcp-title: ROSA with HCP
:hcp-title-first: {product-title} (ROSA) with {hcp} (HCP)
//ROSA CLI variables
:word: Testing this variable let's go www.google.com
:context: oadp-troubleshooting
:namespace: openshift-adp
:local-product: OADP
:must-gather: registry.redhat.io/oadp/oadp-mustgather-rhel8:v1.1

toc::[]

You can debug Velero custom resources (CRs) by using the xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-debugging-oc-cli_oadp-troubleshooting[OpenShift CLI tool] or the xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#migration-debugging-velero-resources_oadp-troubleshooting[Velero CLI tool]. The Velero CLI tool provides more detailed logs and information.

You can check xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-installation-issues_oadp-troubleshooting[installation issues], xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-backup-restore-cr-issues_oadp-troubleshooting[backup and restore CR issues], and xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#oadp-restic-issues_oadp-troubleshooting[Restic issues].

You can collect logs and CR information by using the xref:../../backup_and_restore/application_backup_and_restore/troubleshooting.adoc#migration-using-must-gather_oadp-troubleshooting[`must-gather` tool].

You can obtain the Velero CLI tool by:

* Downloading the Velero CLI tool
* Accessing the Velero binary in the Velero deployment in the cluster

:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="velero-obtaining-by-downloading_{context}"]
= Downloading the Velero CLI tool

You can download and install the Velero CLI tool by following the instructions on the link:https://{velero-domain}/docs/v{velero-version}/basic-install/#install-the-cli[Velero documentation page].

The page includes instructions for:

* macOS by using Homebrew
* GitHub
* Windows by using Chocolatey

.Prerequisites

* You have access to a Kubernetes cluster, v1.16 or later, with DNS and container networking enabled.
* You have installed `kubectl` locally.

.Procedure

. Open a browser and navigate to link:https://{velero-domain}/docs/v{velero-version}/basic-install/#install-the-cli["Install the CLI" on the Velero website].
. Follow the appropriate procedure for macOS, GitHub, or Windows.
. Download the Velero version appropriate for your version of OADP and {product-title}.

:leveloffset!:
:leveloffset: +2

:_mod-docs-content-type: CONCEPT
[id="velero-oadp-version-relationship_{context}"]
= OADP-Velero-{product-title} version relationship

[cols="3", options="header"]
|===
|OADP version |Velero version |{product-title} version
| 1.1.0 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.9 and later
| 1.1.1 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.9 and later
| 1.1.2 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.9 and later
| 1.1.3 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.9 and later
| 1.1.4 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.9 and later
| 1.1.5 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.9 and later
| 1.1.6 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.11 and later
| 1.1.7 | link:https://{velero-domain}/docs/v1.9]/[1.9] | 4.11 and later
| 1.2.0 | link:https://{velero-domain}/docs/v1.11/[1.11] | 4.11 and later
| 1.2.1 | link:https://{velero-domain}/docs/v1.11/[1.11] | 4.11 and later
| 1.2.2 | link:https://{velero-domain}/docs/v1.11/[1.11] | 4.11 and later
| 1.2.3 | link:https://{velero-domain}/docs/v1.11/[1.11] | 4.11 and later
|===

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="velero-obtaining-by-accessing-binary_{context}"]
= Accessing the Velero binary in the Velero deployment in the cluster

You can use a shell command to access the Velero binary in the Velero deployment in the cluster.

.Prerequisites

* Your `DataProtectionApplication` custom resource has a status of `Reconcile complete`.

.Procedure

* Enter the following command to set the needed alias:
+
[source,terminal]
----
$ alias velero='oc -n openshift-adp exec deployment/velero -c velero -it -- ./velero'
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: REFERENCE
[id="oadp-debugging-oc-cli_{context}"]
= Debugging Velero resources with the OpenShift CLI tool

You can debug a failed backup or restore by checking Velero custom resources (CRs) and the `Velero` pod log with the OpenShift CLI tool.

[discrete]
[id="oc-velero-cr_{context}"]
== Velero CRs

Use the `oc describe` command to retrieve a summary of warnings and errors associated with a `Backup` or `Restore` CR:

[source,terminal]
----
$ oc describe <velero_cr> <cr_name>
----

[discrete]
[id="oc-velero-pod-logs_{context}"]
== Velero pod logs

Use the `oc logs` command to retrieve the `Velero` pod logs:

[source,terminal]
----
$ oc logs pod/<velero>
----

[discrete]
[id="oc-velero-debug-logs_{context}"]
== Velero pod debug logs

You can specify the Velero log level in the `DataProtectionApplication` resource as shown in the following example.

[NOTE]
====
This option is available starting from OADP 1.0.3.
====

[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
  name: velero-sample
spec:
  configuration:
    velero:
      logLevel: warning
----

The following `logLevel` values are available:

* `trace`
* `debug`
* `info`
* `warning`
* `error`
* `fatal`
* `panic`

It is recommended to use `debug` for most logs.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc
// * migrating_from_ocp_3_to_4/troubleshooting-3-4.adoc
// * migration_toolkit_for_containers/troubleshooting-mtc

[id="migration-debugging-velero-resources_{context}"]
= Debugging Velero resources with the Velero CLI tool

You can debug `Backup` and `Restore` custom resources (CRs) and retrieve logs with the Velero CLI tool.

The Velero CLI tool provides more detailed information than the OpenShift CLI tool.

[discrete]
[id="velero-command-syntax_{context}"]
== Syntax

Use the `oc exec` command to run a Velero CLI command:

[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  <backup_restore_cr> <command> <cr_name>
----

.Example
[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  backup describe 0e44ae00-5dc3-11eb-9ca8-df7e5254778b-2d8ql
----

[discrete]
[id="velero-help-option_{context}"]
== Help option

Use the `velero --help` option to list all Velero CLI commands:

[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  --help
----

[discrete]
[id="velero-describe-command_{context}"]
== Describe command

Use the `velero describe` command to retrieve a summary of warnings and errors associated with a `Backup` or `Restore` CR:

[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  <backup_restore_cr> describe <cr_name>
----

.Example
[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  backup describe 0e44ae00-5dc3-11eb-9ca8-df7e5254778b-2d8ql
----

[discrete]
[id="velero-logs-command_{context}"]
== Logs command

Use the `velero logs` command to retrieve the logs of a `Backup` or `Restore` CR:

[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  <backup_restore_cr> logs <cr_name>
----

.Example
[source,terminal,subs="attributes+"]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  restore logs ccc7c2d0-6017-11eb-afab-85d0007f5a19-x4lbf
----

:leveloffset!:



[id="oadp-pod-crash-resource-request"]
== Pods crash or restart due to lack of memory or CPU

If a Velero or Restic pod crashes due to a lack of memory or CPU, you can set specific resource requests for either of those resources.
[role="_additional-resources"]
.Additional resources
* xref:../../backup_and_restore/application_backup_and_restore/installing/about-installing-oadp.adoc#oadp-velero-cpu-memory-requirements_about-installing-oadp[CPU and memory requirements]

:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="oadp-pod-crash-resource-request-velero_{context}"]
= Setting resource requests for a Velero pod

You can use the `configuration.velero.podConfig.resourceAllocations` specification field in the `oadp_v1alpha1_dpa.yaml` file to set specific resource requests for a `Velero` pod.

.Procedure

* Set the `cpu` and `memory` resource requests in the YAML file:
+
.Example Velero file

[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
configuration:
  velero:
    podConfig:
      resourceAllocations: <1>
        requests:
          cpu: 200m
          memory: 256Mi
----
<1> The `resourceAllocations` listed are for average usage.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="oadp-pod-crash-resource-request-retics_{context}"]
= Setting resource requests for a Restic pod

You can use the `configuration.restic.podConfig.resourceAllocations` specification field to set specific resource requests for a `Restic` pod.

.Procedure

* Set the `cpu` and `memory` resource requests in the YAML file:
+
.Example Restic file

[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
...
configuration:
  restic:
    podConfig:
      resourceAllocations: <1>
        requests:
          cpu: 1000m
          memory: 16Gi
----
<1> The `resourceAllocations` listed are for average usage.

:leveloffset!:

[IMPORTANT]
====
The values for the resource request fields must follow the same format as Kubernetes resource requirements.
Also, if you do not specify `configuration.velero.podConfig.resourceAllocations` or `configuration.restic.podConfig.resourceAllocations`, the default `resources` specification for a Velero pod or a Restic pod is as follows:

[source,yaml]
----
requests:
  cpu: 500m
  memory: 128Mi
----
====

[id="issues-with-velero-and-admission-workbooks"]
== Issues with Velero and admission webhooks

Velero has limited abilities to resolve admission webhook issues during a restore. If you have workloads with admission webhooks, you might need to use an additional Velero plugin or make changes to how you restore the workload.

Typically, workloads with admission webhooks require you to create a resource of a specific kind first. This is especially true if your workload has child resources because admission webhooks typically block child resources.

For example, creating or restoring a top-level object such as `service.serving.knative.dev` typically creates child resources automatically. If you do this first, you will not need to use Velero to create and restore these resources. This avoids the problem of child resources being blocked by an admission webhook that Velero might use.

[id="velero-restore-workarounds-for-workloads-with-admission-webhooks"]
=== Restoring workarounds for Velero backups that use admission webhooks

This section describes the additional steps required to restore resources for several types of Velero backups that use admission webhooks.

:leveloffset: +3

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc
:_mod-docs-content-type: PROCEDURE
[id="migration-debugging-velero-admission-webhooks-knative_{context}"]
= Restoring Knative resources

You might encounter problems using Velero to back up Knative resources that use admission webhooks.

You can avoid such problems by restoring the top level `Service` resource first whenever you back up and restore Knative resources that use admission webhooks.

.Procedure

* Restore the top level `service.serving.knavtive.dev Service` resource:
+
[source,terminal]
----
$ velero restore <restore_name> \
  --from-backup=<backup_name> --include-resources \
  service.serving.knavtive.dev
----

:leveloffset!:
:leveloffset: +3

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc
:_mod-docs-content-type: PROCEDURE
[id="migration-debugging-velero-admission-webhooks-ibm-appconnect_{context}"]
= Restoring IBM AppConnect resources

If you experience issues when you use Velero to a restore an IBM AppConnect resource that has an admission webhook, you can run the checks in this procedure.

.Procedure

. Check if you have any mutating admission plugins of `kind: MutatingWebhookConfiguration` in the cluster:
+
[source,terminal]
----
$ oc get mutatingwebhookconfigurations
----

. Examine the YAML file of each `kind: MutatingWebhookConfiguration` to ensure that none of its rules block creation of the objects that are experiencing issues. For more information, see link:https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.23/#rulewithoperations-v1-admissionregistration-k8s-io[the official Kubernetes documentation].

. Check that any `spec.version` in `type: Configuration.appconnect.ibm.com/v1beta1` used at backup time is supported by the installed Operator.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../../architecture/admission-plug-ins.adoc[Admission plugins]
* xref:../../architecture/admission-plug-ins.adoc#admission-webhooks-about_admission-plug-ins[Webhook admission plugins]
* xref:../../architecture/admission-plug-ins.adoc#admission-webhook-types_admission-plug-ins[Types of webhook admission plugins]

:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: CONCEPT
[id="oadp-installation-issues_{context}"]
= Installation issues

You might encounter issues caused by using invalid directories or incorrect credentials when you install the Data Protection Application.

[id="oadp-backup-location-contains-invalid-directories_{context}"]
== Backup storage contains invalid directories

The `Velero` pod log displays the error message, `Backup storage contains invalid top-level directories`.

.Cause

The object storage contains top-level directories that are not Velero directories.

.Solution

If the object storage is not dedicated to Velero, you must specify a prefix for the bucket by setting the `spec.backupLocations.velero.objectStorage.prefix` parameter in the `DataProtectionApplication` manifest.

[id="oadp-incorrect-aws-credentials_{context}"]
== Incorrect AWS credentials

The `oadp-aws-registry` pod log displays the error message, `InvalidAccessKeyId: The AWS Access Key Id you provided does not exist in our records.`

The `Velero` pod log displays the error message, `NoCredentialProviders: no valid providers in chain`.

.Cause

The `credentials-velero` file used to create the `Secret` object is incorrectly formatted.

.Solution

Ensure that the `credentials-velero` file is correctly formatted, as in the following example:

.Example `credentials-velero` file
----
[default] <1>
aws_access_key_id=AKIAIOSFODNN7EXAMPLE <2>
aws_secret_access_key=wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY
----
<1> AWS default profile.
<2> Do not enclose the values with quotation marks (`"`, `'`).

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: REFERENCE
[id="oadp-timeouts_{context}"]
= OADP timeouts

Extending a timeout allows complex or resource-intensive processes to complete successfully without premature termination. This configuration can reduce the likelihood of errors, retries, or failures.

Ensure that you balance timeout extensions in a logical manner so that you do not configure excessively long timeouts that might hide underlying issues in the process. Carefully consider and monitor an appropriate timeout value that meets the needs of the process and the overall system performance.

The following are various OADP timeouts, with instructions of how and when to implement these parameters:


:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="restic-timeout_{context}"]
= Restic timeout

`timeout` defines the Restic timeout. The default value is `1h`.

Use the Restic `timeout` for the following scenarios:

* For Restic backups with total PV data usage that is greater than 500GB.
* If backups are timing out with the following error:
+
[source,terminal]
----
level=error msg="Error backing up item" backup=velero/monitoring error="timed out waiting for all PodVolumeBackups to complete"
----

.Procedure
* Edit the values in the `spec.configuration.restic.timeout` block of the `DataProtectionApplication` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: <dpa_name>
spec:
  configuration:
    restic:
      timeout: 1h
# ...
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="velero-timeout_{context}"]
= Velereo resource timeout

`resourceTimeout` defines how long to wait for several Velero resources before timeout occurs, such as Velero custom resource definition (CRD) availability, `volumeSnapshot` deletion, and repository availability. The default is `10m`.

Use the `resourceTimeout` for the following scenarios:

* For backups with total PV data usage that is greater than 1TB. This parameter is used as a timeout value when Velero tries to clean up or delete the Container Storage Interface (CSI) snapshots, before marking the backup as complete.
** A sub-task of this cleanup tries to patch VSC and this timeout can be used for that task.
+
* To create or ensure a backup repository is ready for filesystem based backups for Restic or Kopia.
* To check if the Velero CRD is available in the cluster before restoring the custom resource (CR) or resource from the backup.

.Procedure
* Edit the values in the `spec.configuration.velero.resourceTimeout` block of the `DataProtectionApplication` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: <dpa_name>
spec:
  configuration:
    velero:
      resourceTimeout: 10m
# ...
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="datamover-timeout_{context}"]
= Data Mover timeout

`timeout` is a user-supplied timeout to complete `VolumeSnapshotBackup` and `VolumeSnapshotRestore`. The default value is `10m`.

Use the Data Mover `timeout` for the following scenarios:

* If creation of `VolumeSnapshotBackups` (VSBs) and `VolumeSnapshotRestores` (VSRs), times out after 10 minutes.
* For large scale environments with total PV data usage that is greater than 500GB. Set the timeout for `1h`.
* With the `VolumeSnapshotMover` (VSM) plugin.
* Only with OADP 1.1.x.

.Procedure
* Edit the values in the `spec.features.dataMover.timeout` block of the `DataProtectionApplication` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: <dpa_name>
spec:
  features:
    dataMover:
      timeout: 10m
# ...
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="csisnapshot-timeout_{context}"]
= CSI snapshot timeout

`CSISnapshotTimeout` specifies the time during creation to wait until  the `CSI VolumeSnapshot` status becomes `ReadyToUse`, before returning error as timeout. The default value is `10m`.

Use the `CSISnapshotTimeout`  for the following scenarios:

* With the CSI plugin.
* For very large storage volumes that may take longer than 10 minutes to snapshot. Adjust this timeout if timeouts are found in the logs.

[NOTE]
====
Typically, the default value for `CSISnapshotTimeout` does not require adjustment, because the default setting can accommodate large storage volumes.
====

.Procedure
* Edit the values in the `spec.csiSnapshotTimeout` block of the `Backup` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
 name: <backup_name>
spec:
 csiSnapshotTimeout: 10m
# ...
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="velero-default-item-operation-timeout_{context}"]
= Velereo default item operation timeout

`defaultItemOperationTimeout` defines how long to wait on asynchronous `BackupItemActions` and `RestoreItemActions` to complete before timing out. The default value is `1h`.

Use the `defaultItemOperationTimeout` for the following scenarios:

* Only with Data Mover 1.2.x.
* To specify the amount of time a particular backup or restore should wait for the Asynchronous actions to complete. In the context of OADP features, this value is used for the Asynchronous actions involved in the Container Storage Interface (CSI) Data Mover feature.
* When `defaultItemOperationTimeout` is defined in the Data Protection Application (DPA)  using the `defaultItemOperationTimeout`, it applies to both backup and restore operations. You can use `itemOperationTimeout` to define only the backup or only the restore of those CRs, as described in the following "Item operation timeout - restore", and "Item operation timeout - backup" sections.

.Procedure
* Edit the values in the `spec.configuration.velero.defaultItemOperationTimeout` block of the `DataProtectionApplication` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: oadp.openshift.io/v1alpha1
kind: DataProtectionApplication
metadata:
 name: <dpa_name>
spec:
  configuration:
    velero:
      defaultItemOperationTimeout: 1h
# ...
----


:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="item-operation-timeout-restore_{context}"]
= Item operation timeout - restore

`ItemOperationTimeout` specifies the time that is used to wait for `RestoreItemAction` operations. The default value is `1h`.

Use the restore `ItemOperationTimeout` for the following scenarios:

* Only with Data Mover 1.2.x.
* For Data Mover uploads and downloads to or from the `BackupStorageLocation`. If the restore action is not completed when the timeout is reached, it will be marked as failed. If Data Mover operations are failing due to timeout issues, because of large storage volume sizes, then this timeout setting may need to be increased.

.Procedure
* Edit the values in the `Restore.spec.itemOperationTimeout` block of the `Restore` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: velero.io/v1
kind: Restore
metadata:
 name: <restore_name>
spec:
 itemOperationTimeout: 1h
# ...
----


:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="item-operation-timeout-backup_{context}"]
= Item operation timeout - backup

`ItemOperationTimeout` specifies the time used to wait for asynchronous
`BackupItemAction` operations. The default value is `1h`.

Use the backup `ItemOperationTimeout` for the following scenarios:

* Only with Data Mover 1.2.x.
* For Data Mover uploads and downloads to or from the `BackupStorageLocation`. If the backup action is not completed when the timeout is reached, it will be marked as failed. If Data Mover operations are failing due to timeout issues, because of large storage volume sizes, then this timeout setting may need to be increased.

.Procedure
* Edit the values in the `Backup.spec.itemOperationTimeout` block of the `Backup` CR manifest, as in the following example:
+
[source,yaml]
----
apiVersion: velero.io/v1
kind: Backup
metadata:
 name: <backup_name>
spec:
 itemOperationTimeout: 1h
# ...
----


:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: CONCEPT
[id="oadp-backup-restore-cr-issues_{context}"]
= Backup and Restore CR issues

You might encounter these common issues with `Backup` and `Restore` custom resources (CRs).

[id="backup-cannot-retrieve-volume_{context}"]
== Backup CR cannot retrieve volume

The `Backup` CR displays the error message, `InvalidVolume.NotFound: The volume ‘vol-xxxx’ does not exist`.

.Cause

The persistent volume (PV) and the snapshot locations are in different regions.

.Solution

. Edit the value of the `spec.snapshotLocations.velero.config.region` key in the `DataProtectionApplication` manifest so that the snapshot location is in the same region as the PV.
. Create a new `Backup` CR.

[id="backup-cr-remains-in-progress_{context}"]
== Backup CR status remains in progress

The status of a `Backup` CR remains in the `InProgress` phase and does not complete.

.Cause

If a backup is interrupted, it cannot be resumed.

.Solution

. Retrieve the details of the `Backup` CR:
+
[source,terminal]
----
$ oc -n {namespace} exec deployment/velero -c velero -- ./velero \
  backup describe <backup>
----

. Delete the `Backup` CR:
+
[source,terminal]
----
$ oc delete backup <backup> -n openshift-adp
----
+
You do not need to clean up the backup location because a `Backup` CR in progress has not uploaded  files to object storage.

. Create a new `Backup` CR.

[id="backup-cr-remains-partiallyfailed_{context}"]
== Backup CR status remains in PartiallyFailed

The status of a `Backup` CR without Restic in use remains in the `PartiallyFailed` phase and does not complete. A snapshot of the affiliated PVC is not created.

.Cause

If the backup is created based on the CSI snapshot class, but the label is missing, CSI snapshot plugin fails to create a snapshot. As a result, the `Velero` pod logs an error similar to the following:

[source,text]
----
time="2023-02-17T16:33:13Z" level=error msg="Error backing up item" backup=openshift-adp/user1-backup-check5 error="error executing custom action (groupResource=persistentvolumeclaims, namespace=busy1, name=pvc1-user1): rpc error: code = Unknown desc = failed to get volumesnapshotclass for storageclass ocs-storagecluster-ceph-rbd: failed to get volumesnapshotclass for provisioner openshift-storage.rbd.csi.ceph.com, ensure that the desired volumesnapshot class has the velero.io/csi-volumesnapshot-class label" logSource="/remote-source/velero/app/pkg/backup/backup.go:417" name=busybox-79799557b5-vprq
----

.Solution

. Delete the `Backup` CR:
+
[source,terminal]
----
$ oc delete backup <backup> -n openshift-adp
----

. If required, clean up the stored data on the `BackupStorageLocation` to free up space.

. Apply label `velero.io/csi-volumesnapshot-class=true` to the `VolumeSnapshotClass` object:
+
[source,terminal]
----
$ oc label volumesnapshotclass/<snapclass_name> velero.io/csi-volumesnapshot-class=true
----

. Create a new `Backup` CR.

:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: CONCEPT
[id="oadp-restic-issues_{context}"]
= Restic issues

You might encounter these issues when you back up applications with Restic.

[id="restic-permission-error-nfs-root-squash-enabled_{context}"]
== Restic permission error for NFS data volumes with root_squash enabled

The `Restic` pod log displays the error message: `controller=pod-volume-backup error="fork/exec/usr/bin/restic: permission denied"`.

.Cause

If your NFS data volumes have `root_squash` enabled, `Restic` maps to `nfsnobody` and does not have permission to create backups.

.Solution

You can resolve this issue by creating a supplemental group for `Restic` and adding the group ID to the `DataProtectionApplication` manifest:

. Create a supplemental group for `Restic` on the NFS data volume.
. Set the `setgid` bit on the NFS directories so that group ownership is inherited.
. Add the `spec.configuration.restic.supplementalGroups` parameter and the group ID to the `DataProtectionApplication` manifest, as in the following example:
+
[source,yaml]
----
spec:
  configuration:
    restic:
      enable: true
      supplementalGroups:
      - <group_id> <1>
----
<1> Specify the supplemental group ID.

. Wait for the `Restic` pods to restart so that the changes are applied.

[id="restic-backup-cannot-be-recreated-after-s3-bucket-emptied_{context}"]
== Restic Backup CR cannot be recreated after bucket is emptied

If you create a Restic `Backup` CR for a namespace, empty the object storage bucket, and then recreate the `Backup` CR for the same namespace, the recreated `Backup` CR fails.

The `velero` pod log displays the following error message: `stderr=Fatal: unable to open config file: Stat: The specified key does not exist.\nIs there a repository at the following location?`.

.Cause

Velero does not recreate or update the Restic repository from the `ResticRepository` manifest if the Restic directories are deleted from object storage. See link:https://github.com/vmware-tanzu/velero/issues/4421[Velero issue 4421] for more information.

.Solution

* Remove the related Restic repository from the namespace by running the following command:
+
[source,terminal]
----
$ oc delete resticrepository openshift-adp <name_of_the_restic_repository>
----
+

In the following error log, `mysql-persistent` is the problematic Restic repository. The name of the repository appears in italics for clarity.
+
[source,text,options="nowrap",subs="+quotes,verbatim"]
----
 time="2021-12-29T18:29:14Z" level=info msg="1 errors
 encountered backup up item" backup=velero/backup65
 logSource="pkg/backup/backup.go:431" name=mysql-7d99fc949-qbkds
 time="2021-12-29T18:29:14Z" level=error msg="Error backing up item"
 backup=velero/backup65 error="pod volume backup failed: error running
 restic backup, stderr=Fatal: unable to open config file: Stat: The
 specified key does not exist.\nIs there a repository at the following
 location?\ns3:http://minio-minio.apps.mayap-oadp-
 veleo-1234.qe.devcluster.openshift.com/mayapvelerooadp2/velero1/
 restic/_mysql-persistent_\n: exit status 1" error.file="/remote-source/
 src/github.com/vmware-tanzu/velero/pkg/restic/backupper.go:184"
 error.function="github.com/vmware-tanzu/velero/
 pkg/restic.(*backupper).BackupPodVolumes"
 logSource="pkg/backup/backup.go:435" name=mysql-7d99fc949-qbkds
----

:leveloffset!:
:leveloffset: +2

:_mod-docs-content-type: PROCEDURE
[id="oadp-restic-restore-failing-psa-policy_{context}"]
= Restic restore partially failing on OCP 4.14 due to changed PSA policy

{ocp} 4.14 enforces a Pod Security Admission (PSA) policy that can hinder the readiness of pods during a Restic restore process. 

If a `SecurityContextConstraints` (SCC) resource is not found when a pod is created, and the PSA policy on the pod is not set up to meet the required standards, pod admission is denied. 

This issue arises due to the resource restore order of Velero.

.Sample error
[source,text]
----
\"level=error\" in line#2273: time=\"2023-06-12T06:50:04Z\"
level=error msg=\"error restoring mysql-869f9f44f6-tp5lv: pods\\\
"mysql-869f9f44f6-tp5lv\\\" is forbidden: violates PodSecurity\\\
"restricted:v1.24\\\": privil eged (container \\\"mysql\\\
" must not set securityContext.privileged=true),
allowPrivilegeEscalation != false (containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.capabilities.drop=[\\\"ALL\\\"]), seccompProfile (pod or containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.seccompProfile.type to \\\
"RuntimeDefault\\\" or \\\"Localhost\\\")\" logSource=\"/remote-source/velero/app/pkg/restore/restore.go:1388\" restore=openshift-adp/todolist-backup-0780518c-08ed-11ee-805c-0a580a80e92c\n
velero container contains \"level=error\" in line#2447: time=\"2023-06-12T06:50:05Z\"
level=error msg=\"Namespace todolist-mariadb,
resource restore error: error restoring pods/todolist-mariadb/mysql-869f9f44f6-tp5lv: pods \\\
"mysql-869f9f44f6-tp5lv\\\" is forbidden: violates PodSecurity \\\"restricted:v1.24\\\": privileged (container \\\
"mysql\\\" must not set securityContext.privileged=true),
allowPrivilegeEscalation != false (containers \\\
"restic-wait\\\",\\\"mysql\\\" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.capabilities.drop=[\\\"ALL\\\"]), seccompProfile (pod or containers \\\
"restic-wait\\\", \\\"mysql\\\" must set securityContext.seccompProfile.type to \\\
"RuntimeDefault\\\" or \\\"Localhost\\\")\"
logSource=\"/remote-source/velero/app/pkg/controller/restore_controller.go:510\"
restore=openshift-adp/todolist-backup-0780518c-08ed-11ee-805c-0a580a80e92c\n]",
----

.Solution

. In your DPA custom resource (CR), check or set the `restore-resource-priorities` field on the Velero server to ensure that `securitycontextconstraints` is listed in order before `pods` in the list of resources:
+
[source,terminal]
----
$ oc get dpa -o yaml
----
+
.Example DPA CR
[source,yaml]
----
# ...
configuration:
  restic:
    enable: true
  velero:
    args:
      restore-resource-priorities: 'securitycontextconstraints,customresourcedefinitions,namespaces,storageclasses,volumesnapshotclass.snapshot.storage.k8s.io,volumesnapshotcontents.snapshot.storage.k8s.io,volumesnapshots.snapshot.storage.k8s.io,datauploads.velero.io,persistentvolumes,persistentvolumeclaims,serviceaccounts,secrets,configmaps,limitranges,pods,replicasets.apps,clusterclasses.cluster.x-k8s.io,endpoints,services,-,clusterbootstraps.run.tanzu.vmware.com,clusters.cluster.x-k8s.io,clusterresourcesets.addons.cluster.x-k8s.io' <1>
    defaultPlugins:
    - gcp
    - openshift
----
<1> If you have an existing restore resource priority list, ensure you combine that existing list with the complete list.

. Ensure that the security standards for the application pods are aligned, as provided in link:https://access.redhat.com/solutions/7002730[Fixing PodSecurity Admission warnings for deployments], to prevent deployment warnings. If the application is not aligned with security standards, an error can occur regardless of the SCC. 

[NOTE]
====
This solution is temporary, and ongoing discussions are in progress to address it. 
====


[role="_additional-resources"]
.Additional resources

* link:https://access.redhat.com/solutions/7002730[Fixing PodSecurity Admission warnings for deployments]

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * migrating_from_ocp_3_to_4/troubleshooting-3-4.adoc
// * migration_toolkit_for_containers/troubleshooting-mtc.adoc
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="migration-using-must-gather_{context}"]
= Using the must-gather tool

You can collect logs, metrics, and information about {local-product} custom resources by using the `must-gather` tool.

The `must-gather` data must be attached to all customer cases.


.Prerequisites

* You must be logged in to the {product-title} cluster as a user with the `cluster-admin` role.
* You must have the OpenShift CLI (`oc`) installed.

.Procedure

. Navigate to the directory where you want to store the `must-gather` data.
. Run the `oc adm must-gather` command for one of the following data collection options:

+
[source,terminal,subs="attributes+"]
----
$ oc adm must-gather --image={must-gather}
----
+
The data is saved as `must-gather/must-gather.tar.gz`. You can upload this file to a support case on the link:https://access.redhat.com/[Red Hat Customer Portal].

+
[source,terminal,subs="attributes+"]
----
$ oc adm must-gather --image={must-gather} \
  -- /usr/bin/gather_metrics_dump
----
+
This operation can take a long time. The data is saved as `must-gather/metrics/prom_data.tar.gz`.


:leveloffset!:
:leveloffset: +1

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: CONCEPT
[id="oadp-monitoring_{context}"]
= OADP Monitoring

The {product-title} provides a monitoring stack that allows users and administrators to effectively monitor and manage their clusters, as well as monitor and analyze the workload performance of user applications and services running on the clusters, including receiving alerts if an event occurs.



:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../../monitoring/monitoring-overview.adoc#about-openshift-monitoring[Monitoring stack]

:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="oadp-monitoring-setup-monitor_{context}"]
= OADP monitoring setup

The OADP Operator leverages an OpenShift User Workload Monitoring provided by the OpenShift Monitoring Stack for retrieving metrics from the Velero service endpoint. The monitoring stack allows creating user-defined Alerting Rules or querying metrics by using the OpenShift Metrics query front end.

With enabled User Workload Monitoring, it is possible to configure and use any Prometheus-compatible third-party UI, such as Grafana, to visualize Velero metrics.

Monitoring metrics requires enabling monitoring for the user-defined projects and creating a `ServiceMonitor` resource to scrape those metrics from the already enabled OADP service endpoint that resides in the `openshift-adp` namespace.

.Prerequisites
* You have access to an {product-title} cluster using an account with `cluster-admin` permissions.
* You have created a cluster monitoring config map.

.Procedure

. Edit the `cluster-monitoring-config` `ConfigMap` object in the `openshift-monitoring` namespace:
+
[source,terminal]
----
$ oc edit configmap cluster-monitoring-config -n openshift-monitoring
----

. Add or enable the `enableUserWorkload` option in the `data` section's `config.yaml` field:
+
[source,yaml]
----
apiVersion: v1
data:
  config.yaml: |
    enableUserWorkload: true <1>
kind: ConfigMap
metadata:
# ...
----
<1> Add this option or set to `true`

. Wait a short period of time to verify the User Workload Monitoring Setup by checking if the following components are up and running in the `openshift-user-workload-monitoring` namespace:
+
[source,terminal]
----
$ oc get pods -n openshift-user-workload-monitoring
----
+
.Example output
[source,terminal]
----
NAME                                   READY   STATUS    RESTARTS   AGE
prometheus-operator-6844b4b99c-b57j9   2/2     Running   0          43s
prometheus-user-workload-0             5/5     Running   0          32s
prometheus-user-workload-1             5/5     Running   0          32s
thanos-ruler-user-workload-0           3/3     Running   0          32s
thanos-ruler-user-workload-1           3/3     Running   0          32s
----
+
. Verify the existence of the `user-workload-monitoring-config` ConfigMap in the `openshift-user-workload-monitoring`. If it exists, skip the remaining steps in this procedure.
+
[source,terminal]
----
$ oc get configmap user-workload-monitoring-config -n openshift-user-workload-monitoring
----
+
.Example output
[source,terminal]
----
Error from server (NotFound): configmaps "user-workload-monitoring-config" not found
----
+
. Create a `user-workload-monitoring-config` `ConfigMap` object for the User Workload Monitoring, and save it under the `2_configure_user_workload_monitoring.yaml` file name:
+
.Example output
[source,yaml]
+
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: user-workload-monitoring-config
  namespace: openshift-user-workload-monitoring
data:
  config.yaml: |
----
+
. Apply the `2_configure_user_workload_monitoring.yaml` file:
+
[source,terminal]
----
$ oc apply -f 2_configure_user_workload_monitoring.yaml
configmap/user-workload-monitoring-config created
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="oadp-creating-service-monitor_{context}"]
= Creating OADP service monitor

OADP provides an `openshift-adp-velero-metrics-svc` service which is created when the DPA is configured. The service monitor used by the user workload monitoring must point to the defined service.

Get details about the service by running the following commands:

.Procedure

. Ensure the `openshift-adp-velero-metrics-svc` service exists. It should contain `app.kubernetes.io/name=velero` label, which will be used as selector for the `ServiceMonitor` object.

+
[source,terminal]
----
$ oc get svc -n openshift-adp -l app.kubernetes.io/name=velero
----
+
.Example output
[source,terminal]
----
NAME                               TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
openshift-adp-velero-metrics-svc   ClusterIP   172.30.38.244   <none>        8085/TCP   1h
----
+
. Create a `ServiceMonitor` YAML file that matches the existing service label, and save the file as `3_create_oadp_service_monitor.yaml`. The service monitor is created in the `openshift-adp` namespace where the `openshift-adp-velero-metrics-svc` service resides.
+
.Example `ServiceMonitor` object
[source,yaml]
+
----
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  labels:
    app: oadp-service-monitor
  name: oadp-service-monitor
  namespace: openshift-adp
spec:
  endpoints:
  - interval: 30s
    path: /metrics
    targetPort: 8085
    scheme: http
  selector:
    matchLabels:
      app.kubernetes.io/name: "velero"
----
+
. Apply the `3_create_oadp_service_monitor.yaml` file:
+
[source,terminal]
----
$ oc apply -f 3_create_oadp_service_monitor.yaml
----
+
.Example output
[source,terminal]
----
servicemonitor.monitoring.coreos.com/oadp-service-monitor created
----

.Verification

* Confirm that the new service monitor is in an *Up* state by using the *Administrator* perspective of the {product-title} web console:
.. Navigate to the *Observe* -> *Targets* page.
.. Ensure the *Filter* is unselected or that the *User* source is selected and type `openshift-adp` in the `Text` search field.
.. Verify that the status for the *Status* for the service monitor is *Up*.
+
.OADP metrics targets

image::oadp-metrics-targets.png[OADP metrics targets]

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-alerting-rules_{context}"]
= Creating an alerting rule

The {product-title} monitoring stack allows to receive Alerts configured using Alerting Rules. To create an Alerting rule for the OADP project, use one of the Metrics which are scraped with the user workload monitoring.

.Procedure

. Create a `PrometheusRule` YAML file with the sample `OADPBackupFailing` alert and save it as `4_create_oadp_alert_rule.yaml`.
+
.Sample `OADPBackupFailing` alert
[source,yaml]
+
----
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: sample-oadp-alert
  namespace: openshift-adp
spec:
  groups:
  - name: sample-oadp-backup-alert
    rules:
    - alert: OADPBackupFailing
      annotations:
        description: 'OADP had {{$value | humanize}} backup failures over the last 2 hours.'
        summary: OADP has issues creating backups
      expr: |
        increase(velero_backup_failure_total{job="openshift-adp-velero-metrics-svc"}[2h]) > 0
      for: 5m
      labels:
        severity: warning
----
+
In this sample, the Alert displays under the following conditions:
+
* There is an increase of new failing backups during the 2 last hours that is greater than 0 and the state persists for at least 5 minutes.
* If the time of the first increase is less than 5 minutes, the Alert will be in a `Pending` state, after which it will turn into a `Firing` state.
+
. Apply the `4_create_oadp_alert_rule.yaml` file, which creates the `PrometheusRule` object in the `openshift-adp` namespace:
+
[source,terminal]
----
$ oc apply -f 4_create_oadp_alert_rule.yaml
----
+
.Example output
[source,terminal]
----
prometheusrule.monitoring.coreos.com/sample-oadp-alert created
----

.Verification
* After the Alert is triggered, you can view it in the following ways:
** In the *Developer* perspective, select the *Observe* menu.
** In the *Administrator* perspective under the *Observe* -> *Alerting* menu, select *User* in the *Filter* box. Otherwise, by default only the *Platform* Alerts are displayed.
+
.OADP backup failing alert

image::oadp-backup-failing-alert.png[OADP backup failing alert]



:leveloffset!:
[role="_additional-resources"]
.Additional resources
* xref:../../monitoring/managing-alerts.adoc#managing-alerts[Managing alerts]

:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: REFERENCE
[id="list-of-metrics_{context}"]
= List of available metrics

These are the list of metrics provided by the OADP together with their https://prometheus.io/docs/concepts/metric_types/[Types].

|===
|Metric name |Description |Type

|`kopia_content_cache_hit_bytes`
|Number of bytes retrieved from the cache
|Counter

|`kopia_content_cache_hit_count`
|Number of times content was retrieved from the cache
|Counter

|`kopia_content_cache_malformed`
|Number of times malformed content was read from the cache
|Counter

|`kopia_content_cache_miss_count`
|Number of times content was not found in the cache and fetched
|Counter

|`kopia_content_cache_missed_bytes`
|Number of bytes retrieved from the underlying storage
|Counter

|`kopia_content_cache_miss_error_count`
|Number of times content could not be found in the underlying storage
|Counter

|`kopia_content_cache_store_error_count`
|Number of times content could not be saved in the cache
|Counter

|`kopia_content_get_bytes`
|Number of bytes retrieved using `GetContent()`
|Counter

|`kopia_content_get_count`
|Number of times `GetContent()` was called
|Counter

|`kopia_content_get_error_count`
|Number of times `GetContent()` was called and the result was an error
|Counter

|`kopia_content_get_not_found_count`
|Number of times `GetContent()` was called and the result was not found
|Counter

|`kopia_content_write_bytes`
|Number of bytes passed to `WriteContent()`
|Counter

|`kopia_content_write_count`
|Number of times `WriteContent()` was called
|Counter

|`velero_backup_attempt_total`
|Total number of attempted backups
|Counter

|`velero_backup_deletion_attempt_total`
|Total number of attempted backup deletions
|Counter

|`velero_backup_deletion_failure_total`
|Total number of failed backup deletions
|Counter

|`velero_backup_deletion_success_total`
|Total number of successful backup deletions
|Counter

|`velero_backup_duration_seconds`
|Time taken to complete backup, in seconds
|Histogram

|`velero_backup_failure_total`
|Total number of failed backups
|Counter

|`velero_backup_items_errors`
|Total number of errors encountered during backup
|Gauge

|`velero_backup_items_total`
|Total number of items backed up
|Gauge

|`velero_backup_last_status`
|Last status of the backup. A value of 1 is success, 0.
|Gauge

|`velero_backup_last_successful_timestamp`
|Last time a backup ran successfully, Unix timestamp in seconds
|Gauge

|`velero_backup_partial_failure_total`
|Total number of partially failed backups
|Counter

|`velero_backup_success_total`
|Total number of successful backups
|Counter

|`velero_backup_tarball_size_bytes`
|Size, in bytes, of a backup
|Gauge

|`velero_backup_total`
|Current number of existent backups
|Gauge

|`velero_backup_validation_failure_total`
|Total number of validation failed backups
|Counter

|`velero_backup_warning_total`
|Total number of warned backups
|Counter

|`velero_csi_snapshot_attempt_total`
|Total number of CSI attempted volume snapshots
|Counter

|`velero_csi_snapshot_failure_total`
|Total number of CSI failed volume snapshots
|Counter

|`velero_csi_snapshot_success_total`
|Total number of CSI successful volume snapshots
|Counter

|`velero_restore_attempt_total`
|Total number of attempted restores
|Counter

|`velero_restore_failed_total`
|Total number of failed restores
|Counter

|`velero_restore_partial_failure_total`
|Total number of partially failed restores
|Counter

|`velero_restore_success_total`
|Total number of successful restores
|Counter

|`velero_restore_total`
|Current number of existent restores
|Gauge

|`velero_restore_validation_failed_total`
|Total number of failed restores failing validations
|Counter

|`velero_volume_snapshot_attempt_total`
|Total number of attempted volume snapshots
|Counter

|`velero_volume_snapshot_failure_total`
|Total number of failed volume snapshots
|Counter

|`velero_volume_snapshot_success_total`
|Total number of successful volume snapshots
|Counter

|===


:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/application_backup_and_restore/troubleshooting.adoc

:_mod-docs-content-type: PROCEDURE
[id="viewing-metrics-observe-ui_{context}"]
= Viewing metrics using the Observe UI

You can view metrics in the {product-title} web console from the *Administrator* or *Developer* perspective, which must have access to the `openshift-adp` project.

.Procedure

* Navigate to the *Observe* -> *Metrics* page:
** If you are using the *Developer* perspective, follow these steps:
.. Select *Custom query*, or click on the *Show PromQL* link.
.. Type the query and click *Enter*.
** If you are using the *Administrator* perspective, type the expression in the text field and select *Run Queries*.
+
.OADP metrics query
image::oadp-metrics-query.png[OADP metrics query]




:leveloffset!:

:!oadp-troubleshooting:

//# includes=_attributes/common-attributes,_attributes/attributes-openshift-dedicated,modules/velero-obtaining-by-downloading,modules/velero-oadp-version-relationship,modules/velero-obtaining-by-accessing-binary,modules/oadp-debugging-oc-cli,modules/migration-debugging-velero-resources,modules/oadp-pod-crash-set-resource-request-velero,modules/oadp-pod-crash-set-resource-request-restic,modules/migration-debugging-velero-admission-webhooks-knative,modules/migration-debugging-velero-admission-webhooks-ibm-appconnect,modules/oadp-installation-issues,modules/oadp-timeouts,modules/oadp-restic-timeouts,modules/oadp-velero-timeouts,modules/oadp-datamover-timeouts,modules/oadp-csi-snapshot-timeouts,modules/oadp-velero-default-timeouts,modules/oadp-item-restore-timeouts,modules/oadp-item-backup-timeouts,modules/oadp-backup-restore-cr-issues,modules/oadp-restic-issues,modules/oadp-restic-restore-failing-psa-policy,modules/migration-using-must-gather,modules/oadp-monitoring,modules/oadp-monitoring-setup,modules/oadp-creating-service-monitor,modules/oadp-creating-alerting-rule,modules/oadp-list-of-metrics,modules/oadp-viewing-metrics-ui

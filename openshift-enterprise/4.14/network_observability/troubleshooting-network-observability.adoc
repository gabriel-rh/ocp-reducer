:_mod-docs-content-type: ASSEMBLY
[id="installing-troubleshooting"]
= Troubleshooting Network Observability
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: network-observability-troubleshooting

toc::[]

To assist in troubleshooting Network Observability issues, you can perform some troubleshooting actions.

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/network_observability/troubleshooting-network-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="network-observability-must-gather_{context}"]
= Using the must-gather tool
You can use the must-gather tool to collect information about the Network Observability Operator resources and cluster-wide resources, such as pod logs, `FlowCollector`, and `webhook` configurations.

.Procedure
. Navigate to the directory where you want to store the must-gather data.
. Run the following command to collect cluster-wide must-gather resources:
+
[source,terminal]
----
$ oc adm must-gather
 --image-stream=openshift/must-gather \
 --image=quay.io/netobserv/must-gather
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/network_observability/troubleshooting-network-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="configure-network-traffic-console_{context}"]
= Configuring network traffic menu entry in the {product-title} console

Manually configure the network traffic menu entry in the {product-title} console when the network traffic menu entry is not listed in *Observe* menu in the {product-title} console.

.Prerequisites

* You have installed {product-title} version 4.10 or newer.

.Procedure

. Check if the `spec.consolePlugin.register` field is set to `true` by running the following command:
+
[source,terminal]
----
$ oc -n netobserv get flowcollector cluster -o yaml
----
+
.Example output
----
apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  consolePlugin:
    register: false
----

. Optional: Add the `netobserv-plugin` plugin by manually editing the Console Operator config:
+
[source,terminal]
----
$ oc edit console.operator.openshift.io cluster
----
+
.Example output
----
...
spec:
  plugins:
  - netobserv-plugin
...
----

. Optional: Set the `spec.consolePlugin.register` field to `true` by running the following command:
+
[source,terminal]
----
$ oc -n netobserv edit flowcollector cluster -o yaml
----
+
.Example output
----
apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  consolePlugin:
    register: true
----

. Ensure the status of console pods is `running` by running the following command:
+
[source,terminal]
----
$ oc get pods -n openshift-console -l app=console
----

. Restart the console pods by running the following command:
+
[source,terminal]
----
$ oc delete pods -n openshift-console -l app=console
----

. Clear your browser cache and history.

. Check the status of Network Observability plugin pods by running the following command:
+
[source,terminal]
----
$ oc get pods -n netobserv -l app=netobserv-plugin
----
+
.Example output
----
NAME                                READY   STATUS    RESTARTS   AGE
netobserv-plugin-68c7bbb9bb-b69q6   1/1     Running   0          21s
----

. Check the logs of the Network Observability plugin pods by running the following command:
+
[source,terminal]
----
$ oc logs -n netobserv -l app=netobserv-plugin
----
+
.Example output
[source,terminal]
----
time="2022-12-13T12:06:49Z" level=info msg="Starting netobserv-console-plugin [build version: , build date: 2022-10-21 15:15] at log level info" module=main
time="2022-12-13T12:06:49Z" level=info msg="listening on https://:9001" module=server
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/network_observability/troubleshooting-network-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="configure-network-traffic-flowlogs-pipeline-kafka_{context}"]
= Flowlogs-Pipeline does not consume network flows after installing Kafka

If you deployed the flow collector first with `deploymentModel: KAFKA` and then deployed Kafka, the flow collector might not connect correctly to Kafka. Manually restart the flow-pipeline pods where Flowlogs-pipeline does not consume network flows from Kafka.

.Procedure

. Delete the flow-pipeline pods to restart them by running the following command:
+
[source,terminal]
----
$ oc delete pods -n netobserv -l app=flowlogs-pipeline-transformer
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/network_observability/troubleshooting-network-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="configure-network-traffic-interfaces_{context}"]
= Failing to see network flows from both `br-int` and `br-ex` interfaces

br-ex` and `br-int` are virtual bridge devices operated at OSI layer 2. The eBPF agent works at the IP and TCP levels, layers 3 and 4 respectively. You can expect that the eBPF agent captures the network traffic passing through `br-ex` and `br-int`, when the network traffic is processed by other interfaces such as physical host or virtual pod interfaces. If you restrict the eBPF agent network interfaces to attach only to `br-ex` and `br-int`, you do not see any network flow.

Manually remove the part in the `interfaces` or `excludeInterfaces` that restricts the network interfaces to `br-int` and `br-ex`.

.Procedure

. Remove the `interfaces: [ 'br-int', 'br-ex' ]` field. This allows the agent to fetch information from all the interfaces. Alternatively, you can specify the Layer-3 interface for example, `eth0`. Run the following command:
+
[source,terminal]
----
$ oc edit -n netobserv flowcollector.yaml -o yaml
----
+
.Example output
----
apiVersion: flows.netobserv.io/v1alpha1
kind: FlowCollector
metadata:
  name: cluster
spec:
  agent:
    type: EBPF
    ebpf:
      interfaces: [ 'br-int', 'br-ex' ] <1>
----
<1> Specifies the network interfaces.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * networking/network_observability/troubleshooting-network-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="controller-manager-pod-runs-out-of-memory_{context}"]
= Network Observability controller manager pod runs out of memory

You can increase memory limits for the Network Observability operator by patching the Cluster Service Version (CSV), where Network Observability controller manager pod runs out of memory.

.Procedure

. Run the following command to patch the CSV:
+
[source,terminal]
----
$ oc -n netobserv patch csv network-observability-operator.v1.0.0 --type='json' -p='[{"op": "replace", "path":"/spec/install/spec/deployments/0/spec/template/spec/containers/0/resources/limits/memory", value: "1Gi"}]'
----
+
.Example output
----
clusterserviceversion.operators.coreos.com/network-observability-operator.v1.0.0 patched
----

. Run the following command to view the updated CSV:
+
[source,terminal]
----
$ oc -n netobserv get csv network-observability-operator.v1.0.0 -o jsonpath='{.spec.install.spec.deployments[0].spec.template.spec.containers[0].resources.limits.memory}'
1Gi
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:

// * networking/network_observability/troubleshooting-network-observability.adoc

:_content-type: PROCEDURE
[id="network-observability-troubleshooting-loki-resource-exhausted_{context}"]
= Troubleshooting Loki ResourceExhausted error
Loki may return a `ResourceExhausted` error when network flow data sent by Network Observability exceeds the configured maximum message size. If you are using the Loki Operator, this maximum message size is configured to 100 MiB.

.Procedure
. Navigate to *Operators* -> *Installed Operators*, viewing *All projects* from the *Project* drop-down menu.
. In the *Provided APIs* list, select the Network Observability Operator.
. Click the *Flow Collector* then the *YAML view* tab.
.. If you are using the Loki Operator, check that the `spec.loki.batchSize` value does not exceed 98 MiB.
.. If you are using a Loki installation method that is different from the Red Hat Loki Operator, such as Grafana Loki, verify that the `grpc_server_max_recv_msg_size` link:https://grafana.com/docs/loki/latest/configure/#server[Grafana Loki server setting] is higher than the `FlowCollector` resource `spec.loki.batchSize` value. If it is not, you must either increase the `grpc_server_max_recv_msg_size` value, or decrease the `spec.loki.batchSize` value so that it is lower than the limit.
. Click *Save* if you edited the *FlowCollector*.

:leveloffset!:

== Resource troubleshooting

:leveloffset: +1

// Module included in the following assemblies:

// * networking/network_observability/troubleshooting-network-observability.adoc

:_mod-docs-content-type: PROCEDURE
[id="network-observability-troubleshooting-loki-tenant-rate-limit_{context}"]
= LokiStack rate limit errors
A rate-limit placed on the Loki tenant can result in potential temporary loss of data and a 429 error: `Per stream rate limit exceeded (limit:xMB/sec) while attempting to ingest for stream`. You might consider having an alert set to notify you of this error. For more information, see "Creating Loki rate limit alerts for the NetObserv dashboard" in the Additional resources of this section.

You can update the LokiStack CRD with the `perStreamRateLimit` and `perStreamRateLimitBurst` specifications, as shown in the following procedure.

.Procedure
. Navigate to *Operators* -> *Installed Operators*, viewing *All projects* from the *Project* dropdown.
. Look for *Loki Operator*, and select the *LokiStack* tab.
. Create or edit an existing *LokiStack* instance using the *YAML view* to add the `perStreamRateLimit` and `perStreamRateLimitBurst` specifications:
+
[source, yaml]
----
apiVersion: loki.grafana.com/v1
kind: LokiStack
metadata:
  name: loki
  namespace: netobserv
spec:
  limits:
    global:
      ingestion:
        perStreamRateLimit: 6        <1>
        perStreamRateLimitBurst: 30  <2>
  tenants:
    mode: openshift-network
  managementState: Managed
----
<1> The default value for `perStreamRateLimit` is `3`.
<2> The default value for `perStreamRateLimitBurst` is `15`.

. Click *Save*.

.Verification
Once you update the `perStreamRateLimit` and `perStreamRateLimitBurst` specifications, the pods in your cluster restart and the 429 rate-limit error no longer occurs.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/troubleshooting-network-observability-must-gather,modules/troubleshooting-network-observability-after-installation,modules/troubleshooting-network-observability-flowlogs-pipeline-kafka,modules/troubleshooting-network-observability-network-flow,modules/troubleshooting-network-observability-controller-manager-pod-out-of-memory,modules/troubleshooting-network-observability-loki-resource-exhausted,modules/troubleshooting-network-observability-loki-tenant-rate-limit

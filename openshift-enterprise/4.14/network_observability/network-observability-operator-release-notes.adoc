//Network Observability Operator Release Notes
:_mod-docs-content-type: ASSEMBLY
[id="network-observability-operator-release-notes"]
= Network Observability Operator release notes
:context: network-observability-operator-release-notes-v0
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

The Network Observability Operator enables administrators to observe and analyze network traffic flows for {product-title} clusters.

These release notes track the development of the Network Observability Operator in the {product-title}.

For an overview of the Network Observability Operator, see xref:../network_observability/network-observability-overview.adoc#dependency-network-observability[About Network Observability Operator].
[id="network-observability-operator-release-notes-1-4-2"]
== Network Observability Operator 1.4.2
The following advisory is available for the Network Observability Operator 1.4.2:

* link:https://access.redhat.com/errata/RHSA-2023:6787[2023:6787 Network Observability Operator 1.4.2]

=== CVEs
* link:https://access.redhat.com/security/cve/CVE-2023-39325[2023-39325]
* link:https://access.redhat.com/security/cve/CVE-2023-44487[2023-44487]

[id="network-observability-operator-release-notes-1-4-1"]
== Network Observability Operator 1.4.1
The following advisory is available for the Network Observability Operator 1.4.1:

* link:https://access.redhat.com/errata/RHSA-2023:5974[2023:5974 Network Observability Operator 1.4.1]

=== CVEs
* link:https://access.redhat.com/security/cve/cve-2023-44487[2023-44487]
* link:https://access.redhat.com/security/cve/cve-2023-39325[2023-39325]
* link:https://access.redhat.com/security/cve/cve-2023-29406[2023-29406]
* link:https://access.redhat.com/security/cve/CVE-2023-29409[2023-29409]
* link:https://access.redhat.com/security/cve/cve-2023-39322[2023-39322]
* link:https://access.redhat.com/security/cve/cve-2023-39318[2023-39318]
* link:https://access.redhat.com/security/cve/cve-2023-39319[2023-39319]
* link:https://access.redhat.com/security/cve/cve-2023-39321[2023-39321]

=== Bug fixes

* In 1.4, there was a known issue when sending network flow data to Kafka. The Kafka message key was ignored, causing an error with connection tracking. Now the key is used for partitioning, so each flow from the same connection is sent to the same processor. (link:https://issues.redhat.com/browse/NETOBSERV-926[*NETOBSERV-926*])

* In 1.4, the `Inner` flow direction was introduced to account for flows between pods running on the same node. Flows with the `Inner` direction were not taken into account in the generated Prometheus metrics derived from flows, resulting in under-evaluated bytes and packets rates.
Now, derived metrics are including flows with the `Inner` direction, providing correct bytes and packets rates. (link:https://issues.redhat.com/browse/NETOBSERV-1344[*NETOBSERV-1344*])

[id="network-observability-operator-release-notes-1-4"]
== Network Observability Operator 1.4.0
The following advisory is available for the Network Observability Operator 1.4.0:

* link:https://access.redhat.com/errata/RHSA-2023:5379[RHSA-2023:5379 Network Observability Operator 1.4.0]

[id="network-observability-channel-removal-1.4"]
=== Channel removal
You must switch your channel from `v1.0.x` to `stable` to receive the latest Operator updates. The `v1.0.x` channel is now removed.

[id="network-observability-operator-1.4.0-features-enhancements"]
=== New features and enhancements

[id="network-observability-enhanced-configuration-and-ui-1.4"]
==== Notable enhancements
The 1.4 release of the Network Observability Operator adds improvements and new capabilities to the {product-title} web console plugin and the Operator configuration.

[discrete]
[id="web-console-enhancements-1.4_{context}"]
===== Web console enhancements:

* In the *Query Options*, the *Duplicate flows* checkbox is added to choose whether or not to show duplicated flows.
* You can now filter source and destination traffic with image:arrow-up-long-solid.png[,10] *One-way*, image:arrow-up-long-solid.png[,10] image:arrow-down-long-solid.png[,10] *Back-and-forth*, and *Swap* filters.
* The Network Observability metrics dashboards in *Observe* -> *Dashboards* -> *NetObserv* and *NetObserv / Health* are modified as follows:
** The *NetObserv* dashboard shows top bytes, packets sent, packets received per nodes, namespaces, and workloads. Flow graphs are removed from this dashboard.
** The *NetObserv / Health* dashboard shows flows overhead as well as top flow rates per nodes, namespaces, and workloads.
** Infrastructure and Application metrics are shown in a split-view for namespaces and workloads.

For more information, see xref:../network_observability/network-observability-overview.adoc#network-observability-dashboards[Network Observability metrics] and xref:../network_observability/observing-network-traffic.adoc#network-observability-quickfilternw-observe-network-traffic[Quick filters].

[discrete]
[id="configuration-enhancements-1.4_{context}"]
===== Configuration enhancements:

* You now have the option to specify different namespaces for any configured ConfigMap or Secret reference, such as in certificates configuration.
* The `spec.processor.clusterName` parameter is added so that the name of the cluster appears in the flows data. This is useful in a multi-cluster context. When using {product-title}, leave empty to make it automatically determined.

For more information, see xref:../network_observability/configuring-operator.adoc#network-observability-flowcollector-view_network_observability[Flow Collector sample resource] and xref:../network_observability/flowcollector-api.adoc#network-observability-flowcollector-api-specifications_network_observability[Flow Collector API Reference].

[id="network-observability-without-loki-1.4"]
==== Network Observability without Loki
The Network Observability Operator is now functional and usable without Loki. If Loki is not installed, it can only export flows to KAFKA or IPFIX format and provide metrics in the Network Observability metrics dashboards. For more information, see xref:../network_observability/installing-operators.adoc#network-observability-without-loki_network_observability[Network Observability without Loki].

[id="network-observability-dns-tracking-1.4"]
==== DNS tracking
In 1.4, the Network Observability Operator makes use of eBPF tracepoint hooks to enable DNS tracking. You can monitor your network, conduct security analysis, and troubleshoot DNS issues in the *Network Traffic* and *Overview* pages in the web console.

For more information, see xref:../network_observability/observing-network-traffic.adoc#network-observability-dns-overview_nw-observe-network-traffic[Configuring DNS tracking] and xref:../network_observability/observing-network-traffic.adoc#network-observability-dns-tracking_nw-observe-network-traffic[Working with DNS tracking].

//Packet drops needs separate RN PR that doesn't cherrypick to 4.10+ since its only supported in 4.13+. This PR will go to 4.10+
[id="SR-IOV-configuration-1.4"]
==== SR-IOV support
You can now collect traffic from a cluster with Single Root I/O Virtualization (SR-IOV) device. For more information, see xref:../network_observability/configuring-operator.adoc#network-observability-SR-IOV-config_network_observability[Configuring the monitoring of SR-IOV interface traffic].

[id="IPFIX-support-1.4"]
==== IPFIX exporter support
You can now export eBPF-enriched network flows to the IPFIX collector. For more information, see xref:../network_observability/configuring-operator.adoc#network-observability-enriched-flows_network_observability[Export enriched network flow data].

[id="network-observability-packet-drop-1.4"]
==== Packet drops
In the 1.4 release of the Network Observability Operator, eBPF tracepoint hooks are used to enable packet drop tracking. You can now detect and analyze the cause for packet drops and make decisions to optimize network performance. In {product-title} 4.14 and later, both host drops and OVS drops are detected. In {product-title} 4.13, only host drops are detected. For more information, see xref:../network_observability/observing-network-traffic#network-observability-pktdrop-overview_nw-observe-network-traffic[Configuring packet drop tracking] and xref:../network_observability/observing-network-traffic#network-observability-packet-drops_nw-observe-network-traffic[Working with packet drops].

==== s390x architecture support
Network Observability Operator can now run on `s390x` architecture. Previously it ran on `amd64`, `ppc64le`, or `arm64`.

[id="network-observability-operator-1.4.0-bug-fixes"]
=== Bug fixes
* Previously, the Prometheus metrics exported by Network Observability were computed out of potentially duplicated network flows. In the related dashboards, from *Observe* -> *Dashboards*, this could result in potentially doubled rates. Note that dashboards from the *Network Traffic* view were not affected. Now, network flows are filtered to eliminate duplicates prior to metrics calculation, which results in correct traffic rates displayed in the dashboards. (link:https://issues.redhat.com/browse/NETOBSERV-1131[*NETOBSERV-1131*])

* Previously, the Network Observability Operator agents were not able to capture traffic on network interfaces when configured with Multus or SR-IOV, non-default network namespaces. Now, all available network namespaces are recognized and used for capturing flows, allowing capturing traffic for SR-IOV. There are xref:../network_observability/configuring-operator.adoc#network-observability-SR-IOV-config_network_observability[configurations needed] for the `FlowCollector` and `SRIOVnetwork` custom resource to collect traffic.
(link:https://issues.redhat.com/browse/NETOBSERV-1283[*NETOBSERV-1283*])

* Previously, in the Network Observability Operator details from *Operators* -> *Installed Operators*, the `FlowCollector` *Status* field might have reported incorrect information about the state of the deployment. The status field now shows the proper conditions with improved messages. The history of events is kept, ordered by event date. (link:https://issues.redhat.com/browse/NETOBSERV-1224[*NETOBSERV-1224*])

* Previously, during spikes of network traffic load, certain eBPF pods were OOM-killed and went into a `CrashLoopBackOff` state. Now, the `eBPF` agent memory footprint is improved, so pods are not OOM-killed and entering a `CrashLoopBackOff` state. (link:https://issues.redhat.com/browse/NETOBSERV-975[*NETOBSERV-975*])

* Previously when `processor.metrics.tls` was set to `PROVIDED` the `insecureSkipVerify` option value was forced to be `true`. Now you can set `insecureSkipVerify` to `true` or `false`, and provide a CA certificate if needed.  (link:https://issues.redhat.com/browse/NETOBSERV-1087[NETOBSERV-1087])

[id="network-observability-operator-1.4.0-known-issues"]
=== Known issues
* Since the 1.2.0 release of the Network Observability Operator, using Loki Operator 5.6, a Loki certificate change periodically affects the `flowlogs-pipeline` pods and results in dropped flows rather than flows written to Loki. The problem self-corrects after some time, but it still causes temporary flow data loss during the Loki certificate change. This issue has only been observed in large-scale environments of 120 nodes or greater. (link:https://issues.redhat.com/browse/NETOBSERV-980[*NETOBSERV-980*])

* Currently, when `spec.agent.ebpf.features` includes DNSTracking, larger DNS packets require the `eBPF` agent to look for DNS header outside of the 1st socket buffer (SKB) segment. A new `eBPF` agent helper function needs to be implemented to support it. Currently, there is no workaround for this issue. (link:https://issues.redhat.com/browse/NETOBSERV-1304[*NETOBSERV-1304*])

* Currently, when `spec.agent.ebpf.features` includes DNSTracking, DNS over TCP packets requires the `eBPF` agent to look for DNS header outside of the 1st SKB segment. A new `eBPF` agent helper function needs to be implemented to support it. Currently, there is no workaround for this issue. (link:https://issues.redhat.com/browse/NETOBSERV-1245[*NETOBSERV-1245*])

* Currently, when using a `KAFKA` deployment model, if conversation tracking is configured, conversation events might be duplicated across Kafka consumers, resulting in inconsistent tracking of conversations, and incorrect volumetric data. For that reason, it is not recommended to configure conversation tracking when `deploymentModel` is set to `KAFKA`. (link:https://issues.redhat.com/browse/NETOBSERV-926[*NETOBSERV-926*])

* Currently, when the `processor.metrics.server.tls.type` is configured to use a `PROVIDED` certificate, the operator enters an unsteady state that might affect its performance and resource consumption. It is recommended to not use a `PROVIDED` certificate until this issue is resolved, and instead using an auto-generated certificate, setting `processor.metrics.server.tls.type` to `AUTO`. (link:https://issues.redhat.com/browse/NETOBSERV-1293)[*NETOBSERV-1293*]

[id="network-observability-operator-release-notes-1-3"]
== Network Observability Operator 1.3.0
The following advisory is available for the Network Observability Operator 1.3.0:

* link:https://access.redhat.com/errata/RHSA-2023:3905[RHSA-2023:3905 Network Observability Operator 1.3.0]

[id="network-observability-channel-deprecation"]
=== Channel deprecation
You must switch your channel from `v1.0.x` to `stable` to receive future Operator updates. The `v1.0.x` channel is deprecated and planned for removal in the next release.

[id="network-observability-operator-1.3.0-features-enhancements"]
=== New features and enhancements

[id="multi-tenancy-1.3"]
==== Multi-tenancy in Network Observability
* System administrators can allow and restrict individual user access, or group access, to the flows stored in Loki. For more information, see xref:../network_observability/installing-operators.adoc#network-observability-multi-tenancynetwork_observability[Multi-tenancy in Network Observability].

[id="flow-based-dashboard-1.3"]
==== Flow-based metrics dashboard
* This release adds a new dashboard, which provides an overview of the network flows in your {product-title} cluster. For more information, see xref:../network_observability/network-observability-overview.adoc#network-observability-dashboards[Network Observability metrics].

[id="must-gather-1.3"]
==== Troubleshooting with the must-gather tool
* Information about the Network Observability Operator can now be included in the must-gather data for troubleshooting. For more information, see xref:../network_observability/troubleshooting-network-observability.adoc#network-observability-must-gather_network-observability-troubleshooting[Network Observability must-gather].

[id="multi-arch-1.3"]
==== Multiple architectures now supported
* Network Observability Operator can now run on an `amd64`, `ppc64le`, or `arm64` architectures. Previously, it only ran on `amd64`.

[id="deprecated-features-1.3"]
=== Deprecated features

[id="authToken-host"]
==== Deprecated configuration parameter setting
The release of Network Observability Operator 1.3 deprecates the `spec.Loki.authToken` `HOST` setting. When using the Loki Operator, you must now only use the `FORWARD` setting.

[id="network-observability-operator-1.3.0-bug-fixes"]
=== Bug fixes
* Previously, when the Operator was installed from the CLI, the `Role` and `RoleBinding` that are necessary for the Cluster Monitoring Operator to read the metrics were not installed as expected. The issue did not occur when the operator was installed from the web console. Now, either way of installing the Operator installs the required `Role` and `RoleBinding`. (link:https://issues.redhat.com/browse/NETOBSERV-1003[*NETOBSERV-1003*])

* Since version 1.2, the Network Observability Operator can raise alerts when a problem occurs with the flows collection. Previously, due to a bug, the related configuration to disable alerts, `spec.processor.metrics.disableAlerts` was not working as expected and sometimes ineffectual. Now, this configuration is fixed so that it is possible to disable the alerts. (link:https://issues.redhat.com/browse/NETOBSERV-976[*NETOBSERV-976*])

* Previously, when Network Observability was configured with `spec.loki.authToken` set to `DISABLED`, only a `kubeadmin` cluster administrator was able to view network flows. Other types of cluster administrators received authorization failure. Now, any cluster administrator is able to view network flows. (link:https://issues.redhat.com/browse/NETOBSERV-972[*NETOBSERV-972*])

* Previously, a bug prevented users from setting `spec.consolePlugin.portNaming.enable` to `false`. Now, this setting can be set to `false` to disable port-to-service name translation. (link:https://issues.redhat.com/browse/NETOBSERV-971[*NETOBSERV-971*])

* Previously, the metrics exposed by the console plugin were not collected by the Cluster Monitoring Operator (Prometheus), due to an incorrect configuration. Now the configuration has been fixed so that the console plugin metrics are correctly collected and accessible from the {product-title} web console. (link:https://issues.redhat.com/browse/NETOBSERV-765[*NETOBSERV-765*])

* Previously, when `processor.metrics.tls` was set to `AUTO` in the `FlowCollector`, the `flowlogs-pipeline servicemonitor` did not adapt the appropriate TLS scheme, and metrics were not visible in the web console. Now the issue is fixed for AUTO mode. (link:https://issues.redhat.com/browse/NETOBSERV-1070[*NETOBSERV-1070*])

* Previously, certificate configuration, such as used for Kafka and Loki, did not allow specifying a namespace field, implying that the certificates had to be in the same namespace where Network Observability is deployed. Moreover, when using Kafka with TLS/mTLS, the user had to manually copy the certificate(s) to the privileged namespace where the `eBPF` agent pods are deployed and manually manage certificate updates, such as in the case of certificate rotation. Now, Network Observability setup is simplified by adding a namespace field for certificates in the `FlowCollector` resource. As a result, users can now install Loki or Kafka in different namespaces without needing to manually copy their certificates in the Network Observability namespace. The original certificates are watched so that the copies are automatically updated when needed. (link:https://issues.redhat.com/browse/NETOBSERV-773[*NETOBSERV-773*])

* Previously, the SCTP, ICMPv4 and ICMPv6 protocols were not covered by the Network Observability agents, resulting in a less comprehensive network flows coverage. These protocols are now recognized to improve the flows coverage. (link:https://issues.redhat.com/browse/NETOBSERV-934[*NETOBSERV-934*])

[id="network-observability-operator-1.3.0-known-issues"]
=== Known issues
* When `processor.metrics.tls` is set to `PROVIDED` in the `FlowCollector`, the `flowlogs-pipeline` `servicemonitor` is not adapted to the TLS scheme. (link:https://issues.redhat.com/browse/NETOBSERV-1087[*NETOBSERV-1087*])

* Since the 1.2.0 release of the Network Observability Operator, using Loki Operator 5.6, a Loki certificate change periodically affects the `flowlogs-pipeline` pods and results in dropped flows rather than flows written to Loki. The problem self-corrects after some time, but it still causes temporary flow data loss during the Loki certificate change. This issue has only been observed in large-scale environments of 120 nodes or greater.(link:https://issues.redhat.com/browse/NETOBSERV-980[*NETOBSERV-980*])

[id="network-observability-operator-release-notes-1-2"]
== Network Observability Operator 1.2.0
The following advisory is available for the Network Observability Operator 1.2.0:

* https://access.redhat.com/errata/RHSA-2023:1817[RHSA-2023:1817 Network Observability Operator 1.2.0]

[id="network-observability-operator-preparing-to-update"]
=== Preparing for the next update
The subscription of an installed Operator specifies an update channel that tracks and receives updates for the Operator. Until the 1.2 release of the Network Observability Operator, the only channel available was `v1.0.x`. The 1.2 release of the Network Observability Operator introduces the `stable` update channel for tracking and receiving updates. You must switch your channel from `v1.0.x` to `stable` to receive future Operator updates. The `v1.0.x` channel is deprecated and planned for removal in a following release.

[id="network-observability-operator-1.2.0-features-enhancements"]
=== New features and enhancements

[id="histogram-feature-1.2"]
==== Histogram in Traffic Flows view
* You can now choose to show a histogram bar chart of flows over time. The histogram enables you to visualize the history of flows without hitting the Loki query limit. For more information, see xref:../network_observability/observing-network-traffic.adoc#network-observability-histogram-trafficflow_nw-observe-network-traffic[Using the histogram].

[id="conversation-tracking-feature-1.2"]
==== Conversation tracking
* You can now query flows by *Log Type*, which enables grouping network flows that are part of the same conversation. For more information, see xref:../network_observability/observing-network-traffic.adoc#network-observability-working-with-conversations_nw-observe-network-traffic[Working with conversations].

[id="health-alerts-feature-1.2"]
==== Network Observability health alerts
* The Network Observability Operator now creates automatic alerts if the `flowlogs-pipeline` is dropping flows because of errors at the write stage or if the Loki ingestion rate limit has been reached. For more information, see xref:../network_observability/network-observability-operator-monitoring.adoc#network-observability-alert-dashboard_network_observability[Viewing health information].

[id="network-observability-operator-1.2.0-bug-fixes"]
=== Bug fixes

* Previously, after changing the `namespace` value in the FlowCollector spec, `eBPF` agent pods running in the previous namespace were not appropriately deleted. Now, the pods running in the previous namespace are appropriately deleted. (link:https://issues.redhat.com/browse/NETOBSERV-774[*NETOBSERV-774*])

* Previously, after changing the `caCert.name` value in the FlowCollector spec (such as in Loki section), FlowLogs-Pipeline pods and Console plug-in pods were not restarted, therefore they were unaware of the configuration change. Now, the pods are restarted, so they get the configuration change. (link:https://issues.redhat.com/browse/NETOBSERV-772[*NETOBSERV-772*])

* Previously, network flows between pods running on different nodes were sometimes not correctly identified as being duplicates because they are captured by different network interfaces. This resulted in over-estimated metrics displayed in the console plug-in. Now, flows are correctly identified as duplicates, and the console plug-in displays accurate metrics. (link:https://issues.redhat.com/browse/NETOBSERV-755[*NETOBSERV-755*])

* The "reporter" option in the console plug-in is used to filter flows based on the observation point of either source node or destination node. Previously, this option mixed the flows regardless of the node observation point. This was due to network flows being incorrectly reported as Ingress or Egress at the node level.  Now, the network flow direction reporting is correct. The "reporter" option filters for source observation point, or destination observation point, as expected. (link:https://issues.redhat.com/browse/NETOBSERV-696[*NETOBSERV-696*])

* Previously, for agents configured to send flows directly to the processor as gRPC+protobuf requests, the submitted payload could be too large and is rejected by the processors' GRPC server. This occurred under  very-high-load scenarios and with only some configurations of the agent. The agent logged an error message, such as: _grpc: received message larger than max_. As a consequence, there was information loss about those flows. Now, the gRPC payload is split into several messages when the size exceeds a threshold. As a result, the server maintains connectivity. (link:https://issues.redhat.com/browse/NETOBSERV-617[*NETOBSERV-617*])

[id="network-observability-operator-1.2.0-known-issues"]
=== Known issue
* In the 1.2.0 release of the Network Observability Operator, using Loki Operator 5.6, a Loki certificate transition periodically affects the `flowlogs-pipeline` pods and results in dropped flows rather than flows written to Loki. The problem self-corrects after some time, but it still causes temporary flow data loss during the Loki certificate transition. (link:https://issues.redhat.com/browse/NETOBSERV-980[*NETOBSERV-980*])

[id="network-observability-operator-1.2.0-notable-technical-changes"]
=== Notable technical changes
* Previously, you could install the Network Observability Operator using a custom namespace. This release introduces the `conversion webhook` which changes the `ClusterServiceVersion`. Because of this change, all the available namespaces are no longer listed. Additionally, to enable Operator metrics collection, namespaces that are shared with other Operators, like the `openshift-operators` namespace, cannot be used. Now, the Operator must be installed in the `openshift-netobserv-operator` namespace. You cannot automatically upgrade to the new Operator version if you previously installed the Network Observability Operator using a custom namespace. If you previously installed the Operator using a custom namespace, you must delete the instance of the Operator that was installed and re-install your operator in the `openshift-netobserv-operator` namespace. It is important to note that custom namespaces, such as the commonly used `netobserv` namespace, are still possible for the `FlowCollector`, Loki, Kafka, and other plug-ins. (link:https://issues.redhat.com/browse/NETOBSERV-907[*NETOBSERV-907*])(link:https://https://issues.redhat.com/browse/NETOBSERV-956[*NETOBSERV-956*])

[id="network-observability-operator-release-notes-1-1"]
== Network Observability Operator 1.1.0

The following advisory is available for the Network Observability Operator 1.1.0:

* link:https://access.redhat.com/errata/RHSA-2023:0786[RHSA-2023:0786 Network Observability Operator Security Advisory Update]

The Network Observability Operator is now stable and the release channel is upgraded to `v1.1.0`.

[id="network-observability-operator-1.1.0-bug-fixes"]
=== Bug fix

* Previously, unless the Loki `authToken` configuration was set to `FORWARD` mode, authentication was no longer enforced, allowing any user who could connect to the {product-title} console in an {product-title} cluster to retrieve flows without authentication.
Now, regardless of the Loki `authToken` mode, only cluster administrators can retrieve flows. (link:https://bugzilla.redhat.com/show_bug.cgi?id=2169468[*BZ#2169468*])

//# includes=_attributes/common-attributes

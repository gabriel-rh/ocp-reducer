:_mod-docs-content-type: ASSEMBLY
[id="virt-creating-vms-from-container-disks"]
= Creating VMs by using container disks
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: virt-creating-vms-from-container-disks

toc::[]

You can create virtual machines (VMs) by using container disks built from operating system images.

You can enable auto updates for your container disks. See xref:../../../virt/storage/virt-automatic-bootsource-updates.adoc#virt-automatic-bootsource-updates[Managing automatic boot source updates] for details.

[IMPORTANT]
====
If the container disks are large, the I/O traffic might increase and cause worker nodes to be unavailable. You can perform the following tasks to resolve this issue:

* xref:../../../applications/pruning-objects.adoc#pruning-deployments_pruning-objects[Pruning `DeploymentConfig` objects].
* xref:../../../nodes/nodes/nodes-nodes-garbage-collection.adoc#nodes-nodes-garbage-collection-configuring_nodes-nodes-configuring[Configuring garbage collection].
====

You create a VM from a container disk by performing the following steps:

. xref:../../../virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc#virt-preparing-container-disk-for-vms_virt-creating-vms-from-container-disks[Build an operating system image into a container disk and upload it to your container registry].
. If your container registry does not have TLS, xref:../../../virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc#virt-disabling-tls-for-registry_virt-creating-vms-from-container-disks[configure your environment to disable TLS for your registry].
. Create a VM with the container disk as the disk source by using the xref:../../../virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc#virt-creating-vm-custom-image-web_virt-creating-vms-from-container-disks[web console] or the xref:../../../virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc#virt-creating-vm-import-cli_virt-creating-vms-from-container-disks[command line].

[IMPORTANT]
====
You must install the xref:../../../virt/virtual_machines/creating_vms_custom/virt-installing-qemu-guest-agent.adoc#virt-installing-qemu-guest-agent[QEMU guest agent] on VMs created from operating system images that are not provided by Red Hat.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-preparing-container-disk-for-vms_{context}"]
= Building and uploading a container disk

You can build a virtual machine (VM) image into a container disk and upload it to a registry.

The size of a container disk is limited by the maximum layer size of the registry where the container disk is hosted.

[NOTE]
====
For link:https://access.redhat.com/documentation/en-us/red_hat_quay/[Red Hat Quay], you can change the maximum layer size by editing the YAML configuration file that is created when Red Hat Quay is first deployed.
====

.Prerequisites

* You must have `podman` installed.
* You must have a QCOW2 or RAW image file.

.Procedure

. Create a Dockerfile to build the VM image into a container image. The VM image must be owned by QEMU, which has a UID of `107`, and placed in the `/disk/` directory inside the container. Permissions for the `/disk/` directory must then be set to `0440`.
+
The following example uses the Red Hat Universal Base Image (UBI) to handle these configuration changes in the first stage, and uses the minimal `scratch` image in the second stage to store the result:
+
[source,terminal]
----
$ cat > Dockerfile << EOF
FROM registry.access.redhat.com/ubi8/ubi:latest AS builder
ADD --chown=107:107 <vm_image>.qcow2 /disk/ \// <1>
RUN chmod 0440 /disk/*

FROM scratch
COPY --from=builder /disk/* /disk/
EOF
----
<1> Where `<vm_image>` is the image in either QCOW2 or RAW format. If you use a remote image, replace `<vm_image>.qcow2` with the complete URL.

. Build and tag the container:
+
[source,terminal]
----
$ podman build -t <registry>/<container_disk_name>:latest .
----

. Push the container image to the registry:
+
[source,terminal]
----
$ podman push <registry>/<container_disk_name>:latest
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-disabling-tls-for-registry_{context}"]
= Disabling TLS for a container registry

You can disable TLS (transport layer security) for one or more container registries by editing the `insecureRegistries` field of the `HyperConverged` custom resource.

.Prerequisites

* Log in to the cluster as a user with the `cluster-admin` role.

.Procedure

* Edit the `HyperConverged` custom resource and add a list of insecure registries to the `spec.storageImport.insecureRegistries` field.
+
[source,yaml,subs="attributes+"]
----
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
  namespace: {CNVNamespace}
spec:
  storageImport:
    insecureRegistries: <1>
      - "private-registry-example-1:5000"
      - "private-registry-example-2:5000"
----
<1> Replace the examples in this list with valid registry hostnames.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-by-cloning-pvcs.adoc
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-web-images.adoc

:container-disks:
:title-frag: from a container disk
:a-object: a container disk
:object: container disk
:data-source: container registry
:menu-item: Registry (creates PVC)

:_mod-docs-content-type: PROCEDURE
[id="virt-creating-vm-custom-image-web_{context}"]
= Creating a VM {title-frag} by using the web console

You can create a virtual machine (VM) by importing {a-object} from a {data-source} by using the {product-title} web console.

.Prerequisites


.Procedure

. Navigate to *Virtualization* -> *Catalog* in the web console.
. Click a template tile without an available boot source.
. Click *Customize VirtualMachine*.
. On the *Customize template parameters* page, expand *Storage* and select *{menu-item}* from the *Disk source* list.
. Enter the container image URL. Example: `\https://mirror.arizona.edu/fedora/linux/releases/38/Cloud/x86_64/images/Fedora-Cloud-Base-38-1.6.x86_64.qcow2`
. Set the disk size.
. Click *Customize VirtualMachine*.
. Click *Create VirtualMachine*.

:!container-disks:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-container-disks.adoc
// * virt/virtual_machines/creating_vms_custom/virt-creating-vms-from-web-images.adoc

:container-disks:
:title-frag: from a container disk
:a-object: a container disk
:object: container disk
:data-source: container registry

:_mod-docs-content-type: PROCEDURE
[id="virt-creating-vm-import-cli_{context}"]
= Creating a VM {title-frag} by using the command line

You can create a virtual machine (VM) {title-frag} by using the command line.

When the virtual machine (VM) is created, the data volume with the {object} is imported into persistent storage.

.Prerequisites

* You must have access credentials for the {data-source} that contains the {object}.

.Procedure

. If the {data-source} requires authentication, create a `Secret` manifest, specifying the credentials, and save it as a `data-source-secret.yaml` file:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: data-source-secret
  labels:
    app: containerized-data-importer
type: Opaque
data:
  accessKeyId: "" <1>
  secretKey:   "" <2>
----
<1> Specify the Base64-encoded key ID or user name.
<2> Specify the Base64-encoded secret key or password.

. Apply the `Secret` manifest by running the following command:
+
[source,terminal]
----
$ oc apply -f data-source-secret.yaml
----

. If the VM must communicate with servers that use self-signed certificates or certificates that are not signed by the system CA bundle, create a config map in the same namespace as the VM:
+
[source,terminal]
----
$ oc create configmap tls-certs <1>
  --from-file=</path/to/file/ca.pem> <2>
----
<1> Specify the config map name.
<2> Specify the path to the CA certificate.

. Edit the `VirtualMachine` manifest and save it as a `vm-fedora-datavolume.yaml` file:
+
[%collapsible]
====
[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  creationTimestamp: null
  labels:
    kubevirt.io/vm: vm-fedora-datavolume
  name: vm-fedora-datavolume <1>
spec:
  dataVolumeTemplates:
  - metadata:
      creationTimestamp: null
      name: fedora-dv <2>
    spec:
      storage:
        resources:
          requests:
            storage: 10Gi <3>
        storageClassName: <storage_class> <4>
      source:
        registry:
          url: "docker://kubevirt/fedora-cloud-container-disk-demo:latest" <5>
          secretRef: data-source-secret <6>
          certConfigMap: tls-certs <7>
    status: {}
  running: true
  template:
    metadata:
      creationTimestamp: null
      labels:
        kubevirt.io/vm: vm-fedora-datavolume
    spec:
      domain:
        devices:
          disks:
          - disk:
              bus: virtio
            name: datavolumedisk1
        machine:
          type: ""
        resources:
          requests:
            memory: 1.5Gi
      terminationGracePeriodSeconds: 180
      volumes:
      - dataVolume:
          name: fedora-dv
        name: datavolumedisk1
status: {}
----
<1> Specify the name of the VM.
<2> Specify the name of the data volume.
<3> Specify the size of the storage requested for the data volume.
<4> Optional: If you do not specify a storage class, the default storage class is used.
<5> Specify the URL of the {data-source}.
<6> Optional: Specify the secret name if you created a secret for the {data-source} access credentials.
<7> Optional: Specify a CA certificate config map.
====

. Create the VM by running the following command:
+
[source,terminal]
----
$ oc create -f vm-fedora-datavolume.yaml
----
+
The `oc create` command creates the data volume and the VM. The CDI controller creates an underlying PVC with the correct annotation and the import process begins. When the import is complete, the data volume status changes to `Succeeded`. You can start the VM.
+
Data volume provisioning happens in the background, so there is no need to monitor the process.

.Verification

. The importer pod downloads the {object} from the specified URL and stores it on the provisioned persistent volume. View the status of the importer pod by running the following command:
+
[source,terminal]
----
$ oc get pods
----

. Monitor the data volume until its status is `Succeeded` by running the following command:
+
[source,terminal]
----
$ oc describe dv fedora-dv <1>
----
<1> Specify the data volume name that you defined in the `VirtualMachine` manifest.

. Verify that provisioning is complete and that the VM has started by accessing its serial console:
+
[source,terminal]
----
$ virtctl console vm-fedora-datavolume
----


:leveloffset!:

//# includes=_attributes/common-attributes,modules/virt-preparing-container-disk-for-vms,modules/virt-disabling-tls-for-registry,modules/virt-creating-vm-custom-image-web,modules/virt-creating-vm-import-cli

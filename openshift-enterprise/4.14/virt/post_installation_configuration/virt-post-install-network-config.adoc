:_mod-docs-content-type: ASSEMBLY
[id="virt-post-install-network-config"]
= Postinstallation network configuration
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: virt-post-install-network-config

toc::[]

By default, {VirtProductName} is installed with a single, internal pod network.

After you install {VirtProductName}, you can install networking Operators and configure additional networks.

[id="installing-operators"]
== Installing networking Operators

You must install the xref:../../networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.adoc#k8s-nmstate-about-the-k8s-nmstate-operator[Kubernetes NMState Operator] to configure a Linux bridge network for live migration or external access to virtual machines (VMs).

You can install the xref:../../networking/hardware_networks/about-sriov.adoc#about-sriov[SR-IOV Operator] to manage SR-IOV network devices and network attachments.

:leveloffset: +2

// This is included in the following assemblies:
//
// networking/k8s_nmstate/k8s-nmstate-about-the-kubernetes-nmstate-operator.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-the-kubernetes-nmstate-operator-web-console_{context}"]
= Installing the Kubernetes NMState Operator by using the web console

You can install the Kubernetes NMState Operator by using the web console. After it is installed, the Operator can deploy the NMState State Controller as a daemon set across all of the cluster nodes.

.Prerequisites

* You are logged in as a user with `cluster-admin` privileges.

.Procedure

. Select *Operators* -> *OperatorHub*.

. In the search field below *All Items*, enter `nmstate` and click *Enter* to search for the Kubernetes NMState Operator.

. Click on the Kubernetes NMState Operator search result.

. Click on *Install* to open the *Install Operator* window.

. Click *Install* to install the Operator.

. After the Operator finishes installing, click *View Operator*.

. Under *Provided APIs*, click *Create Instance* to open the dialog box for creating an instance of `kubernetes-nmstate`.

. In the *Name* field of the dialog box, ensure the name of the instance is `nmstate.`
+
[NOTE]
====
The name restriction is a known issue. The instance is a singleton for the entire cluster.
====

. Accept the default settings and click *Create* to create the instance.

.Summary

Once complete, the Operator has deployed the NMState State Controller as a daemon set across all of the cluster nodes.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/hardware_networks/installing-sriov-operator.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="installing-sr-iov-operator_{context}"]
= Installing the SR-IOV Network Operator

As a cluster administrator, you can install the Single Root I/O Virtualization (SR-IOV) Network Operator by using the {product-title} CLI or the web console.

[id="install-operator-cli_{context}"]
== CLI: Installing the SR-IOV Network Operator

As a cluster administrator, you can install the Operator using the CLI.

.Prerequisites

* A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.
* Install the OpenShift CLI (`oc`).
* An account with `cluster-admin` privileges.

.Procedure

. To create the `openshift-sriov-network-operator` namespace, enter the following command:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
apiVersion: v1
kind: Namespace
metadata:
  name: openshift-sriov-network-operator
  annotations:
    workload.openshift.io/allowed: management
EOF
----

. To create an OperatorGroup CR, enter the following command:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: sriov-network-operators
  namespace: openshift-sriov-network-operator
spec:
  targetNamespaces:
  - openshift-sriov-network-operator
EOF
----

. Subscribe to the SR-IOV Network Operator.

.. Run the following command to get the {product-title} major and minor version. It is required for the `channel` value in the next
step.
+
[source,terminal]
----
$ OC_VERSION=$(oc version -o yaml | grep openshiftVersion | \
    grep -o '[0-9]*[.][0-9]*' | head -1)
----

.. To create a Subscription CR for the SR-IOV Network Operator, enter the following command:
+
[source,terminal]
----
$ cat << EOF| oc create -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: sriov-network-operator-subscription
  namespace: openshift-sriov-network-operator
spec:
  channel: "${OC_VERSION}"
  name: sriov-network-operator
  source: redhat-operators
  sourceNamespace: openshift-marketplace
EOF
----

. To verify that the Operator is installed, enter the following command:
+
[source,terminal]
----
$ oc get csv -n openshift-sriov-network-operator \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase
----
+
.Example output
[source,terminal,subs="attributes+"]
----
Name                                         Phase
sriov-network-operator.{product-version}.0-202310121402   Succeeded
----

[id="install-operator-web-console_{context}"]
== Web console: Installing the SR-IOV Network Operator

As a cluster administrator, you can install the Operator using the web console.

.Prerequisites

* A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.
* Install the OpenShift CLI (`oc`).
* An account with `cluster-admin` privileges.

.Procedure


. Install the SR-IOV Network Operator:

.. In the {product-title} web console, click *Operators* -> *OperatorHub*.

.. Select *SR-IOV Network Operator* from the list of available Operators, and then click *Install*.

.. On the *Install Operator* page, under *Installed Namespace*, select *Operator recommended Namespace*.

.. Click *Install*.

. Verify that the SR-IOV Network Operator is installed successfully:

.. Navigate to the *Operators* -> *Installed Operators* page.

.. Ensure that *SR-IOV Network Operator* is listed in the *openshift-sriov-network-operator* project with a *Status* of *InstallSucceeded*.
+
[NOTE]
====
During installation an Operator might display a *Failed* status.
If the installation later succeeds with an *InstallSucceeded* message, you can ignore the *Failed* message.
====

+
If the Operator does not appear as installed, to troubleshoot further:

+
* Inspect the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors under *Status*.
* Navigate to the *Workloads* -> *Pods* page and check the logs for pods in the `openshift-sriov-network-operator` project.
* Check the namespace of the YAML file. If the annotation is missing, you can add the annotation `workload.openshift.io/allowed=management` to the Operator namespace with the following command:
+
[source,terminal]
----
$ oc annotate ns/openshift-sriov-network-operator workload.openshift.io/allowed=management
----
+
[NOTE]
====
For {sno} clusters, the annotation `workload.openshift.io/allowed=management` is required for the namespace.
====

:leveloffset!:

[id="configuring-linux-bridge-network"]
== Configuring a Linux bridge network

After you install the Kubernetes NMState Operator, you can configure a Linux bridge network for live migration or external access to virtual machines (VMs).

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/vm_networking/virt-connecting-vm-to-linux-bridge.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-creating-linux-bridge-nncp_{context}"]
= Creating a Linux bridge NNCP

You can create a `NodeNetworkConfigurationPolicy` (NNCP) manifest for a Linux bridge network.

.Prerequisites
* You have installed the Kubernetes NMState Operator.

.Procedure

* Create the `NodeNetworkConfigurationPolicy` manifest. This example includes sample values that you must replace with your own information.
+
[source,yaml]
----
apiVersion: nmstate.io/v1
kind: NodeNetworkConfigurationPolicy
metadata:
  name: br1-eth1-policy <1>
spec:
  desiredState:
    interfaces:
      - name: br1 <2>
        description: Linux bridge with eth1 as a port <3>
        type: linux-bridge <4>
        state: up <5>
        ipv4:
          enabled: false <6>
        bridge:
          options:
            stp:
              enabled: false <7>
          port:
            - name: eth1 <8>
----
<1> Name of the policy.
<2> Name of the interface.
<3> Optional: Human-readable description of the interface.
<4> The type of interface. This example creates a bridge.
<5> The requested state for the interface after creation.
<6> Disables IPv4 in this example.
<7> Disables STP in this example.
<8> The node NIC to which the bridge is attached.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/vm_networking/virt-connecting-vm-to-linux-bridge.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc
//This file contains UI elements and/or package names that need to be updated.

:_mod-docs-content-type: PROCEDURE
[id="virt-creating-linux-bridge-nad-web_{context}"]
= Creating a Linux bridge NAD by using the web console

You can create a network attachment definition (NAD) to provide layer-2 networking to pods and virtual machines by using the {product-title} web console.

A Linux bridge network attachment definition is the most efficient method for connecting a virtual machine to a VLAN.

[WARNING]
====
Configuring IP address management (IPAM) in a network attachment definition for virtual machines is not supported.
====

.Procedure

. In the web console, click *Networking* -> *NetworkAttachmentDefinitions*.
. Click *Create Network Attachment Definition*.
+
[NOTE]
====
The network attachment definition must be in the same namespace as the pod or virtual machine.
====
+
. Enter a unique *Name* and optional *Description*.
. Select *CNV Linux bridge* from the *Network Type* list.
. Enter the name of the bridge in the *Bridge Name* field.
. Optional: If the resource has VLAN IDs configured, enter the ID numbers in the *VLAN Tag Number* field.
. Optional: Select *MAC Spoof Check* to enable MAC spoof filtering. This feature provides security against a MAC spoofing attack by allowing only a single MAC address to exit the pod.
. Click *Create*.

:leveloffset!:

[id="next-steps_configuring-linux-bridge-network"]
=== Next steps
* xref:../../virt/vm_networking/virt-connecting-vm-to-linux-bridge.adoc#virt-attaching-vm-secondary-network-cli_virt-connecting-vm-to-linux-bridge[Attaching a virtual machine (VM) to a Linux bridge network]

[id="configuring-network-live-migration"]
== Configuring a network for live migration

After you have configured a Linux bridge network, you can configure a dedicated network for live migration. A dedicated network minimizes the effects of network saturation on tenant workloads during live migration.

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/vm_networking/virt-dedicated-network-live-migration.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-configuring-secondary-network-vm-live-migration_{context}"]
= Configuring a dedicated secondary network for live migration

To configure a dedicated secondary network for live migration, you must first create a bridge network attachment definition (NAD) by using the CLI. Then, you add the name of the `NetworkAttachmentDefinition` object to the `HyperConverged` custom resource (CR).

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You logged in to the cluster as a user with the `cluster-admin` role.
* Each node has at least two Network Interface Cards (NICs).
* The NICs for live migration are connected to the same VLAN.

.Procedure

. Create a `NetworkAttachmentDefinition` manifest according to the following example:
+
.Example configuration file
[source,yaml,subs="attributes+"]
----
apiVersion: "k8s.cni.cncf.io/v1"
kind: NetworkAttachmentDefinition
metadata:
  name: my-secondary-network <1>
  namespace: {CNVNamespace} <2>
spec:
  config: '{
    "cniVersion": "0.3.1",
    "name": "migration-bridge",
    "type": "macvlan",
    "master": "eth1", <2>
    "mode": "bridge",
    "ipam": {
      "type": "whereabouts", <3>
      "range": "10.200.5.0/24" <4>
    }
  }'
----
<1> Specify the name of the `NetworkAttachmentDefinition` object.
<2> Specify the name of the NIC to be used for live migration.
<3> Specify the name of the CNI plugin that provides the network for the NAD.
<4> Specify an IP address range for the secondary network. This range must not overlap the IP addresses of the main network.

. Open the `HyperConverged` CR in your default editor by running the following command:
+
[source,terminal,subs="attributes+"]
----
oc edit hyperconverged kubevirt-hyperconverged -n {CNVNamespace}
----

. Add the name of the `NetworkAttachmentDefinition` object to the `spec.liveMigrationConfig` stanza of the `HyperConverged` CR:
+
.Example `HyperConverged` manifest
[source,yaml]
----
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  liveMigrationConfig:
    completionTimeoutPerGiB: 800
    network: <network> <1>
    parallelMigrationsPerCluster: 5
    parallelOutboundMigrationsPerNode: 2
    progressTimeout: 150
# ...
----
<1> Specify the name of the Multus `NetworkAttachmentDefinition` object to be used for live migrations.

. Save your changes and exit the editor. The `virt-handler` pods restart and connect to the secondary network.

.Verification

* When the node that the virtual machine runs on is placed into maintenance mode, the VM automatically migrates to another node in the cluster. You can verify that the migration occurred over the secondary network and not the default pod network by checking the target IP address in the virtual machine instance (VMI) metadata.
+
[source,terminal]
----
$ oc get vmi <vmi_name> -o jsonpath='{.status.migrationState.targetNodeAddress}'
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/live_migration/virt-migrating-vm-on-secondary-network.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-selecting-migration-network-ui_{context}"]
= Selecting a dedicated network by using the web console

You can select a dedicated network for live migration by using the {product-title} web console.

.Prerequisites

* You configured a Multus network for live migration.

.Procedure

. Navigate to *Virtualization > Overview* in the {product-title} web console.
. Click the *Settings* tab and then click *Live migration*.
. Select the network from the *Live migration network* list.

:leveloffset!:

[id="configuring-sriov-network"]
== Configuring an SR-IOV network

After you install the SR-IOV Operator, you can configure an SR-IOV network.

:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/hardware_networks/configuring-sriov-device.adoc
// * virt/vm_networking/virt-connecting-vm-to-sriov.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc


:virt-sriov:

:_mod-docs-content-type: PROCEDURE
[id="nw-sriov-configuring-device_{context}"]
= Configuring SR-IOV network devices

The SR-IOV Network Operator adds the `SriovNetworkNodePolicy.sriovnetwork.openshift.io` CustomResourceDefinition to {product-title}.
You can configure an SR-IOV network device by creating a SriovNetworkNodePolicy custom resource (CR).

[NOTE]
=====
When applying the configuration specified in a `SriovNetworkNodePolicy` object, the SR-IOV Operator might drain the nodes, and in some cases, reboot nodes.

It might take several minutes for a configuration change to apply.
=====

.Prerequisites

* You installed the OpenShift CLI (`oc`).
* You have access to the cluster as a user with the `cluster-admin` role.
* You have installed the SR-IOV Network Operator.
* You have enough available nodes in your cluster to handle the evicted workload from drained nodes.
* You have not selected any control plane nodes for SR-IOV network device configuration.

.Procedure

. Create an `SriovNetworkNodePolicy` object, and then save the YAML in the `<name>-sriov-node-network.yaml` file. Replace `<name>` with the name for this configuration.
+
[source,yaml]
----
apiVersion: sriovnetwork.openshift.io/v1
kind: SriovNetworkNodePolicy
metadata:
  name: <name> <1>
  namespace: openshift-sriov-network-operator <2>
spec:
  resourceName: <sriov_resource_name> <3>
  nodeSelector:
    feature.node.kubernetes.io/network-sriov.capable: "true" <4>
  priority: <priority> <5>
  mtu: <mtu> <6>
  numVfs: <num> <7>
  nicSelector: <8>
    vendor: "<vendor_code>" <9>
    deviceID: "<device_id>" <10>
    pfNames: ["<pf_name>", ...] <11>
    rootDevices: ["<pci_bus_id>", "..."] <12>
  deviceType: vfio-pci <13>
  isRdma: false <14>
----
<1> Specify a name for the CR object.
<2> Specify the namespace where the SR-IOV Operator is installed.
<3> Specify the resource name of the SR-IOV device plugin. You can create multiple `SriovNetworkNodePolicy` objects for a resource name.
<4> Specify the node selector to select which nodes are configured.
Only SR-IOV network devices on selected nodes are configured. The SR-IOV
Container Network Interface (CNI) plugin and device plugin are deployed only on selected nodes.
<5> Optional: Specify an integer value between `0` and `99`. A smaller number gets higher priority, so a priority of `10` is higher than a priority of `99`. The default value is `99`.
<6> Optional: Specify a value for the maximum transmission unit (MTU) of the virtual function. The maximum MTU value can vary for different NIC models.
<7> Specify the number of the virtual functions (VF) to create for the SR-IOV physical network device. For an Intel network interface controller (NIC), the number of VFs cannot be larger than the total VFs supported by the device. For a Mellanox NIC, the number of VFs cannot be larger than `128`.
<8> The `nicSelector` mapping selects the Ethernet device for the Operator to configure. You do not need to specify values for all the parameters. It is recommended to identify the Ethernet adapter with enough precision to minimize the possibility of selecting an Ethernet device unintentionally.
If you specify `rootDevices`, you must also specify a value for `vendor`, `deviceID`, or `pfNames`.
If you specify both `pfNames` and `rootDevices` at the same time, ensure that they point to an identical device.
<9> Optional: Specify the vendor hex code of the SR-IOV network device. The only allowed values are either `8086` or `15b3`.
<10> Optional: Specify the device hex code of SR-IOV network device. The only allowed values are `158b`, `1015`, `1017`.
<11> Optional: The parameter accepts an array of one or more physical function (PF) names for the Ethernet device.
<12> The parameter accepts an array of one or more PCI bus addresses for the physical function of the Ethernet device. Provide the address in the following format: `0000:02:00.1`.
<13> The `vfio-pci` driver type is required for virtual functions in {VirtProductName}.
<14> Optional: Specify whether to enable remote direct memory access (RDMA) mode. For a Mellanox card, set `isRdma` to `false`. The default value is `false`.
+
[NOTE]
====
If `isRDMA` flag is set to `true`, you can continue to use the RDMA enabled VF as a normal network device.
A device can be used in either mode.
====

. Optional: Label the SR-IOV capable cluster nodes with `SriovNetworkNodePolicy.Spec.NodeSelector` if they are not already labeled. For more information about labeling nodes, see "Understanding how to update labels on nodes".

. Create the `SriovNetworkNodePolicy` object:
+
[source,terminal]
----
$ oc create -f <name>-sriov-node-network.yaml
----
+
where `<name>` specifies the name for this configuration.
+
After applying the configuration update, all the pods in `sriov-network-operator` namespace transition to the `Running` status.

. To verify that the SR-IOV network device is configured, enter the following command. Replace `<node_name>` with the name of a node with the SR-IOV network device that you just configured.
+
[source,terminal]
----
$ oc get sriovnetworknodestates -n openshift-sriov-network-operator <node_name> -o jsonpath='{.status.syncStatus}'
----

:!virt-sriov:


:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * networking/hardware_networks/configuring-sriov-net-attach.adoc
// * virt/vm_networking/virt-connecting-vm-to-sriov.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

// Note: IB does not support ipam with `type=dhcp`.




:_mod-docs-content-type: PROCEDURE

// LEGACY





:leveloffset!:

[id="next-steps_configuring-sriov-network"]
=== Next steps
* xref:../../virt/vm_networking/virt-connecting-vm-to-sriov.adoc#virt-attaching-vm-to-sriov-network_virt-connecting-vm-to-sriov[Attaching a virtual machine (VM) to an SR-IOV network]

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/virtual_machines/virt-accessing-vm-ssh.adoc
// * virt/post_installation_configuration/virt-post-install-network-config.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-enabling-load-balancer-service-web_{context}"]
= Enabling load balancer service creation by using the web console

You can enable the creation of load balancer services for a virtual machine (VM) by using the {product-title} web console.

.Prerequisites

* You have configured a load balancer for the cluster.
* You are logged in as a user with the `cluster-admin` role.

.Procedure

. Navigate to *Virtualization* -> *Overview*.
. On the *Settings* tab, click *Cluster*.
. Expand *LoadBalancer service* and select *Enable the creation of LoadBalancer services for SSH connections to VirtualMachines*.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/k8s-nmstate-installing-the-kubernetes-nmstate-operator,modules/nw-sriov-installing-operator,modules/virt-creating-linux-bridge-nncp,modules/virt-creating-linux-bridge-nad-web,modules/virt-configuring-secondary-network-vm-live-migration,modules/virt-selecting-migration-network-ui,modules/nw-sriov-configuring-device,modules/nw-sriov-network-attachment,modules/virt-enabling-load-balancer-service-web

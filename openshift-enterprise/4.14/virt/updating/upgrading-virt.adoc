:_mod-docs-content-type: ASSEMBLY
[id="upgrading-virt"]
= Updating {VirtProductName}
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: upgrading-virt

toc::[]

Learn how Operator Lifecycle Manager (OLM) delivers z-stream and minor version updates for {VirtProductName}.

[IMPORTANT]
====
Updating to {VirtProductName} 4.13 from {VirtProductName} 4.12.2 is not supported.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: CONCEPT
[id="virt-rhel-9_{context}"]
= {VirtProductName} on {op-system-base} 9

{VirtProductName} {VirtVersion} is based on {op-system-base-full} 9. You can update to {VirtProductName} {VirtVersion} from a version that was based on {op-system-base} 8 by following the standard {VirtProductName} update procedure. No additional steps are required.

As in previous versions, you can perform the update without disrupting running workloads. {VirtProductName} {VirtVersion} supports live migration from {op-system-base} 8 nodes to {op-system-base} 9 nodes.

[id="new-rhel-9-machine-type_{context}"]
== New {op-system-base} 9 machine type

This update also introduces a new {op-system-base} 9 machine type for VMs: `machineType: pc-q35-rhel9.2.0`. All VM templates that are included with {VirtProductName} now use this machine type by default.

Updating {VirtProductName} does not change the `machineType` value of any existing VMs. These VMs continue to function as they did before the update.

While it is not required, you might want to change a VM's machine type to `pc-q35-rhel9.2.0` so that it can benefit from {op-system-base} 9 improvements.

[IMPORTANT]
====
Before you change a VM's `machineType` value, you must shut down the VM.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: CONCEPT
[id="virt-about-upgrading-virt_{context}"]
= About updating {VirtProductName}

* Operator Lifecycle Manager (OLM) manages the lifecycle of the {VirtProductName} Operator. The Marketplace Operator, which is deployed during {product-title} installation, makes external Operators available to your cluster.

* OLM provides z-stream and minor version updates for {VirtProductName}. Minor version updates become available when you update {product-title} to the next minor version. You cannot update {VirtProductName} to the next minor version without first updating {product-title}.

* {VirtProductName} subscriptions use a single update channel that is named *stable*. The *stable* channel ensures that your {VirtProductName} and {product-title} versions are compatible.

* If your subscription's approval strategy is set to *Automatic*, the update process starts as soon as a new version of the Operator is available in the *stable* channel. It is highly recommended to use the *Automatic* approval strategy to maintain a supportable environment. Each minor version of {VirtProductName} is only supported if you run the corresponding {product-title} version. For example, you must run {VirtProductName} {VirtVersion} on {product-title} {VirtVersion}.

** Though it is possible to select the *Manual* approval strategy, this is not recommended because it risks the supportability and functionality of your cluster. With the *Manual* approval strategy, you must manually approve every pending update. If {product-title} and {VirtProductName} updates are out of sync, your cluster becomes unsupported.

* The amount of time an update takes to complete depends on your network
connection. Most automatic updates complete within fifteen minutes.

* Updating {VirtProductName} does not interrupt network connections.

* Data volumes and their associated persistent volume claims are preserved during update.

[IMPORTANT]
====
If you have virtual machines running that use hostpath provisioner storage, they cannot be live migrated and might block an {product-title} cluster update.

As a workaround, you can reconfigure the virtual machines so that they can be powered off automatically during a cluster update. Remove the `evictionStrategy: LiveMigrate` field and set the `runStrategy` field to `Always`.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: CONCEPT
[id="virt-about-workload-updates_{context}"]
= About workload updates

When you update {VirtProductName}, virtual machine workloads, including `libvirt`, `virt-launcher`, and `qemu`, update automatically if they support live migration.

[NOTE]
====
Each virtual machine has a `virt-launcher` pod that runs the virtual machine
instance (VMI). The `virt-launcher` pod runs an instance of `libvirt`, which is
used to manage the virtual machine (VM) process.
====

You can configure how workloads are updated by editing the `spec.workloadUpdateStrategy` stanza of the `HyperConverged` custom resource (CR). There are two available workload update methods: `LiveMigrate` and `Evict`.

Because the `Evict` method shuts down VMI pods, only the `LiveMigrate` update strategy is enabled by default.

When `LiveMigrate` is the only update strategy enabled:

* VMIs that support live migration are migrated during the update process. The VM guest moves into a new pod with the updated components enabled.

* VMIs that do not support live migration are not disrupted or updated.

** If a VMI has the `LiveMigrate` eviction strategy but does not support live migration, it is not updated.

If you enable both `LiveMigrate` and `Evict`:

* VMIs that support live migration use the `LiveMigrate` update strategy.

* VMIs that do not support live migration use the `Evict` update strategy. If a VMI is controlled by a `VirtualMachine` object that has `runStrategy: Always` set, a new VMI is created in a new pod with updated components.

[discrete]
[id="migration-attempts-timeouts_{context}"]
== Migration attempts and timeouts

When updating workloads, live migration fails if a pod is in the `Pending` state for the following periods:

5 minutes:: If the pod is pending because it is `Unschedulable`.

15 minutes:: If the pod is stuck in the pending state for any reason.

When a VMI fails to migrate, the `virt-controller` tries to migrate it again. It repeats this process until all migratable VMIs are running on new `virt-launcher` pods. If a VMI is improperly configured, however, these attempts can repeat indefinitely.

[NOTE]
====
Each attempt corresponds to a migration object. Only the five most recent attempts are held in a buffer. This prevents migration objects from accumulating on the system while retaining information for debugging.
====




:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: CONCEPT
[id="virt-about-eus-updates_{context}"]
= About EUS-to-EUS updates

Every even-numbered minor version of {product-title}, including 4.10 and 4.12, is an Extended Update Support (EUS) version. However, because Kubernetes design mandates serial minor version updates, you cannot directly update from one EUS version to the next.

After you update from the source EUS version to the next odd-numbered minor version, you must sequentially update {VirtProductName} to all z-stream releases of that minor version that are on your update path. When you have upgraded to the latest applicable z-stream version, you can then update {product-title} to the target EUS minor version.

When the {product-title} update succeeds, the corresponding update for {VirtProductName} becomes available. You can now update {VirtProductName} to the target EUS version.

[id="preparing-to-update_{context}"]
== Preparing to update

Before beginning an EUS-to-EUS update, you must:

* Pause worker nodes' machine config pools before you start an EUS-to-EUS update so that the workers are not rebooted twice.

* Disable automatic workload updates before you begin the update process. This is to prevent {VirtProductName} from migrating or evicting your virtual machines (VMs) until you update to your target EUS version.

[NOTE]
====
By default, {VirtProductName} automatically updates workloads, such as the `virt-launcher` pod, when you update the {VirtProductName} Operator. You can configure this behavior in the `spec.workloadUpdateStrategy` stanza of the `HyperConverged` custom resource.
====

// link to EUS to EUS docs in assembly due to module limitations

:leveloffset!:

Learn more about xref:../../updating/updating_a_cluster/eus-eus-update.adoc#eus-eus-update[performing an EUS-to-EUS update].

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-preventing-workload-updates-during-eus-update_{context}"]
= Preventing workload updates during an EUS-to-EUS update

When you update from one Extended Update Support (EUS) version to the next, you must manually disable automatic workload updates to prevent {VirtProductName} from migrating or evicting workloads during the update process.

.Prerequisites

* You are running an EUS version of {product-title} and want to update to the next EUS version. You have not yet updated to the odd-numbered version in between.

* You read "Preparing to perform an EUS-to-EUS update" and learned the caveats and requirements that pertain to your {product-title} cluster.

* You paused the worker nodes' machine config pools as directed by the {product-title} documentation.

* It is recommended that you use the default *Automatic* approval strategy. If you use the *Manual* approval strategy, you must approve all pending updates in the web console. For more details, refer to the "Manually approving a pending Operator update" section.

.Procedure

. Back up the current `workloadUpdateMethods` configuration by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ WORKLOAD_UPDATE_METHODS=$(oc get kv kubevirt-kubevirt-hyperconverged \
  -n {CNVNamespace} -o jsonpath='{.spec.workloadUpdateStrategy.workloadUpdateMethods}')
----

. Turn off all workload update methods by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc patch hyperconverged kubevirt-hyperconverged -n {CNVNamespace} \
  --type json -p '[{"op":"replace","path":"/spec/workloadUpdateStrategy/workloadUpdateMethods", "value":[]}]'
----
+
.Example output
[source,terminal]
----
hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched
----

. Ensure that the `HyperConverged` Operator is `Upgradeable` before you continue. Enter the following command and monitor the output:
+
[source,terminal,subs="attributes+"]
----
$ oc get hyperconverged kubevirt-hyperconverged -n {CNVNamespace} -o json | jq ".status.conditions"
----
+
.Example output
[%collapsible]
====
[source,json]
----
[
  {
    "lastTransitionTime": "2022-12-09T16:29:11Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "True",
    "type": "ReconcileComplete"
  },
  {
    "lastTransitionTime": "2022-12-09T20:30:10Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "True",
    "type": "Available"
  },
  {
    "lastTransitionTime": "2022-12-09T20:30:10Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "False",
    "type": "Progressing"
  },
  {
    "lastTransitionTime": "2022-12-09T16:39:11Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "False",
    "type": "Degraded"
  },
  {
    "lastTransitionTime": "2022-12-09T20:30:10Z",
    "message": "Reconcile completed successfully",
    "observedGeneration": 3,
    "reason": "ReconcileCompleted",
    "status": "True",
    "type": "Upgradeable" <1>
  }
]
----
====
<1> The {VirtProductName} Operator has the `Upgradeable` status.

. Manually update your cluster from the source EUS version to the next minor version of {product-title}:
+
[source,terminal]
+
----
$ oc adm upgrade
----
+
.Verification
* Check the current version by running the following command:
+
[source,terminal]
----
$ oc get clusterversion
----
+
[NOTE]
====
Updating {product-title} to the next version is a prerequisite for updating {VirtProductName}. For more details, refer to the "Updating clusters" section of the {product-title} documentation.
====

. Update {VirtProductName}.
* With the default *Automatic* approval strategy, {VirtProductName} automatically updates to the corresponding version after you update {product-title}.
* If you use the *Manual* approval strategy, approve the pending updates by using the web console.

. Monitor the {VirtProductName} update by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc get csv -n {CNVNamespace}
----

. Update {VirtProductName} to every z-stream version that is available for the non-EUS minor version, monitoring each update by running the command shown in the previous step.

. Confirm that {VirtProductName} successfully updated to the latest z-stream release of the non-EUS version by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc get hyperconverged kubevirt-hyperconverged -n {CNVNamespace} -o json | jq ".status.versions"
----
+
.Example output
[source,terminal,subs="attributes+"]
----
[
  {
    "name": "operator",
    "version": "{HCOVersion}"
  }
]
----

. Wait until the `HyperConverged` Operator has the `Upgradeable` status before you perform the next update. Enter the following command and monitor the output:
+
[source,terminal,subs="attributes+"]
----
$ oc get hyperconverged kubevirt-hyperconverged -n {CNVNamespace} -o json | jq ".status.conditions"
----

. Update {product-title} to the target EUS version.

. Confirm that the update succeeded by checking the cluster version:
+
[source,terminal]
----
$ oc get clusterversion
----

. Update {VirtProductName} to the target EUS version.
* With the default *Automatic* approval strategy, {VirtProductName} automatically updates to the corresponding version after you update {product-title}.
* If you use the *Manual* approval strategy, approve the pending updates by using the web console.

. Monitor the {VirtProductName} update by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc get csv -n {CNVNamespace}
----
+
The update completes when the `VERSION` field matches the target EUS version and the `PHASE` field reads `Succeeded`.

. Restore the workload update methods configuration that you backed up:
+
[source,terminal,subs="attributes+"]
----
$ oc patch hyperconverged kubevirt-hyperconverged -n {CNVNamespace} --type json -p \
  "[{\"op\":\"add\",\"path\":\"/spec/workloadUpdateStrategy/workloadUpdateMethods\", \"value\":$WORKLOAD_UPDATE_METHODS}]"
----
+
.Example output
[source,terminal]
----
hyperconverged.hco.kubevirt.io/kubevirt-hyperconverged patched
----
+
.Verification

* Check the status of VM migration by running the following command:
+
[source,terminal]
----
$ oc get vmim -A
----

.Next steps

* You can now unpause the worker nodes' machine config pools.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-configuring-workload-update-methods_{context}"]
= Configuring workload update methods

You can configure workload update methods by editing the `HyperConverged` custom resource (CR).

.Prerequisites

* To use live migration as an update method, you must first enable live migration in the cluster.
+
[NOTE]
====
If a `VirtualMachineInstance` CR contains `evictionStrategy: LiveMigrate` and the virtual machine instance (VMI) does not support live migration, the VMI will not update.
====

.Procedure

. To open the `HyperConverged` CR in your default editor, run the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc edit hyperconverged kubevirt-hyperconverged -n {CNVNamespace}
----

. Edit the `workloadUpdateStrategy` stanza of the `HyperConverged` CR. For example:
+
[source,yaml]
----
apiVersion: hco.kubevirt.io/v1beta1
kind: HyperConverged
metadata:
  name: kubevirt-hyperconverged
spec:
  workloadUpdateStrategy:
    workloadUpdateMethods: <1>
    - LiveMigrate <2>
    - Evict <3>
    batchEvictionSize: 10 <4>
    batchEvictionInterval: "1m0s" <5>
# ...
----
<1> The methods that can be used to perform automated workload updates. The available values are `LiveMigrate` and `Evict`. If you enable both options as shown in this example, updates use `LiveMigrate` for VMIs that support live migration and `Evict` for any VMIs that do not support live migration. To disable automatic workload updates, you can either remove the `workloadUpdateStrategy` stanza or set `workloadUpdateMethods: []` to leave the array empty.
//NOTE: in 4.10, removing the stanza will not disable the feature.
<2> The least disruptive update method. VMIs that support live migration are updated by migrating the virtual machine (VM) guest into a new pod with the updated components enabled. If `LiveMigrate` is the only workload update method listed, VMIs that do not support live migration are not disrupted or updated.
<3> A disruptive method that shuts down VMI pods during upgrade. `Evict` is the only update method available if live migration is not enabled in the cluster. If a VMI is controlled by a `VirtualMachine` object that has `runStrategy: Always` configured, a new VMI is created in a new pod with updated components.
<4> The number of VMIs that can be forced to be updated at a time by using the `Evict` method. This does not apply to the `LiveMigrate` method.
<5> The interval to wait before evicting the next batch of workloads. This does not apply to the `LiveMigrate` method.
+
[NOTE]
====
You can configure live migration limits and timeouts by editing the `spec.liveMigrationConfig` stanza of the `HyperConverged` CR.
====

. To apply your changes, save and exit the editor.

:leveloffset!:

[id="approving-operator-upgrades_upgrading-virt"]
== Approving pending Operator updates

:leveloffset: +2

// Module included in the following assemblies:
//
// * operators/admin/olm-upgrading-operators.adoc
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: PROCEDURE
[id="olm-approving-pending-upgrade_{context}"]
= Manually approving a pending Operator update

If an installed Operator has the approval strategy in its subscription set to *Manual*, when new updates are released in its current update channel, the update must be manually approved before installation can begin.

.Prerequisites

* An Operator previously installed using Operator Lifecycle Manager (OLM).

.Procedure

. In the *Administrator* perspective of the {product-title} web console, navigate to *Operators -> Installed Operators*.

. Operators that have a pending update display a status with *Upgrade available*. Click the name of the Operator you want to update.

. Click the *Subscription* tab. Any updates requiring approval are displayed next to *Upgrade status*. For example, it might display *1 requires approval*.

. Click *1 requires approval*, then click *Preview Install Plan*.

. Review the resources that are listed as available for update. When satisfied, click *Approve*.

. Navigate back to the *Operators -> Installed Operators* page to monitor the progress of the update. When complete, the status changes to *Succeeded* and *Up to date*.

:leveloffset!:

[id="monitoring-upgrade-status_upgrading-virt"]
== Monitoring update status

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-monitoring-upgrade-status_{context}"]
= Monitoring {VirtProductName} upgrade status

To monitor the status of a {VirtProductName} Operator upgrade, watch the cluster service version (CSV) `PHASE`. You can also monitor the CSV conditions in the web console or by running the command provided here.

[NOTE]
====
The `PHASE` and conditions values are approximations that are based on
available information.
====

.Prerequisites

* Log in to the cluster as a user with the `cluster-admin` role.
* Install the OpenShift CLI (`oc`).

.Procedure

. Run the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc get csv -n {CNVNamespace}
----

. Review the output, checking the `PHASE` field. For example:
+
.Example output
[source,terminal,subs="attributes+"]
----
VERSION  REPLACES                                        PHASE
4.9.0    kubevirt-hyperconverged-operator.v4.8.2         Installing
4.9.0    kubevirt-hyperconverged-operator.v4.9.0         Replacing
----

. Optional: Monitor the aggregated status of all {VirtProductName} component
conditions by running the following command:
+
[source,terminal,subs="attributes+"]
----
$ oc get hyperconverged kubevirt-hyperconverged -n {CNVNamespace} \
  -o=jsonpath='{range .status.conditions[*]}{.type}{"\t"}{.status}{"\t"}{.message}{"\n"}{end}'
----
+
A successful upgrade results in the following output:
+
.Example output
[source,terminal]
----
ReconcileComplete  True  Reconcile completed successfully
Available          True  Reconcile completed successfully
Progressing        False Reconcile completed successfully
Degraded           False Reconcile completed successfully
Upgradeable        True  Reconcile completed successfully
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/updating/upgrading-virt.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-viewing-outdated-workloads_{context}"]
= Viewing outdated {VirtProductName} workloads

You can view a list of outdated workloads by using the CLI.

[NOTE]
====
If there are outdated virtualization pods in your cluster, the `OutdatedVirtualMachineInstanceWorkloads` alert fires.
====

.Procedure

* To view a list of outdated virtual machine instances (VMIs), run the following command:
+
[source,terminal]
----
$ oc get vmi -l kubevirt.io/outdatedLauncherImage --all-namespaces
----

:leveloffset!:

[NOTE]
====
Configure workload updates to ensure that VMIs update automatically.
====

[id="additional-resources_upgrading-virt"]
[role="_additional-resources"]
== Additional resources
* xref:../../updating/updating_a_cluster/eus-eus-update.adoc#eus-eus-update[Performing an EUS-to-EUS update]
* xref:../../operators/understanding/olm-what-operators-are.adoc#olm-what-operators-are[What are Operators?]
* xref:../../operators/understanding/olm/olm-understanding-olm.adoc#olm-understanding-olm[Operator Lifecycle Manager concepts and resources]
* xref:../../operators/understanding/olm/olm-understanding-olm.adoc#olm-csv_olm-understanding-olm[Cluster service versions (CSVs)]
* xref:../../virt/live_migration/virt-about-live-migration.adoc#virt-about-live-migration[About live migration]
* xref:../../virt/nodes/virt-node-maintenance.adoc#eviction-strategies[Configuring eviction strategies]
* xref:../../virt/live_migration/virt-configuring-live-migration.adoc#virt-configuring-live-migration-limits_virt-configuring-live-migration[Configuring live migration limits and timeouts]

//# includes=_attributes/common-attributes,modules/virt-rhel-9,modules/virt-about-upgrading-virt,modules/virt-about-workload-updates,modules/virt-about-eus-updates,modules/virt-preventing-workload-updates-during-eus-update,modules/virt-configuring-workload-update-methods,modules/olm-approving-pending-upgrade,modules/virt-monitoring-upgrade-status,modules/virt-viewing-outdated-workloads

:_mod-docs-content-type: ASSEMBLY
[id="virt-monitoring-vm-health"]
= Virtual machine health checks
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: virt-monitoring-vm-health

toc::[]

You can configure virtual machine (VM) health checks by defining readiness and liveness probes in the `VirtualMachine` resource.

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/support/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: CONCEPT
[id="virt-about-readiness-liveness-probes_{context}"]

= About readiness and liveness probes

Use readiness and liveness probes to detect and handle unhealthy virtual machines (VMs). You can include one or more probes in the specification of the VM to ensure that traffic does not reach a VM that is not ready for it and that a new VM is created when a VM becomes unresponsive.

A _readiness probe_ determines whether a VM is ready to accept service requests. If the probe fails, the VM is removed from the list of available endpoints until the VM is ready.

A _liveness probe_ determines whether a VM is responsive. If the probe fails, the VM is deleted and a new VM is created to restore responsiveness.

You can configure readiness and liveness probes by setting the `spec.readinessProbe` and the `spec.livenessProbe` fields of the `VirtualMachine` object. These fields support the following tests:

HTTP GET:: The probe determines the health of the VM by using a web hook. The test is successful if the HTTP response code is between 200 and 399. You can use an HTTP GET test with applications that return HTTP status codes when they are completely initialized.

TCP socket:: The probe attempts to open a socket to the VM. The VM is only considered healthy if the probe can establish a connection. You can use a TCP socket test with applications that do not start listening until initialization is complete.

Guest agent ping:: The probe uses the `guest-ping` command to determine if the QEMU guest agent is running on the virtual machine.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/support/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-define-http-readiness-probe_{context}"]

= Defining an HTTP readiness probe

Define an HTTP readiness probe by setting the `spec.readinessProbe.httpGet` field of the virtual machine (VM) configuration.


.Procedure
. Include details of the readiness probe in the VM configuration file.
+

.Sample readiness probe with an HTTP GET test
[source,yaml]
----
# ...
spec:
  readinessProbe:
    httpGet: <1>
      port: 1500 <2>
      path: /healthz <3>
      httpHeaders:
      - name: Custom-Header
        value: Awesome
    initialDelaySeconds: 120 <4>
    periodSeconds: 20 <5>
    timeoutSeconds: 10 <6>
    failureThreshold: 3 <7>
    successThreshold: 3 <8>
# ...
----
<1> The HTTP GET request to perform to connect to the VM.
<2> The port of the VM that the probe queries. In the above example, the probe queries port 1500.
<3> The path to access on the HTTP server. In the above example, if the handler for the server’s /healthz path returns a success code, the VM is considered to be healthy. If the handler returns a failure code, the VM is removed from the list of available endpoints.
<4> The time, in seconds, after the VM starts before the readiness probe is initiated.
<5> The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than `timeoutSeconds`.
<6> The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than `periodSeconds`.
<7> The number of times that the probe is allowed to fail. The default is 3. After the specified number of attempts, the pod is marked `Unready`.
<8> The number of times that the probe must report success, after a failure, to be considered successful. The default is 1.

. Create the VM by running the following command:
+
[source,terminal]
----
$ oc create -f <file_name>.yaml
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/support/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-define-tcp-readiness-probe_{context}"]

= Defining a TCP readiness probe

Define a TCP readiness probe by setting the `spec.readinessProbe.tcpSocket` field of the virtual machine (VM) configuration.


.Procedure

. Include details of the TCP readiness probe in the VM configuration file.
+

.Sample readiness probe with a TCP socket test
[source,yaml]
----
# ...
spec:
  readinessProbe:
    initialDelaySeconds: 120 <1>
    periodSeconds: 20 <2>
    tcpSocket: <3>
      port: 1500 <4>
    timeoutSeconds: 10 <5>
# ...
----
<1> The time, in seconds, after the VM starts before the readiness probe is initiated.
<2> The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than `timeoutSeconds`.
<3> The TCP action to perform.
<4> The port of the VM that the probe queries.
<5> The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than `periodSeconds`.

. Create the VM by running the following command:
+
[source,terminal]
----
$ oc create -f <file_name>.yaml
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/support/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-define-http-liveness-probe_{context}"]

= Defining an HTTP liveness probe

Define an HTTP liveness probe by setting the `spec.livenessProbe.httpGet` field of the virtual machine (VM) configuration. You can define both HTTP and TCP tests for liveness probes in the same way as readiness probes. This procedure configures a sample liveness probe with an HTTP GET test.


.Procedure

. Include details of the HTTP liveness probe in the VM configuration file.
+

.Sample liveness probe with an HTTP GET test
[source,yaml]
----
# ...
spec:
  livenessProbe:
    initialDelaySeconds: 120 <1>
    periodSeconds: 20 <2>
    httpGet: <3>
      port: 1500 <4>
      path: /healthz <5>
      httpHeaders:
      - name: Custom-Header
        value: Awesome
    timeoutSeconds: 10 <6>
# ...
----
<1> The time, in seconds, after the VM starts before the liveness probe is initiated.
<2> The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than `timeoutSeconds`.
<3> The HTTP GET request to perform to connect to the VM.
<4> The port of the VM that the probe queries. In the above example, the probe queries port 1500. The VM installs and runs a minimal HTTP server on port 1500 via cloud-init.
<5> The path to access on the HTTP server. In the above example, if the handler for the server's `/healthz` path returns a success code, the VM is considered to be healthy. If the handler returns a failure code, the VM is deleted and a new VM is created.
<6> The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than `periodSeconds`.

. Create the VM by running the following command:
+
[source,terminal]
----
$ oc create -f <file_name>.yaml
----

:leveloffset!:

[id="watchdog_{context}"]
== Defining a watchdog

You can define a watchdog to monitor the health of the guest operating system by performing the following steps:

. Configure a watchdog device for the virtual machine (VM).
. Install the watchdog agent on the guest.

The watchdog device monitors the agent and performs one of the following actions if the guest operating system is unresponsive:

* `poweroff`: The VM powers down immediately. If `spec.running` is set to `true` or `spec.runStrategy` is not set to `manual`, then the VM reboots.
* `reset`: The VM reboots in place and the guest operating system cannot react.
+
[NOTE]
====
The reboot time might cause liveness probes to time out. If cluster-level protections detect a failed liveness probe, the VM might be forcibly rescheduled, increasing the reboot time.
====

* `shutdown`: The VM gracefully powers down by stopping all services.

[NOTE]
====
Watchdog is not available for Windows VMs.
====

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/monitoring/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-defining-watchdog-device-vm"]
= Configuring a watchdog device for the virtual machine

You configure a watchdog device for the virtual machine (VM).

.Prerequisites

* The VM must have kernel support for an `i6300esb` watchdog device. {op-system-base-full} images support `i6300esb`.

.Procedure

. Create a `YAML` file with the following contents:
+
[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  labels:
    kubevirt.io/vm: vm2-rhel84-watchdog
  name: <vm-name>
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm2-rhel84-watchdog
    spec:
      domain:
        devices:
          watchdog:
            name: <watchdog>
            i6300esb:
              action: "poweroff" <1>
# ...
----
<1> Specify `poweroff`, `reset`, or `shutdown`.
+
The example above configures the `i6300esb` watchdog device on a RHEL8 VM with the poweroff action and exposes the device as `/dev/watchdog`.
+
This device can now be used by the watchdog binary.

. Apply the YAML file to your cluster by running the following command:
+
[source,yaml]
----
$ oc apply -f <file_name>.yaml
----

.Verification

--
[IMPORTANT]
====
This procedure is provided for testing watchdog functionality only and must not be run on production machines.
====
--

. Run the following command to verify that the VM is connected to the watchdog device:
+
[source,terminal]
----
$ lspci | grep watchdog -i
----

. Run one of the following commands to confirm the watchdog is active:

* Trigger a kernel panic:
+
[source,terminal]
----
# echo c > /proc/sysrq-trigger
----

* Stop the watchdog service:
+
[source,terminal]
----
# pkill -9 watchdog
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * virt/monitoring/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-installing-watchdog-agent_{context}"]
= Installing the watchdog agent on the guest

You install the watchdog agent on the guest and start the `watchdog` service.

.Procedure

. Log in to the virtual machine as root user.

. Install the `watchdog` package and its dependencies:
+
[source,terminal]
----
# yum install watchdog
----

. Uncomment the following line in the `/etc/watchdog.conf` file and save the changes:
+
[source,terminal]
----
#watchdog-device = /dev/watchdog
----

. Enable the `watchdog` service to start on boot:

+
[source,terminal]
----
# systemctl enable --now watchdog.service
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/support/virt-monitoring-vm-health.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-define-guest-agent-ping-probe_{context}"]

= Defining a guest agent ping probe

Define a guest agent ping probe by setting the `spec.readinessProbe.guestAgentPing` field of the virtual machine (VM) configuration.

:FeatureName: The guest agent ping probe
// When including this file, ensure that {FeatureName} is set immediately before
// the include. Otherwise it will result in an incorrect replacement.

[IMPORTANT]
====
[subs="attributes+"]
{FeatureName} is a Technology Preview feature only. Technology Preview features are not supported with Red Hat production service level agreements (SLAs) and might not be functionally complete. Red Hat does not recommend using them in production. These features provide early access to upcoming product features, enabling customers to test functionality and provide feedback during the development process.

For more information about the support scope of Red Hat Technology Preview features, see link:https://access.redhat.com/support/offerings/techpreview/[Technology Preview Features Support Scope].
====
// Undefine {FeatureName} attribute, so that any mistakes are easily spotted
:!FeatureName:

.Prerequisites

* The QEMU guest agent must be installed and enabled on the virtual machine.

.Procedure

. Include details of the guest agent ping probe in the VM configuration file. For example:
+

.Sample guest agent ping probe
[source,yaml]
----
# ...
spec:
  readinessProbe:
    guestAgentPing: {} <1>
    initialDelaySeconds: 120 <2>
    periodSeconds: 20 <3>
    timeoutSeconds: 10 <4>
    failureThreshold: 3 <5>
    successThreshold: 3 <6>
# ...
----
<1> The guest agent ping probe to connect to the VM.
<2> Optional: The time, in seconds, after the VM starts before the guest agent probe is initiated.
<3> Optional: The delay, in seconds, between performing probes. The default delay is 10 seconds. This value must be greater than `timeoutSeconds`.
<4> Optional: The number of seconds of inactivity after which the probe times out and the VM is assumed to have failed. The default value is 1. This value must be lower than `periodSeconds`.
<5> Optional: The number of times that the probe is allowed to fail. The default is 3. After the specified number of attempts, the pod is marked `Unready`.
<6> Optional: The number of times that the probe must report success, after a failure, to be considered successful. The default is 1.

. Create the VM by running the following command:
+
[source,terminal]
----
$ oc create -f <file_name>.yaml
----

:leveloffset!:

[id="additional-resources_monitoring-vm-health"]
[role="_additional-resources"]
== Additional resources

* xref:../../applications/application-health.adoc#application-health[Monitoring application health by using health checks]

//# includes=_attributes/common-attributes,modules/virt-about-readiness-liveness-probes,modules/virt-define-http-readiness-probe,modules/virt-define-tcp-readiness-probe,modules/virt-define-http-liveness-probe,modules/virt-defining-watchdog-device-vm,modules/virt-installing-watchdog-agent,modules/virt-define-guest-agent-ping-probe,modules/snippets/technology-preview

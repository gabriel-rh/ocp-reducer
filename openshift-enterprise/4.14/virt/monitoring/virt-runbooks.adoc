:_mod-docs-content-type: ASSEMBLY
[id="virt-runbooks"]
= {VirtProductName} runbooks
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: virt-runbooks

toc::[]

You can use the procedures in these runbooks to diagnose and resolve issues that trigger {VirtProductName} xref:../../monitoring/managing-alerts.adoc#managing-alerts[alerts].

{VirtProductName} alerts are displayed on the *Virtualization* -> *Overview* -> xref:../../virt/getting_started/virt-web-console-overview.adoc#overview-overview_virt-web-console-overview[*Overview* tab] in the web console.

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-CDIDataImportCronOutdated"]
= CDIDataImportCronOutdated

[discrete]
[id="meaning-cdidataimportcronoutdated"]
== Meaning

This alert fires when `DataImportCron` cannot poll or import the latest disk
image versions.

`DataImportCron` polls disk images, checking for the latest versions, and
imports the images as persistent volume claims (PVCs). This process ensures
that PVCs are updated to the latest version so that they can be used as
reliable clone sources or golden images for virtual machines (VMs).

For golden images, _latest_ refers to the latest operating system of the
distribution. For other disk images, _latest_ refers to the latest hash of the
image that is available.

[discrete]
[id="impact-cdidataimportcronoutdated"]
== Impact

VMs might be created from outdated disk images.

VMs might fail to start because no source PVC is available for cloning.

[discrete]
[id="diagnosis-cdidataimportcronoutdated"]
== Diagnosis

. Check the cluster for a default storage class:
+
[source,terminal]
----
$ oc get sc
----
+
The output displays the storage classes with `(default)` beside the name
of the default storage class. You must set a default storage class, either on
the cluster or in the `DataImportCron` specification, in order for the
`DataImportCron` to poll and import golden images. If no storage class is
defined, the DataVolume controller fails to create PVCs and the following
event is displayed: `DataVolume.storage spec is missing accessMode and no
storageClass to choose profile`.

. Obtain the `DataImportCron` namespace and name:
+
[source,terminal]
----
$ oc get dataimportcron -A -o json | jq -r '.items[] | \
  select(.status.conditions[] | select(.type == "UpToDate" and \
  .status == "False")) | .metadata.namespace + "/" + .metadata.name'
----

. If a default storage class is not defined on the cluster, check the
`DataImportCron` specification for a default storage class:
+
[source,terminal]
----
$ oc get dataimportcron <dataimportcron> -o yaml | \
  grep -B 5 storageClassName
----
+
.Example output
+
[source,yaml]
----
      url: docker://.../cdi-func-test-tinycore
    storage:
      resources:
        requests:
          storage: 5Gi
    storageClassName: rook-ceph-block
----

. Obtain the name of the `DataVolume` associated with the `DataImportCron`
object:
+
[source,terminal]
----
$ oc -n <namespace> get dataimportcron <dataimportcron> -o json | \
  jq .status.lastImportedPVC.name
----

. Check the `DataVolume` log for error messages:
+
[source,terminal]
----
$ oc -n <namespace> get dv <datavolume> -o yaml
----

. Set the `CDI_NAMESPACE` environment variable:
+
[source,terminal]
----
$ export CDI_NAMESPACE="$(oc get deployment -A | \
  grep cdi-operator | awk '{print $1}')"
----

. Check the `cdi-deployment` log for error messages:
+
[source,terminal]
----
$ oc logs -n $CDI_NAMESPACE deployment/cdi-deployment
----

[discrete]
[id="mitigation-cdidataimportcronoutdated"]
== Mitigation

. Set a default storage class, either on the cluster or in the `DataImportCron`
specification, to poll and import golden images. The updated Containerized Data
Importer (CDI) will resolve the issue within a few seconds.
. If the issue does not resolve itself, delete the data volumes associated
with the affected `DataImportCron` objects. The CDI will recreate the data
volumes with the default storage class.
. If your cluster is installed in a restricted network environment, disable
the `enableCommonBootImageImport` feature gate in order to opt out of automatic
updates:
+
[source,terminal]
----
$ oc patch hco kubevirt-hyperconverged -n $CDI_NAMESPACE --type json \
  -p '[{"op": "replace", "path": \
  "/spec/featureGates/enableCommonBootImageImport", "value": false}]'
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case, attaching
the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-CDIDataVolumeUnusualRestartCount"]
= CDIDataVolumeUnusualRestartCount

[discrete]
[id="meaning-cdidatavolumeunusualrestartcount"]
== Meaning

This alert fires when a `DataVolume` object restarts more than three times.

[discrete]
[id="impact-cdidatavolumeunusualrestartcount"]
== Impact

Data volumes are responsible for importing and creating a virtual machine disk
on a persistent volume claim. If a data volume restarts more than three times,
these operations are unlikely to succeed. You must diagnose and resolve the issue.

[discrete]
[id="diagnosis-cdidatavolumeunusualrestartcount"]
== Diagnosis

. Find Containerized Data Importer (CDI) pods with more than three restarts:
+
[source,terminal]
----
$ oc get pods --all-namespaces -l app=containerized-data-importer \
  -o=jsonpath='{range .items[?(@.status.containerStatuses[0].restartCount>3)]}{.metadata.name}{"/"}{.metadata.namespace}{"\n"}'
----

. Obtain the details of the pods:
+
[source,terminal]
----
$ oc -n <namespace> describe pods <pod>
----

. Check the pod logs for error messages:
+
[source,terminal]
----
$ oc -n <namespace> logs <pod>
----

[discrete]
[id="mitigation-cdidatavolumeunusualrestartcount"]
== Mitigation

Delete the data volume, resolve the issue, and create a new data volume.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the Diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-CDINotReady"]
= CDINotReady

[discrete]
[id="meaning-cdinotready"]
== Meaning

This alert fires when the Containerized Data Importer (CDI) is in
a degraded state:

* Not progressing
* Not available to use

[discrete]
[id="impact-cdinotready"]
== Impact

CDI is not usable, so users cannot build virtual machine disks on
persistent volume claims (PVCs) using CDI's data volumes.
CDI components are not ready and they stopped progressing towards
a ready state.

[discrete]
[id="diagnosis-cdinotready"]
== Diagnosis

. Set the `CDI_NAMESPACE` environment variable:
+
[source,terminal]
----
$ export CDI_NAMESPACE="$(oc get deployment -A | \
  grep cdi-operator | awk '{print $1}')"
----

. Check the CDI deployment for components that are not ready:
+
[source,terminal]
----
$ oc -n $CDI_NAMESPACE get deploy -l cdi.kubevirt.io
----

. Check the details of the failing pod:
+
[source,terminal]
----
$ oc -n $CDI_NAMESPACE describe pods <pod>
----

. Check the logs of the failing pod:
+
[source,terminal]
----
$ oc -n $CDI_NAMESPACE logs <pod>
----

[discrete]
[id="mitigation-cdinotready"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-CDIOperatorDown"]
= CDIOperatorDown

[discrete]
[id="meaning-cdioperatordown"]
== Meaning

This alert fires when the Containerized Data Importer (CDI) Operator is down.
The CDI Operator deploys and manages the CDI infrastructure components, such
as data volume and persistent volume claim (PVC) controllers. These controllers
help users build virtual machine disks on PVCs.

[discrete]
[id="impact-cdioperatordown"]
== Impact

The CDI components might fail to deploy or to stay in a required state. The
CDI installation might not function correctly.

[discrete]
[id="diagnosis-cdioperatordown"]
== Diagnosis

. Set the `CDI_NAMESPACE` environment variable:
+
[source,terminal]
----
$ export CDI_NAMESPACE="$(oc get deployment -A | grep cdi-operator | \
  awk '{print $1}')"
----

. Check whether the `cdi-operator` pod is currently running:
+
[source,terminal]
----
$ oc -n $CDI_NAMESPACE get pods -l name=cdi-operator
----

. Obtain the details of the `cdi-operator` pod:
+
[source,terminal]
----
$ oc -n $CDI_NAMESPACE describe pods -l name=cdi-operator
----

. Check the log of the `cdi-operator` pod for errors:
+
[source,terminal]
----
$ oc -n $CDI_NAMESPACE logs -l name=cdi-operator
----

[discrete]
[id="mitigation-cdioperatordown"]
== Mitigation

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-CDIStorageProfilesIncomplete"]
= CDIStorageProfilesIncomplete

[discrete]
[id="meaning-cdistorageprofilesincomplete"]
== Meaning

This alert fires when a Containerized Data Importer (CDI) storage profile is
incomplete.

If a storage profile is incomplete, the CDI cannot infer persistent volume claim
(PVC) fields, such as `volumeMode` and  `accessModes`, which are required to
create a virtual machine (VM) disk.

[discrete]
[id="impact-cdistorageprofilesincomplete"]
== Impact

The CDI cannot create a VM disk on the PVC.

[discrete]
[id="diagnosis-cdistorageprofilesincomplete"]
== Diagnosis

* Identify the incomplete storage profile:
+
[source,terminal]
----
$ oc get storageprofile <storage_class>
----

[discrete]
[id="mitigation-cdistorageprofilesincomplete"]
== Mitigation

* Add the missing storage profile information as in the following
example:
+
[source,terminal]
----
$ oc patch storageprofile local --type=merge -p '{"spec": \
  {"claimPropertySets": [{"accessModes": ["ReadWriteOnce"], \
  "volumeMode": "Filesystem"}]}}'
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-CnaoDown"]
= CnaoDown

[discrete]
[id="meaning-cnaodown"]
== Meaning

This alert fires when the Cluster Network Addons Operator (CNAO) is down.
The CNAO deploys additional networking components on top of the cluster.

[discrete]
[id="impact-cnaodown"]
== Impact

If the CNAO is not running, the cluster cannot reconcile changes to virtual
machine components. As a result, the changes might fail to take effect.

[discrete]
[id="diagnosis-cnaodown"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get deployment -A | \
  grep cluster-network-addons-operator | awk '{print $1}')"
----

. Check the status of the `cluster-network-addons-operator` pod:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l name=cluster-network-addons-operator
----

. Check the `cluster-network-addons-operator` logs for error messages:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs -l name=cluster-network-addons-operator
----

. Obtain the details of the `cluster-network-addons-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods -l name=cluster-network-addons-operator
----

[discrete]
[id="mitigation-cnaodown"]
== Mitigation

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-HCOInstallationIncomplete"]
= HCOInstallationIncomplete

[discrete]
[id="meaning-hcoinstallationincomplete"]
== Meaning

This alert fires when the HyperConverged Cluster Operator (HCO) runs for
more than an hour without a `HyperConverged` custom resource (CR).

This alert has the following causes:

* During the installation process, you installed the HCO but you did not
create the `HyperConverged` CR.
* During the uninstall process, you removed the `HyperConverged` CR before
uninstalling the HCO and the HCO is still running.

[discrete]
[id="mitigation-hcoinstallationincomplete"]
== Mitigation

The mitigation depends on whether you are installing or uninstalling
the HCO:

* Complete the installation by creating a `HyperConverged` CR with its
default values:
+
[source,terminal]
----
$ cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: hco-operatorgroup
  namespace: kubevirt-hyperconverged
spec: {}
EOF
----

* Uninstall the HCO. If the uninstall process continues to run, you must
resolve that issue in order to cancel the alert.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-HPPNotReady"]
= HPPNotReady

[discrete]
[id="meaning-hppnotready"]
== Meaning

This alert fires when a hostpath provisioner (HPP) installation is in a
degraded state.

The HPP dynamically provisions hostpath volumes to provide storage for
persistent volume claims (PVCs).

[discrete]
[id="impact-hppnotready"]
== Impact

HPP is not usable. Its components are not ready and they are not progressing
towards a ready state.

[discrete]
[id="diagnosis-hppnotready"]
== Diagnosis

. Set the `HPP_NAMESPACE` environment variable:
+
[source,terminal]
----
$ export HPP_NAMESPACE="$(oc get deployment -A | \
  grep hostpath-provisioner-operator | awk '{print $1}')"
----

. Check for HPP components that are currently not ready:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE get all -l k8s-app=hostpath-provisioner
----

. Obtain the details of the failing pod:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE describe pods <pod>
----

. Check the logs of the failing pod:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE logs <pod>
----

[discrete]
[id="mitigation-hppnotready"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-HPPOperatorDown"]
= HPPOperatorDown

[discrete]
[id="meaning-hppoperatordown"]
== Meaning

This alert fires when the hostpath provisioner (HPP) Operator is down.

The HPP Operator deploys and manages the HPP infrastructure components, such
as the daemon set that provisions hostpath volumes.

[discrete]
[id="impact-hppoperatordown"]
== Impact

The HPP components might fail to deploy or to remain in the required state.
As a result, the HPP installation might not work correctly in the cluster.

[discrete]
[id="diagnosis-hppoperatordown"]
== Diagnosis

. Configure the `HPP_NAMESPACE` environment variable:
+
[source,terminal]
----
$ HPP_NAMESPACE="$(oc get deployment -A | grep \
  hostpath-provisioner-operator | awk '{print $1}')"
----

. Check whether the `hostpath-provisioner-operator` pod is currently running:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE get pods -l name=hostpath-provisioner-operator
----

. Obtain the details of the `hostpath-provisioner-operator` pod:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE describe pods -l name=hostpath-provisioner-operator
----

. Check the log of the `hostpath-provisioner-operator` pod for errors:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE logs -l name=hostpath-provisioner-operator
----

[discrete]
[id="mitigation-hppoperatordown"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-HPPSharingPoolPathWithOS"]
= HPPSharingPoolPathWithOS

[discrete]
[id="meaning-hppsharingpoolpathwithos"]
== Meaning

This alert fires when the hostpath provisioner (HPP) shares a file
system with other critical components, such as `kubelet` or the operating
system (OS).

HPP dynamically provisions hostpath volumes to provide storage for
persistent volume claims (PVCs).

[discrete]
[id="impact-hppsharingpoolpathwithos"]
== Impact

A shared hostpath pool puts pressure on the node's disks. The node
might have degraded performance and stability.

[discrete]
[id="diagnosis-hppsharingpoolpathwithos"]
== Diagnosis

. Configure the `HPP_NAMESPACE` environment variable:
+
[source,terminal]
----
$ export HPP_NAMESPACE="$(oc get deployment -A | \
  grep hostpath-provisioner-operator | awk '{print $1}')"
----

. Obtain the status of the `hostpath-provisioner-csi` daemon set
pods:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE get pods | grep hostpath-provisioner-csi
----

. Check the `hostpath-provisioner-csi` logs to identify the shared
pool and path:
+
[source,terminal]
----
$ oc -n $HPP_NAMESPACE logs <csi_daemonset> -c hostpath-provisioner
----
+
.Example output
+
[source,text]
----
I0208 15:21:03.769731       1 utils.go:221] pool (<legacy, csi-data-dir>/csi),
shares path with OS which can lead to node disk pressure
----

[discrete]
[id="mitigation-hppsharingpoolpathwithos"]
== Mitigation

Using the data obtained in the Diagnosis section, try to prevent the
pool path from being shared with the OS. The specific steps vary based
on the node and other circumstances.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubemacpoolDown"]
= KubemacpoolDown

[discrete]
[id="meaning-kubemacpooldown"]
== Meaning

`KubeMacPool` is down. `KubeMacPool` is responsible for allocating MAC
addresses and preventing MAC address conflicts.

[discrete]
[id="impact-kubemacpooldown"]
== Impact

If `KubeMacPool` is down, `VirtualMachine` objects cannot be created.

[discrete]
[id="diagnosis-kubemacpooldown"]
== Diagnosis

. Set the `KMP_NAMESPACE` environment variable:
+
[source,terminal]
----
$ export KMP_NAMESPACE="$(oc get pod -A --no-headers -l \
  control-plane=mac-controller-manager | awk '{print $1}')"
----

. Set the `KMP_NAME` environment variable:
+
[source,terminal]
----
$ export KMP_NAME="$(oc get pod -A --no-headers -l \
  control-plane=mac-controller-manager | awk '{print $2}')"
----

. Obtain the `KubeMacPool-manager` pod details:
+
[source,terminal]
----
$ oc describe pod -n $KMP_NAMESPACE $KMP_NAME
----

. Check the `KubeMacPool-manager` logs for error messages:
+
[source,terminal]
----
$ oc logs -n $KMP_NAMESPACE $KMP_NAME
----

[discrete]
[id="mitigation-kubemacpooldown"]
== Mitigation

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubeMacPoolDuplicateMacsFound"]
= KubeMacPoolDuplicateMacsFound

[discrete]
[id="meaning-kubemacpoolduplicatemacsfound"]
== Meaning

This alert fires when `KubeMacPool` detects duplicate MAC addresses.

`KubeMacPool` is responsible for allocating MAC addresses and preventing MAC
address conflicts. When `KubeMacPool` starts, it scans the cluster for the MAC
addresses of virtual machines (VMs) in managed namespaces.

[discrete]
[id="impact-kubemacpoolduplicatemacsfound"]
== Impact

Duplicate MAC addresses on the same LAN might cause network issues.

[discrete]
[id="diagnosis-kubemacpoolduplicatemacsfound"]
== Diagnosis

. Obtain the namespace and the name of the `kubemacpool-mac-controller` pod:
+
[source,terminal]
----
$ oc get pod -A -l control-plane=mac-controller-manager --no-headers \
  -o custom-columns=":metadata.namespace,:metadata.name"
----

. Obtain the duplicate MAC addresses from the `kubemacpool-mac-controller`
logs:
+
[source,terminal]
----
$ oc logs -n <namespace> <kubemacpool_mac_controller> | \
  grep "already allocated"
----
+
.Example output
+
[source,text]
----
mac address 02:00:ff:ff:ff:ff already allocated to
vm/kubemacpool-test/testvm, br1,
conflict with: vm/kubemacpool-test/testvm2, br1
----

[discrete]
[id="mitigation-kubemacpoolduplicatemacsfound"]
== Mitigation

. Update the VMs to remove the duplicate MAC addresses.
. Restart the `kubemacpool-mac-controller` pod:
+
[source,terminal]
----
$ oc delete pod -n <namespace> <kubemacpool_mac_controller>
----

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-kubevirtcomponentexceedsrequestedcpu_{context}"]
= KubeVirtComponentExceedsRequestedCPU

[discrete]
[id="meaning-kubevirtcomponentexceedsrequestedcpu"]
== Meaning

This alert fires when a component's CPU usage exceeds the requested limit.

[discrete]
[id="impact-kubevirtcomponentexceedsrequestedcpu"]
== Impact

Usage of CPU resources is not optimal and the node might be overloaded.

[discrete]
[id="diagnosis-kubevirtcomponentexceedsrequestedcpu"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the component's CPU request limit:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment <component> -o yaml | grep requests: -A 2
----

. Check the actual CPU usage by using a PromQL query:
+
[source,text]
----
node_namespace_pod_container:container_cpu_usage_seconds_total:sum_rate
{namespace="$NAMESPACE",container="<component>"}
----

See the
link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus documentation]
for more information.

[discrete]
[id="mitigation-kubevirtcomponentexceedsrequestedcpu"]
== Mitigation

Update the CPU request limit in the `HCO` custom resource.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubeVirtComponentExceedsRequestedMemory"]
= KubeVirtComponentExceedsRequestedMemory

[discrete]
[id="meaning-kubevirtcomponentexceedsrequestedmemory"]
== Meaning

This alert fires when a component's memory usage exceeds the requested limit.

[discrete]
[id="impact-kubevirtcomponentexceedsrequestedmemory"]
== Impact

Usage of memory resources is not optimal and the node might be overloaded.

[discrete]
[id="diagnosis-kubevirtcomponentexceedsrequestedmemory"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the component's memory request limit:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deployment <component> -o yaml | \
  grep requests: -A 2
----

. Check the actual memory usage by using a PromQL query:
+
[source,text]
----
container_memory_usage_bytes{namespace="$NAMESPACE",container="<component>"}
----

See the
link:https://prometheus.io/docs/prometheus/latest/querying/basics/[Prometheus documentation]
for more information.

[discrete]
[id="mitigation-kubevirtcomponentexceedsrequestedmemory"]
== Mitigation

Update the memory request limit in the `HCO` custom resource.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubeVirtCRModified"]
= KubeVirtCRModified

[discrete]
[id="meaning-kubevirtcrmodified"]
== Meaning

This alert fires when an operand of the HyperConverged Cluster Operator (HCO)
is changed by someone or something other than HCO.

HCO configures {VirtProductName} and its supporting operators in an
opinionated way and overwrites its operands when there is an unexpected change
to them. Users must not modify the operands directly. The `HyperConverged`
custom resource is the source of truth for the configuration.

[discrete]
[id="impact-kubevirtcrmodified"]
== Impact

Changing the operands manually causes the cluster configuration to fluctuate
and might lead to instability.

[discrete]
[id="diagnosis-kubevirtcrmodified"]
== Diagnosis

* Check the `component_name` value in the alert details to determine the operand
kind (`kubevirt`) and the operand name (`kubevirt-kubevirt-hyperconverged`)
that are being changed:
+
[source,text]
----
Labels
  alertname=KubevirtHyperconvergedClusterOperatorCRModification
  component_name=kubevirt/kubevirt-kubevirt-hyperconverged
  severity=warning
----

[discrete]
[id="mitigation-kubevirtcrmodified"]
== Mitigation

Do not change the HCO operands directly. Use `HyperConverged` objects to configure
the cluster.

The alert resolves itself after 10 minutes if the operands are not changed manually.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubeVirtDeprecatedAPIRequested"]
= KubeVirtDeprecatedAPIRequested

[discrete]
[id="meaning-kubevirtdeprecatedapirequested"]
== Meaning

This alert fires when a deprecated `KubeVirt` API is used.

[discrete]
[id="impact-kubevirtdeprecatedapirequested"]
== Impact

Using a deprecated API is not recommended because the request will
fail when the API is removed in a future release.

[discrete]
[id="diagnosis-kubevirtdeprecatedapirequested"]
== Diagnosis

* Check the *Description* and *Summary* sections of the alert to identify the
deprecated API as in the following example:
+
*Description*
+
`Detected requests to the deprecated virtualmachines.kubevirt.io/v1alpha3 API.`
+
*Summary*
+
`2 requests were detected in the last 10 minutes.`

[discrete]
[id="mitigation-kubevirtdeprecatedapirequested"]
== Mitigation

Use fully supported APIs. The alert resolves itself after 10 minutes if the deprecated
API is not used.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubeVirtNoAvailableNodesToRunVMs"]
= KubeVirtNoAvailableNodesToRunVMs

[discrete]
[id="meaning-kubevirtnoavailablenodestorunvms"]
== Meaning

This alert fires when the node CPUs in the cluster do not support virtualization
or the virtualization extensions are not enabled.

[discrete]
[id="impact-kubevirtnoavailablenodestorunvms"]
== Impact

The nodes must support virtualization and the virtualization features must be
enabled in the BIOS to run virtual machines (VMs).

[discrete]
[id="diagnosis-kubevirtnoavailablenodestorunvms"]
== Diagnosis

* Check the nodes for hardware virtualization support:
+
[source,terminal]
----
$ oc get nodes -o json|jq '.items[]|{"name": .metadata.name, "kvm": .status.allocatable["devices.kubevirt.io/kvm"]}'
----
+
.Example output
+
[source,text]
----
{
  "name": "shift-vwpsz-master-0",
  "kvm": null
}
{
  "name": "shift-vwpsz-master-1",
  "kvm": null
}
{
  "name": "shift-vwpsz-master-2",
  "kvm": null
}
{
  "name": "shift-vwpsz-worker-8bxkp",
  "kvm": "1k"
}
{
  "name": "shift-vwpsz-worker-ctgmc",
  "kvm": "1k"
}
{
  "name": "shift-vwpsz-worker-gl5zl",
  "kvm": "1k"
}
----
+
Nodes with `"kvm": null` or `"kvm": 0` do not support virtualization extensions.
+
Nodes with `"kvm": "1k"` do support virtualization extensions.

[discrete]
[id="mitigation-kubevirtnoavailablenodestorunvms"]
== Mitigation

Ensure that hardware and CPU virtualization extensions are enabled on all nodes
and that the nodes are correctly labeled.

See link:https://access.redhat.com/solutions/5106121[{VirtProductName} reports no nodes are available, cannot start VMs]
for details.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubevirtVmHighMemoryUsage"]
= KubevirtVmHighMemoryUsage

[discrete]
[id="meaning-kubevirtvmhighmemoryusage"]
== Meaning

This alert fires when a container hosting a virtual machine (VM) has less
than 20 MB free memory.

[discrete]
[id="impact-kubevirtvmhighmemoryusage"]
== Impact

The virtual machine running inside the container is terminated by the runtime
if the container's memory limit is exceeded.

[discrete]
[id="diagnosis-kubevirtvmhighmemoryusage"]
== Diagnosis

. Obtain the `virt-launcher` pod details:
+
[source,terminal]
----
$ oc get pod <virt-launcher> -o yaml
----

. Identify `compute` container processes with high memory usage in the
`virt-launcher` pod:
+
[source,terminal]
----
$ oc exec -it <virt-launcher> -c compute -- top
----

[discrete]
[id="mitigation-kubevirtvmhighmemoryusage"]
== Mitigation

* Increase the memory limit in the `VirtualMachine` specification as in
the following example:
+
[source,yaml]
----
spec:
  running: false
  template:
    metadata:
      labels:
        kubevirt.io/vm: vm-name
    spec:
      domain:
        resources:
          limits:
            memory: 200Mi
          requests:
            memory: 128Mi
----

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-KubeVirtVMIExcessiveMigrations"]
= KubeVirtVMIExcessiveMigrations

[discrete]
[id="meaning-kubevirtvmiexcessivemigrations"]
== Meaning

This alert fires when a virtual machine instance (VMI) live migrates more than
12 times over a period of 24 hours.

This migration rate is abnormally high, even during an upgrade. This alert might
indicate a problem in the cluster infrastructure, such as network disruptions
or insufficient resources.

[discrete]
[id="impact-kubevirtvmiexcessivemigrations"]
== Impact

A virtual machine (VM) that migrates too frequently might experience degraded
performance because memory page faults occur during the transition.

[discrete]
[id="diagnosis-kubevirtvmiexcessivemigrations"]
== Diagnosis

. Verify that the worker node has sufficient resources:
+
[source,terminal]
----
$ oc get nodes -l node-role.kubernetes.io/worker= -o json | \
  jq .items[].status.allocatable
----
+
.Example output
+
[source,json]
----
{
  "cpu": "3500m",
  "devices.kubevirt.io/kvm": "1k",
  "devices.kubevirt.io/sev": "0",
  "devices.kubevirt.io/tun": "1k",
  "devices.kubevirt.io/vhost-net": "1k",
  "ephemeral-storage": "38161122446",
  "hugepages-1Gi": "0",
  "hugepages-2Mi": "0",
  "memory": "7000128Ki",
  "pods": "250"
}
----

. Check the status of the worker node:
+
[source,terminal]
----
$ oc get nodes -l node-role.kubernetes.io/worker= -o json | \
  jq .items[].status.conditions
----
+
.Example output
+
[source,json]
----
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:12:02Z",
  "message": "kubelet has sufficient memory available",
  "reason": "KubeletHasSufficientMemory",
  "status": "False",
  "type": "MemoryPressure"
},
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:12:02Z",
  "message": "kubelet has no disk pressure",
  "reason": "KubeletHasNoDiskPressure",
  "status": "False",
  "type": "DiskPressure"
},
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:12:02Z",
  "message": "kubelet has sufficient PID available",
  "reason": "KubeletHasSufficientPID",
  "status": "False",
  "type": "PIDPressure"
},
{
  "lastHeartbeatTime": "2022-05-26T07:36:01Z",
  "lastTransitionTime": "2022-05-23T08:24:15Z",
  "message": "kubelet is posting ready status",
  "reason": "KubeletReady",
  "status": "True",
  "type": "Ready"
}
----

. Log in to the worker node and verify that the `kubelet` service is running:
+
[source,terminal]
----
$ systemctl status kubelet
----

. Check the `kubelet` journal log for error messages:
+
[source,terminal]
----
$ journalctl -r -u kubelet
----

[discrete]
[id="mitigation-kubevirtvmiexcessivemigrations"]
== Mitigation

Ensure that the worker nodes have sufficient resources (CPU, memory, disk) to
run VM workloads without interruption.

If the problem persists, try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-LowKVMNodesCount"]
= LowKVMNodesCount

[discrete]
[id="meaning-lowkvmnodescount"]
== Meaning

This alert fires when fewer than two nodes in the cluster have KVM resources.

[discrete]
[id="impact-lowkvmnodescount"]
== Impact

The cluster must have at least two nodes with KVM resources for live migration.

Virtual machines cannot be scheduled or run if no nodes have KVM resources.

[discrete]
[id="diagnosis-lowkvmnodescount"]
== Diagnosis

* Identify the nodes with KVM resources:
+
[source,terminal]
----
$ oc get nodes -o jsonpath='{.items[*].status.allocatable}' | \
  grep devices.kubevirt.io/kvm
----

[discrete]
[id="mitigation-lowkvmnodescount"]
== Mitigation

Install KVM on the nodes without KVM resources.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-LowReadyVirtControllersCount"]
= LowReadyVirtControllersCount

[discrete]
[id="meaning-lowreadyvirtcontrollerscount"]
== Meaning

This alert fires when one or more `virt-controller` pods are running, but
none of these pods has been in the `Ready` state for the past 5 minutes.

A `virt-controller` device monitors the custom resource definitions (CRDs)
of a virtual machine instance (VMI) and manages the associated pods. The
device creates pods for VMIs and manages their lifecycle. The device is
critical for cluster-wide virtualization functionality.

[discrete]
[id="impact-lowreadyvirtcontrollerscount"]
== Impact

This alert indicates that a cluster-level failure might occur. Actions
related to VM lifecycle management, such as launching a new VMI or
shutting down an existing VMI, will fail.

[discrete]
[id="diagnosis-lowreadyvirtcontrollerscount"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Verify a `virt-controller` device is available:
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-controller \
  -o jsonpath='{.status.readyReplicas}'
----

. Check the status of the `virt-controller` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-controller -o yaml
----

. Obtain the details of the `virt-controller` deployment to check for
status conditions, such as crashing pods or failures to pull images:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-controller
----

. Check if any problems occurred with the nodes. For example, they might
be in a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-lowreadyvirtcontrollerscount"]
== Mitigation

This alert can have multiple causes, including the following:

* The cluster has insufficient memory.
* The nodes are down.
* The API server is overloaded. For example, the scheduler might be under
a heavy load and therefore not completely available.
* There are network issues.

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-LowReadyVirtOperatorsCount"]
= LowReadyVirtOperatorsCount

[discrete]
[id="meaning-lowreadyvirtoperatorscount"]
== Meaning

This alert fires when one or more `virt-operator` pods are running, but
none of these pods has been in a `Ready` state for the last 10 minutes.

The `virt-operator` is the first Operator to start in a cluster. The `virt-operator`
deployment has a default replica of two `virt-operator` pods.

Its primary responsibilities include the following:

* Installing, live-updating, and live-upgrading a cluster
* Monitoring the lifecycle of top-level controllers, such as `virt-controller`,
`virt-handler`, `virt-launcher`, and managing their reconciliation
* Certain cluster-wide tasks, such as certificate rotation and infrastructure
management

[discrete]
[id="impact-lowreadyvirtoperatorscount"]
== Impact

A cluster-level failure might occur. Critical cluster-wide management
functionalities, such as certification rotation, upgrade, and reconciliation of
controllers, might become unavailable. Such a state also triggers the
`NoReadyVirtOperator` alert.

The `virt-operator` is not directly responsible for virtual machines (VMs)
in the cluster. Therefore, its temporary unavailability does not significantly
affect VM workloads.

[discrete]
[id="diagnosis-lowreadyvirtoperatorscount"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Obtain the name of the `virt-operator` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-operator -o yaml
----

. Obtain the details of the `virt-operator` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-operator
----

. Check for node issues, such as a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-lowreadyvirtoperatorscount"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-LowVirtAPICount"]
= LowVirtAPICount

[discrete]
[id="meaning-lowvirtapicount"]
== Meaning

This alert fires when only one available `virt-api` pod is detected during a
60-minute period, although at least two nodes are available for scheduling.

[discrete]
[id="impact-lowvirtapicount"]
== Impact

An API call outage might occur during node eviction because the `virt-api` pod
becomes a single point of failure.

[discrete]
[id="diagnosis-lowvirtapicount"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the number of available `virt-api` pods:
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-api \
  -o jsonpath='{.status.readyReplicas}'
----

. Check the status of the `virt-api` deployment for error conditions:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-api -o yaml
----

. Check the nodes for issues such as nodes in a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-lowvirtapicount"]
== Mitigation

Try to identify the root cause and to resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-LowVirtControllersCount"]
= LowVirtControllersCount

[discrete]
[id="meaning-lowvirtcontrollerscount"]
== Meaning

This alert fires when a low number of `virt-controller` pods is detected. At
least one `virt-controller` pod must be available in order to ensure high
availability. The default number of replicas is 2.

A `virt-controller` device monitors the custom resource definitions (CRDs) of a
virtual machine instance (VMI) and manages the associated pods. The device
create pods for VMIs and manages the lifecycle of the pods. The device is
critical for cluster-wide virtualization functionality.

[discrete]
[id="impact-lowvirtcontrollerscount"]
== Impact

The responsiveness of {VirtProductName} might become negatively
affected. For example, certain requests might be missed.

In addition, if another `virt-launcher` instance terminates unexpectedly,
{VirtProductName} might become completely unresponsive.

[discrete]
[id="diagnosis-lowvirtcontrollerscount"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Verify that running `virt-controller` pods are available:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-controller
----

. Check the `virt-launcher` logs for error messages:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-launcher>
----

. Obtain the details of the `virt-launcher` pod to check for status conditions
such as unexpected termination or a `NotReady` state.
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod/<virt-launcher>
----

[discrete]
[id="mitigation-lowvirtcontrollerscount"]
== Mitigation

This alert can have a variety of causes, including:

* Not enough memory on the cluster
* Nodes are down
* The API server is overloaded. For example, the scheduler might be under a
heavy load and therefore not completely available.
* Networking issues

Identify the root cause and fix it, if possible.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-LowVirtOperatorCount"]
= LowVirtOperatorCount

[discrete]
[id="meaning-lowvirtoperatorcount"]
== Meaning

This alert fires when only one `virt-operator` pod in a `Ready` state has
been running for the last 60 minutes.

The `virt-operator` is the first Operator to start in a cluster. Its primary
responsibilities include the following:

* Installing, live-updating, and live-upgrading a cluster
* Monitoring the lifecycle of top-level controllers, such as `virt-controller`,
`virt-handler`, `virt-launcher`, and managing their reconciliation
* Certain cluster-wide tasks, such as certificate rotation and infrastructure
management

[discrete]
[id="impact-lowvirtoperatorcount"]
== Impact

The `virt-operator` cannot provide high availability (HA) for the deployment.
HA requires two or more `virt-operator` pods in a `Ready` state. The default
deployment is two pods.

The `virt-operator` is not directly responsible for virtual machines (VMs)
in the cluster. Therefore, its decreased availability does not significantly
affect VM workloads.

[discrete]
[id="diagnosis-lowvirtoperatorcount"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the states of the `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----

. Review the logs of the affected `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-operator>
----

. Obtain the details of the affected `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <virt-operator>
----

[discrete]
[id="mitigation-lowvirtoperatorcount"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the link:https://access.redhat.com[Customer Portal]
and open a support case, attaching the artifacts gathered during the Diagnosis
procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-NetworkAddonsConfigNotReady"]
= NetworkAddonsConfigNotReady

[discrete]
[id="meaning-networkaddonsconfignotready"]
== Meaning

This alert fires when the `NetworkAddonsConfig` custom resource (CR) of the
Cluster Network Addons Operator (CNAO) is not ready.

CNAO deploys additional networking components on the cluster. This alert indicates
that one of the deployed components is not ready.

[discrete]
[id="impact-networkaddonsconfignotready"]
== Impact

Network functionality is affected.

[discrete]
[id="diagnosis-networkaddonsconfignotready"]
== Diagnosis

. Check the status conditions of the `NetworkAddonsConfig` CR to identify the
deployment or daemon set that is not ready:
+
[source,terminal]
----
$ oc get networkaddonsconfig \
  -o custom-columns="":.status.conditions[*].message
----
+
.Example output
+
[source,text]
----
DaemonSet "cluster-network-addons/macvtap-cni" update is being processed...
----

. Check the component's pod for errors:
+
[source,terminal]
----
$ oc -n cluster-network-addons get daemonset <pod> -o yaml
----

. Check the component's logs:
+
[source,terminal]
----
$ oc -n cluster-network-addons logs <pod>
----

. Check the component's details for error conditions:
+
[source,terminal]
----
$ oc -n cluster-network-addons describe <pod>
----

[discrete]
[id="mitigation-networkaddonsconfignotready"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-NoLeadingVirtOperator"]
= NoLeadingVirtOperator

[discrete]
[id="meaning-noleadingvirtoperator"]
== Meaning

This alert fires when no `virt-operator` pod with a leader lease has been detected
for 10 minutes, although the `virt-operator` pods are in a `Ready` state. The
alert indicates that no leader pod is available.

The `virt-operator` is the first Operator to start in a cluster. Its primary
responsibilities include the following:

* Installing, live updating, and live upgrading a cluster
* Monitoring the lifecycle of top-level controllers, such as `virt-controller`,
`virt-handler`, `virt-launcher`, and managing their reconciliation
* Certain cluster-wide tasks, such as certificate rotation and infrastructure
management

The `virt-operator` deployment has a default replica of 2 pods, with one pod
holding a leader lease.

[discrete]
[id="impact-noleadingvirtoperator"]
== Impact

This alert indicates a failure at the level of the cluster. As a result, critical
cluster-wide management functionalities, such as certification rotation, upgrade,
and reconciliation of controllers, might not be available.

[discrete]
[id="diagnosis-noleadingvirtoperator"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A -o \
  custom-columns="":.metadata.namespace)"
----

. Obtain the status of the `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----

. Check the `virt-operator` pod logs to determine the leader status:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs | grep lead
----
+
Leader pod example:
+
[source,text]
----
{"component":"virt-operator","level":"info","msg":"Attempting to acquire
leader status","pos":"application.go:400","timestamp":"2021-11-30T12:15:18.635387Z"}
I1130 12:15:18.635452       1 leaderelection.go:243] attempting to acquire
leader lease <namespace>/virt-operator...
I1130 12:15:19.216582       1 leaderelection.go:253] successfully acquired
lease <namespace>/virt-operator
{"component":"virt-operator","level":"info","msg":"Started leading",
"pos":"application.go:385","timestamp":"2021-11-30T12:15:19.216836Z"}
----
+
Non-leader pod example:
+
[source,text]
----
{"component":"virt-operator","level":"info","msg":"Attempting to acquire
leader status","pos":"application.go:400","timestamp":"2021-11-30T12:15:20.533696Z"}
I1130 12:15:20.533792       1 leaderelection.go:243] attempting to acquire
leader lease <namespace>/virt-operator...
----

. Obtain the details of the affected `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <virt-operator>
----

[discrete]
[id="mitigation-noleadingvirtoperator"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to find
the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-NoReadyVirtController"]
= NoReadyVirtController

[discrete]
[id="meaning-noreadyvirtcontroller"]
== Meaning

This alert fires when no available `virt-controller` devices have been
detected for 5 minutes.

The `virt-controller` devices monitor the custom resource definitions of
virtual machine instances (VMIs) and manage the associated pods. The devices
create pods for VMIs and manage the lifecycle of the pods.

Therefore, `virt-controller` devices are critical for all cluster-wide
virtualization functionality.

[discrete]
[id="impact-noreadyvirtcontroller"]
== Impact

Any actions related to VM lifecycle management fail. This notably includes
launching a new VMI or shutting down an existing VMI.

[discrete]
[id="diagnosis-noreadyvirtcontroller"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Verify the number of `virt-controller` devices:
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-controller \
  -o jsonpath='{.status.readyReplicas}'
----

. Check the status of the `virt-controller` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-controller -o yaml
----

. Obtain the details of the `virt-controller` deployment to check for
status conditions such as crashing pods or failure to pull images:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-controller
----

. Obtain the details of the `virt-controller` pods:
+
[source,terminal]
----
$ get pods -n $NAMESPACE | grep virt-controller
----

. Check the logs of the `virt-controller` pods for error messages:
+
[source,terminal]
----
$ oc logs -n $NAMESPACE <virt-controller>
----

. Check the nodes for problems, such as a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-noreadyvirtcontroller"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to find
the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-NoReadyVirtOperator"]
= NoReadyVirtOperator

[discrete]
[id="meaning-noreadyvirtoperator"]
== Meaning

This alert fires when no `virt-operator` pod in a `Ready` state has been
detected for 10 minutes.

The `virt-operator` is the first Operator to start in a cluster. Its primary
responsibilities include the following:

* Installing, live-updating, and live-upgrading a cluster
* Monitoring the life cycle of top-level controllers, such as `virt-controller`,
`virt-handler`, `virt-launcher`, and managing their reconciliation
* Certain cluster-wide tasks, such as certificate rotation and infrastructure
management

The default deployment is two `virt-operator` pods.

[discrete]
[id="impact-noreadyvirtoperator"]
== Impact

This alert indicates a cluster-level failure. Critical cluster management
functionalities, such as certification rotation, upgrade, and reconciliation
of controllers, might not be not available.

The `virt-operator` is not directly responsible for virtual machines in
the cluster. Therefore, its temporary unavailability does not significantly
affect workloads.

[discrete]
[id="diagnosis-noreadyvirtoperator"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Obtain the name of the `virt-operator` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-operator -o yaml
----

. Generate the description of the `virt-operator` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-operator
----

. Check for node issues, such as a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-noreadyvirtoperator"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the link:https://access.redhat.com[Customer Portal]
and open a support case, attaching the artifacts gathered during the Diagnosis
procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-OrphanedVirtualMachineInstances"]
= OrphanedVirtualMachineInstances

[discrete]
[id="meaning-orphanedvirtualmachineinstances"]
== Meaning

This alert fires when a virtual machine instance (VMI), or `virt-launcher`
pod, runs on a node that does not have a running `virt-handler` pod.
Such a VMI is called _orphaned_.

[discrete]
[id="impact-orphanedvirtualmachineinstances"]
== Impact

Orphaned VMIs cannot be managed.

[discrete]
[id="diagnosis-orphanedvirtualmachineinstances"]
== Diagnosis

. Check the status of the `virt-handler` pods to view the nodes on
which they are running:
+
[source,terminal]
----
$ oc get pods --all-namespaces -o wide -l kubevirt.io=virt-handler
----

. Check the status of the VMIs to identify VMIs running on nodes
that do not have a running `virt-handler` pod:
+
[source,terminal]
----
$ oc get vmis --all-namespaces
----

. Check the status of the `virt-handler` daemon:
+
[source,terminal]
----
$ oc get daemonset virt-handler --all-namespaces
----
+
.Example output
+
[source,text]
----
NAME          DESIRED  CURRENT  READY  UP-TO-DATE  AVAILABLE ...
virt-handler  2        2        2      2           2         ...
----
+
The daemon set is considered healthy if the `Desired`, `Ready`,
and `Available` columns contain the same value.

. If the `virt-handler` daemon set is not healthy, check the `virt-handler`
daemon set for pod deployment issues:
+
[source,terminal]
----
$ oc get daemonset virt-handler --all-namespaces -o yaml | jq .status
----

. Check the nodes for issues such as a `NotReady` status:
+
[source,terminal]
----
$ oc get nodes
----

. Check the `spec.workloads` stanza of the `KubeVirt` custom resource
(CR) for a workloads placement policy:
+
[source,terminal]
----
$ oc get kubevirt kubevirt --all-namespaces -o yaml
----

[discrete]
[id="mitigation-orphanedvirtualmachineinstances"]
== Mitigation

If a workloads placement policy is configured, add the node with the
VMI to the policy.

Possible causes for the removal of a `virt-handler` pod from a node
include changes to the node's taints and tolerations or to a pod's
scheduling rules.

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-OutdatedVirtualMachineInstanceWorkloads"]
= OutdatedVirtualMachineInstanceWorkloads

[discrete]
[id="meaning-outdatedvirtualmachineinstanceworkloads"]
== Meaning

This alert fires when running virtual machine instances (VMIs) in
outdated `virt-launcher` pods are detected 24 hours after the OpenShift
Virtualization control plane has been updated.

[discrete]
[id="impact-outdatedvirtualmachineinstanceworkloads"]
== Impact

Outdated VMIs might not have access to new {VirtProductName}
features.

Outdated VMIs will not receive the security fixes associated with
the `virt-launcher` pod update.

[discrete]
[id="diagnosis-outdatedvirtualmachineinstanceworkloads"]
== Diagnosis

. Identify the outdated VMIs:
+
[source,terminal]
----
$ oc get vmi -l kubevirt.io/outdatedLauncherImage --all-namespaces
----

. Check the `KubeVirt` custom resource (CR) to determine whether
`workloadUpdateMethods` is configured in the `workloadUpdateStrategy`
stanza:
+
[source,terminal]
----
$ oc get kubevirt kubevirt --all-namespaces -o yaml
----

. Check each outdated VMI to determine whether it is live-migratable:
+
[source,terminal]
----
$ oc get vmi <vmi> -o yaml
----
+
.Example output
+
[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstance
# ...
  status:
    conditions:
    - lastProbeTime: null
      lastTransitionTime: null
      message: cannot migrate VMI which does not use masquerade
      to connect to the pod network
      reason: InterfaceNotLiveMigratable
      status: "False"
      type: LiveMigratable
----

[discrete]
[id="mitigation-outdatedvirtualmachineinstanceworkloads"]
== Mitigation

[discrete]
[id="configuring-automated-workload-updates-outdatedvirtualmachineinstanceworkloads"]
=== Configuring automated workload updates

Update the `HyperConverged` CR to enable automatic workload updates.

[discrete]
[id="stopping-a-vm-associated-with-a-non-live-migratable-vmi-outdatedvirtualmachineinstanceworkloads"]
=== Stopping a VM associated with a non-live-migratable VMI

* If a VMI is not live-migratable and if `runStrategy: always` is
set in the corresponding `VirtualMachine` object, you can update the
VMI by manually stopping the virtual machine (VM):
+
[source,terminal]
----
$ virctl stop --namespace <namespace> <vm>
----

A new VMI spins up immediately in an updated `virt-launcher` pod to
replace the stopped VMI. This is the equivalent of a restart action.

NOTE: Manually stopping a _live-migratable_ VM is destructive and
not recommended because it interrupts the workload.

[discrete]
[id="migrating-a-live-migratable-vmi-outdatedvirtualmachineinstanceworkloads"]
=== Migrating a live-migratable VMI

If a VMI is live-migratable, you can update it by creating a `VirtualMachineInstanceMigration`
object that targets a specific running VMI. The VMI is migrated into
an updated `virt-launcher` pod.

. Create a `VirtualMachineInstanceMigration` manifest and save it
as `migration.yaml`:
+
[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachineInstanceMigration
metadata:
  name: <migration_name>
  namespace: <namespace>
spec:
  vmiName: <vmi_name>
----

. Create a `VirtualMachineInstanceMigration` object to trigger the
migration:
+
[source,terminal]
----
$ oc create -f migration.yaml
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-SingleStackIPv6Unsupported"]
= SingleStackIPv6Unsupported

[discrete]
[id="meaning-singlestackipv6unsupported"]
== Meaning

This alert fires when you install {VirtProductName} on a single stack
IPv6 cluster.

[discrete]
[id="impact-singlestackipv6unsupported"]
== Impact

You cannot create virtual machines.

[discrete]
[id="diagnosis-singlestackipv6unsupported"]
== Diagnosis

* Check the cluster network configuration by running the following command:
+
[,shell]
----
$ oc get network.config cluster -o yaml
----
+
The output displays only an IPv6 CIDR for the cluster network.
+
.Example output
+
[source,text]
----
apiVersion: config.openshift.io/v1
kind: Network
metadata:
  name: cluster
spec:
  clusterNetwork:
  - cidr: fd02::/48
    hostPrefix: 64
----

[discrete]
[id="mitigation-singlestackipv6unsupported"]
== Mitigation

Install {VirtProductName} on a single stack IPv4 cluster or on a
dual stack IPv4/IPv6 cluster.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-SSPCommonTemplatesModificationReverted"]
= SSPCommonTemplatesModificationReverted

[discrete]
[id="meaning-sspcommontemplatesmodificationreverted"]
== Meaning

This alert fires when the Scheduling, Scale, and Performance (SSP) Operator
reverts changes to common templates as part of its reconciliation procedure.

The SSP Operator deploys and reconciles the common templates and the Template
Validator. If a user or script changes a common template, the changes are reverted
by the SSP Operator.

[discrete]
[id="impact-sspcommontemplatesmodificationreverted"]
== Impact

Changes to common templates are overwritten.

[discrete]
[id="diagnosis-sspcommontemplatesmodificationreverted"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"
----

. Check the `ssp-operator` logs for templates with reverted changes:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs --tail=-1 -l control-plane=ssp-operator | \
  grep 'common template' -C 3
----

[discrete]
[id="mitigation-sspcommontemplatesmodificationreverted"]
== Mitigation

Try to identify and resolve the cause of the changes.

Ensure that changes are made only to copies of templates, and not to the templates
themselves.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-SSPDown"]
= SSPDown

[discrete]
[id="meaning-sspdown"]
== Meaning

This alert fires when all the Scheduling, Scale and Performance (SSP) Operator
pods are down.

The SSP Operator is responsible for deploying and reconciling the common
templates and the Template Validator.

[discrete]
[id="impact-sspdown"]
== Impact

Dependent components might not be deployed. Changes in the components might
not be reconciled. As a result, the common templates and/or the Template
Validator might not be updated or reset if they fail.

[discrete]
[id="diagnosis-sspdown"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"
----

. Check the status of the `ssp-operator` pods.
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l control-plane=ssp-operator
----

. Obtain the details of the `ssp-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods -l control-plane=ssp-operator
----

. Check the `ssp-operator` logs for error messages:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs --tail=-1 -l control-plane=ssp-operator
----

[discrete]
[id="mitigation-sspdown"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-SSPFailingToReconcile"]
= SSPFailingToReconcile

[discrete]
[id="meaning-sspfailingtoreconcile"]
== Meaning

This alert fires when the reconcile cycle of the Scheduling, Scale and
Performance (SSP) Operator fails repeatedly, although the SSP Operator
is running.

The SSP Operator is responsible for deploying and reconciling the common
templates and the Template Validator.

[discrete]
[id="impact-sspfailingtoreconcile"]
== Impact

Dependent components might not be deployed. Changes in the components might
not be reconciled. As a result, the common templates or the Template
Validator might not be updated or reset if they fail.

[discrete]
[id="diagnosis-sspfailingtoreconcile"]
== Diagnosis

. Export the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"
----

. Obtain the details of the `ssp-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods -l control-plane=ssp-operator
----

. Check the `ssp-operator` logs for errors:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs --tail=-1 -l control-plane=ssp-operator
----

. Obtain the status of the `virt-template-validator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l name=virt-template-validator
----

. Obtain the details of the `virt-template-validator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods -l name=virt-template-validator
----

. Check the `virt-template-validator` logs for errors:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs --tail=-1 -l name=virt-template-validator
----

[discrete]
[id="mitigation-sspfailingtoreconcile"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-SSPHighRateRejectedVms"]
= SSPHighRateRejectedVms

[discrete]
[id="meaning-ssphighraterejectedvms"]
== Meaning

This alert fires when a user or script attempts to create or modify a large
number of virtual machines (VMs), using an invalid configuration.

[discrete]
[id="impact-ssphighraterejectedvms"]
== Impact

The VMs are not created or modified. As a result, the environment might not
behave as expected.

[discrete]
[id="diagnosis-ssphighraterejectedvms"]
== Diagnosis

. Export the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"
----

. Check the `virt-template-validator` logs for errors that might indicate the
cause:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs --tail=-1 -l name=virt-template-validator
----
+
.Example output
+
[source,text]
----
{"component":"kubevirt-template-validator","level":"info","msg":"evalution
summary for ubuntu-3166wmdbbfkroku0:\nminimal-required-memory applied: FAIL,
value 1073741824 is lower than minimum [2147483648]\n\nsucceeded=false",
"pos":"admission.go:25","timestamp":"2021-09-28T17:59:10.934470Z"}
----

[discrete]
[id="mitigation-ssphighraterejectedvms"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-SSPTemplateValidatorDown"]
= SSPTemplateValidatorDown

[discrete]
[id="meaning-ssptemplatevalidatordown"]
== Meaning

This alert fires when all the Template Validator pods are down.

The Template Validator checks virtual machines (VMs) to ensure that they
do not violate their templates.

[discrete]
[id="impact-ssptemplatevalidatordown"]
== Impact

VMs are not validated against their templates. As a result, VMs might be
created with specifications that do not match their respective workloads.

[discrete]
[id="diagnosis-ssptemplatevalidatordown"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get deployment -A | grep ssp-operator | \
  awk '{print $1}')"
----

. Obtain the status of the `virt-template-validator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l name=virt-template-validator
----

. Obtain the details of the `virt-template-validator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pods -l name=virt-template-validator
----

. Check the  `virt-template-validator` logs for error messages:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs --tail=-1 -l name=virt-template-validator
----

[discrete]
[id="mitigation-ssptemplatevalidatordown"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-UnsupportedHCOModification"]
= UnsupportedHCOModification

[discrete]
[id="meaning-unsupportedhcomodification"]
== Meaning

This alert fires when a JSON Patch annotation is used to change an operand
of the HyperConverged Cluster Operator (HCO).

HCO configures {VirtProductName} and its supporting operators in
an opinionated way and overwrites its operands when there is an unexpected
change to them. Users must not modify the operands directly.

However, if a change is required and it is not supported by the HCO API,
you can force HCO to set a change in an operator by using JSON Patch annotations.
These changes are not reverted by HCO during its reconciliation process.

[discrete]
[id="impact-unsupportedhcomodification"]
== Impact

Incorrect use of JSON Patch annotations might lead to unexpected results
or an unstable environment.

Upgrading a system with JSON Patch annotations is dangerous because the
structure of the component custom resources might change.

[discrete]
[id="diagnosis-unsupportedhcomodification"]
== Diagnosis

* Check the `annotation_name` in the alert details to identify the JSON
Patch annotation:
+
[source,text]
----
Labels
  alertname=KubevirtHyperconvergedClusterOperatorUSModification
  annotation_name=kubevirt.kubevirt.io/jsonpatch
  severity=info
----

[discrete]
[id="mitigation-unsupportedhcomodification"]
== Mitigation

It is best to use the HCO API to change an operand. However, if the change
can only be done with a JSON Patch annotation, proceed with caution.

Remove JSON Patch annotations before upgrade to avoid potential issues.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtAPIDown"]
= VirtAPIDown

[discrete]
[id="meaning-virtapidown"]
== Meaning

This alert fires when all the API Server pods are down.

[discrete]
[id="impact-virtapidown"]
== Impact

{VirtProductName} objects cannot send API calls.

[discrete]
[id="diagnosis-virtapidown"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-api` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api
----

. Check the status of the `virt-api` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-api -o yaml
----

. Check the `virt-api` deployment details for issues such as crashing pods or
image pull failures:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-api
----

. Check for issues such as nodes in a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-virtapidown"]
== Mitigation

Try to identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtApiRESTErrorsBurst"]
= VirtApiRESTErrorsBurst

[discrete]
[id="meaning-virtapiresterrorsburst"]
== Meaning

More than 80% of REST calls have failed in the `virt-api` pods in the last
5 minutes.

[discrete]
[id="impact-virtapiresterrorsburst"]
== Impact

A very high rate of failed REST calls to `virt-api` might lead to slow
response and execution of API calls, and potentially to API calls being
completely dismissed.

However, currently running virtual machine workloads are not likely to
be affected.

[discrete]
[id="diagnosis-virtapiresterrorsburst"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Obtain the list of `virt-api` pods on your deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api
----

. Check the `virt-api` logs for error messages:
+
[source,terminal]
----
$ oc logs -n $NAMESPACE <virt-api>
----

. Obtain the details of the `virt-api` pods:
+
[source,terminal]
----
$ oc describe -n $NAMESPACE <virt-api>
----

. Check if any problems occurred with the nodes. For example, they might
be in a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

. Check the status of the `virt-api` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-api -o yaml
----

. Obtain the details of the `virt-api` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-api
----

[discrete]
[id="mitigation-virtapiresterrorsburst"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtApiRESTErrorsHigh"]
= VirtApiRESTErrorsHigh

[discrete]
[id="meaning-virtapiresterrorshigh"]
== Meaning

More than 5% of REST calls have failed in the `virt-api` pods in the last 60 minutes.

[discrete]
[id="impact-virtapiresterrorshigh"]
== Impact

A high rate of failed REST calls to `virt-api` might lead to slow response and
execution of API calls.

However, currently running virtual machine workloads are not likely to be affected.

[discrete]
[id="diagnosis-virtapiresterrorshigh"]
== Diagnosis

. Set the `NAMESPACE` environment variable as follows:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-api` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-api
----

. Check the `virt-api` logs:
+
[source,terminal]
----
$ oc logs -n  $NAMESPACE <virt-api>
----

. Obtain the details of the `virt-api` pods:
+
[source,terminal]
----
$ oc describe -n $NAMESPACE <virt-api>
----

. Check if any problems occurred with the nodes. For example, they might be in
a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

. Check the status of the `virt-api` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-api -o yaml
----

. Obtain the details of the `virt-api` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-api
----

[discrete]
[id="mitigation-virtapiresterrorshigh"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to
identify the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtControllerDown"]
= VirtControllerDown

[discrete]
[id="meaning-virtcontrollerdown"]
== Meaning

No running `virt-controller` pod has been detected for 5 minutes.

[discrete]
[id="impact-virtcontrollerdown"]
== Impact

Any actions related to virtual machine (VM) lifecycle management fail.
This notably includes launching a new virtual machine instance (VMI)
or shutting down an existing VMI.

[discrete]
[id="diagnosis-virtcontrollerdown"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-controller` deployment:
+
[source,terminal]
----
$ oc get deployment -n $NAMESPACE virt-controller -o yaml
----

. Review the logs of the `virt-controller` pod:
+
[source,terminal]
----
$ oc get logs <virt-controller>
----

[discrete]
[id="mitigation-virtcontrollerdown"]
== Mitigation

This alert can have a variety of causes, including the following:

* Node resource exhaustion
* Not enough memory on the cluster
* Nodes are down
* The API server is overloaded. For example, the scheduler might be
under a heavy load and therefore not completely available.
* Networking issues

Identify the root cause and fix it, if possible.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtControllerRESTErrorsBurst"]
= VirtControllerRESTErrorsBurst

[discrete]
[id="meaning-virtcontrollerresterrorsburst"]
== Meaning

More than 80% of REST calls in `virt-controller` pods failed in the last 5
minutes.

The `virt-controller` has likely fully lost the connection to the API server.

This error is frequently caused by one of the following problems:

* The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.
* The `virt-controller` pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.

[discrete]
[id="impact-virtcontrollerresterrorsburst"]
== Impact

Status updates are not propagated and actions like migrations cannot take place.
However, running workloads are not impacted.

[discrete]
[id="diagnosis-virtcontrollerresterrorsburst"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. List the available `virt-controller` pods:
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-controller
----

. Check the `virt-controller` logs for error messages when connecting to the
API server:
+
[source,terminal]
----
$ oc logs -n  $NAMESPACE <virt-controller>
----

[discrete]
[id="mitigation-virtcontrollerresterrorsburst"]
== Mitigation

* If the `virt-controller` pod cannot connect to the API server, delete the
pod to force a restart:
+
[source,terminal]
----
$ oc delete -n $NAMESPACE <virt-controller>
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtControllerRESTErrorsHigh"]
= VirtControllerRESTErrorsHigh

[discrete]
[id="meaning-virtcontrollerresterrorshigh"]
== Meaning

More than 5% of REST calls failed in `virt-controller` in the last 60 minutes.

This is most likely because `virt-controller` has partially lost connection
to the API server.

This error is frequently caused by one of the following problems:

* The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.
* The `virt-controller` pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.

[discrete]
[id="impact-virtcontrollerresterrorshigh"]
== Impact

Node-related actions, such as starting and migrating, and scheduling virtual
machines, are delayed. Running workloads are not affected, but reporting
their current status might be delayed.

[discrete]
[id="diagnosis-virtcontrollerresterrorshigh"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. List the available `virt-controller` pods:
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-controller
----

. Check the `virt-controller` logs for error messages when connecting
to the API server:
+
[source,terminal]
----
$ oc logs -n  $NAMESPACE <virt-controller>
----

[discrete]
[id="mitigation-virtcontrollerresterrorshigh"]
== Mitigation

* If the `virt-controller` pod cannot connect to the API server, delete
the pod to force a restart:
+
[source,terminal]
----
$ oc delete -n $NAMESPACE <virt-controller>
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtHandlerDaemonSetRolloutFailing"]
= VirtHandlerDaemonSetRolloutFailing

[discrete]
[id="meaning-virthandlerdaemonsetrolloutfailing"]
== Meaning

The `virt-handler` daemon set has failed to deploy on one or more worker
nodes after 15 minutes.

[discrete]
[id="impact-virthandlerdaemonsetrolloutfailing"]
== Impact

This alert is a warning. It does not indicate that all `virt-handler` daemon
sets have failed to deploy. Therefore, the normal lifecycle of virtual
machines is not affected unless the cluster is overloaded.

[discrete]
[id="diagnosis-virthandlerdaemonsetrolloutfailing"]
== Diagnosis

Identify worker nodes that do not have a running `virt-handler` pod:

. Export the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-handler` pods to identify pods that have
not deployed:
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler
----

. Obtain the name of the worker node of the `virt-handler` pod:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pod <virt-handler> -o jsonpath='{.spec.nodeName}'
----

[discrete]
[id="mitigation-virthandlerdaemonsetrolloutfailing"]
== Mitigation

If the `virt-handler` pods failed to deploy because of insufficient resources,
you can delete other pods on the affected worker node.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtHandlerRESTErrorsBurst"]
= VirtHandlerRESTErrorsBurst

[discrete]
[id="meaning-virthandlerresterrorsburst"]
== Meaning

More than 80% of REST calls failed in `virt-handler` in the last 5 minutes.
This alert usually indicates that the `virt-handler` pods cannot connect
to the API server.

This error is frequently caused by one of the following problems:

* The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.
* The `virt-handler` pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.

[discrete]
[id="impact-virthandlerresterrorsburst"]
== Impact

Status updates are not propagated and node-related actions, such as migrations,
fail. However, running workloads on the affected node are not impacted.

[discrete]
[id="diagnosis-virthandlerresterrorsburst"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-handler` pod:
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler
----

. Check the `virt-handler` logs for error messages when connecting to
the API server:
+
[source,terminal]
----
$ oc logs -n  $NAMESPACE <virt-handler>
----

[discrete]
[id="mitigation-virthandlerresterrorsburst"]
== Mitigation

* If the `virt-handler` cannot connect to the API server, delete the pod
to force a restart:
+
[source,terminal]
----
$ oc delete -n $NAMESPACE <virt-handler>
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtHandlerRESTErrorsHigh"]
= VirtHandlerRESTErrorsHigh

[discrete]
[id="meaning-virthandlerresterrorshigh"]
== Meaning

More than 5% of REST calls failed in `virt-handler` in the last 60 minutes.
This alert usually indicates that the `virt-handler` pods have partially
lost connection to the API server.

This error is frequently caused by one of the following problems:

* The API server is overloaded, which causes timeouts. To verify if this
is the case, check the metrics of the API server, and view its response
times and overall calls.
* The `virt-handler` pod cannot reach the API server. This is commonly
caused by DNS issues on the node and networking connectivity issues.

[discrete]
[id="impact-virthandlerresterrorshigh"]
== Impact

Node-related actions, such as starting and migrating workloads, are delayed
on the node that `virt-handler` is running on. Running workloads are not
affected, but reporting their current status might be delayed.

[discrete]
[id="diagnosis-virthandlerresterrorshigh"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-handler` pod:
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-handler
----

. Check the `virt-handler` logs for error messages when connecting to
the API server:
+
[source,terminal]
----
$ oc logs -n $NAMESPACE <virt-handler>
----

[discrete]
[id="mitigation-virthandlerresterrorshigh"]
== Mitigation

* If the `virt-handler` cannot connect to the API server, delete the pod
to force a restart:
+
[source,terminal]
----
$ oc delete -n $NAMESPACE <virt-handler>
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtOperatorDown"]
= VirtOperatorDown

[discrete]
[id="meaning-virtoperatordown"]
== Meaning

This alert fires when no `virt-operator` pod in the `Running` state has
been detected for 10 minutes.

The `virt-operator` is the first Operator to start in a cluster. Its primary
responsibilities include the following:

* Installing, live-updating, and live-upgrading a cluster
* Monitoring the life cycle of top-level controllers, such as `virt-controller`,
`virt-handler`, `virt-launcher`, and managing their reconciliation
* Certain cluster-wide tasks, such as certificate rotation and infrastructure
management

The `virt-operator` deployment has a default replica of 2 pods.

[discrete]
[id="impact-virtoperatordown"]
== Impact

This alert indicates a failure at the level of the cluster. Critical cluster-wide
management functionalities, such as certification rotation, upgrade, and
reconciliation of controllers, might not be available.

The `virt-operator` is not directly responsible for virtual machines (VMs)
in the cluster. Therefore, its temporary unavailability does not significantly
affect VM workloads.

[discrete]
[id="diagnosis-virtoperatordown"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-operator` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE get deploy virt-operator -o yaml
----

. Obtain the details of the `virt-operator` deployment:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe deploy virt-operator
----

. Check the status of the `virt-operator` pods:
+
[source,terminal]
----
$ oc get pods -n $NAMESPACE -l=kubevirt.io=virt-operator
----

. Check for node issues, such as a `NotReady` state:
+
[source,terminal]
----
$ oc get nodes
----

[discrete]
[id="mitigation-virtoperatordown"]
== Mitigation

Based on the information obtained during the diagnosis procedure, try to find
the root cause and resolve the issue.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtOperatorRESTErrorsBurst"]
= VirtOperatorRESTErrorsBurst

[discrete]
[id="meaning-virtoperatorresterrorsburst"]
== Meaning

This alert fires when more than 80% of the REST calls in the `virt-operator`
pods failed in the last 5 minutes. This usually indicates that the `virt-operator`
pods cannot connect to the API server.

This error is frequently caused by one of the following problems:

* The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.
* The `virt-operator` pod cannot reach the API server. This is commonly caused
by DNS issues on the node and networking connectivity issues.

[discrete]
[id="impact-virtoperatorresterrorsburst"]
== Impact

Cluster-level actions, such as upgrading and controller reconciliation, might
not be available.

However, workloads such as virtual machines (VMs) and VM instances
(VMIs) are not likely to be affected.

[discrete]
[id="diagnosis-virtoperatorresterrorsburst"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----

. Check the `virt-operator` logs for error messages when connecting to the
API server:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-operator>
----

. Obtain the details of the `virt-operator` pod:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <virt-operator>
----

[discrete]
[id="mitigation-virtoperatorresterrorsburst"]
== Mitigation

* If the `virt-operator` pod cannot connect to the API server, delete the pod
to force a restart:
+
[source,terminal]
----
$ oc delete -n $NAMESPACE <virt-operator>
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtOperatorRESTErrorsHigh"]
= VirtOperatorRESTErrorsHigh

[discrete]
[id="meaning-virtoperatorresterrorshigh"]
== Meaning

This alert fires when more than 5% of the REST calls in `virt-operator` pods
failed in the last 60 minutes. This usually indicates the `virt-operator` pods
cannot connect to the API server.

This error is frequently caused by one of the following problems:

* The API server is overloaded, which causes timeouts. To verify if this is
the case, check the metrics of the API server, and view its response times and
overall calls.
* The `virt-operator` pod cannot reach the API server. This is commonly caused
by DNS issues on the node and networking connectivity issues.

[discrete]
[id="impact-virtoperatorresterrorshigh"]
== Impact

Cluster-level actions, such as upgrading and controller reconciliation, might
be delayed.

However, workloads such as virtual machines (VMs) and VM instances
(VMIs) are not likely to be affected.

[discrete]
[id="diagnosis-virtoperatorresterrorshigh"]
== Diagnosis

. Set the `NAMESPACE` environment variable:
+
[source,terminal]
----
$ export NAMESPACE="$(oc get kubevirt -A \
  -o custom-columns="":.metadata.namespace)"
----

. Check the status of the `virt-operator` pods:
+
[source,terminal]
----
$ oc -n $NAMESPACE get pods -l kubevirt.io=virt-operator
----

. Check the `virt-operator` logs for error messages when connecting to the
API server:
+
[source,terminal]
----
$ oc -n $NAMESPACE logs <virt-operator>
----

. Obtain the details of the `virt-operator` pod:
+
[source,terminal]
----
$ oc -n $NAMESPACE describe pod <virt-operator>
----

[discrete]
[id="mitigation-virtoperatorresterrorshigh"]
== Mitigation

* If the `virt-operator` pod cannot connect to the API server, delete the pod
to force a restart:
+
[source,terminal]
----
$ oc delete -n $NAMESPACE <virt-operator>
----

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VirtualMachineCRCErrors"]
= VirtualMachineCRCErrors

[discrete]
[id="meaning-virtualmachinecrcerrors"]
== Meaning

This alert fires when the storage class is incorrectly configured.
A system-wide, shared dummy page causes CRC errors when data is
written and read across different processes or threads.

[discrete]
[id="impact-virtualmachinecrcerrors"]
== Impact

A large number of CRC errors might cause the cluster to display
severe performance degradation.

[discrete]
[id="diagnosis-virtualmachinecrcerrors"]
== Diagnosis

. Navigate to *Observe* -> *Metrics* in the web console.
. Obtain a list of virtual machines with incorrectly configured storage classes
by running the following PromQL query:
+
[source,text]
----
kubevirt_ssp_vm_rbd_volume{rxbounce_enabled="false", volume_mode="Block"} == 1
----
+
The output displays a list of virtual machines that use a storage
class without `rxbounce_enabled`.
+
.Example output
+
[source,text]
----
kubevirt_ssp_vm_rbd_volume{name="testvmi-gwgdqp22k7", namespace="test_ns", pv_name="testvmi-gwgdqp22k7", rxbounce_enabled="false", volume_mode="Block"} 1
----

. Obtain the storage class name by running the following command:
+
[source,terminal]
----
$ oc get pv <pv_name> -o=jsonpath='{.spec.storageClassName}'
----

[discrete]
[id="mitigation-virtualmachinecrcerrors"]
== Mitigation

Add the `krbd:rxbounce` map option to the storage class configuration to use
a bounce buffer when receiving data:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: vm-sc
parameters:
  # ...
  mounter: rbd
  mapOptions: "krbd:rxbounce"
provisioner: openshift-storage.rbd.csi.ceph.com
# ...
----

The `krbd:rxbounce` option creates a bounce buffer to receive data. The default
behavior is for the destination buffer to receive data directly. A bounce buffer
is required if the stability of the destination buffer cannot be guaranteed.

If you cannot resolve the issue, log in to the
link:https://access.redhat.com[Customer Portal] and open a support case,
attaching the artifacts gathered during the diagnosis procedure.

:leveloffset!:

:leveloffset: +1

// Do not edit this module. It is generated with a script.
// Do not reuse this module. The anchor IDs do not contain a context statement.
// Module included in the following assemblies:
//
// * virt/monitoring/virt-runbooks.adoc

:_mod-docs-content-type: REFERENCE
[id="virt-runbook-VMCannotBeEvicted"]
= VMCannotBeEvicted

[discrete]
[id="meaning-vmcannotbeevicted"]
== Meaning

This alert fires when the eviction strategy of a virtual machine (VM) is set
to `LiveMigration` but the VM is not migratable.

[discrete]
[id="impact-vmcannotbeevicted"]
== Impact

Non-migratable VMs prevent node eviction. This condition affects operations
such as node drain and updates.

[discrete]
[id="diagnosis-vmcannotbeevicted"]
== Diagnosis

. Check the VMI configuration to determine whether the value of
`evictionStrategy` is `LiveMigrate`:
+
[source,terminal]
----
$ oc get vmis -o yaml
----

. Check for a `False` status in the `LIVE-MIGRATABLE` column to identify VMIs
that are not migratable:
+
[source,terminal]
----
$ oc get vmis -o wide
----

. Obtain the details of the VMI and check `spec.conditions` to identify the
issue:
+
[source,terminal]
----
$ oc get vmi <vmi> -o yaml
----
+
.Example output
+
[source,yaml]
----
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: null
    message: cannot migrate VMI which does not use masquerade to connect
    to the pod network
    reason: InterfaceNotLiveMigratable
    status: "False"
    type: LiveMigratable
----

[discrete]
[id="mitigation-vmcannotbeevicted"]
== Mitigation

Set the `evictionStrategy` of the VMI to `shutdown` or resolve the issue that
prevents the VMI from migrating.

:leveloffset!:

//# includes=_attributes/common-attributes,modules/virt-runbook-cdidataimportcronoutdated,modules/virt-runbook-cdidatavolumeunusualrestartcount,modules/virt-runbook-cdinotready,modules/virt-runbook-cdioperatordown,modules/virt-runbook-cdistorageprofilesincomplete,modules/virt-runbook-cnaodown,modules/virt-runbook-hcoinstallationincomplete,modules/virt-runbook-hppnotready,modules/virt-runbook-hppoperatordown,modules/virt-runbook-hppsharingpoolpathwithos,modules/virt-runbook-kubemacpooldown,modules/virt-runbook-kubemacpoolduplicatemacsfound,modules/virt-runbook-kubevirtcomponentexceedsrequestedcpu,modules/virt-runbook-kubevirtcomponentexceedsrequestedmemory,modules/virt-runbook-kubevirtcrmodified,modules/virt-runbook-kubevirtdeprecatedapirequested,modules/virt-runbook-kubevirtnoavailablenodestorunvms,modules/virt-runbook-kubevirtvmhighmemoryusage,modules/virt-runbook-kubevirtvmiexcessivemigrations,modules/virt-runbook-lowkvmnodescount,modules/virt-runbook-lowreadyvirtcontrollerscount,modules/virt-runbook-lowreadyvirtoperatorscount,modules/virt-runbook-lowvirtapicount,modules/virt-runbook-lowvirtcontrollerscount,modules/virt-runbook-lowvirtoperatorcount,modules/virt-runbook-networkaddonsconfignotready,modules/virt-runbook-noleadingvirtoperator,modules/virt-runbook-noreadyvirtcontroller,modules/virt-runbook-noreadyvirtoperator,modules/virt-runbook-orphanedvirtualmachineinstances,modules/virt-runbook-outdatedvirtualmachineinstanceworkloads,modules/virt-runbook-singlestackipv6unsupported,modules/virt-runbook-sspcommontemplatesmodificationreverted,modules/virt-runbook-sspdown,modules/virt-runbook-sspfailingtoreconcile,modules/virt-runbook-ssphighraterejectedvms,modules/virt-runbook-ssptemplatevalidatordown,modules/virt-runbook-unsupportedhcomodification,modules/virt-runbook-virtapidown,modules/virt-runbook-virtapiresterrorsburst,modules/virt-runbook-virtapiresterrorshigh,modules/virt-runbook-virtcontrollerdown,modules/virt-runbook-virtcontrollerresterrorsburst,modules/virt-runbook-virtcontrollerresterrorshigh,modules/virt-runbook-virthandlerdaemonsetrolloutfailing,modules/virt-runbook-virthandlerresterrorsburst,modules/virt-runbook-virthandlerresterrorshigh,modules/virt-runbook-virtoperatordown,modules/virt-runbook-virtoperatorresterrorsburst,modules/virt-runbook-virtoperatorresterrorshigh,modules/virt-runbook-virtualmachinecrcerrors,modules/virt-runbook-vmcannotbeevicted

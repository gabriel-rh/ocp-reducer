:_mod-docs-content-type: ASSEMBLY
[id="virt-connecting-vm-to-default-pod-network"]
= Connecting a virtual machine to the default pod network
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: virt-connecting-vm-to-default-pod-network

toc::[]

You can connect a virtual machine to the default internal pod network by configuring its network interface to use the `masquerade` binding mode.

[NOTE]
====
Traffic passing through network interfaces to the default pod network is interrupted during live migration.
====

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/vm_networking/virt-connecting-vm-to-default-pod-network.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-configuring-masquerade-mode-cli_{context}"]
= Configuring masquerade mode from the command line

You can use masquerade mode to hide a virtual machine's outgoing traffic behind
the pod IP address. Masquerade mode uses Network Address Translation (NAT) to
connect virtual machines to the pod network backend through a Linux bridge.

Enable masquerade mode and allow traffic to enter the virtual machine by
editing your virtual machine configuration file.

.Prerequisites

* The virtual machine must be configured to use DHCP to acquire IPv4 addresses.

.Procedure

. Edit the `interfaces` spec of your virtual machine configuration file:
+

[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm
spec:
  template:
    spec:
      domain:
        devices:
          interfaces:
            - name: default
              masquerade: {} <1>
              ports: <2>
                - port: 80
# ...
      networks:
      - name: default
        pod: {}
----
<1> Connect using masquerade mode.
<2> Optional: List the ports that you want to expose from the virtual machine, each specified by the `port` field. The `port` value must be a number between 0 and 65536. When the `ports` array is not used, all ports in the valid range are open to incoming traffic. In this example, incoming traffic is allowed on port `80`.
+
[NOTE]
====
Ports 49152 and 49153 are reserved for use by the libvirt platform and all other incoming traffic to these ports is dropped.
====

. Create the virtual machine:
+

[source,terminal]
----
$ oc create -f <vm-name>.yaml
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/vm_networking/virt-connecting-vm-to-default-pod-network.adoc

:_mod-docs-content-type: PROCEDURE
[id="virt-configuring-masquerade-mode-dual-stack_{context}"]
= Configuring masquerade mode with dual-stack (IPv4 and IPv6)

You can configure a new virtual machine (VM) to use both IPv6 and IPv4 on the default pod network by using cloud-init.

The `Network.pod.vmIPv6NetworkCIDR` field in the virtual machine instance configuration determines the static IPv6 address of the VM and the gateway IP address. These are used by the virt-launcher pod to route IPv6 traffic to the virtual machine and are not used externally. The `Network.pod.vmIPv6NetworkCIDR` field specifies an IPv6 address block in Classless Inter-Domain Routing (CIDR) notation. The default value is `fd10:0:2::2/120`. You can edit this value based on your network requirements.

When the virtual machine is running, incoming and outgoing traffic for the virtual machine is routed to both the IPv4 address and the unique IPv6 address of the virt-launcher pod. The virt-launcher pod then routes the IPv4 traffic to the DHCP address of the virtual machine, and the IPv6 traffic to the statically set IPv6 address of the virtual machine.

.Prerequisites

* The {product-title} cluster must use the OVN-Kubernetes Container Network Interface (CNI) network plugin configured for dual-stack.

.Procedure

. In a new virtual machine configuration, include an interface with `masquerade` and configure the IPv6 address and default gateway by using cloud-init.
+
[source,yaml]
----
apiVersion: kubevirt.io/v1
kind: VirtualMachine
metadata:
  name: example-vm-ipv6
spec:
  template:
    spec:
      domain:
        devices:
          interfaces:
            - name: default
              masquerade: {} <1>
              ports:
                - port: 80 <2>
# ...
      networks:
      - name: default
        pod: {}
      volumes:
      - cloudInitNoCloud:
          networkData: |
            version: 2
            ethernets:
              eth0:
                dhcp4: true
                addresses: [ fd10:0:2::2/120 ] <3>
                gateway6: fd10:0:2::1 <4>
----
<1> Connect using masquerade mode.
<2> Allows incoming traffic on port 80 to the virtual machine.
<3> The static IPv6 address as determined by the `Network.pod.vmIPv6NetworkCIDR` field in the virtual machine instance configuration. The default value is `fd10:0:2::2/120`.
<4> The gateway IP address as determined by the `Network.pod.vmIPv6NetworkCIDR` field in the virtual machine instance configuration. The default value is `fd10:0:2::1`.

. Create the virtual machine in the namespace:
+
[source,terminal]
----
$ oc create -f example-vm-ipv6.yaml
----

.Verification

* To verify that IPv6 has been configured, start the virtual machine and view the interface status of the virtual machine instance to ensure it has an IPv6 address:

[source,terminal]
----
$ oc get vmi <vmi-name> -o jsonpath="{.status.interfaces[*].ipAddresses}"
----

:leveloffset!:

// TO DO: This should be moved to an optimization section
:leveloffset: +1

// Module included in the following assemblies:
//
// * virt/vm_networking/virt-connecting-vm-to-default-pod-network.adoc

:_mod-docs-content-type: CONCEPT
[id="virt-jumbo-frames-vm-pod-nw_{context}"]
= About jumbo frames support

When using the OVN-Kubernetes CNI plugin, you can send unfragmented jumbo frame packets between two virtual machines (VMs) that are connected on the default pod network. Jumbo frames have a maximum transmission unit (MTU) value greater than 1500 bytes.

The VM automatically gets the MTU value of the cluster network, set by the cluster administrator, in one of the following ways:

* `libvirt`: If the guest OS has the latest version of the VirtIO driver that can interpret incoming data via a Peripheral Component Interconnect (PCI) config register in the emulated device.

* DHCP: If the guest DHCP client can read the MTU value from the DHCP server response.

[NOTE]
====
For Windows VMs that do not have a VirtIO driver, you must set the MTU manually by using `netsh` or a similar tool. This is because the Windows DHCP client does not read the MTU value.
====

:leveloffset!:

[role="_additional-resources"]
[id="additional-resources_virt-connecting-vm-to-default-pod-network"]
== Additional resources
* xref:../../networking/changing-cluster-network-mtu.adoc#changing-cluster-network-mtu[Changing the MTU for the cluster network]
* xref:../../scalability_and_performance/optimization/optimizing-networking.adoc#optimizing-mtu_optimizing-networking[Optimizing the MTU for your network]

//# includes=_attributes/common-attributes,modules/virt-configuring-masquerade-mode-cli,modules/virt-configuring-masquerade-mode-dual-stack,modules/virt-jumbo-frames-vm-pod-nw

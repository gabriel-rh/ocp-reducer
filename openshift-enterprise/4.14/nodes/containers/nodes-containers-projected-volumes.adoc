:_mod-docs-content-type: ASSEMBLY
:context: nodes-containers-projected-volumes
[id="nodes-containers-projected-volumes"]
= Mapping volumes using projected volumes
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]






A _projected volume_ maps several existing volume sources into the same directory.

The following types of volume sources can be projected:

* Secrets

* Config Maps

* Downward API

[NOTE]
====
All sources are required to be in the same namespace as the pod.
====

// The following include statements pull in the module files that comprise
// the assembly. Include any combination of concept, procedure, or reference
// modules required to cover the user story. You can also include other
// assemblies.

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-containers-projected-volumes.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-containers-projected-volumes-about_{context}"]
= Understanding projected volumes

Projected volumes can map any combination of these volume sources into a single directory, allowing the user to:

* automatically populate a single volume with the keys from multiple secrets, config maps, and with downward API information,
so that I can synthesize a single directory with various sources of information;
* populate a single volume with the keys from multiple secrets, config maps, and with downward API information,
explicitly specifying paths for each item, so that I can have full control over the contents of that volume.

[IMPORTANT]
====
When the `RunAsUser` permission is set in the security context of a Linux-based pod, the projected files have the correct permissions set, including container user ownership. However, when the Windows equivalent `RunAsUsername` permission is set in a Windows pod, the kubelet is unable to correctly set ownership on the files in the projected volume.

Therefore, the `RunAsUsername` permission set in the security context of a Windows pod is not honored for Windows projected volumes running in {product-title}.
====

The following general scenarios show how you can use projected volumes.

*Config map, secrets, Downward API.*::
Projected volumes allow you to deploy containers with configuration data that includes passwords.
An application using these resources could be deploying {rh-openstack-first} on Kubernetes. The configuration data might have to be assembled differently depending on if the services are going to be used for production or for testing. If a pod is labeled with production or testing, the downward API selector `metadata.labels` can be used to produce the correct {rh-openstack} configs.

*Config map + secrets.*::
Projected volumes allow you to deploy containers involving configuration data and passwords.
For example, you might execute a config map with some sensitive encrypted tasks that are decrypted using a vault password file.

*ConfigMap + Downward API.*::
Projected volumes allow you to generate a config including the pod name (available via the `metadata.name` selector). This application can then pass the pod name along with requests to easily determine the source without using IP tracking.

*Secrets + Downward API.*::
Projected volumes allow you to use a secret as a public key to encrypt the namespace of the pod (available via the `metadata.namespace` selector).
This example allows the Operator to use the application to deliver the namespace information securely without using an encrypted transport.

[id="projected-volumes-examples_{context}"]
== Example Pod specs

The following are examples of `Pod` specs for creating projected volumes.

.Pod with a secret, a Downward API, and a config map

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts: <1>
    - name: all-in-one
      mountPath: "/projected-volume"<2>
      readOnly: true <3>
  volumes: <4>
  - name: all-in-one <5>
    projected:
      defaultMode: 0400 <6>
      sources:
      - secret:
          name: mysecret <7>
          items:
            - key: username
              path: my-group/my-username <8>
      - downwardAPI: <9>
          items:
            - path: "labels"
              fieldRef:
                fieldPath: metadata.labels
            - path: "cpu_limit"
              resourceFieldRef:
                containerName: container-test
                resource: limits.cpu
      - configMap: <10>
          name: myconfigmap
          items:
            - key: config
              path: my-group/my-config
              mode: 0777 <11>
----

<1> Add a `volumeMounts` section for each container that needs the secret.
<2> Specify a path to an unused directory where the secret will appear.
<3> Set `readOnly` to `true`.
<4> Add a `volumes` block to list each projected volume source.
<5> Specify any name for the volume.
<6> Set the execute permission on the files.
<7> Add a secret. Enter the name of the secret object. Each secret you want to use must be listed.
<8> Specify the path to the secrets file under the `mountPath`. Here, the secrets file is in *_/projected-volume/my-group/my-username_*.
<9> Add a Downward API source.
<10> Add a ConfigMap source.
<11> Set the mode for the specific projection

[NOTE]
====
If there are multiple containers in the pod, each container needs a `volumeMounts` section, but only one `volumes` section is needed.
====

.Pod with multiple secrets with a non-default permission mode set

[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      defaultMode: 0755
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/my-username
      - secret:
          name: mysecret2
          items:
            - key: password
              path: my-group/my-password
              mode: 511
----

[NOTE]
====
The `defaultMode` can only be specified at the projected level and not for each
volume source. However, as illustrated above, you can explicitly set the `mode`
for each individual projection.
====

[id="projected-volumes-pathing_{context}"]
== Pathing Considerations

*Collisions Between Keys when Configured Paths are Identical*:: If you configure any keys with the same path, the pod spec will not be accepted as valid.
In the following example, the specified path for `mysecret` and `myconfigmap` are the same:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: volume-test
spec:
  containers:
  - name: container-test
    image: busybox
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret
          items:
            - key: username
              path: my-group/data
      - configMap:
          name: myconfigmap
          items:
            - key: config
              path: my-group/data
----

Consider the following situations related to the volume file paths.

*Collisions Between Keys without Configured Paths*:: The only run-time validation that can occur is when all the paths are known at pod creation, similar to the above scenario. Otherwise, when a conflict occurs the most recent specified resource will overwrite anything preceding it
(this is true for resources that are updated after pod creation as well).

*Collisions when One Path is Explicit and the Other is Automatically Projected*:: In the event that there is a collision due to a user specified path matching data that is automatically projected,
the latter resource will overwrite anything preceding it as before

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-containers-projected-volumes.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-containers-projected-volumes-creating_{context}"]
= Configuring a Projected Volume for a Pod

When creating projected volumes, consider the volume file path situations described in _Understanding projected volumes_.

The following example shows how to use a projected volume to mount an existing secret volume source. The steps can be used to create a user name and password secrets from local files. You then create a pod that runs one container, using a projected volume to mount the secrets into the same shared directory.

The user name and password values can be any valid string that is *base64* encoded.

The following example shows `admin` in base64:

[source,terminal]
----
$ echo -n "admin" | base64
----

.Example output
[source,terminal]
----
YWRtaW4=
----

The following example shows the password `1f2d1e2e67df` in base64:

[source,terminal]
----
$ echo -n "1f2d1e2e67df" | base64
----

.Example output
[source,terminal]
----
MWYyZDFlMmU2N2Rm
----

.Procedure

To use a projected volume to mount an existing secret volume source.

. Create the secret:

.. Create a YAML file similar to the following, replacing the password and user information as appropriate:
+
[source,yaml]
----
apiVersion: v1
kind: Secret
metadata:
  name: mysecret
type: Opaque
data:
  pass: MWYyZDFlMmU2N2Rm
  user: YWRtaW4=
----
+
.. Use the following command to create the secret:
+
[source,terminal]
----
$ oc create -f <secrets-filename>
----
+
For example:
+
[source,terminal]
----
$ oc create -f secret.yaml
----
+
.Example output
[source,terminal]
----
secret "mysecret" created
----

.. You can check that the secret was created using the following commands:
+
[source,terminal]
----
$ oc get secret <secret-name>
----
+
For example:
+
[source,terminal]
----
$ oc get secret mysecret
----
+
.Example output
[source,terminal]
----
NAME       TYPE      DATA      AGE
mysecret   Opaque    2         17h
----
+
[source,terminal]
----
$ oc get secret <secret-name> -o yaml
----
+
For example:
+
[source,terminal]
----
$ oc get secret mysecret -o yaml
----
+
[source,yaml]
----
apiVersion: v1
data:
  pass: MWYyZDFlMmU2N2Rm
  user: YWRtaW4=
kind: Secret
metadata:
  creationTimestamp: 2017-05-30T20:21:38Z
  name: mysecret
  namespace: default
  resourceVersion: "2107"
  selfLink: /api/v1/namespaces/default/secrets/mysecret
  uid: 959e0424-4575-11e7-9f97-fa163e4bd54c
type: Opaque
----

. Create a pod with a projected volume.

.. Create a YAML file similar to the following, including a `volumes` section:
+
[source,yaml]
----
kind: Pod
metadata:
  name: test-projected-volume
spec:
  containers:
  - name: test-projected-volume
    image: busybox
    args:
    - sleep
    - "86400"
    volumeMounts:
    - name: all-in-one
      mountPath: "/projected-volume"
      readOnly: true
      securityContext:
        allowPrivilegeEscalation: false
        capabilities:
          drop:
            - ALL
  volumes:
  - name: all-in-one
    projected:
      sources:
      - secret:
          name: mysecret <1>
----
<1> The name of the secret you created.

.. Create the pod from the configuration file:
+
[source,terminal]
----
$ oc create -f <your_yaml_file>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f secret-pod.yaml
----
+
.Example output
[source,terminal]
----
pod "test-projected-volume" created
----

. Verify that the pod container is running, and then watch for changes to
the pod:
+
[source,terminal]
----
$ oc get pod <name>
----
+
For example:
+
[source,terminal]
----
$ oc get pod test-projected-volume
----
+
The output should appear similar to the following:
+
.Example output
[source,terminal]
----
NAME                    READY     STATUS    RESTARTS   AGE
test-projected-volume   1/1       Running   0          14s
----

. In another terminal, use the `oc exec` command to open a shell to the running container:
+
[source,terminal]
----
$ oc exec -it <pod> <command>
----
+
For example:
+
[source,terminal]
----
$ oc exec -it test-projected-volume -- /bin/sh
----

. In your shell, verify that the `projected-volumes` directory contains your projected sources:
+
[source,terminal]
----
/ # ls
----
+
.Example output
[source,terminal]
----
bin               home              root              tmp
dev               proc              run               usr
etc               projected-volume  sys               var
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/nodes-containers-projected-volumes-about,modules/nodes-containers-projected-volumes-creating

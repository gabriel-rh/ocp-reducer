:_mod-docs-content-type: ASSEMBLY
:context: nodes-pods-autoscaling
[id='nodes-pods-autoscaling']
= Automatically scaling pods with the horizontal pod autoscaler
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

As a developer, you can use a horizontal pod autoscaler (HPA) to
specify how {product-title} should automatically increase or decrease the scale of
a replication controller or deployment configuration, based on metrics collected
from the pods that belong to that replication controller or deployment
configuration. You can create an HPA for any any deployment, deployment config, replica set, replication controller, or stateful set.

For information on scaling pods based on custom metrics, see xref:../../nodes/cma/nodes-cma-autoscaling-custom.adoc#nodes-cma-autoscaling-custom[Automatically scaling pods based on custom metrics].


[NOTE]
====
It is recommended to use a `Deployment` object or `ReplicaSet` object unless you need a specific feature or behavior provided by other objects. For more information on
these objects, see xref:../../applications/deployments/what-deployments-are.adoc#what-deployments-are[Understanding deployments].
====

// The following include statements pull in the module files that comprise
// the assembly. Include any combination of concept, procedure, or reference
// modules required to cover the user story. You can also include other
// assemblies.

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-autoscaling-about_{context}"]
= Understanding horizontal pod autoscalers

You can create a horizontal pod autoscaler to specify the minimum and maximum number of pods
you want to run, as well as the CPU utilization or memory utilization your pods should target.

After you create a horizontal pod autoscaler, {product-title} begins to query the CPU and/or memory resource metrics on the pods.
When these metrics are available, the horizontal pod autoscaler computes
the ratio of the current metric utilization with the desired metric utilization,
and scales up or down accordingly. The query and scaling occurs at a regular interval,
but can take one to two minutes before metrics become available.

For replication controllers, this scaling corresponds directly to the replicas
of the replication controller. For deployment configurations, scaling corresponds
directly to the replica count of the deployment configuration. Note that autoscaling
applies only to the latest deployment in the `Complete` phase.

{product-title} automatically accounts for resources and prevents unnecessary autoscaling
during resource spikes, such as during start up. Pods in the `unready` state
have `0 CPU` usage when scaling up and the autoscaler ignores the pods when scaling down.
Pods without known metrics have `0% CPU` usage when scaling up and `100% CPU` when scaling down.
This allows for more stability during the HPA decision. To use this feature, you must configure
readiness checks to determine if a new pod is ready for use.


== Supported metrics

The following metrics are supported by horizontal pod autoscalers:

.Metrics
[cols="3a,5a,5a",options="header"]
|===

|Metric |Description |API version

|CPU utilization
|Number of CPU cores used. Can be used to calculate a percentage of the pod's requested CPU.
|`autoscaling/v1`, `autoscaling/v2`

|Memory utilization
|Amount of memory used. Can be used to calculate a percentage of the pod's requested memory.
|`autoscaling/v2`
|===

[IMPORTANT]
====
For memory-based autoscaling, memory usage must increase and decrease
proportionally to the replica count. On average:

* An increase in replica count must lead to an overall decrease in memory
(working set) usage per-pod.
* A decrease in replica count must lead to an overall increase in per-pod memory
usage.

Use the {product-title} web console to check the memory behavior of your application
and ensure that your application meets these requirements before using
memory-based autoscaling.
====

The following example shows autoscaling for the `image-registry` `Deployment` object. The initial deployment requires 3 pods. The HPA object increases the minimum to 5. If CPU usage on the pods reaches 75%, the pods increase to 7:

[source,terminal]
----
$ oc autoscale deployment/image-registry --min=5 --max=7 --cpu-percent=75
----

.Example output
[source,terminal]
----
horizontalpodautoscaler.autoscaling/image-registry autoscaled
----

.Sample HPA for the `image-registry` `Deployment` object with `minReplicas` set to 3
[source,yaml]
----
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  name: image-registry
  namespace: default
spec:
  maxReplicas: 7
  minReplicas: 3
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: image-registry
  targetCPUUtilizationPercentage: 75
status:
  currentReplicas: 5
  desiredReplicas: 0
----

. View the new state of the deployment:
+
[source,terminal]
----
$ oc get deployment image-registry
----
+
There are now 5 pods in the deployment:
+
.Example output
[source,terminal]
----
NAME             REVISION   DESIRED   CURRENT   TRIGGERED BY
image-registry   1          5         5         config
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-autoscaling-workflow-hpa_{context}"]
= How does the HPA work?

The horizontal pod autoscaler (HPA) extends the concept of pod auto-scaling. The HPA lets you create and manage a group of load-balanced nodes. The HPA automatically increases or decreases the number of pods when a given CPU or memory threshold is crossed.

.High level workflow of the HPA
image::HPAflow.png[workflow]

The HPA is an API resource in the Kubernetes autoscaling API group. The autoscaler works as a control loop with a default of 15 seconds for the sync period. During this period, the controller manager queries the CPU, memory utilization, or both, against what is defined in the YAML file for the HPA.
The controller manager obtains the utilization metrics from the resource metrics API for per-pod resource metrics like CPU or memory, for each pod that is targeted by the HPA.

If a utilization value target is set, the controller calculates the utilization value as a percentage of the equivalent resource request on the containers in each pod. The controller then takes the average of utilization across all targeted pods and produces a ratio that is used to scale the number of desired replicas.
The HPA is configured to fetch metrics from `metrics.k8s.io`, which is provided by the metrics server. Because of the dynamic nature of metrics evaluation, the number of replicas can fluctuate during scaling for a group of replicas.

[NOTE]
====
To implement the HPA, all targeted pods must have a resource request set on their containers.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-autoscaling-requests-and-limits-hpa_{context}"]
= About requests and limits

The scheduler uses the resource request that you specify for containers in a pod, to decide which node to place the pod on. The kubelet enforces the resource limit that you specify for a container to ensure that the container is not allowed to use more than the specified limit.
The kubelet also reserves the request amount of that system resource specifically for that container to use.

.How to use resource metrics?

In the pod specifications, you must specify the resource requests, such as CPU and memory. The HPA uses this specification to determine the resource utilization and then scales the target up or down.

For example, the HPA object uses the following metric source:

[source,yaml]
----
type: Resource
resource:
  name: cpu
  target:
    type: Utilization
    averageUtilization: 60
----

In this example, the HPA keeps the average utilization of the pods in the scaling target at 60%. Utilization is the ratio between the current resource usage to the requested resource of the pod.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-autoscaling-best-practices-hpa_{context}"]
= Best practices

.All pods must have resource requests configured
The HPA makes a scaling decision based on the observed CPU or memory utilization values of pods in an {product-title} cluster. Utilization values are calculated as a percentage of the resource requests of each pod.
Missing resource request values can affect the optimal performance of the HPA.

.Configure the cool down period
During horizontal pod autoscaling, there might be a rapid scaling of events without a time gap. Configure the cool down period to prevent frequent replica fluctuations.
You can specify a cool down period by configuring the `stabilizationWindowSeconds` field. The stabilization window is used to restrict the fluctuation of replicas count when the metrics used for scaling keep fluctuating.
The autoscaling algorithm uses this window to infer a previous desired state and avoid unwanted changes to workload scale.

For example, a stabilization window is specified for the `scaleDown` field:

[source,yaml]
----
behavior:
  scaleDown:
    stabilizationWindowSeconds: 300
----

In the above example, all desired states for the past 5 minutes are considered. This approximates a rolling maximum, and avoids having the scaling algorithm frequently remove pods only to trigger recreating an equivalent pod just moments later.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling.adoc

[id="nodes-pods-autoscaling-policies_{context}"]
= Scaling policies

The `autoscaling/v2` API allows you to add _scaling policies_ to a horizontal pod autoscaler. A scaling policy controls how the {product-title} horizontal pod autoscaler (HPA) scales pods. Scaling policies allow you to restrict the rate that HPAs scale pods up or down by setting a specific number or specific percentage to scale in a specified period of time. You can also define a _stabilization window_, which uses previously computed desired states to control scaling if the metrics are fluctuating. You can create multiple policies for the same scaling direction, and determine which policy is used, based on the amount of change. You can also restrict the scaling by timed iterations. The HPA scales pods during an iteration, then performs scaling, as needed, in further iterations.

.Sample HPA object with a scaling policy
[source, yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-resource-metrics-memory
  namespace: default
spec:
  behavior:
    scaleDown: <1>
      policies: <2>
      - type: Pods <3>
        value: 4 <4>
        periodSeconds: 60 <5>
      - type: Percent
        value: 10 <6>
        periodSeconds: 60
      selectPolicy: Min <7>
      stabilizationWindowSeconds: 300 <8>
    scaleUp: <9>
      policies:
      - type: Pods
        value: 5 <10>
        periodSeconds: 70
      - type: Percent
        value: 12 <11>
        periodSeconds: 80
      selectPolicy: Max
      stabilizationWindowSeconds: 0
...
----
<1> Specifies the direction for the scaling policy, either `scaleDown` or `scaleUp`. This example creates a policy for scaling down.
<2> Defines the scaling policy.
<3> Determines if the policy scales by a specific number of pods or a percentage of pods during each iteration. The default value is `pods`.
<4> Limits the amount of scaling, either the number of pods or percentage of pods, during each iteration. There is no default value for scaling down by number of pods.
<5> Determines the length of a scaling iteration. The default value is `15` seconds.
<6> The default value for scaling down by percentage is 100%.
<7> Determines which policy to use first, if multiple policies are defined. Specify `Max` to use the policy that allows the highest amount of change, `Min` to use the policy that allows the lowest amount of change, or `Disabled` to prevent the HPA from scaling in that policy direction. The default value is `Max`.
<8> Determines the time period the HPA should look back at desired states. The default value is `0`.
<9> This example creates a policy for scaling up.
<10> Limits the amount of scaling up by the number of pods. The default value for scaling up the number of pods is 4%.
<11> Limits the amount of scaling up by the percentage of pods. The default value for scaling up by percentage is 100%.

.Example policy for scaling down
[source,yaml]
----
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-resource-metrics-memory
  namespace: default
spec:
...
  minReplicas: 20
...
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 4
        periodSeconds: 30
      - type: Percent
        value: 10
        periodSeconds: 60
      selectPolicy: Max
    scaleUp:
      selectPolicy: Disabled
----

In this example, when the number of pods is greater than 40, the percent-based policy is used for scaling down, as that policy results in a larger change, as required by the `selectPolicy`.

If there are 80 pod replicas, in the first iteration the HPA reduces the pods by 8, which is 10% of the 80 pods (based on the `type: Percent` and `value: 10` parameters), over one minute (`periodSeconds: 60`). For the next iteration, the number of pods is 72. The HPA calculates that 10% of the remaining pods is 7.2, which it rounds up to 8 and scales down 8 pods. On each subsequent iteration, the number of pods to be scaled is re-calculated based on the number of remaining pods. When the number of pods falls below 40, the pods-based policy is applied, because the pod-based number is greater than the percent-based number. The HPA reduces 4 pods at a time (`type: Pods` and `value: 4`), over 30 seconds (`periodSeconds: 30`), until there are 20 replicas remaining (`minReplicas`).

The `selectPolicy: Disabled` parameter prevents the HPA from scaling up the pods. You can manually scale up by adjusting the number of replicas in the replica set or deployment set, if needed.

If set, you can view the scaling policy by using the `oc edit` command:

[source,terminal]
----
$ oc edit hpa hpa-resource-metrics-memory
----

.Example output
[source,terminal]
----
apiVersion: autoscaling/v1
kind: HorizontalPodAutoscaler
metadata:
  annotations:
    autoscaling.alpha.kubernetes.io/behavior:\
'{"ScaleUp":{"StabilizationWindowSeconds":0,"SelectPolicy":"Max","Policies":[{"Type":"Pods","Value":4,"PeriodSeconds":15},{"Type":"Percent","Value":100,"PeriodSeconds":15}]},\
"ScaleDown":{"StabilizationWindowSeconds":300,"SelectPolicy":"Min","Policies":[{"Type":"Pods","Value":4,"PeriodSeconds":60},{"Type":"Percent","Value":10,"PeriodSeconds":60}]}}'
...
----


:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-pods-autoscaling-creating-web-console_{context}"]
= Creating a horizontal pod autoscaler by using the web console

From the web console, you can create a horizontal pod autoscaler (HPA) that specifies the minimum and maximum number of pods you want to run on a `Deployment` or `DeploymentConfig` object. You can also define the amount of CPU or memory usage that your pods should target.

[NOTE]
====
An HPA cannot be added to deployments that are part of an Operator-backed service, Knative service, or Helm chart.
====

.Procedure

To create an HPA in the web console:

. In the *Topology* view, click the node to reveal the side pane.
. From the *Actions* drop-down list, select *Add HorizontalPodAutoscaler* to open the *Add HorizontalPodAutoscaler* form.
+
.Add HorizontalPodAutoscaler
image::node-add-hpa-action.png[Add HorizontalPodAutoscaler form]

. From the *Add HorizontalPodAutoscaler* form, define the name, minimum and maximum pod limits, the CPU and memory usage, and click *Save*.
+
[NOTE]
====
If any of the values for CPU and memory usage are missing, a warning is displayed.
====

To edit an HPA in the web console:

. In the *Topology* view, click the node to reveal the side pane.
. From the *Actions* drop-down list, select *Edit HorizontalPodAutoscaler* to open the *Edit Horizontal Pod Autoscaler* form.
. From the *Edit Horizontal Pod Autoscaler* form, edit the minimum and maximum pod limits and the CPU and memory usage, and click *Save*.

[NOTE]
====
While creating or editing the horizontal pod autoscaler in the web console, you can switch from *Form view* to *YAML view*.
====

To remove an HPA in the web console:

. In the *Topology* view, click the node to reveal the side panel.
. From the *Actions* drop-down list, select *Remove HorizontalPodAutoscaler*.
. In the confirmation pop-up window, click *Remove* to remove the HPA.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-pods-autoscaling-creating-cpu_{context}"]
= Creating a horizontal pod autoscaler for CPU utilization by using the CLI

Using the {product-title} CLI, you can create a horizontal pod autoscaler (HPA) to automatically scale an existing `Deployment`, `DeploymentConfig`, `ReplicaSet`, `ReplicationController`, or `StatefulSet` object. The HPA scales the pods associated with that object to maintain the CPU usage you specify.

[NOTE]
====
It is recommended to use a `Deployment` object or `ReplicaSet` object unless you need a specific feature or behavior provided by other objects.
====

The HPA increases and decreases the number of replicas between the minimum and maximum numbers to maintain the specified CPU utilization across all pods.

When autoscaling for CPU utilization, you can use the `oc autoscale` command and specify the minimum and maximum number of pods you want to run at any given time and the average CPU utilization your pods should target. If you do not specify a minimum, the pods are given default values from the {product-title} server.

To autoscale for a specific CPU value, create a `HorizontalPodAutoscaler` object with the target CPU and pod limits.

.Prerequisites

To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics.
You can use the `oc describe PodMetrics <pod-name>` command to determine if metrics are configured. If metrics are
configured, the output appears similar to the following, with `Cpu` and `Memory` displayed under `Usage`.

[source,terminal]
----
$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
----

.Example output
[source,text,options="nowrap"]
----
Name:         openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Namespace:    openshift-kube-scheduler
Labels:       <none>
Annotations:  <none>
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2019-05-23T18:47:56Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Timestamp:             2019-05-23T18:47:56Z
Window:                1m0s
Events:                <none>
----

.Procedure

To create a horizontal pod autoscaler for CPU utilization:

. Perform one of the following:

** To scale based on the percent of CPU utilization, create a `HorizontalPodAutoscaler` object for an existing object:
+
[source,terminal]
----
$ oc autoscale <object_type>/<name> \// <1>
  --min <number> \// <2>
  --max <number> \// <3>
  --cpu-percent=<percent> <4>
----
+
<1> Specify the type and name of the object to autoscale. The object must exist and be a `Deployment`, `DeploymentConfig`/`dc`, `ReplicaSet`/`rs`, `ReplicationController`/`rc`, or `StatefulSet`.
<2> Optionally, specify the minimum number of replicas when scaling down.
<3> Specify the maximum number of replicas when scaling up.
<4> Specify the target average CPU utilization over all the pods, represented as a percent of requested CPU. If not specified or negative, a default autoscaling policy is used.
+
For example, the following command shows autoscaling for the `image-registry` `Deployment` object. The initial deployment requires 3 pods. The HPA object increases the minimum to 5. If CPU usage on the pods reaches 75%, the pods will increase to 7:
+
[source,terminal]
----
$ oc autoscale deployment/image-registry --min=5 --max=7 --cpu-percent=75
----

** To scale for a specific CPU value, create a YAML file similar to the following for an existing object:
+
.. Create a YAML file similar to the following:
+
[source,yaml,options="nowrap"]
----
apiVersion: autoscaling/v2 <1>
kind: HorizontalPodAutoscaler
metadata:
  name: cpu-autoscale <2>
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <3>
    kind: Deployment <4>
    name: example <5>
  minReplicas: 1 <6>
  maxReplicas: 10 <7>
  metrics: <8>
  - type: Resource
    resource:
      name: cpu <9>
      target:
        type: AverageValue <10>
        averageValue: 500m <11>
----
<1> Use the `autoscaling/v2` API.
<2> Specify a name for this horizontal pod autoscaler object.
<3> Specify the API version of the object to scale:
* For a `Deployment`, `ReplicaSet`, `Statefulset` object, use `apps/v1`.
* For a `ReplicationController`, use `v1`.
* For a `DeploymentConfig`, use `apps.openshift.io/v1`.
<4> Specify the type of object. The object must be a `Deployment`, `DeploymentConfig`/`dc`, `ReplicaSet`/`rs`, `ReplicationController`/`rc`, or `StatefulSet`.
<5> Specify the name of the object to scale. The object must exist.
<6> Specify the minimum number of replicas when scaling down.
<7> Specify the maximum number of replicas when scaling up.
<8> Use the `metrics` parameter for memory utilization.
<9> Specify `cpu` for CPU utilization.
<10> Set to `AverageValue`.
<11> Set to `averageValue` with the targeted CPU value.

.. Create the horizontal pod autoscaler:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----

. Verify that the horizontal pod autoscaler was created:
+
[source,terminal]
----
$ oc get hpa cpu-autoscale
----
+
.Example output
[source,terminal]
----
NAME            REFERENCE            TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
cpu-autoscale   Deployment/example   173m/500m       1         10        1          20m
----


:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-pods-autoscaling-creating-memory_{context}"]

= Creating a horizontal pod autoscaler object for memory utilization by using the CLI

Using the {product-title} CLI, you can create a horizontal pod autoscaler (HPA) to automatically scale an existing
`Deployment`, `DeploymentConfig`, `ReplicaSet`, `ReplicationController`, or `StatefulSet` object. The HPA
scales the pods associated with that object to maintain the average memory utilization you specify, either a direct value or a percentage
of requested memory.

[NOTE]
====
It is recommended to use a `Deployment` object or `ReplicaSet` object unless you need a specific feature or behavior provided by other objects.
====

The HPA increases and decreases the number of replicas between the minimum and maximum numbers to maintain
the specified memory utilization across all pods.

For memory utilization, you can specify the minimum and maximum number of pods and the average memory utilization
your pods should target. If you do not specify a minimum, the pods are given default values from the {product-title} server.

.Prerequisites

To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics.
You can use the `oc describe PodMetrics <pod-name>` command to determine if metrics are configured. If metrics are
configured, the output appears similar to the following, with `Cpu` and `Memory` displayed under `Usage`.

[source,terminal]
----
$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-129-223.compute.internal -n openshift-kube-scheduler
----

.Example output
[source,text,options="nowrap"]
----
Name:         openshift-kube-scheduler-ip-10-0-129-223.compute.internal
Namespace:    openshift-kube-scheduler
Labels:       <none>
Annotations:  <none>
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Cpu:     0
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2020-02-14T22:21:14Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-129-223.compute.internal
Timestamp:             2020-02-14T22:21:14Z
Window:                5m0s
Events:                <none>
----

.Procedure

To create a horizontal pod autoscaler for memory utilization:

. Create a YAML file for one of the following:

** To scale for a specific memory value, create a `HorizontalPodAutoscaler` object similar to the following for an existing object:
+
[source,yaml,options="nowrap"]
----
apiVersion: autoscaling/v2 <1>
kind: HorizontalPodAutoscaler
metadata:
  name: hpa-resource-metrics-memory <2>
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <3>
    kind: Deployment <4>
    name: example <5>
  minReplicas: 1 <6>
  maxReplicas: 10 <7>
  metrics: <8>
  - type: Resource
    resource:
      name: memory <9>
      target:
        type: AverageValue <10>
        averageValue: 500Mi <11>
  behavior: <12>
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Pods
        value: 4
        periodSeconds: 60
      - type: Percent
        value: 10
        periodSeconds: 60
      selectPolicy: Max
----
<1> Use the `autoscaling/v2` API.
<2> Specify a name for this horizontal pod autoscaler object.
<3> Specify the API version of the object to scale:
* For a `Deployment`, `ReplicaSet`, or `Statefulset` object, use `apps/v1`.
* For a `ReplicationController`, use `v1`.
* For a `DeploymentConfig`, use `apps.openshift.io/v1`.
<4> Specify the type of object. The object must be a `Deployment`, `DeploymentConfig`,
`ReplicaSet`, `ReplicationController`, or `StatefulSet`.
<5> Specify the name of the object to scale. The object must exist.
<6> Specify the minimum number of replicas when scaling down.
<7> Specify the maximum number of replicas when scaling up.
<8> Use the `metrics` parameter for memory utilization.
<9> Specify `memory` for memory utilization.
<10> Set the type to `AverageValue`.
<11> Specify `averageValue` and a specific memory value.
<12> Optional: Specify a scaling policy to control the rate of scaling up or down.

** To scale for a percentage, create a `HorizontalPodAutoscaler` object similar to the following for an existing object:
+
[source,yaml,options="nowrap"]
----
apiVersion: autoscaling/v2 <1>
kind: HorizontalPodAutoscaler
metadata:
  name: memory-autoscale <2>
  namespace: default
spec:
  scaleTargetRef:
    apiVersion: apps/v1 <3>
    kind: Deployment <4>
    name: example <5>
  minReplicas: 1 <6>
  maxReplicas: 10 <7>
  metrics: <8>
  - type: Resource
    resource:
      name: memory <9>
      target:
        type: Utilization <10>
        averageUtilization: 50 <11>
  behavior: <12>
    scaleUp:
      stabilizationWindowSeconds: 180
      policies:
      - type: Pods
        value: 6
        periodSeconds: 120
      - type: Percent
        value: 10
        periodSeconds: 120
      selectPolicy: Max
----
<1> Use the `autoscaling/v2` API.
<2> Specify a name for this horizontal pod autoscaler object.
<3> Specify the API version of the object to scale:
* For a ReplicationController, use `v1`.
* For a DeploymentConfig, use `apps.openshift.io/v1`.
* For a Deployment, ReplicaSet, Statefulset object, use `apps/v1`.
<4> Specify the type of object. The object must be a `Deployment`, `DeploymentConfig`,
`ReplicaSet`, `ReplicationController`, or `StatefulSet`.
<5> Specify the name of the object to scale. The object must exist.
<6> Specify the minimum number of replicas when scaling down.
<7> Specify the maximum number of replicas when scaling up.
<8> Use the `metrics` parameter for memory utilization.
<9> Specify `memory` for memory utilization.
<10> Set to `Utilization`.
<11> Specify `averageUtilization` and a target average memory utilization over all the pods,
represented as a percent of requested memory. The target pods must have memory requests configured.
<12> Optional: Specify a scaling policy to control the rate of scaling up or down.

. Create the horizontal pod autoscaler:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----
+
For example:
+
[source,terminal]
----
$ oc create -f hpa.yaml
----
+
.Example output
[source,terminal]
----
horizontalpodautoscaler.autoscaling/hpa-resource-metrics-memory created
----

. Verify that the horizontal pod autoscaler was created:
+
[source,terminal]
----
$ oc get hpa hpa-resource-metrics-memory
----
+
.Example output
[source,terminal]
----
NAME                          REFERENCE            TARGETS         MINPODS   MAXPODS   REPLICAS   AGE
hpa-resource-metrics-memory   Deployment/example   2441216/500Mi   1         10        1          20m
----
+
[source,terminal]
----
$ oc describe hpa hpa-resource-metrics-memory
----
+
.Example output
[source,text]
----
Name:                        hpa-resource-metrics-memory
Namespace:                   default
Labels:                      <none>
Annotations:                 <none>
CreationTimestamp:           Wed, 04 Mar 2020 16:31:37 +0530
Reference:                   Deployment/example
Metrics:                     ( current / target )
  resource memory on pods:   2441216 / 500Mi
Min replicas:                1
Max replicas:                10
ReplicationController pods:  1 current / 1 desired
Conditions:
  Type            Status  Reason              Message
  ----            ------  ------              -------
  AbleToScale     True    ReadyForNewScale    recommended size matches current size
  ScalingActive   True    ValidMetricFound    the HPA was able to successfully calculate a replica count from memory resource
  ScalingLimited  False   DesiredWithinRange  the desired count is within the acceptable range
Events:
  Type     Reason                   Age                 From                       Message
  ----     ------                   ----                ----                       -------
  Normal   SuccessfulRescale        6m34s               horizontal-pod-autoscaler  New size: 1; reason: All metrics below target
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-autoscaling-status-about_{context}"]

= Understanding horizontal pod autoscaler status conditions by using the CLI

You can use the status conditions set to determine
whether or not the horizontal pod autoscaler (HPA) is able to scale and whether or not it is currently restricted
in any way.

The HPA status conditions are available with the `v2` version of the
autoscaling API.

The HPA responds with the following status conditions:

* The `AbleToScale` condition indicates whether HPA is able to fetch and update metrics, as well as whether any backoff-related conditions could prevent scaling.
** A `True` condition indicates scaling is allowed.
** A `False` condition indicates scaling is not allowed for the reason specified.

* The `ScalingActive` condition indicates whether the HPA is enabled (for example, the replica count of the target is not zero) and is able to calculate desired metrics.
** A `True` condition indicates metrics is working properly.
** A `False` condition generally indicates a problem with fetching metrics.

* The `ScalingLimited` condition indicates that the desired scale was capped by the maximum or minimum of the horizontal pod autoscaler.
** A `True` condition indicates that you need to raise or lower the minimum or maximum replica count in order to scale.
** A `False` condition indicates that the requested scaling is allowed.
+
[source,terminal]
----
$ oc describe hpa cm-test
----
+
.Example output
[source,text]
----
Name:                           cm-test
Namespace:                      prom
Labels:                         <none>
Annotations:                    <none>
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        ( current / target )
  "http_requests" on pods:      66m / 500m
Min replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions: <1>
  Type              Status    Reason              Message
  ----              ------    ------              -------
  AbleToScale       True      ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive     True      ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric http_request
  ScalingLimited    False     DesiredWithinRange  the desired replica count is within the acceptable range
Events:
----
<1> The horizontal pod autoscaler status messages.

// The above output and bullets from https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/#appendix-horizontal-pod-autoscaler-status-conditions

The following is an example of a pod that is unable to scale:

.Example output
[source,text]
----
Conditions:
  Type         Status  Reason          Message
  ----         ------  ------          -------
  AbleToScale  False   FailedGetScale  the HPA controller was unable to get the target's current scale: no matches for kind "ReplicationController" in group "apps"
Events:
  Type     Reason          Age               From                       Message
  ----     ------          ----              ----                       -------
  Warning  FailedGetScale  6s (x3 over 36s)  horizontal-pod-autoscaler  no matches for kind "ReplicationController" in group "apps"
----

The following is an example of a pod that could not obtain the needed metrics for scaling:

.Example output
[source,text]
----
Conditions:
  Type                  Status    Reason                    Message
  ----                  ------    ------                    -------
  AbleToScale           True     SucceededGetScale          the HPA controller was able to get the target's current scale
  ScalingActive         False    FailedGetResourceMetric    the HPA was unable to compute the replica count: failed to get cpu utilization: unable to get metrics for resource cpu: no metrics returned from resource metrics API
----

The following is an example of a pod where the requested autoscaling was less than the required minimums:

.Example output
[source,text]
----
Conditions:
  Type              Status    Reason              Message
  ----              ------    ------              -------
  AbleToScale       True      ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive     True      ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric http_request
  ScalingLimited    False     DesiredWithinRange  the desired replica count is within the acceptable range
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/nodes-pods-autoscaling-about.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-pods-autoscaling-status-viewing_{context}"]

= Viewing horizontal pod autoscaler status conditions by using the CLI

You can view the status conditions set on a pod by the horizontal pod autoscaler (HPA).

[NOTE]
====
The horizontal pod autoscaler status conditions are available with the `v2` version of the autoscaling API.
====

.Prerequisites

To use horizontal pod autoscalers, your cluster administrator must have properly configured cluster metrics.
You can use the `oc describe PodMetrics <pod-name>` command to determine if metrics are configured.  If metrics are
configured, the output appears similar to the following, with `Cpu` and `Memory` displayed under `Usage`.

[source,terminal]
----
$ oc describe PodMetrics openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
----

.Example output
[source,terminal]
----
Name:         openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Namespace:    openshift-kube-scheduler
Labels:       <none>
Annotations:  <none>
API Version:  metrics.k8s.io/v1beta1
Containers:
  Name:  wait-for-host-port
  Usage:
    Memory:  0
  Name:      scheduler
  Usage:
    Cpu:     8m
    Memory:  45440Ki
Kind:        PodMetrics
Metadata:
  Creation Timestamp:  2019-05-23T18:47:56Z
  Self Link:           /apis/metrics.k8s.io/v1beta1/namespaces/openshift-kube-scheduler/pods/openshift-kube-scheduler-ip-10-0-135-131.ec2.internal
Timestamp:             2019-05-23T18:47:56Z
Window:                1m0s
Events:                <none>
----

.Procedure

To view the status conditions on a pod, use the following command with the name of the pod:

[source,terminal]
----
$ oc describe hpa <pod-name>
----

For example:

[source,terminal]
----
$ oc describe hpa cm-test
----

The conditions appear in the `Conditions` field in the output.

.Example output
[source,terminal]
----
Name:                           cm-test
Namespace:                      prom
Labels:                         <none>
Annotations:                    <none>
CreationTimestamp:              Fri, 16 Jun 2017 18:09:22 +0000
Reference:                      ReplicationController/cm-test
Metrics:                        ( current / target )
  "http_requests" on pods:      66m / 500m
Min replicas:                   1
Max replicas:                   4
ReplicationController pods:     1 current / 1 desired
Conditions: <1>
  Type              Status    Reason              Message
  ----              ------    ------              -------
  AbleToScale       True      ReadyForNewScale    the last scale time was sufficiently old as to warrant a new scale
  ScalingActive     True      ValidMetricFound    the HPA was able to successfully calculate a replica count from pods metric http_request
  ScalingLimited    False     DesiredWithinRange  the desired replica count is within the acceptable range
----

:leveloffset!:


[role="_additional-resources"]
== Additional resources

* For more information on replication controllers and deployment controllers,
see xref:../../applications/deployments/what-deployments-are.adoc#what-deployments-are[Understanding deployments and deployment configs].

* For an example on the usage of HPA, see https://cloud.redhat.com/blog/horizontal-pod-autoscaling-of-quarkus-application-based-on-memory-utilization[Horizontal Pod Autoscaling of Quarkus Application Based on Memory Utilization].

//# includes=_attributes/common-attributes,modules/nodes-pods-autoscaling-about,modules/nodes-pods-autoscaling-workflow-hpa,modules/nodes-pods-autoscaling-requests-and-limits-hpa,modules/nodes-pods-autoscaling-best-practices-hpa,modules/nodes-pods-autoscaling-policies,modules/nodes-pods-autoscaling-creating-web-console,modules/nodes-pods-autoscaling-creating-cpu,modules/nodes-pods-autoscaling-creating-memory,modules/nodes-pods-autoscaling-status-about,modules/nodes-pods-autoscaling-status-viewing

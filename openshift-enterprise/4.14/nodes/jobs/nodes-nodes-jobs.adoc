:_mod-docs-content-type: ASSEMBLY
[id="nodes-nodes-jobs"]
= Running tasks in pods using jobs
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:context: nodes-nodes-jobs

toc::[]


A _job_ executes a task in your {product-title} cluster.

A job tracks the overall progress of a task and updates its status with information
about active, succeeded, and failed pods. Deleting a job will clean up any pod
replicas it created. Jobs are part of the Kubernetes API, which can be managed
with `oc` commands like other object types.

.Sample Job specification

[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  parallelism: 1    <1>
  completions: 1    <2>
  activeDeadlineSeconds: 1800 <3>
  backoffLimit: 6   <4>
  template:         <5>
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure    <6>
#...
----
<1> The pod replicas a job should run in parallel.
<2> Successful pod completions are needed to mark a job completed.
<3> The maximum duration the job can run.
<4> The number of retries for a job.
<5> The template for the pod the controller creates.
<6> The restart policy of the pod.

.Additional resources

* link:https://kubernetes.io/docs/concepts/workloads/controllers/job/[Jobs] in the Kubernetes documentation

// The following include statements pull in the module files that comprise
// the assembly. Include any combination of concept, procedure, or reference
// modules required to cover the user story. You can also include other
// assemblies.

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-nodes-jobs.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-nodes-jobs-about_{context}"]
= Understanding jobs and cron jobs

A job tracks the overall progress of a task and updates its status with information
about active, succeeded, and failed pods. Deleting a job cleans up any pods it created.
Jobs are part of the Kubernetes API, which can be managed
with `oc` commands like other object types.

There are two possible resource types that allow creating run-once objects in {product-title}:

Job::
A regular job is a run-once object that creates a task and ensures the job finishes.
+
There are three main types of task suitable to run as a job:
+
* Non-parallel jobs:
** A job that starts only one pod, unless the pod fails.
** The job is complete as soon as its pod terminates successfully.
+
* Parallel jobs with a fixed completion count:
** a job that starts multiple pods.
** The job represents the overall task and is complete when there is one successful pod for each value in the range `1` to the `completions` value.
+
* Parallel jobs with a work queue:
** A job with multiple parallel worker processes in a given pod.
** {product-title} coordinates pods to determine what each should work on or use an external queue service.
** Each pod is independently capable of determining whether or not all peer pods are complete and that the entire job is done.
** When any pod from the job terminates with success, no new pods are created.
** When at least one pod has terminated with success and all pods are terminated, the job is successfully completed.
** When any pod has exited with success, no other pod should be doing any work for this task or writing any output. Pods should all be in the process of exiting.
+
For more information about how to make use of the different types of job, see link:https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/#job-patterns[Job Patterns] in the Kubernetes documentation.

Cron job::

A job can be scheduled to run multiple times, using a cron job.
+
A _cron job_ builds on a regular job by allowing you to specify
how the job should be run. Cron jobs are part of the
link:http://kubernetes.io/docs/user-guide/cron-jobs[Kubernetes] API, which
can be managed with `oc` commands like other object types.
+
Cron jobs are useful for creating periodic and recurring tasks, like running backups or sending emails.
Cron jobs can also schedule individual tasks for a specific time, such as if you want to schedule a job for a low activity period. A cron job creates a `Job` object based on the timezone configured on the control plane node that runs the cronjob controller.
+
[WARNING]
====
A cron job creates a `Job` object approximately once per execution time of its
schedule, but there are circumstances in which it fails to create a job or
two jobs might be created. Therefore, jobs must be idempotent and you must
configure history limits.
====

[id="jobs-create_{context}"]
== Understanding how to create jobs

Both resource types require a job configuration that consists of the following key parts:

- A pod template, which describes the pod that {product-title} creates.
- The `parallelism` parameter, which specifies how many pods running in parallel at any point in time should execute a job.
** For non-parallel jobs, leave unset. When unset, defaults to `1`.
- The `completions` parameter, specifying how many successful pod completions are needed to finish a job.
** For non-parallel jobs, leave unset. When unset, defaults to `1`.
** For parallel jobs with a fixed completion count, specify a value.
** For parallel jobs with a work queue, leave unset. When unset defaults to the `parallelism` value.

[id="jobs-set-max_{context}"]
== Understanding how to set a maximum duration for jobs

When defining a job, you can define its maximum duration by setting
the `activeDeadlineSeconds` field. It is specified in seconds and is not
set by default. When not set, there is no maximum duration enforced.

The maximum duration is counted from the time when a first pod gets scheduled in
the system, and defines how long a job can be active. It tracks overall time of
an execution. After reaching the specified timeout, the job is terminated by {product-title}.

[id="jobs-set-backoff_{context}"]
== Understanding how to set a job back off policy for pod failure

A job can be considered failed, after a set amount of retries due to a
logical error in configuration or other similar reasons. Failed pods associated with the job are recreated by the controller with
an exponential back off delay (`10s`, `20s`, `40s` …) capped at six minutes. The
limit is reset if no new failed pods appear between controller checks.

Use the `spec.backoffLimit` parameter to set the number of retries for a job.

[id="jobs-artifacts_{context}"]
== Understanding how to configure a cron job to remove artifacts

Cron jobs can leave behind artifact resources such as jobs or pods.  As a user it is important
to configure history limits so that old jobs and their pods are properly cleaned.  There are two fields within cron job's spec responsible for that:

* `.spec.successfulJobsHistoryLimit`. The number of successful finished jobs to retain (defaults to 3).

* `.spec.failedJobsHistoryLimit`. The number of failed finished jobs to retain (defaults to 1).

[TIP]
====
* Delete cron jobs that you no longer need:
+
[source,terminal]
----
$ oc delete cronjob/<cron_job_name>
----
+
Doing this prevents them from generating unnecessary artifacts.

* You can suspend further executions by setting the `spec.suspend` to true.  All subsequent executions are suspended until you reset to `false`.
====

[id="jobs-limits_{context}"]
== Known limitations

The job specification restart policy only applies to the _pods_, and not the _job controller_. However, the job controller is hard-coded to keep retrying jobs to completion.

As such, `restartPolicy: Never` or `--restart=Never` results in the same behavior as `restartPolicy: OnFailure` or `--restart=OnFailure`. That is, when a job fails it is restarted automatically until it succeeds (or is manually discarded). The policy only sets which subsystem performs the restart.

With the `Never` policy, the _job controller_ performs the restart. With each attempt, the job controller increments the number of failures in the job status and create new pods. This means that with each failed attempt, the number of pods increases.

With the `OnFailure` policy, _kubelet_ performs the restart. Each attempt does not increment the number of failures in the job status. In addition, kubelet will retry failed jobs starting pods on the same nodes.

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-nodes-jobs.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-nodes-jobs-creating_{context}"]
= Creating jobs

You create a job in {product-title} by creating a job object.

.Procedure

To create a job:

. Create a YAML file similar to the following:
+
[source,yaml]
----
apiVersion: batch/v1
kind: Job
metadata:
  name: pi
spec:
  parallelism: 1    <1>
  completions: 1    <2>
  activeDeadlineSeconds: 1800 <3>
  backoffLimit: 6   <4>
  template:         <5>
    metadata:
      name: pi
    spec:
      containers:
      - name: pi
        image: perl
        command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
      restartPolicy: OnFailure    <6>
#...
----
<1> Optional: Specify how many pod replicas a job should run in parallel; defaults to `1`.
* For non-parallel jobs, leave unset. When unset, defaults to `1`.
<2> Optional: Specify how many successful pod completions are needed to mark a job completed.
* For non-parallel jobs, leave unset. When unset, defaults to `1`.
* For parallel jobs with a fixed completion count, specify the number of completions.
* For parallel jobs with a work queue, leave unset. When unset defaults to the `parallelism` value.
<3> Optional: Specify the maximum duration the job can run.
<4> Optional: Specify the number of retries for a job. This field defaults to six.
<5> Specify the template for the pod the controller creates.
<6> Specify the restart policy of the pod:
* `Never`. Do not restart the job.
* `OnFailure`. Restart the job only if it fails.
* `Always`. Always restart the job.
+
For details on how {product-title} uses restart policy with failed containers, see
the link:https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#example-states[Example States] in the Kubernetes documentation.

. Create the job:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----

[NOTE]
====
You can also create and launch a job from a single command using `oc create job`. The following command creates and launches a job similar to the one specified in the previous example:

[source,terminal]
----
$ oc create job pi --image=perl -- perl -Mbignum=bpi -wle 'print bpi(2000)'
----
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-nodes-jobs.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-nodes-jobs-creating-cron_{context}"]
= Creating cron jobs

You create a cron job in {product-title} by creating a job object.

.Procedure

To create a cron job:

. Create a YAML file similar to the following:
+
[source,yaml]
----
apiVersion: batch/v1
kind: CronJob
metadata:
  name: pi
spec:
  schedule: "*/1 * * * *"          <1>
  timeZone: Etc/UTC                <2>
  concurrencyPolicy: "Replace"     <3>
  startingDeadlineSeconds: 200     <4>
  suspend: true                    <5>
  successfulJobsHistoryLimit: 3    <6>
  failedJobsHistoryLimit: 1        <7>
  jobTemplate:                     <8>
    spec:
      template:
        metadata:
          labels:                  <9>
            parent: "cronjobpi"
        spec:
          containers:
          - name: pi
            image: perl
            command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]
          restartPolicy: OnFailure <10>
#...
----
+
<1> Schedule for the job specified in link:https://en.wikipedia.org/wiki/Cron[cron format]. In this example, the job will run every minute.
<2> An optional time zone for the schedule. See link:https://en.wikipedia.org/wiki/List_of_tz_database_time_zones[List of tz database time zones] for valid options. If not specified, the Kubernetes controller manager interprets the schedule relative to its local time zone. This setting is offered as a link:https://access.redhat.com/support/offerings/techpreview[Technology Preview].
<3> An optional concurrency policy, specifying how to treat concurrent jobs within a cron job. Only one of the following concurrent policies may be specified. If not specified, this defaults to allowing concurrent executions.
* `Allow` allows cron jobs to run concurrently.
* `Forbid` forbids concurrent runs, skipping the next run if the previous has not
finished yet.
* `Replace` cancels the currently running job and replaces
it with a new one.
<4> An optional deadline (in seconds) for starting the job if it misses its
scheduled time for any reason. Missed jobs executions will be counted as failed
ones. If not specified, there is no deadline.
<5> An optional flag allowing the suspension of a cron job. If set to `true`,
all subsequent executions will be suspended.
<6> The number of successful finished jobs to retain (defaults to 3).
<7> The number of failed finished jobs to retain (defaults to 1).
<8> Job template. This is similar to the job example.
<9> Sets a label for jobs spawned by this cron job.
<10> The restart policy of the pod. This does not apply to the job controller.

. Create the cron job:
+
[source,terminal]
----
$ oc create -f <file-name>.yaml
----

[NOTE]
====
You can also create and launch a cron job from a single command using `oc create cronjob`. The following command creates and launches a cron job similar to the one specified in the previous example:

[source,terminal]
----
$ oc create cronjob pi --image=perl --schedule='*/1 * * * *' -- perl -Mbignum=bpi -wle 'print bpi(2000)'
----

With `oc create cronjob`, the `--schedule` option accepts schedules in link:https://en.wikipedia.org/wiki/Cron[cron format].
====

:leveloffset!:

//# includes=_attributes/common-attributes,modules/nodes-nodes-jobs-about,modules/nodes-nodes-jobs-creating,modules/nodes-nodes-jobs-creating-cron

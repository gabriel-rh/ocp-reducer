:_mod-docs-content-type: ASSEMBLY
:context: post-install-storage-configuration
[id="post-install-storage-configuration"]
= Postinstallation storage configuration
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS
:gluster: GlusterFS
:gluster-native: Containerized GlusterFS
:gluster-external: External GlusterFS
:gluster-install-link: https://docs.gluster.org/en/latest/Install-Guide/Overview/
:gluster-admin-link: https://docs.gluster.org/en/latest/Administrator%20Guide/overview/
:gluster-role-link: https://github.com/openshift/openshift-ansible/tree/master/roles/openshift_storage_glusterfs

toc::[]

After installing {product-title}, you can further expand and customize your
cluster to your requirements, including storage configuration.

[id="post-install-dynamic-provisioning"]
== Dynamic provisioning

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc
// * microshift_storage/dynamic-provisioning-microshift.adoc

:_mod-docs-content-type: CONCEPT
[id="about_{context}"]
= About dynamic provisioning

The `StorageClass` resource object describes and classifies storage that can
be requested, as well as provides a means for passing parameters for
dynamically provisioned storage on demand. `StorageClass` objects can also
serve as a management mechanism for controlling different levels of
storage and access to the storage. Cluster Administrators (`cluster-admin`)
 or Storage Administrators (`storage-admin`) define and create the
`StorageClass` objects that users can request without needing any detailed
knowledge about the underlying storage volume sources.

The {product-title} persistent volume framework enables this functionality
and allows administrators to provision a cluster with persistent storage.
The framework also gives users a way to request those resources without
having any knowledge of the underlying infrastructure.

Many storage types are available for use as persistent volumes in
{product-title}. While all of them can be statically provisioned by an
administrator, some types of storage are created dynamically using the
built-in provider and plugin APIs.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="available-plug-ins_{context}"]
= Available dynamic provisioning plugins

{product-title} provides the following provisioner plugins, which have
generic implementations for dynamic provisioning that use the cluster's
configured provider's API to create new storage resources:


[options="header",cols="1,1,1"]
|===

|Storage type
|Provisioner plugin name
|Notes

|{rh-openstack-first} Cinder
|`kubernetes.io/cinder`
|

|{rh-openstack} Manila Container Storage Interface (CSI)
|`manila.csi.openstack.org`
|Once installed, the OpenStack Manila CSI Driver Operator and ManilaDriver automatically create the required storage classes for all available Manila share types needed for dynamic provisioning.

|Amazon Elastic Block Store (Amazon EBS)
|`kubernetes.io/aws-ebs`
|For dynamic provisioning when using multiple clusters in different zones,
tag each node with `Key=kubernetes.io/cluster/<cluster_name>,Value=<cluster_id>`
where `<cluster_name>` and `<cluster_id>` are unique per cluster.

|Azure Disk
|`kubernetes.io/azure-disk`
|

|Azure File
|`kubernetes.io/azure-file`
|The `persistent-volume-binder` service account requires permissions to create
and get secrets to store the Azure storage account and keys.

|GCE Persistent Disk (gcePD)
|`kubernetes.io/gce-pd`
|In multi-zone configurations, it is advisable to run one {product-title}
cluster per GCE project to avoid PVs from being created in zones where
no node in the current cluster exists.

|{ibmpowerProductName} Virtual Server Block
|`powervs.csi.ibm.com`
|After installation, the IBM Power Virtual Server Block CSI Driver Operator and IBM Power Virtual Server Block CSI Driver automatically create the required storage classes for dynamic provisioning.

//|GlusterFS
//|`kubernetes.io/glusterfs`
//|

//|Ceph RBD
//|`kubernetes.io/rbd`
//|

//|Trident from NetApp
//|`netapp.io/trident`
//|Storage orchestrator for NetApp ONTAP, SolidFire, and E-Series storage.

|link:https://www.vmware.com/support/vsphere.html[VMware vSphere]
|`kubernetes.io/vsphere-volume`
|

//|HPE Nimble Storage
//|`hpe.com/nimble`
//|Dynamic provisioning of HPE Nimble Storage resources using the
//HPE Nimble Kube Storage Controller.

|===

[IMPORTANT]
====
Any chosen provisioner plugin also requires configuration for the relevant
cloud, host, or third-party provider as per the relevant documentation.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc
// * microshift_storage/dynamic-provisioning-microshift.adoc


[id="defining-storage-classes_{context}"]
= Defining a storage class

`StorageClass` objects are currently a globally scoped object and must be
created by `cluster-admin` or `storage-admin` users.

[IMPORTANT]
====
The Cluster Storage Operator might install a default storage class depending
on the platform in use. This storage class is owned and controlled by the
Operator. It cannot be deleted or modified beyond defining annotations
and labels. If different behavior is desired, you must define a custom
storage class.
====

The following sections describe the basic definition for a
`StorageClass` object and specific examples for each of the supported plugin types.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc
// * microshift_storage/dynamic-provisioning-microshift.adoc


[id="basic-storage-class-definition_{context}"]
= Basic StorageClass object definition

The following resource shows the parameters and default values that you
use to configure a storage class. This example uses the AWS
ElasticBlockStore (EBS) object definition.

.Sample `StorageClass` definition
[source,yaml]
----
kind: StorageClass <1>
apiVersion: storage.k8s.io/v1 <2>
metadata:
  name: <storage-class-name> <3>
  annotations: <4>
    storageclass.kubernetes.io/is-default-class: 'true'
    ...
provisioner: kubernetes.io/aws-ebs <5>
parameters: <6>
  type: gp3
...
----
<1> (required) The API object type.
<2> (required) The current apiVersion.
<3> (required) The name of the storage class.
<4> (optional) Annotations for the storage class.
<5> (required) The type of provisioner associated with this storage class.
<6> (optional) The parameters required for the specific provisioner, this
will change from plugin to plug-iin.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc
// * microshift_storage/dynamic-provisioning-microshift.adoc


[id="storage-class-annotations_{context}"]
= Storage class annotations

To set a storage class as the cluster-wide default, add
the following annotation to your storage class metadata:

[source,yaml]
----
storageclass.kubernetes.io/is-default-class: "true"
----

For example:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    storageclass.kubernetes.io/is-default-class: "true"
...
----

This enables any persistent volume claim (PVC) that does not specify a
specific storage class to automatically be provisioned through the
default storage class. However, your cluster can have more than one storage class, but only one of them can be the default storage class.

[NOTE]
====
The beta annotation `storageclass.beta.kubernetes.io/is-default-class` is
still working; however, it will be removed in a future release.
====

To set a storage class description, add the following annotation
to your storage class metadata:

[source,yaml]
----
kubernetes.io/description: My Storage Class Description
----

For example:

[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  annotations:
    kubernetes.io/description: My Storage Class Description
...
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="openstack-cinder-storage-class_{context}"]
= {rh-openstack} Cinder object definition

.cinder-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: <storage-class-name> <1>
provisioner: kubernetes.io/cinder
parameters:
  type: fast  <2>
  availability: nova <3>
  fsType: ext4 <4>
----
<1> Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
<2> Volume type created in Cinder. Default is empty.
<3> Availability Zone. If not specified, volumes are generally
round-robined across all active zones where the {product-title} cluster
has a node.
<4> File system that is created on dynamically provisioned volumes. This
value is copied to the `fsType` field of dynamically provisioned
persistent volumes and the file system is created when the volume is
mounted for the first time. The default value is `ext4`.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="aws-definition_{context}"]
= AWS Elastic Block Store (EBS) object definition

.aws-ebs-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: <storage-class-name> <1>
provisioner: kubernetes.io/aws-ebs
parameters:
  type: io1 <2>
  iopsPerGB: "10" <3>
  encrypted: "true" <4>
  kmsKeyId: keyvalue <5>
  fsType: ext4 <6>
----
<1> (required) Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
<2> (required) Select from `io1`, `gp3`, `sc1`, `st1`. The default is `gp3`.
See the
link:http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html[AWS documentation]
for valid Amazon Resource Name (ARN) values.
<3> Optional: Only for *io1* volumes. I/O operations per second per GiB.
The AWS volume plugin multiplies this with the size of the requested
volume to compute IOPS of the volume. The value cap is 20,000 IOPS, which
is the maximum supported by AWS. See the
link:http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html[AWS documentation]
for further details.
<4> Optional: Denotes whether to encrypt the EBS volume. Valid values
are `true` or `false`.
<5> Optional: The full ARN of the key to use when encrypting the volume.
If none is supplied, but `encypted` is set to `true`, then AWS generates a
key. See the
link:http://docs.aws.amazon.com/general/latest/gr/aws-arns-and-namespaces.html[AWS documentation]
for a valid ARN value.
<6> Optional: File system that is created on dynamically provisioned
volumes. This value is copied to the `fsType` field of dynamically
provisioned persistent volumes and the file system is created when the
volume is mounted for the first time. The default value is `ext4`.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="azure-disk-definition_{context}"]
= Azure Disk object definition

.azure-advanced-disk-storageclass.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: <storage-class-name> <1>
provisioner: kubernetes.io/azure-disk
volumeBindingMode: WaitForFirstConsumer <2>
allowVolumeExpansion: true
parameters:
  kind: Managed <3>
  storageaccounttype: Premium_LRS <4>
reclaimPolicy: Delete
----
<1> Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
<2> Using `WaitForFirstConsumer` is strongly recommended. This provisions the volume while allowing enough storage to schedule the pod on a free worker node from an available zone.
<3> Possible values are `Shared` (default), `Managed`, and `Dedicated`.
+
[IMPORTANT]
====
Red Hat only supports the use of `kind: Managed` in the storage class.

With `Shared` and `Dedicated`, Azure creates unmanaged disks, while {product-title} creates a managed disk for machine OS (root) disks. But because Azure Disk does not allow the use of both managed and unmanaged disks on a node, unmanaged disks created with `Shared` or `Dedicated` cannot be attached to {product-title} nodes.
====

<4> Azure storage account SKU tier. Default is empty. Note that Premium VMs can attach both `Standard_LRS` and `Premium_LRS` disks, Standard VMs can only attach `Standard_LRS` disks, Managed VMs can only attach managed disks, and unmanaged VMs can only attach unmanaged disks.
+
.. If `kind` is set to `Shared`, Azure creates all unmanaged disks in a few shared storage accounts in the same resource group as the cluster.
.. If `kind` is set to `Managed`, Azure creates new managed disks.
.. If `kind` is set to `Dedicated` and a `storageAccount` is specified, Azure uses the specified storage account for the new unmanaged disk in the same resource group as the cluster. For this to work:
 * The specified storage account must be in the same region.
 * Azure Cloud Provider must have write access to the storage account.
.. If `kind` is set to `Dedicated` and a `storageAccount` is not specified, Azure creates a new dedicated storage account for the new unmanaged disk in the same resource group as the cluster.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc


:_mod-docs-content-type: PROCEDURE
[id="azure-file-definition_{context}"]
= Azure File object definition

The Azure File storage class uses secrets to store the Azure storage account name
and the storage account key that are required to create an Azure Files share. These
permissions are created as part of the following procedure.

.Procedure

. Define a `ClusterRole` object that allows access to create and view secrets:
+
[source,yaml]
----
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
#  name: system:azure-cloud-provider
  name: <persistent-volume-binder-role> <1>
rules:
- apiGroups: ['']
  resources: ['secrets']
  verbs:     ['get','create']
----
<1> The name of the cluster role to view and create secrets.

. Add the cluster role to the service account:
+
[source,terminal]
----
$ oc adm policy add-cluster-role-to-user <persistent-volume-binder-role> system:serviceaccount:kube-system:persistent-volume-binder
----

. Create the Azure File `StorageClass` object:
+
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: <azure-file> <1>
provisioner: kubernetes.io/azure-file
parameters:
  location: eastus <2>
  skuName: Standard_LRS <3>
  storageAccount: <storage-account> <4>
reclaimPolicy: Delete
volumeBindingMode: Immediate
----
<1> Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
<2> Location of the Azure storage account, such as `eastus`. Default is empty, meaning that a new Azure storage account will be created in the {product-title} cluster's location.
<3> SKU tier of the Azure storage account, such as `Standard_LRS`. Default is empty, meaning that a new Azure storage account will be created with the `Standard_LRS` SKU.
<4> Name of the Azure storage account. If a storage account is provided, then
`skuName` and `location` are ignored. If no storage account is provided, then
the storage class searches for any storage account that is associated with the
resource group for any accounts that match the defined `skuName` and `location`.

:leveloffset!:

:leveloffset: +3

// Module included in the following assemblies:
//
// storage/persistent_storage/persistent-storage-azure-file.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="azure-file-considerations_{context}"]
= Considerations when using Azure File

The following file system features are not supported by the default Azure File storage class:

* Symlinks
* Hard links
* Extended attributes
* Sparse files
* Named pipes

Additionally, the owner user identifier (UID) of the Azure File mounted directory is different from the process UID of the container. The `uid` mount option can be specified in the `StorageClass` object to define
a specific user identifier to use for the mounted directory.

The following `StorageClass` object demonstrates modifying the user and group identifier, along with enabling symlinks for the mounted directory.

[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: azure-file
mountOptions:
  - uid=1500 <1>
  - gid=1500 <2>
  - mfsymlinks <3>
provisioner: kubernetes.io/azure-file
parameters:
  location: eastus
  skuName: Standard_LRS
reclaimPolicy: Delete
volumeBindingMode: Immediate
----
<1> Specifies the user identifier to use for the mounted directory.
<2> Specifies the group identifier to use for the mounted directory.
<3> Enables symlinks.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="gce-persistentdisk-storage-class_{context}"]
= GCE PersistentDisk (gcePD) object definition

.gce-pd-storageclass.yaml
[source,yaml]
----
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: <storage-class-name> <1>
provisioner: kubernetes.io/gce-pd
parameters:
  type: pd-standard <2>
  replication-type: none
volumeBindingMode: WaitForFirstConsumer
allowVolumeExpansion: true
reclaimPolicy: Delete
----
<1> Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
<2> Select either `pd-standard` or `pd-ssd`. The default is `pd-standard`.

:leveloffset!:

// include::modules/dynamic-provisioning-gluster-definition.adoc[leveloffset=+2]

// include::modules/dynamic-provisioning-ceph-rbd-definition.adoc[leveloffset=+2]

:leveloffset: +2

// Module included in the following definitions:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc


[id="vsphere-definition_{context}"]
= VMware vSphere object definition

.vsphere-storageclass.yaml
[source,yaml]
----
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: <storage-class-name> <1>
provisioner: csi.vsphere.vmware.com <2>
----
<1> Name of the storage class. The persistent volume claim uses this storage class for provisioning the associated persistent volumes.
<2> For more information about using VMware vSphere CSI with {product-title},
see the
link:https://kubernetes.io/docs/concepts/storage/volumes/#vsphere-csi-migration[Kubernetes documentation].

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/dynamic-provisioning.adoc
// * post_installation_configuration/storage-configuration.adoc
// * microshift_storage/dynamic-provisioning-microshift.adoc


[id="change-default-storage-class_{context}"]
= Changing the default storage class

Use the following procedure to change the default storage class.

For example, if you have two defined storage classes, `gp3` and `standard`, and you want to change the default storage class from `gp3` to `standard`.

.Prerequisites

* Access to the cluster with cluster-admin privileges.

.Procedure

To change the default storage class:

. List the storage classes:
+
[source,terminal]
----
$ oc get storageclass
----
+
.Example output
[source,terminal]
----
NAME                 TYPE
gp3 (default)        kubernetes.io/aws-ebs <1>
standard             kubernetes.io/aws-ebs
----
<1> `(default)` indicates the default storage class.

. Make the desired storage class the default.
+
For the desired storage class, set the `storageclass.kubernetes.io/is-default-class` annotation to `true` by running the following command:
+
[source,terminal]
----
$ oc patch storageclass standard -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "true"}}}'
----
+
[NOTE]
====
You can have multiple default storage classes for a short time. However, you should ensure that only one default storage class exists eventually.

With multiple default storage classes present, any persistent volume claim (PVC) requesting the default storage class (`pvc.spec.storageClassName`=nil) gets the most recently created default storage class, regardless of the default status of that storage class, and the administrator receives an alert in the alerts dashboard that there are multiple default storage classes, `MultipleDefaultStorageClasses`.

// add xref to multi/no default SC module
====

. Remove the default storage class setting from the old default storage class.
+
For the old default storage class, change the value of the `storageclass.kubernetes.io/is-default-class` annotation to `false` by running the following command:
+
[source,terminal]
----
$ oc patch storageclass gp3 -p '{"metadata": {"annotations": {"storageclass.kubernetes.io/is-default-class": "false"}}}'
----

. Verify the changes:
+
[source,terminal]
----
$ oc get storageclass
----
+
.Example output
[source,terminal]
----
NAME                 TYPE
gp3                  kubernetes.io/aws-ebs
standard (default)   kubernetes.io/aws-ebs
----

:leveloffset!:

[id="post-install-optimizing-storage"]
== Optimizing storage

Optimizing storage helps to minimize storage use across all resources. By
optimizing storage, administrators help ensure that existing storage resources
are working in an efficient manner.

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/optimizing-storage.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="available-persistent-storage-options_{context}"]
= Available persistent storage options

Understand your persistent storage options so that you can optimize your
{product-title} environment.

.Available storage options
[cols="1,4,3",options="header"]
|===
| Storage type | Description | Examples

|Block
a|* Presented to the operating system (OS) as a block device
* Suitable for applications that need full control of storage and operate at a low level on files
bypassing the file system
* Also referred to as a Storage Area Network (SAN)
* Non-shareable, which means that only one client at a time can mount an endpoint of this type
| AWS EBS and VMware vSphere support dynamic persistent volume (PV) provisioning natively in {product-title}.
// Ceph RBD, OpenStack Cinder, Azure Disk, GCE persistent disk

|File
a| * Presented to the OS as a file system export to be mounted
* Also referred to as Network Attached Storage (NAS)
* Concurrency, latency, file locking mechanisms, and other capabilities vary widely between protocols, implementations, vendors, and scales.
|RHEL NFS, NetApp NFS ^[1]^, and Vendor NFS
// Azure File, AWS EFS

| Object
a| * Accessible through a REST API endpoint
* Configurable for use in the {product-registry}
* Applications must build their drivers into the application and/or container.
| AWS S3
// Aliyun OSS, Ceph Object Storage (RADOS Gateway)
// Google Cloud Storage, Azure Blob Storage, OpenStack Swift
|===
[.small]
--
1. NetApp NFS supports dynamic PV provisioning when using the Trident plugin.
--

[IMPORTANT]
====
Currently, CNS is not supported in {product-title} {product-version}.
====

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * storage/optimizing-storage.adoc
// * post_installation_configuration/storage-configuration.adoc

[id="recommended-configurable-storage-technology_{context}"]
= Recommended configurable storage technology

The following table summarizes the recommended and configurable storage technologies for the given {product-title} cluster application.

.Recommended and configurable storage technology
[options="header,footer"]
|===
|Storage type|Block|File|Object

| ROX^1^
| Yes^4^
| Yes^4^
| Yes

| RWX^2^
| No
| Yes
| Yes

| Registry
| Configurable
| Configurable
| Recommended

| Scaled registry
| Not configurable
| Configurable
| Recommended

| Metrics^3^
| Recommended
| Configurable^5^
| Not configurable

| Elasticsearch Logging
| Recommended
| Configurable^6^
| Not supported^6^

| Loki Logging
| Configurable
| Not configurable
| Recommended

| Apps
| Recommended
| Recommended
| Not configurable^7^

4+a|
^1^ `ReadOnlyMany`

^2^ `ReadWriteMany`

^3^ Prometheus is the underlying technology used for metrics.

^4^ This does not apply to physical disk, VM physical disk, VMDK, loopback over NFS, AWS EBS, and Azure Disk.

^5^ For metrics, using file storage with the `ReadWriteMany` (RWX) access mode is unreliable. If you use file storage, do not configure the RWX access mode on any persistent volume claims (PVCs) that are configured for use with metrics.

^6^ For logging, review the recommended storage solution in Configuring persistent storage for the log store section. Using NFS storage as a persistent volume or through NAS, such as Gluster, can corrupt the data. Hence, NFS is not supported for Elasticsearch storage and LokiStack log store in {product-title} Logging. You must use one persistent volume type per log store.

^7^ Object storage is not consumed through {product-title}'s PVs or PVCs. Apps must integrate with the object storage REST API.

|===

[NOTE]
====
A scaled registry is an {product-registry} where two or more pod replicas are running.
====

== Specific application storage recommendations

[IMPORTANT]
====
Testing shows issues with using the NFS server on {op-system-base-full} as storage backend for core services. This includes the OpenShift Container Registry and Quay, Prometheus for monitoring storage, and Elasticsearch for logging storage. Therefore, using {op-system-base} NFS to back PVs used by core services is not recommended.

Other NFS implementations on the marketplace might not have these issues. Contact the individual NFS implementation vendor for more information on any testing that was possibly completed against these {product-title} core components.
====

=== Registry

In a non-scaled/high-availability (HA) {product-registry} cluster deployment:

* The storage technology does not have to support RWX access mode.
* The storage technology must ensure read-after-write consistency.
* The preferred storage technology is object storage followed by block storage.
* File storage is not recommended for {product-registry} cluster deployment with production workloads.

=== Scaled registry

In a scaled/HA {product-registry} cluster deployment:

* The storage technology must support RWX access mode.
* The storage technology must ensure read-after-write consistency.
* The preferred storage technology is object storage.
* Red Hat OpenShift Data Foundation (ODF), Amazon Simple Storage Service (Amazon S3), Google Cloud Storage (GCS), Microsoft Azure Blob Storage, and OpenStack Swift are supported.
* Object storage should be S3 or Swift compliant.
* For non-cloud platforms, such as vSphere and bare metal installations, the only configurable technology is file storage.
* Block storage is not configurable.

=== Metrics

In an {product-title} hosted metrics cluster deployment:

* The preferred storage technology is block storage.
* Object storage is not configurable.

[IMPORTANT]
====
It is not recommended to use file storage for a hosted metrics cluster deployment with production workloads.
====

=== Logging

In an {product-title} hosted logging cluster deployment:

* The preferred storage technology is block storage.
* Object storage is not configurable.

=== Applications

Application use cases vary from application to application, as described in the following examples:

* Storage technologies that support dynamic PV provisioning have low mount time latencies, and are not tied to nodes to support a healthy cluster.
* Application developers are responsible for knowing and understanding the storage requirements for their application, and how it works with the provided storage to ensure that issues do not occur when an application scales or interacts with the storage layer.

== Other specific application storage recommendations

[IMPORTANT]
====
It is not recommended to use RAID configurations on `Write` intensive workloads, such as `etcd`. If you are running `etcd` with a RAID configuration, you might be at risk of encountering performance issues with your workloads.
====

* {rh-openstack-first} Cinder: {rh-openstack} Cinder tends to be adept in ROX access mode use cases.
* Databases: Databases (RDBMSs, NoSQL DBs, etc.) tend to perform best with dedicated block storage.
* The etcd database must have enough storage and adequate performance capacity to enable a large cluster. Information about monitoring and benchmarking tools to establish ample storage and a high-performance environment is described in _Recommended etcd practices_.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.adoc#recommended-etcd-practices[Recommended etcd practices]


[id="post-install-deploy-OCS"]
== Deploy Red Hat OpenShift Data Foundation
// This section is sourced from storage/persistent_storage/persistent-storage-ocs.adoc
{rh-storage-first} is a provider of agnostic persistent storage for {product-title} supporting file, block, and object storage, either in-house or in hybrid clouds. As a Red Hat storage solution, {rh-storage-first} is completely integrated with {product-title} for deployment, management, and monitoring.

:leveloffset: +1

// Module included in the following assemblies:
//
// * post_installation_configuration/storage-configuration.adoc

[options="header",cols="1,1"]
|===

|If you are looking for {rh-storage-first} information about...
|See the following {rh-storage-first} documentation:

|What's new, known issues, notable bug fixes, and Technology Previews
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/4.12_release_notes[OpenShift Data Foundation 4.12 Release Notes]

|Supported workloads, layouts, hardware and software requirements, sizing and scaling recommendations
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/planning_your_deployment[Planning your OpenShift Data Foundation 4.12 deployment]

|Instructions on deploying {rh-storage} to use an external Red Hat Ceph Storage cluster
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_in_external_mode[Deploying OpenShift Data Foundation 4.12 in external mode]

|Instructions on deploying {rh-storage} to local storage on bare metal infrastructure
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure[Deploying OpenShift Data Foundation 4.12 using bare metal infrastructure]

|Instructions on deploying {rh-storage} on Red Hat {product-title} VMware vSphere clusters
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_on_vmware_vsphere[Deploying OpenShift Data Foundation 4.12 on VMware vSphere]

|Instructions on deploying {rh-storage} using Amazon Web Services for local or cloud storage
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_amazon_web_services[Deploying OpenShift Data Foundation 4.12 using Amazon Web Services]

|Instructions on deploying and managing {rh-storage} on existing Red Hat {product-title} Google Cloud clusters
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_and_managing_openshift_data_foundation_using_google_cloud[Deploying and managing OpenShift Data Foundation 4.12 using Google Cloud]

|Instructions on deploying and managing {rh-storage} on existing Red Hat {product-title} Azure clusters
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_microsoft_azure/index[Deploying and managing OpenShift Data Foundation 4.12 using Microsoft Azure]

|Instructions on deploying {rh-storage} to use local storage on IBM Power infrastructure
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html-single/deploying_openshift_data_foundation_using_ibm_power/index[Deploying OpenShift Data Foundation on IBM Power]

|Instructions on deploying {rh-storage} to use local storage on IBM Z infrastructure
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/deploying_openshift_data_foundation_using_ibm_z_infrastructure/index[Deploying OpenShift Data Foundation on IBM Z infrastructure]

|Allocating storage to core services and hosted applications in {rh-storage-first}, including snapshot and clone
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_and_allocating_storage_resources[Managing and allocating resources]

|Managing storage resources across a hybrid cloud or multicloud environment using the Multicloud Object Gateway (NooBaa)
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/managing_hybrid_and_multicloud_resources[Managing hybrid and multicloud resources]

|Safely replacing storage devices for {rh-storage-first}
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/replacing_devices[Replacing devices]

|Safely replacing a node in a {rh-storage-first} cluster
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/replacing_nodes[Replacing nodes]

|Scaling operations in {rh-storage-first}
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/scaling_storage[Scaling storage]

|Monitoring a {rh-storage-first} 4.12 cluster
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/monitoring_openshift_data_foundation[Monitoring Red Hat OpenShift Data Foundation 4.12]

|Resolve issues encountered during operations
|link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12/html/troubleshooting_openshift_data_foundation[Troubleshooting OpenShift Data Foundation 4.12]

|Migrating your {product-title} cluster from version 3 to version 4
|link:https://access.redhat.com/documentation/en-us/openshift_container_platform/4.12/html/migrating_from_version_3_to_4/index[Migration]

|===

:leveloffset!:

[role="_additional-resources"]
[id="admission-plug-ins-additional-resources"]
== Additional resources

* xref:../logging/config/cluster-logging-log-store.adoc#cluster-logging-elasticsearch-storage_cluster-logging-log-store[Configuring persistent storage for the log store]

//# includes=_attributes/common-attributes,modules/dynamic-provisioning-about,modules/dynamic-provisioning-available-plugins,modules/dynamic-provisioning-defining-storage-class,modules/dynamic-provisioning-storage-class-definition,modules/dynamic-provisioning-annotations,modules/dynamic-provisioning-cinder-definition,modules/dynamic-provisioning-aws-definition,modules/dynamic-provisioning-azure-disk-definition,modules/dynamic-provisioning-azure-file-definition,modules/dynamic-provisioning-azure-file-considerations,modules/dynamic-provisioning-gce-definition,modules/dynamic-provisioning-vsphere-definition,modules/dynamic-provisioning-change-default-class,modules/available-persistent-storage-options,modules/recommended-configurable-storage-technology,modules/deploy-red-hat-openshift-container-storage

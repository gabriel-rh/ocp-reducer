:_mod-docs-content-type: ASSEMBLY
:context: post-install-cluster-tasks
[id="post-install-cluster-tasks"]
= Postinstallation cluster tasks
// The {product-title} attribute provides the context-sensitive name of the relevant OpenShift distribution, for example, "OpenShift Container Platform" or "OKD". The {product-version} attribute provides the product version relative to the distribution, for example "4.9".
// {product-title} and {product-version} are parsed when AsciiBinder queries the _distro_map.yml file in relation to the base branch of a pull request.
// See https://github.com/openshift/openshift-docs/blob/main/contributing_to_docs/doc_guidelines.adoc#product-name-and-version for more information on this topic.
// Other common attributes are defined in the following lines:
:data-uri:
:icons:
:experimental:
:toc: macro
:toc-title:
:imagesdir: images
:prewrap!:
:op-system-first: Red Hat Enterprise Linux CoreOS (RHCOS)
:op-system: RHCOS
:op-system-lowercase: rhcos
:op-system-base: RHEL
:op-system-base-full: Red Hat Enterprise Linux (RHEL)
:op-system-version: 8.x
:tsb-name: Template Service Broker
:kebab: image:kebab.png[title="Options menu"]
:rh-openstack-first: Red Hat OpenStack Platform (RHOSP)
:rh-openstack: RHOSP
:ai-full: Assisted Installer
:ai-version: 2.3
:cluster-manager-first: Red Hat OpenShift Cluster Manager
:cluster-manager: OpenShift Cluster Manager
:cluster-manager-url: link:https://console.redhat.com/openshift[OpenShift Cluster Manager Hybrid Cloud Console]
:cluster-manager-url-pull: link:https://console.redhat.com/openshift/install/pull-secret[pull secret from the Red Hat OpenShift Cluster Manager]
:insights-advisor-url: link:https://console.redhat.com/openshift/insights/advisor/[Insights Advisor]
:hybrid-console: Red Hat Hybrid Cloud Console
:hybrid-console-second: Hybrid Cloud Console
:oadp-first: OpenShift API for Data Protection (OADP)
:oadp-full: OpenShift API for Data Protection
:oc-first: pass:quotes[OpenShift CLI (`oc`)]
:product-registry: OpenShift image registry
:rh-storage-first: Red Hat OpenShift Data Foundation
:rh-storage: OpenShift Data Foundation
:rh-rhacm-first: Red Hat Advanced Cluster Management (RHACM)
:rh-rhacm: RHACM
:rh-rhacm-version: 2.8
:sandboxed-containers-first: OpenShift sandboxed containers
:sandboxed-containers-operator: OpenShift sandboxed containers Operator
:sandboxed-containers-version: 1.3
:sandboxed-containers-version-z: 1.3.3
:sandboxed-containers-legacy-version: 1.3.2
:cert-manager-operator: cert-manager Operator for Red Hat OpenShift
:secondary-scheduler-operator-full: Secondary Scheduler Operator for Red Hat OpenShift
:secondary-scheduler-operator: Secondary Scheduler Operator
// Backup and restore
:velero-domain: velero.io
:velero-version: 1.11
:launch: image:app-launcher.png[title="Application Launcher"]
:mtc-short: MTC
:mtc-full: Migration Toolkit for Containers
:mtc-version: 1.8
:mtc-version-z: 1.8.0
// builds (Valid only in 4.11 and later)
:builds-v2title: Builds for Red Hat OpenShift
:builds-v2shortname: OpenShift Builds v2
:builds-v1shortname: OpenShift Builds v1
//gitops
:gitops-title: Red Hat OpenShift GitOps
:gitops-shortname: GitOps
:gitops-ver: 1.1
:rh-app-icon: image:red-hat-applications-menu-icon.jpg[title="Red Hat applications"]
//pipelines
:pipelines-title: Red Hat OpenShift Pipelines
:pipelines-shortname: OpenShift Pipelines
:pipelines-ver: pipelines-1.12
:pipelines-version-number: 1.12
:tekton-chains: Tekton Chains
:tekton-hub: Tekton Hub
:artifact-hub: Artifact Hub
:pac: Pipelines as Code
//odo
:odo-title: odo
//OpenShift Kubernetes Engine
:oke: OpenShift Kubernetes Engine
//OpenShift Platform Plus
:opp: OpenShift Platform Plus
//openshift virtualization (cnv)
:VirtProductName: OpenShift Virtualization
:VirtVersion: 4.14
:KubeVirtVersion: v0.59.0
:HCOVersion: 4.14.0
:CNVNamespace: openshift-cnv
:CNVOperatorDisplayName: OpenShift Virtualization Operator
:CNVSubscriptionSpecSource: redhat-operators
:CNVSubscriptionSpecName: kubevirt-hyperconverged
:delete: image:delete.png[title="Delete"]
//distributed tracing
:DTProductName: Red Hat OpenShift distributed tracing platform
:DTShortName: distributed tracing platform
:DTProductVersion: 2.9
:JaegerName: Red Hat OpenShift distributed tracing platform (Jaeger)
:JaegerShortName: distributed tracing platform (Jaeger)
:JaegerVersion: 1.47.0
:OTELName: Red Hat OpenShift distributed tracing data collection
:OTELShortName: distributed tracing data collection
:OTELOperator: Red Hat OpenShift distributed tracing data collection Operator
:OTELVersion: 0.81.0
:TempoName: Red Hat OpenShift distributed tracing platform (Tempo)
:TempoShortName: distributed tracing platform (Tempo)
:TempoOperator: Tempo Operator
:TempoVersion: 2.1.1
//logging
:logging-title: logging subsystem for Red Hat OpenShift
:logging-title-uc: Logging subsystem for Red Hat OpenShift
:logging: logging subsystem
:logging-uc: Logging subsystem
//serverless
:ServerlessProductName: OpenShift Serverless
:ServerlessProductShortName: Serverless
:ServerlessOperatorName: OpenShift Serverless Operator
:FunctionsProductName: OpenShift Serverless Functions
//service mesh v2
:product-dedicated: Red Hat OpenShift Dedicated
:product-rosa: Red Hat OpenShift Service on AWS
:SMProductName: Red Hat OpenShift Service Mesh
:SMProductShortName: Service Mesh
:SMProductVersion: 2.4.4
:MaistraVersion: 2.4
//Service Mesh v1
:SMProductVersion1x: 1.1.18.2
//Windows containers
:productwinc: Red Hat OpenShift support for Windows Containers
// Red Hat Quay Container Security Operator
:rhq-cso: Red Hat Quay Container Security Operator
// Red Hat Quay
:quay: Red Hat Quay
:sno: single-node OpenShift
:sno-caps: Single-node OpenShift
//TALO and Redfish events Operators
:cgu-operator-first: Topology Aware Lifecycle Manager (TALM)
:cgu-operator-full: Topology Aware Lifecycle Manager
:cgu-operator: TALM
:redfish-operator: Bare Metal Event Relay
//Formerly known as CodeReady Containers and CodeReady Workspaces
:openshift-local-productname: Red Hat OpenShift Local
:openshift-dev-spaces-productname: Red Hat OpenShift Dev Spaces
// Factory-precaching-cli tool
:factory-prestaging-tool: factory-precaching-cli tool
:factory-prestaging-tool-caps: Factory-precaching-cli tool
:openshift-networking: Red Hat OpenShift Networking
// TODO - this probably needs to be different for OKD
//ifdef::openshift-origin[]
//:openshift-networking: OKD Networking
//endif::[]
// logical volume manager storage
:lvms-first: Logical volume manager storage (LVM Storage)
:lvms: LVM Storage
//Operator SDK version
:osdk_ver: 1.31.0
//Operator SDK version that shipped with the previous OCP 4.x release
:osdk_ver_n1: 1.28.0
//Next-gen (OCP 4.14+) Operator Lifecycle Manager, aka "v1"
:olmv1: OLM 1.0
:olmv1-first: Operator Lifecycle Manager (OLM) 1.0
:ztp-first: GitOps Zero Touch Provisioning (ZTP)
:ztp: GitOps ZTP
:3no: three-node OpenShift
:3no-caps: Three-node OpenShift
:run-once-operator: Run Once Duration Override Operator
// Web terminal
:web-terminal-op: Web Terminal Operator
:devworkspace-op: DevWorkspace Operator
:secrets-store-driver: Secrets Store CSI driver
:secrets-store-operator: Secrets Store CSI Driver Operator
//AWS STS
:sts-first: Security Token Service (STS)
:sts-full: Security Token Service
:sts-short: STS
//Cloud provider names
//AWS
:aws-first: Amazon Web Services (AWS)
:aws-full: Amazon Web Services
:aws-short: AWS
//GCP
:gcp-first: Google Cloud Platform (GCP)
:gcp-full: Google Cloud Platform
:gcp-short: GCP
//alibaba cloud
:alibaba: Alibaba Cloud
// IBM Cloud VPC
:ibmcloudVPCProductName: IBM Cloud VPC
:ibmcloudVPCRegProductName: IBM(R) Cloud VPC
// IBM Cloud
:ibm-cloud-bm: IBM Cloud Bare Metal (Classic)
:ibm-cloud-bm-reg: IBM Cloud(R) Bare Metal (Classic)
// IBM Power
:ibmpowerProductName: IBM Power
:ibmpowerRegProductName: IBM(R) Power
// IBM zSystems
:ibmzProductName: IBM Z
:ibmzRegProductName: IBM(R) Z
:linuxoneProductName: IBM(R) LinuxONE
//Azure
:azure-full: Microsoft Azure
:azure-short: Azure
//vSphere
:vmw-full: VMware vSphere
:vmw-short: vSphere
//Oracle
:oci-first: Oracle(R) Cloud Infrastructure
:oci: OCI
:ocvs-first: Oracle(R) Cloud VMware Solution (OCVS)
:ocvs: OCVS

toc::[]

After installing {product-title}, you can further expand and customize your cluster to your requirements.

[id="available_cluster_customizations"]
== Available cluster customizations

You complete most of the cluster configuration and customization after you deploy your {product-title} cluster. A number of _configuration resources_ are available.

[NOTE]
====
If you install your cluster on {ibmzProductName}, not all features and functions are available.
====

You modify the configuration resources to configure the major features of the
cluster, such as the image registry, networking configuration, image build
behavior, and the identity provider.

For current documentation of the settings that you control by using these resources, use
the `oc explain` command, for example `oc explain builds --api-version=config.openshift.io/v1`

[id="configuration-resources_{context}"]
=== Cluster configuration resources

All cluster configuration resources are globally scoped (not namespaced) and named `cluster`.
////
Config changes should not require coordinated changes between config resources, if you find
yourself struggling to update these docs to explain coordinated changes, please reach out
to @api-approvers (github) or #forum-api-review (slack).
////

[cols="2a,8a",options="header"]
|===
|Resource name
|Description

|`apiserver.config.openshift.io`
|Provides API server configuration such as xref:../security/certificates/api-server.adoc#api-server-certificates[certificates and certificate authorities].

|`authentication.config.openshift.io`
|Controls the xref:../authentication/understanding-identity-provider.adoc#understanding-identity-provider[identity provider] and authentication configuration for the cluster.

|`build.config.openshift.io`
|Controls default and enforced xref:../cicd/builds/build-configuration.adoc#build-configuration[configuration] for all builds on the cluster.

|`console.config.openshift.io`
|Configures the behavior of the web console interface, including the xref:../web_console/configuring-web-console.adoc#configuring-web-console[logout behavior].

|`featuregate.config.openshift.io`
|Enables xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling[FeatureGates]
so that you can use Tech Preview features.

|`image.config.openshift.io`
|Configures how specific xref:../openshift_images/image-configuration.adoc#image-configuration[image registries] should be treated (allowed, disallowed, insecure, CA details).

|`ingress.config.openshift.io`
|Configuration details related to xref:../networking/ingress-operator.adoc#nw-installation-ingress-config-asset_configuring-ingress[routing] such as the default domain for routes.

|`oauth.config.openshift.io`
|Configures identity providers and other behavior related to xref:../authentication/configuring-internal-oauth.adoc#configuring-internal-oauth[internal OAuth server] flows.

|`project.config.openshift.io`
|Configures xref:../applications/projects/configuring-project-creation.adoc#configuring-project-creation[how projects are created] including the project template.

|`proxy.config.openshift.io`
|Defines proxies to be used by components needing external network access.  Note: not all components currently consume this value.

|`scheduler.config.openshift.io`
|Configures xref:../nodes/scheduling/nodes-scheduler-profiles.adoc#nodes-scheduler-profiles[scheduler] behavior such as profiles and default node selectors.

|===

[id="operator-configuration-resources_{context}"]
=== Operator configuration resources

These configuration resources are cluster-scoped instances, named `cluster`, which control the behavior of a specific component as
owned by a particular Operator.

[cols="2a,8a",options="header"]
|===
|Resource name
|Description

|`consoles.operator.openshift.io`
|Controls console appearance such as branding customizations

|`config.imageregistry.operator.openshift.io`
|Configures xref:../registry/configuring-registry-operator.adoc#registry-operator-configuration-resource-overview_configuring-registry-operator[{product-registry} settings] such as public routing, log levels, proxy settings, resource constraints, replica counts, and storage type.

|`config.samples.operator.openshift.io`
|Configures the
xref:../openshift_images/configuring-samples-operator.adoc#configuring-samples-operator[Samples Operator]
to control which example image streams and templates are installed on the cluster.

|===


[id="additional-configuration-resources_{context}"]
=== Additional configuration resources

These configuration resources represent a single instance of a particular component. In some cases, you can request multiple
instances by creating multiple instances of the resource. In other cases, the Operator can use only a specific
resource instance name in a specific namespace. Reference the component-specific
documentation for details on how and when you can create additional resource instances.

[cols="2a,2a,2a,8a",options="header"]
|===
|Resource name
|Instance name
|Namespace
|Description

|`alertmanager.monitoring.coreos.com`
|`main`
|`openshift-monitoring`
|Controls the xref:../monitoring/managing-alerts.adoc#managing-alerts[Alertmanager] deployment parameters.

|`ingresscontroller.operator.openshift.io`
|`default`
|`openshift-ingress-operator`
|Configures xref:../networking/ingress-operator.adoc#configuring-ingress[Ingress Operator] behavior such as domain, number of replicas, certificates, and controller placement.

|===


[id="informational-resources_{context}"]
=== Informational Resources

You use these resources to retrieve information about the cluster. Some configurations might require you to edit these resources directly.

[cols="2a,2a,8a",options="header"]
|===
|Resource name|Instance name|Description

|`clusterversion.config.openshift.io`
|`version`
|In {product-title} {product-version}, you must not customize the `ClusterVersion`
resource for production clusters. Instead, follow the process to
xref:../updating/updating_a_cluster/updating-cluster-web-console.adoc#updating-cluster-web-console[update a cluster].

|`dns.config.openshift.io`
|`cluster`
|You cannot modify the DNS settings for your cluster. You can
xref:../networking/dns-operator.adoc#dns-operator[view the DNS Operator status].

|`infrastructure.config.openshift.io`
|`cluster`
|Configuration details allowing the cluster to interact with its cloud provider.

|`network.config.openshift.io`
|`cluster`
|You cannot modify your cluster networking after installation. To customize your network, follow the process to
xref:../installing/installing_aws/installing-aws-network-customizations.adoc#installing-aws-network-customizations[customize networking during installation].

|===

:leveloffset: +1

// Module included in the following assemblies:
// * openshift_images/managing_images/using-image-pull-secrets.adoc
// * post_installation_configuration/cluster-tasks.adoc
// * updating/updating_a_cluster/updating_disconnected_cluster/disconnected-update-osus.adoc
// * support/remote_health_monitoring/opting-out-of-remote-health-reporting.adoc
//
// Not included, but linked to from:
// * operators/admin/olm-managing-custom-catalogs.adoc


:_mod-docs-content-type: PROCEDURE
[id="images-update-global-pull-secret_{context}"]
= Updating the global cluster pull secret

You can update the global pull secret for your cluster by either replacing the current pull secret or appending a new pull secret.

The procedure is required when users use a separate registry to store images than the registry used during installation.


.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure
. Optional: To append a new pull secret to the existing pull secret, complete the following steps:

.. Enter the following command to download the pull secret:
+
[source,terminal]
----
$ oc get secret/pull-secret -n openshift-config --template='{{index .data ".dockerconfigjson" | base64decode}}' ><pull_secret_location> <1>
----
<1> Provide the path to the pull secret file.

.. Enter the following command to add the new pull secret:
+
[source,terminal]
----
$ oc registry login --registry="<registry>" \ <1>
--auth-basic="<username>:<password>" \ <2>
--to=<pull_secret_location> <3>
----
<1> Provide the new registry. You can include multiple repositories within the same registry, for example: `--registry="<registry/my-namespace/my-repository>"`.
<2> Provide the credentials of the new registry.
<3> Provide the path to the pull secret file.
+
Alternatively, you can perform a manual update to the pull secret file.

. Enter the following command to update the global pull secret for your cluster:
+
[source,terminal]
----
$ oc set data secret/pull-secret -n openshift-config --from-file=.dockerconfigjson=<pull_secret_location> <1>
----
<1> Provide the path to the new pull secret file.
+
This update is rolled out to all nodes, which can take some time depending on the size of your cluster.
+
[NOTE]
====
As of {product-title} 4.7.4, changes to the global pull secret no longer trigger a node drain or reboot.
====
//Also referred to as the cluster-wide pull secret.



:leveloffset!:

[id="adding-worker-nodes_{context}"]
== Adding worker nodes

After you deploy your {product-title} cluster, you can add worker nodes to scale cluster resources. There are different ways you can add worker nodes depending on the installation method and the environment of your cluster.

=== Adding worker nodes to installer-provisioned infrastructure clusters

For installer-provisioned infrastructure clusters, you can manually or automatically scale the `MachineSet` object to match the number of available bare-metal hosts.

To add a bare-metal host, you must configure all network prerequisites, configure an associated `baremetalhost` object, then provision the worker node to the cluster. You can add a bare-metal host manually or by using the web console.

* xref:../scalability_and_performance/managing-bare-metal-hosts.adoc#adding-bare-metal-host-to-cluster-using-web-console_managing-bare-metal-hosts[Adding worker nodes using the web console]

* xref:../scalability_and_performance/managing-bare-metal-hosts.adoc#adding-bare-metal-host-to-cluster-using-yaml_managing-bare-metal-hosts[Adding worker nodes using YAML in the web console]

* xref:../installing/installing_bare_metal_ipi/ipi-install-expanding-the-cluster.adoc#preparing-the-bare-metal-node_ipi-install-expanding[Manually adding a worker node to an installer-provisioned infrastructure cluster]

=== Adding worker nodes to user-provisioned infrastructure clusters

For user-provisioned infrastructure clusters, you can add worker nodes by using a {op-system-base} or {op-system} ISO image and connecting it to your cluster using cluster Ignition config files. For RHEL worker nodes, the following example uses Ansible playbooks to add worker nodes to the cluster. For RHCOS worker nodes, the following example uses an ISO image and network booting to add worker nodes to the cluster.

* xref:../post_installation_configuration/node-tasks.adoc#post-install-config-adding-fcos-compute[Adding RHCOS worker nodes to a user-provisioned infrastructure cluster]

* xref:../post_installation_configuration/node-tasks.adoc#post-install-config-adding-rhel-compute[Adding RHEL worker nodes to a user-provisioned infrastructure cluster]

=== Adding worker nodes to clusters managed by the Assisted Installer

For clusters managed by the Assisted Installer, you can add worker nodes by using the {cluster-manager-first} console, the Assisted Installer REST API or you can manually add worker nodes using an ISO image and cluster Ignition config files.

* xref:../nodes/nodes/nodes-sno-worker-nodes.adoc#sno-adding-worker-nodes-to-sno-clusters_add-workers[Adding worker nodes using the OpenShift Cluster Manager]

* xref:../nodes/nodes/nodes-sno-worker-nodes.adoc#adding-worker-nodes-using-the-assisted-installer-api[Adding worker nodes using the Assisted Installer REST API]

* xref:../nodes/nodes/nodes-sno-worker-nodes.adoc#sno-adding-worker-nodes-to-single-node-clusters-manually_add-workers[Manually adding worker nodes to a SNO cluster]

=== Adding worker nodes to clusters managed by the multicluster engine for Kubernetes

For clusters managed by the multicluster engine for Kubernetes, you can add worker nodes by using the dedicated multicluster engine console.

* link:https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.8/html/clusters/cluster_mce_overview#scale-hosts-infrastructure-env[Scaling hosts to an infrastructure environment]

////
[id="default-crds_{context}"]
== Custom resources

A number of Custom Resource Definitions (CRDs) are available for you to use to
further tune your {product-title} deployment. You can deploy Custom Resources
that are based on many of these CRDs to add more functionality to your
{product-title} cluster.

.Default CRDs
[cols="2a,2a,8a,2a,2a",options="header"]
|===
|Name
|Group
|Description
|Namespaced
|Can deploy CR


|Alertmanager
|monitoring.coreos.com
|
|Namespaced
|

|Authentication
|config.openshift.io
|
|Global
|

|Build
|config.openshift.io
|
|Global
|

|CatalogSourceConfig
|operators.coreos.com
|
|Namespaced
|

|CatalogSource
|operators.coreos.com
|
|Namespaced
|

|ClusterAutoscaler
|autoscaling.openshift.io
|
|Global
|Yes

|ClusterDNS
|dns.openshift.io
|
|Global
|

|IngressController
|operator.openshift.io
|
|Namespaced
|

|ClusterNetwork
|network.openshift.io
|
|Global
|

|ClusterOperator
|config.openshift.io
|
|Global
|

|ClusterOperator
|operatorstatus.openshift.io
|
|Namespaced
|

|Cluster
|machine.openshift.io
|
|Namespaced
|

|ClusterServiceVersion
|operators.coreos.com
|
|Namespaced
|

|ClusterVersion
|config.openshift.io
|
|Global
|

|Config
|imageregistry.operator.openshift.io
|
|Global
|

|Config
|samples.operator.openshift.io
|
|Global
|

|Console
|console.config.openshift.io
|The top-level configuration for the web console.
|Namespaced
|The console CR is created by default with more or less empty values. It honors
new values. If it is deleted, it recreates automatically.

|ControllerConfig
|machineconfiguration.openshift.io
|
|Global
|

|CredentialsRequest
|cloudcredential.openshift.io
|
|Namespaced
|

|DNS
|config.openshift.io
|
|Global
|

|EgressNetworkPolicy
|network.openshift.io
|
|Namespaced
|

|HostSubnet
|network.openshift.io
|
|Global
|

|Image
|config.openshift.io
|
|Global
|

|Infrastructure
|config.openshift.io
|
|Global
|

|Ingress
|config.openshift.io
|
|Global
|

|InstallPlan
|operators.coreos.com
|
|Namespaced
|

|KubeControllerManager
|operator.openshift.io
|
|Global
|

|KubeletConfig
|machineconfiguration.openshift.io
|
|Global
|

|MachineAutoscaler
|autoscaling.openshift.io
|
|Namespaced
|Yes

|MachineClass
|machine.openshift.io
|
|Namespaced
|

|MachineConfigPool
|machineconfiguration.openshift.io
|
|Global
|

|MachineConfig
|machineconfiguration.openshift.io
|
|Global
|

|MachineDeployment
|machine.openshift.io
|
|Namespaced
|

|MachineHealthCheck
|healthchecking.openshift.io
|
|Namespaced
|

|Machine
|machine.openshift.io
|
|Namespaced
|

|MachineSet
|machine.openshift.io
|
|Namespaced
|

|MCOConfig
|machineconfiguration.openshift.io
|
|Global
|

|NetNamespace
|network.openshift.io
|
|Global
|

|NetworkAttachmentDefinition
|k8s.cni.cncf.io
|
|Namespaced
|

|NetworkConfig
|networkoperator.openshift.io
|
|Global
|

|Network
|config.openshift.io
|
|Global
|

|OAuth
|config.openshift.io
|
|Global
|

|OpenShiftAPIServer
|operator.openshift.io
|
|Global
|

|OpenShiftControllerManagerOperatorConfig
|openshiftcontrollermanager.operator.openshift.io
|
|Global
|

|OperatorGroup
|operators.coreos.com
|
|Namespaced
|

|Project
|config.openshift.io
|
|Global
|

|Prometheus
|monitoring.coreos.com
|
|Namespaced
|

|PrometheusRule
|monitoring.coreos.com
|
|Namespaced
|

|ServiceCertSignerOperatorConfig
|servicecertsigner.config.openshift.io
|
|Global
|

|ServiceMonitor
|monitoring.coreos.com
|
|Namespaced
|

|Subscription
|operators.coreos.com
|
|Namespaced
|

|===
////

[id="post-install-adjust-worker-nodes"]
== Adjust worker nodes
If you incorrectly sized the worker nodes during deployment, adjust them by creating one or more new compute machine sets, scale them up, then scale the original compute machine set down before removing them.

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/node-tasks.adoc
// * post_installation_configuration/cluster-tasks.adoc


:_mod-docs-content-type: CONCEPT
[id="differences-between-machinesets-and-machineconfigpool_{context}"]
= Understanding the difference between compute machine sets and the machine config pool

`MachineSet` objects describe {product-title} nodes with respect to the cloud or machine provider.

The `MachineConfigPool` object allows `MachineConfigController` components to define and provide the status of machines in the context of upgrades.

The `MachineConfigPool` object allows users to configure how upgrades are rolled out to the {product-title} nodes in the machine config pool.

The `NodeSelector` object can be replaced with a reference to the `MachineSet` object.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/manually-scaling-machineset.adoc
// * post_installation_configuration/cluster-tasks.adoc
// * windows_containers/scheduling-windows-workloads.adoc

:_mod-docs-content-type: PROCEDURE
[id="machineset-manually-scaling_{context}"]
= Scaling a compute machine set manually

To add or remove an instance of a machine in a compute machine set, you can manually scale the compute machine set.

This guidance is relevant to fully automated, installer-provisioned infrastructure installations. Customized, user-provisioned infrastructure installations do not have compute machine sets.

.Prerequisites

* Install an {product-title} cluster and the `oc` command line.
* Log in to  `oc` as a user with `cluster-admin` permission.

.Procedure

. View the compute machine sets that are in the cluster by running the following command:
+
[source,terminal]
----
$ oc get machinesets -n openshift-machine-api
----
+
The compute machine sets are listed in the form of `<clusterid>-worker-<aws-region-az>`.

. View the compute machines that are in the cluster by running the following command:
+
[source,terminal]
----
$ oc get machine -n openshift-machine-api
----

. Set the annotation on the compute machine that you want to delete by running the following command:
+
[source,terminal]
----
$ oc annotate machine/<machine_name> -n openshift-machine-api machine.openshift.io/delete-machine="true"
----

. Scale the compute machine set by running one of the following commands:
+
[source,terminal]
----
$ oc scale --replicas=2 machineset <machineset> -n openshift-machine-api
----
+
Or:
+
[source,terminal]
----
$ oc edit machineset <machineset> -n openshift-machine-api
----
+
[TIP]
====
You can alternatively apply the following YAML to scale the compute machine set:

[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: <machineset>
  namespace: openshift-machine-api
spec:
  replicas: 2
----
====
+
You can scale the compute machine set up or down. It takes several minutes for the new machines to be available.
+
[IMPORTANT]
====
By default, the machine controller tries to drain the node that is backed by the machine until it succeeds. In some situations, such as with a misconfigured pod disruption budget, the drain operation might not be able to succeed. If the drain operation fails, the machine controller cannot proceed removing the machine.

You can skip draining the node by annotating `machine.openshift.io/exclude-node-draining` in a specific machine.
====

.Verification

* Verify the deletion of the intended machine by running the following command:
+
[source,terminal]
----
$ oc get machines
----

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/manually-scaling-machineset.adoc
// * post_installation_configuration/cluster-tasks.adoc

[id="machineset-delete-policy_{context}"]
= The compute machine set deletion policy

`Random`, `Newest`, and `Oldest` are the three supported deletion options. The default is `Random`, meaning that random machines are chosen and deleted when scaling compute machine sets down. The deletion policy can be set according to the use case by modifying the particular compute machine set:

[source,yaml]
----
spec:
  deletePolicy: <delete_policy>
  replicas: <desired_replica_count>
----

Specific machines can also be prioritized for deletion by adding the annotation `machine.openshift.io/delete-machine=true` to the machine of interest, regardless of the deletion policy.

[IMPORTANT]
====
By default, the {product-title} router pods are deployed on workers. Because the router is required to access some cluster resources, including the web console, do not scale the worker compute machine set to `0` unless you first relocate the router pods.
====

[NOTE]
====
Custom compute machine sets can be used for use cases requiring that services run on specific nodes and that those services are ignored by the controller when the worker compute machine sets are scaling down. This prevents service disruption.
====

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/nodes-scheduler-node-selector.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-scheduler-node-selectors-cluster_{context}"]
= Creating default cluster-wide node selectors

You can use default cluster-wide node selectors on pods together with labels on nodes to constrain all pods created in a cluster to specific nodes.

With cluster-wide node selectors, when you create a pod in that cluster, {product-title} adds the default node selectors to the pod and schedules
the pod on nodes with matching labels.

You configure cluster-wide node selectors by editing the Scheduler Operator custom resource (CR). You add labels to a node, a compute machine set, or a machine config. Adding the label to the compute machine set ensures that if the node or machine goes down, new nodes have the label. Labels added to a node or machine config do not persist if the node or machine goes down.

[NOTE]
====
You can add additional key/value pairs to a pod. But you cannot add a different value for a default key.
====

.Procedure

To add a default cluster-wide node selector:

. Edit the Scheduler Operator CR to add the default cluster-wide node selectors:
+
[source,terminal]
----
$ oc edit scheduler cluster
----
+
.Example Scheduler Operator CR with a node selector
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
...
spec:
  defaultNodeSelector: type=user-node,region=east <1>
  mastersSchedulable: false
----
<1> Add a node selector with the appropriate `<key>:<value>` pairs.
+
After making this change, wait for the pods in the `openshift-kube-apiserver` project to redeploy. This can take several minutes. The default cluster-wide node selector does not take effect until the pods redeploy.

. Add labels to a node by using a compute machine set or editing the node directly:

* Use a compute machine set to add labels to nodes managed by the compute machine set when a node is created:

.. Run the following command to add labels to a `MachineSet` object:
+
[source,terminal]
----
$ oc patch MachineSet <name> --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"<key>"="<value>","<key>"="<value>"}}]'  -n openshift-machine-api <1>
----
<1> Add a `<key>/<value>` pair for each label.
+
For example:
+
[source,terminal]
----
$ oc patch MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c --type='json' -p='[{"op":"add","path":"/spec/template/spec/metadata/labels", "value":{"type":"user-node","region":"east"}}]'  -n openshift-machine-api
----
+
[TIP]
====
You can alternatively apply the following YAML to add labels to a compute machine set:

[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  name: <machineset>
  namespace: openshift-machine-api
spec:
  template:
    spec:
      metadata:
        labels:
          region: "east"
          type: "user-node"
----
====

.. Verify that the labels are added to the `MachineSet` object by using the `oc edit` command:
+
For example:
+
[source,terminal]
----
$ oc edit MachineSet abc612-msrtw-worker-us-east-1c -n openshift-machine-api
----
+
.Example `MachineSet` object
[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
  ...
spec:
  ...
  template:
    metadata:
  ...
    spec:
      metadata:
        labels:
          region: east
          type: user-node
  ...
----

.. Redeploy the nodes associated with that compute machine set by scaling down to `0` and scaling up the nodes:
+
For example:
+
[source,terminal]
----
$ oc scale --replicas=0 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api
----
+
[source,terminal]
----
$ oc scale --replicas=1 MachineSet ci-ln-l8nry52-f76d1-hl7m7-worker-c -n openshift-machine-api
----

.. When the nodes are ready and available, verify that the label is added to the nodes by using the `oc get` command:
+
[source,terminal]
----
$ oc get nodes -l <key>=<value>
----
+
For example:
+
[source,terminal]
----
$ oc get nodes -l type=user-node
----
+
.Example output
[source,terminal]
----
NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-c-vmqzp   Ready    worker   61s   v1.27.3
----

* Add labels directly to a node:

.. Edit the `Node` object for the node:
+
[source,terminal]
----
$ oc label nodes <name> <key>=<value>
----
+
For example, to label a node:
+
[source,terminal]
----
$ oc label nodes ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49 type=user-node region=east
----
+
[TIP]
====
You can alternatively apply the following YAML to add labels to a node:

[source,yaml]
----
kind: Node
apiVersion: v1
metadata:
  name: <node_name>
  labels:
    type: "user-node"
    region: "east"
----
====

.. Verify that the labels are added to the node using the `oc get` command:
+
[source,terminal]
----
$ oc get nodes -l <key>=<value>,<key>=<value>
----
+
For example:
+
[source,terminal]
----
$ oc get nodes -l type=user-node,region=east
----
+
.Example output
[source,terminal]
----
NAME                                       STATUS   ROLES    AGE   VERSION
ci-ln-l8nry52-f76d1-hl7m7-worker-b-tgq49   Ready    worker   17m   v1.27.3
----

:leveloffset!:

[id="post-worker-latency-profiles"]
== Improving cluster stability in high latency environments using worker latency profiles

// Text snippet included in the following modules:
//
// * nodes/clusters/nodes-cluster-worker-latency-profiles
// * nodes/edge/nodes-edge-remote-workers
// * post_installation_configuration/cluster-tasks
// * scalability_and_performance/scaling-worker-latency-profiles.adoc


:_mod-docs-content-type: SNIPPET

All nodes send heartbeats to the Kubernetes Controller Manager Operator (kube controller) in the {product-title} cluster every 10 seconds, by default. If the cluster does not receive heartbeats from a node, {product-title} responds using several default mechanisms.

For example, if the Kubernetes Controller Manager Operator loses contact with a node after a configured period:

. The node controller on the control plane updates the node health to `Unhealthy` and marks the node `Ready` condition as `Unknown`.

. In response, the scheduler stops scheduling pods to that node.

. The on-premise node controller adds a `node.kubernetes.io/unreachable` taint with a `NoExecute` effect to the node and schedules any pods on the node for eviction after five minutes, by default.

This behavior can cause problems if your network is prone to latency issues, especially if you have nodes at the network edge. In some cases, the Kubernetes Controller Manager Operator might not receive an update from a healthy node due to network latency. The Kubernetes Controller Manager Operator would then evict pods from the node even though the node is healthy. To avoid this problem, you can use _worker latency profiles_ to adjust the frequency that the kubelet and the Kubernetes Controller Manager Operator wait for status updates before taking action. These adjustments help to ensure that your cluster runs properly in the event that network latency between the control plane and the worker nodes is not optimal.

These worker latency profiles are three sets of parameters that are pre-defined with carefully tuned values that let you control the reaction of the cluster to latency issues  without needing to determine the best values manually.

:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/clusters/nodes-cluster-worker-latency-profiles
// * nodes/edge/nodes-edge-remote-workers. ??
// * post_installation_configuration/cluster-tasks ??

:_mod-docs-content-type: CONCEPT
[id="nodes-cluster-worker-latency-profiles-about_{context}"]
= Understanding worker latency profiles

Worker latency profiles are multiple sets of carefully-tuned values for the `node-status-update-frequency`, `node-monitor-grace-period`, `default-not-ready-toleration-seconds` and `default-unreachable-toleration-seconds` parameters. These parameters let you control the reaction of the cluster to latency issues without needing to determine the best values manually.

All worker latency profiles configure the following parameters:

--
* `node-status-update-frequency`. Specifies the amount of time in seconds that a kubelet updates its status to the Kubernetes Controller Manager Operator.
*  `node-monitor-grace-period`.  Specifies the amount of time in seconds that the Kubernetes Controller Manager Operator waits for an update from a kubelet before marking the node unhealthy and adding the `node.kubernetes.io/not-ready` or `node.kubernetes.io/unreachable` taint to the node.
* `default-not-ready-toleration-seconds`. Specifies the amount of time in seconds after marking a node unhealthy that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
* `default-unreachable-toleration-seconds`. Specifies the amount of time in seconds after marking a node unreachable that the Kubernetes Controller Manager Operator waits before evicting pods from that node.
--

[IMPORTANT]
====
Manually modifying the `node-monitor-grace-period` parameter is not supported.
====

The following Operators monitor the changes to the worker latency profiles and respond accordingly:

* The Machine Config Operator (MCO) updates the `node-status-update-frequency` parameter on the worker nodes.
* The Kubernetes Controller Manager Operator updates the `node-monitor-grace-period` parameter on the control plane nodes.
* The Kubernetes API Server Operator updates the `default-not-ready-toleration-seconds` and `default-unreachable-toleration-seconds` parameters on the control plance nodes.

Although the default configuration works in most cases, {product-title} offers two other worker latency profiles for situations where the network is experiencing higher latency than usual. The three worker latency profiles are described in the following sections:

Default worker latency profile:: With the `Default` profile, each kubelet reports its node status to the Kubelet Controller Manager Operator (kube controller) every 10 seconds. The Kubelet Controller Manager Operator checks the kubelet for a status every 5 seconds.
+
The Kubernetes Controller Manager Operator waits 40 seconds for a status update before considering that node unhealthy. It marks the node with the `node.kubernetes.io/not-ready` or `node.kubernetes.io/unreachable` taint and evicts the pods on that node. If a pod on that node has the `NoExecute` toleration, the pod gets evicted in 300 seconds. If the pod has the `tolerationSeconds` parameter, the eviction waits for the period specified by that parameter.
+
[cols="2,1,2,1"]
|===
| Profile | Component | Parameter | Value

.4+| Default
| kubelet
| `node-status-update-frequency`
| 10s

| Kubelet Controller Manager
| `node-monitor-grace-period`
| 40s

| Kubernetes API Server
| `default-not-ready-toleration-seconds`
| 300s

| Kubernetes API Server
| `default-unreachable-toleration-seconds`
| 300s

|===

Medium worker latency profile:: Use the `MediumUpdateAverageReaction` profile if the network latency is slightly higher than usual.
+
The `MediumUpdateAverageReaction` profile reduces the frequency of kubelet updates to 20 seconds and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 2 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the `tolerationSeconds` parameter, the eviction waits for the period specified by that parameter.
+
The Kubernetes Controller Manager Operator waits for 2 minutes to consider a node unhealthy. In another minute, the eviction process starts.
+
[cols="2,1,2,1"]
|===
| Profile | Component | Parameter | Value

.4+| MediumUpdateAverageReaction
| kubelet
| `node-status-update-frequency`
| 20s

| Kubelet Controller Manager
| `node-monitor-grace-period`
| 2m

| Kubernetes API Server
| `default-not-ready-toleration-seconds`
| 60s

| Kubernetes API Server
| `default-unreachable-toleration-seconds`
| 60s

|===

Low worker latency profile:: Use the `LowUpdateSlowReaction` profile if the network latency is extremely high.
+
The `LowUpdateSlowReaction` profile reduces the frequency of kubelet updates to 1 minute and changes the period that the Kubernetes Controller Manager Operator waits for those updates to 5 minutes. The pod eviction period for a pod on that node is reduced to 60 seconds. If the pod has the `tolerationSeconds` parameter, the eviction waits for the period specified by that parameter.
+
The Kubernetes Controller Manager Operator waits for 5 minutes to consider a node unhealthy. In another minute, the eviction process starts.
+
[cols="2,1,2,1"]
|===
| Profile | Component | Parameter | Value

.4+| LowUpdateSlowReaction
| kubelet
| `node-status-update-frequency`
| 1m

| Kubelet Controller Manager
| `node-monitor-grace-period`
| 5m

| Kubernetes API Server
| `default-not-ready-toleration-seconds`
| 60s

| Kubernetes API Server
| `default-unreachable-toleration-seconds`
| 60s

|===

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/clusters/nodes-cluster-worker-latency-profiles
// * Need to determine if these are good locations:
// * nodes/edge/nodes-edge-remote-workers
// * post_installation_configuration/cluster-tasks

:_mod-docs-content-type: PROCEDURE
[id="nodes-cluster-worker-latency-profiles-using_{context}"]
= Using worker latency profiles

To implement a worker latency profile to deal with network latency, edit the `node.config` object to add the name of the profile. You can change the profile at any time as latency increases or decreases.

You must move one worker latency profile at a time. For example, you cannot move directly from the `Default` profile to the `LowUpdateSlowReaction` worker latency profile. You must move from the `default` worker latency profile to the `MediumUpdateAverageReaction` profile first, then to `LowUpdateSlowReaction`. Similarly, when returning to the default profile, you must move from the low profile to the medium profile first, then to the default.

[NOTE]
====
You can also configure worker latency profiles upon installing an {product-title} cluster.
====

.Procedure

To move from the default worker latency profile:

. Move to the medium worker latency profile:

.. Edit the `node.config` object:
+
[source,terminal]
----
$ oc edit nodes.config/cluster
----

.. Add `spec.workerLatencyProfile: MediumUpdateAverageReaction`:
+
.Example `node.config` object
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: MediumUpdateAverageReaction <1>

# ...
----
<1> Specifies the medium worker latency policy.
+
Scheduling on each worker node is disabled as the change is being applied.

. Optional: Move to the low worker latency profile:

.. Edit the `node.config` object:
+
[source,terminal]
----
$ oc edit nodes.config/cluster
----

.. Change the `spec.workerLatencyProfile` value to `LowUpdateSlowReaction`:
+
.Example `node.config` object
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v1
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  workerLatencyProfile: LowUpdateSlowReaction <1>

# ...
----
<1> Specifies to use the low worker latency policy.
+
Scheduling on each worker node is disabled as the change is being applied.

.Verification

* When all nodes return to the `Ready` condition, you can use the following command to look in the Kubernetes Controller Manager to ensure it was applied:
+
[source,terminal]
----
$ oc get KubeControllerManager -o yaml | grep -i workerlatency -A 5 -B 5
----
+
.Example output
[source,terminal]
----
# ...
    - lastTransitionTime: "2022-07-11T19:47:10Z"
      reason: ProfileUpdated
      status: "False"
      type: WorkerLatencyProfileProgressing
    - lastTransitionTime: "2022-07-11T19:47:10Z" <1>
      message: all static pod revision(s) have updated latency profile
      reason: ProfileUpdated
      status: "True"
      type: WorkerLatencyProfileComplete
    - lastTransitionTime: "2022-07-11T19:20:11Z"
      reason: AsExpected
      status: "False"
      type: WorkerLatencyProfileDegraded
    - lastTransitionTime: "2022-07-11T19:20:36Z"
      status: "False"
# ...
----
<1> Specifies that the profile is applied and active.

To change the medium profile to default or change the default to medium, edit the `node.config` object and set the `spec.workerLatencyProfile` parameter to the appropriate value.


:leveloffset!:

[id="post-install-cpms-setup"]
== Managing control plane machines

xref:../machine_management/control_plane_machine_management/cpmso-about.adoc#cpmso-about[Control plane machine sets] provide management capabilities for control plane machines that are similar to what compute machine sets provide for compute machines. The availability and initial status of control plane machine sets on your cluster depend on your cloud provider and the version of {product-title} that you installed. For more information, see xref:../machine_management/control_plane_machine_management/cpmso-getting-started.adoc#cpmso-getting-started[Getting started with control plane machine sets].

[id="post-install-creating-infrastructure-machinesets-production"]
== Creating infrastructure machine sets for production environments

You can create a compute machine set to create machines that host only infrastructure components, such as the default router, the integrated container image registry, and components for cluster metrics and monitoring. These infrastructure machines are not counted toward the total number of subscriptions that are required to run the environment.

In a production deployment, it is recommended that you deploy at least three compute machine sets to hold infrastructure components. Both OpenShift Logging and {SMProductName} deploy Elasticsearch, which requires three instances to be installed on different nodes. Each of these nodes can be deployed to different availability zones for high availability. A configuration like this requires three different compute machine sets, one for each availability zone. In global Azure regions that do not have multiple availability zones, you can use availability sets to ensure high availability.

For information on infrastructure nodes and which components can run on infrastructure nodes, see xref:../machine_management/creating-infrastructure-machinesets.adoc#creating-infrastructure-machinesets[Creating infrastructure machine sets].

To create an infrastructure node, you can xref:../post_installation_configuration/cluster-tasks.adoc#machineset-creating_post-install-cluster-tasks[use a machine set], xref:../post_installation_configuration/cluster-tasks.adoc#creating-an-infra-node_post-install-cluster-tasks[assign a label to the nodes], or xref:../post_installation_configuration/cluster-tasks.adoc#creating-infra-machines_post-install-cluster-tasks[use a machine config pool].

For sample machine sets that you can use with these procedures, see xref:../machine_management/creating-infrastructure-machinesets.adoc#creating-infrastructure-machinesets-clouds[Creating machine sets for different clouds].

Applying a specific node selector to all infrastructure components causes {product-title} to xref:../post_installation_configuration/cluster-tasks.adoc#moving-resources-to-infrastructure-machinesets[schedule those workloads on nodes with that label].

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc
// * machine_management/creating_machinesets/creating-machineset-aws.adoc
// * machine_management/creating_machinesets/creating-machineset-azure.adoc
// * machine_management/creating_machinesets/creating-machineset-azure-stack-hub.adoc
// * machine_management/creating_machinesets/creating-machineset-gcp.adoc
// * machine_management/creating_machinesets/creating-machineset-osp.adoc
// * machine_management/creating_machinesets/creating-machineset-vsphere.adoc
// * windows_containers/creating_windows_machinesets/creating-windows-machineset-aws.adoc
// * windows_containers/creating_windows_machinesets/creating-windows-machineset-azure.adoc
// * windows_containers/creating_windows_machinesets/creating-windows-machineset-vsphere.adoc
// * windows_containers/creating_windows_machinesets/creating-windows-machineset-gcp.adoc
// * post_installation_configuration/cluster-tasks.adoc
// * post_installation_configuration/installation-creating-aws-subnet-localzone.adoc
// * post_installation_configuration/aws-compute-edge-tasks.adoc


:_mod-docs-content-type: PROCEDURE
[id="machineset-creating_{context}"]
= Creating a compute machine set

In addition to the compute machine sets created by the installation program, you can create your own to dynamically manage the machine compute resources for specific workloads of your choice.


.Prerequisites

* Deploy an {product-title} cluster.
* Install the OpenShift CLI (`oc`).
* Log in to `oc` as a user with `cluster-admin` permission.

.Procedure

. Create a new YAML file that contains the compute machine set custom resource (CR) sample and is named `<file_name>.yaml`.
+
Ensure that you set the `<clusterID>` and `<role>` parameter values.

. Optional: If you are not sure which value to set for a specific field, you can check an existing compute machine set from your cluster.

.. To list the compute machine sets in your cluster, run the following command:
+
[source,terminal]
----
$ oc get machinesets -n openshift-machine-api
----
+
.Example output
[source,terminal]
----
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m
----

.. To view values of a specific compute machine set custom resource (CR), run the following command:
+
[source,terminal]
----
$ oc get machineset <machineset_name> \
  -n openshift-machine-api -o yaml
----
+
--
.Example output
[source,yaml]
----
apiVersion: machine.openshift.io/v1beta1
kind: MachineSet
metadata:
  labels:
    machine.openshift.io/cluster-api-cluster: <infrastructure_id> <1>
  name: <infrastructure_id>-<role> <2>
  namespace: openshift-machine-api
spec:
  replicas: 1
  selector:
    matchLabels:
      machine.openshift.io/cluster-api-cluster: <infrastructure_id>
      machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>
  template:
    metadata:
      labels:
        machine.openshift.io/cluster-api-cluster: <infrastructure_id>
        machine.openshift.io/cluster-api-machine-role: <role>
        machine.openshift.io/cluster-api-machine-type: <role>
        machine.openshift.io/cluster-api-machineset: <infrastructure_id>-<role>
    spec:
      providerSpec: <3>
        ...
----
<1> The cluster infrastructure ID.
<2> A default node label.
+
[NOTE]
====
For clusters that have user-provisioned infrastructure, a compute machine set can only create `worker` and `infra` type machines.
====
<3> The values in the `<providerSpec>` section of the compute machine set CR are platform-specific. For more information about `<providerSpec>` parameters in the CR, see the sample compute machine set CR configuration for your provider.
--


. Create a `MachineSet` CR by running the following command:
+
[source,terminal]
----
$ oc create -f <file_name>.yaml
----


.Verification

* View the list of compute machine sets by running the following command:
+
[source,terminal]
----
$ oc get machineset -n openshift-machine-api
----
+
.Example output
[source,terminal]
----
NAME                                DESIRED   CURRENT   READY   AVAILABLE   AGE
agl030519-vplxk-infra-us-east-1a    1         1         1       1           11m
agl030519-vplxk-worker-us-east-1a   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1b   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1c   1         1         1       1           55m
agl030519-vplxk-worker-us-east-1d   0         0                             55m
agl030519-vplxk-worker-us-east-1e   0         0                             55m
agl030519-vplxk-worker-us-east-1f   0         0                             55m
----
+
When the new compute machine set is available, the `DESIRED` and `CURRENT` values match. If the compute machine set is not available, wait a few minutes and run the command again.



:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc
// * machine_management/creating-infrastructure-machinesets.adoc
// * nodes/nodes/nodes-nodes-creating-infrastructure-nodes.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-an-infra-node_{context}"]
= Creating an infrastructure node

[IMPORTANT]
====
See Creating infrastructure machine sets for installer-provisioned infrastructure environments or for any cluster where the control plane nodes are managed by the machine API.
====

Requirements of the cluster dictate that infrastructure, also called `infra` nodes, be provisioned. The installer only provides provisions for control plane and worker nodes. Worker nodes can be designated as infrastructure nodes or application, also called `app`, nodes through labeling.

.Procedure

. Add a label to the worker node that you want to act as application node:
+
[source,terminal]
----
$ oc label node <node-name> node-role.kubernetes.io/app=""
----

. Add a label to the worker nodes that you want to act as infrastructure nodes:
+
[source,terminal]
----
$ oc label node <node-name> node-role.kubernetes.io/infra=""
----

. Check to see if applicable nodes now have the `infra` role and `app` roles:
+
[source,terminal]
----
$ oc get nodes
----

. Create a default cluster-wide node selector. The default node selector is applied to pods created in all namespaces. This creates an intersection with any existing node selectors on a pod, which additionally constrains the pod's selector.
+
[IMPORTANT]
====
If the default node selector key conflicts with the key of a pod's label, then the default node selector is not applied.

However, do not set a default node selector that might cause a pod to become unschedulable. For example, setting the default node selector to a specific node role, such as `node-role.kubernetes.io/infra=""`, when a pod's label is set to a different node role, such as `node-role.kubernetes.io/master=""`, can cause the pod to become unschedulable. For this reason, use caution when setting the default node selector to specific node roles.

You can alternatively use a project node selector to avoid cluster-wide node selector key conflicts.
====

.. Edit the `Scheduler` object:
+
[source,terminal]
----
$ oc edit scheduler cluster
----

.. Add the `defaultNodeSelector` field with the appropriate node selector:
+
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: Scheduler
metadata:
  name: cluster
spec:
  defaultNodeSelector: topology.kubernetes.io/region=us-east-1 <1>
# ...
----
<1> This example node selector deploys pods on nodes in the `us-east-1` region by default.

.. Save the file to apply the changes.

You can now move infrastructure resources to the newly labeled `infra` nodes.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* For information on how to configure project node selectors to avoid cluster-wide node selector key conflicts, see xref:../nodes/scheduling/nodes-scheduler-node-selectors.adoc#project-node-selectors_nodes-scheduler-node-selectors[Project node selectors].

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="creating-infra-machines_{context}"]
= Creating a machine config pool for infrastructure machines

If you need infrastructure machines to have dedicated configurations, you must create an infra pool.

.Procedure

. Add a label to the node you want to assign as the infra node with a specific label:
+
[source,terminal]
----
$ oc label node <node_name> <label>
----
+
[source,terminal]
----
$ oc label node ci-ln-n8mqwr2-f76d1-xscn2-worker-c-6fmtx node-role.kubernetes.io/infra=
----

. Create a machine config pool that contains both the worker role and your custom role as machine config selector:
+
[source,terminal]
----
$ cat infra.mcp.yaml
----
+
.Example output
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfigPool
metadata:
  name: infra
spec:
  machineConfigSelector:
    matchExpressions:
      - {key: machineconfiguration.openshift.io/role, operator: In, values: [worker,infra]} <1>
  nodeSelector:
    matchLabels:
      node-role.kubernetes.io/infra: "" <2>
----
<1> Add the worker role and your custom role.
<2> Add the label you added to the node as a `nodeSelector`.
+
[NOTE]
====
Custom machine config pools inherit machine configs from the worker pool. Custom pools use any machine config targeted for the worker pool, but add the ability to also deploy changes that are targeted at only the custom pool. Because a custom pool inherits resources from the worker pool, any change to the worker pool also affects the custom pool.
====

. After you have the YAML file, you can create the machine config pool:
+
[source,terminal]
----
$ oc create -f infra.mcp.yaml
----

. Check the machine configs to ensure that the infrastructure configuration rendered successfully:
+
[source,terminal]
----
$ oc get machineconfig
----
+
.Example output
[source,terminal]
----
NAME                                                        GENERATEDBYCONTROLLER                      IGNITIONVERSION   CREATED
00-master                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
00-worker                                                   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-master-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-container-runtime                                 365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
01-worker-kubelet                                           365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-1ae2a1e0-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-master-ssh                                                                                          3.2.0             31d
99-worker-1ae64748-a115-11e9-8f14-005056899d54-registries   365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             31d
99-worker-ssh                                                                                          3.2.0             31d
rendered-infra-4e48906dca84ee702959c71a53ee80e7             365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             23m
rendered-master-072d4b2da7f88162636902b074e9e28e            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-master-3e88ec72aed3886dec061df60d16d1af            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-master-419bee7de96134963a15fdf9dd473b25            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-master-53f5c91c7661708adce18739cc0f40fb            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d
rendered-master-a6a357ec18e5bce7f5ac426fc7c5ffcd            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-master-dc7f874ec77fc4b969674204332da037            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-1a75960c52ad18ff5dfa6674eb7e533d            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-2640531be11ba43c61d72e82dc634ce6            5b6fb8349a29735e48446d435962dec4547d3090   3.2.0             31d
rendered-worker-4e48906dca84ee702959c71a53ee80e7            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             7d3h
rendered-worker-4f110718fe88e5f349987854a1147755            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             17d
rendered-worker-afc758e194d6188677eb837842d3b379            02c07496ba0417b3e12b78fb32baf6293d314f79   3.2.0             31d
rendered-worker-daa08cc1e8f5fcdeba24de60cd955cc3            365c1cfd14de5b0e3b85e0fc815b0060f36ab955   3.2.0             13d
----
+
You should see a new machine config, with the `rendered-infra-*` prefix.

. Optional: To deploy changes to a custom pool, create a machine config that uses the custom pool name as the label, such as `infra`. Note that this is not required and only shown for instructional purposes. In this manner, you can apply any custom configurations specific to only your infra nodes.
+
[NOTE]
====
After you create the new machine config pool, the MCO generates a new rendered config for that pool, and associated nodes of that pool reboot to apply the new configuration.
====

.. Create a machine config:
+
[source,terminal]
----
$ cat infra.mc.yaml
----
+
.Example output
[source,yaml]
----
apiVersion: machineconfiguration.openshift.io/v1
kind: MachineConfig
metadata:
  name: 51-infra
  labels:
    machineconfiguration.openshift.io/role: infra <1>
spec:
  config:
    ignition:
      version: 3.2.0
    storage:
      files:
      - path: /etc/infratest
        mode: 0644
        contents:
          source: data:,infra
----
<1> Add the label you added to the node as a `nodeSelector`.

..  Apply the machine config to the infra-labeled nodes:
+
[source,terminal]
----
$ oc create -f infra.mc.yaml
----

. Confirm that your new machine config pool is available:
+
[source,terminal]
----
$ oc get mcp
----
+
.Example output
[source,terminal]
----
NAME     CONFIG                                             UPDATED   UPDATING   DEGRADED   MACHINECOUNT   READYMACHINECOUNT   UPDATEDMACHINECOUNT   DEGRADEDMACHINECOUNT   AGE
infra    rendered-infra-60e35c2e99f42d976e084fa94da4d0fc    True      False      False      1              1                   1                     0                      4m20s
master   rendered-master-9360fdb895d4c131c7c4bebbae099c90   True      False      False      3              3                   3                     0                      91m
worker   rendered-worker-60e35c2e99f42d976e084fa94da4d0fc   True      False      False      2              2                   2                     0                      91m
----
+
In this example, a worker node was changed to an infra node.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* See xref:../architecture/control-plane.adoc#architecture-machine-config-pools_control-plane[Node configuration management with machine config pools] for more information on grouping infra machines in a custom pool.

[id="assigning-machine-set-resources-to-infra-nodes"]
== Assigning machine set resources to infrastructure nodes

After creating an infrastructure machine set, the `worker` and `infra` roles are applied to new infra nodes. Nodes with the `infra` role are not counted toward the total number of subscriptions that are required to run the environment, even when the `worker` role is also applied.

However, when an infra node is assigned the worker role, there is a chance that user workloads can get assigned inadvertently to the infra node. To avoid this, you can apply a taint to the infra node and tolerations for the pods that you want to control.

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="binding-infra-node-workloads-using-taints-tolerations_{context}"]
= Binding infrastructure node workloads using taints and tolerations

If you have an infra node that has the `infra` and `worker` roles assigned, you must configure the node so that user workloads are not assigned to it.

[IMPORTANT]
====
It is recommended that you preserve the dual `infra,worker` label that is created for infra nodes and use taints and tolerations to manage nodes that user workloads are scheduled on. If you remove the `worker` label from the node, you must create a custom pool to manage it. A node with a label other than `master` or `worker` is not recognized by the MCO without a custom pool. Maintaining the `worker` label allows the node to be managed by the default worker machine config pool, if no custom pools that select the custom label exists. The `infra` label communicates to the cluster that it does not count toward the total number of subscriptions.
====

.Prerequisites

* Configure additional `MachineSet` objects in your {product-title} cluster.

.Procedure

. Add a taint to the infra node to prevent scheduling user workloads on it:

.. Determine if the node has the taint:
+
[source,terminal]
----
$ oc describe nodes <node_name>
----
+
.Sample output
[source,text]
----
oc describe node ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Name:               ci-ln-iyhx092-f76d1-nvdfm-worker-b-wln2l
Roles:              worker
 ...
Taints:             node-role.kubernetes.io/infra:NoSchedule
 ...
----
+
This example shows that the node has a taint. You can proceed with adding a toleration to your pod in the next step.

.. If you have not configured a taint to prevent scheduling user workloads on it:
+
[source,terminal]
----
$ oc adm taint nodes <node_name> <key>=<value>:<effect>
----
+
For example:
+
[source,terminal]
----
$ oc adm taint nodes node1 node-role.kubernetes.io/infra=reserved:NoExecute
----
+
[TIP]
====
You can alternatively apply the following YAML to add the taint:

[source,yaml]
----
kind: Node
apiVersion: v1
metadata:
  name: <node_name>
  labels:
    ...
spec:
  taints:
    - key: node-role.kubernetes.io/infra
      effect: NoExecute
      value: reserved
  ...
----
====
+
This example places a taint on `node1` that has key `node-role.kubernetes.io/infra` and taint effect `NoSchedule`. Nodes with the `NoSchedule` effect schedule only pods that tolerate the taint, but allow existing pods to remain scheduled on the node.
+
[NOTE]
====
If a descheduler is used, pods violating node taints could be evicted from the cluster.
====

. Add tolerations for the pod configurations you want to schedule on the infra node, like router, registry, and monitoring workloads. Add the following code to the `Pod` object specification:
+
[source,yaml]
----
tolerations:
  - effect: NoExecute <1>
    key: node-role.kubernetes.io/infra <2>
    operator: Exists <3>
    value: reserved <4>
----
<1> Specify the effect that you added to the node.
<2> Specify the key that you added to the node.
<3> Specify the `Exists` Operator to require a taint with the key `node-role.kubernetes.io/infra` to be present on the node.
<4> Specify the value of the key-value pair taint that you added to the node.
+
This toleration matches the taint created by the `oc adm taint` command. A pod with this toleration can be scheduled onto the infra node.
+
[NOTE]
====
Moving pods for an Operator installed via OLM to an infra node is not always possible. The capability to move Operator pods depends on the configuration of each Operator.
====

. Schedule the pod to the infra node using a scheduler. See the documentation for _Controlling pod placement onto nodes_ for details.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* See xref:../nodes/scheduling/nodes-scheduler-about.adoc#nodes-scheduler-about[Controlling pod placement using the scheduler] for general information on scheduling a pod to a node.

[id="moving-resources-to-infrastructure-machinesets"]
== Moving resources to infrastructure machine sets

Some of the infrastructure resources are deployed in your cluster by default. You can move them to the infrastructure machine sets that you created.

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc

:_mod-docs-content-type: PROCEDURE
[id="infrastructure-moving-router_{context}"]
= Moving the router

You can deploy the router pod to a different compute machine set. By default, the pod is deployed to a worker node.

.Prerequisites

* Configure additional compute machine sets in your {product-title} cluster.

.Procedure

. View the `IngressController` custom resource for the router Operator:
+
[source,terminal]
----
$ oc get ingresscontroller default -n openshift-ingress-operator -o yaml
----
+
The command output resembles the following text:
+
[source,yaml]
----
apiVersion: operator.openshift.io/v1
kind: IngressController
metadata:
  creationTimestamp: 2019-04-18T12:35:39Z
  finalizers:
  - ingresscontroller.operator.openshift.io/finalizer-ingresscontroller
  generation: 1
  name: default
  namespace: openshift-ingress-operator
  resourceVersion: "11341"
  selfLink: /apis/operator.openshift.io/v1/namespaces/openshift-ingress-operator/ingresscontrollers/default
  uid: 79509e05-61d6-11e9-bc55-02ce4781844a
spec: {}
status:
  availableReplicas: 2
  conditions:
  - lastTransitionTime: 2019-04-18T12:36:15Z
    status: "True"
    type: Available
  domain: apps.<cluster>.example.com
  endpointPublishingStrategy:
    type: LoadBalancerService
  selector: ingresscontroller.operator.openshift.io/deployment-ingresscontroller=default
----

. Edit the `ingresscontroller` resource and change the `nodeSelector` to use the `infra` label:
+
[source,terminal]
----
$ oc edit ingresscontroller default -n openshift-ingress-operator
----
+
[source,yaml]
----
  spec:
    nodePlacement:
      nodeSelector: <1>
        matchLabels:
          node-role.kubernetes.io/infra: ""
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
----
<1> Add a `nodeSelector` parameter with the appropriate value to the component you want to move. You can use a `nodeSelector` in the format shown or use `<key>: <value>` pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.

. Confirm that the router pod is running on the `infra` node.
.. View the list of router pods and note the node name of the running pod:
+
[source,terminal]
----
$ oc get pod -n openshift-ingress -o wide
----
+
.Example output
[source,terminal]
----
NAME                              READY     STATUS        RESTARTS   AGE       IP           NODE                           NOMINATED NODE   READINESS GATES
router-default-86798b4b5d-bdlvd   1/1      Running       0          28s       10.130.2.4   ip-10-0-217-226.ec2.internal   <none>           <none>
router-default-955d875f4-255g8    0/1      Terminating   0          19h       10.129.2.4   ip-10-0-148-172.ec2.internal   <none>           <none>
----
+
In this example, the running pod is on the `ip-10-0-217-226.ec2.internal` node.

.. View the node status of the running pod:
+
[source,terminal]
----
$ oc get node <node_name> <1>
----
<1> Specify the `<node_name>` that you obtained from the pod list.
+
.Example output
[source,terminal]
----
NAME                          STATUS  ROLES         AGE   VERSION
ip-10-0-217-226.ec2.internal  Ready   infra,worker  17h   v1.27.3
----
+
Because the role list includes `infra`, the pod is running on the correct node.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc

:_mod-docs-content-type: PROCEDURE
[id="infrastructure-moving-registry_{context}"]
= Moving the default registry

You configure the registry Operator to deploy its pods to different nodes.

.Prerequisites

* Configure additional compute machine sets in your {product-title} cluster.

.Procedure

. View the `config/instance` object:
+
[source,terminal]
----
$ oc get configs.imageregistry.operator.openshift.io/cluster -o yaml
----
+
.Example output
[source,yaml]
----
apiVersion: imageregistry.operator.openshift.io/v1
kind: Config
metadata:
  creationTimestamp: 2019-02-05T13:52:05Z
  finalizers:
  - imageregistry.operator.openshift.io/finalizer
  generation: 1
  name: cluster
  resourceVersion: "56174"
  selfLink: /apis/imageregistry.operator.openshift.io/v1/configs/cluster
  uid: 36fd3724-294d-11e9-a524-12ffeee2931b
spec:
  httpSecret: d9a012ccd117b1e6616ceccb2c3bb66a5fed1b5e481623
  logging: 2
  managementState: Managed
  proxy: {}
  replicas: 1
  requests:
    read: {}
    write: {}
  storage:
    s3:
      bucket: image-registry-us-east-1-c92e88cad85b48ec8b312344dff03c82-392c
      region: us-east-1
status:
...
----

. Edit the `config/instance` object:
+
[source,terminal]
----
$ oc edit configs.imageregistry.operator.openshift.io/cluster
----
+
[source,yaml]
----
spec:
  affinity:
    podAntiAffinity:
      preferredDuringSchedulingIgnoredDuringExecution:
      - podAffinityTerm:
          namespaces:
          - openshift-image-registry
          topologyKey: kubernetes.io/hostname
        weight: 100
  logLevel: Normal
  managementState: Managed
  nodeSelector: <1>
    node-role.kubernetes.io/infra: ""
  tolerations:
  - effect: NoSchedule
    key: node-role.kubernetes.io/infra
    value: reserved
  - effect: NoExecute
    key: node-role.kubernetes.io/infra
    value: reserved
----
<1> Add a `nodeSelector` parameter with the appropriate value to the component you want to move. You can use a `nodeSelector` in the format shown or use `<key>: <value>` pairs, based on the value specified for the node.  If you added a taint to the infrasructure node, also add a matching toleration.

. Verify the registry pod has been moved to the infrastructure node.
+
.. Run the following command to identify the node where the registry pod is located:
+
[source,terminal]
----
$ oc get pods -o wide -n openshift-image-registry
----
+
.. Confirm the node has the label you specified:
+
[source,terminal]
----
$ oc describe node <node_name>
----
+
Review the command output and confirm that `node-role.kubernetes.io/infra` is in the `LABELS` list.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc

:_mod-docs-content-type: PROCEDURE
[id="infrastructure-moving-monitoring_{context}"]
= Moving the monitoring solution

The monitoring stack includes multiple components, including Prometheus, Thanos Querier, and Alertmanager.
The Cluster Monitoring Operator manages this stack. To redeploy the monitoring stack to infrastructure nodes, you can create and apply a custom config map.

.Procedure

. Edit the `cluster-monitoring-config` config map and change the `nodeSelector` to use the `infra` label:
+
[source,terminal]
----
$ oc edit configmap cluster-monitoring-config -n openshift-monitoring
----
+
[source,yaml]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: cluster-monitoring-config
  namespace: openshift-monitoring
data:
  config.yaml: |+
    alertmanagerMain:
      nodeSelector: <1>
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusK8s:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    prometheusOperator:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    k8sPrometheusAdapter:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    kubeStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    telemeterClient:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    openshiftStateMetrics:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    thanosQuerier:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
    monitoringPlugin:
      nodeSelector:
        node-role.kubernetes.io/infra: ""
      tolerations:
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoSchedule
      - key: node-role.kubernetes.io/infra
        value: reserved
        effect: NoExecute
----
<1> Add a `nodeSelector` parameter with the appropriate value to the component you want to move. You can use a `nodeSelector` in the format shown or use `<key>: <value>` pairs, based on the value specified for the node. If you added a taint to the infrastructure node, also add a matching toleration.

. Watch the monitoring pods move to the new machines:
+
[source,terminal]
----
$ watch 'oc get pod -n openshift-monitoring -o wide'
----

. If a component has not moved to the `infra` node, delete the pod with this component:
+
[source,terminal]
----
$ oc delete pod -n openshift-monitoring <pod>
----
+
The component from the deleted pod is re-created on the `infra` node.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/creating-infrastructure-machinesets.adoc
// * logging/cluster-logging-moving.adoc

:_mod-docs-content-type: PROCEDURE
[id="infrastructure-moving-logging_{context}"]
= Moving OpenShift Logging resources

You can configure the Cluster Logging Operator to deploy the pods for {logging} components, such as Elasticsearch and Kibana, to different nodes. You cannot move the Cluster Logging Operator pod from its installed location.

For example, you can move the Elasticsearch pods to a separate node because of high CPU, memory, and disk requirements.

.Prerequisites

* The Red Hat OpenShift Logging and Elasticsearch Operators must be installed. These features are not installed by default.

.Procedure

. Edit the `ClusterLogging` custom resource (CR) in the `openshift-logging` project:
+
[source,terminal]
----
$ oc edit ClusterLogging instance
----
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging

...

spec:
  collection:
    logs:
      fluentd:
        resources: null
      type: fluentd
  logStore:
    elasticsearch:
      nodeCount: 3
      nodeSelector: <1>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      redundancyPolicy: SingleRedundancy
      resources:
        limits:
          cpu: 500m
          memory: 16Gi
        requests:
          cpu: 500m
          memory: 16Gi
      storage: {}
    type: elasticsearch
  managementState: Managed
  visualization:
    kibana:
      nodeSelector: <1>
        node-role.kubernetes.io/infra: ''
      tolerations:
      - effect: NoSchedule
        key: node-role.kubernetes.io/infra
        value: reserved
      - effect: NoExecute
        key: node-role.kubernetes.io/infra
        value: reserved
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana

...
----
<1> Add a `nodeSelector` parameter with the appropriate value to the component you want to move. You can use a `nodeSelector` in the format shown or use `<key>: <value>` pairs, based on the value specified for the node.  If you added a taint to the infrasructure node, also add a matching toleration.

.Verification

To verify that a component has moved, you can use the `oc get pod -o wide` command.

For example:

* You want to move the Kibana pod from the `ip-10-0-147-79.us-east-2.compute.internal` node:
+
[source,terminal]
----
$ oc get pod kibana-5b8bdf44f9-ccpq9 -o wide
----
+
.Example output
[source,terminal]
----
NAME                      READY   STATUS    RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-5b8bdf44f9-ccpq9   2/2     Running   0          27s   10.129.2.18   ip-10-0-147-79.us-east-2.compute.internal   <none>           <none>
----

* You want to move the Kibana pod to the `ip-10-0-139-48.us-east-2.compute.internal` node, a dedicated infrastructure node:
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source,terminal]
----
NAME                                         STATUS   ROLES          AGE   VERSION
ip-10-0-133-216.us-east-2.compute.internal   Ready    master         60m   v1.27.3
ip-10-0-139-146.us-east-2.compute.internal   Ready    master         60m   v1.27.3
ip-10-0-139-192.us-east-2.compute.internal   Ready    worker         51m   v1.27.3
ip-10-0-139-241.us-east-2.compute.internal   Ready    worker         51m   v1.27.3
ip-10-0-147-79.us-east-2.compute.internal    Ready    worker         51m   v1.27.3
ip-10-0-152-241.us-east-2.compute.internal   Ready    master         60m   v1.27.3
ip-10-0-139-48.us-east-2.compute.internal    Ready    infra          51m   v1.27.3
----
+
Note that the node has a `node-role.kubernetes.io/infra: ''` label:
+
[source,terminal]
----
$ oc get node ip-10-0-139-48.us-east-2.compute.internal -o yaml
----
+
.Example output
[source,yaml]
----
kind: Node
apiVersion: v1
metadata:
  name: ip-10-0-139-48.us-east-2.compute.internal
  selfLink: /api/v1/nodes/ip-10-0-139-48.us-east-2.compute.internal
  uid: 62038aa9-661f-41d7-ba93-b5f1b6ef8751
  resourceVersion: '39083'
  creationTimestamp: '2020-04-13T19:07:55Z'
  labels:
    node-role.kubernetes.io/infra: ''
...
----

* To move the Kibana pod, edit the `ClusterLogging` CR to add a node selector:
+
[source,yaml]
----
apiVersion: logging.openshift.io/v1
kind: ClusterLogging

...

spec:

...

  visualization:
    kibana:
      nodeSelector: <1>
        node-role.kubernetes.io/infra: ''
      proxy:
        resources: null
      replicas: 1
      resources: null
    type: kibana
----
<1> Add a node selector to match the label in the node specification.

* After you save the CR, the current Kibana pod is terminated and new pod is deployed:
+
[source,terminal]
----
$ oc get pods
----
+
.Example output
[source,terminal]
----
NAME                                            READY   STATUS        RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running       0          29m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running       0          28m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running       0          28m
fluentd-42dzz                                   1/1     Running       0          28m
fluentd-d74rq                                   1/1     Running       0          28m
fluentd-m5vr9                                   1/1     Running       0          28m
fluentd-nkxl7                                   1/1     Running       0          28m
fluentd-pdvqb                                   1/1     Running       0          28m
fluentd-tflh6                                   1/1     Running       0          28m
kibana-5b8bdf44f9-ccpq9                         2/2     Terminating   0          4m11s
kibana-7d85dcffc8-bfpfp                         2/2     Running       0          33s
----

* The new pod is on the `ip-10-0-139-48.us-east-2.compute.internal` node:
+
[source,terminal]
----
$ oc get pod kibana-7d85dcffc8-bfpfp -o wide
----
+
.Example output
[source,terminal]
----
NAME                      READY   STATUS        RESTARTS   AGE   IP            NODE                                        NOMINATED NODE   READINESS GATES
kibana-7d85dcffc8-bfpfp   2/2     Running       0          43s   10.131.0.22   ip-10-0-139-48.us-east-2.compute.internal   <none>           <none>
----

* After a few moments, the original Kibana pod is removed.
+
[source,terminal]
----
$ oc get pods
----
+
.Example output
[source,terminal]
----
NAME                                            READY   STATUS    RESTARTS   AGE
cluster-logging-operator-84d98649c4-zb9g7       1/1     Running   0          30m
elasticsearch-cdm-hwv01pf7-1-56588f554f-kpmlg   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-2-84c877d75d-75wqj   2/2     Running   0          29m
elasticsearch-cdm-hwv01pf7-3-f5d95b87b-4nx78    2/2     Running   0          29m
fluentd-42dzz                                   1/1     Running   0          29m
fluentd-d74rq                                   1/1     Running   0          29m
fluentd-m5vr9                                   1/1     Running   0          29m
fluentd-nkxl7                                   1/1     Running   0          29m
fluentd-pdvqb                                   1/1     Running   0          29m
fluentd-tflh6                                   1/1     Running   0          29m
kibana-7d85dcffc8-bfpfp                         2/2     Running   0          62s
----


:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/nodes-about-autoscaling-nodes.adoc
// * post_installation_configuration/cluster-tasks.adoc
// * machine_management/applying-autoscaling.adoc
// * osd_cluster_admin/osd_nodes/osd-nodes-about-autoscaling-nodes.adoc
// * osd_cluster_admin/osd-cluster-autoscaling.adoc
// * rosa_cluster_admin/rosa-cluster-autoscaling.adoc

:_mod-docs-content-type: CONCEPT
[id="cluster-autoscaler-about_{context}"]
= About the cluster autoscaler

The cluster autoscaler adjusts the size of an {product-title} cluster to meet its current deployment needs. It uses declarative, Kubernetes-style arguments to provide infrastructure management that does not rely on objects of a specific cloud provider. The cluster autoscaler has a cluster scope, and is not associated with a particular namespace.

The cluster autoscaler increases the size of the cluster when there are pods that fail to schedule on any of the current worker nodes due to insufficient resources or when another node is necessary to meet deployment needs. The cluster autoscaler does not increase the cluster resources beyond the limits that you specify.

The cluster autoscaler computes the total
memory, CPU, and GPU
on all nodes the cluster, even though it does not manage the control plane nodes. These values are not single-machine oriented. They are an aggregation of all the resources in the entire cluster. For example, if you set the maximum memory resource limit, the cluster autoscaler includes all the nodes in the cluster when calculating the current memory usage. That calculation is then used to determine if the cluster autoscaler has the capacity to add more worker resources.

[IMPORTANT]
====
Ensure that the `maxNodesTotal` value in the `ClusterAutoscaler` resource definition that you create is large enough to account for the total possible number of machines in your cluster. This value must encompass the number of control plane machines and the possible number of compute machines that you might scale to.
====

Every 10 seconds, the cluster autoscaler checks which nodes are unnecessary in the cluster and removes them. The cluster autoscaler considers a node for removal if the following conditions apply:

* The node utilization is less than the _node utilization level_ threshold for the cluster. The node utilization level is the sum of the requested resources divided by the allocated resources for the node. If you do not specify a value in the `ClusterAutoscaler` custom resource, the cluster autoscaler uses a default value of `0.5`, which corresponds to 50% utilization.
* The cluster autoscaler can move all pods running on the node to the other nodes. The Kubernetes scheduler is responsible for scheduling pods on the nodes.
* The cluster autoscaler does not have scale down disabled annotation.

If the following types of pods are present on a node, the cluster autoscaler will not remove the node:

* Pods with restrictive pod disruption budgets (PDBs).
* Kube-system pods that do not run on the node by default.
* Kube-system pods that do not have a PDB or have a PDB that is too restrictive.
* Pods that are not backed by a controller object such as a deployment, replica set, or stateful set.
* Pods with local storage.
* Pods that cannot be moved elsewhere because of a lack of resources, incompatible node selectors or affinity, matching anti-affinity, and so on.
* Unless they also have a `"cluster-autoscaler.kubernetes.io/safe-to-evict": "true"` annotation, pods that have a `"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"` annotation.

For example, you set the maximum CPU limit to 64 cores and configure the cluster autoscaler to only create machines that have 8 cores each. If your cluster starts with 30 cores, the cluster autoscaler can add up to 4 more nodes with 32 cores, for a total of 62.

If you configure the cluster autoscaler, additional usage restrictions apply:

* Do not modify the nodes that are in autoscaled node groups directly. All nodes within the same node group have the same capacity and labels and run the same system pods.
* Specify requests for your pods.
* If you have to prevent pods from being deleted too quickly, configure appropriate PDBs.
* Confirm that your cloud provider quota is large enough to support the maximum node pools that you configure.
* Do not run additional node group autoscalers, especially the ones offered by your cloud provider.

The horizontal pod autoscaler (HPA) and the cluster autoscaler modify cluster resources in different ways. The HPA changes the deployment's or replica set's number of replicas based on the current CPU load. If the load increases, the HPA creates new replicas, regardless of the amount of resources available to the cluster. If there are not enough resources, the cluster autoscaler adds resources so that the HPA-created pods can run. If the load decreases, the HPA stops some replicas. If this action causes some nodes to be underutilized or completely empty, the cluster autoscaler deletes the unnecessary nodes.

The cluster autoscaler takes pod priorities into account. The Pod Priority and Preemption feature enables scheduling pods based on priorities if the cluster does not have enough resources, but the cluster autoscaler ensures that the cluster has resources to run all pods. To honor the intention of both features, the cluster autoscaler includes a priority cutoff function. You can use this cutoff to schedule "best-effort" pods, which do not cause the cluster autoscaler to increase resources but instead run only when spare resources are available.

Pods with priority lower than the cutoff value do not cause the cluster to scale up or prevent the cluster from scaling down. No new nodes are added to run the pods, and nodes running these pods might be deleted to free resources.

Cluster autoscaling is supported for the platforms that have machine API available on it.

////
Default priority cutoff is 0. It can be changed using `--expendable-pods-priority-cutoff` flag, but we discourage it. cluster autoscaler also doesn't trigger scale-up if an unschedulable Pod is already waiting for a lower priority Pod preemption.
////

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: REFERENCE
[id="cluster-autoscaler-cr_{context}"]
= Cluster autoscaler resource definition

This `ClusterAutoscaler` resource definition shows the parameters and sample values for the cluster autoscaler.


[source,yaml]
----
apiVersion: "autoscaling.openshift.io/v1"
kind: "ClusterAutoscaler"
metadata:
  name: "default"
spec:
  podPriorityThreshold: -10 <1>
  resourceLimits:
    maxNodesTotal: 24 <2>
    cores:
      min: 8 <3>
      max: 128 <4>
    memory:
      min: 4 <5>
      max: 256 <6>
    gpus:
      - type: nvidia.com/gpu <7>
        min: 0 <8>
        max: 16 <9>
      - type: amd.com/gpu
        min: 0
        max: 4
  logVerbosity: 4 <10>
  scaleDown: <11>
    enabled: true <12>
    delayAfterAdd: 10m <13>
    delayAfterDelete: 5m <14>
    delayAfterFailure: 30s <15>
    unneededTime: 5m <16>
    utilizationThreshold: "0.4" <17>
----
<1> Specify the priority that a pod must exceed to cause the cluster autoscaler to deploy additional nodes. Enter a 32-bit integer value. The `podPriorityThreshold` value is compared to the value of the `PriorityClass` that you assign to each pod.
<2> Specify the maximum number of nodes to deploy. This value is the total number of machines that are deployed in your cluster, not just the ones that the autoscaler controls. Ensure that this value is large enough to account for all of your control plane and compute machines and the total number of replicas that you specify in your `MachineAutoscaler` resources.
<3> Specify the minimum number of cores to deploy in the cluster.
<4> Specify the maximum number of cores to deploy in the cluster.
<5> Specify the minimum amount of memory, in GiB, in the cluster.
<6> Specify the maximum amount of memory, in GiB, in the cluster.
<7> Optional: Specify the type of GPU node to deploy. Only `nvidia.com/gpu` and `amd.com/gpu` are valid types.
<8> Specify the minimum number of GPUs to deploy in the cluster.
<9> Specify the maximum number of GPUs to deploy in the cluster.
<10> Specify the logging verbosity level between `0` and `10`. The following log level thresholds are provided for guidance:
+
--
* `1`: (Default) Basic information about changes.
* `4`: Debug-level verbosity for troubleshooting typical issues.
* `9`: Extensive, protocol-level debugging information.
--
+
If you do not specify a value, the default value of `1` is used.
<11> In this section, you can specify the period to wait for each action by using any valid link:https://golang.org/pkg/time/#ParseDuration[ParseDuration] interval, including `ns`, `us`, `ms`, `s`, `m`, and `h`.
<12> Specify whether the cluster autoscaler can remove unnecessary nodes.
<13> Optional: Specify the period to wait before deleting a node after a node has recently been _added_. If you do not specify a value, the default value of `10m` is used.
<14> Optional: Specify the period to wait before deleting a node after a node has recently been _deleted_. If you do not specify a value, the default value of `0s` is used.
<15> Optional: Specify the period to wait before deleting a node after a scale down failure occurred. If you do not specify a value, the default value of `3m` is used.
<16> Optional: Specify a period of time before an unnecessary node is eligible for deletion. If you do not specify a value, the default value of `10m` is used.
<17> Optional:  Specify the _node utilization level_. Nodes below this utilization level are eligible for deletion. If you do not specify a value, the default value of `10m` is used.. The node utilization level is the sum of the requested resources divided by the allocated resources for the node, and must be a value greater than `"0"` but less than `"1"`. If you do not specify a value, the cluster autoscaler uses a default value of `"0.5"`, which corresponds to 50% utilization. This value must be expressed as a string.
// Might be able to add a formula to show this visually, but need to look into asciidoc math formatting and what our tooling supports.

[NOTE]
====
When performing a scaling operation, the cluster autoscaler remains within the ranges set in the `ClusterAutoscaler` resource definition, such as the minimum and maximum number of cores to deploy or the amount of memory in the cluster. However, the cluster autoscaler does not correct the current values in your cluster to be within those ranges.

The minimum and maximum CPUs, memory, and GPU values are determined by calculating those resources on all nodes in the cluster, even if the cluster autoscaler does not manage the nodes. For example, the control plane nodes are considered in the total memory in the cluster, even though the cluster autoscaler does not manage the control plane nodes.
====

:leveloffset!:
:FeatureName: cluster autoscaler
:FeatureResourceName: ClusterAutoscaler
:leveloffset: +2

// Be sure to set the :FeatureName: and :FeatureResourceName: values in each assembly on the lines before
// the include statement for this module. For example, add the following lines to the assembly:
// :FeatureName: cluster autoscaler
// :FeatureResourceName: ClusterAutoscaler
//
// Module included in the following assemblies:
//
// * machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="{FeatureResourceName}-deploying_{context}"]
= Deploying a {FeatureName}

To deploy a {FeatureName}, you create an instance of the `{FeatureResourceName}` resource.

.Procedure

. Create a YAML file for a `{FeatureResourceName}` resource that contains the custom resource definition.

. Create the custom resource in the cluster by running the following command:
+
[source,terminal]
----
$ oc create -f <filename>.yaml <1>
----
<1> `<filename>` is the name of the custom resource file.

// Undefine attributes, so that any mistakes are easily spotted
:!FeatureName:
:!FeatureResourceName:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: CONCEPT
[id="machine-autoscaler-about_{context}"]
= About the machine autoscaler

The machine autoscaler adjusts the number of Machines in the compute machine sets that you deploy in an {product-title} cluster. You can scale both the default `worker` compute machine set and any other compute machine sets that you create. The machine autoscaler makes more Machines when the cluster runs out of resources to support more deployments. Any changes to the values in `MachineAutoscaler` resources, such as the minimum or maximum number of instances, are immediately applied to the compute machine set they target.

[IMPORTANT]
====
You must deploy a machine autoscaler for the cluster autoscaler to scale your machines. The cluster autoscaler uses the annotations on compute machine sets that the machine autoscaler sets to determine the resources that it can scale. If you define a cluster autoscaler without also defining machine autoscalers, the cluster autoscaler will never scale your cluster.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: REFERENCE
[id="machine-autoscaler-cr_{context}"]
= Machine autoscaler resource definition

This `MachineAutoscaler` resource definition shows the parameters and sample values for the machine autoscaler.


[source,yaml]
----
apiVersion: "autoscaling.openshift.io/v1beta1"
kind: "MachineAutoscaler"
metadata:
  name: "worker-us-east-1a" <1>
  namespace: "openshift-machine-api"
spec:
  minReplicas: 1 <2>
  maxReplicas: 12 <3>
  scaleTargetRef: <4>
    apiVersion: machine.openshift.io/v1beta1
    kind: MachineSet <5>
    name: worker-us-east-1a <6>
----
<1> Specify the machine autoscaler name. To make it easier to identify which compute machine set this machine autoscaler scales, specify or include the name of the compute machine set to scale. The compute machine set name takes the following form: `<clusterid>-<machineset>-<region>`.
<2> Specify the minimum number machines of the specified type that must remain in the specified zone after the cluster autoscaler initiates cluster scaling. If running in AWS, GCP, Azure, {rh-openstack}, or vSphere, this value can be set to `0`. For other providers, do not set this value to `0`.
+
You can save on costs by setting this value to `0` for use cases such as running expensive or limited-usage hardware that is used for specialized workloads, or by scaling a compute machine set with extra large machines. The cluster autoscaler scales the compute machine set down to zero if the machines are not in use.
+
[IMPORTANT]
====
Do not set the `spec.minReplicas` value to `0` for the three compute machine sets that are created during the {product-title} installation process for an installer provisioned infrastructure.
====
<3> Specify the maximum number machines of the specified type that the cluster autoscaler can deploy in the specified zone after it initiates cluster scaling. Ensure that the `maxNodesTotal` value in the `ClusterAutoscaler` resource definition is large enough to allow the machine autoscaler to deploy this number of machines.
<4> In this section, provide values that describe the existing compute machine set to scale.
<5> The `kind` parameter value is always `MachineSet`.
<6> The `name` value must match the name of an existing compute machine set, as shown in the `metadata.name` parameter value.

:leveloffset!:
:FeatureName: machine autoscaler
:FeatureResourceName: MachineAutoscaler
:leveloffset: +2

// Be sure to set the :FeatureName: and :FeatureResourceName: values in each assembly on the lines before
// the include statement for this module. For example, add the following lines to the assembly:
// :FeatureName: cluster autoscaler
// :FeatureResourceName: ClusterAutoscaler
//
// Module included in the following assemblies:
//
// * machine_management/applying-autoscaling.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="{FeatureResourceName}-deploying_{context}"]
= Deploying a {FeatureName}

To deploy a {FeatureName}, you create an instance of the `{FeatureResourceName}` resource.

.Procedure

. Create a YAML file for a `{FeatureResourceName}` resource that contains the custom resource definition.

. Create the custom resource in the cluster by running the following command:
+
[source,terminal]
----
$ oc create -f <filename>.yaml <1>
----
<1> `<filename>` is the name of the custom resource file.

// Undefine attributes, so that any mistakes are easily spotted
:!FeatureName:
:!FeatureResourceName:

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
//
// * nodes/clusters/nodes-cluster-cgroups-2.adoc
// * post_installation_configuration/machine-configuration-tasks.adoc

:post:

:_mod-docs-content-type: PROCEDURE
[id="nodes-clusters-cgroups-2_{context}"]
= Configuring Linux cgroup

As of {product-title} 4.14, {product-title} uses link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v2.html[Linux control group version 2] (cgroup v2) in your cluster. If you are using cgroup v1 on {product-title} 4.13 or earlier, migrating to {product-title} 4.14 will not automatically update your cgroup configuration to version 2. A fresh installation of {product-title} 4.14 will use cgroup v2 by default. However, you can enable link:https://www.kernel.org/doc/html/latest/admin-guide/cgroup-v1/index.html[Linux control group version 1] (cgroup v1) upon installation. Enabling cgroup v1 in {product-title} disables all cgroup v2 controllers and hierarchies in your cluster.

cgroup v2 is the current version of the Linux cgroup API. cgroup v2 offers several improvements over cgroup v1, including a unified hierarchy, safer sub-tree delegation, new features such as link:https://www.kernel.org/doc/html/latest/accounting/psi.html[Pressure Stall Information], and enhanced resource management and isolation.

You can change between cgroup v1 and cgroup v2, as needed.  For more information, see "Configuring the Linux cgroup on your nodes" in the "Additional resources" of this section.


[NOTE]
====
Currently, disabling CPU load balancing is not supported by cgroup v2. As a result, you might not get the desired behavior from performance profiles if you have cgroup v2 enabled. Enabling cgroup v2 is not recommended if you are using performance profiles.
====

.Prerequisites
* You have a running {product-title} cluster that uses version 4.12 or later.
* You are logged in to the cluster as a user with administrative privileges.

.Procedure

. Enable cgroup v1 on nodes:

.. Edit the `node.config` object:
+
[source,terminal]
----
$ oc edit nodes.config/cluster
----

.. Add `spec.cgroupMode: "v1"`:
+
.Example `node.config` object
[source,yaml]
----
apiVersion: config.openshift.io/v2
kind: Node
metadata:
  annotations:
    include.release.openshift.io/ibm-cloud-managed: "true"
    include.release.openshift.io/self-managed-high-availability: "true"
    include.release.openshift.io/single-node-developer: "true"
    release.openshift.io/create-only: "true"
  creationTimestamp: "2022-07-08T16:02:51Z"
  generation: 1
  name: cluster
  ownerReferences:
  - apiVersion: config.openshift.io/v2
    kind: ClusterVersion
    name: version
    uid: 36282574-bf9f-409e-a6cd-3032939293eb
  resourceVersion: "1865"
  uid: 0c0f7a4c-4307-4187-b591-6155695ac85b
spec:
  cgroupMode: "v1" <1>
...
----
<1> Enables cgroup v1.


.Verification

. Check the machine configs to see that the new machine configs were added:
+
[source,terminal]
----
$ oc get mc
----
+
.Example output
[source,terminal]
----
NAME                                               GENERATEDBYCONTROLLER                      IGNITIONVERSION   AGE
00-master                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
00-worker                                          52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-master-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-container-runtime                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
01-worker-kubelet                                  52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
97-master-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-generated-kubelet                        52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-master-ssh                                                                                 3.2.0             40m
99-worker-generated-registries                     52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
99-worker-ssh                                                                                 3.2.0             40m
rendered-master-23d4317815a5f854bd3553d689cfe2e9   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s <1>
rendered-master-23e785de7587df95a4b517e0647e5ab7   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-5d596d9293ca3ea80c896a1191735bb1   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             33m
rendered-worker-dcc7f1b92892d34db74d6832bcc9ccd4   52dd3ba6a9a527fc3ab42afac8d12b693534c8c9   3.2.0             10s
----
<1> New machine configs are created, as expected.

. Check that the new `kernelArguments` were added to the new machine configs:
+
[source,terminal]
----
$ oc describe mc <name>
----
+
.Example output for cgroup v1
[source,terminal]
----
apiVersion: machineconfiguration.openshift.io/v2
kind: MachineConfig
metadata:
  labels:
    machineconfiguration.openshift.io/role: worker
  name: 05-worker-kernelarg-selinuxpermissive
spec:
  kernelArguments:
  - systemd_unified_cgroup_hierarchy=1 <1>
  - cgroup_no_v1="all" <2>
  - psi=1 <3>
----
<1> Enables cgroup v1 in systemd.
<2> Disables cgroup v2.
<3> Enables the Linux Pressure Stall Information (PSI) feature.

. Check the nodes to see that scheduling on the nodes is disabled. This indicates that the change is being applied:
+
[source,terminal]
----
$ oc get nodes
----
+
.Example output
[source,terminal]
----
NAME                                       STATUS                     ROLES    AGE   VERSION
ci-ln-fm1qnwt-72292-99kt6-master-0         Ready,SchedulingDisabled   master   58m   v1.27.3
ci-ln-fm1qnwt-72292-99kt6-master-1         Ready                      master   58m   v1.27.3
ci-ln-fm1qnwt-72292-99kt6-master-2         Ready                      master   58m   v1.27.3
ci-ln-fm1qnwt-72292-99kt6-worker-a-h5gt4   Ready,SchedulingDisabled   worker   48m   v1.27.3
ci-ln-fm1qnwt-72292-99kt6-worker-b-7vtmd   Ready                      worker   48m   v1.27.3
ci-ln-fm1qnwt-72292-99kt6-worker-c-rhzkv   Ready                      worker   48m   v1.27.3
----

. After a node returns to the `Ready` state, start a debug session for that node:
+
[source,terminal]
----
$ oc debug node/<node_name>
----

. Set `/host` as the root directory within the debug shell:
+
[source,terminal]
----
sh-4.4# chroot /host
----

. Check that the `sys/fs/cgroup/cgroup2fs` file is present on your nodes. This file is created by cgroup v1:
+
[source,terminal]
----
$ stat -c %T -f /sys/fs/cgroup
----
+
.Example output
[source,terminal]
----
cgroup2fs
----

:!post:

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../nodes/clusters/nodes-cluster-cgroups-2.adoc#nodes-cluster-cgroups-2[Configuring the Linux cgroup version on your nodes]

[id="post-install-tp-tasks"]
== Enabling Technology Preview features using FeatureGates

You can turn on a subset of the current Technology Preview features on for all nodes in the cluster by editing the `FeatureGate` custom resource (CR).

:leveloffset: +2

// Module included in the following assemblies:
//
// nodes/clusters/nodes-cluster-enabling-features.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-cluster-enabling-features-about_{context}"]
= Understanding feature gates

You can use the `FeatureGate` custom resource (CR) to enable specific feature sets in your cluster. A feature set is a collection of {product-title} features that are not enabled by default.

You can activate the following feature set by using the `FeatureGate` CR:

* `TechPreviewNoUpgrade`. This feature set is a subset of the current Technology Preview features. This feature set allows you to enable these Technology Preview features on test clusters, where you can fully test them, while leaving the features disabled on production clusters.
+
[WARNING]
====
Enabling the `TechPreviewNoUpgrade` feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
====
+
The following Technology Preview features are enabled by this feature set:
+
--
** External cloud providers. Enables support for external cloud providers for clusters on vSphere, AWS, Azure, and GCP. Support for OpenStack is GA. This is an internal feature that most users do not need to interact with. (`ExternalCloudProvider`)
** Shared Resources CSI Driver in OpenShift Builds. Enables the Container Storage Interface (CSI). (`CSIDriverSharedResource`)
** Swap memory on nodes. Enables swap memory use for {product-title} workloads on a per-node basis. (`NodeSwap`)
** OpenStack Machine API Provider. This gate has no effect and is planned to be removed from this feature set in a future release. (`MachineAPIProviderOpenStack`)
** Insights Operator. Enables the Insights Operator, which gathers {product-title} configuration data and sends it to Red Hat. (`InsightsConfigAPI`)
** Retroactive Default Storage Class. Enables {product-title} to retroactively assign the default storage class to PVCs if there was no default storage class when the PVC was created.(`RetroactiveDefaultStorageClass`)
** Dynamic Resource Allocation API. Enables a new API for requesting and sharing resources between pods and containers. This is an internal feature that most users do not need to interact with. (`DynamicResourceAllocation`)
** Pod security admission enforcement. Enables the restricted enforcement mode for pod security admission. Instead of only logging a warning, pods are rejected if they violate pod security standards. (`OpenShiftPodSecurityAdmission`)
** StatefulSet pod availability upgrading limits. Enables users to define the maximum number of statefulset pods unavailable during updates which reduces application downtime. (`MaxUnavailableStatefulSet`)
** Admin Network Policy and Baseline Admin Network Policy. Enables `AdminNetworkPolicy` and `BaselineAdminNetworkPolicy` resources, which are part of the Network Policy V2 API, in clusters running the OVN-Kubernetes CNI plugin. Cluster administrators can apply cluster-scoped policies and safeguards for an entire cluster before namespaces are created. Network administrators can secure clusters by enforcing network traffic controls that cannot be overridden by users. Network administrators can enforce optional baseline network traffic controls that can be overridden by users in the cluster, if necessary. Currently, these APIs support only expressing policies for intra-cluster traffic. (`AdminNetworkPolicy`)
** `MatchConditions` is a list of conditions that must be met for a request to be sent to this webhook. Match conditions filter requests that have already been matched by the rules, namespaceSelector, and objectSelector. An empty list of `matchConditions` matches all requests. (`admissionWebhookMatchConditions`)
** Gateway API. To enable the {product-title} Gateway API, set the value of the `enabled` field to `true` in the `techPreview.gatewayAPI` specification of the `ServiceMeshControlPlane` resource.(`gateGatewayAPI`)
** `sigstoreImageVerification`
** `gcpLabelsTags`
** `vSphereStaticIPs`
** `routeExternalCertificate`
** `automatedEtcdBackup`
--

////
Do not document per Derek Carr: https://github.com/openshift/api/pull/370#issuecomment-510632939
|`CustomNoUpgrade` ^[2]^
|Allows the enabling or disabling of any feature. Turning on this feature set on is not supported, cannot be undone, and prevents upgrades.

[.small]
--
1.
2. If you use the `CustomNoUpgrade` feature set to disable a feature that appears in the web console, you might see that feature, but
no objects are listed. For example, if you disable builds, you can see the *Builds* tab in the web console, but there are no builds present. If you attempt to use commands associated with a disabled feature, such as `oc start-build`, {product-title} displays an error.

[NOTE]
====
If you disable a feature that any application in the cluster relies on, the application might not
function properly, depending upon the feature disabled and how the application uses that feature.
====
////

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/clusters/nodes-cluster-enabling-features.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-cluster-enabling-features-console_{context}"]
= Enabling feature sets using the web console

You can use the {product-title} web console to enable feature sets for all of the nodes in a cluster by editing the `FeatureGate` custom resource (CR).

.Procedure

To enable feature sets:

. In the {product-title} web console, switch to the *Administration* -> *Custom Resource Definitions* page.

. On the *Custom Resource Definitions* page, click *FeatureGate*.

. On the *Custom Resource Definition Details* page, click the *Instances* tab.

. Click the *cluster* feature gate, then click the *YAML* tab.

. Edit the *cluster* instance to add specific feature sets:
+
[WARNING]
====
Enabling the `TechPreviewNoUpgrade` feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
====

+
.Sample Feature Gate custom resource
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <1>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <2>
----
+
--
<1> The name of the `FeatureGate` CR must be `cluster`.
<2> Add the feature set that you want to enable:
* `TechPreviewNoUpgrade` enables specific Technology Preview features.
--
+
After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.

.Verification

// Text snippet included in the following modules:
//
// * modules/clusters/nodes-cluster-enabling-features-install.adoc
// * modules/clusters/nodes-cluster-enabling-features-console.adoc
// * modules/nodes-cluster-enabling-features-cli.adoc

:_mod-docs-content-type: SNIPPET


You can verify that the feature gates are enabled by looking at the `kubelet.conf` file on a node after the nodes return to the ready state.

. From the *Administrator* perspective in the web console, navigate to *Compute* -> *Nodes*.

. Select a node.

. In the *Node details* page, click *Terminal*.

. In the terminal window, change your root directory to `/host`:
+
[source,terminal]
----
sh-4.2# chroot /host
----

. View the `kubelet.conf` file:
+
[source,terminal]
----
sh-4.2# cat /etc/kubernetes/kubelet.conf
----
+
.Sample output
+
[source,terminal]
----
# ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...
----
+
The features that are listed as `true` are enabled on your cluster.
+
[NOTE]
====
The features listed vary depending upon the {product-title} version.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/cluster/nodes-cluster-enabling-features.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-cluster-enabling-features-cli_{context}"]
= Enabling feature sets using the CLI

You can use the OpenShift CLI (`oc`) to enable feature sets for all of the nodes in a cluster by editing the `FeatureGate` custom resource (CR).

.Prerequisites

* You have installed the OpenShift CLI (`oc`).

.Procedure

To enable feature sets:

. Edit the `FeatureGate` CR named `cluster`:
+
[source,terminal]
----
$ oc edit featuregate cluster
----
+
[WARNING]
====
Enabling the `TechPreviewNoUpgrade` feature set on your cluster cannot be undone and prevents minor version updates. You should not enable this feature set on production clusters.
====

+
.Sample FeatureGate custom resource
[source,yaml]
----
apiVersion: config.openshift.io/v1
kind: FeatureGate
metadata:
  name: cluster <1>
# ...
spec:
  featureSet: TechPreviewNoUpgrade <2>
----
+
--
<1> The name of the `FeatureGate` CR must be `cluster`.
<2> Add the feature set that you want to enable:
* `TechPreviewNoUpgrade` enables specific Technology Preview features.
--
+
After you save the changes, new machine configs are created, the machine config pools are updated, and scheduling on each node is disabled while the change is being applied.

.Verification

// Text snippet included in the following modules:
//
// * modules/clusters/nodes-cluster-enabling-features-install.adoc
// * modules/clusters/nodes-cluster-enabling-features-console.adoc
// * modules/nodes-cluster-enabling-features-cli.adoc

:_mod-docs-content-type: SNIPPET


You can verify that the feature gates are enabled by looking at the `kubelet.conf` file on a node after the nodes return to the ready state.

. From the *Administrator* perspective in the web console, navigate to *Compute* -> *Nodes*.

. Select a node.

. In the *Node details* page, click *Terminal*.

. In the terminal window, change your root directory to `/host`:
+
[source,terminal]
----
sh-4.2# chroot /host
----

. View the `kubelet.conf` file:
+
[source,terminal]
----
sh-4.2# cat /etc/kubernetes/kubelet.conf
----
+
.Sample output
+
[source,terminal]
----
# ...
featureGates:
  InsightsOperatorPullingSCA: true,
  LegacyNodeRoleBehavior: false
# ...
----
+
The features that are listed as `true` are enabled on your cluster.
+
[NOTE]
====
The features listed vary depending upon the {product-title} version.
====

:leveloffset!:

[id="post-install-etcd-tasks"]
== etcd tasks
Back up etcd, enable or disable etcd encryption, or defragment etcd data.

:leveloffset: +2

// Module included in the following assemblies:
//
// * security/encrypting-etcd.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: CONCEPT
[id="about-etcd_{context}"]
= About etcd encryption

By default, etcd data is not encrypted in {product-title}. You can enable etcd encryption for your cluster to provide an additional layer of data security. For example, it can help protect the loss of sensitive data if an etcd backup is exposed to the incorrect parties.

When you enable etcd encryption, the following OpenShift API server and Kubernetes API server resources are encrypted:

* Secrets
* Config maps
* Routes
* OAuth access tokens
* OAuth authorize tokens

When you enable etcd encryption, encryption keys are created. You must have these keys to restore from an etcd backup.

[NOTE]
====
Etcd encryption only encrypts values, not keys. Resource types, namespaces, and object names are unencrypted.

If etcd encryption is enabled during a backup, the `__static_kuberesources_<datetimestamp>.tar.gz__` file contains the encryption keys for the etcd snapshot. For security reasons, store this file separately from the etcd snapshot. However, this file is required to restore a previous state of etcd from the respective etcd snapshot.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * security/encrypting-etcd.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: CONCEPT
[id="etcd-encryption-types_{context}"]
= Supported encryption types

The following encryption types are supported for encrypting etcd data in {product-title}:

AES-CBC:: Uses AES-CBC with PKCS#7 padding and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.

AES-GCM:: Uses AES-GCM with a random nonce and a 32 byte key to perform the encryption. The encryption keys are rotated weekly.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * security/encrypting-etcd.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="enabling-etcd-encryption_{context}"]
= Enabling etcd encryption

You can enable etcd encryption to encrypt sensitive resources in your cluster.

[WARNING]
====
Do not back up etcd resources until the initial encryption process is completed. If the encryption process is not completed, the backup might be only partially encrypted.

After you enable etcd encryption, several changes can occur:

* The etcd encryption might affect the memory consumption of a few resources.
* You might notice a transient affect on backup performance because the leader must serve the backup.
* A disk I/O can affect the node that receives the backup state.
====

You can encrypt the etcd database in either AES-GCM or AES-CBC encryption.

[NOTE]
====
To migrate your etcd database from one encryption type to the other, you can modify the API server's `spec.encryption.type` field. Migration of the etcd data to the new encryption type occurs automatically.
====

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Modify the `APIServer` object:
+
[source,terminal]
----
$ oc edit apiserver
----

. Set the `spec.encryption.type` field to `aesgcm` or `aescbc`:
+
[source,yaml]
----
spec:
  encryption:
    type: aesgcm <1>
----
<1> Set to `aesgcm` for AES-GCM encryption or `aescbc` for AES-CBC encryption.

. Save the file to apply the changes.
+
The encryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of the etcd database.

. Verify that etcd encryption was successful.

.. Review the `Encrypted` status condition for the OpenShift API server to verify that its resources were successfully encrypted:
+
[source,terminal]
----
$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
The output shows `EncryptionCompleted` upon successful encryption:
+
[source,terminal]
----
EncryptionCompleted
All resources encrypted: routes.route.openshift.io
----
+
If the output shows `EncryptionInProgress`, encryption is still in progress. Wait a few minutes and try again.

.. Review the `Encrypted` status condition for the Kubernetes API server to verify that its resources were successfully encrypted:
+
[source,terminal]
----
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
The output shows `EncryptionCompleted` upon successful encryption:
+
[source,terminal]
----
EncryptionCompleted
All resources encrypted: secrets, configmaps
----
+
If the output shows `EncryptionInProgress`, encryption is still in progress. Wait a few minutes and try again.

.. Review the `Encrypted` status condition for the OpenShift OAuth API server to verify that its resources were successfully encrypted:
+
[source,terminal]
----
$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
The output shows `EncryptionCompleted` upon successful encryption:
+
[source,terminal]
----
EncryptionCompleted
All resources encrypted: oauthaccesstokens.oauth.openshift.io, oauthauthorizetokens.oauth.openshift.io
----
+
If the output shows `EncryptionInProgress`, encryption is still in progress. Wait a few minutes and try again.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * security/encrypting-etcd.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="disabling-etcd-encryption_{context}"]
= Disabling etcd encryption

You can disable encryption of etcd data in your cluster.

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Modify the `APIServer` object:
+
[source,terminal]
----
$ oc edit apiserver
----

. Set the `encryption` field type to `identity`:
+
[source,yaml]
----
spec:
  encryption:
    type: identity <1>
----
<1> The `identity` type is the default value and means that no encryption is performed.

. Save the file to apply the changes.
+
The decryption process starts. It can take 20 minutes or longer for this process to complete, depending on the size of your cluster.

. Verify that etcd decryption was successful.

.. Review the `Encrypted` status condition for the OpenShift API server to verify that its resources were successfully decrypted:
+
[source,terminal]
----
$ oc get openshiftapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
The output shows `DecryptionCompleted` upon successful decryption:
+
[source,terminal]
----
DecryptionCompleted
Encryption mode set to identity and everything is decrypted
----
+
If the output shows `DecryptionInProgress`, decryption is still in progress. Wait a few minutes and try again.

.. Review the `Encrypted` status condition for the Kubernetes API server to verify that its resources were successfully decrypted:
+
[source,terminal]
----
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
The output shows `DecryptionCompleted` upon successful decryption:
+
[source,terminal]
----
DecryptionCompleted
Encryption mode set to identity and everything is decrypted
----
+
If the output shows `DecryptionInProgress`, decryption is still in progress. Wait a few minutes and try again.

.. Review the `Encrypted` status condition for the OpenShift OAuth API server to verify that its resources were successfully decrypted:
+
[source,terminal]
----
$ oc get authentication.operator.openshift.io -o=jsonpath='{range .items[0].status.conditions[?(@.type=="Encrypted")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
The output shows `DecryptionCompleted` upon successful decryption:
+
[source,terminal]
----
DecryptionCompleted
Encryption mode set to identity and everything is decrypted
----
+
If the output shows `DecryptionInProgress`, decryption is still in progress. Wait a few minutes and try again.

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * backup_and_restore/control_plane_backup_and_restore/backing-up-etcd.adoc
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="backing-up-etcd-data_{context}"]
= Backing up etcd data

Follow these steps to back up etcd data by creating an etcd snapshot and backing up the resources for the static pods. This backup can be saved and used at a later time if you need to restore etcd.

[IMPORTANT]
====
Only save a backup from a single control plane host. Do not take a backup from each control plane host in the cluster.
====

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.
* You have checked whether the cluster-wide proxy is enabled.
+
[TIP]
====
You can check whether the proxy is enabled by reviewing the output of `oc get proxy cluster -o yaml`. The proxy is enabled if the `httpProxy`, `httpsProxy`, and `noProxy` fields have values set.
====

.Procedure

. Start a debug session as root for a control plane node:
+
[source,terminal]
----
$ oc debug --as-root node/<node_name>
----

. Change your root directory to `/host` in the debug shell:
+
[source,terminal]
----
sh-4.4# chroot /host
----

. If the cluster-wide proxy is enabled, be sure that you have exported the `NO_PROXY`, `HTTP_PROXY`, and `HTTPS_PROXY` environment variables.

. Run the `cluster-backup.sh` script in the debug shell and pass in the location to save the backup to.
+
[TIP]
====
The `cluster-backup.sh` script is maintained as a component of the etcd Cluster Operator and is a wrapper around the `etcdctl snapshot save` command.
====
+
[source,terminal]
----
sh-4.4# /usr/local/bin/cluster-backup.sh /home/core/assets/backup
----
+
.Example script output
[source,terminal]
----
found latest kube-apiserver: /etc/kubernetes/static-pod-resources/kube-apiserver-pod-6
found latest kube-controller-manager: /etc/kubernetes/static-pod-resources/kube-controller-manager-pod-7
found latest kube-scheduler: /etc/kubernetes/static-pod-resources/kube-scheduler-pod-6
found latest etcd: /etc/kubernetes/static-pod-resources/etcd-pod-3
ede95fe6b88b87ba86a03c15e669fb4aa5bf0991c180d3c6895ce72eaade54a1
etcdctl version: 3.4.14
API version: 3.4
{"level":"info","ts":1624647639.0188997,"caller":"snapshot/v3_snapshot.go:119","msg":"created temporary db file","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db.part"}
{"level":"info","ts":"2021-06-25T19:00:39.030Z","caller":"clientv3/maintenance.go:200","msg":"opened snapshot stream; downloading"}
{"level":"info","ts":1624647639.0301006,"caller":"snapshot/v3_snapshot.go:127","msg":"fetching snapshot","endpoint":"https://10.0.0.5:2379"}
{"level":"info","ts":"2021-06-25T19:00:40.215Z","caller":"clientv3/maintenance.go:208","msg":"completed snapshot read; closing"}
{"level":"info","ts":1624647640.6032252,"caller":"snapshot/v3_snapshot.go:142","msg":"fetched snapshot","endpoint":"https://10.0.0.5:2379","size":"114 MB","took":1.584090459}
{"level":"info","ts":1624647640.6047094,"caller":"snapshot/v3_snapshot.go:152","msg":"saved","path":"/home/core/assets/backup/snapshot_2021-06-25_190035.db"}
Snapshot saved at /home/core/assets/backup/snapshot_2021-06-25_190035.db
{"hash":3866667823,"revision":31407,"totalKey":12828,"totalSize":114446336}
snapshot db and kube resources are successfully saved to /home/core/assets/backup
----
+
In this example, two files are created in the `/home/core/assets/backup/` directory on the control plane host:

* `snapshot_<datetimestamp>.db`: This file is the etcd snapshot. The `cluster-backup.sh` script confirms its validity.
* `static_kuberesources_<datetimestamp>.tar.gz`: This file contains the resources for the static pods. If etcd encryption is enabled, it also contains the encryption keys for the etcd snapshot.
+
[NOTE]
====
If etcd encryption is enabled, it is recommended to store this second file separately from the etcd snapshot for security reasons. However, this file is required to restore from the etcd snapshot.

Keep in mind that etcd encryption only encrypts values, not keys. This means that resource types, namespaces, and object names are unencrypted.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc
// * scalability_and_performance/recommended-performance-scale-practices/recommended-etcd-practices.adoc

:_mod-docs-content-type: PROCEDURE
[id="etcd-defrag_{context}"]
= Defragmenting etcd data

For large and dense clusters, etcd can suffer from poor performance if the keyspace grows too large and exceeds the space quota. Periodically maintain and defragment etcd to free up space in the data store. Monitor Prometheus for etcd metrics and defragment it when required; otherwise, etcd can raise a cluster-wide alarm that puts the cluster into a maintenance mode that accepts only key reads and deletes.

Monitor these key metrics:

* `etcd_server_quota_backend_bytes`, which is the current quota limit
* `etcd_mvcc_db_total_size_in_use_in_bytes`, which indicates the actual database usage after a history compaction
* `etcd_mvcc_db_total_size_in_bytes`, which shows the database size, including free space waiting for defragmentation

Defragment etcd data to reclaim disk space after events that cause disk fragmentation, such as etcd history compaction.

History compaction is performed automatically every five minutes and leaves gaps in the back-end database. This fragmented space is available for use by etcd, but is not available to the host file system. You must defragment etcd to make this space available to the host file system.

Defragmentation occurs automatically, but you can also trigger it manually.

[NOTE]
====
Automatic defragmentation is good for most cases, because the etcd operator uses cluster information to determine the most efficient operation for the user.
====

[id="automatic-defrag-etcd-data_{context}"]
== Automatic defragmentation

The etcd Operator automatically defragments disks. No manual intervention is needed.

Verify that the defragmentation process is successful by viewing one of these logs:

* etcd logs
* cluster-etcd-operator pod
* operator status error log

[WARNING]
====
Automatic defragmentation can cause leader election failure in various OpenShift core components, such as the Kubernetes controller manager, which triggers a restart of the failing component. The restart is harmless and either triggers failover to the next running instance or the component resumes work again after the restart.
====

.Example log output for successful defragmentation
[source,terminal]
[subs="+quotes"]
----
etcd member has been defragmented: __<member_name>__, memberID: __<member_id>__
----

.Example log output for unsuccessful defragmentation
[source,terminal]
[subs="+quotes"]
----
failed defrag on member: __<member_name>__, memberID: __<member_id>__: __<error_message>__
----

[id="manual-defrag-etcd-data_{context}"]
== Manual defragmentation

//You can monitor the `etcd_db_total_size_in_bytes` metric to determine whether manual defragmentation is necessary.

A Prometheus alert indicates when you need to use manual defragmentation. The alert is displayed in two cases:

   * When etcd uses more than 50% of its available space for more than 10 minutes
   * When etcd is actively using less than 50% of its total database size for more than 10 minutes

You can also determine whether defragmentation is needed by checking the etcd database size in MB that will be freed by defragmentation with the PromQL expression: `(etcd_mvcc_db_total_size_in_bytes - etcd_mvcc_db_total_size_in_use_in_bytes)/1024/1024`

[WARNING]
====
Defragmenting etcd is a blocking action. The etcd member will not respond until defragmentation is complete. For this reason, wait at least one minute between defragmentation actions on each of the pods to allow the cluster to recover.
====

Follow this procedure to defragment etcd data on each etcd member.

.Prerequisites

* You have access to the cluster as a user with the `cluster-admin` role.

.Procedure

. Determine which etcd member is the leader, because the leader should be defragmented last.

.. Get the list of etcd pods:
+
[source,terminal]
----
$ oc -n openshift-etcd get pods -l k8s-app=etcd -o wide
----
+
.Example output
[source,terminal]
----
etcd-ip-10-0-159-225.example.redhat.com                3/3     Running     0          175m   10.0.159.225   ip-10-0-159-225.example.redhat.com   <none>           <none>
etcd-ip-10-0-191-37.example.redhat.com                 3/3     Running     0          173m   10.0.191.37    ip-10-0-191-37.example.redhat.com    <none>           <none>
etcd-ip-10-0-199-170.example.redhat.com                3/3     Running     0          176m   10.0.199.170   ip-10-0-199-170.example.redhat.com   <none>           <none>
----

.. Choose a pod and run the following command to determine which etcd member is the leader:
+
[source,terminal]
----
$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com etcdctl endpoint status --cluster -w table
----
+
.Example output
[source,terminal]
----
Defaulting container name to etcdctl.
Use 'oc describe pod/etcd-ip-10-0-159-225.example.redhat.com -n openshift-etcd' to see all of the containers in this pod.
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
----
+
Based on the `IS LEADER` column of this output, the [x-]`https://10.0.199.170:2379` endpoint is the leader. Matching this endpoint with the output of the previous step, the pod name of the leader is `etcd-ip-10-0-199-170.example.redhat.com`.

. Defragment an etcd member.

.. Connect to the running etcd container, passing in the name of a pod that is _not_ the leader:
+
[source,terminal]
----
$ oc rsh -n openshift-etcd etcd-ip-10-0-159-225.example.redhat.com
----

.. Unset the `ETCDCTL_ENDPOINTS` environment variable:
+
[source,terminal]
----
sh-4.4# unset ETCDCTL_ENDPOINTS
----

.. Defragment the etcd member:
+
[source,terminal]
----
sh-4.4# etcdctl --command-timeout=30s --endpoints=https://localhost:2379 defrag
----
+
.Example output
[source,terminal]
----
Finished defragmenting etcd member[https://localhost:2379]
----
+
If a timeout error occurs, increase the value for `--command-timeout` until the command succeeds.

.. Verify that the database size was reduced:
+
[source,terminal]
----
sh-4.4# etcdctl endpoint status -w table --cluster
----
+
.Example output
[source,terminal]
----
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|         ENDPOINT          |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|  https://10.0.191.37:2379 | 251cd44483d811c3 |   3.5.9 |  104 MB |     false |      false |         7 |      91624 |              91624 |        |
| https://10.0.159.225:2379 | 264c7c58ecbdabee |   3.5.9 |   41 MB |     false |      false |         7 |      91624 |              91624 |        | <1>
| https://10.0.199.170:2379 | 9ac311f93915cc79 |   3.5.9 |  104 MB |      true |      false |         7 |      91624 |              91624 |        |
+---------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
----
This example shows that the database size for this etcd member is now 41 MB as opposed to the starting size of 104 MB.

.. Repeat these steps to connect to each of the other etcd members and defragment them. Always defragment the leader last.
+
Wait at least one minute between defragmentation actions to allow the etcd pod to recover. Until the etcd pod recovers, the etcd member will not respond.

. If any `NOSPACE` alarms were triggered due to the space quota being exceeded, clear them.

.. Check if there are any `NOSPACE` alarms:
+
[source,terminal]
----
sh-4.4# etcdctl alarm list
----
+
.Example output
[source,terminal]
----
memberID:12345678912345678912 alarm:NOSPACE
----

.. Clear the alarms:
+
[source,terminal]
----
sh-4.4# etcdctl alarm disarm
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * disaster_recovery/scenario-2-restoring-cluster-state.adoc
// * post_installation_configuration/cluster-tasks.adoc


:_mod-docs-content-type: PROCEDURE
[id="dr-scenario-2-restoring-cluster-state_{context}"]
= Restoring to a previous cluster state

You can use a saved etcd backup to restore a previous cluster state or restore a cluster that has lost the majority of control plane hosts.

[NOTE]
====
If your cluster uses a control plane machine set, see "Troubleshooting the control plane machine set" for a more simple etcd recovery procedure.
====

[IMPORTANT]
====
When you restore your cluster, you must use an etcd backup that was taken from the same z-stream release. For example, an {product-title} 4.7.2 cluster must use an etcd backup that was taken from 4.7.2.
====

.Prerequisites

* Access to the cluster as a user with the `cluster-admin` role through a certificate-based `kubeconfig` file, like the one that was used during installation.
* A healthy control plane host to use as the recovery host.
* SSH access to control plane hosts.
* A backup directory containing both the etcd snapshot and the resources for the static pods, which were from the same backup. The file names in the directory must be in the following formats: `snapshot_<datetimestamp>.db` and `static_kuberesources_<datetimestamp>.tar.gz`.

[IMPORTANT]
====
For non-recovery control plane nodes, it is not required to establish SSH connectivity or to stop the static pods. You can delete and recreate other non-recovery, control plane machines, one by one.
====

.Procedure

. Select a control plane host to use as the recovery host. This is the host that you will run the restore operation on.

. Establish SSH connectivity to each of the control plane nodes, including the recovery host.
+
The Kubernetes API server becomes inaccessible after the restore process starts, so you cannot access the control plane nodes. For this reason, it is recommended to establish SSH connectivity to each control plane host in a separate terminal.
+
[IMPORTANT]
====
If you do not complete this step, you will not be able to access the control plane hosts to complete the restore procedure, and you will be unable to recover your cluster from this state.
====

. Copy the etcd backup directory to the recovery control plane host.
+
This procedure assumes that you copied the `backup` directory containing the etcd snapshot and the resources for the static pods to the `/home/core/` directory of your recovery control plane host.

. Stop the static pods on any other control plane nodes.
+
[NOTE]
====
You do not need to stop the static pods on the recovery host.
====

.. Access a control plane host that is not the recovery host.

.. Move the existing etcd pod file out of the kubelet manifest directory:
+
[source,terminal]
----
$ sudo mv /etc/kubernetes/manifests/etcd-pod.yaml /tmp
----

.. Verify that the etcd pods are stopped.
+
[source,terminal]
----
$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"
----
+
The output of this command should be empty. If it is not empty, wait a few minutes and check again.

.. Move the existing Kubernetes API server pod file out of the kubelet manifest directory:
+
[source,terminal]
----
$ sudo mv /etc/kubernetes/manifests/kube-apiserver-pod.yaml /tmp
----

.. Verify that the Kubernetes API server pods are stopped.
+
[source,terminal]
----
$ sudo crictl ps | grep kube-apiserver | egrep -v "operator|guard"
----
+
The output of this command should be empty. If it is not empty, wait a few minutes and check again.

.. Move the etcd data directory to a different location:
+
[source,terminal]
----
$ sudo mv /var/lib/etcd/ /tmp
----

.. Repeat this step on each of the other control plane hosts that is not the recovery host.

. Access the recovery control plane host.

. If the cluster-wide proxy is enabled, be sure that you have exported the `NO_PROXY`, `HTTP_PROXY`, and `HTTPS_PROXY` environment variables.
+
[TIP]
====
You can check whether the proxy is enabled by reviewing the output of `oc get proxy cluster -o yaml`. The proxy is enabled if the `httpProxy`, `httpsProxy`, and `noProxy` fields have values set.
====

. Run the restore script on the recovery control plane host and pass in the path to the etcd backup directory:
+
[source,terminal]
----
$ sudo -E /usr/local/bin/cluster-restore.sh /home/core/backup
----
+
.Example script output
[source,terminal]
----
...stopping kube-scheduler-pod.yaml
...stopping kube-controller-manager-pod.yaml
...stopping etcd-pod.yaml
...stopping kube-apiserver-pod.yaml
Waiting for container etcd to stop
.complete
Waiting for container etcdctl to stop
.............................complete
Waiting for container etcd-metrics to stop
complete
Waiting for container kube-controller-manager to stop
complete
Waiting for container kube-apiserver to stop
..........................................................................................complete
Waiting for container kube-scheduler to stop
complete
Moving etcd data-dir /var/lib/etcd/member to /var/lib/etcd-backup
starting restore-etcd static pod
starting kube-apiserver-pod.yaml
static-pod-resources/kube-apiserver-pod-7/kube-apiserver-pod.yaml
starting kube-controller-manager-pod.yaml
static-pod-resources/kube-controller-manager-pod-7/kube-controller-manager-pod.yaml
starting kube-scheduler-pod.yaml
static-pod-resources/kube-scheduler-pod-8/kube-scheduler-pod.yaml
----
+
[NOTE]
====
The restore process can cause nodes to enter the `NotReady` state if the node certificates were updated after the last etcd backup.
====

. Check the nodes to ensure they are in the `Ready` state.

.. Run the following command:
+
[source,terminal]
----
$ oc get nodes -w
----
+
.Sample output
[source,terminal]
----
NAME                STATUS  ROLES          AGE     VERSION
host-172-25-75-28   Ready   master         3d20h   v1.27.3
host-172-25-75-38   Ready   infra,worker   3d20h   v1.27.3
host-172-25-75-40   Ready   master         3d20h   v1.27.3
host-172-25-75-65   Ready   master         3d20h   v1.27.3
host-172-25-75-74   Ready   infra,worker   3d20h   v1.27.3
host-172-25-75-79   Ready   worker         3d20h   v1.27.3
host-172-25-75-86   Ready   worker         3d20h   v1.27.3
host-172-25-75-98   Ready   infra,worker   3d20h   v1.27.3
----
+
It can take several minutes for all nodes to report their state.

.. If any nodes are in the `NotReady` state, log in to the nodes and remove all of the PEM files from the `/var/lib/kubelet/pki` directory on each node. You can SSH into the nodes or use the terminal window in the web console.
+
[source,terminal]
----
$  ssh -i <ssh-key-path> core@<master-hostname>
----
+
.Sample `pki` directory
[source,terminal]
----
sh-4.4# pwd
/var/lib/kubelet/pki
sh-4.4# ls
kubelet-client-2022-04-28-11-24-09.pem  kubelet-server-2022-04-28-11-24-15.pem
kubelet-client-current.pem              kubelet-server-current.pem
----

. Restart the kubelet service on all control plane hosts.

.. From the recovery host, run the following command:
+
[source,terminal]
----
$ sudo systemctl restart kubelet.service
----

.. Repeat this step on all other control plane hosts.

. Approve the pending CSRs:
+
[NOTE]
====
Clusters with no worker nodes, such as single-node clusters or clusters consisting of three schedulable control plane nodes, will not have any pending CSRs to approve. You can skip all the commands listed in this step.
====

.. Get the list of current CSRs:
+
[source,terminal]
----
$ oc get csr
----
+
.Example output
----
NAME        AGE    SIGNERNAME                                    REQUESTOR                                                                   CONDITION
csr-2s94x   8m3s   kubernetes.io/kubelet-serving                 system:node:<node_name>                                                     Pending <1>
csr-4bd6t   8m3s   kubernetes.io/kubelet-serving                 system:node:<node_name>                                                     Pending <1>
csr-4hl85   13m    kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <2>
csr-zhhhp   3m8s   kubernetes.io/kube-apiserver-client-kubelet   system:serviceaccount:openshift-machine-config-operator:node-bootstrapper   Pending <2>
...
----
<1> A pending kubelet service CSR (for user-provisioned installations).
<2> A pending `node-bootstrapper` CSR.

.. Review the details of a CSR to verify that it is valid:
+
[source,terminal]
----
$ oc describe csr <csr_name> <1>
----
<1> `<csr_name>` is the name of a CSR from the list of current CSRs.

.. Approve each valid `node-bootstrapper` CSR:
+
[source,terminal]
----
$ oc adm certificate approve <csr_name>
----

.. For user-provisioned installations, approve each valid kubelet service CSR:
+
[source,terminal]
----
$ oc adm certificate approve <csr_name>
----

. Verify that the single member control plane has started successfully.

.. From the recovery host, verify that the etcd container is running.
+
[source,terminal]
----
$ sudo crictl ps | grep etcd | egrep -v "operator|etcd-guard"
----
+
.Example output
[source,terminal]
----
3ad41b7908e32       36f86e2eeaaffe662df0d21041eb22b8198e0e58abeeae8c743c3e6e977e8009                                                         About a minute ago   Running             etcd                                          0                   7c05f8af362f0
----

.. From the recovery host, verify that the etcd pod is running.
+
[source,terminal]
----
$ oc -n openshift-etcd get pods -l k8s-app=etcd
----
+
.Example output
[source,terminal]
----
NAME                                             READY   STATUS      RESTARTS   AGE
etcd-ip-10-0-143-125.ec2.internal                1/1     Running     1          2m47s
----
+
If the status is `Pending`, or the output lists more than one running etcd pod, wait a few minutes and check again.

. If you are using the `OVNKubernetes` network plugin, you must restart `ovnkube-controlplane` pods.
.. Delete all of the `ovnkube-controlplane` pods by running the following command:
+
[source,terminal]
----
$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-control-plane
----
.. Verify that all of the `ovnkube-controlplane` pods were redeployed by running the following command:
+
[source,terminal]
----
$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-control-plane
----

. If you are using the OVN-Kubernetes network plugin, restart the Open Virtual Network (OVN) Kubernetes pods on all the nodes one by one. Use the following steps to restart OVN-Kubernetes pods on each node:
+
[IMPORTANT]
====
.Restart OVN-Kubernetes pods in the following order:
. The recovery control plane host
. The other control plane hosts (if available)
. The other nodes
====
+
[NOTE]
====
Validating and mutating admission webhooks can reject pods. If you add any additional webhooks with the `failurePolicy` set to `Fail`, then they can reject pods and the restoration process can fail. You can avoid this by saving and deleting webhooks while restoring the cluster state. After the cluster state is restored successfully, you can enable the webhooks again.

Alternatively, you can temporarily set the `failurePolicy` to `Ignore` while restoring the cluster state. After the cluster state is restored successfully, you can set the `failurePolicy` to `Fail`.
====

.. Remove the northbound database (nbdb) and southbound database (sbdb). Access the recovery host and the remaining control plane nodes by using Secure Shell (SSH) and run the following command:
+
[source,terminal]
----
$ sudo rm -f /var/lib/ovn-ic/etc/*.db
----

.. Restart the OpenVSwitch services. Access the node by using Secure Shell (SSH) and run the following command:
+
[source,terminal]
----
$ sudo systemctl restart ovs-vswitchd ovsdb-server
----

.. Delete the `ovnkube-node` pod on the node by running the following command, replacing `<node>` with the name of the node that you are restarting:
+
[source,terminal]
----
$ oc -n openshift-ovn-kubernetes delete pod -l app=ovnkube-node --field-selector=spec.nodeName==<node>
----
+

.. Verify that the `ovnkube-node` pod is running again with the following command:
+
[source,terminal]
----
$ oc -n openshift-ovn-kubernetes get pod -l app=ovnkube-node --field-selector=spec.nodeName==<node>
----
+
[NOTE]
====
It might take several minutes for the pods to restart.
====

. Delete and re-create other non-recovery, control plane machines, one by one. After the machines are re-created, a new revision is forced and etcd automatically scales up.
+
** If you use a user-provisioned bare metal installation, you can re-create a control plane machine by using the same method that you used to originally create it. For more information, see "Installing a user-provisioned cluster on bare metal".
+
[WARNING]
====
Do not delete and re-create the machine for the recovery host.
====
+
** If you are running installer-provisioned infrastructure, or you used the Machine API to create your machines, follow these steps:
+
[WARNING]
====
Do not delete and re-create the machine for the recovery host.

For bare metal installations on installer-provisioned infrastructure, control plane machines are not re-created. For more information, see "Replacing a bare-metal control plane node".
====
.. Obtain the machine for one of the lost control plane hosts.
+
In a terminal that has access to the cluster as a cluster-admin user, run the following command:
+
[source,terminal]
----
$ oc get machines -n openshift-machine-api -o wide
----
+
Example output:
+
[source,terminal]
----
NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-0                  Running   m4.xlarge   us-east-1   us-east-1a   3h37m   ip-10-0-131-183.ec2.internal   aws:///us-east-1a/i-0ec2782f8287dfb7e   stopped <1>
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running
----
<1> This is the control plane machine for the lost control plane host, `ip-10-0-131-183.ec2.internal`.

.. Save the machine configuration to a file on your file system:
+
[source,terminal]
----
$ oc get machine clustername-8qw5l-master-0 \ <1>
    -n openshift-machine-api \
    -o yaml \
    > new-master-machine.yaml
----
<1> Specify the name of the control plane machine for the lost control plane host.

.. Edit the `new-master-machine.yaml` file that was created in the previous step to assign a new name and remove unnecessary fields.

... Remove the entire `status` section:
+
[source,terminal]
----
status:
  addresses:
  - address: 10.0.131.183
    type: InternalIP
  - address: ip-10-0-131-183.ec2.internal
    type: InternalDNS
  - address: ip-10-0-131-183.ec2.internal
    type: Hostname
  lastUpdated: "2020-04-20T17:44:29Z"
  nodeRef:
    kind: Node
    name: ip-10-0-131-183.ec2.internal
    uid: acca4411-af0d-4387-b73e-52b2484295ad
  phase: Running
  providerStatus:
    apiVersion: awsproviderconfig.openshift.io/v1beta1
    conditions:
    - lastProbeTime: "2020-04-20T16:53:50Z"
      lastTransitionTime: "2020-04-20T16:53:50Z"
      message: machine successfully created
      reason: MachineCreationSucceeded
      status: "True"
      type: MachineCreation
    instanceId: i-0fdb85790d76d0c3f
    instanceState: stopped
    kind: AWSMachineProviderStatus
----

... Change the `metadata.name` field to a new name.
+
It is recommended to keep the same base name as the old machine and change the ending number to the next available number. In this example, `clustername-8qw5l-master-0` is changed to `clustername-8qw5l-master-3`:
+
[source,terminal]
----
apiVersion: machine.openshift.io/v1beta1
kind: Machine
metadata:
  ...
  name: clustername-8qw5l-master-3
  ...
----

... Remove the `spec.providerID` field:
+
[source,terminal]
----
providerID: aws:///us-east-1a/i-0fdb85790d76d0c3f
----

... Remove the `metadata.annotations` and `metadata.generation` fields:
+
[source,terminal]
----
annotations:
  machine.openshift.io/instance-state: running
...
generation: 2
----

... Remove the `metadata.resourceVersion` and `metadata.uid` fields:
+
[source,terminal]
----
resourceVersion: "13291"
uid: a282eb70-40a2-4e89-8009-d05dd420d31a
----

.. Delete the machine of the lost control plane host:
+
[source,terminal]
----
$ oc delete machine -n openshift-machine-api clustername-8qw5l-master-0 <1>
----
<1> Specify the name of the control plane machine for the lost control plane host.

.. Verify that the machine was deleted:
+
[source,terminal]
----
$ oc get machines -n openshift-machine-api -o wide
----
+
Example output:
+
[source,terminal]
----
NAME                                        PHASE     TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running   m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running   m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal   aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-worker-us-east-1a-wbtgd   Running   m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running   m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running   m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running
----

.. Create a machine by using the `new-master-machine.yaml` file:
+
[source,terminal]
----
$ oc apply -f new-master-machine.yaml
----

.. Verify that the new machine has been created:
+
[source,terminal]
----
$ oc get machines -n openshift-machine-api -o wide
----
+
Example output:
+
[source,terminal]
----
NAME                                        PHASE          TYPE        REGION      ZONE         AGE     NODE                           PROVIDERID                              STATE
clustername-8qw5l-master-1                  Running        m4.xlarge   us-east-1   us-east-1b   3h37m   ip-10-0-143-125.ec2.internal   aws:///us-east-1b/i-096c349b700a19631   running
clustername-8qw5l-master-2                  Running        m4.xlarge   us-east-1   us-east-1c   3h37m   ip-10-0-154-194.ec2.internal    aws:///us-east-1c/i-02626f1dba9ed5bba  running
clustername-8qw5l-master-3                  Provisioning   m4.xlarge   us-east-1   us-east-1a   85s     ip-10-0-173-171.ec2.internal    aws:///us-east-1a/i-015b0888fe17bc2c8  running <1>
clustername-8qw5l-worker-us-east-1a-wbtgd   Running        m4.large    us-east-1   us-east-1a   3h28m   ip-10-0-129-226.ec2.internal   aws:///us-east-1a/i-010ef6279b4662ced   running
clustername-8qw5l-worker-us-east-1b-lrdxb   Running        m4.large    us-east-1   us-east-1b   3h28m   ip-10-0-144-248.ec2.internal   aws:///us-east-1b/i-0cb45ac45a166173b   running
clustername-8qw5l-worker-us-east-1c-pkg26   Running        m4.large    us-east-1   us-east-1c   3h28m   ip-10-0-170-181.ec2.internal   aws:///us-east-1c/i-06861c00007751b0a   running
----
<1> The new machine, `clustername-8qw5l-master-3` is being created and is ready after the phase changes from `Provisioning` to `Running`.
+
It might take a few minutes for the new machine to be created. The etcd cluster Operator will automatically sync when the machine or node returns to a healthy state.

.. Repeat these steps for each lost control plane host that is not the recovery host.

. Turn off the quorum guard by entering the following command:
+
[source,terminal]
----
$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": {"useUnsupportedUnsafeNonHANonProductionUnstableEtcd": true}}}'
----
+
This command ensures that you can successfully re-create secrets and roll out the static pods.

. In a separate terminal window within the recovery host, export the recovery `kubeconfig` file by running the following command:
+
[source,terminal]
----
$ export KUBECONFIG=/etc/kubernetes/static-pod-resources/kube-apiserver-certs/secrets/node-kubeconfigs/localhost-recovery.kubeconfig
----

. Force etcd redeployment.
+
In the same terminal window where you exported the recovery `kubeconfig` file, run the following command:
+
[source,terminal]
----
$ oc patch etcd cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge <1>
----
<1> The `forceRedeploymentReason` value must be unique, which is why a timestamp is appended.
+
When the etcd cluster Operator performs a redeployment, the existing nodes are started with new pods similar to the initial bootstrap scale up.

. Turn the quorum guard back on by entering the following command:
+
[source,terminal]
----
$ oc patch etcd/cluster --type=merge -p '{"spec": {"unsupportedConfigOverrides": null}}'
----

. You can verify that the `unsupportedConfigOverrides` section is removed from the object by entering this command:
+
[source,terminal]
----
$ oc get etcd/cluster -oyaml
----

. Verify all nodes are updated to the latest revision.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following command:
+
[source,terminal]
----
$ oc get etcd -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
Review the `NodeInstallerProgressing` status condition for etcd to verify that all nodes are at the latest revision. The output shows `AllNodesAtLatestRevision` upon successful update:
+
[source,terminal]
----
AllNodesAtLatestRevision
3 nodes are at revision 7 <1>
----
<1> In this example, the latest revision number is `7`.
+
If the output includes multiple revision numbers, such as `2 nodes are at revision 6; 1 nodes are at revision 7`, this means that the update is still in progress. Wait a few minutes and try again.

. After etcd is redeployed, force new rollouts for the control plane. The Kubernetes API server will reinstall itself on the other nodes because the kubelet is connected to API servers using an internal load balancer.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following commands.

.. Force a new rollout for the Kubernetes API server:
+
[source,terminal]
----
$ oc patch kubeapiserver cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge
----
+
Verify all nodes are updated to the latest revision.
+
[source,terminal]
----
$ oc get kubeapiserver -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
Review the `NodeInstallerProgressing` status condition to verify that all nodes are at the latest revision. The output shows `AllNodesAtLatestRevision` upon successful update:
+
[source,terminal]
----
AllNodesAtLatestRevision
3 nodes are at revision 7 <1>
----
<1> In this example, the latest revision number is `7`.
+
If the output includes multiple revision numbers, such as `2 nodes are at revision 6; 1 nodes are at revision 7`, this means that the update is still in progress. Wait a few minutes and try again.

.. Force a new rollout for the Kubernetes controller manager:
+
[source,terminal]
----
$ oc patch kubecontrollermanager cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge
----
+
Verify all nodes are updated to the latest revision.
+
[source,terminal]
----
$ oc get kubecontrollermanager -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
Review the `NodeInstallerProgressing` status condition to verify that all nodes are at the latest revision. The output shows `AllNodesAtLatestRevision` upon successful update:
+
[source,terminal]
----
AllNodesAtLatestRevision
3 nodes are at revision 7 <1>
----
<1> In this example, the latest revision number is `7`.
+
If the output includes multiple revision numbers, such as `2 nodes are at revision 6; 1 nodes are at revision 7`, this means that the update is still in progress. Wait a few minutes and try again.

.. Force a new rollout for the Kubernetes scheduler:
+
[source,terminal]
----
$ oc patch kubescheduler cluster -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date --rfc-3339=ns )"'"}}' --type=merge
----
+
Verify all nodes are updated to the latest revision.
+
[source,terminal]
----
$ oc get kubescheduler -o=jsonpath='{range .items[0].status.conditions[?(@.type=="NodeInstallerProgressing")]}{.reason}{"\n"}{.message}{"\n"}'
----
+
Review the `NodeInstallerProgressing` status condition to verify that all nodes are at the latest revision. The output shows `AllNodesAtLatestRevision` upon successful update:
+
[source,terminal]
----
AllNodesAtLatestRevision
3 nodes are at revision 7 <1>
----
<1> In this example, the latest revision number is `7`.
+
If the output includes multiple revision numbers, such as `2 nodes are at revision 6; 1 nodes are at revision 7`, this means that the update is still in progress. Wait a few minutes and try again.

. Verify that all control plane hosts have started and joined the cluster.
+
In a terminal that has access to the cluster as a `cluster-admin` user, run the following command:
+
[source,terminal]
----
$ oc -n openshift-etcd get pods -l k8s-app=etcd
----
+
.Example output
[source,terminal]
----
etcd-ip-10-0-143-125.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-154-194.ec2.internal                2/2     Running     0          9h
etcd-ip-10-0-173-171.ec2.internal                2/2     Running     0          9h
----

To ensure that all workloads return to normal operation following a recovery procedure, restart each pod that stores Kubernetes API information. This includes {product-title} components such as routers, Operators, and third-party components.

[NOTE]
====
On completion of the previous procedural steps, you might need to wait a few minutes for all services to return to their restored state. For example, authentication by using `oc login` might not immediately work until the OAuth server pods are restarted.

Consider using the `system:admin` `kubeconfig` file for immediate authentication. This method basis its authentication on SSL/TLS client certificates as against OAuth tokens. You can authenticate with this file by issuing the following command:

[source,terminal]
----
$ export KUBECONFIG=<installation_directory>/auth/kubeconfig
----

Issue the following command to display your authenticated user name:

[source,terminal]
----
$ oc whoami
----
====

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../installing/installing_bare_metal/installing-bare-metal.adoc#installing-bare-metal[Installing a user-provisioned cluster on bare metal]
* xref:../installing/index.adoc#replacing-a-bare-metal-control-plane-node_ipi-install-expanding[Replacing a bare-metal control plane node]

:leveloffset: +2

// Module included in the following assemblies:
//
// * disaster_recovery/scenario-2-restoring-cluster-state.adoc
// * post_installation_configuration/cluster-tasks.adoc

[id="dr-scenario-cluster-state-issues_{context}"]
= Issues and workarounds for restoring a persistent storage state

If your {product-title} cluster uses persistent storage of any form, a state of the cluster is typically stored outside etcd. It might be an Elasticsearch cluster running in a pod or a database running in a `StatefulSet` object. When you restore from an etcd backup, the status of the workloads in {product-title} is also restored. However, if the etcd snapshot is old, the status might be invalid or outdated.

[IMPORTANT]
====
The contents of persistent volumes (PVs) are never part of the etcd snapshot. When you restore an {product-title} cluster from an etcd snapshot, non-critical workloads might gain access to critical data, or vice-versa.
====

The following are some example scenarios that produce an out-of-date status:

* MySQL database is running in a pod backed up by a PV object. Restoring {product-title} from an etcd snapshot does not bring back the volume on the storage provider, and does not produce a running MySQL pod, despite the pod repeatedly attempting to start. You must manually restore this pod by restoring the volume on the storage provider, and then editing the PV to point to the new volume.

* Pod P1 is using volume A, which is attached to node X. If the etcd snapshot is taken while another pod uses the same volume on node Y, then when the etcd restore is performed, pod P1 might not be able to start correctly due to the volume still being attached to node Y. {product-title} is not aware of the attachment, and does not automatically detach it. When this occurs, the volume must be manually detached from node Y so that the volume can attach on node X, and then pod P1 can start.

* Cloud provider or storage provider credentials were updated after the etcd snapshot was taken. This causes any CSI drivers or Operators that depend on the those credentials to not work. You might have to manually update the credentials required by those drivers or Operators.

* A device is removed or renamed from {product-title} nodes after the etcd snapshot is taken. The Local Storage Operator creates symlinks for each PV that it manages from `/dev/disk/by-id` or `/dev` directories. This situation might cause the local PVs to refer to devices that no longer exist.
+
To fix this problem, an administrator must:

. Manually remove the PVs with invalid devices.
. Remove symlinks from respective nodes.
. Delete `LocalVolume` or `LocalVolumeSet` objects (see _Storage_ -> _Configuring persistent storage_ -> _Persistent storage using local volumes_ -> _Deleting the Local Storage Operator Resources_).

:leveloffset!:

[id="post-install-pod-disruption-budgets"]
== Pod disruption budgets

Understand and configure pod disruption budgets.

:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/nodes-pods-configuring.adoc
// * nodes/nodes-cluster-pods-configuring
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: CONCEPT
[id="nodes-pods-pod-distruption-about_{context}"]
= Understanding how to use pod disruption budgets to specify the number of pods that must be up

A _pod disruption budget_ allows the specification of safety constraints on pods during operations, such as draining a node for maintenance.

`PodDisruptionBudget` is an API object that specifies the minimum number or
percentage of replicas that must be up at a time. Setting these in projects can
be helpful during node maintenance (such as scaling a cluster down or a cluster
upgrade) and is only honored on voluntary evictions (not on node failures).

A `PodDisruptionBudget` object's configuration consists of the following key
parts:

* A label selector, which is a label query over a set of pods.
* An availability level, which specifies the minimum number of pods that must be
 available simultaneously, either:
** `minAvailable` is the number of pods must always be available, even during a disruption.
** `maxUnavailable` is the number of pods can be unavailable during a disruption.

[NOTE]
====
`Available` refers to the number of pods that has condition `Ready=True`.
`Ready=True` refers to the pod that is able to serve requests and should be added to the load balancing pools of all matching services.

A `maxUnavailable` of `0%` or `0` or a `minAvailable` of `100%` or equal to the number of replicas
is permitted but can block nodes from being drained.
====

You can check for pod disruption budgets across all projects with the following:

[source,terminal]
----
$ oc get poddisruptionbudget --all-namespaces
----

.Example output
[source,terminal]
----
NAMESPACE                              NAME                                    MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE
openshift-apiserver                    openshift-apiserver-pdb                 N/A             1                 1                     121m
openshift-cloud-controller-manager     aws-cloud-controller-manager            1               N/A               1                     125m
openshift-cloud-credential-operator    pod-identity-webhook                    1               N/A               1                     117m
openshift-cluster-csi-drivers          aws-ebs-csi-driver-controller-pdb       N/A             1                 1                     121m
openshift-cluster-storage-operator     csi-snapshot-controller-pdb             N/A             1                 1                     122m
openshift-cluster-storage-operator     csi-snapshot-webhook-pdb                N/A             1                 1                     122m
openshift-console                      console                                 N/A             1                 1                     116m
#...
----

The `PodDisruptionBudget` is considered healthy when there are at least
`minAvailable` pods running in the system. Every pod above that limit can be evicted.

[NOTE]
====
Depending on your pod priority and preemption settings,
lower-priority pods might be removed despite their pod disruption budget requirements.
====

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/nodes-pods-configuring.adoc
// * nodes/nodes-cluster-pods-configuring
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="nodes-pods-pod-disruption-configuring_{context}"]
= Specifying the number of pods that must be up with pod disruption budgets

You can use a `PodDisruptionBudget` object to specify the minimum number or
percentage of replicas that must be up at a time.

.Procedure

To configure a pod disruption budget:

. Create a YAML file with the an object definition similar to the following:
+
[source,yaml]
----
apiVersion: policy/v1 <1>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2  <2>
  selector:  <3>
    matchLabels:
      name: my-pod
----
<1> `PodDisruptionBudget` is part of the `policy/v1` API group.
<2> The minimum number of pods that must be available simultaneously. This can
be either an integer or a string specifying a percentage, for example, `20%`.
<3> A label query over a set of resources. The result of `matchLabels` and
 `matchExpressions` are logically conjoined. Leave this parameter blank, for example `selector {}`, to select all pods in the project.
+
Or:
+
[source,yaml]
----
apiVersion: policy/v1 <1>
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  maxUnavailable: 25% <2>
  selector: <3>
    matchLabels:
      name: my-pod
----
<1> `PodDisruptionBudget` is part of the `policy/v1` API group.
<2> The maximum number of pods that can be unavailable simultaneously. This can
be either an integer or a string specifying a percentage, for example, `20%`.
<3> A label query over a set of resources. The result of `matchLabels` and
 `matchExpressions` are logically conjoined. Leave this parameter blank, for example `selector {}`, to select all pods in the project.

. Run the following command to add the object to project:
+
[source,terminal]
----
$ oc create -f </path/to/file> -n <project_name>
----

:leveloffset!:
:leveloffset: +2

// Module included in the following assemblies:
//
// * nodes/nodes-pods-configuring.adoc
// * nodes/nodes-cluster-pods-configuring
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="pod-disruption-eviction-policy_{context}"]
= Specifying the eviction policy for unhealthy pods

When you use pod disruption budgets (PDBs) to specify how many pods must be available simultaneously, you can also define the criteria for how unhealthy pods are considered for eviction.

You can choose one of the following policies:

IfHealthyBudget:: Running pods that are not yet healthy can be evicted only if the guarded application is not disrupted.

AlwaysAllow:: Running pods that are not yet healthy can be evicted regardless of whether the criteria in the pod disruption budget is met. This policy can help evict malfunctioning applications, such as ones with pods stuck in the `CrashLoopBackOff` state or failing to report the `Ready` status.
+
[NOTE]
====
It is recommended to set the `unhealthyPodEvictionPolicy` field to `AlwaysAllow` in the `PodDisruptionBudget` object to support the eviction of misbehaving applications during a node drain. The default behavior is to wait for the application pods to become healthy before the drain can proceed.
====

.Procedure

. Create a YAML file that defines a `PodDisruptionBudget` object and specify the unhealthy pod eviction policy:
+
.Example `pod-disruption-budget.yaml` file
[source,yaml]
----
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: my-pdb
spec:
  minAvailable: 2
  selector:
    matchLabels:
      name: my-pod
  unhealthyPodEvictionPolicy: AlwaysAllow <1>
----
<1> Choose either `IfHealthyBudget` or `AlwaysAllow` as the unhealthy pod eviction policy. The default is `IfHealthyBudget` when the `unhealthyPodEvictionPolicy` field is empty.

. Create the `PodDisruptionBudget` object by running the following command:
+
[source,terminal]
----
$ oc create -f pod-disruption-budget.yaml
----

With a PDB that has the `AlwaysAllow` unhealthy pod eviction policy set, you can now drain nodes and evict the pods for a malfunctioning application guarded by this PDB.

:leveloffset!:

[role="_additional-resources"]
.Additional resources

* xref:../nodes/clusters/nodes-cluster-enabling-features.adoc#nodes-cluster-enabling[Enabling features using feature gates]
* link:https://kubernetes.io/docs/tasks/run-application/configure-pdb/#unhealthy-pod-eviction-policy[Unhealthy Pod Eviction Policy] in the Kubernetes documentation

[id="post-install-rotate-remove-cloud-creds"]
== Rotating or removing cloud provider credentials

After installing {product-title}, some organizations require the rotation or removal of the cloud provider credentials that were used during the initial installation.

To allow the cluster to use the new credentials, you must update the secrets that the xref:../operators/operator-reference.adoc#cloud-credential-operator_cluster-operators-ref[Cloud Credential Operator (CCO)] uses to manage cloud provider credentials.

[id="ccoctl-rotate-remove-cloud-creds"]
=== Rotating cloud provider credentials with the Cloud Credential Operator utility

// Right now only IBM can do this, but it makes sense to set this up so that other clouds can be added.
The Cloud Credential Operator (CCO) utility `ccoctl` supports updating secrets for clusters installed on IBM Cloud.

//Rotating IBM Cloud credentials with ccoctl
:leveloffset: +3

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="refreshing-service-ids-ibm-cloud_{context}"]
= Rotating API keys

You can rotate API keys for your existing service IDs and update the corresponding secrets.

.Prerequisites

* You have configured the `ccoctl` binary.
* You have existing service IDs in a live {product-title} cluster installed.

.Procedure

* Use the `ccoctl` utility to rotate your API keys for the service IDs and update the secrets:
+
[source,terminal]
----
$ ccoctl <provider_name> refresh-keys \ <1>
    --kubeconfig <openshift_kubeconfig_file> \ <2>
    --credentials-requests-dir <path_to_credential_requests_directory> \ <3>
    --name <name> <4>
----
<1> The name of the provider. For example: `ibmcloud` or `powervs`.
<2> The `kubeconfig` file associated with the cluster. For example, `<installation_directory>/auth/kubeconfig`.
<3> The directory where the credential requests are stored.
<4> The name of the {product-title} cluster.
+
--
[NOTE]
====
If your cluster uses Technology Preview features that are enabled by the `TechPreviewNoUpgrade` feature set, you must include the `--enable-tech-preview` parameter.
====
--

:leveloffset!:

//Rotating cloud provider credentials manually
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc
// * authentication/managing_cloud_provider_credentials/cco-mode-mint.adoc
// * authentication/managing_cloud_provider_credentials/cco-mode-passthrough.adoc

:post-install:

:_mod-docs-content-type: PROCEDURE
[id="manually-rotating-cloud-creds_{context}"]
= Rotating cloud provider credentials manually
= Maintaining cloud provider credentials

If your cloud provider credentials are changed for any reason, you must manually update the secret that the Cloud Credential Operator (CCO) uses to manage cloud provider credentials.

The process for rotating cloud credentials depends on the mode that the CCO is configured to use. After you rotate credentials for a cluster that is using mint mode, you must manually remove the component credentials that were created by the removed credential.

////
[NOTE]
====
You can also use the command line interface to complete all parts of this procedure.
====
////

.Prerequisites

* Your cluster is installed on a platform that supports rotating cloud credentials manually with the CCO mode that you are using:

** For mint mode, Amazon Web Services (AWS) and Google Cloud Platform (GCP) are supported.

** For passthrough mode, Amazon Web Services (AWS), Microsoft Azure, Google Cloud Platform (GCP), {rh-openstack-first}, and VMware vSphere are supported.

* You have changed the credentials that are used to interface with your cloud provider.

* The new credentials have sufficient permissions for the mode CCO is configured to use in your cluster.

.Procedure

. In the *Administrator* perspective of the web console, navigate to *Workloads* -> *Secrets*.

. In the table on the *Secrets* page, find the root secret for your cloud provider.
+
[cols=2,options=header]
|===
|Platform
|Secret name

|AWS
|`aws-creds`

|Azure
|`azure-credentials`

|GCP
|`gcp-credentials`

|{rh-openstack}
|`openstack-credentials`

|VMware vSphere
|`vsphere-creds`

|===

. Click the *Options* menu {kebab} in the same row as the secret and select *Edit Secret*.

. Record the contents of the *Value* field or fields. You can use this information to verify that the value is different after updating the credentials.

. Update the text in the *Value* field or fields with the new authentication information for your cloud provider, and then click *Save*.

. If you are updating the credentials for a vSphere cluster that does not have the vSphere CSI Driver Operator enabled, you must force a rollout of the Kubernetes controller manager to apply the updated credentials.
+
[NOTE]
====
If the vSphere CSI Driver Operator is enabled, this step is not required.
====
+
To apply the updated vSphere credentials, log in to the {product-title} CLI as a user with the `cluster-admin` role and run the following command:
+
[source,terminal]
----
$ oc patch kubecontrollermanager cluster \
  -p='{"spec": {"forceRedeploymentReason": "recovery-'"$( date )"'"}}' \
  --type=merge
----
+
While the credentials are rolling out, the status of the Kubernetes Controller Manager Operator reports `Progressing=true`. To view the status, run the following command:
+
[source,terminal]
----
$ oc get co kube-controller-manager
----

. If the CCO for your cluster is configured to use mint mode, delete each component secret that is referenced by the individual `CredentialsRequest` objects.

.. Log in to the {product-title} CLI as a user with the `cluster-admin` role.

.. Get the names and namespaces of all referenced component secrets:
+
[source,terminal]
----
$ oc -n openshift-cloud-credential-operator get CredentialsRequest \
  -o json | jq -r '.items[] | select (.spec.providerSpec.kind=="<provider_spec>") | .spec.secretRef'
----
+
where `<provider_spec>` is the corresponding value for your cloud provider:
+
--
* AWS: `AWSProviderSpec`
* GCP: `GCPProviderSpec`
--
+
.Partial example output for AWS
+
[source,json]
----
{
  "name": "ebs-cloud-credentials",
  "namespace": "openshift-cluster-csi-drivers"
}
{
  "name": "cloud-credential-operator-iam-ro-creds",
  "namespace": "openshift-cloud-credential-operator"
}
----

.. Delete each of the referenced component secrets:
+
[source,terminal]
----
$ oc delete secret <secret_name> \//<1>
  -n <secret_namespace> <2>
----
+
<1> Specify the name of a secret.
<2> Specify the namespace that contains the secret.
+
.Example deletion of an AWS secret
+
[source,terminal]
----
$ oc delete secret ebs-cloud-credentials -n openshift-cluster-csi-drivers
----
+
You do not need to manually delete the credentials from your provider console. Deleting the referenced component secrets will cause the CCO to delete the existing credentials from the platform and create new ones.

.Verification

To verify that the credentials have changed:

. In the *Administrator* perspective of the web console, navigate to *Workloads* -> *Secrets*.

. Verify that the contents of the *Value* field or fields have changed.

////
// Provider-side verification also possible, though cluster-side is cleaner process.
. To verify that the credentials have changed from the console of your cloud provider:

.. Get the `CredentialsRequest` CR names for your platform:
+
[source,terminal]
----
$ oc -n openshift-cloud-credential-operator get CredentialsRequest -o json | jq -r '.items[] | select (.spec[].kind=="<provider_spec>") | .metadata.name'
----
+
Where `<provider_spec>` is the corresponding value for your cloud provider: `AWSProviderSpec` for AWS, `AzureProviderSpec` for Azure, or `GCPProviderSpec` for GCP.
+
.Example output for AWS
+
[source,terminal]
----
aws-ebs-csi-driver-operator
cloud-credential-operator-iam-ro
openshift-image-registry
openshift-ingress
openshift-machine-api-aws
----

.. Get the IAM username that corresponds to each `CredentialsRequest` CR name:
+
[source,terminal]
----
$ oc get credentialsrequest <cr_name> -n openshift-cloud-credential-operator -o json | jq -r ".status.providerStatus"
----
+
Where `<cr_name>` is the name of a `CredentialsRequest` CR.
+
.Example output for AWS
+
[source,json]
----
{
  "apiVersion": "cloudcredential.openshift.io/v1",
  "kind": "AWSProviderStatus",
  "policy": "<example-iam-username-policy>",
  "user": "<example-iam-username>"
}
----
+
Where `<example-iam-username>` is the name of an IAM user on the cloud provider.

.. For each IAM username, view the details for the user on the cloud provider. The credentials should show that they were created after being rotated on the cluster.
////

:!post-install:

:leveloffset!:

[role="_additional-resources"]
.Additional resources
* xref:../storage/container_storage_interface/persistent-storage-csi-vsphere.adoc[vSphere CSI Driver Operator]

//Removing cloud provider credentials manually
:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="manually-removing-cloud-creds_{context}"]
= Removing cloud provider credentials

After installing an {product-title} cluster with the Cloud Credential Operator (CCO) in mint mode, you can remove the administrator-level credential secret from the `kube-system` namespace in the cluster. The administrator-level credential is required only during changes that require its elevated permissions, such as upgrades.

[NOTE]
====
Prior to a non z-stream upgrade, you must reinstate the credential secret with the administrator-level credential. If the credential is not present, the upgrade might be blocked.
====

.Prerequisites

* Your cluster is installed on a platform that supports removing cloud credentials from the CCO. Supported platforms are AWS and GCP.

.Procedure

. In the *Administrator* perspective of the web console, navigate to *Workloads* -> *Secrets*.

. In the table on the *Secrets* page, find the root secret for your cloud provider.
+
[cols=2,options=header]
|===
|Platform
|Secret name

|AWS
|`aws-creds`

|GCP
|`gcp-credentials`

|===

. Click the *Options* menu {kebab} in the same row as the secret and select *Delete Secret*.

:leveloffset!:

//These additional resources are for the "Rotating or removing cloud provider credentials" section, do not separate them from that content.
[role="_additional-resources"]
.Additional resources
* xref:../authentication/managing_cloud_provider_credentials/about-cloud-credential-operator.adoc#about-cloud-credential-operator[About the Cloud Credential Operator]
* xref:../authentication/managing_cloud_provider_credentials/cco-mode-passthrough.adoc#admin-credentials-root-secret-formats_cco-mode-passthrough[Admin credentials root secret format]

[id="post-install-must-gather-disconnected"]
== Configuring image streams for a disconnected cluster

After installing {product-title} in a disconnected environment, configure the image streams for the Cluster Samples Operator and the `must-gather` image stream.

:leveloffset: +2

// Module included in the following assemblies:
//
// * installing/install_config/installing-restricted-networks-preparations.adoc
// * openshift_images/samples-operator-alt-registry.adoc
// * openshift_images/configuring-samples-operator.adoc

[id="installation-images-samples-disconnected-mirroring-assist_{context}"]
= Cluster Samples Operator assistance for mirroring

During installation, {product-title} creates a config map named `imagestreamtag-to-image` in the `openshift-cluster-samples-operator` namespace. The `imagestreamtag-to-image` config map contains an entry, the populating image, for each image stream tag.

The format of the key for each entry in the data field in the config map is `<image_stream_name>_<image_stream_tag_name>`.

During a disconnected installation of {product-title}, the status of the Cluster Samples Operator is set to `Removed`. If you choose to change it to `Managed`, it installs samples.
[NOTE]
====
The use of samples in a network-restricted or discontinued environment may require access to services external to your network. Some example services include: Github, Maven Central, npm, RubyGems, PyPi and others. There might be additional steps to take that allow the cluster samples operators's objects to reach the services they require.
====

You can use this config map as a reference for which images need to be mirrored for your image streams to import.

* While the Cluster Samples Operator is set to `Removed`, you can create your mirrored registry, or determine which existing mirrored registry you want to use.
* Mirror the samples you want to the mirrored registry using the new config map as your guide.
* Add any of the image streams you did not mirror to the `skippedImagestreams` list of the Cluster Samples Operator configuration object.
* Set `samplesRegistry` of the Cluster Samples Operator configuration object to the mirrored registry.
* Then set the Cluster Samples Operator to `Managed` to install the image streams you have mirrored.

:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc
// * openshift_images/samples-operator-alt-registry.adoc

:restrictednetwork:


:_mod-docs-content-type: PROCEDURE
[id="installation-restricted-network-samples_{context}"]
= Using Cluster Samples Operator image streams with alternate or mirrored registries

Most image streams in the `openshift` namespace managed by the Cluster Samples Operator
point to images located in the Red Hat registry at link:https://registry.redhat.io[registry.redhat.io].
Mirroring
will not apply to these image streams.

[NOTE]
====
The `cli`, `installer`, `must-gather`, and `tests` image streams, while
part of the install payload, are not managed by the Cluster Samples Operator. These are
not addressed in this procedure.
====

[IMPORTANT]
====
The Cluster Samples Operator must be set to `Managed` in a disconnected environment. To install the image streams, you have a mirrored registry.
====

.Prerequisites
* Access to the cluster as a user with the `cluster-admin` role.
* Create a pull secret for your mirror registry.

.Procedure

. Access the images of a specific image stream to mirror, for example:
+
[source,terminal]
----
$ oc get is <imagestream> -n openshift -o json | jq .spec.tags[].from.name | grep registry.redhat.io
----
+
. Mirror images from link:https://registry.redhat.io[registry.redhat.io] associated with any image streams you need
in the restricted network environment into one of the defined mirrors, for example:
+
[source,terminal]
----
$ oc image mirror registry.redhat.io/rhscl/ruby-25-rhel7:latest ${MIRROR_ADDR}/rhscl/ruby-25-rhel7:latest
----

. Create the cluster's image configuration object:
+
[source,terminal]
----
$ oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..5000=$path/ca.crt -n openshift-config
----

. Add the required trusted CAs for the mirror in the cluster's image
configuration object:
+
[source,terminal]
----
$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge
----

. Update the `samplesRegistry` field in the Cluster Samples Operator configuration object
to contain the `hostname` portion of the mirror location defined in the mirror
configuration:
+
[source,terminal]
----
$ oc edit configs.samples.operator.openshift.io -n openshift-cluster-samples-operator
----
+
[NOTE]
====
This is required because the image stream import process does not use the mirror or search mechanism at this time.
====
+
. Add any image streams that are not mirrored into the `skippedImagestreams` field
of the Cluster Samples Operator configuration object. Or if you do not want to support
any of the sample image streams, set the Cluster Samples Operator to `Removed` in the
Cluster Samples Operator configuration object.
+
[NOTE]
====
The Cluster Samples Operator issues alerts if image stream imports are failing but the Cluster Samples Operator is either periodically retrying or does not appear to be retrying them.
====
+
Many of the templates in the `openshift` namespace
reference the image streams. So using `Removed` to purge both the image streams
and templates will eliminate the possibility of attempts to use them if they
are not functional because of any missing image streams.

:!restrictednetwork:


:leveloffset!:

:leveloffset: +2

// Module included in the following assemblies:
//
// * post_installation_configuration/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="installation-preparing-restricted-cluster-to-gather-support-data_{context}"]
= Preparing your cluster to gather support data

Clusters using a restricted network must import the default must-gather image to gather debugging data for Red Hat support. The must-gather image is not imported by default, and clusters on a restricted network do not have access to the internet to pull the latest image from a remote repository.

.Procedure

. If you have not added your mirror registry's trusted CA to your cluster's image configuration object as part of the Cluster Samples Operator configuration, perform the following steps:
.. Create the cluster's image configuration object:
+
[source,terminal]
----
$ oc create configmap registry-config --from-file=${MIRROR_ADDR_HOSTNAME}..5000=$path/ca.crt -n openshift-config
----

.. Add the required trusted CAs for the mirror in the cluster's image
configuration object:
+
[source,terminal]
----
$ oc patch image.config.openshift.io/cluster --patch '{"spec":{"additionalTrustedCA":{"name":"registry-config"}}}' --type=merge
----

. Import the default must-gather image from your installation payload:
+
[source,terminal]
----
$ oc import-image is/must-gather -n openshift
----

When running the `oc adm must-gather` command, use the `--image` flag and point to the payload image, as in the following example:
[source,terminal]
----
$ oc adm must-gather --image=$(oc adm release info --image-for must-gather)
----

:leveloffset!:

:leveloffset: +1

// Module included in the following assemblies:
// * openshift_images/cluster-tasks.adoc

:_mod-docs-content-type: PROCEDURE
[id="images-cluster-sample-imagestream-import_{context}"]
= Configuring periodic importing of Cluster Sample Operator image stream tags

You can ensure that you always have access to the latest versions of the Cluster Sample Operator images by periodically importing the image stream tags when new versions become available.

.Procedure

. Fetch all the imagestreams in the `openshift` namespace by running the following command:
+
[source,terminal]
----
oc get imagestreams -nopenshift
----

. Fetch the tags for every imagestream in the `openshift` namespace by running the following command:
+
[source,terminal]
----
$ oc get is <image-stream-name> -o jsonpath="{range .spec.tags[*]}{.name}{'\t'}{.from.name}{'\n'}{end}" -nopenshift
----
+
For example:
+
[source,terminal]
----
$ oc get is ubi8-openjdk-17 -o jsonpath="{range .spec.tags[*]}{.name}{'\t'}{.from.name}{'\n'}{end}" -nopenshift
----
+
.Example output
[source,terminal]
----
1.11	registry.access.redhat.com/ubi8/openjdk-17:1.11
1.12	registry.access.redhat.com/ubi8/openjdk-17:1.12
----

. Schedule periodic importing of images for each tag present in the image stream by running the following command:
+
[source,terminal]
----
$ oc tag <repository/image> <image-stream-name:tag> --scheduled -nopenshift
----
+
For example:
+
[source,terminal]
----
$ oc tag registry.access.redhat.com/ubi8/openjdk-17:1.11 ubi8-openjdk-17:1.11 --scheduled -nopenshift
$ oc tag registry.access.redhat.com/ubi8/openjdk-17:1.12 ubi8-openjdk-17:1.12 --scheduled -nopenshift
----
+
This command causes {product-title} to periodically update this particular image stream tag. This period is a cluster-wide setting set to 15 minutes by default.

. Verify the scheduling status of the periodic import by running the following command:
+
[source,terminal]
----
oc get imagestream <image-stream-name> -o jsonpath="{range .spec.tags[*]}Tag: {.name}{'\t'}Scheduled: {.importPolicy.scheduled}{'\n'}{end}" -nopenshift
----
+
For example:
+
[source,terminal]
----
oc get imagestream ubi8-openjdk-17 -o jsonpath="{range .spec.tags[*]}Tag: {.name}{'\t'}Scheduled: {.importPolicy.scheduled}{'\n'}{end}" -nopenshift
----
+
.Example output
[source,terminal]
----
Tag: 1.11	Scheduled: true
Tag: 1.12	Scheduled: true
----

:leveloffset!:

//# includes=_attributes/common-attributes,modules/images-update-global-pull-secret,modules/differences-between-machinesets-and-machineconfigpool,modules/machineset-manually-scaling,modules/machineset-delete-policy,modules/nodes-scheduler-node-selectors-cluster,snippets/worker-latency-profile-intro,modules/nodes-cluster-worker-latency-profiles-about,modules/nodes-cluster-worker-latency-profiles-using,modules/machineset-creating,modules/creating-an-infra-node,modules/creating-infra-machines,modules/binding-infra-node-workloads-using-taints-tolerations,modules/infrastructure-moving-router,modules/infrastructure-moving-registry,modules/infrastructure-moving-monitoring,modules/infrastructure-moving-logging,modules/cluster-autoscaler-about,modules/cluster-autoscaler-cr,modules/deploying-resource,modules/machine-autoscaler-about,modules/machine-autoscaler-cr,modules/nodes-clusters-cgroups-2,modules/nodes-cluster-enabling-features-about,modules/nodes-cluster-enabling-features-console,modules/snippets/nodes-cluster-enabling-features-verification,modules/nodes-cluster-enabling-features-cli,modules/about-etcd-encryption,modules/etcd-encryption-types,modules/enabling-etcd-encryption,modules/disabling-etcd-encryption,modules/backup-etcd,modules/etcd-defrag,modules/dr-restoring-cluster-state,modules/dr-scenario-cluster-state-issues,modules/nodes-pods-pod-disruption-about,modules/nodes-pods-pod-disruption-configuring,modules/pod-disruption-eviction-policy,modules/refreshing-service-ids-ibm-cloud,modules/manually-rotating-cloud-creds,modules/manually-removing-cloud-creds,modules/installation-images-samples-disconnected-mirroring-assist,modules/installation-restricted-network-samples,modules/installation-preparing-restricted-cluster-to-gather-support-data,modules/images-cluster-sample-imagestream-import
